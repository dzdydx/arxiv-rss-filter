<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 28 Jul 2025 12:28:59 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Mon, 28 Jul 2025 12:28:59 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back</title>
        <link>https://arxiv.org/abs/2507.18661</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.18661v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ruixing Zhang, Yang Zhang, Tongyu Zhu, Leilei Sun, Weifeng Lv</dc:creator>
        <description><![CDATA[
            背景：多数现有下一位置预测模型未像人类一样基于地图推理，而视觉语言模型在视觉感知和推理上能力较强。方法：先提出Vision - Guided Location Search评估通用VLM的轨迹推理能力，后提出VLMLocPredictor，分两阶段，一是设计监督微调任务让VLM理解道路网络和轨迹结构，二是引入基于视觉地图反馈的强化学习提升模型预测能力。效果：在四个城市数据集实验中达SOTA，跨城市泛化能力优于其他基于LLM的方法。
            arXiv:2507.18661v1 Announce Type: new 
Abstract: Next Location Prediction is a fundamental task in the study of human mobility, with wide-ranging applications in transportation planning, urban governance, and epidemic forecasting. In practice, when humans attempt to predict the next location in a trajectory, they often visualize the trajectory on a map and reason based on road connectivity and movement trends. However, the vast majority of existing next-location prediction models do not reason over maps \textbf{in the way that humans do}. Fortunately, the recent development of Vision-Language Models (VLMs) has demonstrated strong capabilities in visual perception and even visual reasoning. This opens up a new possibility: by rendering both the road network and trajectory onto an image and leveraging the reasoning abilities of VLMs, we can enable models to perform trajectory inference in a human-like manner. To explore this idea, we first propose a method called Vision-Guided Location Search (VGLS), which evaluates whether a general-purpose VLM is capable of trajectory-based reasoning without modifying any of its internal parameters. Based on insights from the VGLS results, we further propose our main approach: VLMLocPredictor, which is composed of two stages: In the first stage, we design two Supervised Fine-Tuning (SFT) tasks that help the VLM understand road network and trajectory structures and acquire basic reasoning ability on such visual inputs. In the second stage, we introduce Reinforcement Learning from Visual Map Feedback, enabling the model to self-improve its next-location prediction ability through interaction with the environment. Experiments conducted on datasets from four different cities show that our method achieves state-of-the-art (SOTA) performance and exhibits superior cross-city generalization compared to other LLM-based approaches.
        ]]></description>
    </item>
    <item>
        <title>Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs</title>
        <link>https://arxiv.org/abs/2507.18668</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.18668v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Donghee Han, Daehee Kim, Minjun Lee, Daeyoung Roh, Keejun Han, Mun Yong Yi</dc:creator>
        <description><![CDATA[
            背景：在线学习兴起促使多种知识追踪（KT）方法发展，但现有方法在利用大图和长学习序列时存在计算成本增加问题。方法：提出基于双图注意力的知识追踪模型（DGAKT），该图神经网络模型利用表示学生 - 练习 - 知识概念关系的子图高阶信息，采用基于子图的方法提升计算效率。效果：与全全局图模型相比，显著降低内存和计算需求，实验表明其性能优于现有KT模型，资源效率创新高。
            arXiv:2507.18668v1 Announce Type: new 
Abstract: The rise of online learning has led to the development of various knowledge tracing (KT) methods. However, existing methods have overlooked the problem of increasing computational cost when utilizing large graphs and long learning sequences. To address this issue, we introduce Dual Graph Attention-based Knowledge Tracing (DGAKT), a graph neural network model designed to leverage high-order information from subgraphs representing student-exercise-KC relationships. DGAKT incorporates a subgraph-based approach to enhance computational efficiency. By processing only relevant subgraphs for each target interaction, DGAKT significantly reduces memory and computational requirements compared to full global graph models. Extensive experimental results demonstrate that DGAKT not only outperforms existing KT models but also sets a new standard in resource efficiency, addressing a critical need that has been largely overlooked by prior KT approaches.
        ]]></description>
    </item>
    <item>
        <title>A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions</title>
        <link>https://arxiv.org/abs/2507.18910</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.18910v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Agada Joseph Oche, Ademola Glory Folashade, Tirthankar Ghosal, Arpan Biswas</dc:creator>
        <description><![CDATA[
            检索增强生成（RAG）结合大语言模型与信息检索系统，提升自然语言处理效果。本文对RAG进行全面综述，追溯其从开放域问答到多领域应用的发展。介绍了RAG缓解参数模型幻觉和知识过时问题的动机，详细分析其核心技术组件。按年份梳理关键里程碑与研究趋势，探讨其在企业系统部署的挑战。对比评估不同RAG实现方式的性能，分析现存检索质量、隐私等挑战，还介绍了混合检索等新兴解决方案，为未来知识密集型NLP系统发展指明方向。
            arXiv:2507.18910v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.
        ]]></description>
    </item>
    <item>
        <title>Benchmarking Multimodal Understanding and Complex Reasoning for ESG Tasks</title>
        <link>https://arxiv.org/abs/2507.18932</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.18932v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lei Zhang, Xin Zhou, Chaoyue He, Di Wang, Yi Wu, Hong Xu, Wei Liu, Chunyan Miao</dc:creator>
        <description><![CDATA[
            背景：ESG报告结构多样、多模态，现有AI系统难以进行可靠文档级推理，且ESG领域缺乏专用基准。方法：构建MMESGBench基准数据集，通过人机协作多阶段流程，先由多模态大语言模型生成候选问答对，再由大语言模型验证，最后专家验证校准。效果：该数据集含933个有效问答对，涵盖7种文档类型和3类ESG源。实验表明，多模态和检索增强模型显著优于纯文本基线，尤其在视觉和跨页任务上。
            arXiv:2507.18932v1 Announce Type: new 
Abstract: Environmental, Social, and Governance (ESG) reports are essential for evaluating sustainability practices, ensuring regulatory compliance, and promoting financial transparency. However, these documents are often lengthy, structurally diverse, and multimodal, comprising dense text, structured tables, complex figures, and layout-dependent semantics. Existing AI systems often struggle to perform reliable document-level reasoning in such settings, and no dedicated benchmark currently exists in ESG domain. To fill the gap, we introduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted to evaluate multimodal understanding and complex reasoning across structurally diverse and multi-source ESG documents. This dataset is constructed via a human-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates candidate question-answer (QA) pairs by jointly interpreting rich textual, tabular, and visual information from layout-aware document pages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each QA pair. This automated process is followed by an expert-in-the-loop validation, where domain specialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct document types and three major ESG source categories. Questions are categorized as single-page, cross-page, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments validate that multimodal and retrieval-augmented models substantially outperform text-only baselines, particularly on visually grounded and cross-page tasks. MMESGBench is publicly available as an open-source dataset at https://github.com/Zhanglei1103/MMESGBench.
        ]]></description>
    </item>
    <item>
        <title>ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs</title>
        <link>https://arxiv.org/abs/2507.19031</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19031v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang, Yujie Sun, Zheng Liang, Yibing Zhan, Dapeng Tao</dc:creator>
        <description><![CDATA[
            背景：GNN-to-MLP方法可将图神经网络知识提炼到多层感知机以加速计算，但现有方法无法灵活动态调整推理成本和精度。方法：提出ProGMLP框架，采用渐进训练结构依次训练多个MLP学生模型，结合渐进知识蒸馏迭代优化蒸馏过程，运用渐进混合增强生成更难的混合样本提升泛化能力。效果：在八个真实图数据集上实验表明，ProGMLP能在动态适应不同运行场景时保持高精度，适用于多样应用场景。
            arXiv:2507.19031v1 Announce Type: new 
Abstract: GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate Graph Neural Networks (GNNs) by distilling their knowledge into simpler Multi-Layer Perceptrons (MLPs). These methods bridge the gap between the expressive power of GNNs and the computational efficiency of MLPs, making them well-suited for resource-constrained environments. However, existing G2M methods are limited by their inability to flexibly adjust inference cost and accuracy dynamically, a critical requirement for real-world applications where computational resources and time constraints can vary significantly. To address this, we introduce a Progressive framework designed to offer flexible and on-demand trade-offs between inference cost and accuracy for GNN-to-MLP knowledge distillation (ProGMLP). ProGMLP employs a Progressive Training Structure (PTS), where multiple MLP students are trained in sequence, each building on the previous one. Furthermore, ProGMLP incorporates Progressive Knowledge Distillation (PKD) to iteratively refine the distillation process from GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance generalization by progressively generating harder mixed samples. Our approach is validated through comprehensive experiments on eight real-world graph datasets, demonstrating that ProGMLP maintains high accuracy while dynamically adapting to varying runtime scenarios, making it highly effective for deployment in diverse application settings.
        ]]></description>
    </item>
    <item>
        <title>Dynamics-Informed Reservoir Computing with Visibility Graphs</title>
        <link>https://arxiv.org/abs/2507.19046</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19046v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Charlotte Geier (Dynamics Group, Hamburg University of Technology), Merten Stender (Chair of Cyber-Physical Systems in Mechanical Engineering, Technische Universit\"at Berlin, Germany)</dc:creator>
        <description><![CDATA[
            准确预测复杂非线性时间序列是工程和科学领域的难题。传统深度学习计算效率低，储层计算（RC）虽仅训练读出层，但随机的储层图架构常导致网络性能不佳且过大。为此，研究提出动力学信息储层计算（DyRC）框架，采用可见性图（VG）技术将时间序列数据转化为网络，直接从训练序列推断储层网络结构，避免昂贵的超参数调整。在Duffing振子预测任务中评估发现，相比相同大小、谱半径和密度的随机图，DyRC - VG预测质量更高、性能更稳定。
            arXiv:2507.19046v1 Announce Type: new 
Abstract: Accurate prediction of complex and nonlinear time series remains a challenging problem across engineering and scientific disciplines. Reservoir computing (RC) offers a computationally efficient alternative to traditional deep learning by training only the read-out layer while employing a randomly structured and fixed reservoir network. Despite its advantages, the largely random reservoir graph architecture often results in suboptimal and oversized networks with poorly understood dynamics. Addressing this issue, we propose a novel Dynamics-Informed Reservoir Computing (DyRC) framework that systematically infers the reservoir network structure directly from the input training sequence. This work proposes to employ the visibility graph (VG) technique, which converts time series data into networks by representing measurement points as nodes linked by mutual visibility. The reservoir network is constructed by directly adopting the VG network from a training data sequence, leveraging the parameter-free visibility graph approach to avoid expensive hyperparameter tuning. This process results in a reservoir that is directly informed by the specific dynamics of the prediction task under study. We assess the DyRC-VG method through prediction tasks involving the canonical nonlinear Duffing oscillator, evaluating prediction accuracy and consistency. Compared to an Erd\H{o}s-R\'enyi graph of the same size, spectral radius, and comparable density, we observe higher prediction quality and more consistent performance over repeated implementations in the DyRC-VG.
        ]]></description>
    </item>
    <item>
        <title>ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment</title>
        <link>https://arxiv.org/abs/2507.19058</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19058v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chong Xia, Shengjun Zhang, Fangfu Liu, Chang Liu, Khodchaphun Hirunyaratsameewong, Yueqi Duan</dc:creator>
        <description><![CDATA[
            长期3D场景生成可用于长期视频合成和3D场景重建，但现有方法生成的视图序列存在语义漂移问题。为此提出ScenePainter框架，通过将外画器的特定场景先验与当前场景理解对齐来解决该问题。具体引入名为SceneConceptGraph的分层图结构，构建多级场景概念间的关系，指导外画器生成一致的新视图并动态优化以增加多样性。实验表明，该框架克服了语义漂移问题，能生成更连贯、沉浸感更强的3D视图序列。
            arXiv:2507.19058v1 Announce Type: new 
Abstract: Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a "navigate-and-imagine" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences. Project Page: https://xiac20.github.io/ScenePainter/.
        ]]></description>
    </item>
    <item>
        <title>GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network</title>
        <link>https://arxiv.org/abs/2507.19095</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19095v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Binxiong Li, Xu Xiang, Xue Li, Binyu Zhao, Yujie Liu, Huijie Tang, Benhan Yang, Zhixuan Chen</dc:creator>
        <description><![CDATA[
            在现代数据分析中，属性图聚类至关重要，但图数据复杂、节点属性异构，利用图信息聚类仍具挑战。为此提出GCL - GCN模型，引入结合中心性编码和空间关系的Graphormer模块，有效捕捉节点全局与局部信息，提升节点表示质量；还提出对比学习模块，增强特征表示的判别能力。预训练阶段通过对比学习增加特征区分度。在六个数据集上实验表明，该模型在聚类质量和鲁棒性上优于14种先进方法，如在Cora数据集上，ACC、NMI和ARI较MBN分别提升4.94%、13.01%和10.97%。
            arXiv:2507.19095v1 Announce Type: new 
Abstract: Attributed graph clustering holds significant importance in modern data analysis. However, due to the complexity of graph data and the heterogeneity of node attributes, leveraging graph information for clustering remains challenging. To address this, we propose a novel deep graph clustering model, GCL-GCN, specifically designed to address the limitations of existing models in capturing local dependencies and complex structures when dealing with sparse and heterogeneous graph data. GCL-GCN introduces an innovative Graphormer module that combines centrality encoding and spatial relationships, effectively capturing both global and local information between nodes, thereby enhancing the quality of node representations. Additionally, we propose a novel contrastive learning module that significantly enhances the discriminative power of feature representations. In the pre-training phase, this module increases feature distinction through contrastive learning on the original feature matrix, ensuring more identifiable initial representations for subsequent graph convolution and clustering tasks. Extensive experimental results on six datasets demonstrate that GCL-GCN outperforms 14 advanced methods in terms of clustering quality and robustness. Specifically, on the Cora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%, respectively, compared to the primary comparison method MBN.
        ]]></description>
    </item>
    <item>
        <title>RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow</title>
        <link>https://arxiv.org/abs/2507.19280</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19280v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Liang Yao, Fan Liu, Hongbo Lu, Chuanyi Zhang, Rui Min, Shengxiang Xu, Shimin Di, Pai Peng</dc:creator>
        <description><![CDATA[
            背景：遥感影像包含大量非结构化空间数据，现有方法依赖监督微调范式，难以满足复杂推理需求。方法：提出RemoteReasoner，集成多模态大语言模型（MLLM）解读用户指令、定位目标，结合任务自适应策略生成多粒度输出，用强化学习训练框架。效果：初步实验表明，RemoteReasoner在多粒度推理任务中表现出色，包括区域级和像素级，还能实现现有推理流程无法完成的轮廓提取等新功能。
            arXiv:2507.19280v1 Announce Type: new 
Abstract: Remote sensing imagery presents vast, inherently unstructured spatial data, demanding sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should be somewhat autonomous, where predefined ground-truth reasoning paths do not constrain the learning process. Furthermore, its architecture ought to be unified yet flexible, enabling the model to perform diverse reasoning tasks with distinct output formats through a single forward pass. Existing remote sensing approaches fail to address these requirements, as they rely on supervised fine-tuning paradigms that constrain the autonomy of reasoning. To this end, we propose RemoteReasoner, a flexible and robust workflow for remote sensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task adaptation strategies that enable multi-granularity output generation. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient autonomy for precise reasoning. At the inference stage, our adaptation strategies enable diverse output formats at inference time without requiring task-specific decoders or further fine-tuning. Preliminary experiments demonstrated that RemoteReasoner achieves remarkable performance across multi-granularity reasoning tasks, including region-level and pixel-level. Additionally, our framework enables novel capabilities such as the contour extraction task beyond the reach of existing reasoning pipelines.
        ]]></description>
    </item>
    <item>
        <title>Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs</title>
        <link>https://arxiv.org/abs/2507.19334</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19334v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shuo Yang, Zheyu Zhang, Bardh Prenkaj, Gjergji Kasneci</dc:creator>
        <description><![CDATA[
            背景：表格数据在各领域至关重要，但因隐私和收集成本，高质量数据集稀缺。现有大语言模型进行表格数据增强存在特征依赖建模有偏差和采样计算开销大的问题。方法：提出SPADA轻量级生成框架，通过大语言模型诱导图捕获稀疏依赖，将特征视为节点，仅基于父节点合成值，探索非参数和条件归一化流两种合成策略。效果：在四个数据集实验显示，相比扩散法减少4%约束违规，比大语言模型基线加速近9500倍。
            arXiv:2507.19334v1 Announce Type: new 
Abstract: Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models (LLMs) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures sparse dependencies via an LLM-induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over LLM-based baselines.
        ]]></description>
    </item>
    <item>
        <title>Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2507.19102</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19102v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）中基于大语言模型（LLM）进行效用判断计算成本高，限制了评估段落数量，不利于复杂查询。方法：提出将LLM的效用判断能力提炼到更小、更高效的模型中，聚焦基于效用的选择而非排序，通过滑动窗口法动态选段，让学生模型向教师LLM学习伪答案生成和效用判断。效果：显著降低计算成本，提高答案质量，实验表明基于效用的选择比相关性排序更能提升复杂问题的答案生成性能。
            arXiv:2507.19102v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.
        ]]></description>
    </item>
    <item>
        <title>Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2507.19333</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19333v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Minghao Tang, Shiyu Ni, Jiafeng Guo, Keping Bi</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）在知识密集型任务中应用广泛，但检索到的低质量文本会影响其效果，增强大语言模型（LLMs）对噪声的鲁棒性至关重要。方法：提出一种简单有效的方法Passage Injection，将检索到的文本明确纳入LLMs的推理过程，以增强模型识别和抵抗噪声文本的能力。效果：在四个事实问答数据集上的实验表明，该方法显著提升了整体RAG性能，在两种噪声检索设置下也能持续提升鲁棒性，还能有效利用有用文本。
            arXiv:2507.19333v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. The code can be found \href{here}{https://github.com/mh-tang/Passage-Injection}.
        ]]></description>
    </item>
    <item>
        <title>XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</title>
        <link>https://arxiv.org/abs/2405.06270</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2405.06270v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio</dc:creator>
        <description><![CDATA[
            临床决策支持系统需要高精度、公平且能避免漏诊的模型。该研究提出知识引导的上下文学习（ICL）框架，让大语言模型（LLMs）处理结构化临床数据，集成特定领域特征分组、平衡的小样本示例和特定任务提示策略。在70种ICL设计中评估该方法，与传统机器学习模型对比。结果显示，传统模型在平衡精确率 - 召回率上占优，但采用含领域知识叙事提示的LLMs召回率更高，显著降低性别偏差，缩小公平差距一个数量级，虽推理延迟增加，但有零样本部署和公平性优势。
            arXiv:2405.06270v4 Announce Type: replace 
Abstract: Clinical decision support systems require models that are not only highly accurate but also equitable and sensitive to the implications of missed diagnoses. In this study, we introduce a knowledge-guided in-context learning (ICL) framework designed to enable large language models (LLMs) to effectively process structured clinical data. Our approach integrates domain-specific feature groupings, carefully balanced few-shot examples, and task-specific prompting strategies. We systematically evaluate this method across seventy distinct ICL designs by various prompt variations and two different communication styles-natural-language narrative and numeric conversational-and compare its performance to robust classical machine learning (ML) benchmarks on tasks involving heart disease and diabetes prediction.
  Our findings indicate that while traditional ML models maintain superior performance in balanced precision-recall scenarios, LLMs employing narrative prompts with integrated domain knowledge achieve higher recall and significantly reduce gender bias, effectively narrowing fairness disparities by an order of magnitude. Despite the current limitation of increased inference latency, LLMs provide notable advantages, including the capacity for zero-shot deployment and enhanced equity. This research offers the first comprehensive analysis of ICL design considerations for applying LLMs to tabular clinical tasks and highlights distillation and multimodal extensions as promising directions for future research.
        ]]></description>
    </item>
    <item>
        <title>Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2505.16142</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.16142v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shicheng Xu, Liang Pang, Yunchang Zhu, Jia Gu, Zihao Wei, Jingcheng Deng, Feiyang Pan, Huawei Shen, Xueqi Cheng</dc:creator>
        <description><![CDATA[
            背景：通过监督微调（SFT）从教师模型向学生模型提炼推理路径可提升小语言模型推理能力，但SFT会使教师模型推理的多分支结构扁平化，无法有效提炼。方法：提出基于强化学习的蒸馏框架RLKD，利用生成结构奖励模型（GSRM）将推理路径转化为多步元推理 - 解决步骤，并计算奖励衡量师生推理的结构对齐，结合强化学习使学生模型内化教师的隐式多分支推理结构。效果：实验表明，在仅使用0.1%数据的强化学习机制下，RLKD超越标准SFT - RL管道，比基于SFT的蒸馏释放出更大的学生推理潜力。
            arXiv:2505.16142v3 Announce Type: replace 
Abstract: Distilling reasoning paths from teacher to student models via supervised fine-tuning (SFT) provides a shortcut for improving the reasoning ability of smaller Large Language Models (LLMs). However, the reasoning paths generated by teacher models often reflect only surface-level traces of their underlying authentic reasoning. Insights from cognitive neuroscience suggest that authentic reasoning involves a complex interweaving between meta-reasoning (which selects appropriate sub-problems from multiple candidates) and solving (which addresses the sub-problem). This implies authentic reasoning has an implicit multi-branch structure. Supervised fine-tuning collapses this rich structure into a flat sequence of token prediction in the teacher's reasoning path, preventing effective distillation of this structure to students. To address this limitation, we propose RLKD, a reinforcement learning (RL)-based distillation framework guided by a novel Generative Structure Reward Model (GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving steps and computes rewards to measure structural alignment between student and teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to internalize the teacher's implicit multi-branch reasoning structure rather than merely mimicking fixed output paths. Experiments show RLKD surpasses standard SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime, unlocking greater student reasoning potential than SFT-based distillation.
        ]]></description>
    </item>
    <item>
        <title>Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience</title>
        <link>https://arxiv.org/abs/2506.00842</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.00842v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiawei Gu, Ziting Xian, Yuanzhen Xie, Ye Liu, Enjie Liu, Ruichao Zhong, Mochi Gao, Yunzhi Tan, Bo Hu, Zang Li</dc:creator>
        <description><![CDATA[
            大语言模型在纯文本任务上表现出色，但在处理表格和数据库等结构化数据时表现不佳，原因在于预训练时接触不足和文本到结构的转换机制僵化。为此，研究人员提出了CoRE框架，该框架构建经验记忆表征，并通过对比上下文学习增强泛化能力，以模拟人类的知识迁移。在Text-to-SQL和TableQA任务上的实验表明，CoRE显著提升了性能，平均增益分别为3.44%和4.24%，在具有挑战性的任务上增益高达17.2%。蒙特卡罗树搜索生成的经验记忆将训练数据扩大了8 - 9倍，增强了数据的多样性和领域覆盖范围。
            arXiv:2506.00842v2 Announce Type: replace 
Abstract: Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.
        ]]></description>
    </item>
    <item>
        <title>All in One: Visual-Description-Guided Unified Point Cloud Segmentation</title>
        <link>https://arxiv.org/abs/2507.05211</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.05211v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer</dc:creator>
        <description><![CDATA[
            3D点云统一分割对场景理解至关重要，但受稀疏结构、标注有限等因素阻碍。现有方法因监督不足和缺乏多模态线索，难以捕捉丰富语义和上下文信息。为此，提出VDG-Uni3DSeg框架，集成预训练视觉语言模型和大语言模型，利用大模型生成的文本描述和互联网参考图像，融入多模态线索。还设计了语义视觉对比损失和空间增强模块。该方法在语义、实例和全景分割中取得了最优结果，为3D理解提供了可扩展且实用的解决方案。
            arXiv:2507.05211v2 Announce Type: replace 
Abstract: Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.
        ]]></description>
    </item>
    <item>
        <title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
        <link>https://arxiv.org/abs/2507.17307</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.17307v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang</dc:creator>
        <description><![CDATA[
            思维链（CoT）推理能提升大语言模型解决问题的能力，但会带来大量计算开销。现有加速策略存在局限性，如投机解码在大小模型一致性低时加速有限，且未充分利用小模型生成简洁中间推理的优势。本文提出R-Stitch，一种基于置信度的混合解码框架，沿推理轨迹在小语言模型（SLM）和大语言模型（LLM）间切换以加速CoT推理。默认用SLM生成标记，置信度低于阈值时才用LLM。实验表明，它能减少85%的推理延迟，且准确率几乎不变，有效加速CoT推理。
            arXiv:2507.17307v2 Announce Type: replace 
Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing acceleration strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the LLM only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the LLM on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.
        ]]></description>
    </item>
    <item>
        <title>When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label</title>
        <link>https://arxiv.org/abs/2507.18153</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.18153v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Riting Xia, Rucong Wang, Yulin Liu, Anchen Li, Xueyan Liu, Yan Zhang</dc:creator>
        <description><![CDATA[
            背景：类别不平衡的图节点分类问题有现实意义，但现有研究处理此类问题时多假设标签可靠，与含噪声的现实情况不符。方法：提出基于大语言模型和伪标签技术的图增强框架GraphALP，设计基于大语言模型的过采样方法生成少数类节点以缓解不平衡，开发动态加权伪标签方法降低噪声，还采用二次大语言模型引导的过采样机制减轻伪标签带来的分布偏差。效果：在含噪声的类别不平衡图上，GraphALP性能优于现有方法。
            arXiv:2507.18153v2 Announce Type: replace 
Abstract: Class-imbalanced graph node classification is a practical yet underexplored research problem. Although recent studies have attempted to address this issue, they typically assume clean and reliable labels when processing class-imbalanced graphs. This assumption often violates the nature of real-world graphs, where labels frequently contain noise. Given this gap, this paper systematically investigates robust node classification for class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph Augmentation framework based on Large language models (LLMs) and Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling method to generate synthetic minority nodes, producing label-accurate minority nodes to alleviate class imbalance. Based on the class-balanced graphs, we develop a dynamically weighted pseudo-labeling method to obtain high-confidence pseudo labels to reduce label noise ratio. Additionally, we implement a secondary LLM-guided oversampling mechanism to mitigate potential class distribution skew caused by pseudo labels. Experimental results show that GraphALP achieves superior performance over state-of-the-art methods on class-imbalanced graphs with noisy labels.
        ]]></description>
    </item>
    <item>
        <title>HH-Codec: High Compression High-fidelity Discrete Neural Codec for Spoken Language Modeling</title>
        <link>https://arxiv.org/abs/2507.18897</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.18897v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Rongkun Xue, Yazhe Niu, Shuai Hu, Zixin Yin, Yongqiang Yao, Jing Yang</dc:creator>
        <description><![CDATA[
            离散语音token化是语音编解码器的基础，但大规模语音到语音系统中，多量化器并行流的复杂性和高时间维度编解码器的计算成本带来挑战。本文提出HH - Codec，通过为口语语言建模精心设计矢量量化空间，优化压缩效率并减少信息损失。采用非对称编解码器架构，利用双重监督和渐进式训练提高重建稳定性和保真度。该编解码器在24kHz音频下每秒24个token实现极端压缩，以0.3kbps超低带宽实现语音重建的最优性能。
            arXiv:2507.18897v1 Announce Type: new 
Abstract: Discrete speech tokenization is a fundamental component in speech codecs. However, in large-scale speech-to-speech systems, the complexity of parallel streams from multiple quantizers and the computational cost of high-time-dimensional codecs pose significant challenges. In this paper, we introduce HH-Codec, a neural codec that achieves extreme compression at 24 tokens per second for 24 kHz audio while relying on single-quantizer inference. Our approach involves a carefully designed Vector Quantization space for Spoken Language Modeling, optimizing compression efficiency while minimizing information loss. Building on this, we propose an asymmetric encoder-decoder architecture (Audio-VQ-Mel-Audio) that leverages dual supervision and progressive training to enhance reconstruction stability and fidelity. HH-Codec achieves state-of-the-art performance in speech reconstruction with an ultra-low bandwidth of 0.3 kbps. We further evaluate its effectiveness in codebook utilization and generative model adaptation, with extensive ablations validating the necessity of each module. HH-Codec is available at https://github.com/opendilab/HH-Codec.
        ]]></description>
    </item>
    <item>
        <title>MLLM-based Speech Recognition: When and How is Multimodality Beneficial?</title>
        <link>https://arxiv.org/abs/2507.19037</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19037v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型（MLLMs）发展为语音等多模态统一建模带来新可能。方法：本文在前期工作基础上，通过合成和真实数据实验，研究多输入模态在嘈杂环境下提高自动语音识别（ASR）准确率的条件和模型架构。效果：发现利用更多模态通常能提高准确率，改善程度与听觉噪声量有关；同步模态在高噪声下更有用，非同步模态在中等噪声下最有帮助；高质量视觉表征可提升准确率；Mamba和Transformers在多模态益处上趋势相似；模态输入顺序和损失函数权重会显著影响准确率。
            arXiv:2507.19037v1 Announce Type: new 
Abstract: Recent advances in multi-modal large language models (MLLMs) have opened new possibilities for unified modeling of speech, text, images, and other modalities. Building on our prior work, this paper examines the conditions and model architectures under which multiple input modalities can improve automatic speech recognition (ASR) accuracy in noisy environments. Through experiments on synthetic and real-world data, we find that (1) harnessing more modalities usually improves ASR accuracy, as each modality provides complementary information, but the improvement depends on the amount of auditory noise. (2) Synchronized modalities (e.g., lip movements) are more useful at high noise levels whereas unsynchronized modalities (e.g., image context) are most helpful at moderate noise levels. (3) Higher-quality visual representations consistently improve ASR accuracy, highlighting the importance of developing more powerful visual encoders. (4) Mamba exhibits similar trends regarding the benefits of multimodality as do Transformers. (5) The input order of modalities as well as their weights in the loss function can significantly impact accuracy. These findings both offer practical insights and help to deepen our understanding of multi-modal speech recognition under challenging conditions.
        ]]></description>
    </item>
    <item>
        <title>From Continuous to Discrete: Cross-Domain Collaborative General Speech Enhancement via Hierarchical Language Models</title>
        <link>https://arxiv.org/abs/2507.19062</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19062v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhaoxi Mu, Rilin Chen, Andong Li, Meng Yu, Xinyu Yang, Dong Yu</dc:creator>
        <description><![CDATA[
            这是一篇关于语音增强的论文。背景是现有方法多针对单一失真类型，难以应对复杂场景下多种失真同时存在的情况。方法上，提出OmniGSE框架，采用两阶段架构整合判别和生成方法的优势。第一阶段用轻量级通道分离NAC - RoFormer增强连续特征，第二阶段通过语言模型生成离散令牌重建高质量语音，设计了包含RootLM和多个BranchLMs的分层语言模型结构。实验表明，该框架在多个基准测试中超越现有模型，在复合失真场景表现出色。
            arXiv:2507.19062v1 Announce Type: new 
Abstract: This paper introduces OmniGSE, a novel general speech enhancement (GSE) framework designed to mitigate the diverse distortions that speech signals encounter in real-world scenarios. These distortions include background noise, reverberation, bandwidth limitations, signal clipping, and network packet loss. Existing methods typically focus on optimizing for a single type of distortion, often struggling to effectively handle the simultaneous presence of multiple distortions in complex scenarios. OmniGSE bridges this gap by integrating the strengths of discriminative and generative approaches through a two-stage architecture that enables cross-domain collaborative optimization. In the first stage, continuous features are enhanced using a lightweight channel-split NAC-RoFormer. In the second stage, discrete tokens are generated to reconstruct high-quality speech through language models. Specifically, we designed a hierarchical language model structure consisting of a RootLM and multiple BranchLMs. The RootLM models general acoustic features across codebook layers, while the BranchLMs explicitly capture the progressive relationships between different codebook levels. Experimental results demonstrate that OmniGSE surpasses existing models across multiple benchmarks, particularly excelling in scenarios involving compound distortions. These findings underscore the framework's potential for robust and versatile speech enhancement in real-world applications.
        ]]></description>
    </item>
    <item>
        <title>Latent Granular Resynthesis using Neural Audio Codecs</title>
        <link>https://arxiv.org/abs/2507.19202</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19202v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Nao Tokui, Tom Baker</dc:creator>
        <description><![CDATA[
            本文聚焦音频生成领域，提出一种新颖的创造性音频再合成技术。背景是传统音频合成存在局限性。方法上，该技术在潜在向量层面重塑颗粒合成概念，将源音频语料编码为潜在向量片段创建“颗粒码本”，把目标音频信号的每个潜在颗粒与码本中最接近的对应物匹配，再将混合序列解码。效果上，合成音频保留目标音频时间结构，采用源音频音色特征，无需模型训练，适用于多种音频材料，避免了传统拼接合成的不连续性。
            arXiv:2507.19202v1 Announce Type: new 
Abstract: We introduce a novel technique for creative audio resynthesis that operates by reworking the concept of granular synthesis at the latent vector level. Our approach creates a "granular codebook" by encoding a source audio corpus into latent vector segments, then matches each latent grain of a target audio signal to its closest counterpart in the codebook. The resulting hybrid sequence is decoded to produce audio that preserves the target's temporal structure while adopting the source's timbral characteristics. This technique requires no model training, works with diverse audio materials, and naturally avoids the discontinuities typical of traditional concatenative synthesis through the codec's implicit interpolation during decoding. We include supplementary material at https://github.com/naotokui/latentgranular/ , as well as a proof-of-concept implementation to allow users to experiment with their own sounds at https://huggingface.co/spaces/naotokui/latentgranular .
        ]]></description>
    </item>
    <item>
        <title>SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models</title>
        <link>https://arxiv.org/abs/2507.19361</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.19361v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhen Wan, Chao-Han Huck Yang, Yahan Yu, Jinchuan Tian, Sheng Li, Ke Hu, Zhehuai Chen, Shinji Watanabe, Fei Cheng, Chenhui Chu, Sadao Kurohashi</dc:creator>
        <description><![CDATA[
            本文背景是缺乏有效评估语音理解大语言模型（LLM Voice）能力的方法。为此提出基于人类认知的语音智商（SIQ）评估体系，从记忆、理解、应用三个认知层次评估模型，超越了传统的词错误率指标。该方法不仅能量化语音理解能力，还可对级联方法和端到端模型进行统一比较，识别现有基准中的标注错误及模型幻觉。此框架首次将认知原理与语音基准相结合，揭示了多模态训练中被忽视的挑战。
            arXiv:2507.19361v1 Announce Type: cross 
Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training.
        ]]></description>
    </item>
    <item>
        <title>Integrating IP Broadcasting with Audio Tags: Workflow and Challenges</title>
        <link>https://arxiv.org/abs/2407.15423</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2407.15423v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Rhys Burchett-Vass, Arshdeep Singh, Gabriel Bibb\'o, Mark D. Plumbley</dc:creator>
        <description><![CDATA[
            背景：广播行业采用IP技术，为内容制作带来变革，而实时音频标签在内容制作中有诸多用途。方法：将音频标签模型容器化为微服务，使其能集成到多种网络设置中，以开发可无缝部署到不同规模广播工作流的模块化、易访问且灵活的工具。效果：有望为广播工作流提供便利，但也面临所选音频标签模型的延迟及其对最终产品实用性影响等挑战。
            arXiv:2407.15423v3 Announce Type: replace 
Abstract: The broadcasting industry has adopted IP technologies, revolutionising both live and pre-recorded content production, from news gathering to live music events. IP broadcasting allows for the transport of audio and video signals in an easily configurable way, aligning with modern networking techniques. This shift towards an IP workflow allows for much greater flexibility, not only in routing signals but with the integration of tools using standard web development techniques. One possible tool could include the use of live audio tagging, which has a number of uses in the production of content. These could include adding sound effects to automated closed captioning or identifying unwanted sound events within a scene. In this paper, we describe the process of containerising an audio tagging model into a microservice, a small segregated code module that can be integrated into a multitude of different network setups. The goal is to develop a modular, accessible, and flexible tool capable of seamless deployment into broadcasting workflows of all sizes, from small productions to large corporations. Challenges surrounding latency of the selected audio tagging model and its effect on the usefulness of the end product are discussed.
        ]]></description>
    </item>
    <item>
        <title>SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model</title>
        <link>https://arxiv.org/abs/2505.15670</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.15670v4</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr \.Zelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg</dc:creator>
        <description><![CDATA[
            口语对话是人机交互的直观形式，但现有语音语言模型多局限于轮替式交流，缺乏实时适应性。本文提出一种新颖的双工语音到语音（S2S）架构，通过通道融合直接对用户和智能体的同步流进行建模。使用预训练的流式编码器处理用户输入，无需语音预训练。分别为智能体和用户建模的架构，利于编解码器微调，使比特率减半至0.6 kbps。实验表明，该模型在推理、轮替和打断能力上优于先前双工模型，且构建时所需语音数据少，还公开了训练和推理代码。
            arXiv:2505.15670v4 Announce Type: replace-cross 
Abstract: Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility.
        ]]></description>
    </item>
</channel>
</rss>