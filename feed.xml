<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 04 Jun 2025 17:30:00 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Wed, 04 Jun 2025 17:30:00 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>OASIS: Online Sample Selection for Continual Visual Instruction Tuning</title>
        <link>https://arxiv.org/abs/2506.02011</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02011v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Minjae Lee, Minhyuk Seo, Tingyu Qu, Tinne Tuytelaars, Jonghyun Choi</dc:creator>
        <description><![CDATA[
            在持续视觉指令调优（CVIT）场景中，大规模数据的训练延迟阻碍实时适配，现有数据选择策略依赖预训练参考模型，在CVIT中不实用，无参考模型的在线样本选择方法又会受批次分布偏移影响。为此提出OASIS自适应在线样本选择方法，动态调整每批所选样本，并通过迭代更新选择分数减少样本冗余。实验结果表明，OASIS仅用25%的数据就能达到全量数据训练的性能，且优于现有技术。
            arXiv:2506.02011v1 Announce Type: new 
Abstract: In continual visual instruction tuning (CVIT) scenarios, where multi-modal data continuously arrive in an online streaming manner, training delays from large-scale data significantly hinder real-time adaptation. While existing data selection strategies reduce training overheads, they rely on pre-trained reference models, which are impractical in CVIT setups due to unknown future data. Recent reference model-free online sample selection methods address this issue but typically select a fixed number of samples per batch (e.g., top-k), causing them to suffer from distribution shifts where informativeness varies across batches. To address these limitations, we propose OASIS, an adaptive online sample selection approach for CVIT that: (1) dynamically adjusts selected samples per batch based on relative inter-batch informativeness, and (2) minimizes redundancy of selected samples through iterative selection score updates. Empirical results across various MLLMs, such as LLaVA-1.5 and Qwen-VL-2.5, show that OASIS achieves comparable performance to full-data training using only 25% of the data and outperforms the state-of-the-art.
        ]]></description>
    </item>
    <item>
        <title>FinS-Pilot: A Benchmark for Online Financial System</title>
        <link>https://arxiv.org/abs/2506.02037</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02037v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Feng Wang, Yiding Sun, Jiaxin Mao, Wei Xue, Danqing Xu</dc:creator>
        <description><![CDATA[
            背景：大语言模型在各专业领域表现出色，但金融RAG基准的发展受数据保密和缺乏动态数据整合的限制。方法：引入FinS - Pilot这一评估在线金融应用中RAG系统的新基准，其基于真实金融助手交互构建，结合实时API数据和结构化文本源，通过意图分类框架组织。效果：能全面评估金融助手处理静态知识和时效性市场信息的能力，通过对多个中文大语言模型实验，证明其能有效识别适合金融应用的模型，填补金融领域专业评估工具的空白。
            arXiv:2506.02037v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various professional domains, with their performance typically evaluated through standardized benchmarks. However, the development of financial RAG benchmarks has been constrained by data confidentiality issues and the lack of dynamic data integration. To address this issue, we introduces FinS-Pilot, a novel benchmark for evaluating RAG systems in online financial applications. Constructed from real-world financial assistant interactions, our benchmark incorporates both real-time API data and structured text sources, organized through an intent classification framework covering critical financial domains such as equity analysis and macroeconomic forecasting. The benchmark enables comprehensive evaluation of financial assistants' capabilities in handling both static knowledge and time-sensitive market information. Through systematic experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying models suitable for financial applications while addressing the current gap in specialized evaluation tools for the financial domain. Our work contributes both a practical evaluation framework and a curated dataset to advance research in financial NLP systems. The code and dataset are accessible on GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.
        ]]></description>
    </item>
    <item>
        <title>RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection</title>
        <link>https://arxiv.org/abs/2506.02081</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02081v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chihiro Maru, Shoetsu Sato</dc:creator>
        <description><![CDATA[
            受大语言模型在自然语言处理中成功的启发，近期研究探索构建时间序列基础模型并应用于异常检测等任务，但不同领域和任务表现不一。在异常检测中，基于示例提示的测试时间自适应方法虽常见，但时间序列基础模型缺乏解读和利用示例或指令的能力。为此，提出检索增强时间序列基础模型（RATFM），使预训练模型能融入测试时间自适应示例。该模型性能与特定领域微调相当且避免领域依赖微调，在多领域数据集上实验证实了其有效性。
            arXiv:2506.02081v1 Announce Type: new 
Abstract: Inspired by the success of large language models (LLMs) in natural language processing, recent research has explored the building of time series foundation models and applied them to tasks such as forecasting, classification, and anomaly detection. However, their performances vary between different domains and tasks. In LLM-based approaches, test-time adaptation using example-based prompting has become common, owing to the high cost of retraining. In the context of anomaly detection, which is the focus of this study, providing normal examples from the target domain can also be effective. However, time series foundation models do not naturally acquire the ability to interpret or utilize examples or instructions, because the nature of time series data used during training does not encourage such capabilities. To address this limitation, we propose a retrieval augmented time series foundation model (RATFM), which enables pretrained time series foundation models to incorporate examples of test-time adaptation. We show that RATFM achieves a performance comparable to that of in-domain fine-tuning while avoiding domain-dependent fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset including nine domains, confirms the effectiveness of the proposed approach.
        ]]></description>
    </item>
    <item>
        <title>Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains</title>
        <link>https://arxiv.org/abs/2506.02126</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02126v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, Yuyin Zhou</dc:creator>
        <description><![CDATA[
            背景：推理增强的大语言模型虽提升复杂任务表现，但内部推理过程的质量和透明度有待探究。方法：明确将思维轨迹分解为知识和推理两部分，引入细粒度评估框架，衡量知识正确性和推理质量。效果：R1蒸馏模型的通用推理能力难以有效迁移到医学领域；SFT虽提高最终答案准确率，但推理质量平均下降38.9%，不过在医学领域仍关键；RL通过修剪推理路径中不准确或无关知识，提高推理准确性和知识正确性。
            arXiv:2506.02126v1 Announce Type: new 
Abstract: Recent advances in reasoning-enhanced Large Language Models such as OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex tasks. However, the quality and transparency of their internal reasoning processes remain underexplored. This work moves beyond the final-answer accuracy and investigates step-by-step reasoning in the medical and mathematical domains by explicitly decomposing the thinking trajectories into two parts: knowledge and reasoning. Specifically, we introduce a fine-grained evaluation framework that judges: (1) the correctness of knowledge used (measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured by Information Gain (InfoGain)). Using this framework, we study R1-distilled and base Qwen models trained with supervised fine-tuning (SFT) and/or reinforcement learning (RL) in the medical and math domains. Three intriguing findings emerge: (1) The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain through either SFT or RL. (2) SFT raises final-answer accuracy in both domains, but often at the cost of reasoning quality: InfoGain drops by 38.9% on average compared with untrained models; In the medical domain, however, SFT remains crucial because domain knowledge is indispensable. (3) RL enhances medical reasoning by pruning inaccurate or irrelevant knowledge from reasoning paths, thereby improving both reasoning accuracy and knowledge correctness.
        ]]></description>
    </item>
    <item>
        <title>Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability</title>
        <link>https://arxiv.org/abs/2506.02138</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02138v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yarden Bakish, Itamar Zimerman, Hila Chefer, Lior Wolf</dc:creator>
        <description><![CDATA[
            背景：开发有效的Transformer可解释性工具是深度学习研究的重要方向，现有基于LRP的方法忽略Transformer架构的位置编码，导致违反守恒性及重要相关性丢失。方法：将Transformer可解释性的输入空间重新定义为位置 - 标记对，提出基于理论的LRP规则，以在不同位置编码方法中传播归因。效果：在微调分类器和零样本基础模型（如LLaMA 3）的实验中，该方法在视觉和NLP可解释性任务上显著优于现有技术。
            arXiv:2506.02138v1 Announce Type: new 
Abstract: The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available.
        ]]></description>
    </item>
    <item>
        <title>KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2506.02208</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02208v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hongling Xu, Qi Zhu, Heyuan Deng, Jinpeng Li, Lu Hou, Yasheng Wang, Lifeng Shang, Ruifeng Xu, Fei Mi</dc:creator>
        <description><![CDATA[
            背景：大语言模型（LLM）训练中，强化学习（RL）样本效率低，知识蒸馏（KD）泛化能力差。方法：提出KDRL统一后训练框架，通过教师监督（KD）和自我探索（RL）联合优化推理模型，利用策略梯度优化同时最小化学生与教师分布的反向KL散度，最大化基于规则的预期奖励，还探索不同因素对训练的影响。效果：在多个推理基准测试中，KDRL优于GRPO和各种KD基线，在性能和推理标记效率间取得良好平衡。
            arXiv:2506.02208v1 Announce Type: new 
Abstract: Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \textbf{KDRL}, a \textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.
        ]]></description>
    </item>
    <item>
        <title>From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models</title>
        <link>https://arxiv.org/abs/2506.02242</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02242v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yihong Tang, Ao Qu, Xujing Yu, Weipeng Deng, Jun Ma, Jinhua Zhao, Lijun Sun</dc:creator>
        <description><![CDATA[
            背景：城市和交通研究长期致力于揭示关键变量与道路安全等社会结果的关系，但传统方法存在依赖专家、可解释性有限、未充分利用非结构化数据等问题。方法：提出基于多模态大语言模型（MLLM）的可解释假设推理方法，利用MLLM为街景图像提出安全相关问题，提取可解释嵌入并应用于回归统计模型。效果：在曼哈顿街道段的实验表明，该方法优于预训练深度学习模型且具有完全可解释性，还可作为通用框架用于城市科学发现。
            arXiv:2506.02242v1 Announce Type: new 
Abstract: Urban and transportation research has long sought to uncover statistically meaningful relationships between key variables and societal outcomes such as road safety, to generate actionable insights that guide the planning, development, and renewal of urban and transportation systems. However, traditional workflows face several key challenges: (1) reliance on human experts to propose hypotheses, which is time-consuming and prone to confirmation bias; (2) limited interpretability, particularly in deep learning approaches; and (3) underutilization of unstructured data that can encode critical urban context. Given these limitations, we propose a Multimodal Large Language Model (MLLM)-based approach for interpretable hypothesis inference, enabling the automated generation, evaluation, and refinement of hypotheses concerning urban context and road safety outcomes. Our method leverages MLLMs to craft safety-relevant questions for street view images (SVIs), extract interpretable embeddings from their responses, and apply them in regression-based statistical models. UrbanX supports iterative hypothesis testing and refinement, guided by statistical evidence such as coefficient significance, thereby enabling rigorous scientific discovery of previously overlooked correlations between urban design and safety. Experimental evaluations on Manhattan street segments demonstrate that our approach outperforms pretrained deep learning models while offering full interpretability. Beyond road safety, UrbanX can serve as a general-purpose framework for urban scientific discovery, extracting structured insights from unstructured urban data across diverse socioeconomic and environmental outcomes. This approach enhances model trustworthiness for policy applications and establishes a scalable, statistically grounded pathway for interpretable knowledge discovery in urban and transportation studies.
        ]]></description>
    </item>
    <item>
        <title>CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment</title>
        <link>https://arxiv.org/abs/2506.02264</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02264v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Radin Shayanfar, Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu</dc:creator>
        <description><![CDATA[
            背景：由于专家知识、训练数据成本高和技术难度大，向对话系统传授特定任务颇具挑战，构建让非技术专家轻松定义、测试和完善系统行为的框架很关键。方法：引入CoDial框架，将以新型结构化异构图表示的专家知识转化为可执行的对话逻辑，且能在现有语言中实现。效果：在STAR数据集上达到基于推理模型的先进水平，在MultiWOZ数据集上与类似基线有竞争力，还能通过反馈迭代改进，适用于高风险领域大语言模型的专家引导对齐。
            arXiv:2506.02264v1 Announce Type: new 
Abstract: It is often challenging to teach specialized, unseen tasks to dialogue systems due to the high cost of expert knowledge, training data, and high technical difficulty. To support domain-specific applications - such as law, medicine, or finance - it is essential to build frameworks that enable non-technical experts to define, test, and refine system behaviour with minimal effort. Achieving this requires cross-disciplinary collaboration between developers and domain specialists. In this work, we introduce a novel framework, CoDial (Code for Dialogue), that converts expert knowledge, represented as a novel structured heterogeneous graph, into executable conversation logic. CoDial can be easily implemented in existing guardrailing languages, such as Colang, to enable interpretable, modifiable, and true zero-shot specification of task-oriented dialogue systems. Empirically, CoDial achieves state-of-the-art performance on the STAR dataset for inference-based models and is competitive with similar baselines on the well-known MultiWOZ dataset. We also demonstrate CoDial's iterative improvement via manual and LLM-aided feedback, making it a practical tool for expert-guided alignment of LLMs in high-stakes domains.
        ]]></description>
    </item>
    <item>
        <title>ImpRAG: Retrieval-Augmented Generation with Implicit Queries</title>
        <link>https://arxiv.org/abs/2506.02279</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02279v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wenzheng Zhang, Xi Victoria Lin, Karl Stratos, Wen-tau Yih, Mingda Chen</dc:creator>
        <description><![CDATA[
            背景：传统检索增强生成（RAG）系统将检索和生成分离，需显式文本查询，限制了模型跨任务泛化能力。方法：提出无查询的RAG系统ImpRAG，将检索和生成集成到统一模型，通过划分预训练语言模型层组优化任务，采用两阶段推理过程。效果：在8个知识密集型任务实验中，ImpRAG在不同格式的未见任务上精确匹配得分提升3.6 - 11.5，证明其能让模型表达信息需求并跨任务泛化。
            arXiv:2506.02279v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval and generation as separate processes, requiring explicit textual queries to connect them. This separation can limit the ability of models to generalize across diverse tasks. In this work, we propose a query-free RAG system, named ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG allows models to implicitly express their information needs, eliminating the need for human-specified queries. By dividing pretrained decoder-only language models into specialized layer groups, ImpRAG optimizes retrieval and generation tasks simultaneously. Our approach employs a two-stage inference process, using the same model parameters and forward pass for both retrieval and generation, thereby minimizing the disparity between retrievers and language models. Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves 3.6-11.5 improvements in exact match scores on unseen tasks with diverse formats, highlighting its effectiveness in enabling models to articulate their own information needs and generalize across tasks. Our analysis underscores the importance of balancing retrieval and generation parameters and leveraging generation perplexities as retrieval training objectives for enhanced performance.
        ]]></description>
    </item>
    <item>
        <title>Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments</title>
        <link>https://arxiv.org/abs/2506.02302</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02302v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Russell Scheinberg, Ameeta Agrawal, Amber Shore, So Young Lee</dc:creator>
        <description><![CDATA[
            背景：大语言模型能解释语法规则，但判断句子可接受性时往往无法应用规则。方法：提出“语法提示”这一先解释后处理的范式，先让大语言模型对相关句法现象进行简洁解释，再将解释作为额外上下文反馈给目标模型，然后判断最小对立体中哪个句子合乎语法。效果：在多个基准测试中，该提示设计有显著提升；对小语言模型，单独使用可缩小约20%的准确率差距，结合思维链可缩小56%（从13.0个百分点到5.8个百分点） 。
            arXiv:2506.02302v1 Announce Type: new 
Abstract: Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present "grammar prompting", an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the target model -- either an LLM or a smaller language model (SLM) -- before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across many syntactic phenomena. Feeding an LLM's metalinguistic explanation back to the target model bridges the gap between knowing a rule and using it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by 56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings.
        ]]></description>
    </item>
    <item>
        <title>One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL</title>
        <link>https://arxiv.org/abs/2506.02338</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02338v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hyungjoo Chae, Dongjin Kang, Jihyuk Kim, Beong-woo Kwak, Sunghyun Park, Haeju Park, Jinyoung Yeo, Moontae Lee, Kyungjae Lee</dc:creator>
        <description><![CDATA[
            背景：当前研究常基于R1的长思维链推理训练新的大型推理模型，但依赖现有模型限制了领域发展。方法：本文探索用未针对推理时间扩展训练的大语言模型构建长思维链数据集，推出含10万条思维链基本原理的Long CoT Collection，开发诱导推理策略的流程，增强思维长度和可控性。效果：数据集质量与R1相当或略低，基于此训练不仅强化推理能力，还为强化学习奠定基础，使模型在RLVR中增益2 - 3倍。
            arXiv:2506.02338v1 Announce Type: new 
Abstract: With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.
        ]]></description>
    </item>
    <item>
        <title>DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization</title>
        <link>https://arxiv.org/abs/2506.02351</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02351v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jeonghun Kang, Soonmok Kwon, Joonseok Lee, Byung-Hak Kim</dc:creator>
        <description><![CDATA[
            背景：传统棒球高光总结方法难以兼顾比赛战略深度等，人工编辑耗资源且难扩展。方法：提出DIAMOND，它是由大语言模型驱动的上下文感知棒球高光总结代理，将结构化体育分析与自然语言推理相结合，利用棒球数据特征量化比赛重要性，大语言模型模块根据上下文叙事价值增强选择。效果：在五场韩国棒球联赛中，F1分数从仅用WPA方法的42.9%提升到84.8%，超越商业和统计基线。
            arXiv:2506.02351v1 Announce Type: new 
Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.
        ]]></description>
    </item>
    <item>
        <title>GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2506.02404</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02404v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qianwen Zhang, Di Yin, Xing Sun, Xiao Huang</dc:creator>
        <description><![CDATA[
            背景：图检索增强生成（GraphRAG）能增强大语言模型，但现有对其评估存在局限，无法全面评估其推理能力提升。方法：引入GraphRAG - Bench这一大规模、特定领域的基准，有挑战性问题设计、多样任务覆盖、整体评估框架等优势。效果：通过应用九种当代GraphRAG方法，证明其能量化基于图的结构化对模型推理能力的提升，还能为研究界提供关于图架构、检索效果和推理能力的关键见解。
            arXiv:2506.02404v1 Announce Type: new 
Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \((ii)\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.
        ]]></description>
    </item>
    <item>
        <title>Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL</title>
        <link>https://arxiv.org/abs/2506.02406</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02406v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Renat Sergazinov, Jing Wu, Shao-An Yin</dc:creator>
        <description><![CDATA[
            背景：随机傅里叶特征作为表格数据深度学习预处理步骤的作用被忽视，且表格深度学习管道存在不足。方法：通过神经切线核（NTK）分析，将随机傅里叶映射重新用作无参数、与架构无关的变换，在初始化时用正弦和余弦投影将输入投影到固定特征空间。效果：该映射能界定和调节网络初始NTK频谱，缩短优化轨迹，加速基于梯度的训练。实验表明，经傅里叶变换输入训练的深度网络收敛更快，常能用更少轮次和超参数调整取得良好性能。
            arXiv:2506.02406v1 Announce Type: new 
Abstract: While random Fourier features are a classic tool in kernel methods, their utility as a pre-processing step for deep learning on tabular data has been largely overlooked. Motivated by shortcomings in tabular deep learning pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit and repurpose random Fourier mappings as a parameter-free, architecture-agnostic transformation. By projecting each input into a fixed feature space via sine and cosine projections with frequencies drawn once at initialization, this approach circumvents the need for ad hoc normalization or additional learnable embeddings. We show within the NTK framework that this mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii) introduces a bias that shortens the optimization trajectory, thereby accelerating gradient-based training. These effects pre-condition the network with a stable kernel from the outset. Empirically, we demonstrate that deep networks trained on Fourier-transformed inputs converge more rapidly and consistently achieve strong final performance, often with fewer epochs and less hyperparameter tuning. Our findings establish random Fourier pre-processing as a theoretically motivated, plug-and-play enhancement for tabular deep learning.
        ]]></description>
    </item>
    <item>
        <title>Comparative Analysis of AI Agent Architectures for Entity Relationship Classification</title>
        <link>https://arxiv.org/abs/2506.02426</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02426v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Maryam Berijanian, Kuldeep Singh, Amin Sehati</dc:creator>
        <description><![CDATA[
            实体关系分类在信息提取中是一项具有挑战性的任务，尤其在标注数据有限和关系结构复杂的场景下。本文对三种使用大语言模型进行关系分类的人工智能代理架构进行比较分析，包括反思性自我评估、分层任务分解和新型多智能体动态示例生成机制，后一种引入了实时合作和对抗性提示。实验表明，多智能体协作始终优于标准少样本提示，接近微调模型的性能，为基于大语言模型的结构化关系提取系统设计提供了实用指导。
            arXiv:2506.02426v1 Announce Type: new 
Abstract: Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at \href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.
        ]]></description>
    </item>
    <item>
        <title>ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment</title>
        <link>https://arxiv.org/abs/2506.02459</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02459v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Martin JJ. Bucher, Iro Armeni</dc:creator>
        <description><![CDATA[
            3D室内场景合成与编辑是计算机图形学的重要方向，现有方法存在简化物体语义、忽略房间边界等问题。为此提出ReSpace，这是一个使用自回归语言模型的文本驱动3D室内场景合成与编辑的生成框架。其采用紧凑结构化场景表示，将场景编辑作为下一个标记预测任务，结合监督微调与偏好对齐的双阶段训练方法。通过零样本大语言模型处理物体移除。实验结果在物体添加方面超越了现有技术，全场景合成结果也具有竞争力。
            arXiv:2506.02459v1 Announce Type: new 
Abstract: Scene synthesis and editing has emerged as a promising direction in computer graphics. Current trained approaches for 3D indoor scenes either oversimplify object semantics through one-hot class encodings (e.g., 'chair' or 'table'), require masked diffusion for editing, ignore room boundaries, or rely on floor plan renderings that fail to capture complex layouts. In contrast, LLM-based methods enable richer semantics via natural language (e.g., 'modern studio with light wood furniture') but do not support editing, remain limited to rectangular layouts or rely on weak spatial reasoning from implicit world models. We introduce ReSpace, a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models. Our approach features a compact structured scene representation with explicit room boundaries that frames scene editing as a next-token prediction task. We leverage a dual-stage training approach combining supervised fine-tuning and preference alignment, enabling a specially trained language model for object addition that accounts for user instructions, spatial geometry, object semantics, and scene-level composition. For scene editing, we employ a zero-shot LLM to handle object removal and prompts for addition. We further introduce a novel voxelization-based evaluation that captures fine-grained geometry beyond 3D bounding boxes. Experimental results surpass state-of-the-art on object addition while maintaining competitive results on full scene synthesis.
        ]]></description>
    </item>
    <item>
        <title>Comba: Improving Nonlinear RNNs with Closed-loop Control</title>
        <link>https://arxiv.org/abs/2506.02475</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02475v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, Weigao Sun</dc:creator>
        <description><![CDATA[
            背景：近期如Gated DeltaNet等高效序列建模方法通过Delta学习规则监督循环内存管理取得性能提升，但这些模型有优缺点。方法：基于闭环控制理论，提出名为Comba的新型非线性循环神经网络（RNN）变体，采用标量加低秩状态转移，有状态反馈和输出反馈校正，还在Triton中实现硬件高效的分块并行内核，并在大规模语料上训练340M/1.3B参数的模型。效果：Comba在语言和视觉建模中展现出优越性能和计算效率。
            arXiv:2506.02475v1 Announce Type: new 
Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and RWKV-7 have achieved performance improvements by supervising the recurrent memory management through Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, resulting in a nonlinear recursive structure. In this paper, we first introduce the concept of Nonlinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then, based on closed-loop control theory, we propose a novel Nonlinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on large-scale corpus. Comba demonstrates its superior performance and computation efficiency in both language and vision modeling.
        ]]></description>
    </item>
    <item>
        <title>Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks</title>
        <link>https://arxiv.org/abs/2506.02483</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02483v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sina Bagheri Nezhad, Ameeta Agrawal</dc:creator>
        <description><![CDATA[
            背景：大语言模型在长文本场景中进行多目标推理时存在困难，相关信息分散在大量文档中。方法：引入神经符号增强推理（NSAR），结合神经和符号推理，从文本中显式提取符号事实并生成可执行Python代码处理复杂推理步骤。效果：通过对七种语言和不同上下文长度的大量实验表明，NSAR在准确识别和整合多信息方面显著优于普通RAG基线和高级提示策略，能实现多语言环境下可靠、可解释和可扩展的推理。
            arXiv:2506.02483v1 Announce Type: new 
Abstract: Large language models (LLMs) often struggle to perform multi-target reasoning in long-context scenarios where relevant information is scattered across extensive documents. To address this challenge, we introduce NeuroSymbolic Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic reasoning during inference. NSAR explicitly extracts symbolic facts from text and generates executable Python code to handle complex reasoning steps. Through extensive experiments across seven languages and diverse context lengths, we demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and advanced prompting strategies in accurately identifying and synthesizing multiple pieces of information. Our results highlight the effectiveness of combining explicit symbolic operations with neural inference for robust, interpretable, and scalable reasoning in multilingual settings.
        ]]></description>
    </item>
    <item>
        <title>KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG</title>
        <link>https://arxiv.org/abs/2506.02503</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02503v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yongjian Li, HaoCheng Chu, Yukun Yan, Zhenghao Liu, Shi Yu, Zheni Zeng, Ruobing Wang, Sen Song, Zhiyuan Liu, Maosong Sun</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）虽让大语言模型获取更多知识源，但因检索文档有噪声，仍存在事实不一致问题。方法：提出KARE - RAG，通过结构化知识表示辅助训练时检测错误、采用Dense Direct Preference Optimization优先纠正关键错误、利用对比数据生成管道纠正事实错误并保持语义一致。效果：实验表明该方法显著提升标准RAG管道性能，提升领域内和领域外任务表现且不损害通用能力，少量训练数据就能取得效果，指明了RAG改进新方向。
            arXiv:2506.02503v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access broader knowledge sources, yet factual inconsistencies persist due to noise in retrieved documents-even with advanced retrieval methods. We demonstrate that enhancing generative models' capacity to process noisy content is equally critical for robust performance. In this paper, we present KARE-RAG (Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge utilization through three key innovations: (1) structured knowledge representations that facilitate error detection during training, (2) Dense Direct Preference Optimization (DDPO)-a refined training objective that prioritizes correction of critical errors, and (3) a contrastive data generation pipeline that maintains semantic consistency while rectifying factual inaccuracies. Experiments show our method significantly enhances standard RAG pipelines across model scales, improving both in-domain and out-of-domain task performance without compromising general capabilities. Notably, these gains are achieved with modest training data, suggesting data-efficient optimization is possible through targeted learning strategies. Our findings establish a new direction for RAG improvement: by improving how models learn to process retrieved content, we can enhance performance across diverse inference paradigms. All data and code will be publicly available on Github.
        ]]></description>
    </item>
    <item>
        <title>Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025</title>
        <link>https://arxiv.org/abs/2506.02550</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02550v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qiaohui Chu, Haoyu Zhang, Yisen Feng, Meng Liu, Weili Guan, Yaowei Wang, Liqiang Nie</dc:creator>
        <description><![CDATA[
            该论文背景是Ego4D长期动作预测（LTA）任务。其方法受基础模型进展启发，采用三阶段框架，先利用高性能视觉编码器提取视觉特征，再将特征输入Transformer预测动词和名词，并结合动词 - 名词共现矩阵提高识别准确率，最后把预测的动词 - 名词对作为文本提示输入微调的大语言模型以预测未来动作序列。该框架在CVPR 2025的挑战赛中获得第一名，开创了长期动作预测的新水平。
            arXiv:2506.02550v1 Announce Type: new 
Abstract: In this report, we present a novel three-stage framework developed for the Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in foundation models, our method consists of three stages: feature extraction, action recognition, and long-term action anticipation. First, visual features are extracted using a high-performance visual encoder. The features are then fed into a Transformer to predict verbs and nouns, with a verb-noun co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the predicted verb-noun pairs are formatted as textual prompts and input into a fine-tuned large language model (LLM) to anticipate future action sequences. Our framework achieves first place in this challenge at CVPR 2025, establishing a new state-of-the-art in long-term action prediction. Our code will be released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.
        ]]></description>
    </item>
    <item>
        <title>EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving</title>
        <link>https://arxiv.org/abs/2506.02672</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02672v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, Cheng Zhong, Zongzhang Zhang, Tao Gui, Chao Xin, Wei Chengzhi, Lin Yan, Qi Zhang, Xuanjing Huang</dc:creator>
        <description><![CDATA[
            背景：评估大语言模型在挑战性任务中的学习能力和效率是未充分探索的重要方面。方法：提出EvaLearn基准，包含648个跨6种任务类型的难题，分182个序列，要求模型顺序解题，提供5种自动化指标评估。对9个前沿模型进行广泛测试，还研究两种学习设置下的表现。效果：不同模型表现各异，如Claude - 3.7 - sonnet初始表现一般但学习能力强；当前静态能力强的模型在学习能力上无明显优势，为评估模型潜力提供新视角。
            arXiv:2506.02672v1 Announce Type: new 
Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available at the GitHub repository.
        ]]></description>
    </item>
    <item>
        <title>TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression</title>
        <link>https://arxiv.org/abs/2506.02678</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02678v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Xing W, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo, Xiao Liu, Fei Yin, Cheng-Lin Liu</dc:creator>
        <description><![CDATA[
            背景：大语言模型虽借助强化学习和思维链技术取得进展，但高效语言推理尤其是长输出推理仍面临挑战。方法：提出不依赖复杂数据标注或多模型插值的动态比率训练管道，持续平衡模型系统1和系统2数据的权重，消除冗余推理过程。效果：在不同模型和不同难度基准测试中验证，该方法能在保持推理准确性的同时，使输出标记数量减少近40%。
            arXiv:2506.02678v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.
        ]]></description>
    </item>
    <item>
        <title>Towards Geometry Problem Solving in the Large Model Era: A Survey</title>
        <link>https://arxiv.org/abs/2506.02690</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02690v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yurui Zhao, Xiang Wang, Jiahong Liu, Irwin King, Zhitao Huang</dc:creator>
        <description><![CDATA[
            背景：几何问题求解（GPS）是人工智能的关键前沿领域，但自动化求解因需空间理解和逻辑推理而颇具挑战，虽大模型有进展，但该领域在方法、基准和评估框架上较分散。方法：通过基准构建、文本和图表解析、推理范式三个核心维度系统综合GPS进展，还提出统一分析范式。效果：评估了当前局限，指明新兴机会，以引导未来研究实现类人几何推理，如自动基准生成和可解释神经符号集成。
            arXiv:2506.02690v1 Announce Type: new 
Abstract: Geometry problem solving (GPS) represents a critical frontier in artificial intelligence, with profound applications in education, computer-aided design, and computational graphics. Despite its significance, automating GPS remains challenging due to the dual demands of spatial understanding and rigorous logical reasoning. Recent advances in large models have enabled notable breakthroughs, particularly for SAT-level problems, yet the field remains fragmented across methodologies, benchmarks, and evaluation frameworks. This survey systematically synthesizes GPS advancements through three core dimensions: (1) benchmark construction, (2) textual and diagrammatic parsing, and (3) reasoning paradigms. We further propose a unified analytical paradigm, assess current limitations, and identify emerging opportunities to guide future research toward human-level geometric reasoning, including automated benchmark generation and interpretable neuro-symbolic integration.
        ]]></description>
    </item>
    <item>
        <title>From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit</title>
        <link>https://arxiv.org/abs/2506.03093</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03093v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Val\'erie Costa, Thomas Fel, Ekdeep Singh Lubana, Bahareh Tolooshams, Demba Ba</dc:creator>
        <description><![CDATA[
            背景：稀疏自编码器（SAEs）因特定假设成了解释性领域常用工具，但模型表征现象超出该假设范围。方法：采用基于构造的方法，将匹配追踪（MP）算法从稀疏编码重新定位，设计出MP - SAE，其将编码器展开为一系列残差引导步骤。效果：在合成和自然数据上对比显示，现有SAEs无法捕捉分层概念诱导的条件正交特征，MP - SAE的非线性编码步骤能恢复有意义特征，还能在推理时自适应稀疏。
            arXiv:2506.03093v1 Announce Type: new 
Abstract: Motivated by the hypothesis that neural network representations encode abstract, interpretable features as linearly accessible, approximately orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in interpretability. However, recent work has demonstrated phenomenology of model representations that lies outside the scope of this hypothesis, showing signatures of hierarchical, nonlinear, and multi-dimensional features. This raises the question: do SAEs represent features that possess structure at odds with their motivating hypothesis? If not, does avoiding this mismatch help identify said features and gain further insights into neural network representations? To answer these questions, we take a construction-based approach and re-contextualize the popular matching pursuits (MP) algorithm from sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a sequence of residual-guided steps, allowing it to capture hierarchical and nonlinearly accessible features. Comparing this architecture with existing SAEs on a mixture of synthetic and natural data settings, we show: (i) hierarchical concepts induce conditionally orthogonal features, which existing SAEs are unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE recovers highly meaningful features, helping us unravel shared structure in the seemingly dichotomous representation spaces of different modalities in a vision-language model, hence demonstrating the assumption that useful features are solely linearly accessible is insufficient. We also show that the sequential encoder principle of MP-SAE affords an additional benefit of adaptive sparsity at inference time, which may be of independent interest. Overall, we argue our results provide credence to the idea that interpretability should begin with the phenomenology of representations, with methods emerging from assumptions that fit it.
        ]]></description>
    </item>
    <item>
        <title>Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds</title>
        <link>https://arxiv.org/abs/2506.03100</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03100v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）近年在辅助大语言模型方面取得很多实证成功，但理论方面大多未被探索。方法：提出上下文线性回归中RAG的首个有限样本泛化界，推导精确的偏差 - 方差权衡，将检索文本视为依赖查询的含噪上下文示例，通过引入均匀和非均匀RAG噪声对从训练数据和外部语料库的检索进行建模。效果：分析表明RAG存在泛化误差上限，且通过常见问答基准实验验证了ICL和RAG的样本效率。
            arXiv:2506.03100v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA.
        ]]></description>
    </item>
    <item>
        <title>Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback</title>
        <link>https://arxiv.org/abs/2506.03106</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03106v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng</dc:creator>
        <description><![CDATA[
            背景：单纯数值反馈的强化学习虽提升大语言模型推理能力，但存在性能停滞、自我反思效果有限和持续失败等问题。方法：提出Critique - GRPO在线强化学习框架，整合自然语言和数值反馈进行策略优化，使大语言模型能从初始回答和批判引导的改进中同时学习并保持探索。效果：实验表明，该框架在八项推理任务中优于基于监督学习和强化学习的微调方法，平均pass@1得分分别提高约4.5%和5%，还超越含专家示范的基线。
            arXiv:2506.03106v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.
        ]]></description>
    </item>
    <item>
        <title>Hyperband-based Bayesian Optimization for Black-box Prompt Selection</title>
        <link>https://arxiv.org/abs/2412.07820</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.07820v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, Felice Antonio Merra</dc:creator>
        <description><![CDATA[
            背景：最优提示选择对提升大语言模型下游任务性能至关重要，而黑盒提示选择因搜索空间大、无梯度信息、验证集评估成本高等面临挑战。方法：提出HbBoPs方法，结合结构感知的深度核高斯过程与Hyperband多保真调度器，利用指令和少样本示例的嵌入，将其作为提示中的模块化组件。效果：Hyperband可自适应分配资源，减少评估提示所需的验证实例。实验表明，HbBoPs在性能和效率上均优于现有方法。
            arXiv:2412.07820v2 Announce Type: replace 
Abstract: Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks, especially in black-box settings where models are only accessible via APIs. Black-box prompt selection is challenging due to potentially large, combinatorial search spaces, absence of gradient information, and high evaluation cost of prompts on a validation set. We propose HbBoPs, a novel method that combines a structural-aware deep kernel Gaussian Process with Hyperband as a multi-fidelity scheduler to efficiently select prompts. HbBoPs uses embeddings of instructions and few-shot exemplars, treating them as modular components within prompts. This enhances the surrogate model's ability to predict which prompt to evaluate next in a sample-efficient manner. Hyperband improves query-efficiency by adaptively allocating resources across different fidelity levels, reducing the number of validation instances required for evaluating prompts. Extensive experiments across ten diverse benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods in both performance and efficiency.
        ]]></description>
    </item>
    <item>
        <title>Instruction-Following Pruning for Large Language Models</title>
        <link>https://arxiv.org/abs/2501.02086</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.02086v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei</dc:creator>
        <description><![CDATA[
            背景：随着大语言模型规模迅速扩大，结构化剪枝成为从大模型学习高效小模型的常用技术。方法：本文提出动态结构化剪枝方法，即“指令跟随剪枝”，通过稀疏掩码预测器根据用户指令动态选择任务相关的模型参数，并联合优化预测器和大语言模型。效果：实验结果显示该方法在多种评估基准上有效，3B激活模型在数学和编码等领域比3B密集模型绝对优势高5 - 8个百分点，性能可与9B模型媲美。
            arXiv:2501.02086v3 Announce Type: replace 
Abstract: With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed "instruction-following pruning", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model.
        ]]></description>
    </item>
    <item>
        <title>CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought</title>
        <link>https://arxiv.org/abs/2502.17214</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.17214v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Boxuan Zhang, Ruqi Zhang</dc:creator>
        <description><![CDATA[
            大语言模型在生成回复时难以准确量化不确定性，现有方法多为基于提示而非回复，且需多个回复样本，计算成本高。为此，本文提出CoT-UQ，将思维链融入不确定性量化过程，通过提取推理步骤关键词并评估其对最终答案的重要性来捕捉关键信息，进而聚合得到最终不确定性估计。在Llama系列模型上的实验表明，CoT-UQ显著优于现有方法，平均AUROC提升5.9%。
            arXiv:2502.17214v2 Announce Type: replace 
Abstract: Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on Llama Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ.
        ]]></description>
    </item>
    <item>
        <title>Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications</title>
        <link>https://arxiv.org/abs/2504.16972</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.16972v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari</dc:creator>
        <description><![CDATA[
            背景：无线通信、雷达等领域未标注时间序列数据的快速增长推动了无监督学习发展。方法：综述了应用自编码器和视觉Transformer进行无监督信号分析的最新进展，聚焦其架构、应用和新兴趋势，探讨这些模型在不同信号类型中的特征提取、异常检测和分类等作用。效果：强调了混合架构和自监督学习的优势，指出了解释性、可扩展性等方面的挑战，为开发用于信号智能的鲁棒、自适应模型提供了路线图。
            arXiv:2504.16972v2 Announce Type: replace 
Abstract: The rapid growth of unlabeled time-series data in domains such as wireless communications, radar, biomedical engineering, and the Internet of Things (IoT) has driven advancements in unsupervised learning. This review synthesizes recent progress in applying autoencoders and vision transformers for unsupervised signal analysis, focusing on their architectures, applications, and emerging trends. We explore how these models enable feature extraction, anomaly detection, and classification across diverse signal types, including electrocardiograms, radar waveforms, and IoT sensor data. The review highlights the strengths of hybrid architectures and self-supervised learning, while identifying challenges in interpretability, scalability, and domain generalization. By bridging methodological innovations and practical applications, this work offers a roadmap for developing robust, adaptive models for signal intelligence.
        ]]></description>
    </item>
    <item>
        <title>Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation</title>
        <link>https://arxiv.org/abs/2505.13338</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.13338v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw</dc:creator>
        <description><![CDATA[
            背景：当前语音大语言模型在上下文推理和副语言理解方面能力有限，原因是缺乏涵盖这两方面的问答数据集。方法：提出一种从自然语音数据生成数据集的新框架，集成上下文推理与副语言信息，包括基于伪副语言标签的数据浓缩和基于大语言模型的上下文副语言问答生成。效果：通过Qwen2 - Audio - 7B - Instruct模型在该框架创建的数据集与人工生成数据集评估的强相关性验证了有效性，也揭示了语音大语言模型在共情推理任务上的局限。
            arXiv:2505.13338v2 Announce Type: replace 
Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.
        ]]></description>
    </item>
    <item>
        <title>Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains</title>
        <link>https://arxiv.org/abs/2505.16014</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.16014v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yash Saxena, Ankur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur</dc:creator>
        <description><![CDATA[
            背景：传统检索增强生成（RAG）管道依赖基于相似度的检索和重排序，缺乏可解释性、可理解性和对抗恶意内容的鲁棒性。方法：提出METEORA方法，用基于理由的选择方法替代RAG中的重排序，包括对大语言模型进行偏好调优生成理由，分三步选择相关证据块，还利用验证器大语言模型进行一致性检查。效果：在六个数据集上评估显示，METEORA生成准确率提高33.34%，使用的块减少约50%，在对抗环境中F1分数从0.10提升到0.44。
            arXiv:2505.16014v3 Announce Type: replace 
Abstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on similarity-based retrieval and re-ranking, which depend on heuristics such as top-k, and lack explainability, interpretability, and robustness against adversarial content. To address this gap, we propose a novel method METEORA that replaces re-ranking in RAG with a rationale-driven selection approach. METEORA operates in two stages. First, a general-purpose LLM is preference-tuned to generate rationales conditioned on the input query using direct preference optimization. These rationales guide the evidence chunk selection engine, which selects relevant chunks in three stages: pairing individual rationales with corresponding retrieved chunks for local relevance, global selection with elbow detection for adaptive cutoff, and context expansion via neighboring chunks. This process eliminates the need for top-k heuristics. The rationales are also used for consistency check using a Verifier LLM to detect and filter poisoned or misleading content for safe generation. The framework provides explainable and interpretable evidence flow by using rationales consistently across both selection and verification. Our evaluation across six datasets spanning legal, financial, and academic research domains shows that METEORA improves generation accuracy by 33.34% while using approximately 50% fewer chunks than state-of-the-art re-ranking methods. In adversarial settings, METEORA significantly improves the F1 score from 0.10 to 0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating strong resilience to poisoning attacks. Code available at: https://anonymous.4open.science/r/METEORA-DC46/README.md
        ]]></description>
    </item>
    <item>
        <title>JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model</title>
        <link>https://arxiv.org/abs/2505.17257</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.17257v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild</dc:creator>
        <description><![CDATA[
            背景：大语言模型应用于基因序列面临挑战，传统模型架构和训练范式难以捕捉复杂基因组相互作用，标准训练方法对DNA不适用。方法：提出首个双向DNA基础模型JanusDNA，采用结合自回归和掩码建模的预训练范式，采用混合Mamba、注意力机制和专家混合体（MoE）架构。效果：能在单块80GB GPU上以单核苷酸分辨率处理100万个碱基对，在三个基因组表征基准测试中取得新的最优结果，优于激活参数多250倍的模型。
            arXiv:2505.17257v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA
        ]]></description>
    </item>
    <item>
        <title>G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2505.18499</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.18499v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, Yisen Wang</dc:creator>
        <description><![CDATA[
            背景：大语言模型在图相关任务中表现有限，此前方法面临数据稀缺等挑战。方法：提出G1方法，利用强化学习提升大语言模型的图推理能力，还构建了包含50个不同难度图论任务、100k训练数据和5k测试数据的数据集Erdős。效果：G1在图推理上有显著提升，微调后的3B模型性能超Qwen2.5 - 72B - Instruct，且强化学习训练的模型有强零样本泛化能力，不影响一般推理能力。
            arXiv:2505.18499v2 Announce Type: replace 
Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erd\~os, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully. Our implementation is open-sourced at https://github.com/PKU-ML/G1, with models and datasets hosted on Hugging Face collections https://huggingface.co/collections/PKU-ML/g1-683d659e992794fc99618cf2 for broader accessibility.
        ]]></description>
    </item>
    <item>
        <title>LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud Active Learning</title>
        <link>https://arxiv.org/abs/2505.18924</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.18924v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chenxi Li, Nuo Chen, Fengyun Tan, Yantong Chen, Bochun Yuan, Tianrui Li, Chongshou Li</dc:creator>
        <description><![CDATA[
            背景：现有3D点云语义分割主动学习方法将标签视为扁平独立的，存在不足。方法：提出一种新颖的主动学习框架，首次集成大语言模型构建层次化标签结构，通过大语言模型提示自动生成多级语义分类法，引入递归不确定性投影机制在层次间传播不确定性。效果：在S3DIS和ScanNet v2实验中，在极低标注预算（如0.02%）下，mIoU最多提高4%，大幅超越现有基线。
            arXiv:2505.18924v2 Announce Type: replace 
Abstract: We present a novel active learning framework for 3D point cloud semantic segmentation that, for the first time, integrates large language models (LLMs) to construct hierarchical label structures and guide uncertainty-based sample selection. Unlike prior methods that treat labels as flat and independent, our approach leverages LLM prompting to automatically generate multi-level semantic taxonomies and introduces a recursive uncertainty projection mechanism that propagates uncertainty across hierarchy levels. This enables spatially diverse, label-aware point selection that respects the inherent semantic structure of 3D scenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to 4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%), substantially outperforming existing baselines. Our results highlight the untapped potential of LLMs as knowledge priors in 3D vision and establish hierarchical uncertainty modeling as a powerful paradigm for efficient point cloud annotation.
        ]]></description>
    </item>
    <item>
        <title>Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM</title>
        <link>https://arxiv.org/abs/2505.19901</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.19901v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Peng Liu, Xiaoming Ren, Fengkai Liu, Qingsong Xie, Quanlong Zheng, Yanhao Zhang, Haonan Lu, Yujiu Yang</dc:creator>
        <description><![CDATA[
            背景：现有图像到视频（I2V）生成方法在处理复杂场景时面临挑战，且当前I2V基准存在偏向低动态视频的局限性。方法：提出Dynamic - I2V框架，集成多模态大语言模型（MLLMs）为扩散变压器（DiT）架构联合编码视觉和文本条件；提出DIVE评估基准。效果：该模型显著提高合成视频的运动可控性和时间连贯性，能灵活支持多样条件输入。实验表明，相比现有方法，动态范围、可控性和质量分别提升42.5%、7.9%和11.8%。
            arXiv:2505.19901v3 Announce Type: replace 
Abstract: Recent advancements in image-to-video (I2V) generation have shown promising performance in conventional scenarios. However, these methods still encounter significant challenges when dealing with complex scenes that require a deep understanding of nuanced motion and intricate object-action relationships. To address these challenges, we present Dynamic-I2V, an innovative framework that integrates Multimodal Large Language Models (MLLMs) to jointly encode visual and textual conditions for a diffusion transformer (DiT) architecture. By leveraging the advanced multimodal understanding capabilities of MLLMs, our model significantly improves motion controllability and temporal coherence in synthesized videos. The inherent multimodality of Dynamic-I2V further enables flexible support for diverse conditional inputs, extending its applicability to various downstream generation tasks. Through systematic analysis, we identify a critical limitation in current I2V benchmarks: a significant bias towards favoring low-dynamic videos, stemming from an inadequate balance between motion complexity and visual quality metrics. To resolve this evaluation gap, we propose DIVE - a novel assessment benchmark specifically designed for comprehensive dynamic quality measurement in I2V generation. In conclusion, extensive quantitative and qualitative experiments confirm that Dynamic-I2V attains state-of-the-art performance in image-to-video generation, particularly revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range, controllability, and quality, respectively, as assessed by the DIVE metric in comparison to existing methods.
        ]]></description>
    </item>
    <item>
        <title>Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms</title>
        <link>https://arxiv.org/abs/2505.20322</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.20322v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang</dc:creator>
        <description><![CDATA[
            背景：精确控制语言模型生成对确保安全性和可靠性至关重要，但现有干预模型行为的方法因模型参数多、内部表征交织，存在控制精度受限和产生副作用的问题。方法：提出Steering Target Atoms（STA），一种隔离和操纵解纠缠知识组件以提高安全性的新方法。效果：综合实验证明该方法有效，分析显示其在对抗场景中具有更强的鲁棒性和灵活性，应用于大型推理模型也能有效控制精确推理。
            arXiv:2505.20322v2 Announce Type: replace 
Abstract: Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.
        ]]></description>
    </item>
    <item>
        <title>Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation</title>
        <link>https://arxiv.org/abs/2505.23368</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.23368v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank</dc:creator>
        <description><![CDATA[
            背景：推理调优的大语言模型（LLMs）生成思维链（CoTs）的方式引发关注，此前工作多基于给定答案生成解释，而CoTs可提供前向推理路径。方法：提出基于LLM的新流程，借助话语分割器从CoTs中更准确提取支持和反对每个答案选项的陈述，还提出基于排名的HLV评估框架。效果：在三个数据集上，该方法优于直接生成法和基线，且与人类排名方法的对齐性更好，凸显其有效性。
            arXiv:2505.23368v2 Announce Type: replace 
Abstract: The recent rise of reasoning-tuned Large Language Models (LLMs)--which generate chains of thought (CoTs) before giving the final answer--has attracted significant attention and offers new opportunities for gaining insights into human label variation, which refers to plausible differences in how multiple annotators label the same data instance. Prior work has shown that LLM-generated explanations can help align model predictions with human label distributions, but typically adopt a reverse paradigm: producing explanations based on given answers. In contrast, CoTs provide a forward reasoning path that may implicitly embed rationales for each answer option, before generating the answers. We thus propose a novel LLM-based pipeline enriched with linguistically-grounded discourse segmenters to extract supporting and opposing statements for each answer option from CoTs with improved accuracy. We also propose a rank-based HLV evaluation framework that prioritizes the ranking of answers over exact scores, which instead favor direct comparison of label distributions. Our method outperforms a direct generation method as well as baselines on three datasets, and shows better alignment of ranking methods with humans, highlighting the effectiveness of our approach.
        ]]></description>
    </item>
    <item>
        <title>DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2505.23754</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.23754v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu</dc:creator>
        <description><![CDATA[
            背景：定理证明是评估大语言模型复杂推理能力的重要测试平台，但传统自动定理证明方法与大语言模型的优势难以对齐。方法：提出DeepTheorem框架，包含121K个高质量定理及证明的基准数据集，设计专门的强化学习策略RL - Zero，还提出全面的评估指标。效果：实验表明，与现有数据集和监督微调方案相比，DeepTheorem显著提升了大语言模型的定理证明性能，达到了最先进的准确率和推理质量。
            arXiv:2505.23754v2 Announce Type: replace 
Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration.
        ]]></description>
    </item>
    <item>
        <title>Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models</title>
        <link>https://arxiv.org/abs/2506.00653</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.00653v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Femi Bello, Anubrata Das, Fanzhi Zeng, Fangcong Yin, Liu Leqi</dc:creator>
        <description><![CDATA[
            背景：有观点认为相似架构且在相似数据上训练的神经网络会学习与任务相关的共享表征。方法：扩展概念框架，提出线性表征可迁移性（LRT）假设，即不同模型表征空间间存在仿射变换，还学习不同大小模型隐藏状态间的仿射映射。效果：有很强的实证证据表明这种仿射映射能保留引导行为，说明小模型学到的表征可引导大模型行为，LRT假设是理解跨模型尺度表征对齐的有前景方向。
            arXiv:2506.00653v2 Announce Type: replace 
Abstract: It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.
        ]]></description>
    </item>
    <item>
        <title>Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages</title>
        <link>https://arxiv.org/abs/2506.00912</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.00912v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yongdong chi, Hanqing Wang, Zonghan Yang, Jian Yang, Xiao Yan, Yun Chen, Guanhua Chen</dc:creator>
        <description><![CDATA[
            背景：文本到SQL将自然语言查询转换为可执行SQL程序，但现有基于提示的方法因文本与低资源SQL程序语义差距大，准确性受限。方法：提出Pi - SQL，以高资源Python程序为枢纽，先生成能在代码块或注释中提供细粒度分步指导的Python程序，再依其指导生成SQL程序，并从不同策略生成的候选中选择。效果：最终SQL程序与参考Python程序查询结果匹配，奖励有效效率得分比最佳基线高4.55，执行准确率最多提高3.20。
            arXiv:2506.00912v2 Announce Type: replace 
Abstract: Text-to-SQL transforms the user queries from natural language to executable SQL programs, enabling non-experts to interact with complex databases. Existing prompt-based methods craft meticulous text guidelines and examples to facilitate SQL generation, but their accuracy is hindered by the large semantic gap between the texts and the low-resource SQL programs. In this work, we propose Pi-SQL, which incorporates the high-resource Python program as a pivot to bridge between the natural language query and SQL program. In particular, Pi-SQL first generates Python programs that provide fine-grained step-by-step guidelines in their code blocks or comments, and then produces an SQL program following the guidance of each Python program. The final SQL program matches the reference Python program's query results and, through selection from candidates generated by different strategies, achieves superior execution speed, with a reward-based valid efficiency score up to 4.55 higher than the best-performing baseline. Extensive experiments demonstrate the effectiveness of Pi-SQL, which improves the execution accuracy of the best-performing baseline by up to 3.20.
        ]]></description>
    </item>
    <item>
        <title>IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems</title>
        <link>https://arxiv.org/abs/2506.01615</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.01615v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Pasunuti Prasanjith, Prathmesh B More, Anoop Kunchukuttan, Raj Dabre</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）系统能让语言模型获取信息并生成准确回应，但印度语言高质量RAG系统发展受困于缺乏评估基准和大规模多语言检索训练数据集。方法：创建多语言基准IndicMSMarco，通过手动翻译MS MARCO-dev集中1000个多样查询，用于评估13种印度语言的检索质量和响应生成；利用先进大语言模型构建19种印度语言维基百科的大规模数据集；加入MS MARCO数据集翻译版本。效果：为印度语言RAG系统提供资源。
            arXiv:2506.01615v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems enable language models to access relevant information and generate accurate, well-grounded, and contextually informed responses. However, for Indian languages, the development of high-quality RAG systems is hindered by the lack of two critical resources: (1) evaluation benchmarks for retrieval and generation tasks, and (2) large-scale training datasets for multilingual retrieval. Most existing benchmarks and datasets are centered around English or high-resource languages, making it difficult to extend RAG capabilities to the diverse linguistic landscape of India. To address the lack of evaluation benchmarks, we create IndicMSMarco, a multilingual benchmark for evaluating retrieval quality and response generation in 13 Indian languages, created via manual translation of 1000 diverse queries from MS MARCO-dev set. To address the need for training data, we build a large-scale dataset of (question, answer, relevant passage) tuples derived from the Wikipedias of 19 Indian languages using state-of-the-art LLMs. Additionally, we include translated versions of the original MS MARCO dataset to further enrich the training data and ensure alignment with real-world information-seeking tasks. Resources are available here: https://huggingface.co/collections/ai4bharat/indicragsuite-683e7273cb2337208c8c0fcb
        ]]></description>
    </item>
    <item>
        <title>X-Driver: Explainable Autonomous Driving with Vision-Language Models</title>
        <link>https://arxiv.org/abs/2505.05098</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.05098v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wei Liu, Jiyuan Zhang, Binxiong Zheng, Yufeng Hu, Yingzhan Lin, Zengfeng Zeng</dc:creator>
        <description><![CDATA[
            背景：端到端自动驾驶虽有进展，但现有框架在闭环评估中成功率低，限制了实际部署。方法：本文提出X-Driver，这是一个用于闭环自动驾驶的统一多模态大语言模型框架，利用思维链和自回归建模来提升感知和决策能力。效果：在CARLA仿真环境的多个自动驾驶任务中验证，实验结果显示其闭环性能超当前最优水平，还提高了驾驶决策的可解释性，为闭环自动驾驶研究提供了有力基线。
            arXiv:2505.05098v2 Announce Type: replace-cross 
Abstract: End-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. However, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. In this paper, we introduce X-Driver, a unified multi-modal large language models(MLLMs) framework designed for closed-loop autonomous driving, leveraging Chain-of-Thought(CoT) and autoregressive modeling to enhance perception and decision-making. We validate X-Driver across multiple autonomous driving tasks using public benchmarks in CARLA simulation environment, including Bench2Drive[6]. Our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(SOTA) while improving the interpretability of driving decisions. These findings underscore the importance of structured reasoning in end-to-end driving and establish X-Driver as a strong baseline for future research in closed-loop autonomous driving.
        ]]></description>
    </item>
    <item>
        <title>GPR: Empowering Generation with Graph-Pretrained Retriever</title>
        <link>https://arxiv.org/abs/2506.00261</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.00261v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaochen Wang, Zongyu Wu, Yuan Zhong, Xiang Zhang, Suhang Wang, Fenglong Ma</dc:creator>
        <description><![CDATA[
            背景：图检索增强生成（GRAG）对特定图的检索器要求高，现有检索器常依赖基于纯文本预训练的语言模型，因领域不对齐和忽视结构而效果受限。方法：提出GPR，一种直接在知识图谱上预训练的基于图的检索器，通过大语言模型引导的图增强使自然语言问题与相关子图对齐，并采用结构感知目标学习细粒度检索策略。效果：在两个数据集、三个大语言模型骨干和五个基线模型上的实验表明，GPR能持续提升检索质量和下游生成效果。
            arXiv:2506.00261v2 Announce Type: replace-cross 
Abstract: Graph retrieval-augmented generation (GRAG) places high demands on graph-specific retrievers. However, existing retrievers often rely on language models pretrained on plain text, limiting their effectiveness due to domain misalignment and structure ignorance. To address these challenges, we propose GPR, a graph-based retriever pretrained directly on knowledge graphs. GPR aligns natural language questions with relevant subgraphs through LLM-guided graph augmentation and employs a structure-aware objective to learn fine-grained retrieval strategies. Experiments on two datasets, three LLM backbones, and five baselines show that GPR consistently improves both retrieval quality and downstream generation, demonstrating its effectiveness as a robust retrieval solution for GRAG.
        ]]></description>
    </item>
    <item>
        <title>No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction</title>
        <link>https://arxiv.org/abs/2506.02039</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02039v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haoshuai Zhou, Changgeng Mo, Boxuan Cao, Linkai Li, Shan Xiang Wang</dc:creator>
        <description><![CDATA[
            背景：个性化语音可懂度预测具有挑战性，以往基于听力图的方法因仅捕捉纯音听力阈值而存在精度局限。方法：提出利用个体现有可懂度数据预测其在新音频上表现的方法，引入基于支持样本的可懂度预测网络（SSIPNet），借助语音基础模型从多组（音频，分数）对构建听众语音识别能力的高维表示。效果：在Clarity Prediction Challenge数据集上，即使支持对数量少，该方法也优于基于听力图的预测。
            arXiv:2506.02039v1 Announce Type: new 
Abstract: Personalized speech intelligibility prediction is challenging. Previous approaches have mainly relied on audiograms, which are inherently limited in accuracy as they only capture a listener's hearing threshold for pure tones. Rather than incorporating additional listener features, we propose a novel approach that leverages an individual's existing intelligibility data to predict their performance on new audio. We introduce the Support Sample-Based Intelligibility Prediction Network (SSIPNet), a deep learning model that leverages speech foundation models to build a high-dimensional representation of a listener's speech recognition ability from multiple support (audio, score) pairs, enabling accurate predictions for unseen audio. Results on the Clarity Prediction Challenge dataset show that, even with a small number of support (audio, score) pairs, our method outperforms audiogram-based predictions. Our work presents a new paradigm for personalized speech intelligibility prediction.
        ]]></description>
    </item>
    <item>
        <title>Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition</title>
        <link>https://arxiv.org/abs/2506.02059</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02059v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ziwei Gong, Pengyuan Shi, Kaan Donbekci, Lin Ai, Run Chen, David Sasu, Zehui Wu, Julia Hirschberg</dc:creator>
        <description><![CDATA[
            语音情感识别（SER）虽借助深度学习取得进展，但低资源语言（LRLs）因标注数据稀缺面临挑战。本文探索无监督学习以提升低资源环境下的SER。具体采用对比学习（CL）和自引导潜在表征学习（BYOL）等自监督方法增强跨语言泛化能力。实验结果显示，该方法在乌尔都语、德语和孟加拉语中的F1分数分别提升10.6%、15.2%和13.9%，证明其在LRLs中的有效性。此外，还分析了模型行为，为开发面向低资源语言的情感识别系统奠定基础。
            arXiv:2506.02059v1 Announce Type: new 
Abstract: Speech Emotion Recognition (SER) has seen significant progress with deep learning, yet remains challenging for Low-Resource Languages (LRLs) due to the scarcity of annotated data. In this work, we explore unsupervised learning to improve SER in low-resource settings. Specifically, we investigate contrastive learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised approaches to enhance cross-lingual generalization. Our methods achieve notable F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla, demonstrating their effectiveness in LRLs. Additionally, we analyze model behavior to provide insights on key factors influencing performance across languages, and also highlighting challenges in low-resource SER. This work provides a foundation for developing more inclusive, explainable, and robust emotion recognition systems for underrepresented languages.
        ]]></description>
    </item>
    <item>
        <title>Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data</title>
        <link>https://arxiv.org/abs/2506.02078</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02078v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Emmy Postma, Cristian Tejedor-Garcia</dc:creator>
        <description><![CDATA[
            背景：言语障碍是帕金森病（PD）的常见生物标志物，利用语音数据的诊断技术有临床应用需求，但现有研究未充分探讨个体说话者差异对深度声学特征分类效果的影响。方法：研究三种预训练音频嵌入（OpenL3、VGGish和Wav2Vec2.0模型）对PD分类的有效性，使用NeuroVoz数据集。效果：OpenL3在DDK和LR任务中表现优于其他模型；Wav2Vec2.0在DDK任务中对男性说话者结果更优且有显著性别偏差；研究还发现非典型言语模式的分类难题。
            arXiv:2506.02078v1 Announce Type: new 
Abstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD), motivating the development of diagnostic techniques using speech data for clinical applications. Although deep acoustic features have shown promise for PD classification, their effectiveness often varies due to individual speaker differences, a factor that has not been thoroughly explored in the existing literature. This study investigates the effectiveness of three pre-trained audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification. Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK) and listen and repeat (LR) tasks, capturing critical acoustic features for PD detection. Only Wav2Vec2.0 shows significant gender bias, achieving more favorable results for male speakers, in DDK tasks. The misclassified cases reveal challenges with atypical speech patterns, highlighting the need for improved feature extraction and model robustness in PD detection.
        ]]></description>
    </item>
    <item>
        <title>SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction</title>
        <link>https://arxiv.org/abs/2506.02082</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02082v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Saurabh Agrawal, Raj Gohil, Gopal Kumar Agrawal, Vikram C M, Kushal Verma</dc:creator>
        <description><![CDATA[
            语音质量评估在选择文本到语音合成或语音转换模型时至关重要。现有客观指标难以用于筛选最佳模型，主观指标如平均意见得分虽可靠但耗时费力。为解决平均意见得分评估的问题，研究人员开发了SALF - MOS模型，该模型小巧、端到端、具有高泛化性和可扩展性。通过堆叠卷积序列获取音频样本的潜在特征，在均方误差、线性一致性相关系数等指标上取得了最优结果。
            arXiv:2506.02082v1 Announce Type: new 
Abstract: Speech quality assessment is a critical process in selecting text-to-speech synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can be done using objective metrics or subjective metrics. Although there are many objective metrics like the Perceptual Evaluation of Speech Quality (PESQ), Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time Objective Intelligibility (STOI) but none of them is feasible in selecting the best model. On the other hand subjective metric like Mean Opinion Score is highly reliable but it requires a lot of manual efforts and are time-consuming. To counter the issues in MOS Evaluation, we have developed a novel model, Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a small-sized, end-to-end, highly generalized and scalable model for predicting MOS score on a scale of 5. We use the sequences of convolutions and stack them to get the latent features of the audio samples to get the best state-of-the-art results based on mean squared error (MSE), Linear Concordance Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and Kendall Rank Correlation Coefficient (KTAU).
        ]]></description>
    </item>
    <item>
        <title>Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion</title>
        <link>https://arxiv.org/abs/2506.02085</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02085v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ajinkya Kulkarni, Sandipana Dowerah, Tanel Alumae, Mathew Magimai. -Doss</dc:creator>
        <description><![CDATA[
            背景：随着先进AI发展，音频深度伪造愈发逼真，追踪其来源系统很关键。方法：提出一种新颖的音频源追踪系统，结合了深度度量多类N对损失与真实强调和虚假分散框架、Conformer分类网络以及集成分数嵌入融合。N对损失提升判别能力，相关框架增强鲁棒性，Conformer网络捕捉音频信号的全局和局部依赖。效果：集成分数嵌入融合在域内和域外源追踪场景间实现最优权衡，用Frechet距离和标准指标评估，源追踪性能优于基线系统。
            arXiv:2506.02085v1 Announce Type: new 
Abstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced AI. While current research focuses on discerning real speech from spoofed speech, tracing the source system is equally crucial. This work proposes a novel audio source tracing system combining deep metric multi-class N-pair loss with Real Emphasis and Fake Dispersion framework, a Conformer classification network, and ensemble score-embedding fusion. The N-pair loss improves discriminative ability, while Real Emphasis and Fake Dispersion enhance robustness by focusing on differentiating real and fake speech patterns. The Conformer network captures both global and local dependencies in the audio signal, crucial for source tracing. The proposed ensemble score-embedding fusion shows an optimal trade-off between in-domain and out-of-domain source tracing scenarios. We evaluate our method using Frechet Distance and standard metrics, demonstrating superior performance in source tracing over the baseline system.
        ]]></description>
    </item>
    <item>
        <title>Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025</title>
        <link>https://arxiv.org/abs/2506.02088</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02088v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas \'Olives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo Galv\~ao Filho</dc:creator>
        <description><![CDATA[
            背景：在自然、自发语音中训练语音情感识别（SER）模型具有挑战性，因为真实世界音频中情感表达微妙且难以预测。方法：针对2025年国际语音通信大会自然条件下语音情感识别挑战赛，结合先进音频模型与富含韵律和频谱线索的文本特征，研究基频（F0）量化和预训练音频标签模型的有效性，还采用集成模型增强鲁棒性。效果：在官方测试集上Macro F1分数达39.79%（验证集为42.20%），证实了图注意力网络融合技术的有效性。
            arXiv:2506.02088v1 Announce Type: new 
Abstract: Training SER models in natural, spontaneous speech is especially challenging due to the subtle expression of emotions and the unpredictable nature of real-world audio. In this paper, we present a robust system for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing on categorical emotion recognition. Our method combines state-of-the-art audio models with text features enriched by prosodic and spectral cues. In particular, we investigate the effectiveness of Fundamental Frequency (F0) quantization and the use of a pretrained audio tagging model. We also employ an ensemble model to improve robustness. On the official test set, our system achieved a Macro F1-score of 39.79% (42.20% on validation). Our results underscore the potential of these methods, and analysis of fusion techniques confirmed the effectiveness of Graph Attention Networks. Our source code is publicly available.
        ]]></description>
    </item>
    <item>
        <title>Comparison of spectrogram scaling in multi-label Music Genre Recognition</title>
        <link>https://arxiv.org/abs/2506.02091</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02091v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Bartosz Karpi\'nski, Cyryl Leszczy\'nski</dc:creator>
        <description><![CDATA[
            背景：随着数字音频工作站易用性提高，普通听众可获取的音乐数量增多，且音乐流派差异界定模糊、组合多样。方法：本文描述并比较了多种预处理方法和模型训练方法，以适应如今专辑的多样性，并使用包含超18000条记录的自定义手动标注数据集进行实验。该研究与音频分类领域相关，通过对比不同方法来提升音乐流派识别效果。
            arXiv:2506.02091v1 Announce Type: new 
Abstract: As the accessibility and ease-of-use of digital audio workstations increases, so does the quantity of music available to the average listener; additionally, differences between genres are not always well defined and can be abstract, with widely varying combinations of genres across individual records. In this article, multiple preprocessing methods and approaches to model training are described and compared, accounting for the eclectic nature of today's albums. A custom, manually labeled dataset of more than 18000 entries has been used to perform the experiments.
        ]]></description>
    </item>
    <item>
        <title>Cocktail-Party Audio-Visual Speech Recognition</title>
        <link>https://arxiv.org/abs/2506.02178</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02178v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Thai-Binh Nguyen, Ngoc-Quan Pham, Alexander Waibel</dc:creator>
        <description><![CDATA[
            背景：音频视觉语音识别（AVSR）在复杂场景有重要作用，但现有模型多针对理想场景，忽略现实中面部有说话和沉默片段的复杂情况。方法：引入新的视听鸡尾酒会数据集来评估现有AVSR系统，还提供包含说话和沉默面部片段的1526小时AVSR数据集。效果：在鸡尾酒会环境中性能显著提升，相对现有技术将字错误率（WER）降低67%，在极端噪声下将WER从119%降至39.2%，且无需明确的分割线索。
            arXiv:2506.02178v1 Announce Type: new 
Abstract: Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech recognition in challenging environments, such as cocktail-party scenarios, where relying solely on audio proves insufficient. However, current AVSR models are often optimized for idealized scenarios with consistently active speakers, overlooking the complexities of real-world settings that include both speaking and silent facial segments. This study addresses this gap by introducing a novel audio-visual cocktail-party dataset designed to benchmark current AVSR systems and highlight the limitations of prior approaches in realistic noisy conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising both talking-face and silent-face segments, enabling significant performance gains in cocktail-party environments. Our approach reduces WER by 67% relative to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise, without relying on explicit segmentation cues.
        ]]></description>
    </item>
    <item>
        <title>Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models and their Synergistic Power for SingMOS Prediction</title>
        <link>https://arxiv.org/abs/2506.02232</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02232v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma</dc:creator>
        <description><![CDATA[
            背景：以往研究虽显示预训练模型（PTMs）对歌唱语音平均意见得分（SingMOS）预测有性能提升，但未探索说话人识别语音PTMs（SPTMs）。方法：假设SPTMs对SingMOS预测最有效，因其能更好捕捉合成歌声细粒度特征；还引入用Bhattacharya距离融合PTMs的BATCH框架。效果：实验验证了假设，通过BATCH融合SPTMs，在性能比较中超越单个PTMs和基线融合技术，达到了最优水平。
            arXiv:2506.02232v1 Announce Type: new 
Abstract: In this study, we focus on Singing Voice Mean Opinion Score (SingMOS) prediction. Previous research have shown the performance benefit with the use of state-of-the-art (SOTA) pre-trained models (PTMs). However, they haven't explored speaker recognition speech PTMs (SPTMs) such as x-vector, ECAPA and we hypothesize that it will be the most effective for SingMOS prediction. We believe that due to their speaker recognition pre-training, it equips them to capture fine-grained vocal features (e.g., pitch, tone, intensity) from synthesized singing voices in a much more better way than other PTMs. Our experiments with SOTA PTMs including SPTMs and music PTMs validates the hypothesis. Additionally, we introduce a novel fusion framework, BATCH that uses Bhattacharya Distance for fusion of PTMs. Through BATCH with the fusion of speaker recognition SPTMs, we report the topmost performance comparison to all the individual PTMs and baseline fusion techniques as well as setting SOTA.
        ]]></description>
    </item>
    <item>
        <title>Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal Emotion Recognition?</title>
        <link>https://arxiv.org/abs/2506.02258</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02258v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mohd Mujtaba Akhtar, Orchid Chetia Phukan,  Girish, Swarup Ranjan Behera, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru, Rajesh Sharma</dc:creator>
        <description><![CDATA[
            背景：非言语声音的情感识别（NVER）领域需更有效的模型。方法：首次研究基于Mamba的音频基础模型（MAFMs）用于NVER，假设其能通过状态空间建模更有效捕捉情感结构；还提出RENO，利用Renyi散度作为损失函数来有效对齐基础模型（FMs），并使用自注意力促进FMs内部交互。效果：实验验证MAFMs优于基于注意力的音频基础模型（AAFMs），RENO通过融合MAFMs和AAFMs取得顶尖性能，刷新了现有成果。
            arXiv:2506.02258v1 Announce Type: new 
Abstract: In this work, we focus on non-verbal vocal sounds emotion recognition (NVER). We investigate mamba-based audio foundation models (MAFMs) for the first time for NVER and hypothesize that MAFMs will outperform attention-based audio foundation models (AAFMs) for NVER by leveraging its state-space modeling to capture intrinsic emotional structures more effectively. Unlike AAFMs, which may amplify irrelevant patterns due to their attention mechanisms, MAFMs will extract more stable and context-aware representations, enabling better differentiation of subtle non-verbal emotional cues. Our experiments with state-of-the-art (SOTA) AAFMs and MAFMs validates our hypothesis. Further, motivated from related research such as speech emotion recognition, synthetic speech detection, where fusion of foundation models (FMs) have showed improved performance, we also explore fusion of FMs for NVER. To this end, we propose, RENO, that uses renyi-divergence as a novel loss function for effective alignment of the FMs. It also makes use of self-attention for better intra-representation interaction of the FMs. With RENO, through the heterogeneous fusion of MAFMs and AAFMs, we show the topmost performance in comparison to individual FMs, its fusion and also setting SOTA in comparison to previous SOTA work.
        ]]></description>
    </item>
    <item>
        <title>Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss</title>
        <link>https://arxiv.org/abs/2506.02339</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02339v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiawen Huang, Felipe Sousa, Emir Demirel, Emmanouil Benetos, Igor Gadelha</dc:creator>
        <description><![CDATA[
            背景：自动歌词转录（ALT）在处理歌声时因歌声特性面临复杂问题，基础自动语音识别（ASR）模型在歌声转录中性能下降。方法：聚焦性能差距，探索低秩自适应（LoRA）用于ALT，研究单域和双域微调策略，提出用一致性损失更好对齐人声和混合编码器表示，不依赖歌声分离来改进混合转录。效果：简单双域微调表现不佳，带一致性损失的结构化训练有适度且稳定的提升，展现了将ASR基础模型应用于音乐的潜力。
            arXiv:2506.02339v1 Announce Type: new 
Abstract: Automatic Lyrics Transcription (ALT) aims to recognize lyrics from singing voices, similar to Automatic Speech Recognition (ASR) for spoken language, but faces added complexity due to domain-specific properties of the singing voice. While foundation ASR models show robustness in various speech tasks, their performance degrades on singing voice, especially in the presence of musical accompaniment. This work focuses on this performance gap and explores Low-Rank Adaptation (LoRA) for ALT, investigating both single-domain and dual-domain fine-tuning strategies. We propose using a consistency loss to better align vocal and mixture encoder representations, improving transcription on mixture without relying on singing voice separation. Our results show that while na\"ive dual-domain fine-tuning underperforms, structured training with consistency loss yields modest but consistent gains, demonstrating the potential of adapting ASR foundation models for music.
        ]]></description>
    </item>
    <item>
        <title>Breaking the Barriers of Text-Hungry and Audio-Deficient AI</title>
        <link>https://arxiv.org/abs/2506.02443</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02443v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hamidou Tembine, Issa Bamia, Massa NDong, Bakary Coulibaly, Oumar Issiaka Traore, Moussa Traore, Moussa Sanogo, Mamadou Eric Sangare, Salif Kante, Daryl Noupa Yongueng, Hafiz Tiomoko Ali, Malik Tiomoko, Frejus Laleye, Boualem Djehiche, Wesmanegda Elisee Dipama, Idris Baba Saje, Hammid Mohammed Ibrahim, Moumini Sanogo, Marie Coursel Nininahazwe, Abdul-Latif Siita, Haine Mhlongo, Teddy Nelvy Dieu Merci Kouka, Mariam Serine Jeridi, Mutiyamuogo Parfait Mupenge, Lekoueiry Dehah, Abdoul Aziz Bio Sidi Bouko, Wilfried Franceslas Zokoue, Odette Richette Sambila, Alina RS Mbango, Mady Diagouraga, Oumarou Moussa Sanoussi, Gizachew Dessalegn, Mohamed Lamine Samoura, Bintou Laetitia Audrey Coulibaly</dc:creator>
        <description><![CDATA[
            背景：当前主流的机器智能架构存在偏向书面文本的问题，使得超7亿音频素养者被排除在外。方法：引入完全无文本的音频到音频机器智能框架，提出多种绕过文本的翻译架构，以多尺度音频语义变换（MAST）为核心，并将其集成到由分数布朗运动驱动的分数扩散框架中。效果：能在无文本监督下生成高保真、语义一致的语音，可直接从原始音频学习，让更多群体能使用语言技术。
            arXiv:2506.02443v1 Announce Type: new 
Abstract: While global linguistic diversity spans more than 7164 recognized languages, the current dominant architecture of machine intelligence remains fundamentally biased toward written text. This bias excludes over 700 million people particularly in rural and remote regions who are audio-literate. In this work, we introduce a fully textless, audio-to-audio machine intelligence framework designed to serve this underserved population, and all the people who prefer audio-efficiency. Our contributions include novel Audio-to-Audio translation architectures that bypass text entirely, including spectrogram-, scalogram-, wavelet-, and unit-based models. Central to our approach is the Multiscale Audio-Semantic Transform (MAST), a representation that encodes tonal, prosodic, speaker, and expressive features. We further integrate MAST into a fractional diffusion of mean-field-type framework powered by fractional Brownian motion. It enables the generation of high-fidelity, semantically consistent speech without reliance on textual supervision. The result is a robust and scalable system capable of learning directly from raw audio, even in languages that are unwritten or rarely digitized. This work represents a fundamental shift toward audio-native machine intelligence systems, expanding access to language technologies for communities historically left out of the current machine intelligence ecosystem.
        ]]></description>
    </item>
    <item>
        <title>SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based Voice Assistant</title>
        <link>https://arxiv.org/abs/2506.02457</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02457v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yixuan Hou, Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</dc:creator>
        <description><![CDATA[
            背景：随着大语言模型等技术发展，可直接根据用户指令生成语音响应，但生成语音质量的基准测试被忽视，此前评估缺乏对声学质量的量化。方法：提出语音对话式语音助手基准（SOVA - Bench），对现有语音大语言模型的常识、语音识别理解、语义及声学生成能力进行理解比较。效果：SOVA - Bench是最系统的语音大语言模型评估框架之一，为语音交互系统指明方向。
            arXiv:2506.02457v1 Announce Type: new 
Abstract: Thanks to the steady progress of large language models (LLMs), speech encoding algorithms and vocoder structure, recent advancements have enabled generating speech response directly from a user instruction. However, benchmarking the generated speech quality has been a neglected but critical issue, considering the shift from the pursuit of semantic accuracy to vivid and spontaneous speech flow. Previous evaluation focused on the speech-understanding ability, lacking a quantification of acoustic quality. In this paper, we propose Speech cOnversational Voice Assistant Benchmark (SOVA-Bench), providing a comprehension comparison of the general knowledge, speech recognition and understanding, along with both semantic and acoustic generative ability between available speech LLMs. To the best of our knowledge, SOVA-Bench is one of the most systematic evaluation frameworks for speech LLMs, inspiring the direction of voice interaction systems.
        ]]></description>
    </item>
    <item>
        <title>DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing Non-Verbal Sounds</title>
        <link>https://arxiv.org/abs/2506.02499</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02499v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Takuya Hasumi, Yusuke Fujita</dc:creator>
        <description><![CDATA[
            背景：现有电影音频源分离（CASS）数据集仅包含朗读式声音作为语音成分，与实际电影音频不同，导致基于传统数据集训练的模型在处理笑声、尖叫等情感强烈声音时存在分离问题。方法：构建新数据集DnR - nonverbal，在语音成分中纳入笑声、尖叫等非语言声音。效果：实验揭示了当前CASS模型在提取非语言声音方面的问题，且该数据集能有效解决合成及实际电影音频中的此问题。
            arXiv:2506.02499v1 Announce Type: new 
Abstract: We propose a new dataset for cinematic audio source separation (CASS) that handles non-verbal sounds. Existing CASS datasets only contain reading-style sounds as a speech stem. These datasets differ from actual movie audio, which is more likely to include acted-out voices. Consequently, models trained on conventional datasets tend to have issues where emotionally heightened voices, such as laughter and screams, are more easily separated as an effect, not speech. To address this problem, we build a new dataset, DnR-nonverbal. The proposed dataset includes non-verbal sounds like laughter and screams in the speech stem. From the experiments, we reveal the issue of non-verbal sound extraction by the current CASS model and show that our dataset can effectively address the issue in the synthetic and actual movie audio. Our dataset is available at https://zenodo.org/records/15470640.
        ]]></description>
    </item>
    <item>
        <title>Adaptive Differential Denoising for Respiratory Sounds Classification</title>
        <link>https://arxiv.org/abs/2506.02505</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02505v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Gaoyang Dong, Zhicheng Zhang, Ping Sun, Minghui Zhang</dc:creator>
        <description><![CDATA[
            这是一篇关于呼吸音分类的论文。背景是自动呼吸音分类面临背景噪声干扰及现有系统去噪不足的问题。方法上，提出自适应差分去噪网络，有三项创新：采用带可学习频谱掩码和软收缩的自适应频率滤波器；用差分注意力的差分去噪层；无干净标签下联合优化分类和鲁棒性的偏差去噪损失。实验表明，在ICBHI2017数据集上得分65.53%，比之前最优方法提高1.99%。
            arXiv:2506.02505v1 Announce Type: new 
Abstract: Automated respiratory sound classification faces practical challenges from background noise and insufficient denoising in existing systems.
  We propose Adaptive Differential Denoising network, that integrates noise suppression and pathological feature preservation via three innovations:
  1) Adaptive Frequency Filter with learnable spectral masks and soft shrink to eliminate noise while retaining diagnostic high-frequency components;
  2) A Differential Denoise Layer using differential attention to reduce noise-induced variations through augmented sample comparisons;
  3) A bias denoising loss jointly optimizing classification and robustness without clean labels.
  Experiments on the ICBHI2017 dataset show that our method achieves 65.53\% of the Score, which is improved by 1.99\% over the previous sota method.
  The code is available in https://github.com/deegy666/ADD-RSC
        ]]></description>
    </item>
    <item>
        <title>On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs</title>
        <link>https://arxiv.org/abs/2506.02545</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02545v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kemal Altwlkany, Amar Kuric, Emanuel Lacic</dc:creator>
        <description><![CDATA[
            背景：近年来，语音技术中的公平性和包容性备受关注，音频转码时编码机制的固有偏差会带来不良影响，因此无偏的音频编码机制很重要。方法：分析超200万个多语言音频文件经PSTN、VoIP和神经编解码器等转码后的语音质量。效果：研究表明，PSTN编解码器存在强烈的性别偏差，神经编解码器会引入语言偏差。
            arXiv:2506.02545v1 Announce Type: new 
Abstract: In recent years, there has been a growing focus on fairness and inclusivity within speech technology, particularly in areas such as automatic speech recognition and speech sentiment analysis. When audio is transcoded prior to processing, as is the case in streaming or real-time applications, any inherent bias in the coding mechanism may result in disparities. This not only affects user experience but can also have broader societal implications by perpetuating stereotypes and exclusion. Thus, it is important that audio coding mechanisms are unbiased. In this work, we contribute towards the scarce research with respect to language and gender biases of audio codecs. By analyzing the speech quality of over 2 million multilingual audio files after transcoding through a representative subset of codecs (PSTN, VoIP and neural), our results indicate that PSTN codecs are strongly biased in terms of gender and that neural codecs introduce language biases.
        ]]></description>
    </item>
    <item>
        <title>Synthetic Speech Source Tracing using Metric Learning</title>
        <link>https://arxiv.org/abs/2506.02590</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02590v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Dimitrios Koutsianos, Stavros Zacharopoulos, Yannis Panagakis, Themos Stafylakis</dc:creator>
        <description><![CDATA[
            背景：现有工作多聚焦于欺骗检测，合成语音的源追踪缺乏可靠解决方案。方法：评估基于分类和度量学习两种方法，在MLAADv5基准上用ResNet和自监督学习（SSL）主干测试。效果：结果显示，ResNet采用度量学习方法取得了有竞争力的性能，能达到甚至超过基于SSL的系统。该研究证明了ResNet用于源追踪的可行性，也强调了优化SSL表示的必要性，为应对合成媒体操纵提供新方向。
            arXiv:2506.02590v1 Announce Type: new 
Abstract: This paper addresses source tracing in synthetic speech-identifying generative systems behind manipulated audio via speaker recognition-inspired pipelines. While prior work focuses on spoofing detection, source tracing lacks robust solutions. We evaluate two approaches: classification-based and metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet and self-supervised learning (SSL) backbones. The results show that ResNet achieves competitive performance with the metric learning approach, matching and even exceeding SSL-based systems. Our work demonstrates ResNet's viability for source tracing while underscoring the need to optimize SSL representations for this task. Our work bridges speaker recognition methodologies with audio forensic challenges, offering new directions for combating synthetic media manipulation.
        ]]></description>
    </item>
    <item>
        <title>Cross-attention and Self-attention for Audio-visual Speaker Diarization in MISP-Meeting Challenge</title>
        <link>https://arxiv.org/abs/2506.02621</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02621v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhaoyang Li, Haodong Zhou, Longjie Luo, Xiaoxiao Li, Yongxin Chen, Lin Li, Qingyang Hong</dc:creator>
        <description><![CDATA[
            该论文是针对2025年多模态信息语音处理（MISP）挑战赛任务1开发的系统。背景是提升视听说话人分离（AVSD）性能。方法上，提出CASA - Net嵌入融合方法，用交叉注意力模块捕捉视听信号跨模态交互，用自注意力模块学习视听帧上下文关系，采用伪标签细化和再训练的训练策略，还用中值滤波和重叠平均作为后处理技术。效果是评估集上的分离错误率（DER）为8.18%，相对基线15.52%的DER提升47.3%。
            arXiv:2506.02621v1 Announce Type: new 
Abstract: This paper presents the system developed for Task 1 of the Multi-modal Information-based Speech Processing (MISP) 2025 Challenge. We introduce CASA-Net, an embedding fusion method designed for end-to-end audio-visual speaker diarization (AVSD) systems. CASA-Net incorporates a cross-attention (CA) module to effectively capture cross-modal interactions in audio-visual signals and employs a self-attention (SA) module to learn contextual relationships among audio-visual frames. To further enhance performance, we adopt a training strategy that integrates pseudo-label refinement and retraining, improving the accuracy of timestamp predictions. Additionally, median filtering and overlap averaging are applied as post-processing techniques to eliminate outliers and smooth prediction labels. Our system achieved a diarization error rate (DER) of 8.18% on the evaluation set, representing a relative improvement of 47.3% over the baseline DER of 15.52%.
        ]]></description>
    </item>
    <item>
        <title>Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions</title>
        <link>https://arxiv.org/abs/2506.02742</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02742v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaoxue Gao, Huayun Zhang, Nancy F. Chen</dc:creator>
        <description><![CDATA[
            背景：现有表达性文本转语音（TTS）系统主要模拟有限的分类情感，而人类对话的情感更丰富多样，需探索更多样的情感语音生成。方法：提出一种新的prompt-unseen-emotion（PUE）方法，通过情感引导的提示学习生成未见情感语音，利用LLM - TTS架构训练，确保提示与情感语音的情感一致性。效果：能在推理时灵活调整情感比例、利用LLM上下文知识生成混合情感语音，可量化不同情感风格，成功在零样本设置下实现未见情感的表达性语音合成。
            arXiv:2506.02742v1 Announce Type: new 
Abstract: Existing expressive text-to-speech (TTS) systems primarily model a limited set of categorical emotions, whereas human conversations extend far beyond these predefined emotions, making it essential to explore more diverse emotional speech generation for more natural interactions. To bridge this gap, this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate unseen emotional speech via emotion-guided prompt learning. PUE is trained utilizing an LLM-TTS architecture to ensure emotional consistency between categorical emotion-relevant prompts and emotional speech, allowing the model to quantitatively capture different emotion weightings per utterance. During inference, mixed emotional speech can be generated by flexibly adjusting emotion proportions and leveraging LLM contextual knowledge, enabling the model to quantify different emotional styles. Our proposed PUE successfully facilitates expressive speech synthesis of unseen emotions in a zero-shot setting.
        ]]></description>
    </item>
    <item>
        <title>AuralNet: Hierarchical Attention-based 3D Binaural Localization of Overlapping Speakers</title>
        <link>https://arxiv.org/abs/2506.02773</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02773v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Linya Fu, Yu Liu, Zhijie Liu, Zedong Yang, Zhong-Qiu Wang, Youfu Li, He Kong</dc:creator>
        <description><![CDATA[
            背景：在缺乏声源数量先验知识的情况下，实现重叠声源的3D双耳定位具有挑战。方法：提出AuralNet，采用门控粗到细架构，结合粗分类和细粒度回归阶段，通过扇区分割实现灵活空间分辨率；融入多头自注意力机制捕捉双耳信号空间线索；设计掩码多任务损失函数联合优化声音检测、方位角和仰角估计。效果：在嘈杂混响环境的大量实验表明，AuralNet优于现有方法。
            arXiv:2506.02773v1 Announce Type: new 
Abstract: We propose AuralNet, a novel 3D multi-source binaural sound source localization approach that localizes overlapping sources in both azimuth and elevation without prior knowledge of the number of sources. AuralNet employs a gated coarse-tofine architecture, combining a coarse classification stage with a fine-grained regression stage, allowing for flexible spatial resolution through sector partitioning. The model incorporates a multi-head self-attention mechanism to capture spatial cues in binaural signals, enhancing robustness in noisy-reverberant environments. A masked multi-task loss function is designed to jointly optimize sound detection, azimuth, and elevation estimation. Extensive experiments in noisy-reverberant conditions demonstrate the superiority of AuralNet over recent methods
        ]]></description>
    </item>
    <item>
        <title>Fast-Converging Distributed Signal Estimation in Topology-Unconstrained Wireless Acoustic Sensor Networks</title>
        <link>https://arxiv.org/abs/2506.02797</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02797v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Paul Didier, Toon van Waterschoot, Simon Doclo, J\"org Bitzer, Marc Moonen</dc:creator>
        <description><![CDATA[
            背景：拓扑无关的分布式自适应节点特定信号估计（TI - DANSE）算法在非全连接和时变网络拓扑中收敛慢，限制了其在现实场景的应用。方法：提出改进的TI - DANSE+算法，更新节点分别使用来自每个邻居的融合信号的部分网络内和，并结合树修剪策略。效果：在全连接无线声学传感器网络中，TI - DANSE+收敛速度与原DANSE算法一样快，且采用点对点数据传输节省通信带宽，链路故障时也能保持向集中式解决方案收敛。
            arXiv:2506.02797v1 Announce Type: new 
Abstract: This paper focuses on distributed signal estimation in topology-unconstrained wireless acoustic sensor networks (WASNs) where sensor nodes only transmit fused versions of their local sensor signals. For this task, the topology-independent (TI) distributed adaptive node-specific signal estimation (DANSE) algorithm (TI-DANSE) has previously been proposed. It converges towards the centralized signal estimation solution in non-fully connected and time-varying network topologies. However, the applicability of TI-DANSE in real-world scenarios is limited due to its slow convergence. The latter results from the fact that, in TI-DANSE, nodes only have access to the in-network sum of all fused signals in the WASN. We address this low convergence speed by introducing an improved TI-DANSE algorithm, referred to as TI-DANSE+, in which updating nodes separately use the partial in-network sums of fused signals coming from each of their neighbors. Nodes can maximize the number of available degrees of freedom in their local optimization problem, leading to faster convergence. This is further exploited by combining TI-DANSE+ with a tree-pruning strategy that maximizes the number of neighbors at the updating node. In fully connected WASNs, TI-DANSE+ converges as fast as the original DANSE algorithm (the latter only defined for fully connected WASNs) while using peer-to-peer data transmission instead of broadcasting and thus saving communication bandwidth. If link failures occur, the convergence of TI-DANSE+ towards the centralized solution is preserved without any change in its formulation. Altogether, the proposed TI-DANSE+ algorithm can be viewed as an all-round alternative to DANSE and TI-DANSE which (i) merges the advantages of both, (ii) reconciliates their differences into a single formulation, and (iii) shows advantages of its own in terms of communication bandwidth usage.
        ]]></description>
    </item>
    <item>
        <title>DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization</title>
        <link>https://arxiv.org/abs/2506.02858</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02858v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Geonyoung Lee, Geonhee Han, Paul Hongsuck Seo</dc:creator>
        <description><![CDATA[
            背景：现有语言查询音频源分离（LASS）方法依赖特定任务训练，而预训练扩散模型原用于音频生成，能否不训练就进行分离有待探索。方法：提出无需训练的框架，分析朴素适配的局限性，提出扩散引导掩码优化（DGMO）这一测试时优化框架，细化频谱图掩码以实现精确分离。效果：有效将预训练扩散模型用于源分离，在无特定任务监督下取得有竞争力的性能，拓展了扩散模型应用范围。
            arXiv:2506.02858v1 Announce Type: new 
Abstract: Language-queried Audio Source Separation (LASS) enables open-vocabulary sound separation via natural language queries. While existing methods rely on task-specific training, we explore whether pretrained diffusion models, originally designed for audio generation, can inherently perform separation without further training. In this study, we introduce a training-free framework leveraging generative priors for zero-shot LASS. Analyzing na\"ive adaptations, we identify key limitations arising from modality-specific challenges.To address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a test-time optimization framework that refines spectrogram masks for precise, input-aligned separation. Our approach effectively repurposes pretrained diffusion models for source separation, achieving competitive performance without task-specific supervision. This work expands the application of diffusion models beyond generation, establishing a new paradigm for zero-shot audio separation. The code is available at: https://wltschmrz.github.io/DGMO/
        ]]></description>
    </item>
    <item>
        <title>CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech</title>
        <link>https://arxiv.org/abs/2506.02863</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02863v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Helin Wang, Jiarui Hai, Dading Chong, Karan Thakkar, Tiantian Feng, Dongchao Yang, Junhyeok Lee, Laureano Moro Velazquez, Jesus Villalba, Zengyi Qin, Shrikanth Narayanan, Mounya Elhiali, Najim Dehak</dc:creator>
        <description><![CDATA[
            背景：生成式人工智能推动了风格描述文本转语音合成（CapTTS）发展，但因缺乏标准化数据集和下游任务研究，其应用仍面临挑战。方法：提出新基准CapSpeech，涵盖CapTTS相关任务，包含超1000万机器标注和近36万人工标注的音频 - 文本对，还为特定任务引入新数据集，并使用自回归和非自回归模型实验。效果：实现不同风格的高保真、高可懂度语音合成，是目前最大的CapTTS相关综合标注数据集，为开发CapTTS系统提供见解。
            arXiv:2506.02863v1 Announce Type: new 
Abstract: Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.
        ]]></description>
    </item>
    <item>
        <title>Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency</title>
        <link>https://arxiv.org/abs/2506.02908</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02908v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Bunlong Lay, Rostilav Makarov, Timo Gerkmann</dc:creator>
        <description><![CDATA[
            背景：扩散模型用于语音增强效果显著，但推理时计算成本高，难以实时处理流式数据。方法：将滑动窗口扩散框架应用于语音增强任务，逐步对语音信号进行加噪，在缓冲区中给靠近当前的帧分配更多噪声。效果：能输出去噪帧，延迟与所选缓冲区大小成正比，可平衡性能与延迟。该方法优于标准扩散模型，在GPU上高效运行，输入输出延迟为0.3到1秒，是首个实用的在线语音增强扩散方案。
            arXiv:2506.02908v1 Announce Type: new 
Abstract: Diffusion models are a class of generative models that have been recently used for speech enhancement with remarkable success but are computationally expensive at inference time. Therefore, these models are impractical for processing streaming data in real-time. In this work, we adapt a sliding window diffusion framework to the speech enhancement task. Our approach progressively corrupts speech signals through time, assigning more noise to frames close to the present in a buffer. This approach outputs denoised frames with a delay proportional to the chosen buffer size, enabling a trade-off between performance and latency. Empirical results demonstrate that our method outperforms standard diffusion models and runs efficiently on a GPU, achieving an input-output latency in the order of 0.3 to 1 seconds. This marks the first practical diffusion-based solution for online speech enhancement.
        ]]></description>
    </item>
    <item>
        <title>PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech Editing</title>
        <link>https://arxiv.org/abs/2506.02958</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02958v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>You Zhang, Baotong Tian, Lin Zhang, Zhiyao Duan</dc:creator>
        <description><![CDATA[
            背景：神经语音编辑技术在带来便利的同时也引发了深度伪造的新风险。方法：为推动对部分编辑的深度伪造语音的检测研究，引入利用先进神经编辑技术整理的深度伪造语音数据集PartialEdit，并在该数据集上探索检测和定位任务。效果：实验表明，在现有PartialSpoof数据集上训练的模型无法检测神经语音编辑模型生成的部分编辑语音，同时还对模型检测这些深度伪造语音时学到的伪影提供了见解。
            arXiv:2506.02958v1 Announce Type: new 
Abstract: Neural speech editing enables seamless partial edits to speech utterances, allowing modifications to selected content while preserving the rest of the audio unchanged. This useful technique, however, also poses new risks of deepfakes. To encourage research on detecting such partially edited deepfake speech, we introduce PartialEdit, a deepfake speech dataset curated using advanced neural editing techniques. We explore both detection and localization tasks on PartialEdit. Our experiments reveal that models trained on the existing PartialSpoof dataset fail to detect partially edited speech generated by neural speech editing models. As recent speech editing models almost all involve neural audio codecs, we also provide insights into the artifacts the model learned on detecting these deepfakes. Further information about the PartialEdit dataset and audio samples can be found on the project page: https://yzyouzhang.com/PartialEdit/index.html.
        ]]></description>
    </item>
    <item>
        <title>InfiniteAudio: Infinite-Length Audio Generation with Consistency</title>
        <link>https://arxiv.org/abs/2506.03020</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03020v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chaeyoung Jung, Hojoon Ki, Ji-Hoon Kim, Junmo Kim, Joon Son Chung</dc:creator>
        <description><![CDATA[
            背景：当前基于扩散的文本到音频方法在生成音频时因输出大小随输入长度增加存在内存限制，长时长生成困难，拼接短音频段又易因缺乏时间上下文而不一致。方法：提出InfiniteAudio策略，无缝集成到现有流程且无需额外训练，引入FIFO采样（固定大小输入的先进先出推理策略）和弯曲去噪（选择性优先处理关键扩散步骤）。效果：实验表明在所有指标上实现了相当或更优的性能。
            arXiv:2506.03020v1 Announce Type: new 
Abstract: This paper presents InfiniteAudio, a simple yet effective strategy for generating infinite-length audio using diffusion-based text-to-audio methods. Current approaches face memory constraints because the output size increases with input length, making long duration generation challenging. A common workaround is to concatenate short audio segments, but this often leads to inconsistencies due to the lack of shared temporal context. To address this, InfiniteAudio integrates seamlessly into existing pipelines without additional training. It introduces two key techniques: FIFO sampling, a first-in, first-out inference strategy with fixed-size inputs, and curved denoising, which selectively prioritizes key diffusion steps for efficiency. Experiments show that InfiniteAudio achieves comparable or superior performance across all metrics. Audio samples are available on our project page.
        ]]></description>
    </item>
    <item>
        <title>TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models</title>
        <link>https://arxiv.org/abs/2506.03099</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03099v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chetwin Low, Weimin Wang</dc:creator>
        <description><![CDATA[
            背景：为实现自然对话体验，需将预训练视频生成模型转化为实时音频驱动角色动画师。方法：提出TalkingMachines框架，将音频大语言模型与视频生成基础模型结合，把预训练的图像到视频DiT改编为180亿参数的音频驱动头像生成模型，通过不对称知识蒸馏实现无误差积累的无限视频流，还设计了含多项工程优化的推理管道。效果：实现了实时、音频驱动的角色动画，可查看演示视频https://aaxwaz.github.io/TalkingMachines/ 。
            arXiv:2506.03099v1 Announce Type: new 
Abstract: In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/
        ]]></description>
    </item>
    <item>
        <title>Singing Voice Graph Modeling for SingFake Detection</title>
        <link>https://arxiv.org/abs/2406.03111</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.03111v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xuanjun Chen, Haibin Wu, Jyh-Shing Roger Jang, Hung-yi Lee</dc:creator>
        <description><![CDATA[
            这是一篇关于音频鉴伪的研究。现有语音深度伪造检测模型难以适应歌唱领域未知攻击。为此提出SingGraph模型，将MERT声学音乐理解模型用于音高和节奏分析的能力与wav2vec2.0模型对歌词的语言分析能力相结合。同时，采用基于音乐领域知识的RawBoost和节拍匹配技术对歌声进行增强。该方法在SingFake数据集中取得了新的最优结果，在三种场景下超越了之前的最优模型，对已知歌手、未知歌手、使用不同编解码器的未知歌手的等错误率相对分别提升13.2%、24.3%、37.1%。
            arXiv:2406.03111v2 Announce Type: cross 
Abstract: Detecting singing voice deepfakes, or SingFake, involves determining the authenticity and copyright of a singing voice. Existing models for speech deepfake detection have struggled to adapt to unseen attacks in this unique singing voice domain of human vocalization. To bridge the gap, we present a groundbreaking SingGraph model. The model synergizes the capabilities of the MERT acoustic music understanding model for pitch and rhythm analysis with the wav2vec2.0 model for linguistic analysis of lyrics. Additionally, we advocate for using RawBoost and beat matching techniques grounded in music domain knowledge for singing voice augmentation, thereby enhancing SingFake detection performance. Our proposed method achieves new state-of-the-art (SOTA) results within the SingFake dataset, surpassing the previous SOTA model across three distinct scenarios: it improves EER relatively for seen singers by 13.2%, for unseen singers by 24.3%, and unseen singers using different codecs by 37.1%.
        ]]></description>
    </item>
    <item>
        <title>Investigating the Impact of Word Informativeness on Speech Emotion Recognition</title>
        <link>https://arxiv.org/abs/2506.02239</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02239v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sofoklis Kakouros</dc:creator>
        <description><![CDATA[
            在语音情感识别领域，传统方法计算整句或长语音段特征时可能遗漏关键的细粒度变化。该研究利用预训练语言模型得到的词信息性来识别语义重要的语音片段，然后仅针对这些片段计算声学特征以提高情感识别准确率。方法采用了标准声学韵律特征、其泛函及自监督表征。结果显示，基于词信息性选择片段计算特征时，识别性能有显著提升，凸显了该方法的有效性。
            arXiv:2506.02239v1 Announce Type: cross 
Abstract: In emotion recognition from speech, a key challenge lies in identifying speech signal segments that carry the most relevant acoustic variations for discerning specific emotions. Traditional approaches compute functionals for features such as energy and F0 over entire sentences or longer speech portions, potentially missing essential fine-grained variation in the long-form statistics. This research investigates the use of word informativeness, derived from a pre-trained language model, to identify semantically important segments. Acoustic features are then computed exclusively for these identified segments, enhancing emotion recognition accuracy. The methodology utilizes standard acoustic prosodic features, their functionals, and self-supervised representations. Results indicate a notable improvement in recognition performance when features are computed on segments selected based on word informativeness, underscoring the effectiveness of this approach.
        ]]></description>
    </item>
    <item>
        <title>Sounding Like a Winner? Prosodic Differences in Post-Match Interviews</title>
        <link>https://arxiv.org/abs/2506.02283</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02283v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sofoklis Kakouros, Haoyu Chen</dc:creator>
        <description><![CDATA[
            背景：研究网球赛后采访中与胜负相关的韵律特征，探索仅根据赛后采访录音来分类比赛结果的可能性。方法：分析音高、强度等韵律元素，结合Wav2Vec 2.0和HuBERT等自监督学习模型，提取传统声学特征和深度语音表征，用机器学习分类器区分胜负运动员。效果：自监督学习表征能有效区分胜负结果，捕捉与情绪状态相关的细微语音模式，音高可变性等韵律线索仍是胜利的有力指标。
            arXiv:2506.02283v1 Announce Type: cross 
Abstract: This study examines the prosodic characteristics associated with winning and losing in post-match tennis interviews. Additionally, this research explores the potential to classify match outcomes solely based on post-match interview recordings using prosodic features and self-supervised learning (SSL) representations. By analyzing prosodic elements such as pitch and intensity, alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine whether an athlete has won or lost their match. Traditional acoustic features and deep speech representations are extracted from the data, and machine learning classifiers are employed to distinguish between winning and losing players. Results indicate that SSL representations effectively differentiate between winning and losing outcomes, capturing subtle speech patterns linked to emotional states. At the same time, prosodic cues -- such as pitch variability -- remain strong indicators of victory.
        ]]></description>
    </item>
    <item>
        <title>StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion</title>
        <link>https://arxiv.org/abs/2506.02414</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02414v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu</dc:creator>
        <description><![CDATA[
            这是一篇关于语音转换的研究。传统语音转换方法直接从语音中提取说话人信息，忽视对语言内容的明确利用，此前将语义特征融入语音转换的尝试效果有限。为此，研究者提出统一自回归语音转换框架StarVC，先预测文本标记，再合成声学特征。实验表明，StarVC在保留语言内容（WER和CER指标）和说话人特征（SECS和MOS指标）方面优于传统方法。
            arXiv:2506.02414v1 Announce Type: cross 
Abstract: Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content. Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content. Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance. However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling. We propose StarVC, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features. The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found at: https://thuhcsi.github.io/StarVC/.
        ]]></description>
    </item>
    <item>
        <title>Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning</title>
        <link>https://arxiv.org/abs/2506.02584</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02584v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sarenne Wallbridge, Christoph Minixhofer, Catherine Lai, Peter Bell</dc:creator>
        <description><![CDATA[
            背景：人们在文本理解中利用词汇结构的可预测性，但韵律独立于词汇内容对语音结构的贡献程度尚不明确。方法：本研究利用自监督学习（SSL）来研究韵律声学关联结构的时间粒度，提出的掩码韵律模型可预测依赖局部信息的感知标签。效果：该模型在涉及长期结构的标签（如情绪识别）上最具价值，探测实验显示其相对未转换的音高、能量和语音活动特征有显著提升，凸显了SSL训练目标时间尺度的重要性。
            arXiv:2506.02584v1 Announce Type: cross 
Abstract: People exploit the predictability of lexical structures during text comprehension. Though predictable structure is also present in speech, the degree to which prosody, e.g. intonation, tempo, and loudness, contributes to such structure independently of the lexical content is unclear. This study leverages self-supervised learning (SSL) to examine the temporal granularity of structures in the acoustic correlates of prosody. Representations from our proposed Masked Prosody Model can predict perceptual labels dependent on local information, such as word boundaries, but provide the most value for labels involving longer-term structures, like emotion recognition. Probing experiments across various perceptual labels show strong relative gains over untransformed pitch, energy, and voice activity features. Our results reveal the importance of SSL training objective timescale and highlight the value of complex SSL-encoded structures compared to more constrained classical structures.
        ]]></description>
    </item>
    <item>
        <title>FlashAudio: Rectified Flows for Fast and High-Fidelity Text-to-Audio Generation</title>
        <link>https://arxiv.org/abs/2410.12266</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.12266v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Huadai Liu, Jialei Wang, Rongjie Huang, Yang Liu, Heng Lu, Zhou Zhao, Wei Xue</dc:creator>
        <description><![CDATA[
            背景：现有潜在扩散模型虽提升了文本到音频生成能力，但迭代采样计算量大，基于一致性蒸馏的方法单步性能受限。方法：提出FlashAudio，用整流流学习直线流以快速模拟，用双焦采样器优化整流流的时间分布，提出不混溶流减少数据 - 噪声对总距离；提出锚定优化解决分类器自由引导的累积误差。效果：在文本到音频生成实验中，单步生成的音频质量超数百步扩散模型，在单NVIDIA 4090Ti GPU上采样速度比实时快400倍。
            arXiv:2410.12266v2 Announce Type: replace 
Abstract: Recent advancements in latent diffusion models (LDMs) have markedly enhanced text-to-audio generation, yet their iterative sampling processes impose substantial computational demands, limiting practical deployment. While recent methods utilizing consistency-based distillation aim to achieve few-step or single-step inference, their one-step performance is constrained by curved trajectories, preventing them from surpassing traditional diffusion models. In this work, we introduce FlashAudio with rectified flows to learn straight flow for fast simulation. To alleviate the inefficient timesteps allocation and suboptimal distribution of noise, FlashAudio optimizes the time distribution of rectified flow with Bifocal Samplers and proposes immiscible flow to minimize the total distance of data-noise pairs in a batch vias assignment. Furthermore, to address the amplified accumulation error caused by the classifier-free guidance (CFG), we propose Anchored Optimization, which refines the guidance scale by anchoring it to a reference trajectory. Experimental results on text-to-audio generation demonstrate that FlashAudio's one-step generation performance surpasses the diffusion-based models with hundreds of sampling steps on audio quality and enables a sampling speed of 400x faster than real-time on a single NVIDIA 4090Ti GPU. Code will be available at https://github.com/liuhuadai/FlashAudio.
        ]]></description>
    </item>
    <item>
        <title>Score-informed Music Source Separation: Improving Synthetic-to-real Generalization in Classical Music</title>
        <link>https://arxiv.org/abs/2503.07352</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.07352v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Eetu Tunturi, David Diaz-Guerra, Archontis Politis, Tuomas Virtanen</dc:creator>
        <description><![CDATA[
            背景：音乐源分离旨在将乐器混合音分离成单独音轨，以往模型多仅用音频数据训练，利用额外信息可提升分离能力。方法：提出两种利用乐谱辅助音乐源分离的方式，一是将乐谱与音频混合幅度谱拼接作为模型输入，二是仅用乐谱计算分离掩码。效果：在合成数据上训练模型，在真实录音数据集评估，得分信息模型比基线方法有改进但合成到真实数据泛化能力差，仅用乐谱模型在泛化上有明显提升。
            arXiv:2503.07352v2 Announce Type: replace 
Abstract: Music source separation is the task of separating a mixture of instruments into constituent tracks. Music source separation models are typically trained using only audio data, although additional information can be used to improve the model's separation capability. In this paper, we propose two ways of using musical scores to aid music source separation: a score-informed model where the score is concatenated with the magnitude spectrogram of the audio mixture as the input of the model, and a model where we use only the score to calculate the separation mask. We train our models on synthetic data in the SynthSOD dataset and evaluate our methods on the URMP and Aalto anechoic orchestra datasets, comprised of real recordings. The score-informed model improves separation results compared to a baseline approach, but struggles to generalize from synthetic to real data, whereas the score-only model shows a clear improvement in synthetic-to-real generalization.
        ]]></description>
    </item>
    <item>
        <title>OmniAudio: Generating Spatial Audio from 360-Degree Video</title>
        <link>https://arxiv.org/abs/2504.14906</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.14906v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Huadai Liu, Tianyi Luo, Kaicheng Luo, Qikai Jiang, Peiwen Sun, Jialei Wang, Rongjie Huang, Qian Chen, Wen Wang, Xiangtai Li, Shiliang Zhang, Zhijie Yan, Zhou Zhao, Wei Xue</dc:creator>
        <description><![CDATA[
            背景：传统视频到音频生成技术聚焦透视视频和非空间音频，缺少3D环境中准确呈现声源的空间线索。方法：引入360V2SA任务，创建数据集Sphere360及数据采集清理管道；提出OmniAudio框架，利用自监督预训练和双分支结构，结合全景和透视视频输入。效果：在Sphere360上，OmniAudio在客观和主观指标上均达到了最先进水平。
            arXiv:2504.14906v3 Announce Type: replace 
Abstract: Traditional video-to-audio generation techniques primarily focus on perspective video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, 360V2SA, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create Sphere360, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a dual-branch framework that utilizes both panoramic and perspective video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that OmniAudio achieves state-of-the-art performance across both objective and subjective metrics on Sphere360. Code and datasets are available at https://github.com/liuhuadai/OmniAudio. The project website is available at https://OmniAudio-360V2SA.github.io.
        ]]></description>
    </item>
    <item>
        <title>Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding</title>
        <link>https://arxiv.org/abs/2505.15380</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.15380v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zijian Lin, Yang Zhang, Yougen Yuan, Yuming Yan, Jinjiang Liu, Zhiyong Wu, Pengfei Hu, Qun Yu</dc:creator>
        <description><![CDATA[
            背景：现代利用语言模型的自回归语音合成模型虽性能出色，但因逐词预测的顺序性导致推理延迟大，阻碍其在对推理速度要求高的场景中应用。方法：提出语音推测解码（SSD）框架，用轻量级草稿模型生成候选标记序列，再由目标模型用SSD框架并行验证。效果：与传统自回归解码相比，SSD实现1.4倍的显著加速，且保持高保真度和自然度，主观评估也验证其在加速推理时能保留目标模型的感知质量。
            arXiv:2505.15380v2 Announce Type: replace 
Abstract: Modern autoregressive speech synthesis models leveraging language models have demonstrated remarkable performance. However, the sequential nature of next token prediction in these models leads to significant latency, hindering their deployment in scenarios where inference speed is critical. In this work, we propose Speech Speculative Decoding (SSD), a novel framework for autoregressive speech synthesis acceleration. Specifically, our method employs a lightweight draft model to generate candidate token sequences, which are subsequently verified in parallel by the target model using the proposed SSD framework. Experimental results demonstrate that SSD achieves a significant speedup of 1.4x compared with conventional autoregressive decoding, while maintaining high fidelity and naturalness. Subjective evaluations further validate the effectiveness of SSD in preserving the perceptual quality of the target model while accelerating inference.
        ]]></description>
    </item>
    <item>
        <title>Ola: Pushing the Frontiers of Omni-Modal Language Model</title>
        <link>https://arxiv.org/abs/2502.04328</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.04328v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao</dc:creator>
        <description><![CDATA[
            背景：当前大语言模型发展下，通用多模态模型性能与单模态模型有差距。方法：本文提出Omni-modal Language模型Ola，对架构设计、数据管理和训练策略进行全面探索，在主流基线基础上进行改进以融入视觉理解和音频识别能力，还重新思考跨模态关系，提出渐进式训练流程。效果：大量实验表明，Ola在各模态上超越现有开源多模态大语言模型，与同规模先进单模态模型相比也有很强竞争力。
            arXiv:2502.04328v3 Announce Type: replace-cross 
Abstract: Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal Language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts, pushing the frontiers of the omni-modal language model to a large extent. We conduct a comprehensive exploration of architectural design, data curation, and training strategies essential for building a robust omni-modal model. Ola incorporates advanced visual understanding and audio recognition capabilities through several critical and effective improvements over mainstream baselines. Moreover, we rethink inter-modal relationships during omni-modal training, emphasizing cross-modal alignment with video as a central bridge, and propose a progressive training pipeline that begins with the most distinct modalities and gradually moves towards closer modality alignment. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.
        ]]></description>
    </item>
    <item>
        <title>Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation</title>
        <link>https://arxiv.org/abs/2505.13338</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.13338v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw</dc:creator>
        <description><![CDATA[
            当前语音大语言模型在上下文推理和副语言理解方面能力有限，主要原因是缺乏涵盖这两方面的问答数据集。为此，本文提出一种从自然语音数据生成数据集的新框架，该框架结合上下文推理与副语言信息，包括基于伪副语言标签的数据浓缩和基于大语言模型的上下文副语言问答生成。通过在框架生成的数据集上对Qwen2 - Audio - 7B - Instruct模型进行评估，验证了其有效性。研究还揭示了语音大语言模型在处理共情推理任务上的局限，此框架有望用于训练更强大的具有副语言推理能力的语音大语言模型。
            arXiv:2505.13338v2 Announce Type: replace-cross 
Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.
        ]]></description>
    </item>
    <item>
        <title>Continual Speech Learning with Fused Speech Features</title>
        <link>https://arxiv.org/abs/2506.01496</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.01496v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Guitao Wang, Jinming Zhao, Hao Yang, Guilin Qi, Tongtong Wu, Gholamreza Haffari</dc:creator>
        <description><![CDATA[
            背景：语音数据快速增长，传统静态方法无法适应动态多样的语音信息。方法：引入连续语音学习，利用编码器 - 解码器Whisper模型将语音任务标准化为生成格式，在编码器顶部集成可学习的门控融合层，为下游任务动态选择特定任务的特征。效果：在六项语音处理任务中，该方法较传统方法显著提高了准确率，能在不进行完全再训练的情况下适应新的语音任务。
            arXiv:2506.01496v2 Announce Type: replace-cross 
Abstract: Rapid growth in speech data demands adaptive models, as traditional static methods fail to keep pace with dynamic and diverse speech information. We introduce continuous speech learning, a new set-up targeting at bridging the adaptation gap in current speech models. We use the encoder-decoder Whisper model to standardize speech tasks into a generative format. We integrate a learnable gated-fusion layer on the top of the encoder to dynamically select task-specific features for downstream tasks. Our approach improves accuracy significantly over traditional methods in six speech processing tasks, demonstrating gains in adapting to new speech tasks without full retraining.
        ]]></description>
    </item>
</channel>
</rss>