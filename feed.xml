<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 14 Apr 2025 12:11:19 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Mon, 14 Apr 2025 12:11:19 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use</title>
        <link>https://arxiv.org/abs/2504.07981</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.07981v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, Tat-Seng Chua</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型在通用GUI代理任务取得进展，但专业领域应用探索不足，专业工作流对GUI感知模型带来挑战。方法：提出新基准ScreenSpot - Pro，含专业领域高分辨率图像及专家标注；基于实验中缩小搜索区域可提升准确率的发现，提出视觉搜索方法ScreenSeekeR，用强规划器的GUI知识引导级联搜索。效果：现有GUI接地模型在数据集上表现差，最佳仅18.9%，ScreenSeekeR未额外训练达48.1%的先进水平。
            arXiv:2504.07981v1 Announce Type: new 
Abstract: Recent advancements in Multi-modal Large Language Models (MLLMs) have led to significant progress in developing GUI agents for general tasks such as web browsing and mobile phone use. However, their application in professional domains remains under-explored. These specialized workflows introduce unique challenges for GUI perception models, including high-resolution displays, smaller target sizes, and complex environments. In this paper, we introduce ScreenSpot-Pro, a new benchmark designed to rigorously evaluate the grounding capabilities of MLLMs in high-resolution professional settings. The benchmark comprises authentic high-resolution images from a variety of professional domains with expert annotations. It spans 23 applications across five industries and three operating systems. Existing GUI grounding models perform poorly on this dataset, with the best model achieving only 18.9%. Our experiments reveal that strategically reducing the search area enhances accuracy. Based on this insight, we propose ScreenSeekeR, a visual search method that utilizes the GUI knowledge of a strong planner to guide a cascaded search, achieving state-of-the-art performance with 48.1% without any additional training. We hope that our benchmark and findings will advance the development of GUI agents for professional applications. Code, data and leaderboard can be found at https://gui-agent.github.io/grounding-leaderboard.
        ]]></description>
    </item>
    <item>
        <title>SEAL: Steerable Reasoning Calibration of Large Language Models for Free</title>
        <link>https://arxiv.org/abs/2504.07986</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.07986v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, Zhangyang Wang</dc:creator>
        <description><![CDATA[
            背景：大语言模型通过扩展思维链推理机制处理复杂推理任务，但思维链推理痕迹存在冗余，增加推理延迟并影响性能。方法：研究大语言模型内部推理结构，分为执行、反思和过渡三种思维类型，发现冗余思维与失败案例强相关且在潜在空间可分离，提出无训练方法SEAL，离线提取推理引导向量，通过向量进行推理痕迹校准。效果：在多模型和基准测试中验证有效，准确率最高提升11%，推理令牌减少11.8% - 50.4%。
            arXiv:2504.07986v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (Steerable reasoning calibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11% improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our code is publicly available at https://github.com/VITA-Group/SEAL.
        ]]></description>
    </item>
    <item>
        <title>Investigating Vision-Language Model for Point Cloud-based Vehicle Classification</title>
        <link>https://arxiv.org/abs/2504.08154</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08154v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yiqiao Li, Jie Wei, Camille Kamga</dc:creator>
        <description><![CDATA[
            背景：重型卡车因体积大、机动性有限带来安全挑战，传统基于激光雷达的卡车分类方法依赖大量人工标注，成本高。现有视觉 - 语言模型主要基于图像数据训练，难处理点云数据。方法：提出新框架，将路边激光雷达点云数据与视觉 - 语言模型集成，包括利用真实激光雷达数据集、设计预处理流程适配点云数据、用上下文学习和少样本提示进行车辆分类。效果：实验显示该方法性能良好，可减少标注工作，提高分类准确率。
            arXiv:2504.08154v1 Announce Type: new 
Abstract: Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck classification methods rely on extensive manual annotations, which makes them labor-intensive and costly. The rapid advancement of large language models (LLMs) trained on massive datasets presents an opportunity to leverage their few-shot learning capabilities for truck classification. However, existing vision-language models (VLMs) are primarily trained on image datasets, which makes it challenging to directly process point cloud data. This study introduces a novel framework that integrates roadside LiDAR point cloud data with VLMs to facilitate efficient and accurate truck classification, which supports cooperative and safe driving environments. This study introduces three key innovations: (1) leveraging real-world LiDAR datasets for model development, (2) designing a preprocessing pipeline to adapt point cloud data for VLM input, including point cloud registration for dense 3D rendering and mathematical morphological techniques to enhance feature representation, and (3) utilizing in-context learning with few-shot prompting to enable vehicle classification with minimally labeled training data. Experimental results demonstrate encouraging performance of this method and present its potential to reduce annotation efforts while improving classification accuracy.
        ]]></description>
    </item>
    <item>
        <title>Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner</title>
        <link>https://arxiv.org/abs/2504.08247</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08247v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Liu Xiao, Li Zhiyuan, Lin Yueyu</dc:creator>
        <description><![CDATA[
            背景：基于状态的序列模型RWKV - 7虽有优势，但缺乏令牌 - 参数交互机制和原生可扩展性。方法：提出Meta - State，用全状态驱动方法替代注意力机制，通过Self - State Encoder（SSE）机制集成令牌 - 参数交互，将部分RWKV - 7的加权键值（WKV）状态用作转换权重，以线性、状态驱动方式编码交互。效果：支持模型渐进式扩展，复用现有参数无需重新训练，为高效、自适应的序列建模提供灵活框架，具有线性复杂度和恒定内存使用。
            arXiv:2504.08247v1 Announce Type: new 
Abstract: State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \(\text{TC}^0\) complexity class. However, RWKV-7 lacks mechanisms for token-parameter interactions and native scalability, limiting its adaptability and growth without retraining. In this paper, we propose \textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention mechanisms with a fully state-driven approach, integrating token-parameter interactions through a \textbf{Self-State Encoder} (SSE) mechanism. The SSE repurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as transformation weights to encode token-parameter interactions in a linear, state-driven manner without introducing new trainable matrices or softmax operations, while preserving the autoregressive property of token processing. Meta-State supports progressive model scaling by expanding the WKV state and parameter tokens, reusing existing parameters without retraining. Our approach bridges the gap between state-based modeling, token-parameter interactions, and scalable architectures, offering a flexible framework for efficient and adaptable sequence modeling with linear complexity and constant memory usage.
        ]]></description>
    </item>
    <item>
        <title>VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering</title>
        <link>https://arxiv.org/abs/2504.08269</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08269v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qi Zhi Lim, Chin Poo Lee, Kian Ming Lim, Kalaiarasi Sonai Muthu Anbananthen</dc:creator>
        <description><![CDATA[
            背景：多模态数据增多对复杂跨模态推理模型提出挑战，现有多模态多跳问答方法存在推理能力有限等问题。方法：提出VLMT统一架构，融合基于Transformer的视觉编码器和序列到序列语言模型，采用直接令牌级注入机制融合输入，提出三阶段预训练策略增强对齐与推理，构建两阶段MMQA框架。效果：在两个基准数据集上表现出色，在MultimodalQA验证集上，VLMT - Large精确匹配率76.5%、F1值80.1%；在WebQA上QA得分47.6，超越先前模型。 
            arXiv:2504.08269v1 Announce Type: new 
Abstract: The increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) often suffer from limited reasoning capabilities, reliance on modality conversion, and inadequate alignment between visual and textual representations. To address these limitations, this paper introduces Vision-Language Multimodal Transformer (VLMT), a unified architecture that integrates a transformer-based vision encoder with a sequence-to-sequence language model. VLMT employs a direct token-level injection mechanism to fuse visual and textual inputs within a shared embedding space, eliminating the need for intermediate projection layers. To enhance cross-modal alignment and reasoning, a three-stage pretraining strategy is proposed to progressively align vision-language representations and improve the model's capacity for multimodal understanding. Based on the pretrained backbone, two task-specific modules are instantiated to form a two-stage MMQA framework: a multimodal reranker that predicts document relevance scores and utilizes a relative threshold with top-k strategy for context retrieval, and a multimodal question answering model that generates contextually grounded answers based on the retrieved evidence. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed approach. On MultimodalQA validation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1, outperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8% in F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as PERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal reasoning and its potential to advance real-world information retrieval and question answering systems.
        ]]></description>
    </item>
    <item>
        <title>DSM: Building A Diverse Semantic Map for 3D Visual Grounding</title>
        <link>https://arxiv.org/abs/2504.08307</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08307v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qinghongbing Xie, Zijian Liang, Long Zeng</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型用于机器人场景理解时，现有3D视觉定位方法常忽视场景多样语义信息提取和隐含语义属性理解。方法：提出适用于机器人3D视觉定位任务的多样语义地图构建方法，利用大语言模型捕捉场景中物体潜在语义属性和关系，通过几何滑动窗口地图构建策略创建多样语义地图（DSM），并基于此提出DSM-Grounding方法。效果：在语义分割和3D视觉定位等任务上优于现有方法，在导航和抓取任务中也验证了有效性。
            arXiv:2504.08307v1 Announce Type: new 
Abstract: In recent years, with the growing research and application of multimodal large language models (VLMs) in robotics, there has been an increasing trend of utilizing VLMs for robotic scene understanding tasks. Existing approaches that use VLMs for 3D Visual Grounding tasks often focus on obtaining scene information through geometric and visual information, overlooking the extraction of diverse semantic information from the scene and the understanding of rich implicit semantic attributes, such as appearance, physics, and affordance. The 3D scene graph, which combines geometry and language, is an ideal representation method for environmental perception and is an effective carrier for language models in 3D Visual Grounding tasks. To address these issues, we propose a diverse semantic map construction method specifically designed for robotic agents performing 3D Visual Grounding tasks. This method leverages VLMs to capture the latent semantic attributes and relations of objects within the scene and creates a Diverse Semantic Map (DSM) through a geometry sliding-window map construction strategy. We enhance the understanding of grounding information based on DSM and introduce a novel approach named DSM-Grounding. Experimental results show that our method outperforms current approaches in tasks like semantic segmentation and 3D Visual Grounding, particularly excelling in overall metrics compared to the state-of-the-art. In addition, we have deployed this method on robots to validate its effectiveness in navigation and grasping tasks.
        ]]></description>
    </item>
    <item>
        <title>A Systematic Evaluation of Knowledge Graph Embeddings for Gene-Disease Association Prediction</title>
        <link>https://arxiv.org/abs/2504.08445</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08445v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Catarina Canastra, C\'atia Pesquita</dc:creator>
        <description><![CDATA[
            背景：发现基因 - 疾病关联在生物和医学领域很重要，现有机器学习方法在利用知识图谱结构时，常忽略疾病本体的因果和语义关系。方法：提出比较链接预测和节点对分类任务性能的框架，包含数据分割、知识图谱整合、嵌入、建模预测和方法评估五步，还评估了疾病本体语义丰富度和本体间额外链接的影响。效果：丰富疾病语义表示可稍提升性能，额外链接影响更大；链接预测方法能更好挖掘知识图谱语义，虽节点对分类能识别所有真阳性，但链接预测总体表现更优。
            arXiv:2504.08445v1 Announce Type: new 
Abstract: Discovery gene-disease links is important in biology and medicine areas, enabling disease identification and drug repurposing. Machine learning approaches accelerate this process by leveraging biological knowledge represented in ontologies and the structure of knowledge graphs. Still, many existing works overlook ontologies explicitly representing diseases, missing causal and semantic relationships between them. The gene-disease association problem naturally frames itself as a link prediction task, where embedding algorithms directly predict associations by exploring the structure and properties of the knowledge graph. Some works frame it as a node-pair classification task, combining embedding algorithms with traditional machine learning algorithms. This strategy aligns with the logic of a machine learning pipeline. However, the use of negative examples and the lack of validated gene-disease associations to train embedding models may constrain its effectiveness. This work introduces a novel framework for comparing the performance of link prediction versus node-pair classification tasks, analyses the performance of state of the art gene-disease association approaches, and compares the different order-based formalizations of gene-disease association prediction. It also evaluates the impact of the semantic richness through a disease-specific ontology and additional links between ontologies. The framework involves five steps: data splitting, knowledge graph integration, embedding, modeling and prediction, and method evaluation. Results show that enriching the semantic representation of diseases slightly improves performance, while additional links generate a greater impact. Link prediction methods better explore the semantic richness encoded in knowledge graphs. Although node-pair classification methods identify all true positives, link prediction methods outperform overall.
        ]]></description>
    </item>
    <item>
        <title>LGRPool: Hierarchical Graph Pooling Via Local-Global Regularisation</title>
        <link>https://arxiv.org/abs/2504.08530</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08530v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Farshad Noravesh, Reza Haffari, Layki Soon, Arghya Pal</dc:creator>
        <description><![CDATA[
            背景：传统图神经网络缺乏多尺度分析能力，多数分层图池化（HGP）方法未考虑图的全局拓扑结构，且未对齐局部和全局特征。方法：提出LGRPool，它是机器学习期望最大化框架下的HGP方法，使用正则化器使消息传递的局部和全局方面相互对齐，让全局拓扑信息在HGP不同层的表示中与不同尺度的局部消息传递一致。效果：在一些图分类基准测试中，略优于部分基线模型。
            arXiv:2504.08530v1 Announce Type: new 
Abstract: Hierarchical graph pooling(HGP) are designed to consider the fact that conventional graph neural networks(GNN) are inherently flat and are also not multiscale. However, most HGP methods suffer not only from lack of considering global topology of the graph and focusing on the feature learning aspect, but also they do not align local and global features since graphs should inherently be analyzed in a multiscale way. LGRPool is proposed in the present paper as a HGP in the framework of expectation maximization in machine learning that aligns local and global aspects of message passing with each other using a regularizer to force the global topological information to be inline with the local message passing at different scales through the representations at different layers of HGP. Experimental results on some graph classification benchmarks show that it slightly outperforms some baselines.
        ]]></description>
    </item>
    <item>
        <title>Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning</title>
        <link>https://arxiv.org/abs/2504.08672</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08672v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, Zhiyong Wu</dc:creator>
        <description><![CDATA[
            提升大语言模型推理能力备受关注，但现有后训练技术依赖监督信号，存在可扩展性和高标注成本问题。为此，论文提出无监督自训练框架Genius。该框架通过逐步寻找最优响应序列优化大语言模型，引入逐步前瞻重采样策略探索潜在步骤，提出优势校准优化损失函数减轻估计不一致性。Genius为无监督提升大语言模型推理能力迈出重要一步，代码将开源。
            arXiv:2504.08672v1 Announce Type: new 
Abstract: Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.
        ]]></description>
    </item>
    <item>
        <title>Fast-Slow-Thinking: Complex Task Solving with Large Language Models</title>
        <link>https://arxiv.org/abs/2504.08690</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08690v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yiliu Sun, Yanfang Zhang, Zicheng Zhao, Sheng Wan, Dacheng Tao, Chen Gong</dc:creator>
        <description><![CDATA[
            背景：大语言模型用于解决复杂任务时，现有任务分解方法在任务逻辑和约束复杂时效果欠佳。方法：受人类快慢思考系统启发，提出“Fast - Slow - Thinking”（FST）任务分解法，快思考简化任务，去除约束；慢思考召回约束，完善答案，让模型从粗到细解决问题。效果：在三类任务实验中证明了该方法的有效性。
            arXiv:2504.08690v1 Announce Type: new 
Abstract: Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and then solve them separately so that the difficulty of the original task can be reduced. However, the performance of existing task decomposition methods can be suboptimal when the task contains overly complex logic and constraints. In this situation, the solution generated by LLMs may deviate from the original purpose of the task, or contain redundant or even erroneous content. Therefore, inspired by the fact that humans possess two thinking systems including fast thinking and slow thinking, this paper introduces a new task decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates LLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow Thinking (ST) steps. Here FT focuses more on the general and concise aspect of the task, and ST focuses more on the details of the task. In FT, LLMs are prompted to remove the constraints of the original task, therefore simplifying it to a general and concise one. In ST, we recall the constraints removed in FT, so that LLMs can improve the answer generated in FT to meet the requirements of the original task. Therefore, our FST method enables LLMs to consider a complex problem via a human-like cognition process from coarse to fine, the effectiveness of which has been well demonstrated by the experiments on three types of tasks.
        ]]></description>
    </item>
    <item>
        <title>SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language Modeling</title>
        <link>https://arxiv.org/abs/2504.08719</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08719v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Krishna C. Puvvada, Faisal Ladhak, Santiago Akle Serrano, Cheng-Ping Hsieh, Shantanu Acharya, Somshubra Majumdar, Fei Jia, Samuel Kriman, Simeng Sun, Dima Rekesh, Boris Ginsburg</dc:creator>
        <description><![CDATA[
            背景：现有模型在处理远超训练长度的序列时泛化能力不足。方法：提出仅解码器的Transformer架构SWAN - GPT，交错无位置编码层和带旋转位置编码的滑动窗口注意力层，推理时对注意力分数进行动态缩放。效果：无需额外长上下文训练，在远超训练长度的序列上表现出色，比标准GPT架构计算效率更高，能降低训练成本、提高吞吐量，还可将现有预训练模型高效转换为SWAN架构以处理更长上下文。
            arXiv:2504.08719v1 Announce Type: new 
Abstract: We present a decoder-only Transformer architecture that robustly generalizes to sequence lengths substantially longer than those seen during training. Our model, SWAN-GPT, interleaves layers without positional encodings (NoPE) and sliding-window attention layers equipped with rotary positional encodings (SWA-RoPE). Experiments demonstrate strong performance on sequence lengths significantly longer than the training length without the need for additional long-context training. This robust length extrapolation is achieved through our novel architecture, enhanced by a straightforward dynamic scaling of attention scores during inference. In addition, SWAN-GPT is more computationally efficient than standard GPT architectures, resulting in cheaper training and higher throughput. Further, we demonstrate that existing pre-trained decoder-only models can be efficiently converted to the SWAN architecture with minimal continued training, enabling longer contexts. Overall, our work presents an effective approach for scaling language models to longer contexts in a robust and efficient manner.
        ]]></description>
    </item>
    <item>
        <title>Local Distance-Preserving Node Embeddings and Their Performance on Random Graphs</title>
        <link>https://arxiv.org/abs/2504.08216</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08216v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>My Le, Luana Ruiz, Souvik Dhara</dc:creator>
        <description><![CDATA[
            背景：图机器学习中学习节点表示时，现有嵌入方法难以捕捉图距离等全局信息。方法：受Bourgain工作启发，研究局部距离保留节点嵌入，采用基于地标算法，通过计算少量参考节点（地标）的最短路径近似成对距离。效果：理论上证明，与最坏情况图相比，随机图在基于地标的嵌入中所需维度更低；实验表明，基于GNN的地标距离近似方法能很好推广到更大网络，为图表示学习提供可扩展方案。
            arXiv:2504.08216v1 Announce Type: cross 
Abstract: Learning node representations is a fundamental problem in graph machine learning. While existing embedding methods effectively preserve local similarity measures, they often fail to capture global functions like graph distances. Inspired by Bourgain's seminal work on Hilbert space embeddings of metric spaces (1985), we study the performance of local distance-preserving node embeddings. Known as landmark-based algorithms, these embeddings approximate pairwise distances by computing shortest paths from a small subset of reference nodes (i.e., landmarks). Our main theoretical contribution shows that random graphs, such as Erd\H{o}s-R\'enyi random graphs, require lower dimensions in landmark-based embeddings compared to worst-case graphs. Empirically, we demonstrate that the GNN-based approximations for the distances to landmarks generalize well to larger networks, offering a scalable alternative for graph representation learning.
        ]]></description>
    </item>
    <item>
        <title>MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models</title>
        <link>https://arxiv.org/abs/2504.08329</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08329v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junmo Kim, Namkyeong Lee, Jiwon Kim, Kwangsoo Kim</dc:creator>
        <description><![CDATA[
            背景：电子健康记录（EHR）基础模型虽在医疗任务中表现提升，但存在处理未登录医学代码的局限，限制了模型通用性与不同词汇表训练模型的集成。方法：提出基于OMOP通用数据模型的MedRep，用大语言模型提示丰富概念信息，通过图本体增强文本表示，还采用轨迹增强策略让模型处理未登录概念。效果：经MedRep训练的EHR基础模型在外部数据集上能更好维持预测性能，代码公开。
            arXiv:2504.08329v1 Announce Type: cross 
Abstract: Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https://github.com/kicarussays/MedRep.
        ]]></description>
    </item>
    <item>
        <title>Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks</title>
        <link>https://arxiv.org/abs/2504.08525</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08525v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ye Ye</dc:creator>
        <description><![CDATA[
            背景：大语言模型用于多步任务时，现有框架难以对任务状态进行结构化理解，导致性能不稳定、易产生幻觉和缺乏长程连贯性。方法：提出任务记忆引擎（TME），用分层任务记忆树（TMT）跟踪任务执行，树中节点对应任务步骤并存储相关信息，还引入基于活跃节点路径动态生成提示的方法。效果：通过案例和对比实验表明，TME提升了任务完成准确率，行为更具可解释性，且实现开销小。
            arXiv:2504.08525v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at https://github.com/biubiutomato/TME-Agent.
        ]]></description>
    </item>
    <item>
        <title>Transformer Learns Optimal Variable Selection in Group-Sparse Classification</title>
        <link>https://arxiv.org/abs/2504.08638</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08638v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chenyang Zhang, Xuran Meng, Yuan Cao</dc:creator>
        <description><![CDATA[
            背景：Transformer在各领域应用成功，但理论理解不足。方法：以“组稀疏”经典统计模型为例，研究Transformer训练，证明单层Transformer经梯度下降训练可利用注意力机制选变量，忽略无关变量、聚焦分类有益变量，且预训练好的单层Transformer可适配新下游任务。效果：在有限样本下能实现良好预测精度，该研究为Transformer有效学习结构化数据提供了理论依据。
            arXiv:2504.08638v1 Announce Type: cross 
Abstract: Transformers have demonstrated remarkable success across various applications. However, the success of transformers have not been understood in theory. In this work, we give a case study of how transformers can be trained to learn a classic statistical model with "group sparsity", where the input variables form multiple groups, and the label only depends on the variables from one of the groups. We theoretically demonstrate that, a one-layer transformer trained by gradient descent can correctly leverage the attention mechanism to select variables, disregarding irrelevant ones and focusing on those beneficial for classification. We also demonstrate that a well-pretrained one-layer transformer can be adapted to new downstream tasks to achieve good prediction accuracy with a limited number of samples. Our study sheds light on how transformers effectively learn structured data.
        ]]></description>
    </item>
    <item>
        <title>UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition</title>
        <link>https://arxiv.org/abs/2307.11170</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2307.11170v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Aidan Mannion, Thierry Chevalier, Didier Schwab, Lorraine Geouriot</dc:creator>
        <description><![CDATA[
            背景：预训练的Transformer语言模型在应用NLP中占主导，生物医学领域需整合特定领域知识和语言统计建模。方法：提出以数据为中心的范式，从UMLS中提取文本序列丰富生物医学Transformer - 编码器语言模型的语言表示，将基于图的学习目标与掩码语言预训练相结合。效果：初步实验结果显示，该框架在多个生物医学和临床命名实体识别任务上提升了下游性能。
            arXiv:2307.11170v2 Announce Type: replace 
Abstract: Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks.
        ]]></description>
    </item>
    <item>
        <title>IFShip: Interpretable Fine-grained Ship Classification with Domain Knowledge-Enhanced Vision-Language Models</title>
        <link>https://arxiv.org/abs/2408.06631</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2408.06631v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mingning Guo, Mengwei Wu, Yuxiang Shen, Haifeng Li, Chao Tao</dc:creator>
        <description><![CDATA[
            背景：端到端解释主导遥感细粒度船舶分类任务，但推理过程不可解释。方法：提出领域知识增强的思维链提示生成机制，半自动化构建特定任务指令数据集TITANIC - FGS，训练通用视觉语言模型得到IFShip，并开发FGSC视觉聊天机器人，将分类问题转为逐步推理任务。效果：IFShip在可解释性和分类准确率上超越现有算法，相比LLaVA和MiniGPT - 4等模型表现更优，能提供推理链和可解释说明。
            arXiv:2408.06631v3 Announce Type: replace 
Abstract: End-to-end interpretation currently dominates the remote sensing fine-grained ship classification (RS-FGSC) task. However, the inference process remains uninterpretable, leading to criticisms of these models as "black box" systems. To address this issue, we propose a domain knowledge-enhanced Chain-of-Thought (CoT) prompt generation mechanism, which is used to semi-automatically construct a task-specific instruction-following dataset, TITANIC-FGS. By training on TITANIC-FGS, we adapt general-domain vision-language models (VLMs) to the FGSC task, resulting in a model named IFShip. Building upon IFShip, we develop an FGSC visual chatbot that redefines the FGSC problem as a step-by-step reasoning task and conveys the reasoning process in natural language. Experimental results show that IFShip outperforms state-of-the-art FGSC algorithms in both interpretability and classification accuracy. Furthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates superior performance on the FGSC task. It provides an accurate chain of reasoning when fine-grained ship types are recognizable to the human eye and offers interpretable explanations when they are not.
        ]]></description>
    </item>
    <item>
        <title>A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions</title>
        <link>https://arxiv.org/abs/2412.08864</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.08864v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing, Shancheng Fang, Zhineng Chen, Hongtao Xie, Yongdong Zhang</dc:creator>
        <description><![CDATA[
            背景：合成高质量推理数据提升大语言模型性能有效，但以往合成方法扩量难、成本高。方法：提出基于图的合成数据管道（GSDP），受知识图谱启发，从种子数据提取知识点构建关系图探索关联。效果：实现255倍数据扩展，成本降低100倍且合成质量与GPT - 4 - 0613相当；构建含超191万对数学问答的GSDP - MATH数据集，基于Mistral - 7B的GSDP - 7B在MATH和GSM8K上准确率分别达37.7%和78.4%。
            arXiv:2412.08864v3 Announce Type: replace 
Abstract: Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\times$100 lower costs. To tackle the most challenging mathematical reasoning task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on Mistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating the effectiveness of our method. The dataset and models will be released at https://github.com/Jayce1kk/GSDP.
        ]]></description>
    </item>
    <item>
        <title>Tokenphormer: Structure-aware Multi-token Graph Transformer for Node Classification</title>
        <link>https://arxiv.org/abs/2412.15302</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.15302v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zijie Zhou, Zhaoqi Lu, Xuekai Wei, Rongqin Chen, Shenghui Zhang, Pak Lon Ip, Leong Hou U</dc:creator>
        <description><![CDATA[
            背景：传统图神经网络在消息传递中有过平滑和过挤压问题，图Transformer虽有全局感受野，但有无关节点噪声和结构信息丢失问题。方法：受自然语言处理中基于细粒度标记的表示学习启发，提出结构感知多标记图Transformer（Tokenphormer），生成多种标记，包括混合游走生成的walk - token、自监督图预训练模型得到的SGPM - token和hop - token，共同输入Transformer学习节点表示。效果：在节点分类任务上达到了最先进水平。
            arXiv:2412.15302v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) are widely used in graph data mining tasks. Traditional GNNs follow a message passing scheme that can effectively utilize local and structural information. However, the phenomena of over-smoothing and over-squashing limit the receptive field in message passing processes. Graph Transformers were introduced to address these issues, achieving a global receptive field but suffering from the noise of irrelevant nodes and loss of structural information. Therefore, drawing inspiration from fine-grained token-based representation learning in Natural Language Processing (NLP), we propose the Structure-aware Multi-token Graph Transformer (Tokenphormer), which generates multiple tokens to effectively capture local and structural information and explore global information at different levels of granularity. Specifically, we first introduce the walk-token generated by mixed walks consisting of four walk types to explore the graph and capture structure and contextual information flexibly. To ensure local and global information coverage, we also introduce the SGPM-token (obtained through the Self-supervised Graph Pre-train Model, SGPM) and the hop-token, extending the length and density limit of the walk-token, respectively. Finally, these expressive tokens are fed into the Transformer model to learn node representations collaboratively. Experimental results demonstrate that the capability of the proposed Tokenphormer can achieve state-of-the-art performance on node classification tasks.
        ]]></description>
    </item>
    <item>
        <title>Fine-Grained Retrieval-Augmented Generation for Visual Question Answering</title>
        <link>https://arxiv.org/abs/2502.20964</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.20964v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang</dc:creator>
        <description><![CDATA[
            背景：视觉问答（VQA）中前沿多模态大模型虽表现强，但难获取特定或最新知识，传统单模态检索会丢失视觉细节。方法：提出细粒度知识单元，将文本片段与实体图像融合存于向量数据库，引入知识单元检索增强生成框架（KU - RAG），结合细粒度检索与多模态大模型，通过知识校正链确保精准检索和提升推理能力。效果：显著提升领先的基于知识库的VQA方法性能，平均提升约3%，最佳情况达11%。
            arXiv:2502.20964v2 Announce Type: replace 
Abstract: Visual Question Answering (VQA) focuses on providing answers to natural language questions by utilizing information from images. Although cutting-edge multimodal large language models (MLLMs) such as GPT-4o achieve strong performance on VQA tasks, they frequently fall short in accessing domain-specific or the latest knowledge. To mitigate this issue, retrieval-augmented generation (RAG) leveraging external knowledge bases (KBs), referred to as KB-VQA, emerges as a promising approach. Nevertheless, conventional unimodal retrieval techniques, which translate images into textual descriptions, often result in the loss of critical visual details. This study presents fine-grained knowledge units, which merge textual snippets with entity images stored in vector databases. Furthermore, we introduce a knowledge unit retrieval-augmented generation framework (KU-RAG) that integrates fine-grained retrieval with MLLMs. The proposed KU-RAG framework ensures precise retrieval of relevant knowledge and enhances reasoning capabilities through a knowledge correction chain. Experimental findings demonstrate that our approach significantly boosts the performance of leading KB-VQA methods, achieving an average improvement of approximately 3% and up to 11% in the best case.
        ]]></description>
    </item>
    <item>
        <title>Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation</title>
        <link>https://arxiv.org/abs/2503.13947</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.13947v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sayak Nag, Udita Ghosh, Calvin-Khang Ta, Sarosij Bose, Jiachen Li, Amit K Roy Chowdhury</dc:creator>
        <description><![CDATA[
            场景图生成（SGG）旨在通过识别对象及其关系来表征视觉场景，但长尾分布和预测可变性等问题需进行不确定性量化。本文提出基于共形预测（CP）的框架，可适配现有SGG方法，通过在生成的场景图上构建校准良好的预测集来量化预测不确定性，保证统计上的覆盖度。还设计基于多模态大模型（MLLM）的后处理策略，选择最具视觉和语义合理性的场景图。该方法能生成多样场景图，评估SGG方法可靠性，提升整体性能。
            arXiv:2503.13947v2 Announce Type: replace 
Abstract: Scene Graph Generation (SGG) aims to represent visual scenes by identifying objects and their pairwise relationships, providing a structured understanding of image content. However, inherent challenges like long-tailed class distributions and prediction variability necessitate uncertainty quantification in SGG for its practical viability. In this paper, we introduce a novel Conformal Prediction (CP) based framework, adaptive to any existing SGG method, for quantifying their predictive uncertainty by constructing well-calibrated prediction sets over their generated scene graphs. These scene graph prediction sets are designed to achieve statistically rigorous coverage guarantees. Additionally, to ensure these prediction sets contain the most practically interpretable scene graphs, we design an effective MLLM-based post-processing strategy for selecting the most visually and semantically plausible scene graphs within these prediction sets. We show that our proposed approach can produce diverse possible scene graphs from an image, assess the reliability of SGG methods, and improve overall SGG performance.
        ]]></description>
    </item>
    <item>
        <title>Using LLMs for Analyzing AIS Data</title>
        <link>https://arxiv.org/abs/2504.07557</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.07557v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Gaspard Merten, Gilles Dejaegere, Mahmoud Sakr</dc:creator>
        <description><![CDATA[
            背景：大语言模型在各领域产生深远影响，包括移动性数据科学。方法：本文探索用大语言模型分析AIS数据，提出精心设计的查询评估模型在此类任务中的推理能力，实验四种方法，即把大语言模型作为空间数据库自然语言接口、对原始数据推理、对压缩轨迹推理、对语义轨迹推理，并研究其优缺点。效果：旨在为研究者和从业者依据数据分析目标选择合适的基于大语言模型的方法提供有价值的见解。
            arXiv:2504.07557v2 Announce Type: replace 
Abstract: Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks. Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories. We investigate the strengths and weaknesses for the four methods, and discuss the findings. The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives.
        ]]></description>
    </item>
    <item>
        <title>Scaling Laws for Native Multimodal Models</title>
        <link>https://arxiv.org/abs/2504.07951</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.07951v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, Alaaeldin El-Nouby</dc:creator>
        <description><![CDATA[
            构建能通过多模态信号有效感知世界的通用模型是长期目标。当前方法多是集成预训练组件，但晚期融合架构是否固有优势尚不明确。该研究重新审视原生多模态模型架构设计，对457个不同架构和训练混合的模型开展规模定律研究。结果表明，早期融合架构不依赖图像编码器，相比晚期融合架构无固有劣势，在低参数时性能更强、训练更高效、部署更简单。融入专家混合体可使模型学习特定模态权重，显著提升性能。
            arXiv:2504.07951v2 Announce Type: replace 
Abstract: Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.
        ]]></description>
    </item>
    <item>
        <title>DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?</title>
        <link>https://arxiv.org/abs/2409.07703</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.07703v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu</dc:creator>
        <description><![CDATA[
            背景：现有数据科学基准与现实应用存在差距。方法：提出DSBench综合基准，含466个数据分析任务和74个数据建模任务，源于Eloquence和Kaggle竞赛，有长上下文、多模态背景等现实设定。效果：评估显示，最先进的大语言模型、大视觉语言模型和智能体在多数任务上表现不佳，最佳智能体仅解决34.12%的数据分析任务，相对性能差距为34.74%，凸显开发实用智能自主数据科学智能体的必要性。
            arXiv:2409.07703v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.
        ]]></description>
    </item>
    <item>
        <title>Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces</title>
        <link>https://arxiv.org/abs/2410.09918</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.09918v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, Qinqing Zheng</dc:creator>
        <description><![CDATA[
            背景：人类思维分快速直觉的系统1和慢速审慎的系统2，将系统2融入大语言模型可提升推理能力，但纯系统2思维模型计算成本高、响应慢。方法：提出Dualformer，通过在随机推理轨迹数据上训练，训练时丢弃轨迹不同部分，推理时可配置不同模式。效果：在迷宫导航任务中，慢模式下最优解决率97.6%，超基线且推理步骤少45.5%；快模式下最优率80%，远超对比模型。数学问题微调LLM也有性能提升。
            arXiv:2410.09918v2 Announce Type: replace-cross 
Abstract: In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities. Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present Dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. Dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (fast mode) or both the reasoning chain and the final solution (slow mode), or automatically decide which mode to engage (auto mode). In all cases, Dualformer outperforms the corresponding baseline models in both performance and computational efficiency: (1) in slow mode, Dualformer optimally solves unseen 30 x 30 maze navigation tasks 97.6% of the time, surpassing the Searchformer (trained on data with complete reasoning traces) baseline performance of 93.3%, while only using 45.5% fewer reasoning steps; (2) in fast mode, Dualformer completes those tasks with an 80% optimal rate, significantly outperforming the Solution-Only model (trained on solution-only data), which has an optimal rate of only 30%. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models.
        ]]></description>
    </item>
    <item>
        <title>EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios</title>
        <link>https://arxiv.org/abs/2412.04447</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.04447v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, Xihui Liu</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型虽有较强理解和推理能力，但在不同场景下的规划能力研究不足。方法：本文推出 EgoPlan - Bench2 基准，涵盖 4 大领域 24 个日常场景，通过半自动化结合人工验证构建。评估 21 个模型并分析局限，还提出无训练的多模态思维链提示方法。效果：该方法使 GPT - 4V 在 EgoPlan - Bench2 上性能提升 10.24，为模型规划能力提升提供了方向。
            arXiv:2412.04447v2 Announce Type: replace-cross 
Abstract: The advent of Multimodal Large Language Models, leveraging the power of Large Language Models, has recently demonstrated superior multimodal understanding and reasoning abilities, heralding a new era for artificial general intelligence. However, achieving AGI necessitates more than just comprehension and reasoning. A crucial capability required is effective planning in diverse scenarios, which involves making reasonable decisions based on complex environments to solve real-world problems. Despite its importance, the planning abilities of current MLLMs in varied scenarios remain underexplored. In this paper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark designed to assess the planning capabilities of MLLMs across a wide range of real-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4 major domains and 24 detailed scenarios, closely aligned with human daily life. EgoPlan-Bench2 is constructed through a semi-automatic process utilizing egocentric videos, complemented by manual verification. Grounded in a first-person perspective, it mirrors the way humans approach problem-solving in everyday life. We evaluate 21 competitive MLLMs and provide an in-depth analysis of their limitations, revealing that they face significant challenges in real-world planning. To further improve the planning proficiency of current MLLMs, we propose a training-free approach using multimodal Chain-of-Thought (CoT) prompting through investigating the effectiveness of various multimodal prompts in complex planning. Our approach enhances the performance of GPT-4V by 10.24 on EgoPlan-Bench2 without additional training. Our work not only sheds light on the current limitations of MLLMs in planning, but also provides insights for future enhancements in this critical area. We have made data and code available at https://qiulu66.github.io/egoplanbench2/.
        ]]></description>
    </item>
    <item>
        <title>ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis</title>
        <link>https://arxiv.org/abs/2504.06553</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.06553v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yun Chang, Leonor Fermoselle, Duy Ta, Bernadette Bucher, Luca Carlone, Jiuguang Wang</dc:creator>
        <description><![CDATA[
            背景：当前将自然语言与3D环境关联已有进展，但把抽象高层指令关联到3D场景仍具挑战，高层任务分解为具体子任务也依赖环境。方法：提出ASHiTA框架，通过将高层任务分解为关联子任务，生成基于3D场景图的任务层次结构，交替进行大语言模型辅助的层次任务分析和任务驱动的3D场景图构建。效果：在将高层任务分解为依赖环境的子任务方面，显著优于大语言模型基线，关联性能与现有最优方法相当。
            arXiv:2504.06553v3 Announce Type: replace-cross 
Abstract: While recent work in scene reconstruction and understanding has made strides in grounding natural language to physical 3D environments, it is still challenging to ground abstract, high-level instructions to a 3D scene. High-level instructions might not explicitly invoke semantic elements in the scene, and even the process of breaking a high-level task into a set of more concrete subtasks, a process called hierarchical task analysis, is environment-dependent. In this work, we propose ASHiTA, the first framework that generates a task hierarchy grounded to a 3D scene graph by breaking down high-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted hierarchical task analysis, to generate the task breakdown, with task-driven 3D scene graph construction to generate a suitable representation of the environment. Our experiments show that ASHiTA performs significantly better than LLM baselines in breaking down high-level tasks into environment-dependent subtasks and is additionally able to achieve grounding performance comparable to state-of-the-art methods.
        ]]></description>
    </item>
    <item>
        <title>Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation</title>
        <link>https://arxiv.org/abs/2504.08274</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08274v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haowei Lou, Hye-young Paik, Sheng Li, Wen Hu, Lina Yao</dc:creator>
        <description><![CDATA[
            背景：多语言文本转语音（TTS）因不同语言音素词汇、韵律和说话风格差异而面临挑战，现有方法存在资源消耗大或难以捕捉语言风格差异问题。方法：提出非自回归、语言感知风格自适应TTS框架LanStyleTTS，标准化音素表示，实现跨语言细粒度音素级风格控制。效果：与多种先进非自回归TTS架构集成，不同模型骨干性能均有提升；使用潜在编码可显著减小模型规模和计算成本，同时保持高质量语音生成。
            arXiv:2504.08274v1 Announce Type: new 
Abstract: Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation.
        ]]></description>
    </item>
    <item>
        <title>Location-Oriented Sound Event Localization and Detection with Spatial Mapping and Regression Localization</title>
        <link>https://arxiv.org/abs/2504.08365</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08365v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xueping Zhang, Yaxiong Chen, Ruilin Yao, Yunfei Zi, Shengwu Xiong</dc:creator>
        <description><![CDATA[
            背景：现有面向事件的多轨道方法用于声音事件定位与检测（SELD）时，因轨道数量限制，在多音环境通用性欠佳。方法：提出空间映射与回归定位的SELD方法（SMRL - SELD），将3D空间分割并映射到2D平面，提出新的回归定位损失帮助结果向对应事件位置收敛，该方法以位置为导向让模型基于方向学习事件特征。效果：在STARSS23和STARSS22数据集实验显示，SMRL - SELD在整体评估和多音环境中优于现有SELD方法。
            arXiv:2504.08365v1 Announce Type: new 
Abstract: Sound Event Localization and Detection (SELD) combines the Sound Event Detection (SED) with the corresponding Direction Of Arrival (DOA). Recently, adopted event oriented multi-track methods affect the generality in polyphonic environments due to the limitation of the number of tracks. To enhance the generality in polyphonic environments, we propose Spatial Mapping and Regression Localization for SELD (SMRL-SELD). SMRL-SELD segments the 3D spatial space, mapping it to a 2D plane, and a new regression localization loss is proposed to help the results converge toward the location of the corresponding event. SMRL-SELD is location-oriented, allowing the model to learn event features based on orientation. Thus, the method enables the model to process polyphonic sounds regardless of the number of overlapping events. We conducted experiments on STARSS23 and STARSS22 datasets and our proposed SMRL-SELD outperforms the existing SELD methods in overall evaluation and polyphony environments.
        ]]></description>
    </item>
    <item>
        <title>On the Design of Diffusion-based Neural Speech Codecs</title>
        <link>https://arxiv.org/abs/2504.08470</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08470v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Pietro Foti, Andreas Brendel</dc:creator>
        <description><![CDATA[
            背景：生成式神经语音编解码器（NSCs）在低比特率下表现优于传统编解码器，扩散模型（DMs）在图像生成中表现出色，在音频编码等领域有应用，但基于扩散的NSCs设计缺乏系统探索。方法：对基于扩散的NSCs进行全面分析，提出基于DM调节和输出域的分类，在概念框架内创建并评估新的NSCs，通过客观指标和主观听力测试进行比较。效果：研究为基于扩散的NSCs设计提供了系统方法和新的设计思路。
            arXiv:2504.08470v1 Announce Type: new 
Abstract: Recently, neural speech codecs (NSCs) trained as generative models have shown superior performance compared to conventional codecs at low bitrates. Although most state-of-the-art NSCs are trained as Generative Adversarial Networks (GANs), Diffusion Models (DMs), a recent class of generative models, represent a promising alternative due to their superior performance in image generation relative to GANs. Consequently, DMs have been successfully applied for audio and speech coding among various other audio generation applications. However, the design of diffusion-based NSCs has not yet been explored in a systematic way. We address this by providing a comprehensive analysis of diffusion-based NSCs divided into three contributions. First, we propose a categorization based on the conditioning and output domains of the DM. This simple conceptual framework allows us to define a design space for diffusion-based NSCs and to assign a category to existing approaches in the literature. Second, we systematically investigate unexplored designs by creating and evaluating new diffusion-based NSCs within the conceptual framework. Finally, we compare the proposed models to existing GAN and DM baselines through objective metrics and subjective listening tests.
        ]]></description>
    </item>
    <item>
        <title>Reverberation-based Features for Sound Event Localization and Detection with Distance Estimation</title>
        <link>https://arxiv.org/abs/2504.08644</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08644v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Davide Berghi, Philip J. B. Jackson</dc:creator>
        <description><![CDATA[
            背景：声音事件定位与检测（SELD）中定位子任务常忽略声源距离，近期扩展到3D但缺乏用于距离估计的输入特征。方法：提出两种基于混响的3D SELD特征格式，一种使用直达混响比（DRR），另一种利用信号自相关获取早期反射信息，还在合成数据上进行预训练。效果：预训练能改善相对距离误差（RDE）和整体SELD得分，基于自相关的特征使STARSS23数据集上的RDE降低超3个百分点。
            arXiv:2504.08644v1 Announce Type: new 
Abstract: Sound event localization and detection (SELD) involves predicting active sound event classes over time while estimating their positions. The localization subtask in SELD is usually treated as a direction of arrival estimation problem, ignoring source distance. Only recently, SELD was extended to 3D by incorporating distance estimation, enabling the prediction of sound event positions in 3D space (3D SELD). However, existing methods lack input features designed for distance estimation. We argue that reverberation encodes valuable information for this task. This paper introduces two novel feature formats for 3D SELD based on reverberation: one using direct-to-reverberant ratio (DRR) and another leveraging signal autocorrelation to provide the model with insights into early reflections. Pre-training on synthetic data improves relative distance error (RDE) and overall SELD score, with autocorrelation-based features reducing RDE by over 3 percentage points on the STARSS23 dataset. The code to extract the features is available at github.com/dberghi/SELD-distance-features.
        ]]></description>
    </item>
    <item>
        <title>FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications</title>
        <link>https://arxiv.org/abs/2409.03283</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.03283v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hao-Han Guo, Yao Hu, Kun Liu, Fei-Yu Shen, Xu Tang, Yi-Chen Wu, Feng-Long Xie, Kun Xie, Kai-Tuo Xu</dc:creator>
        <description><![CDATA[
            背景：为满足个性化、多样化生成式语音应用需求。方法：提出FireRedTTS基础文本到语音框架，含数据处理、基础系统和下游应用三部分。先构建高质量TTS数据集；再用基于语言模型的TTS系统，通过语义感知语音分词器将语音信号压缩成离散语义标记，由语言模型生成，并用两阶段波形生成器解码；最后给出配音语音克隆和聊天机器人类人语音生成两个应用。效果：该框架上下文学习能力强，能稳定合成高质量语音，配音时可零样本克隆声音，也能通过微调适配专业场景，还能实现可控类人语音生成。
            arXiv:2409.03283v2 Announce Type: replace 
Abstract: This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.
        ]]></description>
    </item>
</channel>
</rss>