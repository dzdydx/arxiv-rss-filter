<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 08 May 2025 12:16:51 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Thu, 08 May 2025 12:16:51 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>ALFRED: Ask a Large-language model For Reliable ECG Diagnosis</title>
        <link>https://arxiv.org/abs/2505.03781</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03781v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jin Yu, JaeHo Park, TaeJun Park, Gyurin Kim, JiHyun Lee, Min Sung Lee, Joon-myoung Kwon, Jeong Min Son, Yong-Yeon Jo</dc:creator>
        <description><![CDATA[
            背景：利用大语言模型和检索增强生成（RAG）分析医疗数据尤其是心电图（ECG）虽有高准确性和便利性，但在医疗等专业领域生成可靠、有依据的结果仍是挑战。方法：提出基于RAG的零样本ECG诊断框架，融入专家整理的知识以提高诊断准确性和可解释性。效果：在PTB - XL数据集上的评估证明了框架的有效性，体现了结构化领域专业知识在自动ECG解读中的价值，还能满足多样诊断需求。 
            arXiv:2505.03781v1 Announce Type: new 
Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers high accuracy and convenience. However, generating reliable, evidence-based results in specialized fields like healthcare remains a challenge, as RAG alone may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG for ECG analysis that incorporates expert-curated knowledge to enhance diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset demonstrates the framework's effectiveness, highlighting the value of structured domain expertise in automated ECG interpretation. Our framework is designed to support comprehensive ECG analysis, addressing diverse diagnostic needs with potential applications beyond the tested dataset.
        ]]></description>
    </item>
    <item>
        <title>When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator</title>
        <link>https://arxiv.org/abs/2505.03786</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03786v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Md Fahim Anjum</dc:creator>
        <description><![CDATA[
            背景：具备推理能力的大语言模型在规划框架的候选评估方面有潜力，但与传统非推理模型的性能对比研究较少。方法：在文本到SQL任务的生成 - 判别式大语言模型规划框架中，将15亿参数的推理模型DeepSeek - R1与多个非推理大模型进行基准测试，提出从思维链输出中提取软分数以对候选者进行细粒度排序。效果：尽管参数少，DeepSeek - R1 - 1.5B比CodeLlama - 7B的F1值高87%、判别准确率高3.7%，比CodeLlama - 13B的执行准确率高3.7%。
            arXiv:2505.03786v1 Announce Type: new 
Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising path for improving candidate evaluation in planning frameworks, but their relative performance against traditional non-reasoning models remains largely underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our central hypothesis is that reasoning models are more effective discriminators than non-reasoning LLMs. Our results show that distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution accuracy than CodeLlama-13B, despite having significantly fewer parameters. Furthermore, we find that there is a limit to the logical capabilities of reasoning models, and only providing more context or allowing more compute budget for reasoning is not enough to improve their discrimination performance. Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find generation more challenging than discrimination and may underperform as generators compared to smaller non-reasoning LLMs. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighing their capabilities as generators, offering insights into their optimal role within LLM planning infrastructures.
        ]]></description>
    </item>
    <item>
        <title>Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding</title>
        <link>https://arxiv.org/abs/2505.03788</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03788v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Trilok Padhi, Ramneet Kaur, Adam D. Cobb, Manoj Acharya, Anirban Roy, Colin Samplawski, Brian Matejek, Alexander M. Berenbeim, Nathaniel D. Bastian, Susmit Jha</dc:creator>
        <description><![CDATA[
            背景：现有多模态大语言模型不确定性量化（UQ）方法在模型持续答错时置信度高，与准确率校准不佳。方法：除自一致性外，利用跨模态一致性改进校准，将文本响应与视觉输入关联，用关联模型的置信度校准整体置信度，再用温度缩放技术校准关联模型置信度。效果：在医学问答和视觉问答等多模态任务上评估，结果表明该框架在两个任务上校准效果显著提升。
            arXiv:2505.03788v1 Announce Type: new 
Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ) tailored for multi-modal large language models (LLMs). Existing state-of-the-art UQ methods rely on consistency among multiple responses generated by the LLM on an input query under diverse settings. However, these approaches often report higher confidence in scenarios where the LLM is consistently incorrect. This leads to a poorly calibrated confidence with respect to accuracy. To address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. Specifically, we ground the textual responses to the visual inputs. The confidence from the grounding model is used to calibrate the overall confidence. Given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. We evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (Slake) and visual question answering (VQAv2), considering multi-modal models such as LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.
        ]]></description>
    </item>
    <item>
        <title>A Time-Series Data Augmentation Model through Diffusion and Transformer Integration</title>
        <link>https://arxiv.org/abs/2505.03790</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03790v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuren Zhang, Zhongnan Pu, Lei Jing</dc:creator>
        <description><![CDATA[
            背景：深度学习完成诸多现实任务需大量数据训练，数据增强在图像和语音领域发展较好，但时间序列数据增强关注较少。方法：提出结合扩散模型和Transformer模型的方法，用调整后的扩散去噪模型生成大量初始时间步动作数据，再用Transformer模型预测后续动作，结合加权损失函数实现收敛。效果：以应用增强数据后模型性能提升为基准，与无数据增强或传统方法对比，该方法能生成高质量增强数据。
            arXiv:2505.03790v1 Announce Type: new 
Abstract: With the development of Artificial Intelligence, numerous real-world tasks have been accomplished using technology integrated with deep learning. To achieve optimal performance, deep neural networks typically require large volumes of data for training. Although advances in data augmentation have facilitated the acquisition of vast datasets, most of this data is concentrated in domains like images and speech. However, there has been relatively less focus on augmenting time-series data. To address this gap and generate a substantial amount of time-series data, we propose a simple and effective method that combines the Diffusion and Transformer models. By utilizing an adjusted diffusion denoising model to generate a large volume of initial time-step action data, followed by employing a Transformer model to predict subsequent actions, and incorporating a weighted loss function to achieve convergence, the method demonstrates its effectiveness. Using the performance improvement of the model after applying augmented data as a benchmark, and comparing the results with those obtained without data augmentation or using traditional data augmentation methods, this approach shows its capability to produce high-quality augmented data.
        ]]></description>
    </item>
    <item>
        <title>Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling</title>
        <link>https://arxiv.org/abs/2505.03799</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03799v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hyun Lee, Chris Yi, Maminur Islam, B. D. S. Aritra</dc:creator>
        <description><![CDATA[
            背景：大语言模型在图相关问题应用受限，主要因可扩展性约束及缺乏处理图结构的专用机制，直接在大语言模型中编码图结构研究不足。方法：提出SDM - InstructGLM框架，引入基于相似度和度中心性的有偏随机游走机制，选择性采样和编码图信息。效果：显著提高了令牌效率，减少随机采样的信息损失，提升了节点分类和链接预测等图任务的性能，证明了仅用大语言模型进行图处理的可行性。
            arXiv:2505.03799v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.
        ]]></description>
    </item>
    <item>
        <title>GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation</title>
        <link>https://arxiv.org/abs/2505.03846</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03846v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kangsheng Wang, Yuhang Li, Chengwei Ye, Yufei Lin, Huanzhen Zhang, Bohan Hu, Linuo Xu, Shuyan Liu</dc:creator>
        <description><![CDATA[
            从短视频进行人格特质分析因多模态信息复杂交互存在挑战。本文提出GAME，一种图增强多模态编码器用于自动人格预测。方法上，视觉流构建面部图，结合图卷积网络与卷积神经网络及注意力机制捕捉面部结构与外观线索，还提取全局与身份特征；用带时间注意力模块的BiGRU处理帧级特征，音频和文本分别用VGGish和XLM - Roberta提取特征；用基于通道注意力的融合模块实现多模态融合，通过多层感知机回归预测人格特质。实验表明，GAME在多基准上优于现有方法。
            arXiv:2505.03846v1 Announce Type: new 
Abstract: Apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. In this paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to robustly model and fuse multi-source features for automatic personality prediction. For the visual stream, we construct a facial graph and introduce a dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks (GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to capture both structural and appearance-based facial cues. Complementing this, global context and iden-tity features are extracted using pretrained ResNet18 and VGGFace back-bones. To capture temporal dynamics, frame-level features are processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio representations are derived from the VGGish network, and linguistic se-mantics are captured via the XLM-Roberta transformer. To achieve effective multimodal integration, we propose a Channel Attention-based Fusion module, followed by a Multi-Layer Perceptron (MLP) regression head for predicting personality traits. Extensive experiments show that GAME con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.
        ]]></description>
    </item>
    <item>
        <title>A Reasoning-Focused Legal Retrieval Benchmark</title>
        <link>https://arxiv.org/abs/2505.03970</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03970v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lucia Zheng, Neel Guha, Javokhir Arifov, Sarah Zhang, Michal Skreta, Christopher D. Manning, Peter Henderson, Daniel E. Ho</dc:creator>
        <description><![CDATA[
            背景：法律界用大语言模型处理法律应用，开发者采用检索增强大语言模型（RAG）提升性能，但缺乏能体现法律检索和下游问答复杂性的真实法律RAG基准。方法：引入两个新的法律RAG基准，即律师考试问答和住房法规问答，任务对应现实法律研究任务，通过类似法律研究的标注流程产生。效果：结果表明法律RAG应用仍具挑战性，为后续研究提供动力。
            arXiv:2505.03970v1 Announce Type: new 
Abstract: As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs ("RAG" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.
        ]]></description>
    </item>
    <item>
        <title>X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains</title>
        <link>https://arxiv.org/abs/2505.03981</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03981v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, Hoifung Poon</dc:creator>
        <description><![CDATA[
            背景：现有开源研究多集中于文本推理模型，且评估局限于数学和通用领域，拓展推理能力至非文本输入和特定领域的方法尚不明确。方法：提出X - Reasoner，仅在通用领域文本上进行后训练以实现跨模态和领域的可泛化推理，采用两阶段方法，先监督微调，再强化学习。效果：X - Reasoner能将推理能力迁移到多模态和域外场景，在通用和医疗基准测试中优于现有模型，其医学变体X - Reasoner - Med在多项医学基准测试中达新的最优水平。
            arXiv:2505.03981v1 Announce Type: new 
Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.
        ]]></description>
    </item>
    <item>
        <title>SLOT: Structuring the Output of Large Language Models</title>
        <link>https://arxiv.org/abs/2505.04016</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04016v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Darren Yow-Bang Wang, Zhengyuan Shen, Soumya Smruti Mishra, Zhichao Xu, Yifei Teng, Haibo Ding</dc:creator>
        <description><![CDATA[
            背景：大语言模型在关键应用中输出结构化内容很重要，但常偏离预设模式，影响可靠应用开发。方法：提出SLOT，一种与模型无关的方法，用微调的轻量级语言模型作为后处理层，将非结构化输出转换为精确结构化格式，还引入数据整理和合成的系统流程及评估方法。效果：微调的Mistral - 7B模型实现近完美模式准确率（99.5%）和内容相似度（94.0%），大幅超越Claude - 3.5 - Sonnet；小模型配备SLOT后也能超越大的专有模型。 
            arXiv:2505.04016v1 Announce Type: new 
Abstract: Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.
        ]]></description>
    </item>
    <item>
        <title>AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding</title>
        <link>https://arxiv.org/abs/2505.04058</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04058v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Feng Xiao, Hongbin Xu, Guocan Zhao, Wenxiong Kang</dc:creator>
        <description><![CDATA[
            背景：3D视觉定位旨在根据自然语言在3D场景中定位目标，但3D与语言模态差距大，通过描述的空间关系区分相似物体是挑战。方法：提出2D辅助的3D视觉定位框架，构建含被指对象区分的语义 - 空间场景图以感知关系，采用双分支视觉编码器，用2D预训练属性引导多模态对象编码，跨模态交互模块用图注意力促进面向关系的信息融合。效果：在流行基准测试中表现优于现有方法，尤其在处理多个相似干扰项挑战上表现出色。
            arXiv:2505.04058v1 Announce Type: new 
Abstract: 3D visual grounding aims to localize the unique target described by natural languages in 3D scenes. The significant gap between 3D and language modalities makes it a notable challenge to distinguish multiple similar objects through the described spatial relationships. Current methods attempt to achieve cross-modal understanding in complex scenes via a target-centered learning mechanism, ignoring the perception of referred objects. We propose a novel 2D-assisted 3D visual grounding framework that constructs semantic-spatial scene graphs with referred object discrimination for relationship perception. The framework incorporates a dual-branch visual encoder that utilizes 2D pre-trained attributes to guide the multi-modal object encoding. Furthermore, our cross-modal interaction module uses graph attention to facilitate relationship-oriented information fusion. The enhanced object representation and iterative relational learning enable the model to establish effective alignment between 3D vision and referential descriptions. Experimental results on the popular benchmarks demonstrate our superior performance compared to state-of-the-art methods, especially in addressing the challenges of multiple similar distractors.
        ]]></description>
    </item>
    <item>
        <title>Alpha Excel Benchmark</title>
        <link>https://arxiv.org/abs/2505.04110</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04110v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>David Noever, Forrest McKee</dc:creator>
        <description><![CDATA[
            背景：缺乏针对大语言模型在实际业务任务中能力评估的标准化框架。方法：研究提出用金融建模世界杯（FMWC）Excel竞赛挑战评估大语言模型，将113个FMWC挑战转化为可程序评估的JSON格式，对比多个领先大语言模型的表现。效果：不同挑战类别下模型表现差异大，在模式识别任务有优势，复杂数值推理能力不足，该基准弥合了学术AI基准与实际业务应用的差距。
            arXiv:2505.04110v1 Announce Type: new 
Abstract: This study presents a novel benchmark for evaluating Large Language Models (LLMs) using challenges derived from the Financial Modeling World Cup (FMWC) Excel competitions. We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats and use this dataset to compare the performance of several leading LLMs. Our findings demonstrate significant variations in performance across different challenge categories, with models showing specific strengths in pattern recognition tasks but struggling with complex numerical reasoning. The benchmark provides a standardized framework for assessing LLM capabilities in realistic business-oriented tasks rather than abstract academic problems. This research contributes to the growing field of AI benchmarking by establishing proficiency among the 1.5 billion people who daily use Microsoft Excel as a meaningful evaluation metric that bridges the gap between academic AI benchmarks and practical business applications.
        ]]></description>
    </item>
    <item>
        <title>Vision Graph Prompting via Semantic Low-Rank Decomposition</title>
        <link>https://arxiv.org/abs/2505.04121</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04121v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zixiang Ai, Zichen Liu, Jiahuan Zhou</dc:creator>
        <description><![CDATA[
            背景：Vision GNN通过将图像表示为图结构表现出色，但现有视觉提示方法多针对Transformer模型，忽略图结构中节点和边的拓扑关系。方法：提出Vision Graph Prompting（VGP）框架，发现图中语义连接组件有低秩特性，引入语义低秩提示方法，分解低秩语义特征并与视觉图拓扑上的提示集成。效果：大量实验表明，该方法显著提升ViG在下游任务的迁移性能，效果与全量微调相当且参数高效。
            arXiv:2505.04121v1 Announce Type: new 
Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as graph structures, providing a more natural way to capture irregular semantic patterns beyond traditional grid or sequence-based representations. To efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning techniques like visual prompting become increasingly essential. However, existing prompting methods are primarily designed for Transformer-based models, neglecting the rich topological relationships among nodes and edges in graph-based representations, limiting their capacity to model complex semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel framework tailored for vision graph structures. Our core insight reveals that semantically connected components in the graph exhibit low-rank properties. Building on this observation, we introduce a semantic low-rank prompting method that decomposes low-rank semantic features and integrates them with prompts on vision graph topologies, capturing both global structural patterns and fine-grained semantic dependencies. Extensive experiments demonstrate our method significantly improves ViG's transfer performance on diverse downstream tasks, achieving results comparable to full fine-tuning while maintaining parameter efficiency. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.
        ]]></description>
    </item>
    <item>
        <title>Retrieval Augmented Time Series Forecasting</title>
        <link>https://arxiv.org/abs/2505.04163</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04163v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sungwon Han, Seungeon Lee, Meeyoung Cha, Sercan O Arik, Jinsung Yoon</dc:creator>
        <description><![CDATA[
            背景：时间序列预测利用历史数据预测未来趋势。方法：提出RAFT，一种检索增强的时间序列预测方法，在预测后续时间帧时，从训练数据集中检索与输入模式最相似的历史数据候选，利用这些候选的未来值和输入来获得预测，通过检索模块从外部提供过去模式信息增强模型能力。效果：在十个基准数据集上的评估显示，RAFT平均胜率达86%，始终优于现有基线模型。
            arXiv:2505.04163v1 Announce Type: new 
Abstract: Time series forecasting uses historical data to predict future trends, leveraging the relationships between past observations and available features. In this paper, we propose RAFT, a retrieval-augmented time series forecasting method to provide sufficient inductive biases and complement the model's learning capacity. When forecasting the subsequent time frames, we directly retrieve historical data candidates from the training dataset with patterns most similar to the input, and utilize the future values of these candidates alongside the inputs to obtain predictions. This simple approach augments the model's capacity by externally providing information about past patterns via retrieval modules. Our empirical evaluations on ten benchmark datasets show that RAFT consistently outperforms contemporary baselines with an average win ratio of 86%.
        ]]></description>
    </item>
    <item>
        <title>On-Device LLM for Context-Aware Wi-Fi Roaming</title>
        <link>https://arxiv.org/abs/2505.04174</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04174v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ju-Hyung Lee, Yanqing Lu</dc:creator>
        <description><![CDATA[
            背景：无线漫游在动态移动环境中保持无缝连接颇具挑战，传统方案常失效。方法：首次跨层使用设备端大语言模型，在应用层进行高层推理并在PHY/MAC层执行实时动作，处理上下文感知AP选择和动态阈值调整两项任务，还应用思维链提示、参数高效微调、量化等优化方法。效果：室内外数据集实验表明，该方法超越传统启发式和DRL基线，在漫游稳定性和信号质量间实现良好平衡。
            arXiv:2505.04174v1 Announce Type: new 
Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless connectivity in dynamic mobile environments. Conventional threshold-based or heuristic schemes often fail, leading to either sticky or excessive handovers. We introduce the first cross-layer use of an on-device large language model (LLM): high-level reasoning in the application layer that issues real-time actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i) context-aware AP selection, where structured prompts fuse environmental cues (e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold adjustment, where the model adaptively decides when to roam. To satisfy the tight latency and resource budgets of edge hardware, we apply a suite of optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and quantization. Experiments on indoor and outdoor datasets show that our approach surpasses legacy heuristics and DRL baselines, achieving a strong balance between roaming stability and signal quality. These findings underscore the promise of application-layer LLM reasoning for lower-layer wireless control in future edge systems.
        ]]></description>
    </item>
    <item>
        <title>VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning</title>
        <link>https://arxiv.org/abs/2505.04192</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04192v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Trinh T. L. Vuong, Jin Tae Kwak</dc:creator>
        <description><![CDATA[
            背景：计算病理学领域缺乏能集成多种图像场景模拟病理学家诊断过程的大模型，且高质量诊断推理数据创建耗时、数量有限。方法：提出VideoPath - LLaVA模型，集成三种图像场景，构建包含4278个视频与诊断思维链指令对的VideoPath - Instruct数据集，从单图像指令数据集迁移知识，先在弱标注关键帧剪辑上训练，再在手动分割视频上微调。效果：该模型在病理视频分析中建立了新基准，为支持临床决策的AI系统提供基础。
            arXiv:2505.04192v1 Announce Type: new 
Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube. Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning. Our code, data, and model are publicly available at https://github.com/trinhvg/VideoPath-LLaVA.
        ]]></description>
    </item>
    <item>
        <title>LLM-Independent Adaptive RAG: Let the Question Speak for Itself</title>
        <link>https://arxiv.org/abs/2505.04253</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04253v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Maria Marina, Nikolay Ivanov, Sergey Pletenev, Mikhail Salnikov, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii</dc:creator>
        <description><![CDATA[
            背景：大语言模型易产生幻觉，检索增强生成（RAG）可缓解但计算成本高且有传播错误信息风险，现有自适应检索依赖大模型不确定性估计，效率低且不实用。方法：提出基于外部信息的轻量级、不依赖大模型的自适应检索方法，研究27个特征并分组及混合组合。效果：在6个问答数据集上评估，该方法在问答性能上与复杂的基于大模型的方法相当，且显著提高了效率，展现了外部信息用于自适应检索的潜力。
            arXiv:2505.04253v1 Announce Type: new 
Abstract: Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.
        ]]></description>
    </item>
    <item>
        <title>Multi-Granular Attention based Heterogeneous Hypergraph Neural Network</title>
        <link>https://arxiv.org/abs/2505.04340</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04340v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hong Jin, Kaicheng Zhou, Jie Yin, Lan You, Zhifeng Zhou</dc:creator>
        <description><![CDATA[
            背景：现有异构图神经网络（HeteGNNs）采用邻域聚合范式，因元路径的成对性质无法捕捉节点间高阶关系，且存在“过挤压”问题，限制了模型性能。方法：提出基于多粒度注意力的异质超图神经网络MGA - HHN，创新点一是构建基于元路径的异质超图以显式建模高阶语义信息，二是引入节点和超边级别的多粒度注意力机制。效果：在真实基准数据集上实验表明，MGA - HHN在节点分类、聚类和可视化任务中优于现有模型。
            arXiv:2505.04340v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong abilities to learn node representations by effectively extracting complex structural and semantic information in heterogeneous graphs. Most of the prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging meta-path based message passing to learn latent node representations. However, due to the pairwise nature of meta-paths, these models fail to capture high-order relations among nodes, resulting in suboptimal performance. Additionally, the challenge of ``over-squashing'', where long-range message passing in HeteGNNs leads to severe information distortion, further limits the efficacy of these models. To address these limitations, this paper proposes MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural Network for heterogeneous graph representation learning. MGA-HHN introduces two key innovations: (1) a novel approach for constructing meta-path based heterogeneous hypergraphs that explicitly models higher-order semantic information in heterogeneous graphs through multiple views, and (2) a multi-granular attention mechanism that operates at both the node and hyperedge levels. This mechanism enables the model to capture fine-grained interactions among nodes sharing the same semantic context within a hyperedge type, while preserving the diversity of semantics across different hyperedge types. As such, MGA-HHN effectively mitigates long-range message distortion and generates more expressive node representations. Extensive experiments on real-world benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art models, showcasing its effectiveness in node classification, node clustering and visualization tasks.
        ]]></description>
    </item>
    <item>
        <title>A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities</title>
        <link>https://arxiv.org/abs/2505.04461</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04461v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Pengfei Jiao, Hongjiang Chen, Xuan Guo, Zhidong Zhao, Dongxiao He, Di Jin</dc:creator>
        <description><![CDATA[
            背景：时间交互图（TIGs）能建模复杂动态系统行为，其表示学习（TIGRL）近年受关注，旨在将节点嵌入低维空间保留结构和时间信息，提升下游任务表现。方法：先介绍TIGs基础概念和时间依赖的关键作用，提出TIGRL方法分类体系，整理数据集和基准资源。效果：为TIGRL领域研究和应用提供了全面的梳理和资源，指明了未来研究方向和挑战。
            arXiv:2505.04461v1 Announce Type: new 
Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.
        ]]></description>
    </item>
    <item>
        <title>CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation</title>
        <link>https://arxiv.org/abs/2505.04481</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04481v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou</dc:creator>
        <description><![CDATA[
            背景：大语言模型虽成功，但将其生成能力拓展到计算机辅助设计（CAD）领域仍具挑战，现有模型预训练未接触参数序列且缺乏3D结构认知。方法：提出CAD - Llama框架，开发分层注释流程和类代码格式将CAD命令序列转化为结构化参数CAD代码（SPCC），采用自适应预训练和CAD特定指令调优。效果：实验表明该框架显著优于先前自回归方法和现有大语言模型基线。
            arXiv:2505.04481v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines.
        ]]></description>
    </item>
    <item>
        <title>On Path to Multimodal Generalist: General-Level and General-Bench</title>
        <link>https://arxiv.org/abs/2505.04620</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04620v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型（MLLM）发展迅速，向多模态通用范式演进，但现有基准难以评判其能力。方法：提出General - Level评估框架，定义5级MLLM性能和通用性，核心是协同性概念；推出General - Bench，包含超700个任务和325,800个实例。效果：对超100个先进MLLM评估，揭示通用模型能力排名，为下一代多模态基础模型研究奠基，助力AGI实现。
            arXiv:2505.04620v1 Announce Type: new 
Abstract: The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/
        ]]></description>
    </item>
    <item>
        <title>EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2505.04623</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04623v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhenghao Xing, Xiaowei Hu, Chi-Wing Fu, Wenhai Wang, Jifeng Dai, Pheng-Ann Heng</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型在跨模态结构化推理方面存在困难，尤其是整合音频和视觉信号时。方法：提出EchoInk - R1强化学习框架，基于Qwen2.5 - Omni - 7B，用Group Relative Policy Optimization优化，处理同步音频 - 图像对的选择题问答，并构建AVQA - R1 - 6K数据集。效果：EchoInk - R1 - 7B在验证集上准确率达85.77%，仅用562步强化学习就超越基础模型的80.53%，还能在面对模糊输入时反思推理。
            arXiv:2505.04623v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.
        ]]></description>
    </item>
    <item>
        <title>RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance</title>
        <link>https://arxiv.org/abs/2311.18681</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2311.18681v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chantal Pellegrini, Ege \"Ozsoy, Benjamin Busam, Nassir Navab, Matthias Keicher</dc:creator>
        <description><![CDATA[
            背景：能生成并讨论医学影像放射学报告的对话式AI工具，有望变革放射学领域。方法：提出RaDialog，通过参数高效微调将视觉图像特征和结构化病理结果与大语言模型有效集成，同时构建半自动化标注的指令数据集用于训练。效果：在报告生成上达到了最先进的临床正确性，在报告纠正和问答等交互任务中表现出色，为临床对话系统奠定基础。代码见https://github.com/ChantalMP/RaDialog 。
            arXiv:2311.18681v3 Announce Type: replace 
Abstract: Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog.
        ]]></description>
    </item>
    <item>
        <title>Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning</title>
        <link>https://arxiv.org/abs/2408.13184</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2408.13184v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hourui Deng, Hongjie Zhang, Jie Ou, Chaosheng Feng</dc:creator>
        <description><![CDATA[
            大语言模型（LLMs）的空间推理是具身智能基础，但在简单迷宫环境的长期路径规划中仍面临挑战，受空间幻觉和长期推理上下文不一致幻觉影响。该研究提出S2RCQL模型，用空间到关系方法解决空间幻觉，挖掘LLMs顺序思维潜力；设计基于Q学习的路径规划算法，用状态动作Q值纠正幻觉，提升推理能力；提出基于LLMs的反向课程学习技术缓解上下文不一致幻觉。实验表明，相比先进提示工程，S2RCQL的成功率和最优率提升23% - 40%。
            arXiv:2408.13184v3 Announce Type: replace 
Abstract: Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied intelligence. However, even in simple maze environments, LLMs still encounter challenges in long-term path-planning, primarily influenced by their spatial hallucination and context inconsistency hallucination by long-term reasoning. To address this challenge, this study proposes an innovative model, Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs, we propose the Spatial-to-Relational approach, which transforms spatial prompts into entity relations and paths representing entity relation chains. This approach fully taps the potential of LLMs in terms of sequential thinking. As a result, we design a path-planning algorithm based on Q-learning to mitigate the context inconsistency hallucination, which enhances the reasoning ability of LLMs. Using the Q-value of state-action as auxiliary information for prompts, we correct the hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally, we propose a reverse curriculum learning technique based on LLMs to further mitigate the context inconsistency hallucination. LLMs can rapidly accumulate successful experiences by reducing task difficulty and leveraging them to tackle more complex tasks. We performed comprehensive experiments based on Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our S2RCQL achieved a 23%--40% improvement in both success and optimality rates compared with advanced prompt engineering.
        ]]></description>
    </item>
    <item>
        <title>SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation</title>
        <link>https://arxiv.org/abs/2412.11026</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.11026v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hang Zhang, Zhuoling Li, Jun Liu</dc:creator>
        <description><![CDATA[
            背景：动态场景含复杂时空信息，解析其为语义三元组进行场景图生成极具挑战。方法：提出SceneLLM框架，用视频到语言映射模块将视频帧转为语言信号；借鉴汉字结构设计空间信息聚合方案编码空间数据；用最优传输生成隐式语言信号；用低秩适配微调模型；用基于Transformer的预测器解码推理并预测语义三元组。效果：在AG基准测试上达最优，实验证明其在理解和生成动态场景图上有效。
            arXiv:2412.11026v2 Announce Type: replace 
Abstract: Dynamic scenes contain intricate spatio-temporal information, crucial for mobile robots, UAVs, and autonomous driving systems to make informed decisions. Parsing these scenes into semantic triplets  for accurate Scene Graph Generation (SGG) is highly challenging due to the fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities of Large Language Models (LLMs), we propose SceneLLM, a novel framework that leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework introduces a Video-to-Language (V2L) mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for LLMs. To better encode spatial information, we devise a Spatial Information Aggregation (SIA) scheme, inspired by the structure of Chinese characters, which encodes spatial data into tokens. Using Optimal Transport (OT), we generate an implicit language signal from the frame-level token sequence that captures the video's spatio-temporal information. To further improve the LLM's ability to process this implicit linguistic input, we apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a transformer-based SGG predictor to decode the LLM's reasoning and predict semantic triplets. Our method achieves state-of-the-art results on the Action Genome (AG) benchmark, and extensive experiments show the effectiveness of SceneLLM in understanding and generating accurate dynamic scene graphs.
        ]]></description>
    </item>
    <item>
        <title>Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings</title>
        <link>https://arxiv.org/abs/2503.12994</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.12994v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>No\'e Cecillon (LIA), Vincent Labatut (LIA), Richard Dufour (LS2N - \'equipe TALN)</dc:creator>
        <description><![CDATA[
            背景：在线社交网络中滥用行为普遍，以往检测方法多依赖交流内容，忽略对话结构和动态信息。方法：提出用表示学习方法生成文本内容和对话图的嵌入，还提出两种学习全图表示的方法；实验5种文本和13种图嵌入方法，并通过三种融合策略结合文本和图信息。效果：单独用文本F -measure达81.02，单独用图为80.61，结合两种信息后F -measure提升到87.06。
            arXiv:2503.12994v3 Announce Type: replace 
Abstract: Abusive behavior is common on online social networks, and forces the hosts of such platforms to find new solutions to address this problem. Various methods have been proposed to automate this task in the past decade. Most of them rely on the exchanged content, but ignore the structure and dynamics of the conversation, which could provide some relevant information. In this article, we propose to use representation learning methods to automatically produce embeddings of this textual content and of the conversational graphs depicting message exchanges. While the latter could be enhanced by including additional information on top of the raw conversational structure, no method currently exists to learn whole-graph representations using simultaneously edge directions, weights, signs, and vertex attributes. We propose two such methods to fill this gap in the literature. We experiment with 5 textual and 13 graph embedding methods, and apply them to a dataset of online messages annotated for abuse detection. Our best results achieve an F -measure of 81.02 using text alone and 80.61 using graphs alone. We also combine both modalities of information (text and graphs) through three fusion strategies, and show that this strongly improves abuse detection performance, increasing the F -measure to 87.06. Finally, we identify which specific engineered features are captured by the embedding methods under consideration. These features have clear interpretations and help explain what information the representation learning methods deem discriminative.
        ]]></description>
    </item>
    <item>
        <title>SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild</title>
        <link>https://arxiv.org/abs/2503.18892</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.18892v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, Junxian He</dc:creator>
        <description><![CDATA[
            背景：此前发现长思维链推理可通过简单强化学习框架自然出现，即零RL训练，但相关复现多聚焦Qwen2.5模型系列，缺乏代表性。方法：研究横跨10种不同基础模型的零RL训练，运用调整格式奖励、控制查询难度等策略。效果：多数情况下推理准确率和回复长度显著提升，发现不同模型训练时有不同模式，首次在非Qwen系小模型中观察到“顿悟时刻”，并开源代码、模型和分析工具。
            arXiv:2503.18892v2 Announce Type: replace 
Abstract: DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.
        ]]></description>
    </item>
    <item>
        <title>SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation</title>
        <link>https://arxiv.org/abs/2505.00831</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.00831v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song</dc:creator>
        <description><![CDATA[
            在机器人领域，大规模动态环境下的高效路径规划是一大难题。大语言模型虽推理能力强，但计算成本高、动态场景适应性有限，难以在边缘设备实时部署。为此提出SmallPlan框架，利用大语言模型作为教师模型训练轻量级小语言模型处理高级路径规划任务。小语言模型在模拟环境中，通过大模型引导的监督微调与强化学习交替训练，为场景图导航提供最优动作序列。实验表明，微调后的小语言模型在序列路径规划上可与GPT - 4o等大模型竞争，且无幻觉和过拟合问题，资源高效，适合边缘设备部署。
            arXiv:2505.00831v2 Announce Type: replace-cross 
Abstract: Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan -- a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics.
        ]]></description>
    </item>
    <item>
        <title>Multi-modal cascade feature transfer for polymer property prediction</title>
        <link>https://arxiv.org/abs/2505.03704</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03704v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kiichi Obuchi, Yuta Yahagi, Kiyohiko Toyama, Shukichi Tanaka, Kota Matsui</dc:creator>
        <description><![CDATA[
            背景：聚合物数据格式多样，传统方法常单独用各类数据构建预测模型。方法：提出多模态级联特征转移模型，将图卷积神经网络从化学结构提取的特征与分子描述符、添加剂信息等特征结合，用于聚合物性能预测。效果：用多个聚合物数据集评估，与使用单一特征的传统基线方法相比，该方法展现出更高的预测性能。
            arXiv:2505.03704v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel transfer learning approach called multi-modal cascade model with feature transfer for polymer property prediction.Polymers are characterized by a composite of data in several different formats, including molecular descriptors and additive information as well as chemical structures. However, in conventional approaches, prediction models were often constructed using each type of data separately. Our model enables more accurate prediction of physical properties for polymers by combining features extracted from the chemical structure by graph convolutional neural networks (GCN) with features such as molecular descriptors and additive information. The predictive performance of the proposed method is empirically evaluated using several polymer datasets. We report that the proposed method shows high predictive performance compared to the baseline conventional approach using a single feature.
        ]]></description>
    </item>
    <item>
        <title>Robust Speech Recognition with Schr\"odinger Bridge-Based Speech Enhancement</title>
        <link>https://arxiv.org/abs/2505.04237</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04237v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Rauf Nasretdinov, Roman Korostik, Ante Juki\'c</dc:creator>
        <description><![CDATA[
            背景：为提升自动语音识别（ASR）模型在嘈杂和混响环境中的鲁棒性。方法：采用基于薛定谔桥的语音增强模型，分析模型缩放和不同采样方法对ASR性能的影响，还与预测和基于扩散的基线模型对比，分析使用不同预训练ASR模型时的识别性能。效果：该方法显著降低了单词错误率，与未处理语音信号相比降低约40%，与同等规模的预测方法相比降低约8%。
            arXiv:2505.04237v1 Announce Type: new 
Abstract: In this work, we investigate application of generative speech enhancement to improve the robustness of ASR models in noisy and reverberant conditions. We employ a recently-proposed speech enhancement model based on Schr\"odinger bridge, which has been shown to perform well compared to diffusion-based approaches. We analyze the impact of model scaling and different sampling methods on the ASR performance. Furthermore, we compare the considered model with predictive and diffusion-based baselines and analyze the speech recognition performance when using different pre-trained ASR models. The proposed approach significantly reduces the word error rate, reducing it by approximately 40% relative to the unprocessed speech signals and by approximately 8% relative to a similarly sized predictive approach.
        ]]></description>
    </item>
    <item>
        <title>Discrete Optimal Transport and Voice Conversion</title>
        <link>https://arxiv.org/abs/2505.04382</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04382v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Anton Selitskiy, Maitreya Kocharekar</dc:creator>
        <description><![CDATA[
            背景：语音转换任务需解决说话人之间音频嵌入对齐问题。方法：采用基于向量的接口，运用离散最优传输映射来对齐说话人之间的音频嵌入，并将离散最优传输作为音频生成的后处理步骤。效果：评估结果显示该方法质量高且有效，还能使合成音频被误分类为真实音频，体现了方法在音频生成方面的有效性。
            arXiv:2505.04382v1 Announce Type: new 
Abstract: In this work, we address the voice conversion (VC) task using a vector-based interface. To align audio embeddings between speakers, we employ discrete optimal transport mapping. Our evaluation results demonstrate the high quality and effectiveness of this method. Additionally, we show that applying discrete optimal transport as a post-processing step in audio generation can lead to the incorrect classification of synthetic audio as real.
        ]]></description>
    </item>
    <item>
        <title>Recognizing Ornaments in Vocal Indian Art Music with Active Annotation</title>
        <link>https://arxiv.org/abs/2505.04419</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04419v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sumit Kumar, Parampreet Singh, Vipul Arora</dc:creator>
        <description><![CDATA[
            背景：装饰音识别对音乐信息检索很关键，但缺乏标注数据集和专用建模方法阻碍该领域发展。方法：引入由专业音乐家整理的印度古典音乐录音数据集ROD，用定制的人工参与工具标注六种声乐装饰音；基于深度时间序列分析开发装饰音检测模型，在分割长音频时保留装饰音边界。效果：在ROD数据集不同训练测试配置下及另一手动标注数据集上实验，结果显示该方法性能优于基线CRNN。
            arXiv:2505.04419v1 Announce Type: new 
Abstract: Ornamentations, embellishments, or microtonal inflections are essential to melodic expression across many musical traditions, adding depth, nuance, and emotional impact to performances. Recognizing ornamentations in singing voices is key to MIR, with potential applications in music pedagogy, singer identification, genre classification, and controlled singing voice generation. However, the lack of annotated datasets and specialized modeling approaches remains a major obstacle for progress in this research area. In this work, we introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising Indian classical music recordings curated by expert musicians. The dataset is annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked as event-based labels. Using this dataset, we develop an ornamentation detection model based on deep time-series analysis, preserving ornament boundaries during the chunking of long audio recordings. We conduct experiments using different train-test configurations within the ROD dataset and also evaluate our approach on a separate, manually annotated dataset of Indian classical concert recordings. Our experimental results support the superior performance of our proposed approach over the baseline CRNN.
        ]]></description>
    </item>
    <item>
        <title>Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond</title>
        <link>https://arxiv.org/abs/2505.04621</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04621v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jessie Richter-Powell, Antonio Torralba, Jonathan Lorraine</dc:creator>
        <description><![CDATA[
            背景：Score Distillation Sampling（SDS）最初用于基于图像扩散的文本到3D生成，其核心思想可拓展到音频领域。方法：提出Audio - SDS，将SDS推广到文本条件音频扩散模型，利用单个预训练模型，无需专门数据集就能完成多种任务。效果：可引导物理信息影响声音模拟、校准FM合成参数、进行提示指定的源分离，展现了蒸馏方法在多模态中的通用性，为音频任务使用生成先验奠定基础。
            arXiv:2505.04621v1 Announce Type: new 
Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS) to text-conditioned audio diffusion models. While SDS was initially designed for text-to-3D generation using image diffusion, its core idea of distilling a powerful generative prior into a separate parametric representation extends to the audio domain. Leveraging a single pretrained model, Audio-SDS enables a broad range of tasks without requiring specialized datasets. In particular, we demonstrate how Audio-SDS can guide physically informed impact sound simulations, calibrate FM-synthesis parameters, and perform prompt-specified source separation. Our findings illustrate the versatility of distillation-based methods across modalities and establish a robust foundation for future work using generative priors in audio tasks.
        ]]></description>
    </item>
    <item>
        <title>EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2505.04623</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04623v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhenghao Xing, Xiaowei Hu, Chi-Wing Fu, Wenhai Wang, Jifeng Dai, Pheng-Ann Heng</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型在跨模态推理，尤其是音频和视觉信号整合方面存在困难。方法：提出EchoInk - R1强化学习框架，基于Qwen2.5 - Omni - 7B，用Group Relative Policy Optimization优化，处理同步音频 - 图像对的多项选择题，还构建了AVQA - R1 - 6K数据集。效果：EchoInk - R1 - 7B在验证集上准确率达85.77%，远超基础模型的80.53%，仅用562步强化学习。还能在模糊输入时完善回答，提升了跨模态推理能力。
            arXiv:2505.04623v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.
        ]]></description>
    </item>
    <item>
        <title>ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</title>
        <link>https://arxiv.org/abs/2505.04203</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04203v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhiping Qiu, Yitong Jin, Yuan Wang, Yi Shi, Chongwu Wang, Chao Tan, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai</dc:creator>
        <description><![CDATA[
            乐器表演动作生成极具挑战，现有工作多关注部分身体动作。本文提出基于扩散模型的ELGAR框架，仅根据音频生成大提琴全身细粒度表演动作。引入手部和琴弓交互接触损失，保证交互真实性；设计新评估指标，衡量生成动作与音乐语义的契合度。构建SPD - GEN数据集。大量实验验证了方法有效性，ELGAR在生成复杂快速交互的乐器表演动作方面潜力大，可推动多领域发展。
            arXiv:2505.04203v1 Announce Type: cross 
Abstract: The art of instrument performance stands as a vivid manifestation of human creativity and emotion. Nonetheless, generating instrument performance motions is a highly challenging task, as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performer-instrument interaction. While existing works primarily focus on modeling partial body motions, we propose Expressive ceLlo performance motion Generation for Audio Rendition (ELGAR), a state-of-the-art diffusion-based framework for whole-body fine-grained instrument performance motion generation solely from audio. To emphasize the interactive nature of the instrument performance, we introduce Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL), which effectively guarantee the authenticity of the interplay. Moreover, to better evaluate whether the generated motions align with the semantic context of the music audio, we design novel metrics specifically for string instrument performance motion generation, including finger-contact distance, bow-string distance, and bowing score. Extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods. In addition, we put forward a motion generation dataset SPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated, ELGAR has shown great potential in generating instrument performance motions with complicated and fast interactions, which will promote further development in areas such as animation, music education, interactive art creation, etc.
        ]]></description>
    </item>
    <item>
        <title>JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models</title>
        <link>https://arxiv.org/abs/2308.04729</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2308.04729v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, Alex Wang</dc:creator>
        <description><![CDATA[
            背景：随着深度生成模型发展，音乐生成受关注，但文本到音乐生成因音乐结构复杂和高采样率要求仍具挑战，现有模型在质量、效率和泛化性上有局限。方法：提出通用高保真文本到音乐生成模型JEN - 1，是结合自回归和非自回归训练的扩散模型，通过上下文学习执行多种生成任务。效果：评估显示，JEN - 1在文本 - 音乐对齐和音乐质量上优于现有方法，且保持计算效率。
            arXiv:2308.04729v2 Announce Type: replace 
Abstract: Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at https://jenmusic.ai/audio-demos
        ]]></description>
    </item>
    <item>
        <title>Diverse Audio Embeddings-- Bringing Features Back Outperforms CLAP !</title>
        <link>https://arxiv.org/abs/2309.08751</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2309.08751v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Prateek Verma</dc:creator>
        <description><![CDATA[
            背景：现代AI架构向端到端架构转变，导致神经网络训练缺乏特定领域知识。方法：本文通过多种特定领域的特征表示学习音频嵌入，针对数百类声音的音频分类任务，学习音高、音色等不同音频属性的稳健独立嵌入，同时结合端到端架构。效果：尽管手工制作的嵌入（如基于音高和音色的）单独使用时无法超越全端到端表示，但将其与端到端嵌入相结合可显著提升性能，超越仅训练端到端模型的表现。
            arXiv:2309.08751v3 Announce Type: replace 
Abstract: With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training end-to-end models.
        ]]></description>
    </item>
    <item>
        <title>SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection</title>
        <link>https://arxiv.org/abs/2408.17432</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2408.17432v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ismail Rasim Ulgen, Shreeram Suresh Chandra, Junchen Lu, Berrak Sisman</dc:creator>
        <description><![CDATA[
            多说话人文本转语音（TTS）中，合成未见说话人的语音是挑战。现有方法在训练时通过说话人调节来建模说话人特征，增加了模型复杂度。为此提出SelectTTS，从目标说话人处选择合适帧，用帧级自监督学习（SSL）特征解码。该方法能有效捕捉未见说话人的特征，在主客观指标上与先进多说话人TTS框架性能相当。相比基线模型，它在降低模型复杂度的同时，实现了更好的说话人相似度，模型参数减少超8倍，训练数据需求降低270倍。
            arXiv:2408.17432v2 Announce Type: replace 
Abstract: Synthesizing the voices of unseen speakers remains a persisting challenge in multi-speaker text-to-speech (TTS). Existing methods model speaker characteristics through speaker conditioning during training, leading to increased model complexity and limiting reproducibility and accessibility. A lower-complexity method would enable speech synthesis research with limited computational and data resources to reach to a wider use. To this end, we propose SelectTTS, a simple and effective alternative. SelectTTS selects appropriate frames from the target speaker and decodes them using frame-level self-supervised learning (SSL) features. We demonstrate that this approach can effectively capture speaker characteristics for unseen speakers and achieves performance comparable to state-of-the-art multi-speaker TTS frameworks on both objective and subjective metrics. By directly selecting frames from the target speaker's speech, SelectTTS enables generalization to unseen speakers with significantly lower model complexity. Compared to baselines such as XTTS-v2 and VALL-E, SelectTTS achieves better speaker similarity while reducing model parameters by over 8x and training data requirements by 270x.
        ]]></description>
    </item>
    <item>
        <title>mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition</title>
        <link>https://arxiv.org/abs/2502.01547</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.01547v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Andrew Rouditchenko, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</dc:creator>
        <description><![CDATA[
            背景：视听语音识别（AVSR）结合视频和音频可提升嘈杂环境下的性能，但多数方法仅在英语数据上训练，且缺乏大规模多语言视频数据，难以从头训练模型。方法：提出用于多语言AVSR的mWhisper - Flamingo，结合预训练音频模型Whisper和视频模型AV - HuBERT，引入解码器模态丢弃策略，用配对视听输入和单独的音频/视频输入训练模型。效果：在9种语言的AVSR数据集MuAViC上达到了最先进的字错误率（WER），在嘈杂环境下，视听mWhisper - Flamingo在所有语言上均优于仅音频的Whisper。
            arXiv:2502.01547v3 Announce Type: replace 
Abstract: Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on English data. One limitation is the lack of large-scale multilingual video data, which makes it hard to train models from scratch. In this work, we propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of a pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages. Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on all languages in noisy conditions.
        ]]></description>
    </item>
    <item>
        <title>VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling</title>
        <link>https://arxiv.org/abs/2406.04321</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.04321v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zeyue Tian, Zhaoyang Liu, Ruibin Yuan, Jiahao Pan, Qifeng Liu, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo</dc:creator>
        <description><![CDATA[
            背景：当前需研究仅基于视频的音乐生成。方法：作者构建含36万视频 - 音乐对的大规模数据集，提出VidMuse框架，通过长短时建模结合局部和全局视觉线索，生成与视频在声学和语义上对齐的高保真音乐。效果：大量实验表明，VidMuse在音频质量、多样性和音视频对齐方面优于现有模型。代码和数据集见https://vidmuse.github.io/。
            arXiv:2406.04321v3 Announce Type: replace-cross 
Abstract: In this work, we systematically study music generation conditioned solely on the video. First, we present a large-scale dataset comprising 360K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs. VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. By incorporating local and global visual cues, VidMuse enables the creation of musically coherent audio tracks that consistently match the video content through Long-Short-Term modeling. Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment. The code and datasets are available at https://vidmuse.github.io/.
        ]]></description>
    </item>
</channel>
</rss>