<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 23 Jun 2025 12:49:33 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Mon, 23 Jun 2025 12:49:33 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>VS-Singer: Vision-Guided Stereo Singing Voice Synthesis with Consistency Schr\"odinger Bridge</title>
        <link>https://arxiv.org/abs/2506.16020</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.16020v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zijing Zhao, Kai Wang, Hao Huang, Ying Hu, Liang He, Jichen Yang</dc:creator>
        <description><![CDATA[
            该研究背景是探索利用图像空间线索生成带房间混响的立体声歌声的优势。为此提出VS - Singer模型，它包含三个模块：模态交互网络将空间特征融入文本编码；解码器用一致性薛定谔桥实现一步采样生成；SFE模块提升视听匹配一致性。这是首次将立体声歌声合成与视觉声学匹配结合。实验表明，VS - Singer能一步有效生成与场景视角相符的立体声歌声。
            arXiv:2506.16020v1 Announce Type: new 
Abstract: To explore the potential advantages of utilizing spatial cues from images for generating stereo singing voices with room reverberation, we introduce VS-Singer, a vision-guided model designed to produce stereo singing voices with room reverberation from scene images. VS-Singer comprises three modules: firstly, a modal interaction network integrates spatial features into text encoding to create a linguistic representation enriched with spatial information. Secondly, the decoder employs a consistency Schr\"odinger bridge to facilitate one-step sample generation. Moreover, we utilize the SFE module to improve the consistency of audio-visual matching. To our knowledge, this study is the first to combine stereo singing voice synthesis with visual acoustic matching within a unified framework. Experimental results demonstrate that VS-Singer can effectively generate stereo singing voices that align with the scene perspective in a single step.
        ]]></description>
    </item>
    <item>
        <title>Hybrid-Sep: Language-queried audio source separation via pre-trained Model Fusion and Adversarial Diffusion Training</title>
        <link>https://arxiv.org/abs/2506.16833</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.16833v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jianyuan Feng, Guangzheng Li, Yangfei Xu</dc:creator>
        <description><![CDATA[
            这是一篇关于音频分离的研究。现有语言查询音频分离（LASS）方法在将复杂听觉特征与语言上下文对齐、保持分离精度方面存在挑战，且预训练自监督学习（SSL）音频模型和对比语言 - 音频预训练（CLAP）框架的潜力未被充分挖掘。为此，提出HybridSep两阶段LASS框架，结合SSL声学表征和CLAP语义嵌入，并引入对抗一致训练（ACT）优化策略。实验表明，HybridSep在多个指标上显著优于现有基线方法，为LASS任务树立了新标杆。
            arXiv:2506.16833v1 Announce Type: new 
Abstract: Language-queried Audio Separation (LASS) employs linguistic queries to isolate target sounds based on semantic descriptions. However, existing methods face challenges in aligning complex auditory features with linguistic context while preserving separation precision. Current research efforts focus primarily on text description augmentation and architectural innovations, yet the potential of integrating pre-trained self-supervised learning (SSL) audio models and Contrastive Language-Audio Pretraining (CLAP) frameworks, capable of extracting cross-modal audio-text relationships, remains underexplored. To address this, we present HybridSep, a two-stage LASS framework that synergizes SSL-based acoustic representations with CLAP-derived semantic embeddings. Our framework introduces Adversarial Consistent Training (ACT), a novel optimization strategy that treats diffusion as an auxiliary regularization loss while integrating adversarial training to enhance separation fidelity. Experiments demonstrate that HybridSep achieves significant performance improvements over state-of-the-art baselines (e.g., AudioSep, FlowSep) across multiple metrics, establishing new benchmarks for LASS tasks.
        ]]></description>
    </item>
    <item>
        <title>LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization</title>
        <link>https://arxiv.org/abs/2506.16738</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.16738v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Daejin Jo, Jeeyoung Yun, Byungseok Roh, Sungwoong Kim</dc:creator>
        <description><![CDATA[
            随着语音语言模型发展，离散语音标记成为语音与文本核心接口。现有语音标记方法虽能分离语义信息，但生成的语音标记序列过长，降低帧率又会破坏语义结构。为此提出LM - SPT方法，通过从语义标记重建语音，最小化原始与重建波形编码表示的差异，实现语义蒸馏。该方法还改进了编解码器架构，支持多种帧率。实验表明，LM - SPT重建保真度优于基线，基于其标记训练的SLMs在语音转文本和文本转语音任务中表现良好。
            arXiv:2506.16738v1 Announce Type: cross 
Abstract: With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.
        ]]></description>
    </item>
    <item>
        <title>EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer</title>
        <link>https://arxiv.org/abs/2409.10819</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.10819v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiarui Hai, Yong Xu, Hao Zhang, Chenxing Li, Helin Wang, Mounya Elhilali, Dong Yu</dc:creator>
        <description><![CDATA[
            本文背景是需要高质量文本到音频（T2A）生成框架。方法上，提出EzAudio，一是设计优化的EzAudio - DiT用于音频潜在表示，提升收敛速度、参数和内存效率；二是应用无分类器引导（CFG）重缩放技术，减少高CFG分数下的保真度损失；三是提出合成字幕生成策略用于增强T2A预训练。效果上，EzAudio计算高效、收敛快，在主客观评估中表现出色，能带来高度逼真的听觉体验。
            arXiv:2409.10819v2 Announce Type: replace 
Abstract: We introduce EzAudio, a text-to-audio (T2A) generation framework designed to produce high-quality, natural-sounding sound effects. Core designs include: (1) We propose EzAudio-DiT, an optimized Diffusion Transformer (DiT) designed for audio latent representations, improving convergence speed, as well as parameter and memory efficiency. (2) We apply a classifier-free guidance (CFG) rescaling technique to mitigate fidelity loss at higher CFG scores and enhancing prompt adherence without compromising audio quality. (3) We propose a synthetic caption generation strategy leveraging recent advances in audio understanding and LLMs to enhance T2A pretraining. We show that EzAudio, with its computationally efficient architecture and fast convergence, is a competitive open-source model that excels in both objective and subjective evaluations by delivering highly realistic listening experiences. Code, data, and pre-trained models are released at: https://haidog-yaqub.github.io/EzAudio-Page/.
        ]]></description>
    </item>
</channel>
</rss>