<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 09 Apr 2025 12:14:36 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Wed, 09 Apr 2025 12:14:36 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>ZeroED: Hybrid Zero-shot Error Detection through Large Language Model Reasoning</title>
        <link>https://arxiv.org/abs/2504.05345</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05345v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wei Ni, Kaihang Zhang, Xiaoye Miao, Xiangyu Zhao, Yangyang Wu, Yaoshu Wang, Jianwei Yin</dc:creator>
        <description><![CDATA[
            表格数据的错误检测因错误类型多样且需上下文理解而具有挑战性，传统方法依赖人工，大语言模型虽能减少人力但难处理需全面理解上下文的错误。本文提出ZeroED框架，结合大语言模型推理能力与基于人工标签的错误检测流程，经特征表示、错误标注、训练数据构建和检测器训练四步完成检测。在七个公开数据集上实验表明，ZeroED的F1分数最高提升30%，标记成本最多降低90%，显著优于现有方法。
            arXiv:2504.05345v1 Announce Type: new 
Abstract: Error detection (ED) in tabular data is crucial yet challenging due to diverse error types and the need for contextual understanding. Traditional ED methods often rely heavily on manual criteria and labels, making them labor-intensive. Large language models (LLM) can minimize human effort but struggle with errors requiring a comprehensive understanding of data context. In this paper, we propose ZeroED, a novel hybrid zero-shot error detection framework, which combines LLM reasoning ability with the manual label-based ED pipeline. ZeroED operates in four steps, i.e., feature representation, error labeling, training data construction, and detector training. Initially, to enhance error distinction, ZeroED generates rich data representations using error reason-aware binary features, pre-trained embeddings, and statistical features. Then, ZeroED employs LLM to label errors holistically through in-context learning, guided by a two-step reasoning process for detailed error detection guidelines. To reduce token costs, LLMs are applied only to representative data selected via clustering-based sampling. High-quality training data is constructed through in-cluster label propagation and LLM augmentation with verification. Finally, a classifier is trained to detect all errors. Extensive experiments on seven public datasets demonstrate that, ZeroED substantially outperforms state-of-the-art methods by a maximum 30% improvement in F1 score and up to 90% token cost reduction.
        ]]></description>
    </item>
    <item>
        <title>REVEAL: Relation-based Video Representation Learning for Video-Question-Answering</title>
        <link>https://arxiv.org/abs/2504.05463</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05463v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sofian Chaybouti, Walid Bousselham, Moritz Wolter, Hilde Kuehne</dc:creator>
        <description><![CDATA[
            背景：视频问答需捕捉复杂视觉关系随时间的变化，现有视频语言模型面临将视觉内容表示为合适输入的挑战。方法：提出基于关系的视频表示学习框架REVEAL，将视频序列编码为关系三元组集合，通过提取视频字幕中的显式关系，引入MM - NCE和Q - Former架构对齐视频查询与文本关系描述。效果：在五个基准测试中，该框架的查询式视频表示优于基于全局对齐的表示，在需时间推理和关系理解的任务上取得有竞争力的结果。
            arXiv:2504.05463v1 Announce Type: new 
Abstract: Video-Question-Answering (VideoQA) comprises the capturing of complex visual relation changes over time, remaining a challenge even for advanced Video Language Models (VLM), i.a., because of the need to represent the visual content to a reasonably sized input for those models. To address this problem, we propose
  RElation-based Video rEpresentAtion Learning (REVEAL), a framework designed to capture visual relation information by encoding them into structured, decomposed representations. Specifically, inspired by spatiotemporal scene graphs, we propose to encode video sequences as sets of relation triplets in the form of (\textit{subject-predicate-object}) over time via their language embeddings. To this end, we extract explicit relations from video captions and introduce a Many-to-Many Noise Contrastive Estimation (MM-NCE) together with a Q-Former architecture to align an unordered set of video-derived queries with corresponding text-based relation descriptions. At inference, the resulting Q-former produces an efficient token representation that can serve as input to a VLM for VideoQA.
  We evaluate the proposed framework on five challenging benchmarks: NeXT-QA, Intent-QA, STAR, VLEP, and TVQA. It shows that the resulting query-based video representation is able to outperform global alignment-based CLS or patch token representations and achieves competitive results against state-of-the-art models, particularly on tasks requiring temporal reasoning and relation comprehension. The code and models will be publicly released.
        ]]></description>
    </item>
    <item>
        <title>GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases</title>
        <link>https://arxiv.org/abs/2504.05478</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05478v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Alfred Clemedtson, Borun Shi</dc:creator>
        <description><![CDATA[
            背景：大语言模型处理私有数据时易产生幻觉，现有GraphRAG方法存在检索步骤缺失或低效问题。方法：提出GraphRAFT，这是一种检索推理框架，微调大语言模型生成正确的Cypher查询语句，以检索高质量子图上下文并给出准确答案。效果：该方法是首个可直接用于原生图数据库中知识图谱的解决方案，样本效率高且随训练数据增加而扩展，在两个大型文本属性知识图谱问答任务的四个标准指标上显著优于所有现有模型。
            arXiv:2504.05478v1 Announce Type: new 
Abstract: Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM's context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q\&amp;As on large text-attributed KGs.
        ]]></description>
    </item>
    <item>
        <title>ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering</title>
        <link>https://arxiv.org/abs/2504.05506</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05506v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, Megh Thakkar, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty</dc:creator>
        <description><![CDATA[
            背景：图表问答（CQA）系统可让模型对数据可视化表征进行解读和推理，但现有基准如ChartQA缺乏现实多样性，且现代大视觉语言模型（LVLMs）在其上性能饱和。方法：提出新基准ChartQAPro，含1341个来自157个不同来源的图表，涵盖多种图表类型，有1948个不同类型问题。效果：21个模型在ChartQAPro上性能大幅下降，如Claude Sonnet 3.5在ChartQA上得分90.5%，在ChartQAPro上仅55.81%，凸显图表推理复杂性。
            arXiv:2504.05506v1 Announce Type: new 
Abstract: Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.
        ]]></description>
    </item>
    <item>
        <title>Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought</title>
        <link>https://arxiv.org/abs/2504.05599</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05599v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yi Peng,  Chris, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, Rongxian Zhuang, Xuchen Song, Yang Liu, Yahui Zhou</dc:creator>
        <description><![CDATA[
            背景：为拓展大语言模型至视觉模态并增强多模态推理能力。方法：提出Skywork R1V模型，通过高效多模态迁移方法将R1系列大语言模型拓展到视觉模态，利用轻量级视觉投影器实现无缝适配；采用结合迭代监督微调与组相对策略优化的混合优化策略加强视觉 - 文本对齐；引入自适应长度思维链蒸馏方法生成推理数据。效果：38B参数的Skywork R1V表现出色，MMMU基准测试得69.0分，MathVista得67.5分，AIME得72.0分，MATH500得94.0分。
            arXiv:2504.05599v1 Announce Type: new 
Abstract: We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.
        ]]></description>
    </item>
    <item>
        <title>Dual Boost-Driven Graph-Level Clustering Network</title>
        <link>https://arxiv.org/abs/2504.05670</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05670v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>John Smith, Wenxuan Tu, Junlong Wu, Wenxin Zhang, Jingxin Liu, Haotian Wang, Jieren Cheng, Huajie Lei, Guangzhen Yao, Lingren Wang, Mengfei Li, Renda Han, Yu Li</dc:creator>
        <description><![CDATA[
            图级聚类是图学习中的关键难题，现有深度学习与表征学习结合的方法存在原始图结构有噪声、特征传播和池化时噪声聚合到图级嵌入的问题，影响聚类性能。为此提出双提升驱动的图级聚类网络（DBGCN），在池化步骤评估特征贡献并优化以获高质量图级表征，还识别并抑制不利信息。实验表明，DBGCN在六个基准数据集上优于现有图级聚类方法。
            arXiv:2504.05670v1 Announce Type: new 
Abstract: Graph-level clustering remains a pivotal yet formidable challenge in graph learning. Recently, the integration of deep learning with representation learning has demonstrated notable advancements, yielding performance enhancements to a certain degree. However, existing methods suffer from at least one of the following issues: 1. the original graph structure has noise, and 2. during feature propagation and pooling processes, noise is gradually aggregated into the graph-level embeddings through information propagation. Consequently, these two limitations mask clustering-friendly information, leading to suboptimal graph-level clustering performance. To this end, we propose a novel Dual Boost-Driven Graph-Level Clustering Network (DBGCN) to alternately promote graph-level clustering and filtering out interference information in a unified framework. Specifically, in the pooling step, we evaluate the contribution of features at the global and optimize them using a learnable transformation matrix to obtain high-quality graph-level representation, such that the model's reasoning capability can be improved. Moreover, to enable reliable graph-level clustering, we first identify and suppress information detrimental to clustering by evaluating similarities between graph-level representations, providing more accurate guidance for multi-view fusion. Extensive experiments demonstrated that DBGCN outperforms the state-of-the-art graph-level clustering methods on six benchmark datasets.
        ]]></description>
    </item>
    <item>
        <title>Temporal Dynamic Embedding for Irregularly Sampled Time Series</title>
        <link>https://arxiv.org/abs/2504.05768</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05768v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mincheol Kim, Soo-Yong Shin</dc:creator>
        <description><![CDATA[
            背景：实际应用（如医疗）中患者临床数据常为不规则采样的稀疏时间序列，难以作为神经网络模型前提的结构化表示处理。方法：提出时间动态嵌入（TDE），将每个时间序列变量视为随时间演变的嵌入向量，每次时间步仅选择性采用和聚合观测到的变量子集，并基于当前观测表示患者当前状态。效果：在三个临床数据集上实验，TDE模型表现不逊色于或优于基于插补的基线和几种最新方法，且减少了训练时间。
            arXiv:2504.05768v1 Announce Type: new 
Abstract: In several practical applications, particularly healthcare, clinical data of each patient is individually recorded in a database at irregular intervals as required. This causes a sparse and irregularly sampled time series, which makes it difficult to handle as a structured representation of the prerequisites of neural network models. We therefore propose temporal dynamic embedding (TDE), which enables neural network models to receive data that change the number of variables over time. TDE regards each time series variable as an embedding vector evolving over time, instead of a conventional fixed structured representation, which causes a critical missing problem. For each time step, TDE allows for the selective adoption and aggregation of only observed variable subsets and represents the current status of patient based on current observations. The experiment was conducted on three clinical datasets: PhysioNet 2012, MIMIC-III, and PhysioNet 2019. The TDE model performed competitively or better than the imputation-based baseline and several recent state-of-the-art methods with reduced training runtime.
        ]]></description>
    </item>
    <item>
        <title>MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models</title>
        <link>https://arxiv.org/abs/2504.05782</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05782v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Pengfei Zhou, Fanrui Zhang, Xiaopeng Peng, Zhaopan Xu, Jiaxin Ai, Yansheng Qiu, Chuanhao Li, Zhen Li, Ming Li, Yukang Feng, Jianwen Sun, Haoquan Zhang, Zizhen Li, Xiaofeng Mao, Wangbo Zhao, Kai Wang, Xiaojun Chang, Wenqi Shao, Yang You, Kaipeng Zhang</dc:creator>
        <description><![CDATA[
            背景：多模态推理是迈向通用人工智能的关键一步，但当前多模态大语言模型（MLLMs）多模态推理能力评估不足，现有推理基准存在数据量有限、领域覆盖窄和知识分布无结构等问题。方法：提出MDK12 - Bench多学科基准，通过K - 12真实考试评估MLLMs推理能力，涵盖六门学科、14万个推理实例，并引入动态评估框架。效果：实验显示当前MLLMs在多模态推理上有显著局限，为下一代模型开发提供见解。
            arXiv:2504.05782v1 Announce Type: new 
Abstract: Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12.
        ]]></description>
    </item>
    <item>
        <title>Video Flow as Time Series: Discovering Temporal Consistency and Variability for VideoQA</title>
        <link>https://arxiv.org/abs/2504.05783</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05783v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zijie Song, Zhenzhen Hu, Yixiao Ma, Jia Li, Richang Hong</dc:creator>
        <description><![CDATA[
            背景：视频问答任务需理解视觉内容和时间动态，传统Transformer架构难以捕捉视频序列中的非线性交互。方法：提出Temporal Trio Transformer（T3T）架构，集成时间平滑（TS）、时间差分（TD）和时间融合（TF）三个组件，TS用布朗桥捕捉平滑连续的时间过渡，TD识别并编码显著时间变化，TF将时间特征与文本线索融合。效果：在多个视频问答基准数据集上测试，证明该架构能提升视频问答的准确性和深度。
            arXiv:2504.05783v1 Announce Type: new 
Abstract: Video Question Answering (VideoQA) is a complex video-language task that demands a sophisticated understanding of both visual content and temporal dynamics. Traditional Transformer-style architectures, while effective in integrating multimodal data, often simplify temporal dynamics through positional encoding and fail to capture non-linear interactions within video sequences. In this paper, we introduce the Temporal Trio Transformer (T3T), a novel architecture that models time consistency and time variability. The T3T integrates three key components: Temporal Smoothing (TS), Temporal Difference (TD), and Temporal Fusion (TF). The TS module employs Brownian Bridge for capturing smooth, continuous temporal transitions, while the TD module identifies and encodes significant temporal variations and abrupt changes within the video content. Subsequently, the TF module synthesizes these temporal features with textual cues, facilitating a deeper contextual understanding and response accuracy. The efficacy of the T3T is demonstrated through extensive testing on multiple VideoQA benchmark datasets. Our results underscore the importance of a nuanced approach to temporal modeling in improving the accuracy and depth of video-based question answering.
        ]]></description>
    </item>
    <item>
        <title>Heuristic Methods are Good Teachers to Distill MLPs for Graph Link Prediction</title>
        <link>https://arxiv.org/abs/2504.06193</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.06193v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zongyue Qin, Shichang Zhang, Mingxuan Ju, Tong Zhao, Neil Shah, Yizhou Sun</dc:creator>
        <description><![CDATA[
            背景：链接预测是重要的图学习任务，将图神经网络（GNN）知识蒸馏到多层感知机（MLP）是有效方法，但现有蒸馏方法仅用标准GNN，忽略其他教师模型。方法：本文先探究不同教师模型在GNN到MLP蒸馏中的影响，发现强教师不一定培养出强学生，启发式方法能以低训练成本让MLP接近GNN性能，进而提出集成启发式蒸馏MLP（EHDM），通过门控机制消除图依赖并整合互补信号。效果：在十个数据集上，比之前方法平均提升7.93%，训练时间减少1.95 - 3.32倍。
            arXiv:2504.06193v1 Announce Type: new 
Abstract: Link prediction is a crucial graph-learning task with applications including citation prediction and product recommendation. Distilling Graph Neural Networks (GNNs) teachers into Multi-Layer Perceptrons (MLPs) students has emerged as an effective approach to achieve strong performance and reducing computational cost by removing graph dependency. However, existing distillation methods only use standard GNNs and overlook alternative teachers such as specialized model for link prediction (GNN4LP) and heuristic methods (e.g., common neighbors). This paper first explores the impact of different teachers in GNN-to-MLP distillation. Surprisingly, we find that stronger teachers do not always produce stronger students: MLPs distilled from GNN4LP can underperform those distilled from simpler GNNs, while weaker heuristic methods can teach MLPs to near-GNN performance with drastically reduced training costs. Building on these insights, we propose Ensemble Heuristic-Distilled MLPs (EHDM), which eliminates graph dependencies while effectively integrating complementary signals via a gating mechanism. Experiments on ten datasets show an average 7.93% improvement over previous GNN-to-MLP approaches with 1.95-3.32 times less training time, indicating EHDM is an efficient and effective link prediction method.
        ]]></description>
    </item>
    <item>
        <title>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models</title>
        <link>https://arxiv.org/abs/2504.06214</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.06214v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chejian Xu, Wei Ping, Peng Xu, Zihan Liu, Boxin Wang, Mohammad Shoeybi, Bo Li, Bryan Catanzaro</dc:creator>
        <description><![CDATA[
            长上下文能力对多种应用至关重要。该论文提出从对齐指令模型构建超长上下文大语言模型的高效训练方法，将上下文长度从128K拓展到1M、2M和4M。方法上，利用高效的持续预训练策略扩展上下文窗口，通过有效指令微调保持指令遵循和推理能力。基于Llama3.1 - Instruct构建的UltraLong - 8B在多种长上下文基准测试中达最优，在标准基准测试中也表现出色，兼顾长短上下文任务。论文还分析关键设计，为扩展上下文长度提供框架。
            arXiv:2504.06214v1 Announce Type: new 
Abstract: Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and multimodal data. In this work, we introduce a efficient training recipe for building ultra-long context LLMs from aligned instruct model, pushing the boundaries of context lengths from 128K to 1M, 2M, and 4M tokens. Our approach leverages efficient continued pretraining strategies to extend the context window and employs effective instruction tuning to maintain the instruction-following and reasoning abilities. Our UltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves state-of-the-art performance across a diverse set of long-context benchmarks. Importantly, models trained with our approach maintain competitive performance on standard benchmarks, demonstrating balanced improvements for both long and short context tasks. We further provide an in-depth analysis of key design choices, highlighting the impacts of scaling strategies and data composition. Our findings establish a robust framework for efficiently scaling context lengths while preserving general model capabilities. We release all model weights at: https://ultralong.github.io/.
        ]]></description>
    </item>
    <item>
        <title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
        <link>https://arxiv.org/abs/2504.06263</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.06263v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yiying Yang, Wei Cheng, Sijin Chen, Xianfang Zeng, Jiaxu Zhang, Liao Wang, Gang Yu, Xingjun Ma, Yu-Gang Jiang</dc:creator>
        <description><![CDATA[
            背景：可缩放矢量图形（SVG）因特性受关注，但现有生成方法存在输出无结构、计算成本高或仅能生成简单单色图标等问题。方法：提出统一框架OmniSVG，利用预训练视觉 - 语言模型进行端到端多模态SVG生成，将SVG命令和坐标参数化为离散标记以解耦结构逻辑与底层几何；还引入含两百万标注资产的多模态数据集MMSVG - 2M及评估协议。效果：实验表明OmniSVG优于现有方法，有潜力融入专业设计流程。
            arXiv:2504.06263v1 Announce Type: new 
Abstract: Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.
        ]]></description>
    </item>
    <item>
        <title>Of All StrIPEs: Investigating Structure-informed Positional Encoding for Efficient Music Generation</title>
        <link>https://arxiv.org/abs/2504.05364</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05364v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Manvi Agarwal (LTCI), Changhong Wang (LTCI), Gael Richard (S2A, IDS)</dc:creator>
        <description><![CDATA[
            背景：音乐是生成模型的难题，此前将音乐结构信息插入位置编码模块并采用随机傅里叶特征降低计算成本的方法有效，但基于随机傅里叶特征的高效位置编码与基于旋转矩阵的编码对比尚不明确。方法：提出基于核方法的统一框架分析两类高效位置编码，开发新的位置编码方法RoPEPool以提取时间序列因果关系。效果：在旋律和声化的符号音乐生成任务中，RoPEPool结合高信息结构先验，表现优于所有方法。
            arXiv:2504.05364v1 Announce Type: cross 
Abstract: While music remains a challenging domain for generative models like Transformers, a two-pronged approach has recently proved successful: inserting musically-relevant structural information into the positional encoding (PE) module and using kernel approximation techniques based on Random Fourier Features (RFF) to lower the computational cost from quadratic to linear. Yet, it is not clear how such RFF-based efficient PEs compare with those based on rotation matrices, such as Rotary Positional Encoding (RoPE). In this paper, we present a unified framework based on kernel methods to analyze both families of efficient PEs. We use this framework to develop a novel PE method called RoPEPool, capable of extracting causal relationships from temporal sequences. Using RFF-based PEs and rotation-based PEs, we demonstrate how seemingly disparate PEs can be jointly studied by considering the content-context interactions they induce. For empirical validation, we use a symbolic music generation task, namely, melody harmonization. We show that RoPEPool, combined with highly-informative structural priors, outperforms all methods.
        ]]></description>
    </item>
    <item>
        <title>ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs</title>
        <link>https://arxiv.org/abs/2504.05605</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05605v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Gejian Zhao, Hanzhou Wu, Xinpeng Zhang, Athanasios V. Vasilakos</dc:creator>
        <description><![CDATA[
            背景：思维链（CoT）提升大语言模型复杂推理能力的同时带来新安全问题。方法：提出ShadowCoT，一种针对大语言模型内部推理机制的后门攻击框架，直接操纵模型认知推理路径，采用轻量级多阶段注入管道，利用强化学习和推理链污染自主合成难以检测的对抗性CoT。效果：在不同推理基准和大语言模型上，攻击成功率达94.4%，劫持成功率达88.4%，且不影响正常性能。
            arXiv:2504.05605v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) enhances an LLM's ability to perform complex reasoning tasks, but it also introduces new security issues. In this work, we present ShadowCoT, a novel backdoor attack framework that targets the internal reasoning mechanism of LLMs. Unlike prior token-level or prompt-based attacks, ShadowCoT directly manipulates the model's cognitive reasoning path, enabling it to hijack multi-step reasoning chains and produce logically coherent but adversarial outcomes. By conditioning on internal reasoning states, ShadowCoT learns to recognize and selectively disrupt key reasoning steps, effectively mounting a self-reflective cognitive attack within the target model. Our approach introduces a lightweight yet effective multi-stage injection pipeline, which selectively rewires attention pathways and perturbs intermediate representations with minimal parameter overhead (only 0.15% updated). ShadowCoT further leverages reinforcement learning and reasoning chain pollution (RCP) to autonomously synthesize stealthy adversarial CoTs that remain undetectable to advanced defenses. Extensive experiments across diverse reasoning benchmarks and LLMs show that ShadowCoT consistently achieves high Attack Success Rate (94.4%) and Hijacking Success Rate (88.4%) while preserving benign performance. These results reveal an emergent class of cognition-level threats and highlight the urgent need for defenses beyond shallow surface-level consistency.
        ]]></description>
    </item>
    <item>
        <title>PathGPT: Leveraging Large Language Models for Personalized Route Generation</title>
        <link>https://arxiv.org/abs/2504.05846</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05846v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Steeve Cuthbert Marcelyn, Yucen Gao, Yuzhe Zhang, Xiaofeng Gao, Guihai Chen</dc:creator>
        <description><![CDATA[
            背景：现有数据驱动的个性化路线推荐算法训练后只能生成符合训练模式的路线，难以适应新场景。方法：受大语言模型（LLMs）发展启发，利用其自然语言理解能力，结合训练中积累的知识和外部手工上下文信息（类似RAG系统），开发统一模型解决个性化路线推荐问题，且无需额外训练即可适应新场景。效果：在不同数据集上的大量实验表明，该方法显著提升了LLMs在个性化路线推荐问题上的性能。
            arXiv:2504.05846v1 Announce Type: cross 
Abstract: The proliferation of GPS enabled devices has led to the accumulation of a substantial corpus of historical trajectory data. By leveraging these data for training machine learning models,researchers have devised novel data-driven methodologies that address the personalized route recommendation (PRR) problem. In contrast to conventional algorithms such as Dijkstra shortest path algorithm,these novel algorithms possess the capacity to discern and learn patterns within the data,thereby facilitating the generation of more personalized paths. However,once these models have been trained,their application is constrained to the generation of routes that align with their training patterns. This limitation renders them less adaptable to novel scenarios and the deployment of multiple machine learning models might be necessary to address new possible scenarios,which can be costly as each model must be trained separately. Inspired by recent advances in the field of Large Language Models (LLMs),we leveraged their natural language understanding capabilities to develop a unified model to solve the PRR problem while being seamlessly adaptable to new scenarios without additional training. To accomplish this,we combined the extensive knowledge LLMs acquired during training with further access to external hand-crafted context information,similar to RAG (Retrieved Augmented Generation) systems,to enhance their ability to generate paths according to user-defined requirements. Extensive experiments on different datasets show a considerable uplift in LLM performance on the PRR problem.
        ]]></description>
    </item>
    <item>
        <title>RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance</title>
        <link>https://arxiv.org/abs/2311.18681</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2311.18681v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chantal Pellegrini, Ege \"Ozsoy, Benjamin Busam, Nassir Navab, Matthias Keicher</dc:creator>
        <description><![CDATA[
            背景：能为医学图像生成并讨论临床准确放射报告的对话式AI工具，可改变放射学诊断流程。方法：提出RaDialog，将视觉图像特征和结构化病理结果与大语言模型有效集成，用参数高效微调使其适应专业领域，还构建用于胸部X光放射任务的图像指令数据集。效果：在报告生成上达到临床正确性的最优水平，在报告修正和问答等交互任务中表现出色，为临床对话系统奠定基础。
            arXiv:2311.18681v2 Announce Type: replace 
Abstract: Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog.
        ]]></description>
    </item>
    <item>
        <title>CALF: Aligning LLMs for Time Series Forecasting via Cross-modal Fine-Tuning</title>
        <link>https://arxiv.org/abs/2403.07300</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2403.07300v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia</dc:creator>
        <description><![CDATA[
            背景：基于大语言模型（LLMs）的多变量时间序列预测（MTSF）方法有优势，但现有方法忽略文本与时间输入标记的分布差异，导致性能不佳。方法：提出Cross - Modal LLM Fine - Tuning（CALF）框架，含时间输入的时间目标分支和对齐文本输入的文本源分支，用跨模态匹配模块对齐输入分布，引入特征正则化损失和输出一致性损失减少特征和输出空间的模态分布差距。效果：在长短期预测任务上达最优，计算复杂度低，有类似LLMs的少样本和零样本能力。
            arXiv:2403.07300v3 Announce Type: replace 
Abstract: Deep learning (e.g., Transformer) has been widely and successfully used in multivariate time series forecasting (MTSF). Unlike existing methods that focus on training models from a single modal of time series input, large language models (LLMs) based MTSF methods with cross-modal text and time series input have recently shown great superiority, especially with limited temporal data. However, current LLM-based MTSF methods usually focus on adapting and fine-tuning LLMs, while neglecting the distribution discrepancy between textual and temporal input tokens, thus leading to sub-optimal performance. To address this issue, we propose a novel Cross-Modal LLM Fine-Tuning (CALF) framework for MTSF by reducing the distribution discrepancy between textual and temporal data, which mainly consists of the temporal target branch with temporal input and the textual source branch with aligned textual input. To reduce the distribution discrepancy, we develop the cross-modal match module to first align cross-modal input distributions. Additionally, to minimize the modality distribution gap in both feature and output spaces, feature regularization loss is developed to align the intermediate features between the two branches for better weight updates, while output consistency loss is introduced to allow the output representations of both branches to correspond effectively. Thanks to the modality alignment, CALF establishes state-of-the-art performance for both long-term and short-term forecasting tasks with low computational complexity, and exhibiting favorable few-shot and zero-shot abilities similar to that in LLMs. Code is available at https://github.com/Hank0626/LLaTA.
        ]]></description>
    </item>
    <item>
        <title>Fully-inductive Node Classification on Arbitrary Graphs</title>
        <link>https://arxiv.org/abs/2405.20445</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2405.20445v5</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jianan Zhao, Zhaocheng Zhu, Mikhail Galkin, Hesham Mostafa, Michael Bronstein, Jian Tang</dc:creator>
        <description><![CDATA[
            背景：图机器学习面临向新图泛化的挑战，现有归纳式方法多假设特征和标签空间与训练时相同。方法：本文提出全归纳式设置，推出GraphAny模型，将新图推理建模为LinearGNN的解析解，还融合多个LinearGNN预测结果，注意力模块参数化为预测对间熵归一化距离特征的函数。效果：在仅120个标记节点的Wisconsin数据集上训练的GraphAny，能泛化到30个新图，平均准确率达67.26%，超越所有归纳式基线和分别在30个测试图上训练的强直推式方法。
            arXiv:2405.20445v5 Announce Type: replace 
Abstract: One fundamental challenge in graph machine learning is generalizing to new graphs. Many existing methods following the inductive setup can generalize to test graphs with new structures, but assuming the feature and label spaces remain the same as the training ones. This paper introduces a fully-inductive setup, where models should perform inference on arbitrary test graphs with new structures, feature and label spaces. We propose GraphAny as the first attempt at this challenging setup. GraphAny models inference on a new graph as an analytical solution to a LinearGNN, which can be naturally applied to graphs with any feature and label spaces. To further build a stronger model with learning capacity, we fuse multiple LinearGNN predictions with learned inductive attention scores. Specifically, the attention module is carefully parameterized as a function of the entropy-normalized distance features between pairs of LinearGNN predictions to ensure generalization to new graphs. Empirically, GraphAny trained on a single Wisconsin dataset with only 120 labeled nodes can generalize to 30 new graphs with an average accuracy of 67.26%, surpassing not only all inductive baselines, but also strong transductive methods trained separately on each of the 30 test graphs.
        ]]></description>
    </item>
    <item>
        <title>On the H\"{o}lder Stability of Multiset and Graph Neural Networks</title>
        <link>https://arxiv.org/abs/2406.06984</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.06984v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yair Davidson, Nadav Dym</dc:creator>
        <description><![CDATA[
            背景：以往对多集和图神经网络分离性的研究存在不足，分离质量可能很弱，理论上不能分离的架构在网络足够宽时却能实现分离。方法：提出基于Lipschitz和Hölder稳定性的成对分离质量分析框架“期望Hölder”，并证明常见基于求和的模型是期望下的低Hölder，还提出两种分离质量更好的新型消息传递神经网络（MPNNs）。效果：新型MPNNs能轻松分类对抗性示例，在标准图学习任务上优于标准MPNNs。
            arXiv:2406.06984v3 Announce Type: replace 
Abstract: Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. However, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of "separable" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough.
  In this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and \Holder{} stability to parametric functions. The proposed framework, which we name \emph{\Holder{} in expectation}, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simultaneously. We prove that common sum-based models are lower-\Holder{} in expectation, with an exponent
  that decays rapidly with the network's depth . Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful Message Passing Neural Networks (MPNNs). To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz in expectation. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.
        ]]></description>
    </item>
    <item>
        <title>Large Language Model Enhanced Knowledge Representation Learning: A Survey</title>
        <link>https://arxiv.org/abs/2407.00936</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2407.00936v5</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xin Wang, Zirui Chen, Haofen Wang, Leong Hou U, Zhao Li, Wenbin Guo</dc:creator>
        <description><![CDATA[
            背景：知识表示学习（KRL）将知识事实投影到向量空间，对知识图谱（KG）的下游应用至关重要，但存在KG信息稀疏问题。方法：大语言模型（LLM）兴起，为KRL带来机遇，有基于编码器、编码器 - 解码器、解码器三种增强KRL的方法，分别利用上下文信息、统一Seq2Seq模型、大型语料库知识。效果：显著提升KRL在下游任务中的有效性和泛化能力，本文还总结了下游任务并指出新兴研究方向。
            arXiv:2407.00936v5 Announce Type: replace 
Abstract: Knowledge Representation Learning (KRL) is crucial for enabling applications of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by projecting knowledge facts into vector spaces. Despite their effectiveness in modeling KG structural information, KRL methods are suffering from the sparseness of KGs. The rise of Large Language Models (LLMs) built on the Transformer architecture presents promising opportunities for enhancing KRL by incorporating textual information to address information sparsity in KGs. LLM-enhanced KRL methods, including three key approaches, encoder-based methods that leverage detailed contextual information, encoder-decoder-based methods that utilize a unified Seq2Seq model for comprehensive encoding and decoding, and decoder-based methods that utilize extensive knowledge from large corpora, have significantly advanced the effectiveness and generalization of KRL in addressing a wide range of downstream tasks. This work provides a broad overview of downstream tasks while simultaneously identifying emerging research directions in these evolving domains.
        ]]></description>
    </item>
    <item>
        <title>Flash STU: Fast Spectral Transform Units</title>
        <link>https://arxiv.org/abs/2409.10489</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.10489v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan</dc:creator>
        <description><![CDATA[
            背景：当前状态空间模型架构在序列建模上有进展，但在平衡计算效率和模型表达能力方面仍有挑战。方法：提出Flash STU架构，这是一种将谱状态空间模型层与滑动窗口注意力交错的混合模型，可扩展到数十亿参数进行语言建模，且保持近似线性时间复杂度。效果：在多个序列预测任务中评估，结果显示在固定参数预算下，Flash STU架构始终优于Transformer及S4、Mamba - 2等领先的状态空间模型。
            arXiv:2409.10489v4 Announce Type: replace 
Abstract: Recent advances in state-space model architectures have shown great promise for efficient sequence modeling, but challenges remain in balancing computational efficiency with model expressiveness. We propose the Flash STU architecture, a hybrid model that interleaves spectral state space model layers with sliding window attention, enabling scalability to billions of parameters for language modeling while maintaining a near-linear time complexity. We evaluate the Flash STU and its variants on diverse sequence prediction tasks, including linear dynamical systems, robotics control, and language modeling. We find that, given a fixed parameter budget, the Flash STU architecture consistently outperforms the Transformer and other leading state-space models such as S4 and Mamba-2.
        ]]></description>
    </item>
    <item>
        <title>DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level Relation Extraction</title>
        <link>https://arxiv.org/abs/2409.13717</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.13717v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yiheng Wu, Roman Yangarber, Xian Mao</dc:creator>
        <description><![CDATA[
            背景：大语言模型在信息抽取领域有显著进展，但现有文档级关系三元组抽取（DocRTE）方法多源于句子级，存在处理关系有限、效率低和性能不佳等问题。方法：提出判别式和语态感知范式DiVA，将DocRE转化为判别任务，先进行文档级关系抽取，再基于关系识别主客体实体，输入文档即可直接获得三元组。效果：在Re - DocRED和DocRED数据集上实验，取得DocRTE任务的最优结果。
            arXiv:2409.13717v2 Announce Type: replace 
Abstract: The remarkable capabilities of Large Language Models (LLMs) in text comprehension and generation have revolutionized Information Extraction (IE). One such advancement is in Document-level Relation Triplet Extraction (DocRTE), a critical task in information systems that aims to extract entities and their semantic relationships from documents. However, existing methods are primarily designed for Sentence level Relation Triplet Extraction (SentRTE), which typically handles a limited set of relations and triplet facts within a single sentence. Additionally, some approaches treat relations as candidate choices integrated into prompt templates, resulting in inefficient processing and suboptimal performance when determining the relation elements in triplets. To address these limitations, we introduce a Discriminative and Voice Aware Paradigm DiVA. DiVA involves only two steps: performing document-level relation extraction (DocRE) and then identifying the subject object entities based on the relation. No additional processing is required simply input the document to directly obtain the triplets. This streamlined process more accurately reflects real-world scenarios for triplet extraction. Our innovation lies in transforming DocRE into a discriminative task, where the model pays attention to each relation and to the often overlooked issue of active vs. passive voice within the triplet. Our experiments on the Re-DocRED and DocRED datasets demonstrate state-of-the-art results for the DocRTE task.
        ]]></description>
    </item>
    <item>
        <title>PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse</title>
        <link>https://arxiv.org/abs/2411.10087</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.10087v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Einari Vaaras, Manu Airaksinen, Okko R\"as\"anen</dc:creator>
        <description><![CDATA[
            背景：自监督学习（SSL）依赖数据固有结构学习，但常出现表征崩溃问题，阻碍其在新数据模态应用。方法：提出用于时间序列数据的SSL算法PFML，通过预测与掩码嵌入对应的输入信号统计泛函避免表征崩溃，可直接应用于不同时间序列数据领域。效果：在三种数据模态的分类任务中验证其有效性，结果显示PFML优于类似SSL方法和基于对比学习的SSL方法，与当前最优方法相当，且概念更简单、无表征崩溃问题。
            arXiv:2411.10087v3 Announce Type: replace 
Abstract: Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar SSL method and a contrastive learning-based SSL method. Additionally, PFML is on par with the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse.
        ]]></description>
    </item>
    <item>
        <title>Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</title>
        <link>https://arxiv.org/abs/2411.16260</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.16260v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fu-Chieh Chang, You-Chen Lin, Pei-Yuan Wu</dc:creator>
        <description><![CDATA[
            背景：大语言模型（LLMs）借助思维链（CoT）提示提升推理能力，但训练CoT需详细推理数据且常稀缺，STaR框架虽有成效但缺乏理论解释，且LLMs单步CoT算术能力机制不明。方法：提出LLMs通过捕捉代数结构（如交换律和单位元性质）学习算术，用自定义算术问题数据集验证。效果：实证表明LLMs能学习代数结构，理论证明特定权重和偏置配置下，基于Transformer的LLMs可生成对输入令牌排列和单位元存在保持不变的嵌入，利用代数结构可增强算术能力。
            arXiv:2411.16260v2 Announce Type: replace 
Abstract: The reasoning abilities of large language models (LLMs) have improved with chain-of-thought (CoT) prompting, allowing models to solve complex tasks stepwise. However, training CoT capabilities requires detailed reasoning data, which is often scarce. The self-taught reasoner (STaR) framework addresses this by using reinforcement learning to automatically generate reasoning steps, reducing reliance on human-labeled data. Although STaR and its variants have demonstrated empirical success, a theoretical foundation explaining these improvements is lacking. Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood. In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties. Since these structures are observable through input-output relationships, they can generalize to unseen data. We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformer-based LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements. Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance.
        ]]></description>
    </item>
    <item>
        <title>SiReRAG: Indexing Similar and Related Information for Multihop Reasoning</title>
        <link>https://arxiv.org/abs/2412.06206</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.06206v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Nan Zhang, Prafulla Kumar Choubey, Alexander Fabbri, Gabriel Bernadett-Shapiro, Rui Zhang, Prasenjit Mitra, Caiming Xiong, Chien-Sheng Wu</dc:creator>
        <description><![CDATA[
            背景：现有检索增强生成（RAG）系统的索引方法不能全面兼顾语义相似性和相关信息，在多跳推理复杂任务上表现不佳。方法：提出SiReRAG索引方法，分别构建基于递归总结的相似性树和基于命题、实体提取及分组的相关性树，并将二者索引到统一检索池。效果：在三个多跳数据集上，SiReRAG的F1分数平均提高1.9%，显著增强现有重排序方法，平均F1分数最高提升7.8%。
            arXiv:2412.06206v2 Announce Type: replace 
Abstract: Indexing is an important step towards strong performance in retrieval-augmented generation (RAG) systems. However, existing methods organize data based on either semantic similarity (similarity) or related information (relatedness), but do not cover both perspectives comprehensively. Our analysis reveals that modeling only one perspective results in insufficient knowledge synthesis, leading to suboptimal performance on complex tasks requiring multihop reasoning. In this paper, we propose SiReRAG, a novel RAG indexing approach that explicitly considers both similar and related information. On the similarity side, we follow existing work and explore some variances to construct a similarity tree based on recursive summarization. On the relatedness side, SiReRAG extracts propositions and entities from texts, groups propositions via shared entities, and generates recursive summaries to construct a relatedness tree. We index and flatten both similarity and relatedness trees into a unified retrieval pool. Our experiments demonstrate that SiReRAG consistently outperforms state-of-the-art indexing methods on three multihop datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA), with an average 1.9% improvement in F1 scores. As a reasonably efficient solution, SiReRAG enhances existing reranking methods significantly, with up to 7.8% improvement in average F1 scores. Our code is available at https://github.com/SalesforceAIResearch/SiReRAG .
        ]]></description>
    </item>
    <item>
        <title>Retrieval-Based Interleaved Visual Chain-of-Thought in Real-World Driving Scenarios</title>
        <link>https://arxiv.org/abs/2501.04671</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.04671v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Charles Corbi\`ere, Simon Roburin, Syrielle Montariol, Antoine Bosselut, Alexandre Alahi</dc:creator>
        <description><![CDATA[
            背景：思维链提示虽提升大语言模型推理能力，但在视觉语言模型（VLMs）中因过度依赖文本线索和记忆知识，效果受限。方法：引入基于驾驶理论考试的视觉问答数据集DrivingVQA，提出基于检索的交错视觉思维链方法RIV - CoT，使VLMs利用与相关实体对应的视觉裁剪进行推理。效果：相比普通思维链提示，RIV - CoT使答案准确率提高3.1%，推理准确率提高4.6%，且在更大的A - OKVQA推理数据集上表现更优。
            arXiv:2501.04671v2 Announce Type: replace 
Abstract: While chain-of-thought (CoT) prompting improves reasoning in large language models, its effectiveness in vision-language models (VLMs) remains limited due to over-reliance on textual cues and memorized knowledge. To investigate the visual reasoning capabilities of VLMs in complex real-world scenarios, we introduce DrivingVQA, a visual question answering dataset derived from driving theory exams, which contains 3,931 multiple-choice problems with expert-written explanations and grounded entities relevant to the reasoning process. Leveraging this dataset, we propose RIV-CoT, a Retrieval-Based Interleaved Visual Chain-of-Thought method that enables VLMs to reason using visual crops corresponding to these relevant entities. Our experiments demonstrate that RIV-CoT improves answer accuracy by 3.1% and reasoning accuracy by 4.6% over vanilla CoT prompting. Furthermore, we demonstrate that our method effectively scales to the larger A-OKVQA reasoning dataset by leveraging automatically generated pseudo-labels, outperforming CoT prompting.
        ]]></description>
    </item>
    <item>
        <title>Large Language Models for Knowledge Graph Embedding: A Survey</title>
        <link>https://arxiv.org/abs/2501.07766</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.07766v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Bingchen Liu, Yuanyuan Fang, Naixing Xu, Shihao Hou, Xin Li, Qian Li</dc:creator>
        <description><![CDATA[
            背景：大语言模型在知识驱动应用中表现出色，其强大能力使其逐渐应用于知识图谱嵌入（KGE）任务。传统KGE方法将实体和关系映射到低维向量空间。方法：本文研究了不同类型KGE场景中执行大语言模型相关任务的多种方法，并对各KGE场景进行分类总结。效果：虽未提及定量效果，但通过总结对比不同方法，探讨了其主要应用，并为该研究领域发展给出前瞻性方向。 
            arXiv:2501.07766v2 Announce Type: replace 
Abstract: Large language models (LLMs) have garnered significant attention for their superior performance in many knowledge-driven applications on the world wide web.These models are designed to train hundreds of millions or more parameters on large amounts of text data, enabling them to understand and generate naturallanguage effectively. As the superior performance of LLMs becomes apparent,they are increasingly being applied to knowledge graph embedding (KGE) related tasks to improve the processing results. Traditional KGE representation learning methods map entities and relations into a low-dimensional vector space, enablingthe triples in the knowledge graph to satisfy a specific scoring function in thevector space. However, based on the powerful language understanding and seman-tic modeling capabilities of LLMs, that have recently been invoked to varying degrees in different types of KGE related scenarios such as multi-modal KGE andopen KGE according to their task characteristics. In this paper, we investigate awide range of approaches for performing LLMs-related tasks in different types of KGE scenarios. To better compare the various approaches, we summarize each KGE scenario in a classification. Finally, we discuss the applications in which the methods are mainly used and suggest several forward-looking directions for the development of this new research area.
        ]]></description>
    </item>
    <item>
        <title>RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry</title>
        <link>https://arxiv.org/abs/2502.03251</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.03251v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Li Sun, Zhenhao Huang, Suyang Zhou, Qiqi Wan, Hao Peng, Philip Yu</dc:creator>
        <description><![CDATA[
            背景：基础模型带来AI新时代，但图神经网络泛化能力不足，现有图基础模型研究多聚焦文本属性图，且为大模型定制的图描述忽略了结构复杂性。方法：提出通用预训练模型RiemannGFM，发现树和循环的结构词汇，构建新的乘积丛整合词汇的不同几何结构，在黎曼流形上堆叠黎曼层学习结构词汇。效果：大量实验表明，RiemannGFM在多种真实图数据上有效。 
            arXiv:2502.03251v2 Announce Type: replace 
Abstract: The foundation model has heralded a new era in artificial intelligence, pretraining a single model to offer cross-domain transferability on different datasets. Graph neural networks excel at learning graph data, the omnipresent non-Euclidean structure, but often lack the generalization capacity. Hence, graph foundation model is drawing increasing attention, and recent efforts have been made to leverage Large Language Models. On the one hand, existing studies primarily focus on text-attributed graphs, while a wider range of real graphs do not contain fruitful textual attributes. On the other hand, the sequential graph description tailored for the Large Language Model neglects the structural complexity, which is a predominant characteristic of the graph. Such limitations motivate an important question: Can we go beyond Large Language Models, and pretrain a universal model to learn the structural knowledge for any graph? The answer in the language or vision domain is a shared vocabulary. We observe the fact that there also exist shared substructures underlying graph domain, and thereby open a new opportunity of graph foundation model with structural vocabulary. The key innovation is the discovery of a simple yet effective structural vocabulary of trees and cycles, and we explore its inherent connection to Riemannian geometry. Herein, we present a universal pretraining model, RiemannGFM. Concretely, we first construct a novel product bundle to incorporate the diverse geometries of the vocabulary. Then, on this constructed space, we stack Riemannian layers where the structural vocabulary, regardless of specific graph, is learned in Riemannian manifold offering cross-domain transferability. Extensive experiments show the effectiveness of RiemannGFM on a diversity of real graphs.
        ]]></description>
    </item>
    <item>
        <title>Diversity Enhances an LLM's Performance in RAG and Long-context Task</title>
        <link>https://arxiv.org/abs/2502.09017</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.09017v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhichao Wang, Bin Bi, Yanqi Luo, Sitaram Asur, Claire Na Cheng</dc:creator>
        <description><![CDATA[
            背景：大语言模型发展中，自注意力机制的二次时间复杂度导致上下文窗口受限，影响问答中的检索增强生成（RAG）和长文本摘要等任务，传统选相似内容的方法有冗余问题。方法：基于最大边际相关性（MMR）和最远点采样（FPS）原则，在内容选择过程中融入多样性。效果：在基于大语言模型的问答和摘要任务前，融入多样性显著提高了相关句子或段落选择的召回率，有助于提升摘要和问答效果。
            arXiv:2502.09017v2 Announce Type: replace 
Abstract: The rapid advancements in large language models (LLMs) have highlighted the challenge of context window limitations, primarily due to the quadratic time complexity of the self-attention mechanism (\(O(N^2)\), where \(N\) denotes the context window length). This constraint impacts tasks such as retrieval-augmented generation (RAG) in question answering (Q\&amp;A) and long context summarization. A common approach involves selecting content with the highest similarity to the query; however, this often leads to redundancy and the exclusion of diverse yet relevant information. Building on principles from Maximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we integrate diversity into the content selection process. Our findings reveal that incorporating diversity substantially increases the recall of selecting relevant sentences or chunks before LLM-based Q\&amp;A and summarization. These results highlight the importance of maintaining diversity in future LLM applications to further improve summarization and Q\&amp;A outcomes.
        ]]></description>
    </item>
    <item>
        <title>On the Consistency of Multilingual Context Utilization in Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2504.00597</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.00597v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jirui Qi, Raquel Fern\'andez, Arianna Bisazza</dc:creator>
        <description><![CDATA[
            背景：多语言检索增强生成（mRAG）中，大语言模型（LLM）有效利用多语言信息存在挑战，且其利用不同多语言上下文生成准确答案的程度研究不足。方法：对LLM在多语言问答任务中利用相关段落、用预期语言回复及聚焦相关段落的能力进行评估。效果：实验表明，LLM有从不同语言段落提取相关信息的能力，但用正确语言完整作答能力较弱，干扰段落会降低答案质量，查询语言的干扰项影响稍大。
            arXiv:2504.00597v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) with large language models (LLMs) has demonstrated strong performance in multilingual question-answering (QA) tasks by leveraging relevant passages retrieved from corpora. In multilingual RAG (mRAG), the retrieved passages can be written in languages other than that of the query entered by the user, making it challenging for LLMs to effectively utilize the provided information. Recent research suggests that retrieving passages from multilingual corpora can improve RAG performance, particularly for low-resource languages. However, the extent to which LLMs can leverage different kinds of multilingual contexts to generate accurate answers, *independently from retrieval quality*, remains understudied. In this paper, we conduct an extensive assessment of LLMs' ability to (i) make consistent use of a relevant passage regardless of its language, (ii) respond in the expected language, and (iii) focus on the relevant passage even when multiple `distracting' passages in different languages are provided in the context. Our experiments with four LLMs across three QA datasets covering a total of 48 languages reveal a surprising ability of LLMs to extract the relevant information from out-language passages, but a much weaker ability to formulate a full answer in the correct language. Our analysis, based on both accuracy and feature attribution techniques, further shows that distracting passages negatively impact answer quality regardless of their language. However, distractors in the query language exert a slightly stronger influence. Taken together, our findings deepen the understanding of how LLMs utilize context in mRAG systems, providing directions for future improvements.
        ]]></description>
    </item>
    <item>
        <title>ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs</title>
        <link>https://arxiv.org/abs/2504.01698</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.01698v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yi-Long Lu, Chunhui Zhang, Jiajun Song, Lifeng Fan, Wei Wang</dc:creator>
        <description><![CDATA[
            背景：规则强化学习（RL）在大模型结构化推理任务中表现出色，但在社会推理尤其是心智理论（ToM）方面效果待探索。方法：研究表明RL能在小规模大模型中解锁ToM推理能力，用含3200个问题的数据集训练。效果：7B参数模型在Hi - ToM基准测试中准确率达84.50%，超GPT - 4o和DeepSeek - v3。小模型推理易崩溃，大模型能稳定表现，且模型对高阶、分布外ToM问题等有强泛化能力。
            arXiv:2504.01698v2 Announce Type: replace 
Abstract: Recent advancements in rule-based reinforcement learning (RL), applied during the post-training phase of large language models (LLMs), have significantly enhanced their capabilities in structured reasoning tasks such as mathematics and logical inference. However, the effectiveness of RL in social reasoning, particularly in Theory of Mind (ToM), the ability to infer others' mental states, remains largely unexplored. In this study, we demonstrate that RL methods effectively unlock ToM reasoning capabilities even in small-scale LLMs (0.5B to 7B parameters). Using a modest dataset comprising 3200 questions across diverse scenarios, our RL-trained 7B model achieves 84.50\% accuracy on the Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite significantly fewer parameters. While smaller models ($\leq$3B parameters) suffer from reasoning collapse, larger models (7B parameters) maintain stable performance through consistent belief tracking. Additionally, our RL-based models demonstrate robust generalization to higher-order, out-of-distribution ToM problems, novel textual presentations, and previously unseen datasets. These findings highlight RL's potential to enhance social cognitive reasoning, bridging the gap between structured problem-solving and nuanced social inference in LLMs.
        ]]></description>
    </item>
    <item>
        <title>DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction</title>
        <link>https://arxiv.org/abs/2408.13378</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2408.13378v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yoshitaka Inoue, Tianci Song, Xinling Wang, Augustin Luna, Tianfan Fu</dc:creator>
        <description><![CDATA[
            背景：大语言模型在多视角场景下回答准确性受限，药物-靶点相互作用（DTI）预测现有方法因生物系统复杂和缺乏可解释性面临挑战。方法：提出DrugAgent多智能体大语言模型系统，采用基于协调器的架构，集成特定领域数据源，结合思维链和ReAct框架进行透明推理。效果：在激酶抑制剂数据集实验中，多智能体大语言模型方法F1分数比非推理多智能体模型高45%（0.514 vs 0.355），能提供可解释推理。
            arXiv:2408.13378v4 Announce Type: replace-cross 
Abstract: Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces. Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. While drug-target interaction (DTI) prediction is important for drug discovery, existing approaches face challenges due to complex biological systems and the lack of interpretability needed for clinical applications. DrugAgent is a multi-agent LLM system for DTI prediction that combines multiple specialized perspectives with transparent reasoning. Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning. We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we demonstrated the contributions of each agent, with the AI agent being the most impactful, followed by the KG agent and search agent. Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance. Code is available at https://anonymous.4open.science/r/DrugAgent-B2EA.
        ]]></description>
    </item>
    <item>
        <title>A Materials Map Integrating Experimental and Computational Data via Graph-Based Machine Learning for Enhanced Materials Discovery</title>
        <link>https://arxiv.org/abs/2503.07378</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.07378v5</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yusuke Hashimoto, Xue Jia, Hao Li, Takaaki Tomai</dc:creator>
        <description><![CDATA[
            背景：材料信息学结合材料科学与数据科学，有望加速材料开发与发现，但计算和实验数据的整合仍具挑战。方法：利用先前整合的数据集，通过MatDeepLearn框架构建材料图谱，该框架用基于图的材料结构表示和深度学习模型预测材料属性。效果：统计分析表明，使用消息传递神经网络架构的框架能有效提取反映材料结构复杂性的特征，但不一定能提高材料属性预测的准确性，这或归因于网络的高学习性能可助力数据点结构化。
            arXiv:2503.07378v5 Announce Type: replace-cross 
Abstract: Materials informatics (MI), emerging from the integration of materials science and data science, is expected to significantly accelerate material development and discovery. The data used in MI are derived from both computational and experimental studies; however, their integration remains challenging. In our previous study, we reported the integration of these datasets by applying a machine learning model that is trained on the experimental dataset to the compositional data stored in the computational database. In this study, we use the obtained datasets to construct materials maps, which visualize the relationships between material properties and structural features, aiming to support experimental researchers. The materials map is constructed using the MatDeepLearn (MDL) framework, which implements materials property prediction using graph-based representations of material structure and deep learning modeling. Through statistical analysis, we find that the MDL framework using the message passing neural network (MPNN) architecture efficiently extracts features reflecting the structural complexity of materials. Moreover, we find that this advantage does not necessarily translate into improved accuracy in the prediction of material properties. We attribute this unexpected outcome to the high learning performance inherent in MPNN, which can contribute to the structuring of data points within the materials map.
        ]]></description>
    </item>
    <item>
        <title>Crowdsourcing-Based Knowledge Graph Construction for Drug Side Effects Using Large Language Models with an Application on Semaglutide</title>
        <link>https://arxiv.org/abs/2504.04346</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.04346v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhijie Duan, Kai Wei, Zhaoqian Xue, Jiayan Zhou, Shu Yang, Siyuan Ma, Jin Jin, Lingyao li</dc:creator>
        <description><![CDATA[
            背景：从非结构化且嘈杂的社交媒体内容中挖掘药物副作用数据是一项挑战。方法：提出利用大语言模型（LLMs）从社交媒体提取药物副作用信息并构建知识图谱（KG）的系统框架，以Reddit上的司美格鲁肽减肥数据为例进行应用。效果：通过构建的KG分析不同品牌司美格鲁肽随时间报告的副作用，并与FAERS数据库对比验证，证明了用LLMs将社交媒体数据转化为结构化KG用于药物警戒的可行性。
            arXiv:2504.04346v2 Announce Type: replace-cross 
Abstract: Social media is a rich source of real-world data that captures valuable patient experience information for pharmacovigilance. However, mining data from unstructured and noisy social media content remains a challenging task. We present a systematic framework that leverages large language models (LLMs) to extract medication side effects from social media and organize them into a knowledge graph (KG). We apply this framework to semaglutide for weight loss using data from Reddit. Using the constructed knowledge graph, we perform comprehensive analyses to investigate reported side effects across different semaglutide brands over time. These findings are further validated through comparison with adverse events reported in the FAERS database, providing important patient-centered insights into semaglutide's side effects that complement its safety profile and current knowledge base of semaglutide for both healthcare professionals and patients. Our work demonstrates the feasibility of using LLMs to transform social media data into structured KGs for pharmacovigilance.
        ]]></description>
    </item>
    <item>
        <title>Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG</title>
        <link>https://arxiv.org/abs/2504.05220</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05220v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hengran Zhang, Minghao Tang, Keping Bi, Jiafeng Guo, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng</dc:creator>
        <description><![CDATA[
            背景：检索模型训练和评估依赖人工标注，成本高，现有利用大语言模型（LLM）标注的方法存在不足。方法：研究利用LLM进行以效用为中心的标注用于大规模检索器训练数据，设计新损失函数Disj - InfoNCE以降低低质量正样本影响。效果：在跨领域设置中，基于效用标注训练的检索器在检索和RAG任务上显著优于基于人工标注训练的；在同领域设置中，结合20%人工标注数据，基于效用标注训练的检索器可达到全人工标注训练模型的性能。
            arXiv:2504.05220v2 Announce Type: replace-cross 
Abstract: Retrieval models typically rely on costly human-labeled query-document relevance annotations for training and evaluation. To reduce this cost and leverage the potential of Large Language Models (LLMs) in relevance judgments, we aim to explore whether LLM-generated annotations can effectively replace human annotations in training retrieval models. Retrieval usually emphasizes relevance, which indicates "topic-relatedness" of a document to a query, while in RAG, the value of a document (or utility) depends on how it contributes to answer generation. Recognizing this mismatch, some researchers use LLM performance on downstream tasks with documents as labels, but this approach requires manual answers for specific tasks, leading to high costs and limited generalization. In another line of work, prompting LLMs to select useful documents as RAG references eliminates the need for human annotation and is not task-specific. If we leverage LLMs' utility judgments to annotate retrieval data, we may retain cross-task generalization without human annotation in large-scale corpora. Therefore, we investigate utility-focused annotation via LLMs for large-scale retriever training data across both in-domain and out-of-domain settings on the retrieval and RAG tasks. To reduce the impact of low-quality positives labeled by LLMs, we design a novel loss function, i.e., Disj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on utility-focused annotations significantly outperform those trained on human annotations in the out-of-domain setting on both tasks, demonstrating superior generalization capabilities. (2) LLM annotation does not replace human annotation in the in-domain setting. However, incorporating just 20% human-annotated data enables retrievers trained with utility-focused annotations to match the performance of models trained entirely with human annotations.
        ]]></description>
    </item>
    <item>
        <title>Of All StrIPEs: Investigating Structure-informed Positional Encoding for Efficient Music Generation</title>
        <link>https://arxiv.org/abs/2504.05364</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05364v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Manvi Agarwal (LTCI), Changhong Wang (LTCI), Gael Richard (S2A, IDS)</dc:creator>
        <description><![CDATA[
            背景：音乐对Transformer等生成模型是挑战领域，虽插入结构信息与用随机傅里叶特征降低计算成本的方法有效，但基于随机傅里叶特征的位置编码（PE）与基于旋转矩阵的PE对比不明。方法：提出基于核方法的统一框架分析两类高效PE，开发新PE方法RoPEPool提取时间序列因果关系。效果：在旋律和声化任务中，RoPEPool结合高信息结构先验，性能超过所有对比方法。 
            arXiv:2504.05364v1 Announce Type: new 
Abstract: While music remains a challenging domain for generative models like Transformers, a two-pronged approach has recently proved successful: inserting musically-relevant structural information into the positional encoding (PE) module and using kernel approximation techniques based on Random Fourier Features (RFF) to lower the computational cost from quadratic to linear. Yet, it is not clear how such RFF-based efficient PEs compare with those based on rotation matrices, such as Rotary Positional Encoding (RoPE). In this paper, we present a unified framework based on kernel methods to analyze both families of efficient PEs. We use this framework to develop a novel PE method called RoPEPool, capable of extracting causal relationships from temporal sequences. Using RFF-based PEs and rotation-based PEs, we demonstrate how seemingly disparate PEs can be jointly studied by considering the content-context interactions they induce. For empirical validation, we use a symbolic music generation task, namely, melody harmonization. We show that RoPEPool, combined with highly-informative structural priors, outperforms all methods.
        ]]></description>
    </item>
    <item>
        <title>SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding</title>
        <link>https://arxiv.org/abs/2504.05576</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05576v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mingfei Chen, Israel D. Gebru, Ishwarya Ananthabhotla, Christian Richardt, Dejan Markovic, Jake Sandakly, Steven Krenn, Todd Keebler, Eli Shlizerman, Alexander Richard</dc:creator>
        <description><![CDATA[
            背景：现有合成任意场景新视角环境声音的方法存在需声源细节约束和先验知识等问题。方法：提出SoundVista方法，利用有限已知录音学习分布式麦克风信号与目标视点信号间的声学传递函数，引入视觉 - 声学绑定模块，从全景RGB和深度数据学习与局部声学特性相关的视觉嵌入，优化参考麦克风放置，合成时根据目标视点获取自适应权重。效果：在公开数据和真实场景中表现优于现有方法。
            arXiv:2504.05576v1 Announce Type: new 
Abstract: We introduce SoundVista, a method to generate the ambient sound of an arbitrary scene at novel viewpoints. Given a pre-acquired recording of the scene from sparsely distributed microphones, SoundVista can synthesize the sound of that scene from an unseen target viewpoint. The method learns the underlying acoustic transfer function that relates the signals acquired at the distributed microphones to the signal at the target viewpoint, using a limited number of known recordings. Unlike existing works, our method does not require constraints or prior knowledge of sound source details. Moreover, our method efficiently adapts to diverse room layouts, reference microphone configurations and unseen environments. To enable this, we introduce a visual-acoustic binding module that learns visual embeddings linked with local acoustic properties from panoramic RGB and depth data. We first leverage these embeddings to optimize the placement of reference microphones in any given scene. During synthesis, we leverage multiple embeddings extracted from reference locations to get adaptive weights for their contribution, conditioned on target viewpoint. We benchmark the task on both publicly available data and real-world settings. We demonstrate significant improvements over existing methods.
        ]]></description>
    </item>
    <item>
        <title>TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis</title>
        <link>https://arxiv.org/abs/2504.05684</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05684v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tri Ton, Ji Woo Hong, Chang D. Yoo</dc:creator>
        <description><![CDATA[
            背景：现有视频到音频合成方法在同步性和音频质量上有待提升。方法：提出TARO框架，基于流基变压器，有两项创新，一是时间步自适应表示对齐（TRA），根据噪声调度调整对齐强度以动态对齐潜在表示；二是起始感知调节（OAC），集成起始线索增强与动态视觉事件的同步。效果：在VGGSound和Landscape数据集上实验，TARO优于先前方法，弗雷歇距离降低53%，弗雷歇音频距离降低29%，对齐准确率达97.19%。
            arXiv:2504.05684v1 Announce Type: new 
Abstract: This paper introduces Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning (TARO), a novel framework for high-fidelity and temporally coherent video-to-audio synthesis. Built upon flow-based transformers, which offer stable training and continuous transformations for enhanced synchronization and audio quality, TARO introduces two key innovations: (1) Timestep-Adaptive Representation Alignment (TRA), which dynamically aligns latent representations by adjusting alignment strength based on the noise schedule, ensuring smooth evolution and improved fidelity, and (2) Onset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp event-driven markers of audio-relevant visual moments to enhance synchronization with dynamic visual events. Extensive experiments on the VGGSound and Landscape datasets demonstrate that TARO outperforms prior methods, achieving relatively 53\% lower Frechet Distance (FD), 29% lower Frechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its superior audio quality and synchronization precision.
        ]]></description>
    </item>
    <item>
        <title>STAGE: Stemmed Accompaniment Generation through Prefix-Based Conditioning</title>
        <link>https://arxiv.org/abs/2504.05690</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05690v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Giorgio Strano, Chiara Ballanti, Donato Crisostomi, Michele Mancusi, Luca Cosmo, Emanuele Rodol\`a</dc:creator>
        <description><![CDATA[
            背景：现有生成模型多专注于从头创作音乐，难以融入人类迭代创作流程。方法：提出STAGE模型，基于MusicGen微调，受语言模型指令微调启发，通过扩展嵌入矩阵添加上下文令牌，实现基于前缀条件生成单音轨乐器伴奏。效果：相比基线模型，生成的伴奏与输入混音更连贯、音频质量更高、与文本提示更契合，且支持节拍约束生成，与目标节奏结构对齐达最优水平，为交互式音乐创作提供实用工具。
            arXiv:2504.05690v1 Announce Type: new 
Abstract: Recent advances in generative models have made it possible to create high-quality, coherent music, with some systems delivering production-level output.Yet, most existing models focus solely on generating music from scratch, limiting their usefulness for musicians who want to integrate such models into a human, iterative composition workflow.In this paper we introduce STAGE, our STemmed Accompaniment GEneration model, fine-tuned from the state-of-the-art MusicGen to generate single-stem instrumental accompaniments conditioned on a given mixture. Inspired by instruction-tuning methods for language models, we extend the transformer's embedding matrix with a context token, enabling the model to attend to a musical context through prefix-based conditioning.Compared to the baselines, STAGE yields accompaniments that exhibit stronger coherence with the input mixture, higher audio quality, and closer alignment with textual prompts.Moreover, by conditioning on a metronome-like track, our framework naturally supports tempo-constrained generation, achieving state-of-the-art alignment with the target rhythmic structure--all without requiring any additional tempo-specific module.As a result, STAGE offers a practical, versatile tool for interactive music creation that can be readily adopted by musicians in real-world workflows.
        ]]></description>
    </item>
    <item>
        <title>DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method</title>
        <link>https://arxiv.org/abs/2411.12363</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.12363v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zihao Chen, Zhentao Lin, Bi Zeng, Linyi Huang, Zhi Li, Jia Cai</dc:creator>
        <description><![CDATA[
            背景：为保障语音系统在不同环境可靠运行，现有噪声添加方法对真实噪声场景覆盖有限，且依赖已有场景信息和噪声。方法：提出基于提示的动态生成场景噪声添加方法（DGSNA），将场景信息动态生成（DGSI）与语音场景噪声添加（SNAS）结合，把干净语音转换到不同噪声环境实现自动场景噪声添加。效果：实验表明，DGSNA显著提升语音识别和关键词检测模型在不同噪声条件下的鲁棒性，相对提升最高达11.21%，还可与其他方法结合提升性能。
            arXiv:2411.12363v3 Announce Type: replace 
Abstract: To ensure the reliable operation of speech systems across diverse environments, noise addition methods have emerged as the prevailing solution. However, existing methods offer limited coverage of real-world noisy scenes and depend on pre-existing scene-based information and noise. This paper presents prompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel noise addition methodology that integrates Dynamic Generation of Scene-based Information (DGSI) with Scene-based Noise Addition for Speech (SNAS). This integration facilitates automated scene-based noise addition by transforming clean speech into various noise environments, thereby providing a more comprehensive and realistic simulation of diverse noise conditions. Experimental results demonstrate that DGSNA significantly enhances the robustness of speech recognition and keyword spotting models across various noise conditions, achieving a relative improvement of up to 11.21%. Furthermore, DGSNA can be effectively integrated with other noise addition methods to enhance performance. Our implementation and demonstrations are available at https://dgsna.github.io.
        ]]></description>
    </item>
    <item>
        <title>Accompaniment Prompt Adherence: A Measure for Evaluating Music Accompaniment Systems</title>
        <link>https://arxiv.org/abs/2503.06346</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.06346v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Maarten Grachten, Javier Nistal</dc:creator>
        <description><![CDATA[
            背景：音乐伴奏生成系统发展迅速，但缺乏评估生成结果与条件音频提示契合度的标准指标。方法：提出基于分布的“伴奏提示契合度”（APA）指标，并通过合成数据扰动的客观实验和人类听力测试进行验证。效果：结果表明，APA与人类对契合度的判断高度一致，且能有效区分降低契合度的变换。此外，还发布了基于预训练CLAP嵌入模型的Python实现，为评估和比较伴奏生成系统提供了有力工具。
            arXiv:2503.06346v2 Announce Type: replace 
Abstract: Generative systems of musical accompaniments are rapidly growing, yet there are no standardized metrics to evaluate how well generations align with the conditional audio prompt. We introduce a distribution-based measure called "Accompaniment Prompt Adherence" (APA), and validate it through objective experiments on synthetic data perturbations, and human listening tests. Results show that APA aligns well with human judgments of adherence and is discriminative to transformations that degrade adherence. We release a Python implementation of the metric using the widely adopted pre-trained CLAP embedding model, offering a valuable tool for evaluating and comparing accompaniment generation systems.
        ]]></description>
    </item>
    <item>
        <title>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and Generative Adversarial Networks</title>
        <link>https://arxiv.org/abs/2503.12936</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.12936v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</dc:creator>
        <description><![CDATA[
            现有远场语音增强模型在真实场景泛化能力有限。本文针对低信噪比、高混响和中高频衰减的真实数据，重新研究单通道远场到近场语音增强任务。提出FNSE - SBGAN框架，将基于薛定谔桥的扩散模型与生成对抗网络相结合。该方法在多项指标和主观评估中达最优，与远场信号相比，字符错误率最高降低14.58%。此外，引入基于时频域矩阵秩分析的评估框架，为模型性能提供系统见解。
            arXiv:2503.12936v2 Announce Type: replace 
Abstract: The prevailing method for neural speech enhancement predominantly utilizes fully-supervised deep learning with simulated pairs of far-field noisy-reverberant speech and clean speech. Nonetheless, these models frequently demonstrate restricted generalizability to mixtures recorded in real-world conditions. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a novel framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce a novel evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods.
        ]]></description>
    </item>
    <item>
        <title>LoopGen: Training-Free Loopable Music Generation</title>
        <link>https://arxiv.org/abs/2504.04466</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.04466v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Davide Marincione, Giorgio Strano, Donato Crisostomi, Roberto Ribuoli, Emanuele Rodol\`a</dc:creator>
        <description><![CDATA[
            背景：循环音频是许多音乐流派的核心，但现有生成音乐模型难以生成真正可循环的音频，常出现可听的不连续性。方法：修改非自回归模型MAGNeT，使其以循环模式生成令牌，在创建音频结尾时关注开头，采用仅推理的方法，无需额外训练或数据。效果：通过计算循环接缝处的令牌困惑度，循环过渡一致性提升55%；盲听测试显示，相比基线方法感知效果显著提升，平均评分提高70%。
            arXiv:2504.04466v2 Announce Type: replace 
Abstract: Loops--short audio segments designed for seamless repetition--are central to many music genres, particularly those rooted in dance and electronic styles. However, current generative music models struggle to produce truly loopable audio, as generating a short waveform alone does not guarantee a smooth transition from its endpoint back to its start, often resulting in audible discontinuities. Loops--short audio segments designed for seamless repetition--are central to many music genres, particularly those rooted in dance and electronic styles. However, current generative music models struggle to produce truly loopable audio, as generating a short waveform alone does not guarantee a smooth transition from its endpoint back to its start, often resulting in audible discontinuities. We address this gap by modifying a non-autoregressive model (MAGNeT) to generate tokens in a circular pattern, letting the model attend to the beginning of the audio when creating its ending. This inference-only approach results in generations that are aware of future context and loop naturally, without the need for any additional training or data. We evaluate the consistency of loop transitions by computing token perplexity around the seam of the loop, observing a 55% improvement. Blind listening tests further confirm significant perceptual gains over baseline methods, improving mean ratings by 70%. Taken together, these results highlight the effectiveness of inference-only approaches in improving generative models and underscore the advantages of non-autoregressive methods for context-aware music generation.
        ]]></description>
    </item>
    <item>
        <title>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</title>
        <link>https://arxiv.org/abs/2412.15322</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.15322v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, Yuki Mitsufuji</dc:creator>
        <description><![CDATA[
            背景：为解决仅基于视频数据单模态训练的局限，实现高质量同步音频合成。方法：提出多模态联合训练框架MMAudio，利用大规模文本 - 音频数据联合训练，以学习生成语义对齐的高质量音频样本；用条件同步模块提升音视频同步性，按流匹配目标训练。效果：在音频质量、语义对齐和音视频同步方面达公开模型最优，推理时间短（生成8s片段仅1.23s），参数量仅1.57亿，文本到音频生成表现也具竞争力。
            arXiv:2412.15322v2 Announce Type: replace-cross 
Abstract: We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. Code and demo are available at: https://hkchengrex.github.io/MMAudio
        ]]></description>
    </item>
</channel>
</rss>