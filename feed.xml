<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 05 Jun 2025 12:32:45 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Thu, 05 Jun 2025 12:32:45 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World</title>
        <link>https://arxiv.org/abs/2506.03155</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03155v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yu Zheng</dc:creator>
        <description><![CDATA[
            背景：现实物理环境复杂，单一信息采集方式难以建模，需融合多源多模态数据，但新问题往往数据不足，需跨领域知识融合，而现有研究多聚焦单领域。方法：定义跨领域多模态数据融合问题，提出包含领域、链接、模型和数据四层的框架，解答融合内容、原因和方式。效果：可将不同结构、分辨率、规模和分布的数据转化为一致表征，有效设计端到端解决方案以解决现实问题。
            arXiv:2506.03155v1 Announce Type: new 
Abstract: The proliferation of artificial intelligence has enabled a diversity of applications that bridge the gap between digital and physical worlds. As physical environments are too complex to model through a single information acquisition approach, it is crucial to fuse multimodal data generated by different sources, such as sensors, devices, systems, and people, to solve a problem in the real world. Unfortunately, it is neither applicable nor sustainable to deploy new resources to collect original data from scratch for every problem. Thus, when data is inadequate in the domain of problem, it is vital to fuse knowledge from multimodal data that is already available in other domains. We call this cross-domain knowledge fusion. Existing research focus on fusing multimodal data in a single domain, supposing the knowledge from different datasets is intrinsically aligned; however, this assumption may not hold in the scenarios of cross-domain knowledge fusion. In this paper, we formally define the cross-domain multimodal data fusion problem, discussing its unique challenges, differences and advantages beyond data fusion in a single domain. We propose a four-layer framework, consisting of Domains, Links, Models and Data layers, answering three key questions: "what to fuse", "why can be fused", and "how to fuse". The Domains Layer selects relevant data from different domains for a given problem. The Links Layer reveals the philosophy of knowledge alignment beyond specific model structures. The Models Layer provides two knowledge fusion paradigms based on the fundamental mechanisms for processing data. The Data Layer turns data of different structures, resolutions, scales and distributions into a consistent representation that can be fed into an AI model. With this framework, we can design end-to-end solutions that fuse cross-domain multimodal data effectively for solving real-world problems.
        ]]></description>
    </item>
    <item>
        <title>DUAL: Dynamic Uncertainty-Aware Learning</title>
        <link>https://arxiv.org/abs/2506.03158</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03158v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiahao Qin, Bei Peng, Feng Liu, Guangliang Cheng, Lu Zong</dc:creator>
        <description><![CDATA[
            背景：深度学习模型在多模态等场景常面临特征不确定性问题，影响性能与可靠性。方法：提出动态不确定性感知学习（DUAL）统一框架，有动态特征不确定性建模、自适应分布感知调制、不确定性感知跨模态关系学习三项创新。效果：在计算机视觉任务中，CIFAR - 10、CIFAR - 100、Tiny - ImageNet准确率分别提升7.1%、6.5%、2.3%；在多模态学习中，CMU - MOSEI、CMU - MOSI情感分析准确率分别提升4.1%、2.8%，MISR准确率提升1.4%。
            arXiv:2506.03158v1 Announce Type: new 
Abstract: Deep learning models frequently encounter feature uncertainty in diverse learning scenarios, significantly impacting their performance and reliability. This challenge is particularly complex in multi-modal scenarios, where models must integrate information from different sources with inherent uncertainties. We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that effectively handles feature uncertainty in both single-modal and multi-modal scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty Modeling, which continuously refines uncertainty estimates through joint consideration of feature characteristics and learning dynamics; Adaptive Distribution-Aware Modulation, which maintains balanced feature distributions through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal Relationship Learning, which explicitly models uncertainties in cross-modal interactions. Through extensive experiments, we demonstrate DUAL's effectiveness across multiple domains: in computer vision tasks, it achieves substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements on MISR. The code will be available on GitHub soon.
        ]]></description>
    </item>
    <item>
        <title>Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes</title>
        <link>https://arxiv.org/abs/2506.03160</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03160v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shriyank Somvanshi, Anannya Ghosh Tusti, Mahmuda Sultana Mimi, Md Monzurul Islam, Sazzad Bin Bashar Polock, Anandi Dutta, Subasish Das</dc:creator>
        <description><![CDATA[
            背景：自动驾驶车辆增多，现有方法在识别碰撞中SAE自动化等级时存在不足。方法：该研究用结构化碰撞数据，评估MambaAttention、TabPFN和TabTransformer三种表格深度学习模型对SAE自动化等级的分类效果，对数据进行类平衡处理后在统一数据集上训练和评估。效果：MambaAttention整体表现最佳，不同等级F1分数较高；TabPFN在零样本推理中表现出色；TabTransformer表现欠佳。结果表明定制的深度学习模型可提升自动化等级分类的准确性和效率。
            arXiv:2506.03160v1 Announce Type: new 
Abstract: The increasing presence of automated vehicles (AVs) presents new challenges for crash classification and safety analysis. Accurately identifying the SAE automation level involved in each crash is essential to understanding crash dynamics and system accountability. However, existing approaches often overlook automation-specific factors and lack model sophistication to capture distinctions between different SAE levels. To address this gap, this study evaluates the performance of three advanced tabular deep learning models MambaAttention, TabPFN, and TabTransformer for classifying SAE automation levels using structured crash data from Texas (2024), covering 4,649 cases categorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level 2), and Advanced Automation (SAE Levels 3-5 combined). Following class balancing using SMOTEENN, the models were trained and evaluated on a unified dataset of 7,300 records. MambaAttention demonstrated the highest overall performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5), while TabPFN excelled in zero-shot inference with high robustness for rare crash categories. In contrast, TabTransformer underperformed, particularly in detecting Partial Automation crashes (F1-score: 55%), suggesting challenges in modeling shared human-system control dynamics. These results highlight the capability of deep learning models tailored for tabular data to enhance the accuracy and efficiency of automation-level classification. Integrating such models into crash analysis frameworks can support policy development, AV safety evaluation, and regulatory decisions, especially in distinguishing high-risk conditions for mid- and high-level automation technologies.
        ]]></description>
    </item>
    <item>
        <title>Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs</title>
        <link>https://arxiv.org/abs/2506.03168</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03168v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Dawen Jiang, Zhishu Shen, Qiushi Zheng, Tiehua Zhang, Wei Xiang, Jiong Jin</dc:creator>
        <description><![CDATA[
            背景：全球人口增长和气候变化下，传统农业物联网系统面临数字转型挑战，智能农业在多模态数据融合等方面存在难题。方法：提出以边缘为中心的多模态农业物联网数据分析框架Farm - LightSeek，集成大语言模型与边缘计算，通过传感器收集多源数据，在边缘节点进行跨模态推理和疾病检测等。效果：在两个真实数据集实验表明，即使在边缘计算资源受限情况下，该框架在关键任务中能取得可靠性能，推动了智能实时农业解决方案发展。
            arXiv:2506.03168v1 Announce Type: new 
Abstract: Amid the challenges posed by global population growth and climate change, traditional agricultural Internet of Things (IoT) systems is currently undergoing a significant digital transformation to facilitate efficient big data processing. While smart agriculture utilizes artificial intelligence (AI) technologies to enable precise control, it still encounters significant challenges, including excessive reliance on agricultural expert knowledge, difficulties in fusing multimodal data, poor adaptability to dynamic environments, and bottlenecks in real-time decision-making at the edge. Large language models (LLMs), with their exceptional capabilities in knowledge acquisition and semantic understanding, provide a promising solution to address these challenges. To this end, we propose Farm-LightSeek, an edge-centric multimodal agricultural IoT data analytics framework that integrates LLMs with edge computing. This framework collects real-time farmland multi-source data (images, weather, geographic information) via sensors, performs cross-modal reasoning and disease detection at edge nodes, conducts low-latency management decisions, and enables cloud collaboration for model updates. The main innovations of Farm-LightSeek include: (1) an agricultural "perception-decision-action" closed-loop architecture; (2) cross-modal adaptive monitoring; and (3)a lightweight LLM deployment strategy balancing performance and efficiency. Experiments conducted on two real-world datasets demonstrate that Farm-LightSeek consistently achieves reliable performance in mission-critical tasks, even under the limitations of edge computing resources. This work advances intelligent real-time agricultural solutions and highlights the potential for deeper integration of agricultural IoT with LLMs.
        ]]></description>
    </item>
    <item>
        <title>FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution</title>
        <link>https://arxiv.org/abs/2506.03173</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03173v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaoyi Liu, Hao Tang</dc:creator>
        <description><![CDATA[
            背景：物理智能对下一代世界模型至关重要。方法：提出FOLIAGE，一种物理感知的多模态世界模型，其在动作 - 感知循环中，统一上下文编码器将图像、网格连接性和点云映射到共享潜态；物理感知预测器推进潜态以生成MAGE；AGN通过特定编码和消息传递捕获动态连接性，还有多种方法增强MAGE表达性。还创建SURF - GARDEN平台生成7200个表面生长序列，用SURF - BENCH评估。效果：FOLIAGE优于专业基线，在动态环境中稳健。
            arXiv:2506.03173v1 Announce Type: new 
Abstract: Physical intelligence -- anticipating and shaping the world from partial, multisensory observations -- is critical for next-generation world models. We propose FOLIAGE, a physics-informed multimodal world model for unbounded accretive surface growth. In its Action-Perception loop, a unified context encoder maps images, mesh connectivity, and point clouds to a shared latent state. A physics-aware predictor, conditioned on physical control actions, advances this latent state in time to align with the target latent of the surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network (AGN) captures dynamic connectivity through Age Positional Encoding and Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances global context with local dynamics. We create SURF-GARDEN, a world model learning platform comprising a Counterfactual Physics Simulator, a Multimodal Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation suite, evaluates six core tasks -- topology recognition, inverse material estimation, growth-stage classification, latent roll-out, cross-modal retrieval, and dense correspondence -- and four stress tests -- sensor dropout, zero-shot modality transfer, long-horizon prediction, and physics ablation -- to probe resilience. FOLIAGE outperforms specialized baselines while remaining robust across dynamic environments, establishing a new world-model based, multimodal pathway to physical intelligence.
        ]]></description>
    </item>
    <item>
        <title>Non-collective Calibrating Strategy for Time Series Forecasting</title>
        <link>https://arxiv.org/abs/2506.03176</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03176v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Bin Wang, Yongqi Han, Minbo Ma, Tianrui Li, Junbo Zhang, Feng Hong, Yanwei Yu</dc:creator>
        <description><![CDATA[
            背景：深度学习在时间序列预测中取得进展，但复杂动态使其难以设计最佳模型架构。方法：提出一种名为Socket+Plug（SoP）的校准策略，为每个预测目标保留专属优化器和早停监测器，同时冻结训练好的Socket主干。该策略与模型无关，可直接校准任何训练好的深度预测模型。效果：在多个时间序列基准和气象数据集上的实验表明，即使使用简单MLP作为Plug，SoP也能实现高达22%的性能提升。
            arXiv:2506.03176v1 Announce Type: new 
Abstract: Deep learning-based approaches have demonstrated significant advancements in time series forecasting. Despite these ongoing developments, the complex dynamics of time series make it challenging to establish the rule of thumb for designing the golden model architecture. In this study, we argue that refining existing advanced models through a universal calibrating strategy can deliver substantial benefits with minimal resource costs, as opposed to elaborating and training a new model from scratch. We first identify a multi-target learning conflict in the calibrating process, which arises when optimizing variables across time steps, leading to the underutilization of the model's learning capabilities. To address this issue, we propose an innovative calibrating strategy called Socket+Plug (SoP). This approach retains an exclusive optimizer and early-stopping monitor for each predicted target within each Plug while keeping the fully trained Socket backbone frozen. The model-agnostic nature of SoP allows it to directly calibrate the performance of any trained deep forecasting models, regardless of their specific architectures. Extensive experiments on various time series benchmarks and a spatio-temporal meteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up to a 22% improvement even when employing a simple MLP as the Plug (highlighted in Figure 1)
        ]]></description>
    </item>
    <item>
        <title>Vid-SME: Membership Inference Attacks against Large Video Understanding Models</title>
        <link>https://arxiv.org/abs/2506.03179</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03179v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qi Li, Runpeng Yu, Xinchao Wang</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型在视频理解应用中发展迅速，但引发数据隐私问题，现有成员推理攻击（MIAs）方法难以有效应用于视频领域。方法：提出Vid - SME，利用模型输出的置信度并集成自适应参数化来计算视频输入的Sharma - Mittal熵（SME），通过自然和时间反转视频帧的SME差异得出成员分数。效果：在各种自训练和开源的视频理解大语言模型（VULLMs）实验中，证明了Vid - SME的有效性。
            arXiv:2506.03179v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) demonstrate remarkable capabilities in handling complex multimodal tasks and are increasingly adopted in video understanding applications. However, their rapid advancement raises serious data privacy concerns, particularly given the potential inclusion of sensitive video content, such as personal recordings and surveillance footage, in their training datasets. Determining improperly used videos during training remains a critical and unresolved challenge. Despite considerable progress on membership inference attacks (MIAs) for text and image data in MLLMs, existing methods fail to generalize effectively to the video domain. These methods suffer from poor scalability as more frames are sampled and generally achieve negligible true positive rates at low false positive rates (TPR@Low FPR), mainly due to their failure to capture the inherent temporal variations of video frames and to account for model behavior differences as the number of frames varies. To address these challenges, we introduce Vid-SME, the first membership inference method tailored for video data used in video understanding LLMs (VULLMs). Vid-SME leverages the confidence of model output and integrates adaptive parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By leveraging the SME difference between natural and temporally-reversed video frames, Vid-SME derives robust membership scores to determine whether a given video is part of the model's training set. Experiments on various self-trained and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.
        ]]></description>
    </item>
    <item>
        <title>Continual Learning in Vision-Language Models via Aligned Model Merging</title>
        <link>https://arxiv.org/abs/2506.03189</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03189v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ghada Sokar, Gintare Karolina Dziugaite, Anurag Arnab, Ahmet Iscen, Pablo Samuel Castro, Cordelia Schmid</dc:creator>
        <description><![CDATA[
            背景：传统的持续学习通过顺序微调解决，虽能适应但易忽视保留先验知识的稳定性，现有方法也存在偏向近期任务的问题。方法：提出基于模型合并的新视角，将新训练的任务参数与先前学习的参数合并，还提出促进与先前权重对齐的机制以避免合并时的干扰。效果：在大型视觉语言模型上评估，该方法能减少遗忘，增强对不同任务顺序和相似性的鲁棒性，提高泛化能力。
            arXiv:2506.03189v1 Announce Type: new 
Abstract: Continual learning is conventionally tackled through sequential fine-tuning, a process that, while enabling adaptation, inherently favors plasticity over the stability needed to retain prior knowledge. While existing approaches attempt to mitigate catastrophic forgetting, a bias towards recent tasks persists as they build upon this sequential nature. In this work we present a new perspective based on model merging to maintain stability while still retaining plasticity. Rather than just sequentially updating the model weights, we propose merging newly trained task parameters with previously learned ones, promoting a better balance. To maximize the effectiveness of the merging process, we propose a simple mechanism that promotes learning aligned weights with previous ones, thereby avoiding interference when merging. We evaluate this approach on large Vision-Language Models (VLMs), and demonstrate its effectiveness in reducing forgetting, increasing robustness to various task orders and similarities, and improving generalization.
        ]]></description>
    </item>
    <item>
        <title>Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs</title>
        <link>https://arxiv.org/abs/2506.03195</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03195v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yunqi Hong, Sohyun An, Andrew Bai, Neil Y. C. Lin, Cho-Jui Hsieh</dc:creator>
        <description><![CDATA[
            多模态大语言模型（MLLMs）在一般零样本图像分类任务中成果显著，但细粒度图像分类仍具挑战。为解决这一问题，研究提出AutoSEP迭代自监督提示学习框架，利用无标签数据学习描述提示，引导MLLMs识别图像关键特征，提升分类准确率。该框架基于实例级分类评分函数，仅需黑盒访问MLLMs，无需训练或微调。在多个细粒度分类数据集上评估，其平均比标准零样本分类提高13%，比最佳基线提高5%。
            arXiv:2506.03195v1 Announce Type: new 
Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on general zero-shot image classification tasks, fine-grained image classification remains challenging. It demands precise attention to subtle visual details to distinguish between visually similar subcategories--details that MLLMs may easily overlook without explicit guidance. To address this, we introduce AutoSEP, an iterative self-supervised prompt learning framework designed to enhance MLLM fine-grained classification capabilities in a fully unsupervised manner. Our core idea is to leverage unlabeled data to learn a description prompt that guides MLLMs in identifying crucial discriminative features within an image, and boosts classification accuracy. We developed an automatic self-enhancing prompt learning framework called AutoSEP to iteratively improve the description prompt using unlabeled data, based on instance-level classification scoring function. AutoSEP only requires black-box access to MLLMs, eliminating the need for any training or fine-tuning. We evaluate our approach on multiple fine-grained classification datasets. It consistently outperforms other unsupervised baselines, demonstrating the effectiveness of our self-supervised optimization framework. Notably, AutoSEP on average improves 13 percent over standard zero-shot classification and 5 percent over the best-performing baselines. Code is available at: https://github.com/yq-hong/AutoSEP
        ]]></description>
    </item>
    <item>
        <title>Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing</title>
        <link>https://arxiv.org/abs/2506.03197</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03197v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Yanjie Liang, Zuming Huang, Haozhe Wang, Jun Huang, Ling Chen, Wei Chu, Yuan Qi</dc:creator>
        <description><![CDATA[
            背景：将扫描文档自动解析为结构化、机器可读格式是文档人工智能的关键瓶颈，传统多阶段流程存在误差传播和布局适应性差的问题。方法：引入端到端强化学习框架layoutRL，通过优化归一化编辑距离、段落计数准确性和阅读顺序保留的综合奖励，使模型具备布局感知能力，并利用新发布的数据集Infinity - Doc - 55K实例化基于视觉语言模型的解析器Infinity - Parser。效果：在中英文基准测试中，该解析器在准确性和结构保真度上达到新的最优水平，超越专业流程和通用视觉语言模型。
            arXiv:2506.03197v1 Announce Type: new 
Abstract: Automated parsing of scanned documents into richly structured, machine-readable formats remains a critical bottleneck in Document AI, as traditional multi-stage pipelines suffer from error propagation and limited adaptability to diverse layouts. We introduce layoutRL, an end-to-end reinforcement learning framework that trains models to be explicitly layout-aware by optimizing a composite reward of normalized edit distance, paragraph count accuracy, and reading order preservation. Leveraging our newly released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic scanned document parsing data with expert-filtered real-world documents, we instantiate layoutRL in a vision-language-model-based parser called Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and formula extraction, and reading order detection, Infinity-Parser achieves new state-of-the-art performance in both accuracy and structural fidelity, outpacing specialist pipelines and general-purpose vision-language models. We will publicly release our code and dataset to accelerate progress in robust document understanding.
        ]]></description>
    </item>
    <item>
        <title>FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment</title>
        <link>https://arxiv.org/abs/2506.03198</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03198v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hao Yin, Lijun Gu, Paritosh Parmar, Lin Xu, Tianxiao Guo, Weiwei Fu, Yang Zhang, Tianyou Zheng</dc:creator>
        <description><![CDATA[
            背景：随着健康意识提升，健身成为趋势，但健身训练有风险，现有动作质量评估（AQA）方法和数据集存在局限。方法：提出FLEX数据集，它是首个将表面肌电信号纳入AQA的多模态、多动作、大规模数据集，还将知识图融入AQA，构建以惩罚函数形式的注释规则。效果：在FLEX上进行多种基线方法测试，表明多模态、多视图数据和细粒度注释能显著提升模型性能，推动AQA发展及人工智能与健身领域融合。
            arXiv:2506.03198v1 Announce Type: new 
Abstract: With the increasing awareness of health and the growing desire for aesthetic physique, fitness has become a prevailing trend. However, the potential risks associated with fitness training, especially with weight-loaded fitness actions, cannot be overlooked. Action Quality Assessment (AQA), a technology that quantifies the quality of human action and provides feedback, holds the potential to assist fitness enthusiasts of varying skill levels in achieving better training outcomes. Nevertheless, current AQA methodologies and datasets are limited to single-view competitive sports scenarios and RGB modality and lack professional assessment and guidance of fitness actions. To address this gap, we propose the FLEX dataset, the first multi-modal, multi-action, large-scale dataset that incorporates surface electromyography (sEMG) signals into AQA. FLEX utilizes high-precision MoCap to collect 20 different weight-loaded actions performed by 38 subjects across 3 different skill levels for 10 repetitions each, containing 5 different views of the RGB video, 3D pose, sEMG, and physiological information. Additionally, FLEX incorporates knowledge graphs into AQA, constructing annotation rules in the form of penalty functions that map weight-loaded actions, action keysteps, error types, and feedback. We conducted various baseline methodologies on FLEX, demonstrating that multimodal data, multiview data, and fine-grained annotations significantly enhance model performance. FLEX not only advances AQA methodologies and datasets towards multi-modal and multi-action scenarios but also fosters the integration of artificial intelligence within the fitness domain. Dataset and code are available at https://haoyin116.github.io/FLEX_Dataset.
        ]]></description>
    </item>
    <item>
        <title>On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models</title>
        <link>https://arxiv.org/abs/2506.03267</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03267v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shahbaz Rezaei, Avishai Halev, Xin Liu</dc:creator>
        <description><![CDATA[
            背景：当前解释时间序列模型常采用时域归因或在归因最稀疏的域呈现，而新兴的解释空间概念提出可在其他域解释模型。方法：引入量子力学中的不确定性原理（UP）到可解释人工智能（XAI）领域，通过该原理判断时域和频域归因是否违反界限。效果：在多种深度学习模型、XAI方法及大量分类和预测数据集上验证了该方法的有效性，发现UP违反情况频繁，凸显了仅关注时域解释的局限性，强调了多域解释的必要性。
            arXiv:2506.03267v1 Announce Type: new 
Abstract: A prevailing approach to explain time series models is to generate attribution in time domain. A recent development in time series XAI is the concept of explanation spaces, where any model trained in the time domain can be interpreted with any existing XAI method in alternative domains, such as frequency. The prevailing approach is to present XAI attributions either in the time domain or in the domain where the attribution is most sparse. In this paper, we demonstrate that in certain cases, XAI methods can generate attributions that highlight fundamentally different features in the time and frequency domains that are not direct counterparts of one another. This suggests that both domains' attributions should be presented to achieve a more comprehensive interpretation. Thus it shows the necessity of multi-domain explanation. To quantify when such cases arise, we introduce the uncertainty principle (UP), originally developed in quantum mechanics and later studied in harmonic analysis and signal processing, to the XAI literature. This principle establishes a lower bound on how much a signal can be simultaneously localized in both the time and frequency domains. By leveraging this concept, we assess whether attributions in the time and frequency domains violate this bound, indicating that they emphasize distinct features. In other words, UP provides a sufficient condition that the time and frequency domain explanations do not match and, hence, should be both presented to the end user. We validate the effectiveness of this approach across various deep learning models, XAI methods, and a wide range of classification and forecasting datasets. The frequent occurrence of UP violations across various datasets and XAI methods highlights the limitations of existing approaches that focus solely on time-domain explanations. This underscores the need for multi-domain explanations as a new paradigm.
        ]]></description>
    </item>
    <item>
        <title>Seeing the Arrow of Time in Large Multimodal Models</title>
        <link>https://arxiv.org/abs/2506.03340</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03340v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zihui Xue, Mi Luo, Kristen Grauman</dc:creator>
        <description><![CDATA[
            时间箭头（AoT）对视频理解至关重要，但现代多模态大模型（LMMs）在感知和利用视频时间方向性上存在困难。为解决此问题，研究团队先对现有基准和模型进行分析，接着提出基于强化学习的训练策略ArrowRL，通过反向奖励机制培养模型对AoT的感知。同时开发多维度基准AoTBench进行严格评估。实验表明，ArrowRL大幅提升了模型的时间感知能力，在AoTBench和标准视频问答基准上的准确率分别提升超20%和10%。
            arXiv:2506.03340v1 Announce Type: new 
Abstract: The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is fundamental to video comprehension, yet remains a significant challenge for modern large multimodal models (LMMs). Current LMMs struggle to perceive and utilize temporal directionality in video when responding to language queries, obstructing deeper temporal understanding. We tackle this deficiency by first providing a critical analysis of existing benchmarks and models. We then introduce ArrowRL, a reinforcement learning (RL)-based training strategy with an innovative reverse reward that instills AoT awareness by encouraging divergent video interpretations between forward and reversed visual frames. For rigorous evaluation, we additionally develop AoTBench, a new multi-faceted benchmark probing temporally challenging questions. Experiments show ArrowRL greatly advances temporal perception: it not only achieves substantial improvements on our challenging AoTBench but also demonstrably boosts performance on standard video question answering (VQA) benchmarks (with peak accuracy gains reaching over 20% and 10% respectively). This validates ArrowRL's effectiveness and highlights the critical need for dedicated AoT understanding in LMMs.
        ]]></description>
    </item>
    <item>
        <title>Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery</title>
        <link>https://arxiv.org/abs/2506.03388</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03388v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Pengyu Chen, Xiao Huang, Teng Fei, Sicheng Wang</dc:creator>
        <description><![CDATA[
            背景：环境声景蕴含大量城市环境信息，但在大规模地理分析中其潜力未被充分挖掘。方法：通过比较不同视觉表征策略捕捉声学语义的程度，采用多模态方法，整合三个全球大城市的地理参考录音、街景和遥感影像，利用多种模型提取嵌入和特征来评估跨模态相似性。效果：街景嵌入与环境声音的对齐性强于分割输出，遥感分割通过BGA框架更能有效解释生态类别，表明基于嵌入的模型语义对齐更优，基于分割的方法能建立视觉结构与声学生态的可解释联系。
            arXiv:2506.03388v1 Announce Type: new 
Abstract: Environmental soundscapes convey substantial ecological and social information regarding urban environments; however, their potential remains largely untapped in large-scale geographic analysis. In this study, we investigate the extent to which urban sounds correspond with visual scenes by comparing various visual representation strategies in capturing acoustic semantics. We employ a multimodal approach that integrates geo-referenced sound recordings with both street-level and remote sensing imagery across three major global cities: London, New York, and Tokyo. Utilizing the AST model for audio, along with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV for semantic segmentation, we extract embeddings and class-level features to evaluate cross-modal similarity. The results indicate that street view embeddings demonstrate stronger alignment with environmental sounds compared to segmentation outputs, whereas remote sensing segmentation is more effective in interpreting ecological categories through a Biophony--Geophony--Anthrophony (BGA) framework. These findings imply that embedding-based models offer superior semantic alignment, while segmentation-based methods provide interpretable links between visual structure and acoustic ecology. This work advances the burgeoning field of multimodal urban sensing by offering novel perspectives for incorporating sound into geospatial analysis.
        ]]></description>
    </item>
    <item>
        <title>DistRAG: Towards Distance-Based Spatial Reasoning in LLMs</title>
        <link>https://arxiv.org/abs/2506.03424</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03424v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Nicole R Schneider, Nandini Ramachandran, Kent O'Sullivan, Hanan Samet</dc:creator>
        <description><![CDATA[
            背景：大语言模型（LLMs）在许多现实任务中需具备空间推理能力，但自身缺乏可靠的空间推理能力，尤其是距离推理。方法：提出DistRAG方法，将城镇间的测地距离编码到图中，并检索与问题相关的上下文子图。效果：使LLMs能够回答原本无法回答的基于距离的推理问题，为LLMs提供了一个灵活的初步“世界模型”，以补充其语言知识。
            arXiv:2506.03424v1 Announce Type: new 
Abstract: Many real world tasks where Large Language Models (LLMs) can be used require spatial reasoning, like Point of Interest (POI) recommendation and itinerary planning. However, on their own LLMs lack reliable spatial reasoning capabilities, especially about distances. To address this problem, we develop a novel approach, DistRAG, that enables an LLM to retrieve relevant spatial information not explicitly learned during training. Our method encodes the geodesic distances between cities and towns in a graph and retrieves a context subgraph relevant to the question. Using this technique, our method enables an LLM to answer distance-based reasoning questions that it otherwise cannot answer. Given the vast array of possible places an LLM could be asked about, DistRAG offers a flexible first step towards providing a rudimentary `world model' to complement the linguistic knowledge held in LLMs.
        ]]></description>
    </item>
    <item>
        <title>Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models</title>
        <link>https://arxiv.org/abs/2506.03434</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03434v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ahmad Dawar Hakimi, Ali Modarressi, Philipp Wicke, Hinrich Sch\"utze</dc:creator>
        <description><![CDATA[
            背景：理解大语言模型获取和存储事实性知识的方式对提高其可解释性和可靠性至关重要。方法：通过追踪OLMo - 7B模型在预训练过程中注意力头和前馈网络（FFNs）的作用，将这些组件分为四类并研究其稳定性和转变。效果：发现大语言模型最初依赖通用组件，训练中逐渐专业化；部分组件会被重新利用；注意力头更替率最高，FFNs更稳定；基于位置的关系比基于名称的关系在训练中更早达到高精度，为知识形成提供了机制性见解。
            arXiv:2506.03434v1 Announce Type: new 
Abstract: Understanding how large language models (LLMs) acquire and store factual knowledge is crucial for enhancing their interpretability and reliability. In this work, we analyze the evolution of factual knowledge representation in the OLMo-7B model by tracking the roles of its attention heads and feed forward networks (FFNs) over the course of pre-training. We classify these components into four roles: general, entity, relation-answer, and fact-answer specific, and examine their stability and transitions. Our results show that LLMs initially depend on broad, general-purpose components, which later specialize as training progresses. Once the model reliably predicts answers, some components are repurposed, suggesting an adaptive learning process. Notably, attention heads display the highest turnover. We also present evidence that FFNs remain more stable throughout training. Furthermore, our probing experiments reveal that location-based relations converge to high accuracy earlier in training than name-based relations, highlighting how task complexity shapes acquisition dynamics. These insights offer a mechanistic view of knowledge formation in LLMs.
        ]]></description>
    </item>
    <item>
        <title>Geometric Visual Fusion Graph Neural Networks for Multi-Person Human-Object Interaction Recognition in Videos</title>
        <link>https://arxiv.org/abs/2506.03440</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03440v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tanqiu Qiao, Ruochen Li, Frederick W. B. Li, Yoshiki Kubotani, Shigeo Morishima, Hubert P. H. Shum</dc:creator>
        <description><![CDATA[
            视频中的人与物体交互（HOI）识别需理解视觉模式和几何关系，但有效融合多模态特征颇具挑战。为此，研究提出几何视觉融合图神经网络（GeoVis - GNN），采用双注意力特征融合与相互依赖的实体图学习，从特定实体表征逐步实现高层交互理解。还引入并发部分交互数据集（MPHOI - 120），以应对复杂的人与物体动态和相互遮挡问题。大量实验表明，该方法在多种HOI场景中表现出色，达到了当前最优性能。
            arXiv:2506.03440v1 Announce Type: new 
Abstract: Human-Object Interaction (HOI) recognition in videos requires understanding both visual patterns and geometric relationships as they evolve over time. Visual and geometric features offer complementary strengths. Visual features capture appearance context, while geometric features provide structural patterns. Effectively fusing these multimodal features without compromising their unique characteristics remains challenging. We observe that establishing robust, entity-specific representations before modeling interactions helps preserve the strengths of each modality. Therefore, we hypothesize that a bottom-up approach is crucial for effective multimodal fusion. Following this insight, we propose the Geometric Visual Fusion Graph Neural Network (GeoVis-GNN), which uses dual-attention feature fusion combined with interdependent entity graph learning. It progressively builds from entity-specific representations toward high-level interaction understanding. To advance HOI recognition to real-world scenarios, we introduce the Concurrent Partial Interaction Dataset (MPHOI-120). It captures dynamic multi-person interactions involving concurrent actions and partial engagement. This dataset helps address challenges like complex human-object dynamics and mutual occlusions. Extensive experiments demonstrate the effectiveness of our method across various HOI scenarios. These scenarios include two-person interactions, single-person activities, bimanual manipulations, and complex concurrent partial interactions. Our method achieves state-of-the-art performance.
        ]]></description>
    </item>
    <item>
        <title>ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch</title>
        <link>https://arxiv.org/abs/2506.03558</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03558v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiawei Chen, Xinyan Guan, Qianhao Yuan, Guozhao Mo, Weixiang Zhou, Yaojie Lu, Hongyu Lin, Ben He, Le Sun, Xianpei Han</dc:creator>
        <description><![CDATA[
            背景：当前指令数据合成方法多关注单轮指令，忽略跨轮连贯性，导致长对话中出现上下文漂移和任务完成率降低。方法：提出骨架引导的多轮对话生成框架，分意图建模和骨架生成两阶段，前者将对话分配到九个意图轨迹以捕捉全局结构，后者构建与意图匹配的用户查询序列。效果：构建了含约15000个多轮对话和224392个话语的数据集，在相关基准测试中，微调模型的聊天一致性提升20 - 30%，任务成功率最高提升15%。
            arXiv:2506.03558v1 Announce Type: new 
Abstract: Current instruction data synthesis methods primarily focus on single-turn instructions and often neglect cross-turn coherence, resulting in context drift and reduced task completion rates in extended conversations. To address this limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a framework that constrains multi-turn instruction synthesis by explicitly modeling human conversational intent. It operates in two stages: (1) Intent Modeling, which captures the global structure of human dialogues by assigning each conversation to one of nine well-defined intent trajectories, ensuring a coherent and goal-oriented information flow; and (2) Skeleton Generation, which constructs a structurally grounded sequence of user queries aligned with the modeled intent, thereby serving as a scaffold that constrains and guides the downstream instruction synthesis process. Based on this process, we construct ConsistentChat, a multi-turn instruction dataset with approximately 15,000 multi-turn conversations and 224,392 utterances. Experiments on the Light, Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat achieve a 20-30% improvement in chat consistency and up to a 15% increase in task success rate, significantly outperforming models trained on existing single-turn and multi-turn instruction datasets.
        ]]></description>
    </item>
    <item>
        <title>KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models</title>
        <link>https://arxiv.org/abs/2506.03576</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03576v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zirui Chen, Xin Wang, Zhao Li, Wenbin Guo, Dongxiao He</dc:creator>
        <description><![CDATA[
            背景：知识表示学习迫切需要将符号知识图谱与语言模型统一以实现更丰富的语义理解，而现有方法多侧重图结构或文本语义。方法：提出KG - BiLM双向语言模型框架，融合知识图谱的结构线索与生成式Transformer的语义表达能力，包含双向知识注意力、知识掩码预测、对比图语义聚合三个关键组件。效果：在标准基准测试中，KG - BiLM在链接预测任务上优于强基线，尤其在具有复杂多跳关系的大规模图上，验证了其统一结构信息和文本语义的有效性。
            arXiv:2506.03576v1 Announce Type: new 
Abstract: Recent advances in knowledge representation learning (KRL) highlight the urgent necessity to unify symbolic knowledge graphs (KGs) with language models (LMs) for richer semantic understanding. However, existing approaches typically prioritize either graph structure or textual semantics, leaving a gap: a unified framework that simultaneously captures global KG connectivity, nuanced linguistic context, and discriminative reasoning semantics. To bridge this gap, we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues from KGs with the semantic expressiveness of generative transformers. KG-BiLM incorporates three key components: (i) Bidirectional Knowledge Attention, which removes the causal mask to enable full interaction among all tokens and entities; (ii) Knowledge-Masked Prediction, which encourages the model to leverage both local semantic contexts and global graph connectivity; and (iii) Contrastive Graph Semantic Aggregation, which preserves KG structure via contrastive alignment of sampled sub-graph representations. Extensive experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong baselines in link prediction, especially on large-scale graphs with complex multi-hop relations - validating its effectiveness in unifying structural information and textual semantics.
        ]]></description>
    </item>
    <item>
        <title>Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts</title>
        <link>https://arxiv.org/abs/2506.03591</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03591v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiaxing Zhang, Xinyi Zeng, Hao Tang</dc:creator>
        <description><![CDATA[
            背景：基于端到端自回归变压器的统一多模态大语言模型虽能整合理解与生成任务，但理解中的高级语义抽象和生成中的细粒度细节保留存在任务目标冲突。方法：提出一种解耦自回归内部组件的新方法，设计UTAMoE框架，通过任务感知的专家混合层解耦内部模块，还引入两阶段训练策略。效果：大量实验表明UTAMoE缓解了任务目标冲突，在多模态基准测试中取得了最优性能。
            arXiv:2506.03591v1 Announce Type: new 
Abstract: Unified multimodal large language models (MLLMs) based on end-to-end autoregressive (AR) transformers effectively integrate both understanding and generation tasks within a single framework. However, intrinsic Task Objective Conflicts between high-level semantic abstraction in understanding and fine-grained detail preservation in generation pose significant challenges, often leading to suboptimal trade-offs and task interference. Existing solutions, such as decoupling shared visual encoders, fall short of fundamentally resolving these conflicts due to inherent AR architecture. In this paper, we propose a novel approach that decouples internal components of AR to resolve task objective conflicts. Specifically, we design UTAMoE, a Unified Task-Aware Mixture-of-Experts (MoE) framework that decouples internal AR modules via a Task-Aware MoE Layer to create task-specific optimization subpaths. To enhance task differentiation while maintaining overall coordination, we introduce a novel Two-Stage Training Strategy. Extensive experiments on multimodal benchmarks demonstrate that UTAMoE mitigates task objective conflicts, achieving state-of-the-art performance across various tasks. Visualizations and ablation studies further validate the effectiveness of our approach.
        ]]></description>
    </item>
    <item>
        <title>Learning to Insert [PAUSE] Tokens for Better Reasoning</title>
        <link>https://arxiv.org/abs/2506.03616</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03616v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Eunki Kim, Sangryul Kim, James Thorne</dc:creator>
        <description><![CDATA[
            为增强大语言模型推理能力，此前研究探索在训练中引入特殊标记。本文在前期研究基础上，提出动态插入标记训练（DIT）方法。该方法依据标记对数似然，找出序列中模型置信度最低的位置，插入[PAUSE]标记，提升模型对后续标记的预测能力。实验表明，在27亿到80亿参数模型的不同数据集上，DIT优于传统微调及先前的标记插入方法，在GSM8K等数据集上，准确率和通过率有显著提升，拓宽了推理研究范围。
            arXiv:2506.03616v1 Announce Type: new 
Abstract: To enhance reasoning capabilities, previous works have explored incorporating special-purpose tokens into the training process. These strategies strengthen the learning mechanism of transformer-based large language models (LLMs). Building on prior research, in which inserting dummy tokens consecutively just before reasoning steps can enhance effectiveness, we introduce a novel approach termed Dynamic Inserting Tokens Training (DIT). Our method identifies positions within sequences where model confidence is lowest according to token log-likelihood. Strategically inserting [PAUSE] tokens on these positions bolsters the model's predictive capabilities for subsequent tokens. Experimental results across diverse datasets and models, from the 2.7B model to the 8B model, demonstrate that DIT consistently outperforms traditional fine-tuning and previous token insertion methods. With this simple yet effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work shows a model-based, dynamic approach rather than a heuristic one, thereby broadening the scope of research in reasoning.
        ]]></description>
    </item>
    <item>
        <title>Spatial Understanding from Videos: Structured Prompts Meet Simulation Data</title>
        <link>https://arxiv.org/abs/2506.03642</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03642v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haoyu Zhang, Meng Liu, Zaijing Li, Haokun Wen, Weili Guan, Yaowei Wang, Liqiang Nie</dc:creator>
        <description><![CDATA[
            视觉空间理解对机器人导航等下游任务至关重要，但现有方法存在空间不确定性和数据稀缺问题，限制了预训练视觉语言模型的3D空间推理能力。为此，本文提出统一框架，结合结构化提示策略SpatialMind和通过自动构建流程生成的可扩展问答数据集ScanForgeQA，在不修改模型架构的情况下增强预训练视觉语言模型的3D空间推理能力。多基准实验证明了提示和微调策略的有效性，为视觉空间理解研究提供启示。
            arXiv:2506.03642v1 Announce Type: new 
Abstract: Visual-spatial understanding, the ability to infer object relationships and layouts from visual input, is fundamental to downstream tasks such as robotic navigation and embodied interaction. However, existing methods face spatial uncertainty and data scarcity, limiting the 3D spatial reasoning capability of pre-trained vision-language models (VLMs). To address these challenges, we present a unified framework for enhancing 3D spatial reasoning in pre-trained VLMs without modifying their architecture. This framework combines SpatialMind, a structured prompting strategy that decomposes complex scenes and questions into interpretable reasoning steps, with ScanForgeQA, a scalable question-answering dataset built from diverse 3D simulation scenes through an automated construction process designed for fine-tuning. Extensive experiments across multiple benchmarks demonstrate the individual and combined effectiveness of our prompting and fine-tuning strategies, and yield insights that may inspire future research on visual-spatial understanding.
        ]]></description>
    </item>
    <item>
        <title>Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring</title>
        <link>https://arxiv.org/abs/2506.03696</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03696v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fang Wang, Paolo Ceravolo, Ernesto Damiani</dc:creator>
        <description><![CDATA[
            背景：现有预测性业务流程监控（PBPM）方法缺乏处理现实挑战的灵活性，静态编码方案和固定LSTM架构难以支持自适应表示和跨异构数据集泛化。方法：提出动态LSTM超模型套件，集成两级分层编码、基于字符的事件标签分解和新的伪嵌入技术，还引入专门的LSTM变体用于同时事件建模。效果：在四个公共和真实世界数据集上实验，平衡数据集准确率达100%，不平衡数据集F1分数超86%，推进了PBPM发展。
            arXiv:2506.03696v1 Announce Type: new 
Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future outcomes of ongoing business processes. However, existing methods often lack flexibility to handle real-world challenges such as simultaneous events, class imbalance, and multi-level attributes. While prior work has explored static encoding schemes and fixed LSTM architectures, they struggle to support adaptive representations and generalize across heterogeneous datasets. To address these limitations, we propose a suite of dynamic LSTM HyperModels that integrate two-level hierarchical encoding for event and sequence attributes, character-based decomposition of event labels, and novel pseudo-embedding techniques for durations and attribute correlations. We further introduce specialized LSTM variants for simultaneous event modeling, leveraging multidimensional embeddings and time-difference flag augmentation. Experimental validation on four public and real-world datasets demonstrates up to 100% accuracy on balanced datasets and F1 scores exceeding 86\% on imbalanced ones. Our approach advances PBPM by offering modular and interpretable models better suited for deployment in complex settings. Beyond PBPM, it contributes to the broader AI community by improving temporal outcome prediction, supporting data heterogeneity, and promoting explainable process intelligence frameworks.
        ]]></description>
    </item>
    <item>
        <title>Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision</title>
        <link>https://arxiv.org/abs/2506.03723</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03723v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chaeyun Jang, Moonseok Choi, Yegon Kim, Hyungi Lee, Juho Lee</dc:creator>
        <description><![CDATA[
            背景：不确定性校准对大语言模型安全部署至关重要，思维链推理的置信度校准研究较少。方法：仅用标量置信度标签进行监督微调，无需明确推理监督或基于强化学习的奖励；还提出基于校准不确定性的测试时缩放重思考方法。效果：模型能针对不同置信度查询生成不同长度响应；在GSM8K等推理任务实验中，置信度感知微调提升了校准度和准确率，且使推理路径与置信度更匹配，增强了可解释性。
            arXiv:2506.03723v1 Announce Type: new 
Abstract: Uncertainty calibration is essential for the safe deployment of large language models (LLMs), particularly when users rely on verbalized confidence estimates. While prior work has focused on classifiers or short-form generation, confidence calibration for chain-of-thought (CoT) reasoning remains largely unexplored. Surprisingly, we find that supervised fine-tuning with scalar confidence labels alone suffices to elicit self-verification behavior of language models, without any explicit reasoning supervision or reinforcement learning-based rewards. Despite being trained only to produce a verbalized confidence score without any self-verifying examples, the model learns to generate longer and self-checking responses for low-confidence queries while providing more concise answers for high-confidence ones. We further propose a simple rethinking method that boosts performance via test-time scaling based on calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning improves both calibration and accuracy, while also enhancing interpretability by aligning the model's reasoning path with its confidence.
        ]]></description>
    </item>
    <item>
        <title>Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation</title>
        <link>https://arxiv.org/abs/2506.03887</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03887v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junyi Chen, Shihao Bai, Zaijun Wang, Siyu Wu, Chuheng Du, Hailong Yang, Ruihao Gong, Shengzhong Liu, Fan Wu, Guihai Chen</dc:creator>
        <description><![CDATA[
            背景：大量大语言模型（LLM）应用需要高效的结构化生成，现有解析LR(1)语法的方法存在运行时执行开销大的问题。方法：提出Pre$^3$，通过预处理时预计算前缀条件边实现提前的边分析和并行转换处理，还将LR(1)转换图转换为确定性下推自动机（DPDA），消除运行时路径探索。效果：可无缝集成到标准LLM推理框架，实验中输出令牌时间（TPOT）最多减少40%，吞吐量最多增加36%。
            arXiv:2506.03887v1 Announce Type: new 
Abstract: Extensive LLM applications demand efficient structured generations, particularly for LR(1) grammars, to produce outputs in specified formats (e.g., JSON). Existing methods primarily parse LR(1) grammars into a pushdown automaton (PDA), leading to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches. To address these issues, we propose Pre$^3$ that exploits deterministic pushdown automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables ahead-of-time edge analysis and thus makes parallel transition processing possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$ introduces a novel approach that transforms LR(1) transition graphs into DPDA, eliminating the need for runtime path exploration and achieving edge transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into standard LLM inference frameworks, reducing time per output token (TPOT) by up to 40% and increasing throughput by up to 36% in our experiments. Our code is available at https://github.com/ModelTC/lightllm.
        ]]></description>
    </item>
    <item>
        <title>Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win</title>
        <link>https://arxiv.org/abs/2506.03919</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03919v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lorenz Kummer, Samir Moustafa, Anatol Ehrlich, Franka Bause, Nikolaus Suess, Wilfried N. Gansterer, Nils M. Kriege</dc:creator>
        <description><![CDATA[
            背景：彩票假设在卷积神经网络中研究充分，但在图神经网络（GNNs）中仅通过实证验证，缺乏理论成果。方法：确定稀疏子网络的表达能力（即区分非同构图的能力）对寻找能保持预测性能的“中奖票”至关重要，建立稀疏初始化GNN表达能力与完整网络匹配的条件，并提出和证明强表达性彩票假设。效果：初始化时更高的表达能力可能加速模型收敛、提高泛化能力，为LTH和GNN研究奠定新理论基础。
            arXiv:2506.03919v1 Announce Type: new 
Abstract: The lottery ticket hypothesis (LTH) is well-studied for convolutional neural networks but has been validated only empirically for graph neural networks (GNNs), for which theoretical findings are largely lacking. In this paper, we identify the expressivity of sparse subnetworks, i.e. their ability to distinguish non-isomorphic graphs, as crucial for finding winning tickets that preserve the predictive performance. We establish conditions under which the expressivity of a sparsely initialized GNN matches that of the full network, particularly when compared to the Weisfeiler-Leman test, and in that context put forward and prove a Strong Expressive Lottery Ticket Hypothesis. We subsequently show that an increased expressivity in the initialization potentially accelerates model convergence and improves generalization. Our findings establish novel theoretical foundations for both LTH and GNN research, highlighting the importance of maintaining expressivity in sparsely initialized GNNs. We illustrate our results using examples from drug discovery.
        ]]></description>
    </item>
    <item>
        <title>HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models</title>
        <link>https://arxiv.org/abs/2506.03922</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03922v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhaolu Kang, Junhao Gong, Jiaxu Yan, Wanke Xia, Yian Wang, Ziwen Wang, Huaxuan Ding, Zhuo Cheng, Wenhao Cao, Zhiyuan Feng, Siqi He, Shannan Yan, Junzhe Chen, Xiaomin He, Chaoya Jiang, Wei Ye, Kaidong Yu, Xuelong Li</dc:creator>
        <description><![CDATA[
            背景：当前评估多模态大语言模型（MLLMs）的基准主要关注理工科知识，忽略了人文社科领域的独特需求。方法：提出HSSBench这一专门评估MLLMs在多语言人文社科任务能力的基准，还引入针对人文社科场景的数据生成流程，由多领域专家和自动化智能体协作生成并迭代优化样本。效果：HSSBench含超13000个精心设计的样本，对超20个主流MLLMs进行测试，结果显示即使最先进的模型也面临挑战。
            arXiv:2506.03922v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.
        ]]></description>
    </item>
    <item>
        <title>TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering</title>
        <link>https://arxiv.org/abs/2506.03949</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03949v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan Xu</dc:creator>
        <description><![CDATA[
            大语言模型在自然语言处理中进步显著，但在表格问答（TableQA）任务中面临挑战，现有TableQA基准存在聚焦简单表格、数据泄露、单语言等局限。为此，研究团队推出新基准TableEval，包含多结构表格、跨语言场景，数据源于近期真实文档。同时提出新评估框架SEAT，评估模型回答与参考答案在子问题层面的对齐度。实验表明，SEAT与人工判断高度一致，揭示了现有模型处理复杂TableQA任务的能力差距。
            arXiv:2506.03949v1 Announce Type: new 
Abstract: LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: https://github.com/wenge-research/TableEval.
        ]]></description>
    </item>
    <item>
        <title>Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models</title>
        <link>https://arxiv.org/abs/2506.03989</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03989v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Alex Laitenberger, Christopher D. Manning, Nelson F. Liu</dc:creator>
        <description><![CDATA[
            背景：长上下文语言模型兴起，需评估多阶段检索增强生成（RAG）管道较单阶段方法是否有优势。方法：在系统缩放的令牌预算下对问答任务进行评估，对比ReadAgent和RAPTOR两个多阶段管道与包括DOS RAG在内的三个基线。效果：简单的DOS RAG在多个长上下文问答基准上表现相当或更优，建议将其作为未来RAG评估的基线。
            arXiv:2506.03989v1 Announce Type: new 
Abstract: With the rise of long-context language models (LMs) capable of processing tens of thousands of tokens in a single pass, do multi-stage retrieval-augmented generation (RAG) pipelines still offer measurable benefits over simpler, single-stage approaches? To assess this question, we conduct a controlled evaluation for QA tasks under systematically scaled token budgets, comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three baselines, including DOS RAG (Document's Original Structure RAG), a simple retrieve-then-read method that preserves original passage order. Despite its straightforward design, DOS RAG consistently matches or outperforms more intricate methods on multiple long-context QA benchmarks. We recommend establishing DOS RAG as a simple yet strong baseline for future RAG evaluations, pairing it with emerging embedding and language models to assess trade-offs between complexity and effectiveness as model capabilities evolve.
        ]]></description>
    </item>
    <item>
        <title>Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning</title>
        <link>https://arxiv.org/abs/2506.04065</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04065v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Muling Wu, Qi Qian, Wenhao Liu, Xiaohua Wang, Zisu Huang, Di Liang, LI Miao, Shihan Dou, Changze Lv, Zhenghua Wang, Zhibo Xu, Lina Chen, Tianlong Li, Xiaoqing Zheng, Xuanjing Huang</dc:creator>
        <description><![CDATA[
            背景：大语言模型在推理任务中表现出色，但后训练存在样本利用效率低和难处理不同难度样本的问题。方法：提出定制化课程学习（CCL）框架，一是引入模型自适应难度定义，根据模型能力定制课程数据集；二是开发“引导提示”，通过策略性提示动态降低样本难度。效果：在五个数学推理基准测试中，CCL显著优于统一训练方法，能提升样本利用率和模型性能。
            arXiv:2506.04065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable performance across various reasoning tasks, yet post-training is constrained by inefficient sample utilization and inflexible difficulty samples processing. To address these limitations, we propose Customized Curriculum Learning (CCL), a novel framework with two key innovations. First, we introduce model-adaptive difficulty definition that customizes curriculum datasets based on each model's individual capabilities rather than using predefined difficulty metrics. Second, we develop "Guided Prompting," which dynamically reduces sample difficulty through strategic hints, enabling effective utilization of challenging samples that would otherwise degrade performance. Comprehensive experiments on supervised fine-tuning and reinforcement learning demonstrate that CCL significantly outperforms uniform training approaches across five mathematical reasoning benchmarks, confirming its effectiveness across both paradigms in enhancing sample utilization and model performance.
        ]]></description>
    </item>
    <item>
        <title>LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward</title>
        <link>https://arxiv.org/abs/2506.04070</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04070v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yi Zhao, Siqi Wang, Jing Li</dc:creator>
        <description><![CDATA[
            为视障人士生成导航指令（NIG - VI）十分关键但研究较少。该研究旨在生成精确、实用的逐步行导航指令。为此提出LaF - GRPO方法，让大语言模型模拟视障用户反馈生成奖励，指导视觉语言模型后训练，减少真实数据需求。还引入含27k样本的开源基准NIG4VI。实验表明，LaF - GRPO效果显著，如Zero - (LaF - GRPO)使BLEU提升14%，SFT+(LaF - GRPO)的METEOR为0.542高于GPT - 4o的0.323，且指令更直观、安全。
            arXiv:2506.04070v1 Announce Type: new 
Abstract: Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study, hence, focuses on producing precise, in-situ, step-by-step navigation instructions that are practically usable by VI users. Concretely, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate rewards guiding the Vision-Language Model (VLM) post-training. This enhances instruction usability while reducing costly real-world data needs. To facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced benchmark. It provides diverse navigation scenarios with accurate spatial coordinates, supporting detailed, open-ended in-situ instruction generation. Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and benchmark are available at \href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.
        ]]></description>
    </item>
    <item>
        <title>A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions</title>
        <link>https://arxiv.org/abs/2506.04077</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04077v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chung-Chun Wang, Jhen-Ke Lin, Hao-Chien Lu, Hong-Yun Lin, Berlin Chen</dc:creator>
        <description><![CDATA[
            背景：自动口语评估（ASA）在观点表达方面常受标注录音稀缺的限制，影响提示多样性和评分可靠性。方法：提出一种新的训练范式，利用大语言模型生成不同水平的多样回应，通过文本转语音合成语音，用动态重要性损失根据合成与真实语音特征分布差异自适应重新加权训练实例，再用多模态大语言模型整合文本特征与语音信号直接预测分数。效果：在LTTC数据集实验表明，该方法优于依赖真实数据或传统增强的方法，能缓解资源限制。
            arXiv:2506.04077v1 Announce Type: new 
Abstract: Automated speaking assessment (ASA) on opinion expressions is often hampered by the scarcity of labeled recordings, which restricts prompt diversity and undermines scoring reliability. To address this challenge, we propose a novel training paradigm that leverages a large language models (LLM) to generate diverse responses of a given proficiency level, converts responses into synthesized speech via speaker-aware text-to-speech synthesis, and employs a dynamic importance loss to adaptively reweight training instances based on feature distribution differences between synthesized and real speech. Subsequently, a multimodal large language model integrates aligned textual features with speech signals to predict proficiency scores directly. Experiments conducted on the LTTC dataset show that our approach outperforms methods relying on real data or conventional augmentation, effectively mitigating low-resource constraints and enabling ASA on opinion expressions with cross-modal information.
        ]]></description>
    </item>
    <item>
        <title>Multimodal Tabular Reasoning with Privileged Structured Information</title>
        <link>https://arxiv.org/abs/2506.04088</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04088v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jun-Peng Jiang, Yu Xia, Hai-Long Sun, Shiyin Lu, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye</dc:creator>
        <description><![CDATA[
            表格推理需对表格数据进行多步信息提取和逻辑推断，现有研究虽用大语言模型处理结构化表格，但现实中表格常以图像形式存在。本文针对表格图像的推理任务，利用训练时的结构化信息增强多模态大语言模型。面临准确对齐结构化信息与视觉表征、跨越模态差距转移推理技能的挑战。为此提出Turbo框架，借助基于DeepSeek - R1的结构感知推理轨迹生成器生成高质量跨模态数据，反复生成并选择有利推理路径。实验表明，用9k有限数据，Turbo在多数据集上达最优性能，较之前最优提升7.2%。
            arXiv:2506.04088v1 Announce Type: new 
Abstract: Tabular reasoning involves multi-step information extraction and logical inference over tabular data. While recent advances have leveraged large language models (LLMs) for reasoning over structured tables, such high-quality textual representations are often unavailable in real-world settings, where tables typically appear as images. In this paper, we tackle the task of tabular reasoning from table images, leveraging privileged structured information available during training to enhance multimodal large language models (MLLMs). The key challenges lie in the complexity of accurately aligning structured information with visual representations, and in effectively transferring structured reasoning skills to MLLMs despite the input modality gap. To address these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a new framework for multimodal tabular reasoning with privileged structured tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, contributing to high-quality modality-bridged data. On this basis, {\sc Turbo} repeatedly generates and selects the advantageous reasoning paths, further enhancing the model's tabular reasoning ability. Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo} achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across multiple datasets.
        ]]></description>
    </item>
    <item>
        <title>MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos</title>
        <link>https://arxiv.org/abs/2506.04141</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04141v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</dc:creator>
        <description><![CDATA[
            视频的序列结构对多模态大语言模型定位多帧证据和进行推理提出挑战，现有视频基准主要关注理解任务。为此提出MMR - V基准，具有长距离多帧推理、超越感知、可靠及具有混淆性等特点，包含317个视频和1257个任务。实验显示，当前模型多模态推理能力欠佳，最佳模型o4 - mini准确率仅52.5%，推理增强策略收效有限，多模态推理所需思维链与文本推理不同。该基准有望推动多模态推理能力研究。
            arXiv:2506.04141v1 Announce Type: new 
Abstract: The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as "question frame") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.
        ]]></description>
    </item>
    <item>
        <title>Does Prompt Design Impact Quality of Data Imputation by LLMs?</title>
        <link>https://arxiv.org/abs/2506.04172</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04172v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shreenidhi Srinivasan, Lydia Manikonda</dc:creator>
        <description><![CDATA[
            生成逼真的合成表格数据是机器学习的关键挑战，数据存在类别不平衡问题时难度更大。本文提出一种新的令牌感知数据插补方法，结合结构化分组的CSV式提示技术和消除输入提示中的无关上下文信息，利用大语言模型的上下文学习能力。通过两个类别不平衡的二分类数据集测试，用基于分类的评估指标评估插补效果。结果显示，该方法显著减小输入提示大小，维持或提升插补质量，为类别不平衡数据集的大模型数据插补提供实用方案。
            arXiv:2506.04172v1 Announce Type: new 
Abstract: Generating realistic synthetic tabular data presents a critical challenge in machine learning. It adds another layer of complexity when this data contain class imbalance problems. This paper presents a novel token-aware data imputation method that leverages the in-context learning capabilities of large language models. This is achieved through the combination of a structured group-wise CSV-style prompting technique and the elimination of irrelevant contextual information in the input prompt. We test this approach with two class-imbalanced binary classification datasets and evaluate the effectiveness of imputation using classification-based evaluation metrics. The experimental results demonstrate that our approach significantly reduces the input prompt size while maintaining or improving imputation quality compared to our baseline prompt, especially for datasets that are of relatively smaller in size. The contributions of this presented work is two-fold -- 1) it sheds light on the importance of prompt design when leveraging LLMs for synthetic data generation and 2) it addresses a critical gap in LLM-based data imputation for class-imbalanced datasets with missing data by providing a practical solution within computational constraints. We hope that our work will foster further research and discussions about leveraging the incredible potential of LLMs and prompt engineering techniques for synthetic data generation.
        ]]></description>
    </item>
    <item>
        <title>OpenThoughts: Data Recipes for Reasoning Models</title>
        <link>https://arxiv.org/abs/2506.04178</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04178v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, Ludwig Schmidt</dc:creator>
        <description><![CDATA[
            背景：推理模型在诸多涉及数学、代码和科学的基准测试中进步迅速，但关于最佳训练方法仍有诸多疑问，且先进模型常依赖专有数据集。方法：OpenThoughts项目旨在创建开源数据集训练推理模型，经探索得到OpenThoughts2 - 1M数据集，后通过超1000次对照实验改进数据集。效果：得到OpenThinker3 - 7B模型，在AIME 2025、LiveCodeBench 06/24 - 01/25、GPQA Diamond上分别取得53%、51%、54%的成绩，相关数据集和模型可在官网获取。
            arXiv:2506.04178v1 Announce Type: new 
Abstract: Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai.
        ]]></description>
    </item>
    <item>
        <title>SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models</title>
        <link>https://arxiv.org/abs/2506.04180</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04180v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, Roy Ka-Wei Lee</dc:creator>
        <description><![CDATA[
            长文本生成对大语言模型是重大挑战，存在连贯性、逻辑一致性及文本质量随长度增加而下降等问题。为此提出SuperWriter - Agent框架，在生成流程中引入显式结构化思考，包括规划和细化阶段。基于此构建数据集训练7B的SuperWriter - LM，还开发分层直接偏好优化（DPO）程序。实验结果显示，SuperWriter - LM在多个基准测试中表现达到了最先进水平，在自动和人工评估中超越更大规模的基线模型，消融研究也证明了分层DPO的有效性。
            arXiv:2506.04180v1 Announce Type: new 
Abstract: Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.
        ]]></description>
    </item>
    <item>
        <title>Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models</title>
        <link>https://arxiv.org/abs/2506.04182</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04182v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ruiqi Zhang, Changyi Xiao, Yixin Cao</dc:creator>
        <description><![CDATA[
            背景：大推理模型发展迅速，长思维链（CoT）提示在复杂任务表现强，但会显著增加token使用量。方法：全面对比长、短CoT策略，提出能自适应选择策略的SwitchCoT框架以平衡推理准确性和计算效率。效果：实验表明SwitchCoT能减少达50%推理成本并保持高精度，在有限token预算下，表现与单独使用长或短CoT相当甚至更优。
            arXiv:2506.04182v1 Announce Type: new 
Abstract: With the rapid advancement of large reasoning models, long Chain-of-Thought (CoT) prompting has demonstrated strong performance on complex tasks. However, this often comes with a significant increase in token usage. In this paper, we conduct a comprehensive empirical analysis comparing long and short CoT strategies. Our findings reveal that while long CoT can lead to performance improvements, its benefits are often marginal relative to its significantly higher token consumption. Specifically, long CoT tends to outperform when ample generation budgets are available, whereas short CoT is more effective under tighter budget constraints. These insights underscore the need for a dynamic approach that selects the proper CoT strategy based on task context and resource availability. To address this, we propose SwitchCoT, an automatic framework that adaptively chooses between long and short CoT strategies to balance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is designed to be budget-aware, making it broadly applicable across scenarios with varying resource constraints. Experimental results demonstrate that SwitchCoT can reduce inference costs by up to 50% while maintaining high accuracy. Notably, under limited token budgets, it achieves performance comparable to, or even exceeding, that of using either long or short CoT alone.
        ]]></description>
    </item>
    <item>
        <title>R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2506.04185</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04185v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu</dc:creator>
        <description><![CDATA[
            大语言模型在多步和长链推理方面取得显著进展，但将推理能力拓展到与搜索深度交互仍面临挑战，模型常难以找到最优推理 - 搜索交互轨迹。为此提出R - Search，这是一个推理 - 搜索集成的强化学习框架，可让大语言模型自主执行与深度搜索交互的多步推理，通过多奖励信号学习最优轨迹，动态决定检索或推理时机并整合关键证据。实验表明，在七个数据集上，R - Search比先进RAG基线在域内和域外分别最多提升32.2%和25.1%。
            arXiv:2506.04185v1 Announce Type: new 
Abstract: Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at https://github.com/QingFei1/R-Search.
        ]]></description>
    </item>
    <item>
        <title>How to Use Graph Data in the Wild to Help Graph Anomaly Detection?</title>
        <link>https://arxiv.org/abs/2506.04190</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04190v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuxuan Cao, Jiarong Xu, Chen Zhao, Jiaan Wang, Carl Yang, Chunping Wang, Yang Yang</dc:creator>
        <description><![CDATA[
            背景：图异常检测应用广泛，但图结构数据的异常检测存在标签稀缺等挑战，现有无监督方法在数据不足时难以准确捕捉正常分布。方法：提出Wild - GAD框架，基于包含大量多样图数据的统一数据库UniWildGraph，并制定基于代表性和多样性的选择标准来挑选合适的外部数据用于异常检测。效果：在六个真实数据集上实验表明，该框架比基线方法平均AUCROC提高18%，AUCPR提高32%。
            arXiv:2506.04190v1 Announce Type: new 
Abstract: In recent years, graph anomaly detection has found extensive applications in various domains such as social, financial, and communication networks. However, anomalies in graph-structured data present unique challenges, including label scarcity, ill-defined anomalies, and varying anomaly types, making supervised or semi-supervised methods unreliable. Researchers often adopt unsupervised approaches to address these challenges, assuming that anomalies deviate significantly from the normal data distribution. Yet, when the available data is insufficient, capturing the normal distribution accurately and comprehensively becomes difficult. To overcome this limitation, we propose to utilize external graph data (i.e., graph data in the wild) to help anomaly detection tasks. This naturally raises the question: How can we use external data to help graph anomaly detection tasks? To answer this question, we propose a framework called Wild-GAD. It is built upon a unified database, UniWildGraph, which comprises a large and diverse collection of graph data with broad domain coverage, ample data volume, and a unified feature space. Further, we develop selection criteria based on representativity and diversity to identify the most suitable external data for anomaly detection task. Extensive experiments on six real-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the baseline methods, our framework has an average 18% AUCROC and 32% AUCPR improvement over the best-competing methods.
        ]]></description>
    </item>
    <item>
        <title>EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation</title>
        <link>https://arxiv.org/abs/2506.04205</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04205v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jinghan Jia, Hadi Reisizadeh, Chongyu Fan, Nathalie Baracaldo, Mingyi Hong, Sijia Liu</dc:creator>
        <description><![CDATA[
            背景：大语言模型用思维链（CoT）监督训练时展现出推理能力，但冗长的CoT痕迹增加了蒸馏过程的训练成本。方法：研究CoT压缩问题，提出Edge - Preserving Condensation方法EPiC，选择性保留CoT痕迹的首尾部分，舍弃中间部分。效果：在多个模型家族和基准测试中，EPiC能减少超34%的训练时间，在MATH500上实现无损推理准确率，与完整CoT监督相当。
            arXiv:2506.04205v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities when trained with chain-of-thought (CoT) supervision. However, the long and verbose CoT traces, especially those distilled from large reasoning models (LRMs) such as DeepSeek-R1, significantly increase training costs during the distillation process, where a non-reasoning base model is taught to replicate the reasoning behavior of an LRM. In this work, we study the problem of CoT condensation for resource-efficient reasoning training, aimed at pruning intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling supervised model training on length-reduced CoT data while preserving both answer accuracy and the model's ability to generate coherent reasoning. Our rationale is that CoT traces typically follow a three-stage structure: problem understanding, exploration, and solution convergence. Through empirical analysis, we find that retaining the structure of the reasoning trace, especially the early stage of problem understanding (rich in reflective cues) and the final stage of solution convergence, is sufficient to achieve lossless reasoning supervision. To this end, we propose an Edge-Preserving Condensation method, EPiC, which selectively retains only the initial and final segments of each CoT trace while discarding the middle portion. This design draws an analogy to preserving the "edge" of a reasoning trajectory, capturing both the initial problem framing and the final answer synthesis, to maintain logical continuity. Experiments across multiple model families (Qwen and LLaMA) and benchmarks show that EPiC reduces training time by over 34% while achieving lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To the best of our knowledge, this is the first study to explore thought-level CoT condensation for efficient reasoning model distillation.
        ]]></description>
    </item>
    <item>
        <title>Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models</title>
        <link>https://arxiv.org/abs/2506.04220</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04220v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang</dc:creator>
        <description><![CDATA[
            背景：在多模态大模型中实现空间推理对智能交互至关重要。方法：提出Struct2D框架，结合鸟瞰图、物体标记和元数据，必要时加入第一人称关键帧；进行零样本分析，构建含20万对问答的Struct2D - Set数据集对开源模型微调。效果：闭源模型在结构化二维输入下展现强空间推理能力；微调后的开源模型在多个基准测试中表现出色，证明结构化二维输入可有效连接感知和语言推理，代码和数据集将公开。
            arXiv:2506.04220v1 Announce Type: new 
Abstract: Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can LMMs reason about 3D space using only structured 2D representations derived from perception? We introduce Struct2D, a perception-guided prompting framework that combines bird's-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with structured 2D inputs, effectively handling tasks such as relative direction estimation and route planning. Building on these insights, we construct Struct2D-Set, a large-scale instruction tuning dataset with 200K fine-grained QA pairs across eight spatial reasoning categories, generated automatically from 3D indoor scenes. We fine-tune an open-source LMM (Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple benchmarks, including 3D question answering, dense captioning, and object grounding. Our approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in LMMs-without requiring explicit 3D representations as input. We will release both our code and dataset to support future research.
        ]]></description>
    </item>
    <item>
        <title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2505.24073</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.24073v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Zhengzhong Tu</dc:creator>
        <description><![CDATA[
            背景：大视觉语言模型在多模态任务有进展，但受限于静态训练数据、易产生幻觉等问题。方法：本文首次系统剖析多模态检索增强生成（RAG）流程，研究检索阶段的模态配置和策略、重排序阶段减轻位置偏差及提高证据相关性的策略，以及生成阶段如何将检索候选融入最终生成过程，还探索了通过自我反思整合重排序和生成的统一框架。效果：对大视觉语言模型RAG的全面探索带来深刻见解，未微调情况下平均性能提升5%。
            arXiv:2505.24073v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.
        ]]></description>
    </item>
    <item>
        <title>From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation</title>
        <link>https://arxiv.org/abs/2506.03801</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03801v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Peter Pfeiffer, Alexander Rombach, Maxim Majlatow, Nijat Mehdiyev</dc:creator>
        <description><![CDATA[
            背景：传统业务流程管理在动态环境中存在刚性、不透明和可扩展性问题，大语言模型带来变革机遇同时也有风险。方法：本文探索四个实际用例，通过结合可信的过程智能的大语言模型，以人机协作解决特定领域挑战，如制造中集成机器学习与对话、建模中用对话界面设计、药物监测中用知识图谱增强大语言模型、纺织设计中用多智能体系统。效果：为在关键业务流程管理环境中应用大语言模型的研究者和从业者提供了可行见解。
            arXiv:2506.03801v1 Announce Type: cross 
Abstract: Traditional Business Process Management (BPM) struggles with rigidity, opacity, and scalability in dynamic environments while emerging Large Language Models (LLMs) present transformative opportunities alongside risks. This paper explores four real-world use cases that demonstrate how LLMs, augmented with trustworthy process intelligence, redefine process modeling, prediction, and automation. Grounded in early-stage research projects with industrial partners, the work spans manufacturing, modeling, life-science, and design processes, addressing domain-specific challenges through human-AI collaboration. In manufacturing, an LLM-driven framework integrates uncertainty-aware explainable Machine Learning (ML) with interactive dialogues, transforming opaque predictions into auditable workflows. For process modeling, conversational interfaces democratize BPMN design. Pharmacovigilance agents automate drug safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable textile design employs multi-agent systems to navigate regulatory and environmental trade-offs. We intend to examine tensions between transparency and efficiency, generalization and specialization, and human agency versus automation. By mapping these trade-offs, we advocate for context-sensitive integration prioritizing domain needs, stakeholder values, and iterative human-in-the-loop workflows over universal solutions. This work provides actionable insights for researchers and practitioners aiming to operationalize LLMs in critical BPM environments.
        ]]></description>
    </item>
    <item>
        <title>Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning</title>
        <link>https://arxiv.org/abs/2506.03939</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03939v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junqi Gao, Xiang Zou, YIng Ai, Dong Li, Yichen Niu, Biqing Qi, Jianxing Liu</dc:creator>
        <description><![CDATA[
            背景：图检索增强生成（GraphRAG）可提升大语言模型在专业领域的事实准确性和生成质量，但现有方法存在信息聚合低效、推理机制僵化的问题。方法：提出基于多智能体协作的GraphRAG方法Graph Counselor，利用自适应图信息提取模块精准建模复杂图结构、动态调整信息提取策略，通过多视角自我反思模块提升推理结果的准确性和语义一致性。效果：在多个图推理任务中优于现有方法，展现出更高的推理准确性和泛化能力。
            arXiv:2506.03939v1 Announce Type: cross 
Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.
        ]]></description>
    </item>
    <item>
        <title>Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models</title>
        <link>https://arxiv.org/abs/2506.04210</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04210v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi</dc:creator>
        <description><![CDATA[
            背景：当前推理模型测试时扩展思维痕迹的做法引发思考，即更多思考是否能提升推理能力。方法：通过详细实证研究发现‘过度思考’会导致性能先升后降，用简单概率模型解释该非单调趋势，并提出受Best - of - N采样启发的并行思维方法，在相同推理预算内生成多条独立推理路径，通过多数投票选择最一致的响应。效果：相比扩展思维，准确率最高可提升20%。
            arXiv:2506.04210v1 Announce Type: cross 
Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like "Wait" or "Let me rethink" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to "overthinking". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from "more thinking" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.
        ]]></description>
    </item>
    <item>
        <title>EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost</title>
        <link>https://arxiv.org/abs/2306.01310</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2306.01310v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jaeseung Heo, Seungbeom Lee, Sungsoo Ahn, Dongwoo Kim</dc:creator>
        <description><![CDATA[
            背景：数据增强对提升模型性能至关重要，但图数据结构复杂不规则，其增强颇具挑战。方法：提出EPIC，利用图编辑距离概念构建编辑路径来表示图间转换过程，还引入上下文敏感的成本模型，通过学习框架确定编辑操作的重要性。效果：随机采样编辑路径上的图丰富训练集，提升分类模型泛化能力。实验表明，该方法在多个基准数据集的诸多任务中优于现有增强技术。
            arXiv:2306.01310v3 Announce Type: replace 
Abstract: Data augmentation plays a critical role in improving model performance across various domains, but it becomes challenging with graph data due to their complex and irregular structure. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. To interpolate between two graphs lying in an irregular domain, EPIC leverages the concept of graph edit distance, constructing an edit path that represents the transformation process between two graphs via edit operations. Moreover, our method introduces a context-sensitive cost model that accounts for the importance of specific edit operations formulated through a learning framework. This allows for a more nuanced transformation process, where the edit distance is not merely count-based but reflects meaningful graph attributes. With randomly sampled graphs from the edit path, we enrich the training set to enhance the generalization capability of classification models. Experimental evaluations across several benchmark datasets demonstrate that our approach outperforms existing augmentation techniques in many tasks.
        ]]></description>
    </item>
    <item>
        <title>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</title>
        <link>https://arxiv.org/abs/2308.09583</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2308.09583v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, Dongmei Zhang</dc:creator>
        <description><![CDATA[
            背景：现有开源大语言模型多仅在大规模互联网数据上预训练，缺乏数学相关优化。方法：提出WizardMath，通过将强化学习方法RLEIF应用于数学领域，在不使用外部Python工具的情况下增强大语言模型的数学思维链推理能力。效果：在GSM8k和MATH两个数学推理基准测试中，WizardMath - Mistral 7B大幅超越顶级开源大语言模型，WizardMath 70B甚至超过GPT - 3.5 - Turbo等模型。
            arXiv:2308.09583v3 Announce Type: replace 
Abstract: Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM
        ]]></description>
    </item>
    <item>
        <title>Easy attention: A simple attention mechanism for temporal predictions with transformers</title>
        <link>https://arxiv.org/abs/2308.12874</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2308.12874v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Marcial Sanchis-Agudo, Yuning Wang, Roger Arnau, Luca Guastoni, Jasmin Lim, Karthik Duraisamy, Ricardo Vinuesa</dc:creator>
        <description><![CDATA[
            背景：为提升用于混沌系统时间动态预测的Transformer神经网络的鲁棒性。方法：提出名为Easy Attention的新型注意力机制，直接将注意力分数作为可学习参数，还通过对softmax注意力分数进行奇异值分解发现自注意力在注意力分数张成的空间中压缩了查询和键的贡献。效果：该方法在混沌系统时间动态的重建和预测中表现出色，比自注意力或常用的LSTM网络更具鲁棒性且复杂度更低，在洛伦兹系统等中性能得到提升。
            arXiv:2308.12874v4 Announce Type: replace 
Abstract: To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention which we demonstrate in time-series reconstruction and prediction. While the standard self attention only makes use of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through the singular-value decomposition (SVD) on the softmax attention score, we further observe that self attention compresses the contributions from both queries and keys in the space spanned by the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than self attention or the widely-used long short-term memory (LSTM) network. We show the improved performance of the easy-attention method in the Lorenz system, a turbulence shear flow and a model of a nuclear reactor.
        ]]></description>
    </item>
    <item>
        <title>Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models</title>
        <link>https://arxiv.org/abs/2310.12049</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2310.12049v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing</dc:creator>
        <description><![CDATA[
            背景：现有文本评分方法存在需大量语料、处理短文本困难或需人工标注数据等问题。方法：开发利用生成式大语言模型的文本评分框架，采用概念引导的思维链（CGCoT），通过研究人员设计的提示链生成文本的概念特定分解，用大语言模型进行成对比较，再用概率模型汇总得分。效果：与广泛使用的无监督文本评分方法相比，与人类判断的相关性更强；在有监督环境下，除小试点数据集外无需额外人工标注数据，预测效果与在数千条人工标注推文上微调的RoBERTa - Large相当。
            arXiv:2310.12049v3 Announce Type: replace 
Abstract: Existing text scoring methods require a large corpus, struggle with short texts, or require hand-labeled data. We develop a text scoring framework that leverages generative large language models (LLMs) to (1) set texts against the backdrop of information from the near-totality of the web and digitized media, and (2) effectively transform pairwise text comparisons from a reasoning problem to a pattern recognition task. Our approach, concept-guided chain-of-thought (CGCoT), utilizes a chain of researcher-designed prompts with an LLM to generate a concept-specific breakdown for each text, akin to guidance provided to human coders. We then pairwise compare breakdowns using an LLM and aggregate answers into a score using a probability model. We apply this approach to better understand speech reflecting aversion to specific political parties on Twitter, a topic that has commanded increasing interest because of its potential contributions to democratic backsliding. We achieve stronger correlations with human judgments than widely used unsupervised text scoring methods like Wordfish. In a supervised setting, besides a small pilot dataset to develop CGCoT prompts, our measures require no additional hand-labeled data and produce predictions on par with RoBERTa-Large fine-tuned on thousands of hand-labeled tweets. This project showcases the potential of combining human expertise and LLMs for scoring tasks.
        ]]></description>
    </item>
    <item>
        <title>LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models</title>
        <link>https://arxiv.org/abs/2405.17829</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2405.17829v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jinho Chang, Jong Chul Ye</dc:creator>
        <description><![CDATA[
            背景：扩散模型成为前沿生成模型，但分子的离散性使扩散模型难以将原始数据与自然语言等复杂条件关联。方法：提出名为LDMol的潜在扩散模型用于文本条件下的分子生成，采用对比学习策略从文本数据中提取嵌入分子结构特征的新特征空间。效果：LDMol在文本到分子生成基准测试中优于现有自回归基线，是首批在文本数据生成中通过更好选择潜在域超越自回归模型的扩散模型之一，还能应用于下游任务。
            arXiv:2405.17829v4 Announce Type: replace 
Abstract: With the emergence of diffusion models as a frontline generative model, many researchers have proposed molecule generation techniques with conditional diffusion models. However, the unavoidable discreteness of a molecule makes it difficult for a diffusion model to connect raw data with highly complex conditions like natural language. To address this, here we present a novel latent diffusion model dubbed LDMol for text-conditioned molecule generation. By recognizing that the suitable latent space design is the key to the diffusion model performance, we employ a contrastive learning strategy to extract novel feature space from text data that embeds the unique characteristics of the molecule structure. Experiments show that LDMol outperforms the existing autoregressive baselines on the text-to-molecule generation benchmark, being one of the first diffusion models that outperforms autoregressive models in textual data generation with a better choice of the latent domain. Furthermore, we show that LDMol can be applied to downstream tasks such as molecule-to-text retrieval and text-guided molecule editing, demonstrating its versatility as a diffusion model.
        ]]></description>
    </item>
    <item>
        <title>CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks</title>
        <link>https://arxiv.org/abs/2406.02524</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.02524v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Maciej Besta, Lorenzo Paleari, Marcin Copik, Robert Gerstenberger, Ales Kubicek, Piotr Nyczyk, Patrick Iff, Eric Schreiber, Tanja Srindran, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler</dc:creator>
        <description><![CDATA[
            大语言模型（LLMs）虽变革诸多领域，但验证其输出仍是挑战，尤其是复杂开放性任务。为解决此问题，研究提出CheckEmbed（CE）验证方法。CE利用强大的现代嵌入大语言模型将LLM答案简化为单个嵌入向量，能在整个答案层面进行快速、语义丰富的比较。通过对13种验证基线分析，凸显了CE的有效性、效率、通用性和简单性。实证结果表明，CE能可靠检测封闭和开放性任务中的幻觉，还能推广到视觉等其他模态。
            arXiv:2406.02524v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are transforming a wide range of domains, yet verifying their outputs remains a significant challenge, especially for complex open-ended tasks such as consolidation, summarization, and knowledge extraction. To address this, we introduce CheckEmbed (CE): a simple, scalable, and accurate verification method. CE reduces each LLM answer to a single embedding vector using powerful modern embedding LLM models like SFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied on weaker encoders like BERT, forcing them to operate at token or sentence granularity. In contrast, CE performs fast, semantically rich comparisons directly at the whole-answer level, overcoming key limitations in both accuracy and scalability. We conduct a comprehensive design and time complexity analysis across 13 verification baselines, including classical text scorers (e.g., BLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators (e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency, versatility, and simplicity of CE. Empirical results show that CE reliably detects hallucinations in both closed and open-ended tasks. We further present evidence that CE generalizes beyond text to other modalities such as vision, establishing it as a practical and versatile verification framework.
        ]]></description>
    </item>
    <item>
        <title>Quantifying Prediction Consistency Under Fine-Tuning Multiplicity in Tabular LLMs</title>
        <link>https://arxiv.org/abs/2407.04173</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2407.04173v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Faisal Hamman, Pasan Dissanayake, Saumitra Mishra, Freddy Lecue, Sanghamitra Dutta</dc:creator>
        <description><![CDATA[
            背景：在表格分类任务中微调大语言模型会出现微调多样性现象，即表现相当的模型对相同输入会有冲突预测，这影响了表格大语言模型在高风险应用中的可靠性。方法：提出一种无需昂贵模型再训练就能量化单个预测一致性的新方法，通过分析嵌入空间中输入周围模型的局部行为来量化。效果：实验表明，该局部稳定性度量能提前捕捉多个微调模型实际多样性下的一致性，优于其他竞争度量。
            arXiv:2407.04173v2 Announce Type: replace 
Abstract: Fine-tuning LLMs on tabular classification tasks can lead to the phenomenon of fine-tuning multiplicity where equally well-performing models make conflicting predictions on the same input. Fine-tuning multiplicity can arise due to variations in the training process, e.g., seed, weight initialization, minor changes to training data, etc., raising concerns about the reliability of Tabular LLMs in high-stakes applications such as finance, hiring, education, healthcare. Our work formalizes this unique challenge of fine-tuning multiplicity in Tabular LLMs and proposes a novel measure to quantify the consistency of individual predictions without expensive model retraining. Our measure quantifies a prediction's consistency by analyzing (sampling) the model's local behavior around that input in the embedding space. Interestingly, we show that sampling in the local neighborhood can be leveraged to provide probabilistic guarantees on prediction consistency under a broad class of fine-tuned models, i.e., inputs with sufficiently high local stability (as defined by our measure) also remain consistent across several fine-tuned models with high probability. We perform experiments on multiple real-world datasets to show that our local stability measure preemptively captures consistency under actual multiplicity across several fine-tuned models, outperforming competing measures.
        ]]></description>
    </item>
    <item>
        <title>KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action Recognition</title>
        <link>https://arxiv.org/abs/2409.09444</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.09444v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhaoyu Chen, Xing Li, Qian Huang, Qiang Geng, Tianjin Yang, Shihao Han</dc:creator>
        <description><![CDATA[
            背景：基于点云序列的3D动作识别虽有进展，但现有方法难以平衡肢体微动作精度和姿态宏观结构完整性，导致动作推理关键信息缺失。方法：引入通过D - Hyperpoint嵌入模块生成的新型数据类型D - Hyperpoint，封装区域瞬时运动和全局静态姿态；提出D - Hyperpoint KANsMixer模块学习动作判别信息，集成Kolmogorov - Arnold Networks增强时空交互；提出时空解耦网络架构KAN - HyperpointNet。效果：在两个公开数据集上表现达到了先进水平。
            arXiv:2409.09444v2 Announce Type: replace 
Abstract: Point cloud sequence-based 3D action recognition has achieved impressive performance and efficiency. However, existing point cloud sequence modeling methods cannot adequately balance the precision of limb micro-movements with the integrity of posture macro-structure, leading to the loss of crucial information cues in action inference. To overcome this limitation, we introduce D-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding module. D-Hyperpoint encapsulates both regional-momentary motion and global-static posture, effectively summarizing the unit human action at each moment. In addition, we present a D-Hyperpoint KANsMixer module, which is recursively applied to nested groupings of D-Hyperpoints to learn the action discrimination information and creatively integrates Kolmogorov-Arnold Networks (KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we propose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for 3D action recognition. Extensive experiments on two public datasets: MSR Action3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our method.
        ]]></description>
    </item>
    <item>
        <title>Exploring Representations and Interventions in Time Series Foundation Models</title>
        <link>https://arxiv.org/abs/2409.12915</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.12915v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Micha{\l} Wili\'nski, Mononito Goswami, Willa Potosnak, Nina \.Zukowska, Artur Dubrawski</dc:creator>
        <description><![CDATA[
            背景：时间序列基础模型（TSFMs）应用广泛，但对其内部表征和学习概念理解不足。方法：研究不同TSFMs表征的结构和冗余性，检查不同模型大小层内和层间的自相似性，还探索模型学习的概念并通过潜在空间操纵来影响模型行为。效果：分析揭示表征存在块状冗余，可用于剪枝以提高推理速度和效率；实验表明操纵干预能引入新特征，如为初始缺乏周期性或趋势的信号添加这些特征。
            arXiv:2409.12915v4 Announce Type: replace 
Abstract: Time series foundation models (TSFMs) promise to be powerful tools for a wide range of applications. However, their internal representations and learned concepts are still not well understood. In this study, we investigate the structure and redundancy of representations across various TSFMs, examining the self-similarity of model layers within and across different model sizes. This analysis reveals block-like redundancy in the representations, which can be utilized for informed pruning to improve inference speed and efficiency. Additionally, we explore the concepts learned by these models - such as periodicity and trends - and how these can be manipulated through latent space steering to influence model behavior. Our experiments show that steering interventions can introduce new features, e.g., adding periodicity or trends to signals that initially lacked them. These findings underscore the value of representational analysis for optimizing models and demonstrate how conceptual steering offers new possibilities for more controlled and efficient time series analysis with TSFMs.
        ]]></description>
    </item>
    <item>
        <title>VinePPO: Refining Credit Assignment in RL Training of LLMs</title>
        <link>https://arxiv.org/abs/2410.01679</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.01679v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux</dc:creator>
        <description><![CDATA[
            背景：大语言模型用于复杂推理任务时，合理的信用分配对提升性能至关重要，而近端策略优化（PPO）中价值网络的有效性存疑。方法：系统评估价值网络，提出VinePPO方法，利用语言环境的灵活性计算无偏的基于蒙特卡罗的估计。效果：在MATH和GSM8K数据集上，VinePPO比PPO等基线方法耗时最多少3.0倍，且在给定训练准确率下有更高测试准确率，能捕捉更多泛化信号。
            arXiv:2410.01679v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a common reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, recent approaches achieve strong results without it, raising questions about the efficacy of value networks in practice. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they often produce poor estimate of expected return and barely outperform a random baseline when comparing alternative steps. This motivates our key question: Can improved credit assignment enhance RL training for LLMs? To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates. Our method consistently outperforms PPO and other baselines across MATH and GSM8K datasets in less wall-clock time (up to 3.0x). Crucially, it achieves higher test accuracy for a given training accuracy, capturing more generalization signal per sample. These results emphasize the importance of accurate credit assignment in RL training of LLM.
        ]]></description>
    </item>
    <item>
        <title>Nudging: Inference-time Alignment of LLMs via Guided Decoding</title>
        <link>https://arxiv.org/abs/2410.09300</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.09300v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yu Fei, Yasaman Razeghi, Sameer Singh</dc:creator>
        <description><![CDATA[
            背景：大语言模型（LLMs）需对齐才能有效安全地遵循用户指令，但为每个基础模型训练对齐版本会产生大量计算开销。方法：提出NUDGING算法，在推理时用小的对齐模型引导基础模型输出，利用基础模型生成某些风格标记时的高不确定性，由小对齐模型生成微调标记。效果：在3个模型族的多种开放指令任务中评估，用小7 - 14倍的对齐模型微调大基础模型，零样本性能可与大对齐模型媲美甚至超越。
            arXiv:2410.09300v4 Announce Type: replace 
Abstract: Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/ .
        ]]></description>
    </item>
    <item>
        <title>RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning</title>
        <link>https://arxiv.org/abs/2410.16502</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.16502v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jason Chan, Robert Gaizauskas, Zhixue Zhao</dc:creator>
        <description><![CDATA[
            背景：形式逻辑虽能让计算机用自然语言推理，但在“规则破坏者”场景中会得出违背人类常识的结论。方法：受认知科学启发，创建RULEBREAKERS数据集，评估7个大语言模型（LLMs）识别和回应“规则破坏者”的能力。效果：多数模型包括GPT - 4o在该数据集上准确率一般，且存在过度僵化应用逻辑规则的倾向，分析认为这可能与模型对世界知识利用不足和注意力分布模式有关，揭示了当前LLMs的局限性。
            arXiv:2410.16502v3 Announce Type: replace 
Abstract: Formal logic enables computers to reason in natural language by representing sentences in symbolic forms and applying rules to derive conclusions. However, in what our study characterizes as "rulebreaker" scenarios, this method can lead to conclusions that are typically not inferred or accepted by humans given their common sense and factual knowledge. Inspired by works in cognitive science, we create RULEBREAKERS, the first dataset for rigorously evaluating the ability of large language models (LLMs) to recognize and respond to rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules unlike what is expected from typical human reasoners. Further analysis suggests that this apparent failure is potentially associated with the models' poor utilization of their world knowledge and their attention distribution patterns. Whilst revealing a limitation of current LLMs, our study also provides a timely counterbalance to a growing body of recent works that propose methods relying on formal logic to improve LLMs' general reasoning capabilities, highlighting their risk of further increasing divergence between LLMs and human-like reasoning.
        ]]></description>
    </item>
    <item>
        <title>Mixture of Experts for Node Classification</title>
        <link>https://arxiv.org/abs/2412.00418</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.00418v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yu Shi, Yiqi Wang, WeiXuan Lang, Jiaxin Zhang, Pan Dong, Aiping Li</dc:creator>
        <description><![CDATA[
            现实世界图中的节点在度和同质性等方面呈现多样模式，但现有节点预测器大多无法捕捉多种节点模式或基于不同模式预测，导致分类性能不佳。为此，本文提出用于节点分类的专家混合框架MoE - NP。该框架结合多种节点预测器，并根据节点模式策略性选择模型。在多个真实数据集上的实验结果表明，MoE - NP显著提升了性能。
            arXiv:2412.00418v3 Announce Type: replace 
Abstract: Nodes in the real-world graphs exhibit diverse patterns in numerous aspects, such as degree and homophily. However, most existent node predictors fail to capture a wide range of node patterns or to make predictions based on distinct node patterns, resulting in unsatisfactory classification performance. In this paper, we reveal that different node predictors are good at handling nodes with specific patterns and only apply one node predictor uniformly could lead to suboptimal result. To mitigate this gap, we propose a mixture of experts framework, MoE-NP, for node classification. Specifically, MoE-NP combines a mixture of node predictors and strategically selects models based on node patterns. Experimental results from a range of real-world datasets demonstrate significant performance improvements from MoE-NP.
        ]]></description>
    </item>
    <item>
        <title>MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale</title>
        <link>https://arxiv.org/abs/2412.05237</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.05237v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue</dc:creator>
        <description><![CDATA[
            背景：开源多模态大语言模型在多模态任务中潜力大，但推理能力受现有指令微调数据集限制，这些数据集任务简单且无中间推理过程。方法：提出可扩展且经济的方法构建含丰富中间推理的大规模多模态指令微调数据集，用开源模型创建含1200万指令-响应对的数据集。效果：在该数据集上训练模型显著提升推理能力，在MathVerse等基准测试中达最优，非推理基准测试最高提升4%，消融实验凸显数据集构建关键组件重要性。
            arXiv:2412.05237v2 Announce Type: replace 
Abstract: Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.
        ]]></description>
    </item>
    <item>
        <title>Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria</title>
        <link>https://arxiv.org/abs/2412.21006</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.21006v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Joonwon Jang, Jaehee Kim, Wonbin Kweon, Seonghyeon Lee, Hwanjo Yu</dc:creator>
        <description><![CDATA[
            背景：大语言模型依赖生成大量中间推理单元提升最终答案质量，但会增加推理成本，以往无明确标准的标记级缩减方法效果不佳。方法：提出一种基于似然标准“冗长性”的句子级推理缩减框架，利用冗长性识别并移除冗余推理句子。效果：在各种推理任务的实验中，与使用完整推理路径训练的模型相比，该方法平均性能提升7.71%，标记生成减少19.87%。
            arXiv:2412.21006v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) rely on generating extensive intermediate reasoning units (e.g., tokens, sentences) to enhance final answer quality across a wide range of complex tasks. While this approach has proven effective, it inevitably increases substantial inference costs. Previous methods adopting token-level reduction without clear criteria result in poor performance compared to models trained with complete rationale. To address this challenge, we propose a novel sentence-level rationale reduction framework leveraging likelihood-based criteria, verbosity, to identify and remove redundant reasoning sentences. Unlike previous approaches, our method leverages verbosity to selectively remove redundant reasoning sentences while preserving reasoning capabilities. Our experimental results across various reasoning tasks demonstrate that our method improves performance by an average of 7.71% while reducing token generation by 19.87% compared to model trained with complete reasoning paths.
        ]]></description>
    </item>
    <item>
        <title>Generative Unordered Flow for Set-Structured Data Generation</title>
        <link>https://arxiv.org/abs/2501.17770</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.17770v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yangming Li, Chaoyu Liu, Carola-Bibiane Sch\"onlieb</dc:creator>
        <description><![CDATA[
            背景：基于流的生成模型在多种数据模态表现良好，但对无序数据（如空间点集）的扩展研究较少，此前模型多针对有序向量数据。方法：提出无序流，一种用于集合结构化数据生成的流基生成模型，将无序数据转换为合适的函数表示，通过函数值流匹配学习其概率测度，用类似粒子滤波的方法实现从函数表示到无序数据的逆映射。效果：在多个真实数据集实验表明，该模型生成集合结构化数据效果好，显著优于先前基线。
            arXiv:2501.17770v2 Announce Type: replace 
Abstract: Flow-based generative models have demonstrated promising performance across a broad spectrum of data modalities (e.g., image and text). However, there are few works exploring their extension to unordered data (e.g., spatial point set), which is not trivial because previous models are mostly designed for vector data that are naturally ordered. In this paper, we present unordered flow, a type of flow-based generative model for set-structured data generation. Specifically, we convert unordered data into an appropriate function representation, and learn the probability measure of such representations through function-valued flow matching. For the inverse map from a function representation to unordered data, we propose a method similar to particle filtering, with Langevin dynamics to first warm-up the initial particles and gradient-based search to update them until convergence. We have conducted extensive experiments on multiple real-world datasets, showing that our unordered flow model is very effective in generating set-structured data and significantly outperforms previous baselines.
        ]]></description>
    </item>
    <item>
        <title>ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration</title>
        <link>https://arxiv.org/abs/2502.00675</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.00675v5</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei Yao, Anupam Datta, Hao Zhang</dc:creator>
        <description><![CDATA[
            这篇论文背景是文本到SQL系统虽能对结构化数据库进行自然语言查询，但因复杂模式、多样方言和复杂查询需求，在企业环境部署困难。为此提出ReFoRCE，方法包括通过模式表分组和大模型引导的模式链接压缩数据库信息、自优化修正错误、用多数投票共识选择高置信度候选、基于执行反馈迭代探索列以解决复杂情况。效果显著，在Spider 2.0 - Snow上得分35.83，在Spider 2.0 - Lite上得分36.56，取得新的最优结果。
            arXiv:2502.00675v5 Announce Type: replace 
Abstract: We present ReFoRCE, a Text-to-SQL agent that tops the Spider 2.0 leaderboard--a challenging benchmark reflecting complex, real-world Text-to-SQL scenarios. While Text-to-SQL systems enable natural language queries over structured databases, deploying them in enterprise environments remains difficult due to large, complex schemas (with over 1,000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake), and sophisticated query requirements (e.g., transformations and analytics). ReFoRCE addresses these challenges through: (a) database information compression via pattern-based table grouping and LLM-guided schema linking to alleviate long-context issues; (b) self-refinement to iteratively correct syntax and semantic errors across dialects; (c) majority-vote consensus to select high-confidence candidates while deferring ambiguous cases arising from sophisticated queries; and (d) iterative column exploration guided by execution feedback to resolve those deferred cases. ReFoRCE achieves new state-of-the-art results, with scores of 35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite.
        ]]></description>
    </item>
    <item>
        <title>Social Genome: Grounded Social Reasoning Abilities of Multimodal Models</title>
        <link>https://arxiv.org/abs/2502.15109</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.15109v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency</dc:creator>
        <description><![CDATA[
            社交推理能力对AI系统在社交场景中理解和回应多模态人类交流至关重要。本文推出首个多模态模型细粒度、有根据的社交推理能力基准SOCIAL GENOME，包含272个互动视频及1486条人工标注推理轨迹。该基准首次研究社交推理中的外部知识，能全面评估模型生成推理轨迹的语义和结构质量。通过实验验证其有效性，指出当前模型性能差距，为提升多模态模型社交推理能力提供研究方向。
            arXiv:2502.15109v4 Announce Type: replace 
Abstract: Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce SOCIAL GENOME, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). SOCIAL GENOME is also the first modeling challenge to study external knowledge in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of SOCIAL GENOME through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.
        ]]></description>
    </item>
    <item>
        <title>FragFM: Hierarchical Framework for Efficient Molecule Generation via Fragment-Level Discrete Flow Matching</title>
        <link>https://arxiv.org/abs/2502.15805</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.15805v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Joongwon Lee, Seonghwan Kim, Seokhyun Moon, Hyunwoo Kim, Woo Youn Kim</dc:creator>
        <description><![CDATA[
            背景：分子图生成研究中需高效方法。方法：提出FragFM框架，通过片段级离散流匹配生成分子，利用粗到细的自动编码器在原子级重建细节，结合随机片段包策略处理大量片段空间，还提出NPGen基准评估模型。效果：基于片段的方法比基于原子的方法能更好控制属性，有更多灵活性；在多个基准测试中表现优越，为大规模、属性感知的分子设计探索化学空间奠定基础。
            arXiv:2502.15805v2 Announce Type: replace 
Abstract: We introduce FragFM, a novel hierarchical framework via fragment-level discrete flow matching for efficient molecular graph generation. FragFM generates molecules at the fragment level, leveraging a coarse-to-fine autoencoder to reconstruct details at the atom level. Together with a stochastic fragment bag strategy to effectively handle an extensive fragment space, our framework enables more efficient and scalable molecular generation. We demonstrate that our fragment-based approach achieves better property control than the atom-based method and additional flexibility through conditioning the fragment bag. We also propose a Natural Product Generation benchmark (NPGen) to evaluate modern molecular graph generative models' ability to generate natural product-like molecules. Since natural products are biologically prevalidated and differ from typical drug-like molecules, our benchmark provides a more challenging yet meaningful evaluation relevant to drug discovery. We conduct a FragFM comparative study against various models on diverse molecular generation benchmarks, including NPGen, demonstrating superior performance. The results highlight the potential of fragment-based generative modeling for large-scale, property-aware molecular design, paving the way for more efficient exploration of chemical space.
        ]]></description>
    </item>
    <item>
        <title>Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces</title>
        <link>https://arxiv.org/abs/2503.05283</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.05283v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodol\`a, Simone Melzi, Maks Ovsjanikov</dc:creator>
        <description><![CDATA[
            背景：当前单模态2D视觉和文本编码器学习的特征有显著结构共性，但3D编码器与其他模态的关系未被探究，现有3D基础模型常通过明确对齐目标训练。方法：研究单模态3D编码器与基于文本特征空间的事后对齐可能性，发现简单的特征对齐效果有限，进而提取对应特征空间的子空间，将学习的表征投影到低维子空间。效果：显著提高了对齐质量，提升了匹配和检索任务的准确率，还揭示了共享子空间的性质。
            arXiv:2503.05283v2 Announce Type: replace 
Abstract: Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional subspaces the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations. Our code and weights are available at https://github.com/Souhail-01/3d-text-alignment
        ]]></description>
    </item>
    <item>
        <title>PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts</title>
        <link>https://arxiv.org/abs/2503.06706</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.06706v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ming Zhang, Yuhui Wang, Yujiong Shen, Tingyi Yang, Changhao Jiang, Yilong Wu, Shihan Dou, Qinhao Chen, Zhiheng Xi, Zhihao Zhang, Yi Dong, Zhen Wang, Zhihui Fei, Mingyang Wan, Tao Liang, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang</dc:creator>
        <description><![CDATA[
            背景：过程驱动对话系统在客服和设备维护场景很重要，但大语言模型难以解决严格约束的对话任务。方法：构建包含12705条高质量中文对话指令的PFDial数据集，将UML流程图转换为结构化五元组的原子对话单元。效果：7B模型用800个样本训练、0.5B模型用全量数据训练，准确率超90%；8B模型超越GPT - 4o最高达43.88%，平均11.00%。还评估了模型在流程逆向转换中的表现并分析数据集格式影响。
            arXiv:2503.06706v2 Announce Type: replace 
Abstract: Process-driven dialogue systems, which operate under strict predefined process constraints, are essential in customer service and equipment maintenance scenarios. Although Large Language Models (LLMs) have shown remarkable progress in dialogue and reasoning, they still struggle to solve these strictly constrained dialogue tasks. To address this challenge, we construct Process Flow Dialogue (PFDial) dataset, which contains 12,705 high-quality Chinese dialogue instructions derived from 440 flowcharts containing 5,055 process nodes. Based on PlantUML specification, each UML flowchart is converted into atomic dialogue units i.e., structured five-tuples. Experimental results demonstrate that a 7B model trained with merely 800 samples, and a 0.5B model trained on total data both can surpass 90% accuracy. Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of 11.00%. We further evaluate models' performance on challenging backward transitions in process flows and conduct an in-depth analysis of various dataset formats to reveal their impact on model performance in handling decision and sequential branches. The data is released in https://github.com/KongLongGeFDU/PFDial.
        ]]></description>
    </item>
    <item>
        <title>Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation</title>
        <link>https://arxiv.org/abs/2503.10691</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.10691v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qiji Zhou, Yifan Gong, Guangsheng Bao, Hongjie Qiu, Jinqiang Li, Xiangrong Zhu, Huajian Zhang, Yue Zhang</dc:creator>
        <description><![CDATA[
            背景：反事实推理对视频理解至关重要，但现有多模态基准对此研究不足。方法：提出多维多模态基准COVER，将复杂查询分解为结构化子问题，以进行细粒度推理分析。效果：实验表明子问题准确率与反事实推理性能强相关，凸显结构化推理在视频理解中的作用，还指出增强模型推理能力对提升视频理解鲁棒性很关键，该基准为评估多模态大模型在动态环境中的逻辑推理能力建立了新标准。
            arXiv:2503.10691v2 Announce Type: replace 
Abstract: Counterfactual reasoning is crucial for robust video understanding but remains underexplored in existing multimodal benchmarks. In this paper, we introduce \textbf{COVER} (\textbf{\underline{CO}}unterfactual \textbf{\underline{V}}id\textbf{\underline{E}}o \textbf{\underline{R}}easoning), a multidimensional multimodal benchmark that systematically evaluates MLLMs across the abstract-concrete and perception-cognition dimensions. Beyond prior multimodal benchmarks, COVER decomposes complex queries into structured sub-questions, enabling fine-grained reasoning analysis. Experiments on commercial and open-source models reveal a strong correlation between sub-question accuracy and counterfactual reasoning performance, highlighting the role of structured inference in video understanding. Furthermore, our results suggest a key insight: enhancing the reasoning capability of models is essential for improving the robustness of video understanding. COVER establishes a new standard for assessing MLLMs' logical reasoning abilities in dynamic environments. Our work is available at https://github.com/gongyifan-hash/COVER-Benchmark.
        ]]></description>
    </item>
    <item>
        <title>Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2504.05276</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.05276v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yucheng Chu, Peng He, Hang Li, Haoyu Han, Kaiqi Yang, Yu Xue, Tingting Li, Joseph Krajcik, Jiliang Tang</dc:creator>
        <description><![CDATA[
            背景：简答题评估是科学教育的重要环节，大语言模型虽能辅助人工评分，但领域知识局限影响其表现，检索增强生成（RAG）是有前景的解决办法。方法：提出自适应RAG框架，基于问题和学生答案动态检索并整合特定领域知识，结合语义搜索和教育资源获取参考资料。效果：在科学教育数据集实验中，该系统相比基线大语言模型方法提高了评分准确性，表明RAG增强的评分系统有高效表现。
            arXiv:2504.05276v2 Announce Type: replace 
Abstract: Short answer assessment is a vital component of science education, allowing evaluation of students' complex three-dimensional understanding. Large language models (LLMs) that possess human-like ability in linguistic tasks are increasingly popular in assisting human graders to reduce their workload. However, LLMs' limitations in domain knowledge restrict their understanding in task-specific requirements and hinder their ability to achieve satisfactory performance. Retrieval-augmented generation (RAG) emerges as a promising solution by enabling LLMs to access relevant domain-specific knowledge during assessment. In this work, we propose an adaptive RAG framework for automated grading that dynamically retrieves and incorporates domain-specific knowledge based on the question and student answer context. Our approach combines semantic search and curated educational sources to retrieve valuable reference materials. Experimental results in a science education dataset demonstrate that our system achieves an improvement in grading accuracy compared to baseline LLM approaches. The findings suggest that RAG-enhanced grading systems can serve as reliable support with efficient performance gains.
        ]]></description>
    </item>
    <item>
        <title>Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning</title>
        <link>https://arxiv.org/abs/2504.16656</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.16656v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou</dc:creator>
        <description><![CDATA[
            背景：现有模型存在平衡复杂推理能力与广泛泛化能力的挑战。方法：提出下一代多模态推理模型Skywork R1V2，引入混合强化学习范式，结合Mixed Preference Optimization和Group Relative Policy Optimization，还提出Selective Sample Buffer机制，通过校准奖励阈值监控并缓解视觉幻觉。效果：R1V2表现出色，在多个基准测试中成绩领先，如OlympiadBench 62.6、AIME2024 78.9等，优于现有开源模型，缩小与顶级专有系统差距。
            arXiv:2504.16656v3 Announce Type: replace 
Abstract: We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that jointly leverages the Mixed Preference Optimization (MPO) and the Group Relative Policy Optimization (GRPO), which harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which effectively addresses the vanishing advantages dilemma inherent in GRPO by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinations--a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and 73.6 on MMMU. These results underscore R1V2's superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility https://huggingface.co/Skywork/Skywork-R1V2-38B.
        ]]></description>
    </item>
    <item>
        <title>THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering</title>
        <link>https://arxiv.org/abs/2505.11626</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11626v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Udita Patel, Rutu Mulkar, Jay Roberts, Cibi Chakravarthy Senthilkumar, Sujay Gandhi, Xiaofei Zheng, Naumaan Nayyar, Parul Kalra, Rafael Castrillo</dc:creator>
        <description><![CDATA[
            背景：大模型RAG问答应用缺乏无参考的评估框架。方法：提出THELMA框架，它由六个相互依赖的指标组成，用于全面、细粒度评估基于RAG的问答应用，能在无需标注源或参考回复的情况下评估、监控和改进端到端RAG问答管道。效果：可通过分析指标间的相互作用，识别问答应用中需改进的特定RAG组件。
            arXiv:2505.11626v2 Announce Type: replace 
Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference responses.We also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.
        ]]></description>
    </item>
    <item>
        <title>SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs</title>
        <link>https://arxiv.org/abs/2505.12910</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.12910v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Le Cheng, Peican Zhu, Yangming Guo, Chao Gao, Zhen Wang, Keke Tang</dc:creator>
        <description><![CDATA[
            图上的源检测在识别谣言源头方面效果显著，但现有机器学习方法难以捕捉谣言传播的内在动态。本文提出SourceDetMamba，利用状态空间模型Mamba的全局建模能力和计算效率解决该问题。先采用超图对社交网络高阶交互建模，将传播过程的时间网络快照逆序输入Mamba推断传播动态，还提出图感知状态更新机制结合结构信息。在8个数据集上评估显示，SourceDetMamba始终优于现有方法。
            arXiv:2505.12910v2 Announce Type: replace 
Abstract: Source detection on graphs has demonstrated high efficacy in identifying rumor origins. Despite advances in machine learning-based methods, many fail to capture intrinsic dynamics of rumor propagation. In this work, we present SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs, which harnesses the recent success of the state space model Mamba, known for its superior global modeling capabilities and computational efficiency, to address this challenge. Specifically, we first employ hypergraphs to model high-order interactions within social networks. Subsequently, temporal network snapshots generated during the propagation process are sequentially fed in reverse order into Mamba to infer underlying propagation dynamics. Finally, to empower the sequential model to effectively capture propagation patterns while integrating structural information, we propose a novel graph-aware state update mechanism, wherein the state of each node is propagated and refined by both temporal dependencies and topological context. Extensive evaluations on eight datasets demonstrate that SourceDetMamba consistently outperforms state-of-the-art approaches.
        ]]></description>
    </item>
    <item>
        <title>Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning</title>
        <link>https://arxiv.org/abs/2505.16836</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.16836v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fanrui Zhang, Dian Li, Qiang Zhang,  Chenjun,  sinbadliu, Junxiong Lin, Jiahong Yan, Jiawei Liu, Zheng-Jun Zha</dc:creator>
        <description><![CDATA[
            背景：社交媒体上多模态错误信息快速传播，而视频错误信息检测研究因缺乏大规模多样数据集受限，现有方法易过拟合且缺乏深度推理。方法：引入含超10万视频 - 文本对且有细粒度可解释注释的FakeVV基准，提出结合深度推理与协作规则强化学习的Fact - R1框架，通过三步训练。效果：使Fact - R1在复杂多模态错误信息场景中展现出类似高级文本强化学习系统的推理行为，建立了错误信息检测新范式。
            arXiv:2505.16836v2 Announce Type: replace 
Abstract: The rapid spread of multimodal misinformation on social media has raised growing concerns, while research on video misinformation detection remains limited due to the lack of large-scale, diverse datasets. Existing methods often overfit to rigid templates and lack deep reasoning over deceptive content. To address these challenges, we introduce FakeVV, a large-scale benchmark comprising over 100,000 video-text pairs with fine-grained, interpretable annotations. In addition, we further propose Fact-R1, a novel framework that integrates deep reasoning with collaborative rule-based reinforcement learning. Fact-R1 is trained through a three-stage process: (1) misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference alignment via Direct Preference Optimization (DPO), and (3) Group Relative Policy Optimization (GRPO) using a novel verifiable reward function. This enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those observed in advanced text-based reinforcement learning systems, but in the more complex multimodal misinformation setting. Our work establishes a new paradigm for misinformation detection, bridging large-scale video understanding, reasoning-guided alignment, and interpretable verification.
        ]]></description>
    </item>
    <item>
        <title>LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning</title>
        <link>https://arxiv.org/abs/2505.16933</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.16933v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li</dc:creator>
        <description><![CDATA[
            背景：当前多模态方法以自回归范式为主。方法：本文介绍LLaDA-V，一种基于扩散的多模态大语言模型，结合视觉指令调优与掩码扩散模型，借助视觉编码器和MLP连接器将视觉特征投影到语言嵌入空间实现多模态对齐。效果：虽在纯文本任务上弱于部分模型，但在多模态任务中表现出色，与LLaMA3-V竞争优势明显且数据可扩展性好，缩小与Qwen2-VL的差距，在多模态理解上达到了最先进水平。
            arXiv:2505.16933v2 Announce Type: replace 
Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: https://ml-gsai.github.io/LLaDA-V-demo/.
        ]]></description>
    </item>
    <item>
        <title>VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis</title>
        <link>https://arxiv.org/abs/2505.18570</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.18570v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tina Khezresmaeilzadeh, Parsa Razmara, Seyedarmin Azizi, Mohammad Erfan Sadeghi, Erfan Baghaei Potraghloo</dc:creator>
        <description><![CDATA[
            这是一篇关于多模态股票预测的研究。背景是股票价格预测复杂且重要，传统用统计模型或语言模型。方法是提出VISTA框架，利用视觉语言模型，结合历史股价文本和对应折线图，通过精心设计的思维链提示预测未来股价。效果是与ARIMA等标准基线对比，VISTA最多可高出89.83%，证明多模态推理对股票时间序列分析有效，凸显视觉语言模型在金融预测中无需特定训练的潜力。
            arXiv:2505.18570v2 Announce Type: replace 
Abstract: Stock price prediction remains a complex and high-stakes task in financial analysis, traditionally addressed using statistical models or, more recently, language models. In this work, we introduce VISTA (Vision-Language Inference for Stock Time-series Analysis), a novel, training-free framework that leverages Vision-Language Models (VLMs) for multi-modal stock forecasting. VISTA prompts a VLM with both textual representations of historical stock prices and their corresponding line charts to predict future price values. By combining numerical and visual modalities in a zero-shot setting and using carefully designed chain-of-thought prompts, VISTA captures complementary patterns that unimodal approaches often miss. We benchmark VISTA against standard baselines, including ARIMA and text-only LLM-based prompting methods. Experimental results show that VISTA outperforms these baselines by up to 89.83%, demonstrating the effectiveness of multi-modal inference for stock time-series analysis and highlighting the potential of VLMs in financial forecasting tasks without requiring task-specific training.
        ]]></description>
    </item>
    <item>
        <title>LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models</title>
        <link>https://arxiv.org/abs/2505.21082</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.21082v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jieyong Kim, Tongyoung Kim, Soojin Yoon, Jaehyung Kim, Dongha Lee</dc:creator>
        <description><![CDATA[
            背景：大语言模型虽能力强，但黑盒大语言模型输出常忽略个人偏好和推理风格，现有方法多关注响应级个性化。方法：提出RPM框架，先从用户历史中提取和分组影响响应的特征构建统计用户特定因素，再构建反映因素使用方式的个性化推理路径，推理阶段通过特征相似度检索推理对齐示例并基于结构化因素和推理路径推理。效果：实验表明，RPM能提升预测准确性和可解释性，且始终优于响应级个性化方法。
            arXiv:2505.21082v3 Announce Type: replace 
Abstract: Large language models (LLMs) have recently achieved impressive performance across a wide range of natural language tasks and are now widely used in real-world applications. Among them, black-box LLMs--served via APIs without access to model internals--are especially dominant due to their scalability and ease of deployment. Despite their strong capabilities, these models typically produce generalized responses that overlook personal preferences and reasoning styles. This has led to growing interest in black-box LLM personalization, which aims to tailor model outputs to user-specific context without modifying model parameters. However, existing approaches primarily focus on response-level personalization, attempting to match final outputs without modeling personal thought process. To address this limitation, we propose RPM, a framework for reasoning-level personalization that aligns the model's reasoning process with a user's personalized logic. RPM first constructs statistical user-specific factors by extracting and grouping response-influential features from user history. It then builds personalized reasoning paths that reflect how these factors are used in context. In the inference stage, RPM retrieves reasoning-aligned examples for new queries via feature-level similarity and performs inference conditioned on the structured factors and retrieved reasoning paths, enabling the model to follow user-specific reasoning trajectories. This reasoning-level personalization enhances both predictive accuracy and interpretability by grounding model outputs in user-specific logic through structured information. Extensive experiments across diverse tasks show that RPM consistently outperforms response-level personalization methods, demonstrating the effectiveness of reasoning-level personalization in black-box LLMs.
        ]]></description>
    </item>
    <item>
        <title>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</title>
        <link>https://arxiv.org/abs/2505.24298</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.24298v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</dc:creator>
        <description><![CDATA[
            背景：强化学习用于训练大语言模型的推理任务时，现有同步系统存在系统效率低、GPU 利用率低的问题。方法：提出 AReaL 完全异步强化学习系统，将生成与训练解耦，卷展工作者持续生成新输出，训练工作者收集一批数据就更新模型，还进行系统级优化，平衡工作负载并采用改进算法处理陈旧样本。效果：在数学和代码推理基准测试中，相比同步系统，训练速度最多提升 2.77 倍，最终性能相当或提升。
            arXiv:2505.24298v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model. This approach stabilizes RL training but suffers from severe system-level inefficiency: generation must wait until the longest output in the batch is completed before model updates, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77$\times$ training speedup compared to synchronous systems with the same number of GPUs and matched or improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.
        ]]></description>
    </item>
    <item>
        <title>Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration</title>
        <link>https://arxiv.org/abs/2505.24688</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.24688v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui</dc:creator>
        <description><![CDATA[
            背景：大语言模型因多样性有限和搜索效率低，在复杂推理上存在困难。方法：提出基于嵌入的搜索框架“软推理”，优化首个标记的嵌入以引导生成，结合嵌入扰动进行可控探索，利用贝叶斯优化通过验证器引导的目标来细化嵌入，平衡探索与利用。效果：实验表明，该方法能提高推理的准确性和连贯性，在最少计算量下保证了较高的正确性，是可扩展、与模型无关的解决方案。
            arXiv:2505.24688v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution.
        ]]></description>
    </item>
    <item>
        <title>MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping</title>
        <link>https://arxiv.org/abs/2506.02308</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02308v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang</dc:creator>
        <description><![CDATA[
            背景：多模态基础模型取得进展，但单纯增加指令微调任务数量不一定提升性能。方法：提出MINT，一种基于多模态交互类型的任务分组策略，通过按模态间的常见交互对任务分组，促使模型学习可迁移技能并抑制不匹配任务的干扰。效果：该方法大大优于现有的多模态指令微调任务分组基线，能在泛化性和专业性之间取得有效平衡。
            arXiv:2506.02308v2 Announce Type: replace 
Abstract: Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization.
        ]]></description>
    </item>
    <item>
        <title>Comparative Analysis of AI Agent Architectures for Entity Relationship Classification</title>
        <link>https://arxiv.org/abs/2506.02426</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02426v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Maryam Berijanian, Kuldeep Singh, Amin Sehati</dc:creator>
        <description><![CDATA[
            背景：实体关系分类在信息提取中颇具挑战，尤其在标注数据有限和关系结构复杂的场景。方法：对三种利用大语言模型进行关系分类的AI智能体架构展开对比分析，包括反思性自我评估、分层任务分解和新型多智能体动态示例生成机制，后者引入实时合作与对抗提示。效果：实验表明多智能体协作始终优于标准少样本提示，接近微调模型的性能，为基于大语言模型的结构化关系提取系统设计提供实用指导。
            arXiv:2506.02426v2 Announce Type: replace 
Abstract: Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at https://github.com/maryambrj/ALIEN.git.
        ]]></description>
    </item>
    <item>
        <title>MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection</title>
        <link>https://arxiv.org/abs/2506.02535</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02535v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Juntong Li, Lingwei Dang, Yukun Su, Yun Hao, Qingxin Xiao, Yongwei Nie, Qingyao Wu</dc:creator>
        <description><![CDATA[
            基于重建或预测的视频异常检测（VAD）方法面临泛化能力强难区分异常模式、依赖低层次线索难识别高层语义等挑战。为此提出新的VAD框架，一是引入稀疏特征过滤模块（SFFM），用瓶颈滤波器动态自适应去除特征中的异常信息，并设计混合专家（MoE）架构确保特征多样性；二是集成视觉语言模型（VLM）生成视频文本描述，通过语义相似性约束和运动帧差对比损失确保模态一致性。多公开数据集实验验证了框架有效性。
            arXiv:2506.02535v2 Announce Type: replace 
Abstract: Video Anomaly Detection (VAD) methods based on reconstruction or prediction face two critical challenges: (1) strong generalization capability often results in accurate reconstruction or prediction of abnormal events, making it difficult to distinguish normal from abnormal patterns; (2) reliance only on low-level appearance and motion cues limits their ability to identify high-level semantic in abnormal events from complex scenes. To address these limitations, we propose a novel VAD framework with two key innovations. First, to suppress excessive generalization, we introduce the Sparse Feature Filtering Module (SFFM) that employs bottleneck filters to dynamically and adaptively remove abnormal information from features. Unlike traditional memory modules, it does not need to memorize the normal prototypes across the training dataset. Further, we design the Mixture of Experts (MoE) architecture for SFFM. Each expert is responsible for extracting specialized principal features during running time, and different experts are selectively activated to ensure the diversity of the learned principal features. Second, to overcome the neglect of semantics in existing methods, we integrate a Vision-Language Model (VLM) to generate textual descriptions for video clips, enabling comprehensive joint modeling of semantic, appearance, and motion cues. Additionally, we enforce modality consistency through semantic similarity constraints and motion frame-difference contrastive loss. Extensive experiments on multiple public datasets validate the effectiveness of our multimodal joint modeling framework and sparse feature filtering paradigm. Project page at https://qzfm.github.io/sfn_vad_project_page/.
        ]]></description>
    </item>
    <item>
        <title>CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG</title>
        <link>https://arxiv.org/abs/2506.02544</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02544v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie</dc:creator>
        <description><![CDATA[
            背景：多模态检索增强生成（MMRAG）虽能增强多模态大语言模型，但存在参数与检索知识不一致、视觉与文本知识不一致的问题。方法：提出Cross - source knowledge Reconciliation for Multimodal RAG（CoRe - MMRAG）框架，采用四阶段流程，还设计了专门训练范式。效果：在KB - VQA基准测试中，相比基线方法有显著提升，在InfoSeek和Encyclopedic - VQA上分别有5.6%和9.3%的性能增益。
            arXiv:2506.02544v2 Announce Type: replace 
Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose Cross-source knowledge \textbf{Re}conciliation for Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.
        ]]></description>
    </item>
    <item>
        <title>ROGRAG: A Robustly Optimized GraphRAG Framework</title>
        <link>https://arxiv.org/abs/2503.06474</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.06474v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhefan Wang, Huanjun Kong, Jie Ying, Wanli Ouyang, Nanqing Dong</dc:creator>
        <description><![CDATA[
            大语言模型处理训练语料中少见的专业或新兴话题时存在困难，图检索增强生成（GraphRAG）可通过将领域知识构建为图进行动态检索来解决该问题，但现有流程复杂且难评估检索效果。本文提出ROGRAG框架，采用多阶段检索机制，结合双级和逻辑形式检索方法提升检索鲁棒性，还融入结果验证方法和增量数据库构建方式。实验表明，在SeedBench上，其使Qwen2.5 - 7B - Instruct得分从60.0%提升至75.0%，优于主流方法。该框架已开源。
            arXiv:2503.06474v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) commonly struggle with specialized or emerging topics which are rarely seen in the training corpus. Graph-based retrieval-augmented generation (GraphRAG) addresses this by structuring domain knowledge as a graph for dynamic retrieval. However, existing pipelines involve complex engineering workflows, making it difficult to isolate the impact of individual components. It is also challenging to evaluate the retrieval effectiveness due to the overlap between the pretraining and evaluation datasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG framework. Specifically, we propose a multi-stage retrieval mechanism that integrates dual-level with logic form retrieval methods to improve retrieval robustness without increasing computational cost. To further refine the system, we incorporate various result verification methods and adopt an incremental database construction approach. Through extensive ablation experiments, we rigorously assess the effectiveness of each component. Our implementation includes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct initially underperformed. ROGRAG significantly improves the score from 60.0% to 75.0% and outperforms mainstream methods. Experiments on domain-specific datasets reveal that dual-level retrieval enhances fuzzy matching, while logic form retrieval improves structured reasoning, highlighting the importance of multi-stage retrieval.ROGRAG is released as an open-source resource and supports installation with pip.
        ]]></description>
    </item>
    <item>
        <title>Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?</title>
        <link>https://arxiv.org/abs/2503.11207</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.11207v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi</dc:creator>
        <description><![CDATA[
            背景：当前需评估大型推理模型在感知不确定下的类比推理能力。方法：对OpenAI的o3 - mini和DeepSeek R1两个模型，用I - RAVEN及扩展数据集I - RAVEN - X进行基准测试，扩展数据集以模拟视觉不确定性，采用引入混淆属性和平滑输入属性值分布两种策略。效果：o3 - mini准确率从86.6%降至17.0%，DeepSeek R1从80.6%降至23.2%，而神经符号概率溯因模型ARLC仅从98.6%降至88.0%，能稳健推理。
            arXiv:2503.11207v2 Announce Type: replace-cross 
Abstract: This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these symbolic analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles, and 2) we smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest accuracy reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.
        ]]></description>
    </item>
    <item>
        <title>SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond</title>
        <link>https://arxiv.org/abs/2505.19641</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.19641v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He</dc:creator>
        <description><![CDATA[
            背景：当前开源复制工作多聚焦数学和编码领域，开发通用推理能力的方法和资源仍待探索，收集适用于强化学习的多样化可验证推理数据存在挑战。方法：提出SynLogic数据合成框架和数据集，能大规模生成涵盖35种逻辑推理任务的多样数据，数据难度和数量可调节且能被简单规则验证。效果：基于7B和32B模型验证其有效性，在开源数据集中实现了最先进的逻辑推理性能，在BBEH上超DeepSeek - R1 - Distill - Qwen - 32B 6分，混合训练还能提升效率和推理泛化能力。
            arXiv:2505.19641v4 Announce Type: replace-cross 
Abstract: Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.
        ]]></description>
    </item>
    <item>
        <title>What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals</title>
        <link>https://arxiv.org/abs/2505.20730</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.20730v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shahrooz Pouryousef, Ali Montazeralghaem</dc:creator>
        <description><![CDATA[
            背景：用户-物品交互包含丰富协作信号，是许多推荐系统的基础，但大语言模型（LLMs）能否有效推理此类信息尚不明确。方法：系统比较LLMs和经典矩阵分解（MF）模型对用户-物品交互数据的利用能力，引入基于结构化交互数据的检索增强生成（RAG）方法提升LLMs。效果：实验表明，当前LLMs难以捕捉MF模型固有的协作模式，而基于RAG的方法显著提高了推荐质量。
            arXiv:2505.20730v2 Announce Type: replace-cross 
Abstract: User-item interactions contain rich collaborative signals that form the backbone of many successful recommender systems. While recent work has explored the use of large language models (LLMs) for recommendation, it remains unclear whether LLMs can effectively reason over this type of collaborative information. In this paper, we conduct a systematic comparison between LLMs and classical matrix factorization (MF) models to assess LLMs' ability to leverage user-item interaction data. We further introduce a simple retrieval-augmented generation (RAG) method that enhances LLMs by grounding their predictions in structured interaction data. Our experiments reveal that current LLMs often fall short in capturing collaborative patterns inherent to MF models, but that our RAG-based approach substantially improves recommendation quality-highlighting a promising direction for future LLM-based recommenders.
        ]]></description>
    </item>
    <item>
        <title>Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning</title>
        <link>https://arxiv.org/abs/2506.02867</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.02867v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao</dc:creator>
        <description><![CDATA[
            背景：大型推理模型在解决复杂问题上能力突出，但内部推理机制不明。方法：从信息论角度研究推理轨迹，跟踪中间表征与正确答案间的互信息变化，发现互信息峰值现象，理论分析该现象，指出互信息增加时模型预测错误概率降低，且峰值对应“思考标记”，并基于此提出改进推理性能的方法。效果：为大型推理模型推理机制提供新见解，有效提升其推理能力，代码开源。
            arXiv:2506.02867v2 Announce Type: replace-cross 
Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at https://github.com/ChnQ/MI-Peaks.
        ]]></description>
    </item>
    <item>
        <title>HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition</title>
        <link>https://arxiv.org/abs/2506.03403</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03403v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma</dc:creator>
        <description><![CDATA[
            背景：基于压缩的表征（CBRs）能捕捉精细声学特征，基于表征学习的表征（RLRs）可编码高层语义和韵律信息，但此前语音情感识别（SER）研究未探索二者融合。方法：提出HYFuse框架，将表征转换到双曲空间进行融合。效果：通过融合x-vector（RLR）和Soundstream（CBR），相比单个表征及同类型表征融合，取得了最佳性能，达到了当前最优水平。
            arXiv:2506.03403v1 Announce Type: new 
Abstract: Compression-based representations (CBRs) from neural audio codecs such as EnCodec capture intricate acoustic features like pitch and timbre, while representation-learning-based representations (RLRs) from pre-trained models trained for speech representation learning such as WavLM encode high-level semantic and prosodic information. Previous research on Speech Emotion Recognition (SER) has explored both, however, fusion of CBRs and RLRs haven't been explored yet. In this study, we solve this gap and investigate the fusion of RLRs and CBRs and hypothesize they will be more effective by providing complementary information. To this end, we propose, HYFuse, a novel framework that fuses the representations by transforming them to hyperbolic space. With HYFuse, through fusion of x-vector (RLR) and Soundstream (CBR), we achieve the top performance in comparison to individual representations as well as the homogeneous fusion of RLRs and CBRs and report SOTA.
        ]]></description>
    </item>
    <item>
        <title>A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations</title>
        <link>https://arxiv.org/abs/2506.03425</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03425v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Petr Grinberg, Ankur Kumar, Surya Koppisetti, Gaurav Bharaj</dc:creator>
        <description><![CDATA[
            在音频深度伪造检测中，因缺乏明确的真实标注，评估SHAP和LRP等可解释性技术颇具挑战，且在有真实标注时，这些方法也难以给出准确解释。为此，本文提出一种新的数据驱动方法来识别深度伪造音频中的伪影区域。该方法考虑成对的真实音频和经过声码处理的音频，以时频表示的差异作为真实解释，用此差异信号监督训练扩散模型，从而揭示给定声码音频中的深度伪造伪影。在VocV4和LibriSeVoc数据集上的实验表明，该方法在定性和定量上均优于传统可解释性技术。
            arXiv:2506.03425v1 Announce Type: new 
Abstract: Evaluating explainability techniques, such as SHAP and LRP, in the context of audio deepfake detection is challenging due to lack of clear ground truth annotations. In the cases when we are able to obtain the ground truth, we find that these methods struggle to provide accurate explanations. In this work, we propose a novel data-driven approach to identify artifact regions in deepfake audio. We consider paired real and vocoded audio, and use the difference in time-frequency representation as the ground-truth explanation. The difference signal then serves as a supervision to train a diffusion model to expose the deepfake artifacts in a given vocoded audio. Experimental results on the VocV4 and LibriSeVoc datasets demonstrate that our method outperforms traditional explainability techniques, both qualitatively and quantitatively.
        ]]></description>
    </item>
    <item>
        <title>BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing</title>
        <link>https://arxiv.org/abs/2506.03515</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03515v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Masaya Kawamura, Takuya Hasumi, Yuma Shirahata, Ryuichi Yamamoto</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。背景是为满足设备端应用需求，需降低文本转语音（TTS）模型大小。方法上，提出量化感知训练（QAT），将模型参数量化至1.58位，多数32位参数量化为三元值；还提出权重索引法，将一组1.58位权重保存为单个int8索引。效果显著，模型大小减少83%，且合成质量优于未量化的同规模基线模型。
            arXiv:2506.03515v1 Announce Type: new 
Abstract: This paper proposes a highly compact, lightweight text-to-speech (TTS) model for on-device applications. To reduce the model size, the proposed model introduces two techniques. First, we introduce quantization-aware training (QAT), which quantizes model parameters during training to as low as 1.58-bit. In this case, most of 32-bit model parameters are quantized to ternary values {-1, 0, 1}. Second, we propose a method named weight indexing. In this method, we save a group of 1.58-bit weights as a single int8 index. This allows for efficient storage of model parameters, even on hardware that treats values in units of 8-bit. Experimental results demonstrate that the proposed method achieved 83 % reduction in model size, while outperforming the baseline of similar model size without quantization in synthesis quality.
        ]]></description>
    </item>
    <item>
        <title>Local Equivariance Error-Based Metrics for Evaluating Sampling-Frequency-Independent Property of Neural Network</title>
        <link>https://arxiv.org/abs/2506.03550</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03550v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kanami Imamura, Tomohiko Nakamura, Norihiro Takamune, Kohei Yatabe, Hiroshi Saruwatari</dc:creator>
        <description><![CDATA[
            背景：基于深度神经网络的音频信号处理方法通常只在单一采样频率下训练，信号重采样处理未训练的采样频率会导致性能下降，且该问题常被忽视。方法：为评估深度神经网络对采样频率变化的鲁棒性，即采样频率无关（SFI）特性，基于局部等变误差（LEE）提出三个量化SFI特性的指标，并将LEE扩展到衡量音频源分离方法对信号重采样的鲁棒性。效果：音乐源分离实验表明，这些指标与未训练采样频率下的性能下降有强相关性。
            arXiv:2506.03550v1 Announce Type: new 
Abstract: Audio signal processing methods based on deep neural networks (DNNs) are typically trained only at a single sampling frequency (SF) and therefore require signal resampling to handle untrained SFs. However, recent studies have shown that signal resampling can degrade performance with untrained SFs. This problem has been overlooked because most studies evaluate only the performance at trained SFs. In this paper, to assess the robustness of DNNs to SF changes, which we refer to as the SF-independent (SFI) property, we propose three metrics to quantify the SFI property on the basis of local equivariance error (LEE). LEE measures the robustness of DNNs to input transformations. By using signal resampling as input transformation, we extend LEE to measure the robustness of audio source separation methods to signal resampling. The proposed metrics are constructed to quantify the SFI property in specific network components responsible for predicting time-frequency masks. Experiments on music source separation demonstrated a strong correlation between the proposed metrics and performance degradation at untrained SFs.
        ]]></description>
    </item>
    <item>
        <title>Conformer-based Ultrasound-to-Speech Conversion</title>
        <link>https://arxiv.org/abs/2506.03831</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03831v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ibrahim Ibrahimov, Zaink\'o Csaba, G\'abor Gosztolya</dc:creator>
        <description><![CDATA[
            背景：深度神经网络在超声到语音转换任务上展现出潜力。方法：应用两种基于Conformer的深度神经网络架构（基础架构和带双向长短期记忆网络的架构），在Ultrasuite - Tal80数据集的四位说话者数据上训练特定说话者模型，用HiFi - GAN声码器将生成的梅尔频谱图合成为音频波形。效果：与标准二维卷积神经网络基线相比，客观测量无显著改进，但MUSHRA听力测试显示带双向长短期记忆网络的Conformer感知质量更好，基础Conformer训练速度快3倍，性能与基线相当。
            arXiv:2506.03831v1 Announce Type: new 
Abstract: Deep neural networks have shown promising potential for ultrasound-to-speech conversion task towards Silent Speech Interfaces. In this work, we applied two Conformer-based DNN architectures (Base and one with bi-LSTM) for this task. Speaker-specific models were trained on the data of four speakers from the Ultrasuite-Tal80 dataset, while the generated mel spectrograms were synthesized to audio waveform using a HiFi-GAN vocoder. Compared to a standard 2D-CNN baseline, objective measurements (MSE and mel cepstral distortion) showed no statistically significant improvement for either model. However, a MUSHRA listening test revealed that Conformer with bi-LSTM provided better perceptual quality, while Conformer Base matched the performance of the baseline along with a 3x faster training time due to its simpler architecture. These findings suggest that Conformer-based models, especially the Conformer with bi-LSTM, offer a promising alternative to CNNs for ultrasound-to-speech conversion.
        ]]></description>
    </item>
    <item>
        <title>Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion</title>
        <link>https://arxiv.org/abs/2506.04013</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04013v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Seymanur Akti, Tuan Nam Nguyen, Alexander Waibel</dc:creator>
        <description><![CDATA[
            背景：表达性语音转换旨在将目标语音的说话人身份和表达属性转移到源语音。方法：改进自监督、非自回归框架，用条件变分自编码器，使用多语言离散语音单元表示内容，通过增强相似性损失和混合风格层归一化强化嵌入；通过交叉注意力融入局部基频信息，提取富含全局音高和能量特征的风格嵌入。效果：实验表明该模型在情感和说话人相似度上优于基线，风格适应能力强，源风格泄漏减少。
            arXiv:2506.04013v1 Announce Type: new 
Abstract: Expressive voice conversion aims to transfer both speaker identity and expressive attributes from a target speech to a given source speech. In this work, we improve over a self-supervised, non-autoregressive framework with a conditional variational autoencoder, focusing on reducing source timbre leakage and improving linguistic-acoustic disentanglement for better style transfer. To minimize style leakage, we use multilingual discrete speech units for content representation and reinforce embeddings with augmentation-based similarity loss and mix-style layer normalization. To enhance expressivity transfer, we incorporate local F0 information via cross-attention and extract style embeddings enriched with global pitch and energy features. Experiments show our model outperforms baselines in emotion and speaker similarity, demonstrating superior style adaptation and reduced source style leakage.
        ]]></description>
    </item>
    <item>
        <title>A Statistics-Driven Differentiable Approach for Sound Texture Synthesis and Analysis</title>
        <link>https://arxiv.org/abs/2506.04073</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04073v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Esteban Guti\'errez, Frederic Font, Xavier Serra, Lonce Wyse</dc:creator>
        <description><![CDATA[
            背景：现有对具有随机结构和感知平稳性的纹理声音分析与合成方法存在不足。方法：提出新的损失函数TexStat，借鉴相关框架识别同纹理类别信号相似性，还将其与FAD结合作为验证指标；推出纹理声音合成器TexEnv，通过对滤波噪声施加幅度包络生成音频；并将这些组件集成到TexDSP生成模型。效果：实验表明TexStat有感知意义、时间不变且抗噪，能有效用于生成任务和作为验证指标，代码开源且高效、可配置。
            arXiv:2506.04073v1 Announce Type: new 
Abstract: In this work, we introduce TexStat, a novel loss function specifically designed for the analysis and synthesis of texture sounds characterized by stochastic structure and perceptual stationarity. Drawing inspiration from the statistical and perceptual framework of McDermott and Simoncelli, TexStat identifies similarities between signals belonging to the same texture category without relying on temporal structure. We also propose using TexStat as a validation metric alongside Frechet Audio Distances (FAD) to evaluate texture sound synthesis models. In addition to TexStat, we present TexEnv, an efficient, lightweight and differentiable texture sound synthesizer that generates audio by imposing amplitude envelopes on filtered noise. We further integrate these components into TexDSP, a DDSP-inspired generative model tailored for texture sounds. Through extensive experiments across various texture sound types, we demonstrate that TexStat is perceptually meaningful, time-invariant, and robust to noise, features that make it effective both as a loss function for generative tasks and as a validation metric. All tools and code are provided as open-source contributions and our PyTorch implementations are efficient, differentiable, and highly configurable, enabling its use in both generative tasks and as a perceptually grounded evaluation metric.
        ]]></description>
    </item>
    <item>
        <title>MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition</title>
        <link>https://arxiv.org/abs/2506.03722</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.03722v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yinfeng Xia, Huiyan Li, Chenyang Le, Manhong Wang, Yutao Sun, Xingyang Ma, Yanmin Qian</dc:creator>
        <description><![CDATA[
            背景：应用像Whisper这样的大预训练语音模型虽能降低语音任务训练成本，但将其集成到流式系统仍有挑战。方法：本文提出一种通过微调Whisper进行流式识别的前缀到前缀训练框架，引入连续积分触发机制建立语音序列与文本标记的准单调对齐，设计单调有限前瞻注意力，还采用wait - k解码策略简化解码过程。效果：理论分析和实验表明，该方法能在延迟和质量间实现可控权衡，适用于多种流式应用。
            arXiv:2506.03722v1 Announce Type: cross 
Abstract: Applying large pre-trained speech models like Whisper has shown promise in reducing training costs for various speech tasks. However, integrating these models into streaming systems remains a challenge. This paper presents a novel prefix-to-prefix training framework for streaming recognition by fine-tuning the Whisper. We introduce the Continuous Integrate-and-Fire mechanism to establish a quasi-monotonic alignment between continuous speech sequences and discrete text tokens. Additionally, we design Monotonic Finite Look-ahead Attention, allowing each token to attend to infinite left-context and finite right-context from the speech sequences. We also employ the wait-k decoding strategy to simplify the decoding process while ensuring consistency between training and testing. Our theoretical analysis and experiments demonstrate that this approach achieves a controllable trade-off between latency and quality, making it suitable for various streaming applications.
        ]]></description>
    </item>
    <item>
        <title>Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems</title>
        <link>https://arxiv.org/abs/2506.04076</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04076v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jhen-Ke Lin, Hao-Chien Lu, Chung-Chun Wang, Hong-Yun Lin, Berlin Chen</dc:creator>
        <description><![CDATA[
            背景：逐字转录对自动口语评估很重要，需准确捕捉言语不流畅现象，但许多自动语音识别（ASR）系统会丢弃或泛化犹豫现象，丢失重要声学细节。方法：在Speak & Improve 2025语料库上使用低秩自适应（LoRA）微调Whisper模型，对比三种注释方案。效果：挑战系统的词错误率（WER）为6.47%（Pure）和5.81%（Extra），“Extra”方案微调Whisper Large V3 Turbo的WER为5.5%，相对“Pure”方案有11.3%的提升，表明明确、真实的填充停顿标注可显著提高ASR准确性。
            arXiv:2506.04076v1 Announce Type: cross 
Abstract: Verbatim transcription for automatic speaking assessment demands accurate capture of disfluencies, crucial for downstream tasks like error analysis and feedback. However, many ASR systems discard or generalize hesitations, losing important acoustic details. We fine-tune Whisper models on the Speak & Improve 2025 corpus using low-rank adaptation (LoRA), without recourse to external audio training data. We compare three annotation schemes: removing hesitations (Pure), generic tags (Rich), and acoustically precise fillers inferred by Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge experiments reveal that fine-tuning Whisper Large V3 Turbo with the "Extra" scheme yielded a 5.5% WER, an 11.3% relative improvement over the "Pure" scheme (6.2% WER). This demonstrates that explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription.
        ]]></description>
    </item>
    <item>
        <title>A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions</title>
        <link>https://arxiv.org/abs/2506.04077</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04077v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chung-Chun Wang, Jhen-Ke Lin, Hao-Chien Lu, Hong-Yun Lin, Berlin Chen</dc:creator>
        <description><![CDATA[
            背景：自动口语评估（ASA）在观点表达方面常受限于标注录音稀缺，影响提示多样性和评分可靠性。方法：提出新训练范式，利用大语言模型（LLM）生成特定水平的多样回复，通过语音合成转换为语音，用动态重要性损失根据合成与真实语音特征差异调整训练实例权重，再用多模态大语言模型整合文本特征与语音信号预测分数。效果：在LTTC数据集实验表明，该方法优于依赖真实数据或传统增强的方法，能缓解资源限制问题。
            arXiv:2506.04077v1 Announce Type: cross 
Abstract: Automated speaking assessment (ASA) on opinion expressions is often hampered by the scarcity of labeled recordings, which restricts prompt diversity and undermines scoring reliability. To address this challenge, we propose a novel training paradigm that leverages a large language models (LLM) to generate diverse responses of a given proficiency level, converts responses into synthesized speech via speaker-aware text-to-speech synthesis, and employs a dynamic importance loss to adaptively reweight training instances based on feature distribution differences between synthesized and real speech. Subsequently, a multimodal large language model integrates aligned textual features with speech signals to predict proficiency scores directly. Experiments conducted on the LTTC dataset show that our approach outperforms methods relying on real data or conventional augmentation, effectively mitigating low-resource constraints and enabling ASA on opinion expressions with cross-modal information.
        ]]></description>
    </item>
    <item>
        <title>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</title>
        <link>https://arxiv.org/abs/2506.04134</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04134v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jinting Wang, Shan Yang, Li Liu</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。背景是当前从手语视频直接生成语音性能差，基于文本的分步跨模态对齐方法存在误差传播和音画不同步问题。方法是提出了UniCUE框架，该框架直接从手语视频生成语音，集成了提供细粒度视觉语义信息的手语识别任务，还采用了细粒度语义对齐池、跨任务表示适配器和姿态感知视觉处理器。效果是在新建立的中文手语数据集上，与单一步骤方法相比，显著降低78.3%的单词错误率，提升32%的唇语同步性。
            arXiv:2506.04134v1 Announce Type: cross 
Abstract: Cued Speech (CS) enhances lipreading through hand coding, providing precise speech perception support for the hearing-impaired. CS Video-to-Speech generation (CSV2S) task aims to convert the CS visual expressions (CS videos) of hearing-impaired individuals into comprehensible speech signals. Direct generation of speech from CS video (called single CSV2S) yields poor performance due to insufficient CS data. Current research mostly focuses on CS Recognition (CSR), which convert video content into linguistic text. Based on this, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech system. This combined architecture relies on text as an intermediate medium for stepwise cross-modal alignment, which may lead to error propagation and temporal misalignment between speech and video dynamics. To address these challenges, we propose a novel approach that directly generates speech from CS videos without relying on intermediate text. Building upon this, we propose UniCUE, the first unified framework for CSV2S, whose core innovation lies in the integration of the CSR task that provides fine-grained visual-semantic information to facilitate speech generation from CS videos. More precisely, (1) a novel fine-grained semantic alignment pool to ensure precise mapping between visual features and speech contents; (2) a VisioPhonetic adapter to bridge cross-task representations, ensuring seamless compatibility between two distinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is introduced to enhance fine-grained spatiotemporal correlations between lip and hand movements in CS video. Experiments on our new established Chinese CS dataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our UniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech synchronization by 32% compared to the single CSV2S.
        ]]></description>
    </item>
    <item>
        <title>Sounding that Object: Interactive Object-Aware Image to Audio Generation</title>
        <link>https://arxiv.org/abs/2506.04214</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04214v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tingle Li, Baihe Huang, Xiaobin Zhuang, Dongya Jia, Jiawei Chen, Yuping Wang, Zhuo Chen, Gopala Anumanchipalli, Yuxuan Wang</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。背景是为复杂视听场景生成准确声音颇具挑战，尤其存在多物体和多声源时。方法是提出交互式对象感知音频生成模型，将以对象为中心的学习融入条件潜在扩散模型，通过多模态注意力关联图像区域和对应声音，测试时用图像分割实现对象级声音交互生成。效果是经理论验证注意力机制能近似测试时的分割掩码，定量和定性评估显示该模型优于基线，物体与关联声音的对齐度更好。
            arXiv:2506.04214v1 Announce Type: cross 
Abstract: Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the {\em object} level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds. Project page: https://tinglok.netlify.app/files/avobject/
        ]]></description>
    </item>
    <item>
        <title>Language-Codec: Bridging Discrete Codec Representations and Speech Language Models</title>
        <link>https://arxiv.org/abs/2402.12208</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2402.12208v4</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shengpeng Ji, Minghui Fang, Jialong Zuo, Ziyue Jiang, Dingdong Wang, Hanting Wang, Hai Huang, Zhou Zhao</dc:creator>
        <description><![CDATA[
            近年来，大语言模型在语音、音频等信号领域的生成任务中取得显著成功，离散声学编解码器是这类模型的关键要素，但它与下游语音语言模型存在差距，如码本初始通道信息过多、码本数量多增加模型负担等。为此，研究团队提出Language - Codec，引入MCRVQ机制等改进结构和设计来解决问题。经评估，该方法显著优于其他音频压缩算法，且验证了其在下游语音语言模型中的效率。
            arXiv:2402.12208v4 Announce Type: replace 
Abstract: In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .
        ]]></description>
    </item>
    <item>
        <title>ControlSpeech: Towards Simultaneous and Independent Zero-shot Speaker Cloning and Zero-shot Language Style Control</title>
        <link>https://arxiv.org/abs/2406.01205</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.01205v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shengpeng Ji, Qian Chen, Wen Wang, Jialong Zuo, Minghui Fang, Ziyue Jiang, Hai Huang, Zehan Wang, Xize Cheng, Siqi Zheng, Zhou Zhao</dc:creator>
        <description><![CDATA[
            背景：以往零样本TTS模型要么只能模仿声音，要么无法进行特定说话人语音生成。方法：本文提出ControlSpeech，以语音、内容、风格提示为输入，利用双向注意力和基于掩码的并行解码在离散解耦编解码空间中捕获音色、内容和风格的编解码表示，还提出基于高斯混合密度网络的SMSD模块解决文本风格控制的多对多问题，并发布新数据集VccmDataset。效果：实验表明其在可控性、音色相似度等方面表现达到或接近最优水平。
            arXiv:2406.01205v3 Announce Type: replace 
Abstract: In this paper, we present ControlSpeech, a text-to-speech (TTS) system capable of fully cloning the speaker's voice and enabling arbitrary control and adjustment of speaking style. Prior zero-shot TTS models only mimic the speaker's voice without further control and adjustment capabilities while prior controllable TTS models cannot perform speaker-specific voice generation. Therefore, ControlSpeech focuses on a more challenging task: a TTS system with controllable timbre, content, and style at the same time. ControlSpeech takes speech prompts, content prompts, and style prompts as inputs and utilizes bidirectional attention and mask-based parallel decoding to capture codec representations corresponding to timbre, content, and style in a discrete decoupling codec space. Moreover, we analyze the many-to-many issue in textual style control and propose the Style Mixture Semantic Density (SMSD) module, which is based on Gaussian mixture density networks, to resolve this problem. To facilitate empirical validations, we make available a new style controllable dataset called VccmDataset. Our experimental results demonstrate that ControlSpeech exhibits comparable or state-of-the-art (SOTA) performance in terms of controllability, timbre similarity, audio quality, robustness, and generalizability. The relevant code and demo are available at https://github.com/jishengpeng/ControlSpeech .
        ]]></description>
    </item>
    <item>
        <title>Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs</title>
        <link>https://arxiv.org/abs/2406.05298</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.05298v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ryan Langman, Ante Juki\'c, Kunal Dhawan, Nithin Rao Koluguri, Jason Li</dc:creator>
        <description><![CDATA[
            背景：以往机器学习中的语音模型多采用梅尔频谱图作为语音表示，近年来神经音频编解码器产生的离散音频标记成为语音合成任务的替代语音表示，但现有编解码器的数据分布复杂，部分TTS模型难以预测。方法：提出一种频谱编解码器，使用有限标量量化（FSQ）压缩梅尔频谱图并重建时域音频信号。效果：客观音频质量指标研究和主观听力测试表明，频谱编解码器与等效音频编解码器的感知质量相当，且FSQ和频谱语音表示可提升并行TTS模型性能。
            arXiv:2406.05298v2 Announce Type: replace 
Abstract: Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.
        ]]></description>
    </item>
    <item>
        <title>ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors</title>
        <link>https://arxiv.org/abs/2502.14627</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.14627v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuguo Yin, Yuxin Xie, Wenyuan Yang, Dongchao Yang, Jinghan Ru, Xianwei Zhuang, Liming Liang, Yuexian Zou</dc:creator>
        <description><![CDATA[
            背景：多语言音频文本检索（ML - ATR）任务具有挑战性，现有方案存在跨语言实例相似度匹配不一致的问题。方法：理论分析不一致性的多语言模态对齐方向误差和权重误差，提出理论权重误差上限，发现问题源于语言随机采样导致的数据分布误差，提出用1对k对比学习和音频 - 英语共锚对比学习的一致ML - ATR方案。效果：在翻译的AudioCaps和Clotho数据集上，该方案在包括英语在内的八种主流语言的召回率和一致性指标上达最优。
            arXiv:2502.14627v3 Announce Type: replace 
Abstract: Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases. However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages. We theoretically analyze the inconsistency in terms of both multilingual modal alignment direction error and weight error, and propose the theoretical weight error upper bound for quantifying the inconsistency. Based on the analysis of the weight error upper bound, we find that the inconsistency problem stems from the data distribution error caused by random sampling of languages. We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English. Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL.
        ]]></description>
    </item>
    <item>
        <title>InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training</title>
        <link>https://arxiv.org/abs/2503.02769</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.02769v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Dingdong Wang, Jin Xu, Ruihang Chu, Zhifang Guo, Xiong Wang, Jincenzi Wu, Dongchao Yang, Shengpeng Ji, Junyang Lin</dc:creator>
        <description><![CDATA[
            背景：当前语音大语言模型在遵循语音指令方面表现欠佳，处理语音输入时智能性低于文本输入。方法：提出InSerter训练方法，对大规模无监督语音 - 文本序列进行预训练，语音由文本语料随机片段经文本转语音合成，使模型能根据语音生成文本；还引入首个语音指令遵循任务基准SpeechInstructBench。效果：InSerter在SpeechInstructBench达SOTA，在多种语音处理任务中表现优或具竞争力。
            arXiv:2503.02769v2 Announce Type: replace 
Abstract: Recent advancements in speech large language models (SpeechLLMs) have attracted considerable attention. Nonetheless, current methods exhibit suboptimal performance in adhering to speech instructions. Notably, the intelligence of models significantly diminishes when processing speech-form input as compared to direct text-form input. Prior work has attempted to mitigate this semantic inconsistency between speech and text representations through techniques such as representation and behavior alignment, which involve the meticulous design of data pairs during the post-training phase. In this paper, we introduce a simple and scalable training method called InSerter, which stands for Interleaved Speech-Text Representation Pre-training. InSerter is designed to pre-train large-scale unsupervised speech-text sequences, where the speech is synthesized from randomly selected segments of an extensive text corpus using text-to-speech conversion. Consequently, the model acquires the ability to generate textual continuations corresponding to the provided speech segments, obviating the need for intensive data design endeavors. To systematically evaluate speech instruction-following capabilities, we introduce SpeechInstructBench, the first comprehensive benchmark specifically designed for speech-oriented instruction-following tasks. Our proposed InSerter achieves SOTA performance in SpeechInstructBench and demonstrates superior or competitive results across diverse speech processing tasks.
        ]]></description>
    </item>
    <item>
        <title>PAST: Phonetic-Acoustic Speech Tokenizer</title>
        <link>https://arxiv.org/abs/2505.14470</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.14470v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Nadav Har-Tuv, Or Tal, Yossi Adi</dc:creator>
        <description><![CDATA[
            本文聚焦音频tokenization，提出新的端到端框架PAST。背景是以往方法依赖预训练自监督模型。方法上，PAST联合建模语音信息和信号重建，无需外部预训练模型，采用有监督语音数据，通过辅助任务将领域知识融入tokenization过程，还推出可流式、有因果关系的变体以用于实时语音应用。效果显示，在语音表示和语音重建等常见评估指标上，PAST超越现有基线分词器，用于语音语言模型时表现也更优。
            arXiv:2505.14470v2 Announce Type: replace 
Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST
        ]]></description>
    </item>
    <item>
        <title>Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling</title>
        <link>https://arxiv.org/abs/2505.19931</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.19931v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qixi Zheng, Yushen Chen, Zhikang Niu, Ziyang Ma, Xiaofei Wang, Kai Yu, Xie Chen</dc:creator>
        <description><![CDATA[
            近年来，基于流匹配的文本转语音（TTS）模型受关注，但需多步采样重构语音，推理速度是关键挑战。为解决此问题，本文提出Fast F5 - TTS这一免训练方法加速推理。通过检查F5 - TTS采样轨迹，识别冗余步骤，提出非均匀时间步采样策略EPSS，有效减少采样步数。该方法在NVIDIA RTX 3090 GPU上实现7步生成，推理RTF为0.030，比原F5 - TTS快4倍且性能相当，在E2 TTS模型上也表现良好，泛化能力强。
            arXiv:2505.19931v2 Announce Type: replace 
Abstract: Flow-matching-based text-to-speech (TTS) models, such as Voicebox, E2 TTS, and F5-TTS, have attracted significant attention in recent years. These models require multiple sampling steps to reconstruct speech from noise, making inference speed a key challenge. Reducing the number of sampling steps can greatly improve inference efficiency. To this end, we introduce Fast F5-TTS, a training-free approach to accelerate the inference of flow-matching-based TTS models. By inspecting the sampling trajectory of F5-TTS, we identify redundant steps and propose Empirically Pruned Step Sampling (EPSS), a non-uniform time-step sampling strategy that effectively reduces the number of sampling steps. Our approach achieves a 7-step generation with an inference RTF of 0.030 on an NVIDIA RTX 3090 GPU, making it 4 times faster than the original F5-TTS while maintaining comparable performance. Furthermore, EPSS performs well on E2 TTS models, demonstrating its strong generalization ability.
        ]]></description>
    </item>
</channel>
</rss>