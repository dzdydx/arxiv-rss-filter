<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 25 Apr 2025 12:20:58 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Fri, 25 Apr 2025 12:20:58 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning</title>
        <link>https://arxiv.org/abs/2504.17091</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17091v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Seunghyun Yoo</dc:creator>
        <description><![CDATA[
            背景：短内容盛行和AI快速应用，使深度思考机会减少，削弱用户批判性思维和对AI推理过程的参与度。方法：提出交互式思维链（CoT）框架，将推理分解为可检查、修改和重新执行的模块，使推理过程透明；集成受偏好学习启发的轻量级编辑适应机制；通过元数据披露等确保伦理透明。效果：该框架可增强以人类为中心的可解释性和负责任的AI使用，促进批判性参与和负责任交互。
            arXiv:2504.17091v1 Announce Type: new 
Abstract: Due to the proliferation of short-form content and the rapid adoption of AI, opportunities for deep, reflective thinking have significantly diminished, undermining users' critical thinking and reducing engagement with the reasoning behind AI-generated outputs. To address this issue, we propose an Interactive Chain-of-Thought (CoT) Framework that enhances human-centered explainability and responsible AI usage by making the model's inference process transparent, modular, and user-editable. The framework decomposes reasoning into clearly defined blocks that users can inspect, modify, and re-execute, encouraging active cognitive engagement rather than passive consumption. It further integrates a lightweight edit-adaptation mechanism inspired by preference learning, allowing the system to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, built-in bias checkpoint functionality, and privacy-preserving safeguards. This work outlines the design principles and architecture necessary to promote critical engagement, responsible interaction, and inclusive adaptation in AI systems aimed at addressing complex societal challenges.
        ]]></description>
    </item>
    <item>
        <title>GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs</title>
        <link>https://arxiv.org/abs/2504.17099</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17099v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Martin Boeckling, Heiko Paulheim, Sarah Detzler</dc:creator>
        <description><![CDATA[
            背景：许多知识图谱包含大量空间实体，其精确几何信息被存储，但现有学习实体表示的方法大多未考虑这些几何信息。方法：提出一种结合几何信息的RDF2Vec变体，通过从地理节点扩散图来扩展不同节点，确保考虑每个可达节点，并基于扩散后的图，应用修改版RDF2Vec，利用空间权重对图游走进行偏置。效果：在多个基准数据集上的评估显示，该方法优于非位置感知的RDF2Vec和GeoTransE。
            arXiv:2504.17099v1 Announce Type: new 
Abstract: Many knowledge graphs contain a substantial number of spatial entities, such as cities, buildings, and natural landmarks. For many of these entities, exact geometries are stored within the knowledge graphs. However, most existing approaches for learning entity representations do not take these geometries into account. In this paper, we introduce a variant of RDF2Vec that incorporates geometric information to learn location-aware embeddings of entities. Our approach expands different nodes by flooding the graph from geographic nodes, ensuring that each reachable node is considered. Based on the resulting flooded graph, we apply a modified version of RDF2Vec that biases graph walks using spatial weights. Through evaluations on multiple benchmark datasets, we demonstrate that our approach outperforms both non-location-aware RDF2Vec and GeoTransE.
        ]]></description>
    </item>
    <item>
        <title>MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation</title>
        <link>https://arxiv.org/abs/2504.17137</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17137v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chanhee Park, Hyeonseok Moon, Chanjun Park, Heuiseok Lim</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）可提升大语言模型生成能力，但RAG系统评估因检索与生成组件的复杂交互而面临挑战，缺乏详细的组件特定评估基准。方法：提出用于RAG评估的问答数据集MIRAGE，含7560个实例对应37800个检索条目，还引入衡量RAG适应性的新评估指标。效果：通过多组实验，为模型对的最优对齐和RAG系统内部细微动态提供新见解，数据集和代码公开。
            arXiv:2504.17137v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.
        ]]></description>
    </item>
    <item>
        <title>Symbolic Representation for Any-to-Any Generative Tasks</title>
        <link>https://arxiv.org/abs/2504.17261</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17261v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiaqi Chen, Xiaoye Zhu, Yue Wang, Tianyang Liu, Xinhui Chen, Ying Chen, Chak Tou Leong, Yifei Ke, Joseph Liu, Yiwen Yuan, Julian McAuley, Li-jia Li</dc:creator>
        <description><![CDATA[
            背景：传统生成模型依赖大规模训练和隐式神经表征学习跨模态映射，计算成本高且灵活性有限。方法：提出一种符号生成任务描述语言和推理引擎，将任意多模态任务表示为结构化符号流，引入函数、参数和拓扑逻辑三个核心原语，利用预训练语言模型以无训练方式将自然语言指令映射到符号工作流。效果：成功执行12种多模态生成任务，内容质量匹配或超越现有模型，且更高效、可编辑和可中断。
            arXiv:2504.17261v1 Announce Type: new 
Abstract: We propose a symbolic generative task description language and a corresponding inference engine capable of representing arbitrary multimodal tasks as structured symbolic flows. Unlike conventional generative models that rely on large-scale training and implicit neural representations to learn cross-modal mappings, often at high computational cost and with limited flexibility, our framework introduces an explicit symbolic representation comprising three core primitives: functions, parameters, and topological logic. Leveraging a pre-trained language model, our inference engine maps natural language instructions directly to symbolic workflows in a training-free manner. Our framework successfully performs over 12 diverse multimodal generative tasks, demonstrating strong performance and flexibility without the need for task-specific tuning. Experiments show that our method not only matches or outperforms existing state-of-the-art unified models in content quality, but also offers greater efficiency, editability, and interruptibility. We believe that symbolic task representations provide a cost-effective and extensible foundation for advancing the capabilities of generative AI.
        ]]></description>
    </item>
    <item>
        <title>M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction</title>
        <link>https://arxiv.org/abs/2504.17353</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17353v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chengguang Gan, Sunbowen Lee, Zhixi Cai, Yanbin Wei, Lei Zheng, Yunhao Liang, Shiwen Ni, Tatsunori Mori</dc:creator>
        <description><![CDATA[
            背景：互强化效应（MRE）在文本领域已被探索验证，但在视觉和多模态领域的适用性未知。方法：首次将MRE拓展到多模态信息提取领域，提出多模态互强化效应（M - MRE）任务并构建对应数据集，还提出与多种大视觉语言模型兼容的提示格式适配器（PFA）。效果：实验表明在M - MRE任务这一多模态文本 - 图像理解场景中也能观察到MRE，证实其在文本领域外的泛化性。
            arXiv:2504.17353v1 Announce Type: new 
Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection of information extraction and model interpretability. MRE aims to leverage the mutual understanding between tasks of different granularities, enhancing the performance of both coarse-grained and fine-grained tasks through joint modeling. While MRE has been explored and validated in the textual domain, its applicability to visual and multimodal domains remains unexplored. In this work, we extend MRE to the multimodal information extraction domain for the first time. Specifically, we introduce a new task: Multimodal Mutual Reinforcement Effect (M-MRE), and construct a corresponding dataset to support this task. To address the challenges posed by M-MRE, we further propose a Prompt Format Adapter (PFA) that is fully compatible with various Large Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can also be observed in the M-MRE task, a multimodal text-image understanding scenario. This provides strong evidence that MRE facilitates mutual gains across three interrelated tasks, confirming its generalizability beyond the textual domain.
        ]]></description>
    </item>
    <item>
        <title>Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization</title>
        <link>https://arxiv.org/abs/2504.17355</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17355v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaohan Huang, Dongjie Wang, Zhiyuan Ning, Ziyue Qiao, Qingqing Long, Haowei Zhu, Yi Du, Min Wu, Yuanchun Zhou, Meng Xiao</dc:creator>
        <description><![CDATA[
            背景：现有特征转换框架常将特征转换视为孤立操作，忽略转换步骤间的动态依赖。方法：提出TCTO，一种基于图驱动路径优化的协作多智能体强化学习框架，通过将特征建模为节点、转换建模为边的交互图，利用图剪枝和回溯消除低影响边、减少冗余操作。效果：在多个数据集上的实验和案例研究表明，该方法性能更优，且交互图使框架能复用历史转换中的高效子图。
            arXiv:2504.17355v1 Announce Type: new 
Abstract: Feature transformation methods aim to find an optimal mathematical feature-feature crossing process that generates high-value features and improves the performance of downstream machine learning tasks. Existing frameworks, though designed to mitigate manual costs, often treat feature transformations as isolated operations, ignoring dynamic dependencies between transformation steps. To address the limitations, we propose TCTO, a collaborative multi-agent reinforcement learning framework that automates feature engineering through graph-driven path optimization. The framework's core innovation lies in an evolving interaction graph that models features as nodes and transformations as edges. Through graph pruning and backtracking, it dynamically eliminates low-impact edges, reduces redundant operations, and enhances exploration stability. This graph also provides full traceability to empower TCTO to reuse high-utility subgraphs from historical transformations. To demonstrate the efficacy and adaptability of our approach, we conduct comprehensive experiments and case studies, which show superior performance across a range of datasets.
        ]]></description>
    </item>
    <item>
        <title>Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening</title>
        <link>https://arxiv.org/abs/2504.17497</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17497v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Radia Berreziga, Mohammed Brahimi, Khairedine Kraim, Hamid Azzoune</dc:creator>
        <description><![CDATA[
            背景：虚拟筛选在药物发现中至关重要，传统机器学习方法有信息损失和潜在偏差，图卷积网络（GCN）能直接处理分子图，大语言模型（LLM）在药物设计中表现出色。方法：提出将GCN与LLM嵌入相结合的混合架构，把预计算的LLM嵌入存储在分子特征库，在每个GCN层后拼接LLM嵌入。效果：模型表现优异，F1分数达88.8%，优于独立GCN（87.9%）、XGBoost（85.5%）和SVM（85.4%）基线模型。
            arXiv:2504.17497v1 Announce Type: new 
Abstract: Virtual screening plays a critical role in modern drug discovery by enabling the identification of promising candidate molecules for experimental validation. Traditional machine learning methods such as support vector machines (SVM) and XGBoost rely on predefined molecular representations, often leading to information loss and potential bias. In contrast, deep learning approaches-particularly Graph Convolutional Networks (GCNs)-offer a more expressive and unbiased alternative by operating directly on molecular graphs. Meanwhile, Large Language Models (LLMs) have recently demonstrated state-of-the-art performance in drug design, thanks to their capacity to capture complex chemical patterns from large-scale data via attention mechanisms.
  In this paper, we propose a hybrid architecture that integrates GCNs with LLM-derived embeddings to combine localized structural learning with global chemical knowledge. The LLM embeddings can be precomputed and stored in a molecular feature library, removing the need to rerun the LLM during training or inference and thus maintaining computational efficiency. We found that concatenating the LLM embeddings after each GCN layer-rather than only at the final layer-significantly improves performance, enabling deeper integration of global context throughout the network. The resulting model achieves superior results, with an F1-score of (88.8%), outperforming standalone GCN (87.9%), XGBoost (85.5%), and SVM (85.4%) baselines.
        ]]></description>
    </item>
    <item>
        <title>Set2Seq Transformer: Temporal and Positional-Aware Set Representations for Sequential Multiple-Instance Learning</title>
        <link>https://arxiv.org/abs/2408.03404</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2408.03404v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Athanasios Efthymiou, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring</dc:creator>
        <description><![CDATA[
            背景：顺序多实例学习需对跨离散时间步的集合表示进行学习，现有方法存在忽视时间动态或缺乏集合表示机制的问题。方法：提出Set2Seq Transformer架构，以端到端多模态方式学习序列中集合的时间和位置感知表示，联合建模排列不变的集合结构和时间依赖关系。效果：在美术分析和野火危险预测两个任务上进行评估，相比传统静态多实例学习方法有显著提升。
            arXiv:2408.03404v2 Announce Type: replace 
Abstract: Sequential multiple-instance learning involves learning representations of sets distributed across discrete timesteps. In many real-world applications, modeling both the internal structure of sets and their temporal relationships across time is essential for capturing complex underlying patterns. However, existing methods either focus on learning set representations at a static level, ignoring temporal dynamics, or treat sequences as ordered lists of individual elements, lacking explicit mechanisms to represent sets. In this work, we propose Set2Seq Transformer, a novel architecture that jointly models permutation-invariant set structure and temporal dependencies by learning temporal and positional-aware representations of sets within a sequence in an end-to-end multimodal manner. We evaluate our Set2Seq Transformer on two tasks that require modeling both set structure alongside temporal and positional patterns, but differ significantly in domain, modality, and objective. First, we consider a fine-art analysis task, modeling artists' oeuvres for predicting artistic success using a novel dataset, WikiArt-Seq2Rank. Second, we utilize our Set2Seq Transformer for a short-term wildfire danger forecasting task. Through extensive experimentation, we show that our Set2Seq Transformer significantly improves over traditional static multiple-instance learning methods by effectively learning permutation-invariant set, temporal, and positional-aware representations across diverse domains, modalities, and tasks. We will release both the dataset and model implementations on GitHub.
        ]]></description>
    </item>
    <item>
        <title>Transfer Learning with Foundational Models for Time Series Forecasting using Low-Rank Adaptations</title>
        <link>https://arxiv.org/abs/2410.11539</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.11539v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>M. Germ\'an-Morales, A. J. Rivera-Rivas, M. J. del Jesus D\'iaz, C. J. Carmona</dc:creator>
        <description><![CDATA[
            基础模型是新兴的通用人工智能技术，具备可扩展性和易迁移学习的特点。为解决时间序列预测问题，本文提出LLIAM方法，通过合适的时间序列提示模式和低秩自适应，用不同时间序列数据集增强大语言模型的知识。研究分两阶段评估该方法有效性，先与多种深度学习算法和基于大语言模型的TimeLLM对比，再进行零样本测试。结果表明，LLIAM简单通用，无需复杂修改就能取得不错效果，还能避免不必要的高成本训练。
            arXiv:2410.11539v2 Announce Type: replace 
Abstract: Foundational Models are an emerging widely used technique of GenAI. These models are distinguished by their scalability and the ease with which they can be adapted through the exploitation of Transfer Learning. The availability of high computational power and large datasets have supported their development, achieving a high generalization capacity due to the enormous and heterogeneous amounts of data used in their initial training. These characteristics contribute to a solid base that can be adapted or adjusted to a wide range of tasks, increasing their applicability. This study proposes the methodology LLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for the Time Series Forecasting task. An adequate time-series prompting schema and Low-Rank Adaptations are used to enhance the knowledge of the model with diverse time series datasets, known as the fine-tuning phase. A study divided in two stages has been performed for evaluating the effectiveness of the proposed methodology. Initially, a comparison was made between the performance of LLIAM and different state-of-the-art DL algorithms, including Recurrent Neural Networks and Temporal Convolutional Networks, as well as a LLM-based method, TimeLLM. Following this, a zero-shot study is presented in order to evaluate the generalization capacity of the proposed methodology with time series datasets from unknown domains not considered in the model training. The outcomes of this investigation demonstrate the efficacy of LLIAM, highlighting that this straightforward and general approach can attain competent results without the necessity for applying complex modifications. This work also encourages the use of available resources (such as these pre-trained models) and efficient fine-tuning techniques to avoid unnecessary and costly training, narrowing the gap between the goals of traditional AI and Green AI.
        ]]></description>
    </item>
    <item>
        <title>Emergent Symbol-like Number Variables in Artificial Neural Networks</title>
        <link>https://arxiv.org/abs/2501.06141</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.06141v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Satchel Grant, Noah D. Goodman, James L. McClelland</dc:creator>
        <description><![CDATA[
            背景：探究神经网络中出现的数值表示类型及如何理解神经网络。方法：用GRU、LSTM和Transformer在数值任务上进行训练，通过多种因果和理论方法，从可解释符号算法（SA）视角，以可解释子空间而非单个神经元来解读神经网络原始活动，还引入对齐函数增强现有分布式对齐搜索方法的灵活性。效果：表明因果干预对神经网络可解释性很重要，循环模型在神经活动中会形成分级的类符号数字变量，提出DAS的推广形式，发现Transformer在注意力层不足时需用非马尔可夫解。
            arXiv:2501.06141v2 Announce Type: replace 
Abstract: What types of numeric representations emerge in neural systems? What would a satisfying answer to this question look like? In this work, we interpret Neural Network (NN) solutions to sequence based counting tasks through a variety of lenses. We seek to understand how well we can understand NNs through the lens of interpretable Symbolic Algorithms (SAs), where SAs are defined by precise, abstract, mutable variables used to perform computations. We use GRUs, LSTMs, and Transformers trained using Next Token Prediction (NTP) on numeric tasks where the solutions to the tasks depend on numeric information only latent in the task structure. We show through multiple causal and theoretical methods that we can interpret NN's raw activity through the lens of simplified SAs when we frame the neural activity in terms of interpretable subspaces rather than individual neurons. Depending on the analysis, however, these interpretations can be graded, existing on a continuum, highlighting the philosophical question of what it means to "interpret" neural activity, and motivating us to introduce Alignment Functions to add flexibility to the existing Distributed Alignment Search (DAS) method. Through our specific analyses we show the importance of causal interventions for NN interpretability; we show that recurrent models develop graded, symbol-like number variables within their neural activity; we introduce a generalization of DAS to frame NN activity in terms of linear functions of interpretable variables; and we show that Transformers must use anti-Markovian solutions -- solutions that avoid using cumulative, Markovian hidden states -- in the absence of sufficient attention layers. We use our results to encourage interpreting NNs at the level of neural subspaces through the lens of SAs.
        ]]></description>
    </item>
    <item>
        <title>GraphRAG under Fire</title>
        <link>https://arxiv.org/abs/2501.14050</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.14050v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiacheng Liang, Yuhui Wang, Changjiang Li, Rongyi Zhu, Tanqiu Jiang, Neil Gong, Ting Wang</dc:creator>
        <description><![CDATA[
            背景：GraphRAG将外部知识构建为多尺度知识图谱，推动了检索增强生成技术，但安全影响未被充分研究。方法：本文研究其对投毒攻击的脆弱性，提出GRAGPoison攻击，利用知识图谱中的共享关系制作投毒文本，采用关系注入、关系增强和叙事生成三种策略。效果：在不同数据集和模型上的实证评估表明，GRAGPoison在有效性（成功率达98%）和可扩展性（使用不到68%的投毒文本）上大幅超越现有攻击。此外还探讨了防御措施及局限。 
            arXiv:2501.14050v2 Announce Type: replace 
Abstract: GraphRAG advances retrieval-augmented generation (RAG) by structuring external knowledge as multi-scale knowledge graphs, enabling language models to integrate both broad context and granular details in their generation. While GraphRAG has demonstrated success across domains, its security implications remain largely unexplored. To bridge this gap, this work examines GraphRAG's vulnerability to poisoning attacks, uncovering an intriguing security paradox: compared to conventional RAG, GraphRAG's graph-based indexing and retrieval enhance resilience against simple poisoning attacks; yet, the same features also create new attack surfaces. We present GRAGPoison, a novel attack that exploits shared relations in the underlying knowledge graph to craft poisoning text capable of compromising multiple queries simultaneously. GRAGPoison employs three key strategies: i) relation injection to introduce false knowledge, ii) relation enhancement to amplify poisoning influence, and iii) narrative generation to embed malicious content within coherent text. Empirical evaluation across diverse datasets and models shows that GRAGPoison substantially outperforms existing attacks in terms of effectiveness (up to 98\% success rate) and scalability (using less than 68\% poisoning text) on various GraphRAG-based systems. We also explore potential defensive measures and their limitations, identifying promising directions for future research.
        ]]></description>
    </item>
    <item>
        <title>Multilingual State Space Models for Structured Question Answering in Indic Languages</title>
        <link>https://arxiv.org/abs/2502.01673</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.01673v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Arpita Vats, Rahul Raja, Mrinal Mathur, Vinija Jain, Aman Chadha</dc:creator>
        <description><![CDATA[
            背景：印度语的多样性和复杂性给自然语言处理问答任务带来挑战。方法：本文探索应用状态空间模型（SSMs）构建适用于印度语的问答系统，因其能建模序列数据的长短时依赖，适合处理印度语特点。研究评估多种SSM架构，对不同印度语数据集进行对比分析，并提出改进现有框架以适配低资源和多语言场景。效果：模型有效捕捉语言微妙之处，在问题解释、上下文对齐和答案生成上有显著提升。
            arXiv:2502.01673v2 Announce Type: replace 
Abstract: The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.
        ]]></description>
    </item>
    <item>
        <title>Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models</title>
        <link>https://arxiv.org/abs/2502.05346</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.05346v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Christopher Nightingale, Dominic Lavington, Jonathan Thistlethwaite, Sebastian Penhaligon, Thomas Belinski, David Boldo</dc:creator>
        <description><![CDATA[
            背景：传统词嵌入在上下文推理中存在表征刚性和语义粒度不足问题。方法：将词嵌入表示为学习流形上的概率分布，在注意力机制中集成概率子空间以实现自适应上下文加权。效果：概率嵌入提高了邻域一致性、降低冗余，增强了对抗修改的鲁棒性，在特定领域应用中适应性更强，计算开销在可行范围内，还能在生成任务中处理长序列的连贯问题。
            arXiv:2502.05346v2 Announce Type: replace 
Abstract: Representing token embeddings as probability distributions over learned manifolds allows for more flexible contextual inference, reducing representational rigidity while enhancing semantic granularity. Comparative evaluations demonstrate that probabilistic embeddings improve neighborhood consistency and decrease redundancy, ensuring that token relationships remain more structurally coherent across fine-tuning iterations. The integration of probabilistic subspaces within attention mechanisms facilitates more adaptive contextual weighting, enabling models to capture latent dependencies that would otherwise be obscured in conventional embeddings. Experimental results highlight increased robustness against adversarial modifications, with probabilistic embeddings preserving contextual integrity even under perturbation-based evaluation scenarios. Performance assessments indicate that probabilistic representations achieve greater adaptability in domain-specific applications, mitigating the need for extensive retraining when shifting across linguistic domains. Computational trade-offs remain within operationally feasible limits, with marginal increases in inference latency balanced against the benefits of enhanced representation stability and contextual expressiveness. The capacity to encode structured uncertainty provides advantages in generative modeling tasks, particularly where maintaining coherence across extended sequences requires a representation framework capable of handling ambiguous or context-dependent linguistic constructs.
        ]]></description>
    </item>
    <item>
        <title>Dynamic Pyramid Network for Efficient Multimodal Large Language Model</title>
        <link>https://arxiv.org/abs/2503.20322</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.20322v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hao Ai, Kunyi Wang, Zezhou Wang, Hao Lu, Jin Tian, Yaxin Luo, Peng Xing, Jen-Yuan Huang, Huaxia Li, Gen luo</dc:creator>
        <description><![CDATA[
            多模态大语言模型在视觉语言任务表现出色，但计算成本高限制应用。现有视觉压缩方法易破坏语义。为此提出动态金字塔网络（DPN），将模型设为分层结构，随深度增加逐步压缩视觉特征，浅层可感知细粒度信息。还提出动态池化专家（DPE），能据输入特征选最优压缩率。实验在两个模型和十个基准上进行，DPN使LLaVA平均FLOPs节省56%，性能提升0.74%，在LLaVA - HR上也验证了泛化性。
            arXiv:2503.20322v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have demonstrated impressive performance in various vision-language (VL) tasks, but their expensive computations still limit the real-world application. To address this issue, recent efforts aim to compress the visual features to save the computational costs of MLLMs. However, direct visual compression methods, e.g. efficient projectors, inevitably destroy the visual semantics in MLLM, especially in difficult samples. To overcome this shortcoming, we propose a novel dynamic pyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as a hierarchical structure where visual features are gradually compressed with increasing depth. In this case, even with a high compression ratio, fine-grained visual information can still be perceived in shallow layers. To maximize the benefit of DPN, we further propose an innovative Dynamic Pooling Experts (DPE) that can dynamically choose the optimal visual compression rate according to input features. With this design, harder samples will be assigned larger computations, thus preserving the model performance. To validate our approach, we conduct extensive experiments on two popular MLLMs and ten benchmarks. Experimental results show that DPN can save up to 56% average FLOPs on LLaVA while further achieving +0.74% performance gains. Besides, the generalization ability of DPN is also validated on the existing high-resolution MLLM called LLaVA-HR. The source code will be released at https://github.com/aihao2000/DPN-LLaVA.
        ]]></description>
    </item>
    <item>
        <title>GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning</title>
        <link>https://arxiv.org/abs/2504.12597</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.12597v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng</dc:creator>
        <description><![CDATA[
            背景：几何问题解决能有效衡量多模态大语言模型（MLLMs）推理能力，但现有基准无法联合评估其类人几何推理机制。方法：提出首个双语基准GeoSense，含平面和立体几何五级几何原理框架、1789个精细标注问题的数据集及创新评估策略。效果：在GeoSense上对多种MLLMs实验，Gemini - 2.0 - pro - flash表现最佳，总分65.3，分析显示几何原理识别和应用仍是领先MLLMs推理能力瓶颈。
            arXiv:2504.12597v2 Announce Type: replace 
Abstract: Geometry problem-solving (GPS), a challenging task requiring both visual comprehension and symbolic reasoning, effectively measures the reasoning capabilities of multimodal large language models (MLLMs). Humans exhibit strong reasoning ability in this task through accurate identification and adaptive application of geometric principles within visual contexts. However, existing benchmarks fail to jointly assess both dimensions of the human-like geometric reasoning mechanism in MLLMs, remaining a critical gap in assessing their ability to tackle GPS. To this end, we introduce GeoSense, the first comprehensive bilingual benchmark designed to systematically evaluate the geometric reasoning abilities of MLLMs through the lens of geometric principles. GeoSense features a five-level hierarchical framework of geometric principles spanning plane and solid geometry, an intricately annotated dataset of 1,789 problems, and an innovative evaluation strategy. Through extensive experiments on GeoSense with various open-source and closed-source MLLMs, we observe that Gemini-2.0-pro-flash performs best, achieving an overall score of $65.3$. Our in-depth analysis reveals that the identification and application of geometric principles remain a bottleneck for leading MLLMs, jointly hindering their reasoning abilities. These findings underscore GeoSense's potential to guide future advancements in MLLMs' geometric reasoning capabilities, paving the way for more robust and human-like reasoning in artificial intelligence.
        ]]></description>
    </item>
    <item>
        <title>Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark</title>
        <link>https://arxiv.org/abs/2504.16427</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.16427v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Haige Zhu, Jie Zhou, Jinchao Zhang</dc:creator>
        <description><![CDATA[
            背景：多模态语言分析领域中，对多模态大语言模型理解认知层面语义能力的研究较少。方法：提出综合基准MMLA，包含超6.1万条多模态话语，涵盖多模态语义六个核心维度，用零样本推理、监督微调、指令调优等三种方法评估八种主流大语言模型和多模态大语言模型。效果：即使微调后的模型准确率也仅约60% - 70%，凸显当前模型理解复杂人类语言的局限性。
            arXiv:2504.16427v2 Announce Type: replace 
Abstract: Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.
        ]]></description>
    </item>
    <item>
        <title>Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?</title>
        <link>https://arxiv.org/abs/2501.15857</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.15857v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yutong Yin, Zhaoran Wang</dc:creator>
        <description><![CDATA[
            背景：人类能整合多源知识进行组合推理，而验证Transformers是否具备此能力及内在机制很有必要。方法：引入“FTCT”合成学习任务，训练数据为因果图分离知识片段，测试时模型需整合片段推断完整因果图轨迹，采用少样本思维链提示。效果：该提示能让Transformers在训练数据未出现的组合上进行组合推理，且推理能力与模型复杂度和训练测试数据相似度强相关，理论和实证表明其能从训练中学到可泛化程序用于测试推理。
            arXiv:2501.15857v3 Announce Type: replace-cross 
Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, "FTCT" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.
        ]]></description>
    </item>
    <item>
        <title>TALES: Text Adventure Learning Environment Suite</title>
        <link>https://arxiv.org/abs/2504.14128</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.14128v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre C\^ot\'e</dc:creator>
        <description><![CDATA[
            背景：推理能力对大语言模型与世界交互至关重要，复杂任务需更复杂多样的推理能力进行序列决策。方法：提出TALES，包含合成和人工编写的文本冒险游戏集合，用于挑战和评估多样推理能力，并对一系列大语言模型进行实验及对表现最佳模型定性分析。效果：即使表现最佳的大语言模型驱动的智能体，在面向人类设计的游戏中成功率也不足15%。
            arXiv:2504.14128v4 Announce Type: replace-cross 
Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tale-suite.
        ]]></description>
    </item>
    <item>
        <title>Unleashing the Power of Natural Audio Featuring Multiple Sound Sources</title>
        <link>https://arxiv.org/abs/2504.17782</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17782v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xize Cheng, Slytherin Wang, Zehan Wang, Rongjie Huang, Tao Jin, Zhou Zhao</dc:creator>
        <description><![CDATA[
            背景：通用声音分离对人工听觉感知至关重要，但现有方法依赖人工混合音频训练，对真实自然混合音频泛化能力有限。方法：提出ClearSep框架，用数据引擎将复杂自然混合音频分解为多个独立音轨，引入基于混音的评估指标并迭代优化，还提出适配分离音轨的训练策略。效果：大量实验表明，ClearSep在多个声音分离任务中达到了最先进水平，有潜力推动自然音频场景下的声音分离发展。
            arXiv:2504.17782v1 Announce Type: new 
Abstract: Universal sound separation aims to extract clean audio tracks corresponding to distinct events from mixed audio, which is critical for artificial auditory perception. However, current methods heavily rely on artificially mixed audio for training, which limits their ability to generalize to naturally mixed audio collected in real-world environments. To overcome this limitation, we propose ClearSep, an innovative framework that employs a data engine to decompose complex naturally mixed audio into multiple independent tracks, thereby allowing effective sound separation in real-world scenarios. We introduce two remix-based evaluation metrics to quantitatively assess separation quality and use these metrics as thresholds to iteratively apply the data engine alongside model training, progressively optimizing separation performance. In addition, we propose a series of training strategies tailored to these separated independent tracks to make the best use of them. Extensive experiments demonstrate that ClearSep achieves state-of-the-art performance across multiple sound separation tasks, highlighting its potential for advancing sound separation in natural audio scenarios. For more examples and detailed results, please visit our demo page at https://clearsep.github.io.
        ]]></description>
    </item>
    <item>
        <title>Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness</title>
        <link>https://arxiv.org/abs/2504.16936</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.16936v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yusheng Zhao, Junyu Luo, Xiao Luo, Weizhi Zhang, Zhiping Xiao, Wei Ju, Philip S. Yu, Ming Zhang</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型（MLLMs）在处理多模态信息方面取得成功，但缺乏对其视听能力的全面评估。方法：本文从有效性、效率、泛化性和鲁棒性四个维度对MLLMs的视听能力进行多方面评估。效果：实验发现MLLMs具有较强的零样本和少样本泛化能力，能在有限数据下表现出色，但严重依赖视觉模态，视觉输入受损或缺失会影响性能。此外，虽易受对抗样本影响，但比传统模型更具鲁棒性。
            arXiv:2504.16936v1 Announce Type: cross 
Abstract: Multi-modal large language models (MLLMs) have recently achieved great success in processing and understanding information from diverse modalities (e.g., text, audio, and visual signals). Despite their growing popularity, there remains a lack of comprehensive evaluation measuring the audio-visual capabilities of these models, especially in diverse scenarios (e.g., distribution shifts and adversarial attacks). In this paper, we present a multifaceted evaluation of the audio-visual capability of MLLMs, focusing on four key dimensions: effectiveness, efficiency, generalizability, and robustness. Through extensive experiments, we find that MLLMs exhibit strong zero-shot and few-shot generalization abilities, enabling them to achieve great performance with limited data. However, their success relies heavily on the vision modality, which impairs performance when visual input is corrupted or missing. Additionally, while MLLMs are susceptible to adversarial samples, they demonstrate greater robustness compared to traditional models. The experimental results and our findings provide insights into the audio-visual capabilities of MLLMs, highlighting areas for improvement and offering guidance for future research.
        ]]></description>
    </item>
</channel>
</rss>