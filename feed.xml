<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 09 Jun 2025 12:35:53 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Mon, 09 Jun 2025 12:35:53 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>Pts3D-LLM: Studying the Impact of Token Structure for 3D Scene Understanding With Large Language Models</title>
        <link>https://arxiv.org/abs/2506.05689</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05689v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hugues Thomas, Chen Chen, Jian Zhang</dc:creator>
        <description><![CDATA[
            背景：有效为多模态大语言模型（MLLMs）表示3D场景至关重要但具挑战，现有方法多依赖2D图像特征且分词方式多样。方法：系统比较基于视频和基于点的表示，提出通过融合Sonata预训练的Point Transformer V3编码器的3D点云特征来丰富视觉标记的新方法。效果：融合显式3D特征显著提升性能，精心采样和排序点时，基于点的标记结构可媲美基于视频的结构，两种结构的最佳模型在多个3D理解基准测试中达最优结果。
            arXiv:2506.05689v1 Announce Type: new 
Abstract: Effectively representing 3D scenes for Multimodal Large Language Models (MLLMs) is crucial yet challenging. Existing approaches commonly only rely on 2D image features and use varied tokenization approaches. This work presents a rigorous study of 3D token structures, systematically comparing video-based and point-based representations while maintaining consistent model backbones and parameters. We propose a novel approach that enriches visual tokens by incorporating 3D point cloud features from a Sonata pretrained Point Transformer V3 encoder. Our experiments demonstrate that merging explicit 3D features significantly boosts performance. Furthermore, we show that point-based token structures can rival video-based ones when the points are cleverly sampled and ordered. Our best models from both structures achieve state-of-the-art results on multiple 3D understanding benchmarks. We emphasize our analysis of token structures as a key contribution, alongside transparent reporting of results averaged over multiple seeds, a practice we believe is vital for robust progress in the field.
        ]]></description>
    </item>
    <item>
        <title>Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models</title>
        <link>https://arxiv.org/abs/2506.06008</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.06008v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Peijie Liu, Fengli Xu, Yong Li</dc:creator>
        <description><![CDATA[
            思维链（CoT）技术能提升大语言模型在复杂推理任务中的表现，但不同任务增益不稳定，其底层机制待解。本文初步观察到词元概率分布单调性或与CoT推理增益相关。基于此，提出两个基于词元概率分布的指标评估不同任务的CoT有效性；结合实例级指标与逻辑回归模型，提出动态选择CoT和直接回答的Dynamic CoT方法，并将决策策略从开源模型迁移到闭源模型。评估指标准确率达89.2%，Dynamic CoT减少超35%词元消耗且保持高精度。
            arXiv:2506.06008v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) technique has proven effective in improving the performance of large language models (LLMs) on complex reasoning tasks. However, the performance gains are inconsistent across different tasks, and the underlying mechanism remains a long-standing research question. In this work, we make a preliminary observation that the monotonicity of token probability distributions may be correlated with the gains achieved through CoT reasoning. Leveraging this insight, we propose two indicators based on the token probability distribution to assess CoT effectiveness across different tasks. By combining instance-level indicators with logistic regression model, we introduce Dynamic CoT, a method that dynamically select between CoT and direct answer. Furthermore, we extend Dynamic CoT to closed-source models by transferring decision strategies learned from open-source models. Our indicators for assessing CoT effectiveness achieve an accuracy of 89.2\%, and Dynamic CoT reduces token consumption by more than 35\% while maintaining high accuracy. Overall, our work offers a novel perspective on the underlying mechanisms of CoT reasoning and provides a framework for its more efficient deployment.
        ]]></description>
    </item>
    <item>
        <title>Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models</title>
        <link>https://arxiv.org/abs/2506.06137</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.06137v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Rihui Jin, Zheyu Xin, Xing Xie, Zuoyi Li, Guilin Qi, Yongrui Chen, Xinbang Dai, Tongtong Wu, Gholamreza Haffari</dc:creator>
        <description><![CDATA[
            表格推理（TR）需对半结构化表格数据进行结构化推理，对小语言模型（SLM）而言颇具挑战。为缩小其与大语言模型的差距，本文探索基于程序的TR（P - TR），但应用于SLM有两大挑战。为此提出Table - r1，分两阶段：第一阶段引入自监督学习任务提升表格布局泛化能力；第二阶段采用策略优化方法增强推理一致性。实验表明，Table - r1在四个TR基准测试中超越所有基于SLM的方法，在各数据集上比基础模型（LLaMA - 8B）准确率至少提高15%，性能可与大语言模型竞争。
            arXiv:2506.06137v1 Announce Type: new 
Abstract: Table reasoning (TR) requires structured reasoning over semi-structured tabular data and remains challenging, particularly for small language models (SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs (LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR), which circumvents key limitations of text-based TR (T-TR), notably in numerical reasoning, by generating executable programs. However, applying P-TR to SLMs introduces two challenges: (i) vulnerability to heterogeneity in table layouts, and (ii) inconsistency in reasoning due to limited code generation capability. We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1 introduces an innovative self-supervised learning task, Layout Transformation Inference, to improve tabular layout generalization from a programmatic view. Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization, enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed. Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all SLM-based methods, achieving at least a 15% accuracy improvement over the base model (LLaMA-8B) across all datasets and reaching performance competitive with LLMs.
        ]]></description>
    </item>
    <item>
        <title>DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models</title>
        <link>https://arxiv.org/abs/2408.04713</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2408.04713v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zifeng Ding, Yifeng Li, Yuan He, Antonio Norelli, Jingcheng Wu, Volker Tresp, Michael Bronstein, Yunpu Ma</dc:creator>
        <description><![CDATA[
            这是一篇关于连续时间动态图（CTDG）表示学习的论文。背景是CTDG学习需兼顾长节点交互历史和细微时间细节，存在计算资源需求大、需筛选关键时间信息的问题。方法是提出基于Mamba状态空间模型的DyGMamba，先利用节点级SSM编码历史节点交互序列，再用时间级SSM挖掘历史图中时间模式并动态筛选信息。实验表明，该模型在动态链接预测任务中多数情况下达最优，且计算资源使用高效。
            arXiv:2408.04713v4 Announce Type: replace 
Abstract: Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget.
        ]]></description>
    </item>
    <item>
        <title>An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction</title>
        <link>https://arxiv.org/abs/2505.21339</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.21339v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Henryk Mustroph, Michel Kunkler, Stefanie Rinderle-Ma</dc:creator>
        <description><![CDATA[
            背景：现有业务流程后缀预测方法聚焦单一场景，在流程有不确定性和高可变性时表现力受限。方法：提出概率后缀预测方法，基于不确定性感知的编解码器LSTM（U - ED - LSTM）和蒙特卡罗（MC）后缀采样算法，通过MC丢弃捕捉认知不确定性，以学习损失衰减捕捉偶然不确定性。效果：用四个真实和一个人工事件日志评估，结果显示概率后缀预测能优于最可能后缀预测，U - ED - LSTM有合理预测性能，且模型预测校准良好。
            arXiv:2505.21339v2 Announce Type: replace 
Abstract: Suffix prediction of business processes forecasts the remaining sequence of events until process completion. Current approaches focus on predicting the most likely suffix, representing a single scenario. However, when the future course of a process is subject to uncertainty and high variability, the expressiveness of such a single scenario can be limited, since other possible scenarios, which together may have a higher overall probability, are overlooked. To address this limitation, we propose probabilistic suffix prediction, a novel approach that approximates a probability distribution of suffixes. The proposed approach is based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. We capture epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. This technical report presents a comprehensive evaluation of the probabilistic suffix prediction approach's predictive performance and calibration under three different hyperparameter settings, using four real-life and one artificial event log. The results show that: i) probabilistic suffix prediction can outperform most likely suffix prediction, the U-ED-LSTM has reasonable predictive performance, and ii) the model's predictions are well calibrated.
        ]]></description>
    </item>
    <item>
        <title>Structure Guided Large Language Model for SQL Generation</title>
        <link>https://arxiv.org/abs/2402.13284</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2402.13284v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qinggang Zhang, Hao Chen, Junnan Dong, Shengyuan Chen, Feiran Huang, Xiao Huang</dc:creator>
        <description><![CDATA[
            背景：大语言模型虽能弥合自然语言查询与数据库管理系统间的差距，但难以理解复杂数据库结构和准确解读用户意图，且基于分解的方法在处理SQL生成任务时存在困难。方法：提出结构引导的文本到SQL框架SGU - SQL，通过基于语法的提示增强大语言模型的SQL生成能力，建立用户查询与数据库模式间的结构感知链接，并分解复杂生成任务。效果：在两个基准数据集上的实验表明，SGU - SQL始终优于现有文本到SQL模型。
            arXiv:2402.13284v3 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have shown promise in bridging the gap between natural language queries and database management systems, enabling users to interact with databases without the background of SQL. However, LLMs often struggle to comprehend complex database structures and accurately interpret user intentions. Decomposition-based methods have been proposed to enhance the performance of LLMs on complex tasks, but decomposing SQL generation into subtasks is non-trivial due to the declarative structure of SQL syntax and the intricate connections between query concepts and database elements. In this paper, we propose a novel Structure GUided text-to-SQL framework~(SGU-SQL) that incorporates syntax-based prompting to enhance the SQL generation capabilities of LLMs. Specifically, SGU-SQL establishes structure-aware links between user queries and database schema and decomposes the complex generation task using syntax-based prompting to enable more accurate LLM-based SQL generation. Extensive experiments on two benchmark datasets demonstrate that SGU-SQL consistently outperforms state-of-the-art text-to-SQL models.
        ]]></description>
    </item>
    <item>
        <title>Do Large Language Models Reason Causally Like Us? Even Better?</title>
        <link>https://arxiv.org/abs/2502.10215</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.10215v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hanna M. Dettki, Brenden M. Lake, Charley M. Wu, Bob Rehder</dc:creator>
        <description><![CDATA[
            因果推理是智能的核心组成部分，大语言模型在生成类人文本方面表现出色，但人们质疑其回答是反映真实理解还是统计模式。研究人员基于对撞机图任务，比较人类和四个大语言模型的因果推理能力，让模型根据其他变量的证据对查询变量出现的可能性进行评分。结果显示，模型的因果推理能力从常出现荒谬结论（GPT - 3.5）到类人表现，再到比人类更符合规范（GPT - 4o、Gemini - Pro和Claude）。计算模型拟合表明，后三者表现更优的原因之一是它们没有人类因果推理中的‘关联偏差’，但它们也未完全掌握对撞机图的微妙推理模式。
            arXiv:2502.10215v2 Announce Type: replace-cross 
Abstract: Causal reasoning is a core component of intelligence. Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns. We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. LLMs' causal inferences ranged from often nonsensical (GPT-3.5) to human-like to often more normatively aligned than those of humans (GPT-4o, Gemini-Pro, and Claude). Computational model fitting showed that one reason for GPT-4o, Gemini-Pro, and Claude's superior performance is they didn't exhibit the "associative bias" that plagues human causal reasoning. Nevertheless, even these LLMs did not fully capture subtler reasoning patterns associated with collider graphs, such as "explaining away".
        ]]></description>
    </item>
    <item>
        <title>ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation</title>
        <link>https://arxiv.org/abs/2502.13581</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.13581v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, Derek Zhiyuan Cheng</dc:creator>
        <description><![CDATA[
            生成式推荐（GR）是新兴范式，但现有GR模型独立标记每个动作，缺乏上下文感知，导致性能不佳。为解决此问题，研究提出ActionPiece方法，在标记动作序列时明确纳入上下文。该方法将每个动作表示为一组项目特征，基于特征模式的共现频率构建词汇表。考虑特征集的无序性，还引入集合排列正则化，产生具有相同语义的动作序列的多种分割。代码已开源。
            arXiv:2502.13581v2 Announce Type: replace-cross 
Abstract: Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a set of item features. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Our code is available at: https://github.com/google-deepmind/action_piece.
        ]]></description>
    </item>
    <item>
        <title>SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models</title>
        <link>https://arxiv.org/abs/2503.00211</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.00211v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiawei Zhang, Xuan Yang, Taiqi Wang, Yu Yao, Aleksandr Petiushko, Bo Li</dc:creator>
        <description><![CDATA[
            传统自动驾驶系统难将高层推理与低层控制关联，导致行为欠佳甚至不安全。多模态大语言模型虽提供统一感知与推理的机会，但将安全知识有效嵌入其中用于自动驾驶仍是挑战。为此提出SafeAuto框架，通过融合非结构化和结构化知识增强基于MLLM的自动驾驶。采用PDCE损失改进低层控制信号预测，开发推理组件将交通规则转化为一阶逻辑并嵌入概率图模型验证动作，利用多模态RAG模型从过往驾驶经验学习。该框架在多数据集上优于现有基线，实现更精准、可靠和安全的自动驾驶。
            arXiv:2503.00211v2 Announce Type: replace-cross 
Abstract: Traditional autonomous driving systems often struggle to connect high-level reasoning with low-level control, leading to suboptimal and sometimes unsafe behaviors. Recent advances in multimodal large language models (MLLMs), which process both visual and textual data, offer an opportunity to unify perception and reasoning. However, effectively embedding precise safety knowledge into MLLMs for autonomous driving remains a significant challenge. To address this, we propose SafeAuto, a framework that enhances MLLM-based autonomous driving by incorporating both unstructured and structured knowledge. First, we introduce a Position-Dependent Cross-Entropy (PDCE) loss to improve low-level control signal predictions when values are represented as text. Second, to explicitly integrate safety knowledge, we develop a reasoning component that translates traffic rules into first-order logic (e.g., "red light $\implies$ stop") and embeds them into a probabilistic graphical model (e.g., Markov Logic Network) to verify predicted actions using recognized environmental attributes. Additionally, our Multimodal Retrieval-Augmented Generation (RAG) model leverages video, control signals, and environmental attributes to learn from past driving experiences. Integrating PDCE, MLN, and Multimodal RAG, SafeAuto outperforms existing baselines across multiple datasets, enabling more accurate, reliable, and safer autonomous driving. The code is available at https://github.com/AI-secure/SafeAuto.
        ]]></description>
    </item>
    <item>
        <title>AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking</title>
        <link>https://arxiv.org/abs/2505.17312</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.17312v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang</dc:creator>
        <description><![CDATA[
            大语言模型处理复杂推理和解决问题任务时，常需有效配置。现有提示方法多采用通用、固定配置，难达特定任务最优。为此提出AdaReasoner，这是一种与大语言模型无关的插件，可自动为不同思维类型任务进行自适应推理配置。它基于强化学习框架训练，结合分解动作空间与目标探索策略及预训练奖励模型。经实验验证其收敛快、策略差距呈亚线性。在六种大模型和多种推理任务中，表现超基线，保持分布外鲁棒性，在知识密集型任务上有提升。
            arXiv:2505.17312v2 Announce Type: replace-cross 
Abstract: LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work 'well enough' across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts.
        ]]></description>
    </item>
    <item>
        <title>Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models</title>
        <link>https://arxiv.org/abs/2505.23091</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.23091v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang</dc:creator>
        <description><![CDATA[
            大语言模型推理能力进步显著，但多模态大模型尤其是小模型存在诸多挑战，如高质量数据集稀缺、视觉处理致推理能力下降等。为此，研究设计了Infi - MMR框架并提出Infi - MMR - 3B模型。该框架分三阶段：先激活基础推理能力，再适应跨模态推理，最后增强多模态推理。模型在多模态数学推理和一般推理能力上达到先进水平，如在MathVerse testmini等测试中有具体得分。
            arXiv:2505.23091v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model's logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini). Resources are available at https://huggingface.co/Reallm-Labs/Infi-MMR-3B.
        ]]></description>
    </item>
    <item>
        <title>Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning</title>
        <link>https://arxiv.org/abs/2506.05671</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05671v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yangui Fang, Jing Peng, Xu Li, Yu Xi, Chengwei Zhang, Guohui Zhong, Kai Yu</dc:creator>
        <description><![CDATA[
            背景：当前自动语音识别将语音编码器与大语言模型结合形成语音大模型，但在低资源场景下适应新领域仍具挑战。方法：提出一种仅用无配对目标领域文本对语音大模型进行微调的策略，且在微调时引入实时评估机制以保留语音 - 文本对齐。效果：在多个数据集实验显示，该方法能实现有竞争力的识别性能，与全音频 - 文本微调相比退化极小，还能提升对新领域的泛化能力，避免灾难性遗忘。
            arXiv:2506.05671v1 Announce Type: new 
Abstract: Recent advances in automatic speech recognition (ASR) have combined speech encoders with large language models (LLMs) through projection, forming Speech LLMs with strong performance. However, adapting them to new domains remains challenging, especially in low-resource settings where paired speech-text data is scarce. We propose a text-only fine-tuning strategy for Speech LLMs using unpaired target-domain text without requiring additional audio. To preserve speech-text alignment, we introduce a real-time evaluation mechanism during fine-tuning. This enables effective domain adaptation while maintaining source-domain performance. Experiments on LibriSpeech, SlideSpeech, and Medical datasets show that our method achieves competitive recognition performance, with minimal degradation compared to full audio-text fine-tuning. It also improves generalization to new domains without catastrophic forgetting, highlighting the potential of text-only fine-tuning for low-resource domain adaptation of ASR.
        ]]></description>
    </item>
    <item>
        <title>Voice Impression Control in Zero-Shot TTS</title>
        <link>https://arxiv.org/abs/2506.05688</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05688v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Keinichi Fujita, Shota Horiguchi, Yusuke Ijima</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。背景是语音中的副语言或非语言信息对塑造听众印象至关重要，但零样本语音合成中调控这些信息以控制语音印象仍具挑战。方法是开发了一种零样本语音合成中的语音印象控制方法，用低维向量表示各种语音印象对的强度，并通过大语言模型生成该向量。效果是客观和主观评估均证明该方法在印象控制上有效，且无需手动优化就能从自然语言描述生成目标印象。
            arXiv:2506.05688v1 Announce Type: new 
Abstract: Para-/non-linguistic information in speech is pivotal in shaping the listeners' impression. Although zero-shot text-to-speech (TTS) has achieved high speaker fidelity, modulating subtle para-/non-linguistic information to control perceived voice characteristics, i.e., impressions, remains challenging. We have therefore developed a voice impression control method in zero-shot TTS that utilizes a low-dimensional vector to represent the intensities of various voice impression pairs (e.g., dark-bright). The results of both objective and subjective evaluations have demonstrated our method's effectiveness in impression control. Furthermore, generating this vector via a large language model enables target-impression generation from a natural language description of the desired impression, thus eliminating the need for manual optimization.
        ]]></description>
    </item>
    <item>
        <title>Bridging the Modality Gap: Softly Discretizing Audio Representation for LLM-based Automatic Speech Recognition</title>
        <link>https://arxiv.org/abs/2506.05706</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05706v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mu Yang, Szu-Jui Chen, Jiamin Xie, John Hansen</dc:creator>
        <description><![CDATA[
            这是一篇关于音频离散化以适配大语言模型（LLM）进行自动语音识别（ASR）的研究。背景是音频数据的连续性与LLM基于离散标记的模式存在差异。方法上，提出将向量量化（VQ）集成到基于LLM的ASR中，用LLM嵌入表作为VQ码本，使音频编码器的连续表示与离散的LLM输入对齐，并通过更新码本和加权求和实现音频表示的软“离散化”。效果显示，该方法显著提升了基于LLM的ASR基线，尤其在域外条件下表现出色。
            arXiv:2506.05706v1 Announce Type: new 
Abstract: One challenge of integrating speech input with large language models (LLMs) stems from the discrepancy between the continuous nature of audio data and the discrete token-based paradigm of LLMs. To mitigate this gap, we propose a method for integrating vector quantization (VQ) into LLM-based automatic speech recognition (ASR). Using the LLM embedding table as the VQ codebook, the VQ module aligns the continuous representations from the audio encoder with the discrete LLM inputs, enabling the LLM to operate on a discretized audio representation that better reflects the linguistic structure. We further create a soft "discretization" of the audio representation by updating the codebook and performing a weighted sum over the codebook embeddings. Empirical results demonstrate that our proposed method significantly improves upon the LLM-based ASR baseline, particularly in out-of-domain conditions. This work highlights the potential of soft discretization as a modality bridge in LLM-based ASR.
        ]]></description>
    </item>
    <item>
        <title>WAKE: Watermarking Audio with Key Enrichment</title>
        <link>https://arxiv.org/abs/2506.05891</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05891v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yaoxun Xu, Jianwei Yu, Hangting Chen, Zhiyong Wu, Xixin Wu, Dong Yu, Rongzhi Gu, Yi Luo</dc:creator>
        <description><![CDATA[
            随着深度学习在音频生成领域的发展，音频安全和版权保护面临挑战，现有基于神经网络的音频水印方法存在防未授权访问、多次嵌入后解码初始水印及嵌入不同长度水印等问题。为此，研究人员提出首个密钥可控的音频水印框架WAKE。该框架用特定密钥嵌入水印，用对应密钥恢复，增强安全性，解决多次嵌入覆盖问题，支持可变长度水印插入。其在水印音频质量和水印检测准确性上均优于现有模型。
            arXiv:2506.05891v1 Announce Type: new 
Abstract: As deep learning advances in audio generation, challenges in audio security and copyright protection highlight the need for robust audio watermarking. Recent neural network-based methods have made progress but still face three main issues: preventing unauthorized access, decoding initial watermarks after multiple embeddings, and embedding varying lengths of watermarks. To address these issues, we propose WAKE, the first key-controllable audio watermark framework. WAKE embeds watermarks using specific keys and recovers them with corresponding keys, enhancing security by making incorrect key decoding impossible. It also resolves the overwriting issue by allowing watermark decoding after multiple embeddings and supports variable-length watermark insertion. WAKE outperforms existing models in both watermarked audio quality and watermark detection accuracy. Code, more results, and demo page: https://thuhcsi.github.io/WAKE.
        ]]></description>
    </item>
    <item>
        <title>WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS Prediction</title>
        <link>https://arxiv.org/abs/2506.05899</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05899v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jakaria Islam Emon, Kazi Tamanna Alam, Md. Abu Salek</dc:creator>
        <description><![CDATA[
            文本到音乐系统的平均意见得分（MOS）预测需评估整体音乐质量和文本提示对齐度。本文提出多模态架构WhisQ，通过序列级共注意力和最优传输正则化解决这一双重评估挑战。它用Whisper Base预训练模型进行音频时间编码，用Qwen 3小语言模型进行文本编码，以进行细粒度跨模态建模。该架构有专门预测路径，分别预测整体音乐质量和文本对齐度，并用Sinkhorn最优传输损失增强语义对齐。在MusicEval Track - 1数据集上，相比基线，Spearman相关性在整体音乐质量上提升7%，在文本对齐度上提升14%。
            arXiv:2506.05899v1 Announce Type: new 
Abstract: Mean Opinion Score (MOS) prediction for text to music systems requires evaluating both overall musical quality and text prompt alignment. This paper introduces WhisQ, a multimodal architecture that addresses this dual-assessment challenge through sequence level co-attention and optimal transport regularization. WhisQ employs the Whisper Base pretrained model for temporal audio encoding and Qwen 3, a 0.6B Small Language Model (SLM), for text encoding, with both maintaining sequence structure for fine grained cross-modal modeling. The architecture features specialized prediction pathways: OMQ is predicted from pooled audio embeddings, while TA leverages bidirectional sequence co-attention between audio and text. Sinkhorn optimal transport loss further enforce semantic alignment in the shared embedding space. On the MusicEval Track-1 dataset, WhisQ achieves substantial improvements over the baseline: 7% improvement in Spearman correlation for OMQ and 14% for TA. Ablation studies reveal that optimal transport regularization provides the largest performance gain (10% SRCC improvement), demonstrating the importance of explicit cross-modal alignment for text-to-music evaluation.
        ]]></description>
    </item>
    <item>
        <title>Audio-Aware Large Language Models as Judges for Speaking Styles</title>
        <link>https://arxiv.org/abs/2506.05984</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05984v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Cheng-Han Chiang, Xiaofei Wang, Chung-Ching Lin, Kevin Lin, Linjie Li, Radu Kopetz, Yao Qian, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang</dc:creator>
        <description><![CDATA[
            背景：音频感知大语言模型（ALLMs）可理解音频中的文本与非文本信息。方法：本文探索将ALLMs作为自动评判者评估演讲的说话风格，用其评估语音语言模型（SLMs）在语音风格指令遵循和角色扮演两项任务中的表现，考虑情感、音量等多种说话风格要素，使用四个SLMs完成任务，由人类和ALLMs评判，比较了GPT - 4o - audio和Gemini - 2.5 - pro两个ALLM评判者与人类评估结果。效果：Gemini与人类评判者的一致性和人类评估者之间的一致性相当，表明ALLMs可用于评估SLMs，同时显示当前SLMs在控制说话风格和生成自然对话方面仍有提升空间。
            arXiv:2506.05984v1 Announce Type: new 
Abstract: Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.
        ]]></description>
    </item>
    <item>
        <title>SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing</title>
        <link>https://arxiv.org/abs/2506.05414</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.05414v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mingfei Chen, Zijun Cui, Xiulong Liu, Jinlin Xiang, Caleb Zheng, Jingyuan Li, Eli Shlizerman</dc:creator>
        <description><![CDATA[
            这是一篇关于音频视觉大语言模型（AV - LLMs）的研究。现有AV - LLMs和基准主要关注静态或2D场景，对动态视听环境下的3D空间推理探索不足。为此，研究人员推出SAVVY - Bench基准，包含数千个涉及动静物体的关系。并提出SAVVY训练无关推理管道，分以自我为中心的空间轨迹估计和动态全局地图构建两阶段。实验表明，SAVVY显著提升了现有AV - LLMs在动态3D空间推理上的性能。
            arXiv:2506.05414v1 Announce Type: cross 
Abstract: 3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.
        ]]></description>
    </item>
    <item>
        <title>Efficient Fine-Grained Guidance for Diffusion Model Based Symbolic Music Generation</title>
        <link>https://arxiv.org/abs/2410.08435</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.08435v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tingyu Zhu, Haoyu Liu, Ziyu Wang, Zhimin Jiang, Zeyu Zheng</dc:creator>
        <description><![CDATA[
            这是一篇关于基于扩散模型的符号音乐生成的论文。背景是数据有限且音符音高需高精度，导致符号音乐生成有挑战。方法是在扩散模型中引入高效的细粒度引导（FGG）方法，引导模型生成更符合专家创作意图的音乐。效果是提高了生成音乐的准确性、可听性和质量，适用于即兴创作等高级应用，论文通过实验和主观评价验证了方法有效性，还发布演示页面实现实时交互生成。
            arXiv:2410.08435v3 Announce Type: replace 
Abstract: Developing generative models to create or conditionally create symbolic music presents unique challenges due to the combination of limited data availability and the need for high precision in note pitch. To address these challenges, we introduce an efficient Fine-Grained Guidance (FGG) approach within diffusion models. FGG guides the diffusion models to generate music that aligns more closely with the control and intent of expert composers, which is critical to improve the accuracy, listenability, and quality of generated music. This approach empowers diffusion models to excel in advanced applications such as improvisation, and interactive music creation. We derive theoretical characterizations for both the challenges in symbolic music generation and the effects of the FGG approach. We provide numerical experiments and subjective evaluation to demonstrate the effectiveness of our approach. We have published a demo page to showcase performances, which enables real-time interactive generation.
        ]]></description>
    </item>
    <item>
        <title>Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2411.05141</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.05141v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mu Yang, Bowen Shi, Matthew Le, Wei-Ning Hsu, Andros Tjandra</dc:creator>
        <description><![CDATA[
            该研究聚焦于提升零样本和少样本设置下的文本到音频（TTA）生成。受检索增强生成（RAG）在大语言模型中成功的启发，提出基于流匹配音频生成模型Audiobox的Audiobox TTA - RAG方法。不同于仅基于文本生成音频的传统方法，它通过结合文本和检索到的音频样本扩充条件输入。检索方法无需外部数据库中有标注音频。实验表明，该模型能有效利用检索到的音频样本，显著提升零样本和少样本TTA性能，多项评估指标提升明显，同时保持在域内生成语义对齐音频的能力。
            arXiv:2411.05141v2 Announce Type: replace 
Abstract: This work focuses on improving Text-To-Audio (TTA) generation on zero-shot and few-shot settings (i.e. generating unseen or uncommon audio events). Inspired by the success of Retrieval-Augmented Generation (RAG) in Large Language Models, we propose Audiobox TTA-RAG, a novel retrieval-augmented TTA approach based on Audiobox, a flow-matching audio generation model. Unlike the vanilla Audiobox TTA solution that generates audio conditioned on text only, we extend the TTA process by augmenting the conditioning input with both text and retrieved audio samples. Our retrieval method does not require the external database to have labeled audio, offering more practical use cases. We show that the proposed model can effectively leverage the retrieved audio samples and significantly improve zero-shot and few-shot TTA performance, with large margins on multiple evaluation metrics, while maintaining the ability to generate semantically aligned audio for the in-domain setting.
        ]]></description>
    </item>
    <item>
        <title>Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Audio-Language Models</title>
        <link>https://arxiv.org/abs/2411.14842</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.14842v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wanqi Yang, Yanda Li, Meng Fang, Yunchao Wei, Ling Chen</dc:creator>
        <description><![CDATA[
            随着大音频语言模型（LALMs）在语音人机交互中应用增多，对抗性音频攻击构成重大威胁。现有研究多针对特定模型，本文引入Chat - Audio Attacks（CAA）基准，包含四种音频攻击类型，用于探索对话场景中LALMs对音频攻击的脆弱性。提出标准评估、基于GPT - 4o评估和人工评估三种策略。用CAA基准的三种评估方法对六个先进LALMs评估，结果显示四种音频攻击对模型性能有影响，GPT - 4o展现出最高恢复力。
            arXiv:2411.14842v2 Announce Type: replace 
Abstract: Adversarial audio attacks pose a significant threat to the growing use of large audio-language models (LALMs) in voice-based human-machine interactions. While existing research focused on model-specific adversarial methods, real-world applications demand a more generalizable and universal approach to audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks (CAA) benchmark including four distinct types of audio attacks, which aims to explore the vulnerabilities of LALMs to these audio attacks in conversational scenarios. To evaluate the robustness of LALMs, we propose three evaluation strategies: Standard Evaluation, utilizing traditional metrics to quantify model performance under attacks; GPT-4o-Based Evaluation, which simulates real-world conversational complexities; and Human Evaluation, offering insights into user perception and trust. We evaluate six state-of-the-art LALMs with voice interaction capabilities, including Gemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on the CAA benchmark. Our comprehensive analysis reveals the impact of four types of audio attacks on the performance of these models, demonstrating that GPT-4o exhibits the highest level of resilience. Our data can be accessed via the following link: \href{https://github.com/crystraldo/CAA}{CAA}.
        ]]></description>
    </item>
    <item>
        <title>DualSpec: Text-to-spatial-audio Generation via Dual-Spectrogram Guided Diffusion Model</title>
        <link>https://arxiv.org/abs/2502.18952</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.18952v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lei Zhao, Sizhou Chen, Linfeng Feng, Jichao Zhang, Xiao-Lei Zhang, Chi Zhang, Xuelong Li</dc:creator>
        <description><![CDATA[
            背景：文本到音频生成近年备受关注，但现有工作多聚焦单声道音频，而空间音频能提供更沉浸听觉体验。方法：提出名为DualSpec的文本到空间音频生成框架，先训练变分自编码器提取潜在声学表征，用预训练大语言模型编码器将文本转为特征，再训练扩散模型用于空间音频生成；还使用两种声学特征提升合成质量和方位精度，构建含文本提示的数据集并引入新评估指标。效果：实验表明该方法能生成高定向和事件一致性的空间音频。
            arXiv:2502.18952v2 Announce Type: replace 
Abstract: Text-to-audio (TTA), which generates audio signals from textual descriptions, has received huge attention in recent years. However, recent works focused on text to monaural audio only. As we know, spatial audio provides more immersive auditory experience than monaural audio, e.g. in virtual reality. To address this issue, we propose a text-to-spatial-audio (TTSA) generation framework named DualSpec. Specifically, it first trains variational autoencoders (VAEs) for extracting the latent acoustic representations from sound event audio. Then, given text that describes sound events and event directions, the proposed method uses the encoder of a pretrained large language model to transform the text into text features. Finally, it trains a diffusion model from the latent acoustic representations and text features for the spatial audio generation. In the inference stage, only the text description is needed to generate spatial audio. Particularly, to improve the synthesis quality and azimuth accuracy of the spatial sound events simultaneously, we propose to use two kinds of acoustic features. One is the Mel spectrograms which is good for improving the synthesis quality, and the other is the short-time Fourier transform spectrograms which is good at improving the azimuth accuracy. We provide a pipeline of constructing spatial audio dataset with text prompts, for the training of the VAEs and diffusion model. We also introduce new spatial-aware evaluation metrics to quantify the azimuth errors of the generated spatial audio recordings. Experimental results demonstrate that the proposed method can generate spatial audio with high directional and event consistency.
        ]]></description>
    </item>
    <item>
        <title>Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment</title>
        <link>https://arxiv.org/abs/2505.04113</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04113v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xueyao Zhang, Yuancheng Wang, Chaoren Wang, Ziniu Li, Zhuo Chen, Zhizheng Wu</dc:creator>
        <description><![CDATA[
            背景：现代零样本文本转语音（TTS）系统虽经大量预训练，但在绕口令、重复词等场景下存在可懂度问题。方法：利用偏好对齐技术构建预训练分布外数据，引入新数据集INTP，扩展DPO框架以适配多种TTS架构。效果：经INTP对齐后，多种TTS模型在不同领域的可懂度、自然度、相似度和音频质量均有提升，还验证了INTP对更易听懂模型的弱到强泛化能力，且展示了基于Ints迭代对齐进一步改进的潜力。
            arXiv:2505.04113v2 Announce Type: replace 
Abstract: Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues. To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance. We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures. After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains. Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints. Moreover, we showcase the potential for further improvements through iterative alignment based on Ints. Audio samples are available at https://intalign.github.io/.
        ]]></description>
    </item>
    <item>
        <title>TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages</title>
        <link>https://arxiv.org/abs/2402.16021</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2402.16021v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Minsu Kim, Jee-weon Jung, Hyeongseop Rha, Soumi Maiti, Siddhant Arora, Xuankai Chang, Shinji Watanabe, Yong Man Ro</dc:creator>
        <description><![CDATA[
            多模态信息联合处理能力是重要任务，但配对多模态数据有限和计算需求大阻碍了其发展。为此提出Tri - Modal Translation (TMT)模型，将不同模态视为不同语言，把多模态翻译当作机器翻译问题。对语音和图像数据进行分词得到离散标记，降低计算成本。多模态编解码器进行核心翻译，特定模态处理仅在分词和去分词阶段。在六项模态翻译任务上评估，TMT始终优于单模型，表明统一任务对实用性和性能都有益。
            arXiv:2402.16021v2 Announce Type: replace-cross 
Abstract: The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.
        ]]></description>
    </item>
    <item>
        <title>Can Masked Autoencoders Also Listen to Birds?</title>
        <link>https://arxiv.org/abs/2504.12880</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.12880v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lukas Rauch, Ren\'e Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz</dc:creator>
        <description><![CDATA[
            这是一篇关于音频分类的论文。背景是通用的Masked Autoencoders（MAEs）模型在细粒度音频领域泛化能力差，如鸟类声音分类难以区分种间差异和种内声学变异性。方法上，作者使用类似AudioSet的大规模生物声学数据集BirdSet，系统调整预训练、微调方法和冻结特征利用方式，还引入参数高效的原型探测。效果上，Bird - MAE在BirdSet多标签分类基准中达新的最优，原型探测在MAP上比线性探测高37%$_	ext{p}$，与微调差距缩至约3.3%$_	ext{p}$，且在少样本基准中表现出强大能力。
            arXiv:2504.12880v3 Announce Type: replace-cross 
Abstract: Masked Autoencoders (MAEs) have shown competitive results in audio classification by learning rich semantic representations through an efficient self-supervised reconstruction task. However, general-purpose models fail to generalize well when applied directly to fine-grained audio domains. Specifically, bird-sound classification requires distinguishing subtle inter-species differences and managing high intra-species acoustic variability, thereby revealing the performance limitations of general-domain Audio-MAE models. This work demonstrates that bridging this domain gap requires more than domain-specific pretraining data; adapting the entire training pipeline is crucial. We systematically revisit and adapt the pretraining recipe, fine-tuning methods, and frozen feature utilization to bird sounds using BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE achieves new state-of-the-art results in BirdSet's multi-label classification benchmark. Additionally, we introduce the parameter-efficient prototypical probing, enhancing the utility of frozen MAE representations and closely approaching fine-tuning performance in low-resource settings. Bird-MAE's prototypical probes outperform linear probing by up to 37%$_\text{p}$ in MAP and narrow the gap to fine-tuning to approximately 3.3%$_\text{p}$ on average across BirdSet downstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with prototypical probing in our newly established few-shot benchmark on BirdSet, highlighting the potential of tailored self-supervised learning pipelines for fine-grained audio domains.
        ]]></description>
    </item>
    <item>
        <title>Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model</title>
        <link>https://arxiv.org/abs/2505.15670</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.15670v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr \.Zelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg</dc:creator>
        <description><![CDATA[
            这是一篇关于语音语言模型的研究。背景是当前语音语言模型多为轮替式交流，缺乏实时适应性。方法上，提出一种新型双工语音到语音架构，采用预训练流式编码器处理用户输入，分别构建代理和用户建模架构。效果显著，编解码器微调使代理语音更好，比特率减半至0.6 kbps；在推理、对话轮替和插入能力上超越以往双工模型；无需语音预训练，减少数据需求，简化构建流程，且提供训练和推理代码。
            arXiv:2505.15670v2 Announce Type: replace-cross 
Abstract: Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility.
        ]]></description>
    </item>
</channel>
</rss>