<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 02 Jul 2025 12:55:40 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Wed, 02 Jul 2025 12:55:40 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding</title>
        <link>https://arxiv.org/abs/2507.00068</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00068v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ziqi Zhong, Daniel Tang</dc:creator>
        <description><![CDATA[
            多模态学习虽有进展，但现有方法常分开处理模态，导致表征和推理不一致。本文提出MANTA框架，将视觉和听觉输入统一到结构化文本空间，以便与大语言模型无缝处理。它解决了四个关键挑战，通过信息论优化实现跨模态语义对齐等。该方法在严格数学框架下形式化，证明了在令牌约束下上下文选择的最优性。长视频问答等实验表明，MANTA使现有模型整体准确率最高提升22.6%，在时长超30分钟视频上提升27.3%，在时间推理和跨模态理解任务上分别提升23.8%和25.1%。
            arXiv:2507.00068v1 Announce Type: new 
Abstract: While multi-modal learning has advanced significantly, current approaches often treat modalities separately, creating inconsistencies in representation and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization via Textual Alignment), a theoretically-grounded framework that unifies visual and auditory inputs into a structured textual space for seamless processing with large language models. MANTA addresses four key challenges: (1) semantic alignment across modalities with information-theoretic optimization, (2) adaptive temporal synchronization for varying information densities, (3) hierarchical content representation for multi-scale understanding, and (4) context-aware retrieval of sparse information from long sequences. We formalize our approach within a rigorous mathematical framework, proving its optimality for context selection under token constraints. Extensive experiments on the challenging task of Long Video Question Answering show that MANTA improves state-of-the-art models by up to 22.6% in overall accuracy, with particularly significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement) and cross-modal understanding (25.1% improvement). Our framework introduces novel density estimation techniques for redundancy minimization while preserving rare signals, establishing new foundations for unifying multimodal representations through structured text.
        ]]></description>
    </item>
    <item>
        <title>Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections</title>
        <link>https://arxiv.org/abs/2507.00263</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00263v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Vignesh Ram Nithin Kappagantula, Shayan Hassantabar</dc:creator>
        <description><![CDATA[
            背景：度假租赁平台上的房产图片缺乏结构化分类，给旅客了解房产空间布局带来挑战。方法：提出一个计算高效的机器学习流程，集成监督式房间类型检测模型、重叠检测模型和聚类算法，还利用多模态大语言模型将卧室组与床型对应。效果：单独评估各模型及整体流程，表现出色，显著优于对比学习和预训练嵌入聚类等现有方法，适用于实时和数据稀缺环境。
            arXiv:2507.00263v1 Announce Type: new 
Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing volume of property images, often uploaded without structured categorization. This lack of organization poses significant challenges for travelers attempting to understand the spatial layout of a property, particularly when multiple rooms of the same type are present. To address this issue, we introduce an effective approach for solving the room scene discovery and grouping problem, as well as identifying bed types within each bedroom group. This grouping is valuable for travelers to comprehend the spatial organization, layout, and the sleeping configuration of the property. We propose a computationally efficient machine learning pipeline characterized by low latency and the ability to perform effectively with sample-efficient learning, making it well-suited for real-time and data-scarce environments. The pipeline integrates a supervised room-type detection model, a supervised overlap detection model to identify the overlap similarity between two images, and a clustering algorithm to group the images of the same space together using the similarity scores. Additionally, the pipeline maps each bedroom group to the corresponding bed types specified in the property's metadata, based on the visual content present in the group's images using a Multi-modal Large Language Model (MLLM) model. We evaluate the aforementioned models individually and also assess the pipeline in its entirety, observing strong performance that significantly outperforms established approaches such as contrastive learning and clustering with pretrained embeddings.
        ]]></description>
    </item>
    <item>
        <title>Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies</title>
        <link>https://arxiv.org/abs/2507.00606</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00606v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tao Xiong, Xavier Hu, Wenyan Fan, Shengyu Zhang</dc:creator>
        <description><![CDATA[
            大语言模型虽能通过思维链、思维树等提示技术处理复杂任务，但依赖手动设计的特定任务提示，限制了适应性和效率。为此提出混合推理（MoR）训练框架，将多种推理策略嵌入大模型，实现无外部提示工程的自适应推理。该框架分思想生成和监督微调数据集构建两阶段。实验表明，MoR显著提升性能，MoR150用思维链提示时达到0.730（提升2.2%），相比基线达0.734（提升13.5%），无需特定任务提示，为多样任务推理提供通用方案。
            arXiv:2507.00606v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.
        ]]></description>
    </item>
    <item>
        <title>Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories</title>
        <link>https://arxiv.org/abs/2507.00711</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00711v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jhouben Cuesta-Ramirez, Samuel Beaussant, Mehdi Mounsif</dc:creator>
        <description><![CDATA[
            背景：通过强化学习训练的大语言模型在推理基准测试中取得不错结果，但有证据显示其生成的思维链常无效。方法：研究发现模型存在过度思考问题，即便有正确方案仍会生成不必要推理步骤。通过AIME2024数学基准对三个最先进模型进行实验。效果：揭示了模型在整合纠正信息能力上的关键局限，为实现可靠且可解释的推理带来新挑战。
            arXiv:2507.00711v1 Announce Type: new 
Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have recently achieved impressive results on reasoning benchmarks. Yet, growing evidence shows that these models often generate longer but ineffective chains of thought (CoTs), calling into question whether benchmark gains reflect real reasoning improvements. We present new evidence of overthinking, where models disregard correct solutions even when explicitly provided, instead continuing to generate unnecessary reasoning steps that often lead to incorrect conclusions. Experiments on three state-of-the-art models using the AIME2024 math benchmark reveal critical limitations in these models ability to integrate corrective information, posing new challenges for achieving robust and interpretable reasoning.
        ]]></description>
    </item>
    <item>
        <title>Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation</title>
        <link>https://arxiv.org/abs/2507.00752</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00752v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hao Xing, Kai Zhe Boey, Yuankai Wu, Darius Burschka, Gordon Cheng</dc:creator>
        <description><![CDATA[
            在协作场景中，智能机器人精准理解人类动作子活动标签及时间结构很关键，但人体姿态估计和目标检测的噪声易致过分割错误。为此，本文提出多模态图卷积网络（MMGCN），结合低帧率视觉数据和高帧率运动数据以减少分割碎片化。方法包括：用正弦编码提升空间表示鲁棒性；通过时间图融合模块对齐多模态输入；设计SmoothLabelMix增强预测的时间一致性。实验显示，该方法在动作分割精度上优于现有技术，F1@10达94.5%，F1@25达92.8%。
            arXiv:2507.00752v1 Announce Type: new 
Abstract: Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.
        ]]></description>
    </item>
    <item>
        <title>TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables</title>
        <link>https://arxiv.org/abs/2507.00041</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00041v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Varun Mannam, Fang Wang, Chaochun Liu, Xin Chen</dc:creator>
        <description><![CDATA[
            在人才管理系统中，关键信息多以复杂表格形式存在，传统语言模型处理表格信息检索面临挑战，现有表格提取方法语义理解不足。为此，本文提出TalentMine框架，运用专门的多模态推理将提取的表格转化为语义丰富的表示，保留表格数据的结构和语义维度。实验表明，在员工福利文档集合中，TalentMine查询回答任务准确率达100%，远高于标准AWS Textract提取的0%和其视觉问答能力的40%。研究还表明Claude v3 Haiku模型在人才管理应用中性能最优。
            arXiv:2507.00041v1 Announce Type: cross 
Abstract: In talent management systems, critical information often resides in complex tabular formats, presenting significant retrieval challenges for conventional language models. These challenges are pronounced when processing Talent documentation that requires precise interpretation of tabular relationships for accurate information retrieval and downstream decision-making. Current table extraction methods struggle with semantic understanding, resulting in poor performance when integrated into retrieval-augmented chat applications. This paper identifies a key bottleneck - while structural table information can be extracted, the semantic relationships between tabular elements are lost, causing downstream query failures. To address this, we introduce TalentMine, a novel LLM-enhanced framework that transforms extracted tables into semantically enriched representations. Unlike conventional approaches relying on CSV or text linearization, our method employs specialized multimodal reasoning to preserve both structural and semantic dimensions of tabular data. Experimental evaluation across employee benefits document collections demonstrates TalentMine's superior performance, achieving 100% accuracy in query answering tasks compared to 0% for standard AWS Textract extraction and 40% for AWS Textract Visual Q&amp;A capabilities. Our comparative analysis also reveals that the Claude v3 Haiku model achieves optimal performance for talent management applications. The key contributions of this work include (1) a systematic analysis of semantic information loss in current table extraction pipelines, (2) a novel LLM-based method for semantically enriched table representation, (3) an efficient integration framework for retrieval-augmented systems as end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks showing substantial improvements across multiple categories.
        ]]></description>
    </item>
    <item>
        <title>ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context</title>
        <link>https://arxiv.org/abs/2507.00417</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00417v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Joongwon Kim, Anirudh Goyal, Liang Tan, Hannaneh Hajishirzi, Srinivasan Iyer, Tianlu Wang</dc:creator>
        <description><![CDATA[
            背景：目前尚不清楚如何提升非推理模型（如Llama 3）的推理能力。方法：提出ASTRO框架，通过蒙特卡罗树搜索生成合成数据集，将搜索轨迹转化为自然语言思维链，对模型进行微调，并通过可验证奖励的强化学习进一步提升性能。效果：将其应用于Llama 3系列模型，在MATH - 500、AMC 2023和AIME 2024上分别取得16.0%、26.9%和20.0%的绝对性能提升，尤其在需要迭代修正的难题上表现出色。
            arXiv:2507.00417v1 Announce Type: cross 
Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. We finetune our models on these search-derived traces and further improve performance via RL with verifiable rewards. We apply ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. Our results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs.
        ]]></description>
    </item>
    <item>
        <title>Transformers from Diffusion: A Unified Framework for Neural Message Passing</title>
        <link>https://arxiv.org/abs/2409.09111</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.09111v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qitian Wu, David Wipf, Junchi Yan</dc:creator>
        <description><![CDATA[
            学习具有特定几何结构的结构化数据表示是一项基础挑战，消息传递神经网络是常用解决方案。本文受物理系统启发，提出能量约束扩散模型，将流形上的扩散归纳偏置与能量最小化的逐层约束相结合。研究发现扩散算子与扩散过程隐式下降的能量函数一一对应，求解该系统的有限差分迭代可导出各类消息传递网络的传播层。基于此构建统一数学框架，设计了扩散启发的Transformer（DIFFormer）。在多类数据集上，该模型在数据结构全观测、部分观测或完全未观测场景中均表现良好。
            arXiv:2409.09111v4 Announce Type: replace 
Abstract: Learning representations for structured data with certain geometries (e.g., observed or unobserved) is a fundamental challenge, wherein message passing neural networks (MPNNs) have become a de facto class of model solutions. In this paper, inspired by physical systems, we propose an energy-constrained diffusion model, which integrates the inductive bias of diffusion on manifolds with layer-wise constraints of energy minimization. We identify that the diffusion operators have a one-to-one correspondence with the energy functions implicitly descended by the diffusion process, and the finite-difference iteration for solving the energy-constrained diffusion system induces the propagation layers of various types of MPNNs operating on observed or latent structures. This leads to a unified mathematical framework for common neural architectures whose computational flows can be cast as message passing (or its special case), including MLPs, GNNs, and Transformers. Building on these insights, we devise a new class of neural message passing models, dubbed diffusion-inspired Transformers (DIFFormer), whose global attention layers are derived from the principled energy-constrained diffusion framework. Across diverse datasets ranging from real-world networks to images, texts, and physical particles, we demonstrate that the new model achieves promising performance in scenarios where the data structures are observed (as a graph), partially observed, or entirely unobserved.
        ]]></description>
    </item>
    <item>
        <title>Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers</title>
        <link>https://arxiv.org/abs/2506.23918</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.23918v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, Yi R. Fung</dc:creator>
        <description><![CDATA[
            背景：当前多模态推理受文本思维链推动，但以文本为中心的方式使视觉与符号思维存在“语义鸿沟”。方法：本文提出从利用图像思考到真正用图像思考的范式转变，按认知自主性提升分三个阶段，包括外部工具探索、编程操作和内在想象，并在四方面作贡献，如确立范式原则与框架、综述各阶段核心方法等。效果：为构建更强大且符合人类认知的多模态AI研究提供清晰路线。
            arXiv:2506.23918v2 Announce Type: replace 
Abstract: Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental "semantic gap" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI.
        ]]></description>
    </item>
    <item>
        <title>Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis</title>
        <link>https://arxiv.org/abs/2507.00227</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00227v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Paul Mayer, Florian Lux, Alejandro P\'erez-Gonz\'alez-de-Martos, Angelina Elizarova, Lindsey Vanderlyn, Dirk V\"ath, Ngoc Thang Vu</dc:creator>
        <description><![CDATA[
            在文本转语音合成中，为语句生成富有表现力的韵律是一项具有挑战性的任务，尤其是对于通过音高、能量和时长等参数显式建模韵律的系统。本文研究了随机方法（包括归一化流、条件流匹配和整流流）在该任务中的有效性，并与传统确定性基线和真实人类表现进行比较。大量主客观评估表明，随机方法通过捕捉人类语音的内在变异性，能生成与人类说话者相当的自然韵律，且可通过调整采样温度增加可控性。
            arXiv:2507.00227v1 Announce Type: new 
Abstract: While generative methods have progressed rapidly in recent years, generating expressive prosody for an utterance remains a challenging task in text-to-speech synthesis. This is particularly true for systems that model prosody explicitly through parameters such as pitch, energy, and duration, which is commonly done for the sake of interpretability and controllability. In this work, we investigate the effectiveness of stochastic methods for this task, including Normalizing Flows, Conditional Flow Matching, and Rectified Flows. We compare these methods to a traditional deterministic baseline, as well as to real human realizations. Our extensive subjective and objective evaluations demonstrate that stochastic methods produce natural prosody on par with human speakers by capturing the variability inherent in human speech. Further, they open up additional controllability options by allowing the sampling temperature to be tuned.
        ]]></description>
    </item>
    <item>
        <title>AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio embedding Sequences</title>
        <link>https://arxiv.org/abs/2507.00475</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00475v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Minoru Kishi, Ryosuke Sakai, Shinnosuke Takamichi, Yusuke Kanamori, Yuki Okamoto</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成领域的论文。背景是文本到音频（TTA）合成中主观评价需成本，现有客观评价指标与主观评价相关性弱。方法是提出新的客观评价指标AudioBERTScore，计算合成音频与参考音频嵌入的相似度，不仅基于传统BERTScore的最大范数，还采用$p$-范数以反映环境声音的非局部特性。效果是该方法所得分数与主观评价的相关性高于传统指标。
            arXiv:2507.00475v1 Announce Type: new 
Abstract: We propose a novel objective evaluation metric for synthesized audio in text-to-audio (TTA), aiming to improve the performance of TTA models. In TTA, subjective evaluation of the synthesized sound is an important, but its implementation requires monetary costs. Therefore, objective evaluation such as mel-cepstral distortion are used, but the correlation between these objective metrics and subjective evaluation values is weak. Our proposed objective evaluation metric, AudioBERTScore, calculates the similarity between embedding of the synthesized and reference sounds. The method is based not only on the max-norm used in conventional BERTScore but also on the $p$-norm to reflect the non-local nature of environmental sounds. Experimental results show that scores obtained by the proposed method have a higher correlation with subjective evaluation values than conventional metrics.
        ]]></description>
    </item>
    <item>
        <title>LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End</title>
        <link>https://arxiv.org/abs/2507.00755</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00755v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jinhai Hu, Zhongyi Zhang, Cong Sheng Leow, Wang Ling Goh, Yuan Gao</dc:creator>
        <description><![CDATA[
            这是一篇关于音频信号分类中可学习模拟前端（AFE）的电路算法协同设计框架的论文。背景是传统分别设计AFE和后端分类器不理想。方法上，提出将后端分类器与AFE传递函数联合优化，在分类器的信噪比感知训练循环中调整模拟带通滤波器组的传递函数参数，采用协同设计损失函数LBPF。效果显著，在开源SKY130 130nm CMOS工艺中，对10关键词分类任务，在5 dB至20 dB输入信号信噪比下准确率达90.5%-94.2%，仅22k分类器参数，且相比传统方法，功率和电容面积分别降低8.7%和12.9%。
            arXiv:2507.00755v1 Announce Type: new 
Abstract: This paper presents a circuit-algorithm co-design framework for learnable analog front-end (AFE) in audio signal classification. Designing AFE and backend classifiers separately is a common practice but non-ideal, as shown in this paper. Instead, this paper proposes a joint optimization of the backend classifier with the AFE's transfer function to achieve system-level optimum. More specifically, the transfer function parameters of an analog bandpass filter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training loop for the classifier. Using a co-design loss function LBPF, this work shows superior optimization of both the filter bank and the classifier. Implemented in open-source SKY130 130nm CMOS process, the optimized design achieved 90.5%-94.2% accuracy for 10-keyword classification task across a wide range of input signal SNR from 5 dB to 20 dB, with only 22k classifier parameters. Compared to conventional approach, the proposed audio AFE achieves 8.7% and 12.9% reduction in power and capacitor area respectively.
        ]]></description>
    </item>
    <item>
        <title>Multi-interaction TTS toward professional recording reproduction</title>
        <link>https://arxiv.org/abs/2507.00808</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00808v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hiroki Kanagawa, Kenichi Fujita, Aya Watanabe, Yusuke Ijima</dc:creator>
        <description><![CDATA[
            背景：在文本转语音合成（TTS）中，基于迭代反馈的细化过程被忽视，初始合成后无法进行细粒度风格细化。方法：提出一种多步交互的TTS方法，模拟配音演员与配音导演的关系，对TTS模型和用户之间的交互进行建模。效果：实验表明，该模型及其对应数据集能根据用户指示进行迭代风格细化，展示了多交互能力。
            arXiv:2507.00808v1 Announce Type: new 
Abstract: Voice directors often iteratively refine voice actors' performances by providing feedback to achieve the desired outcome. While this iterative feedback-based refinement process is important in actual recordings, it has been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained style refinement after the initial synthesis is not possible, even though the synthesized speech often deviates from the user's intended style. To address this issue, we propose a TTS method with multi-step interaction that allows users to intuitively and rapidly refine synthetized speech. Our approach models the interaction between the TTS model and its user to emulate the relationship between voice actors and voice directors. Experiments show that the proposed model with its corresponding dataset enable iterative style refinements in accordance with users' directions, thus demonstrating its multi-interaction capability. Sample audios are available: https://ntt-hilab-gensp. github.io/ssw13multiinteraction_tts/
        ]]></description>
    </item>
    <item>
        <title>Improving Stereo 3D Sound Event Localization and Detection: Perceptual Features, Stereo-specific Data Augmentation, and Distance Normalization</title>
        <link>https://arxiv.org/abs/2507.00874</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.00874v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jun-Wei Yeow, Ee-Leng Tan, Santi Peksi, Woon-Seng Gan</dc:creator>
        <description><![CDATA[
            该报告是对DCASE 2025挑战赛任务3的参赛方案。背景是解决常规视频内容中的立体声声音事件定位与检测（SELD）纯音频任务。方法上，一是设计感知驱动的输入特征，提升事件检测、声源定位和距离估计能力；二是针对立体声音频特点采用特定增强策略，还引入FilterAugment技术；三是训练时应用距离归一化方法稳定回归目标。实验表明，在立体声STARSS23数据集上，所有SELD指标均有稳定性能提升。
            arXiv:2507.00874v1 Announce Type: new 
Abstract: This technical report presents our submission to Task 3 of the DCASE 2025 Challenge: Stereo Sound Event Localization and Detection (SELD) in Regular Video Content. We address the audio-only task in this report and introduce several key contributions. First, we design perceptually-motivated input features that improve event detection, sound source localization, and distance estimation. Second, we adapt augmentation strategies specifically for the intricacies of stereo audio, including channel swapping and time-frequency masking. We also incorporate the recently proposed FilterAugment technique that has yet to be explored for SELD work. Lastly, we apply a distance normalization approach during training to stabilize regression targets. Experiments on the stereo STARSS23 dataset demonstrate consistent performance gains across all SELD metrics. Code to replicate our work is available in this repository: https://github.com/itsjunwei/NTU_SNTL_Task3
        ]]></description>
    </item>
    <item>
        <title>Generative AI-based data augmentation for improved bioacoustic classification in noisy environments</title>
        <link>https://arxiv.org/abs/2412.01530</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.01530v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Anthony Gibbons, Emma King, Ian Donohue, Andrew Parnell</dc:creator>
        <description><![CDATA[
            在物种分类中，获取数据训练鲁棒的AI模型颇具挑战，经典图像增强技术不适用于音频频谱图。该研究采用辅助分类器生成对抗网络（ACGAN）和去噪扩散概率模型（DDPMs）合成频谱图以扩充音频数据，其中DDPMs在生成频谱图的真实性和分类任务准确性上表现出色。研究还提供了640小时爱尔兰风电场鸟类叫声数据集。结合真实和合成数据训练分类模型，与BirdNET预测对比准确率达92.6%（仅用真实数据为90.5%）。该方法可用于更多物种和土地类型，有望提升对珍稀物种的检测能力。
            arXiv:2412.01530v2 Announce Type: replace 
Abstract: 1. Obtaining data to train robust artificial intelligence (AI)-based models for species classification can be challenging, particularly for rare species. Data augmentation can boost classification accuracy by increasing the diversity of training data and is cheaper to obtain than expert-labelled data. However, many classic image-based augmentation techniques are not suitable for audio spectrograms. 2. We investigate two generative AI models as data augmentation tools to synthesise spectrograms and supplement audio data: Auxiliary Classifier Generative Adversarial Networks (ACGAN) and Denoising Diffusion Probabilistic Models (DDPMs). The latter performed particularly well in terms of both realism of generated spectrograms and accuracy in a resulting classification task. 3. Alongside these new approaches, we present a new audio data set of 640 hours of bird calls from wind farm sites in Ireland, approximately 800 samples of which have been labelled by experts. Wind farm data are particularly challenging for classification models given the background wind and turbine noise. 4. Training an ensemble of classification models on real and synthetic data combined gave 92.6% accuracy (and 90.5% with just the real data) when compared with highly confident BirdNET predictions. 5. Our approach can be used to augment acoustic signals for more species and other land-use types, and has the potential to bring about a step-change in our capacity to develop reliable AI-based detection of rare species. Our code is available at https://github.com/gibbona1/SpectrogramGenAI.
        ]]></description>
    </item>
    <item>
        <title>ETTA: Elucidating the Design Space of Text-to-Audio Models</title>
        <link>https://arxiv.org/abs/2412.19351</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.19351v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro</dc:creator>
        <description><![CDATA[
            近年来，文本到音频（TTA）合成取得显著进展，但数据、模型架构等对目标基准的影响尚不明确。为全面理解TTA模型设计空间，研究开展大规模实验，聚焦扩散和流匹配模型。贡献有：构建高质量合成字幕数据集AF - Synthetic；系统比较TTA模型不同设计选择；分析采样方法。基于分析提出ETTA模型，在AudioCaps和MusicCaps上优于公开数据训练的基线模型，与专有数据训练的模型有竞争力，且在生成复杂创意音频上能力更强。
            arXiv:2412.19351v2 Announce Type: replace 
Abstract: Recent years have seen significant progress in Text-To-Audio (TTA) synthesis, enabling users to enrich their creative workflows with synthetic audio generated from natural language prompts. Despite this progress, the effects of data, model architecture, training objective functions, and sampling strategies on target benchmarks are not well understood. With the purpose of providing a holistic understanding of the design space of TTA models, we set up a large-scale empirical experiment focused on diffusion and flow matching models. Our contributions include: 1) AF-Synthetic, a large dataset of high quality synthetic captions obtained from an audio understanding model; 2) a systematic comparison of different architectural, training, and inference design choices for TTA models; 3) an analysis of sampling methods and their Pareto curves with respect to generation quality and inference speed. We leverage the knowledge obtained from this extensive analysis to propose our best model dubbed Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps, ETTA provides improvements over the baselines trained on publicly available data, while being competitive with models trained on proprietary data. Finally, we show ETTA's improved ability to generate creative audio following complex and imaginative captions -- a task that is more challenging than current benchmarks.
        ]]></description>
    </item>
    <item>
        <title>Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples</title>
        <link>https://arxiv.org/abs/2505.14518</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.14518v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chun-Yi Kuan, Hung-yi Lee</dc:creator>
        <description><![CDATA[
            这是一篇关于音频大语言模型的研究。背景是当前音频大语言模型（ALLMs）常出现幻觉，影响其在现实应用中的可靠性。方法上，提出LISTEN训练方法，利用骨干大语言模型合成的数据，增强ALLMs区分存在和不存在声音的能力，且无需修改大语言模型参数，通过轻量级适配器集成音频表示。效果显示，该方法有效减少了幻觉，在现有音频问答和推理基准测试中保持良好表现，且在数据和计算方面更高效。
            arXiv:2505.14518v2 Announce Type: replace 
Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them to process and understand audio inputs. However, these models often hallucinate non-existent sound events, reducing their reliability in real-world applications. To address this, we propose LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method that enhances ALLMs' ability to distinguish between present and absent sounds using synthesized data from the backbone LLM. Unlike prior approaches, our method requires no modification to LLM parameters and efficiently integrates audio representations via a lightweight adapter. Experiments show that LISTEN effectively mitigates hallucinations while maintaining impressive performance on existing audio question and reasoning benchmarks. At the same time, it is more efficient in both data and computation.
        ]]></description>
    </item>
    <item>
        <title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title>
        <link>https://arxiv.org/abs/2505.16211</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.16211v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li</dc:creator>
        <description><![CDATA[
            随着音频大语言模型（ALLMs）快速发展和应用拓展，对其可信度评估的系统研究尚待完善，现有框架多关注文本模态或仅考虑部分安全维度。为此，本文提出首个针对ALLMs的多维度可信度评估框架AudioTrust。它涵盖公平性、幻觉性等六个关键维度，围绕18种实验设置，利用超4420个音频/文本样本数据集和9种音频特定评估指标，通过自动化流程评估。实验揭示了当前ALLMs在高风险音频场景下的可信度边界和局限，为未来模型安全可靠部署提供参考。
            arXiv:2505.16211v2 Announce Type: replace 
Abstract: The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.
        ]]></description>
    </item>
    <item>
        <title>StreamFlow: Streaming Flow Matching with Block-wise Guided Attention Mask for Speech Token Decoding</title>
        <link>https://arxiv.org/abs/2506.23986</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.23986v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Dake Guo, Jixun Yao, Linhan Ma, He Wang, Lei Xie</dc:creator>
        <description><![CDATA[
            在基于离散令牌的语音生成中，令牌到波形的生成对音频质量很重要，但传统语义令牌与流匹配框架因依赖全局感受野难以实现流式处理，逐令牌流式语音生成又会降低音频质量。为此提出StreamFlow，采用局部块式感受野策略，先将序列分块，引入块式注意力掩码调节扩散变压器感受野。实验表明，该方法性能与非流式方法相当，语音质量超其他流式方法，长序列生成时能有效控制推理时间，首包延迟仅180毫秒。
            arXiv:2506.23986v2 Announce Type: replace 
Abstract: Recent advancements in discrete token-based speech generation have highlighted the importance of token-to-waveform generation for audio quality, particularly in real-time interactions. Traditional frameworks integrating semantic tokens with flow matching (FM) struggle with streaming capabilities due to their reliance on a global receptive field. Additionally, directly implementing token-by-token streaming speech generation often results in degraded audio quality. To address these challenges, we propose StreamFlow, a novel neural architecture that facilitates streaming flow matching with diffusion transformers (DiT). To mitigate the long-sequence extrapolation issues arising from lengthy historical dependencies, we design a local block-wise receptive field strategy. Specifically, the sequence is first segmented into blocks, and we introduce block-wise attention masks that enable the current block to receive information from the previous or subsequent block. These attention masks are combined hierarchically across different DiT-blocks to regulate the receptive field of DiTs. Both subjective and objective experimental results demonstrate that our approach achieves performance comparable to non-streaming methods while surpassing other streaming methods in terms of speech quality, all the while effectively managing inference time during long-sequence generation. Furthermore, our method achieves a notable first-packet latency of only 180 ms.\footnote{Speech samples: https://dukguo.github.io/StreamFlow/}
        ]]></description>
    </item>
    <item>
        <title>Contrastive Conditional Latent Diffusion for Audio-visual Segmentation</title>
        <link>https://arxiv.org/abs/2307.16579</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2307.16579v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuxin Mao, Jing Zhang, Mochu Xiang, Yunqiu Lv, Dong Li, Yiran Zhong, Yuchao Dai</dc:creator>
        <description><![CDATA[
            背景：为深入研究音频对视听分割（AVS）的影响，需建模音频与分割图的关联。方法：提出对比条件潜在扩散模型，结合潜在扩散模型进行语义相关表征学习，学习真实分割图的条件生成过程；通过最小化多模态和单模态数据条件概率的密度比，建模音频信号的贡献，以对比学习为约束，扩散部分实现最大似然估计。效果：有效增强音频对AVS的贡献，在基准数据集上验证了有效性。
            arXiv:2307.16579v2 Announce Type: replace-cross 
Abstract: We propose a contrastive conditional latent diffusion model for audio-visual segmentation (AVS) to thoroughly investigate the impact of audio, where the correlation between audio and the final segmentation map is modeled to guarantee the strong correlation between them. To achieve semantic-correlated representation learning, our framework incorporates a latent diffusion model. The diffusion model learns the conditional generation process of the ground-truth segmentation map, resulting in ground-truth aware inference during the denoising process at the test stage. As our model is conditional, it is vital to ensure that the conditional variable contributes to the model output. We thus extensively model the contribution of the audio signal by minimizing the density ratio between the conditional probability of the multimodal data, e.g. conditioned on the audio-visual data, and that of the unimodal data, e.g. conditioned on the audio data only. In this way, our latent diffusion model via density ratio optimization explicitly maximizes the contribution of audio for AVS, which can then be achieved with contrastive learning as a constraint, where the diffusion part serves as the main objective to achieve maximum likelihood estimation, and the density ratio optimization part imposes the constraint. By adopting this latent diffusion model via contrastive learning, we effectively enhance the contribution of audio for AVS. The effectiveness of our solution is validated through experimental results on the benchmark dataset. Code and results are online via our project page: https://github.com/OpenNLPLab/DiffusionAVS.
        ]]></description>
    </item>
    <item>
        <title>ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</title>
        <link>https://arxiv.org/abs/2505.04203</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.04203v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhiping Qiu, Yitong Jin, Yuan Wang, Yi Shi, Chongwu Wang, Chao Tan, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai</dc:creator>
        <description><![CDATA[
            乐器演奏动作生成极具挑战，现有工作多关注部分身体动作建模。本文提出基于扩散模型的ELGAR框架，仅从音频实现全身细粒度乐器演奏动作生成。引入HICL和BICL保证交互真实性，设计新指标评估生成动作与音频语义的契合度。还整理出SPD - GEN数据集。大量评估和消融实验验证了方法有效性，ELGAR在生成复杂快速交互的演奏动作上潜力巨大，有望推动动画、音乐教育等领域发展。
            arXiv:2505.04203v2 Announce Type: replace-cross 
Abstract: The art of instrument performance stands as a vivid manifestation of human creativity and emotion. Nonetheless, generating instrument performance motions is a highly challenging task, as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performer-instrument interaction. While existing works primarily focus on modeling partial body motions, we propose Expressive ceLlo performance motion Generation for Audio Rendition (ELGAR), a state-of-the-art diffusion-based framework for whole-body fine-grained instrument performance motion generation solely from audio. To emphasize the interactive nature of the instrument performance, we introduce Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL), which effectively guarantee the authenticity of the interplay. Moreover, to better evaluate whether the generated motions align with the semantic context of the music audio, we design novel metrics specifically for string instrument performance motion generation, including finger-contact distance, bow-string distance, and bowing score. Extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods. In addition, we put forward a motion generation dataset SPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated, ELGAR has shown great potential in generating instrument performance motions with complicated and fast interactions, which will promote further development in areas such as animation, music education, interactive art creation, etc.
        ]]></description>
    </item>
</channel>
</rss>