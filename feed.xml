<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 19 May 2025 12:17:50 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Mon, 19 May 2025 12:17:50 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices</title>
        <link>https://arxiv.org/abs/2505.10607</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10607v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Patara Trirat, Jae-Gil Lee</dc:creator>
        <description><![CDATA[
            背景：智能手机和物联网设备的广泛使用，使得在资源受限硬件上进行高效时间序列分析变得必要，现有硬件感知神经架构搜索未聚焦边缘部署的通用时间序列分析。方法：提出MONAQ框架，将神经架构搜索转化为多目标神经架构查询任务，具备多模态查询生成能力处理多模态时间序列输入和硬件约束，通过基于大语言模型代理的多目标搜索和代码生成得到可部署模型，集成数值、图像和文本信息提升大模型对时间序列数据的理解。效果：在15个数据集上，MONAQ发现的模型优于手工模型和NAS基线且更高效。
            arXiv:2505.10607v1 Announce Type: new 
Abstract: The growing use of smartphones and IoT devices necessitates efficient time-series analysis on resource-constrained hardware, which is critical for sensing applications such as human activity recognition and air quality prediction. Recent efforts in hardware-aware neural architecture search (NAS) automate architecture discovery for specific platforms; however, none focus on general time-series analysis with edge deployment. Leveraging the problem-solving and reasoning capabilities of large language models (LLM), we propose MONAQ, a novel framework that reformulates NAS into Multi-Objective Neural Architecture Querying tasks. MONAQ is equipped with multimodal query generation for processing multimodal time-series inputs and hardware constraints, alongside an LLM agent-based multi-objective search to achieve deployment-ready models via code generation. By integrating numerical data, time-series images, and textual descriptions, MONAQ improves an LLM's understanding of time-series data. Experiments on fifteen datasets demonstrate that MONAQ-discovered models outperform both handcrafted models and NAS baselines while being more efficient.
        ]]></description>
    </item>
    <item>
        <title>GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?</title>
        <link>https://arxiv.org/abs/2505.10714</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10714v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Jiashu He, Joshua Bergerson, John K Hutchison, Jordan Branham, Camillo J Taylor, Tanwi Mallick</dc:creator>
        <description><![CDATA[
            背景：地理空间数据集因数值密集、时空依赖强、多模态表征独特，对基础模型理解能力提出挑战。方法：提出GeoGrid - Bench基准，采用大规模真实数据，涵盖16个气候变量、150个地点及长时间段，有约3200个问答对，由8个专家模板生成。效果：评估显示视觉 - 语言模型总体表现最佳，还对不同基础模型在地理空间任务中的优劣进行细粒度分析，为基础模型用于地理空间数据分析和科研提供见解。
            arXiv:2505.10714v1 Announce Type: new 
Abstract: We present GeoGrid-Bench, a benchmark designed to evaluate the ability of foundation models to understand geo-spatial data in the grid structure. Geo-spatial datasets pose distinct challenges due to their dense numerical values, strong spatial and temporal dependencies, and unique multimodal representations including tabular data, heatmaps, and geographic visualizations. To assess how foundation models can support scientific research in this domain, GeoGrid-Bench features large-scale, real-world data covering 16 climate variables across 150 locations and extended time frames. The benchmark includes approximately 3,200 question-answer pairs, systematically generated from 8 domain expert-curated templates to reflect practical tasks encountered by human scientists. These range from basic queries at a single location and time to complex spatiotemporal comparisons across regions and periods. Our evaluation reveals that vision-language models perform best overall, and we provide a fine-grained analysis of the strengths and limitations of different foundation models in different geo-spatial tasks. This benchmark offers clearer insights into how foundation models can be effectively applied to geo-spatial data analysis and used to support scientific research.
        ]]></description>
    </item>
    <item>
        <title>Ranked Voting based Self-Consistency of Large Language Models</title>
        <link>https://arxiv.org/abs/2505.10772</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10772v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Weiqin Wang, Yile Wang, Hui Huang</dc:creator>
        <description><![CDATA[
            背景：现有大模型思维链推理方法每次只生成一个答案，忽略其他潜在答案，影响后续投票。方法：提出在每个推理过程中生成排序答案，并在不同回复的多个排序答案间进行排序投票，采用即时决胜投票、博尔达计数投票和平均倒数排名投票三种方法。效果：在六个数据集上验证，包括选择题和开放式问答任务，使用开源和闭源大模型，结果显示该方法优于基线，证明利用排序答案信息和排序投票可提升推理性能。代码见https://github.com/szu-tera/RankedVotingSC。
            arXiv:2505.10772v1 Announce Type: new 
Abstract: Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest "self-consistency" among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers. As a result, these alternative answers are often overlooked in subsequent voting processes. In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable. Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting. We validate our methods on six datasets, including three multiple-choice and three open-ended question-answering tasks, using both advanced open-source and closed-source large language models. Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance. The code is available at https://github.com/szu-tera/RankedVotingSC.
        ]]></description>
    </item>
    <item>
        <title>Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting</title>
        <link>https://arxiv.org/abs/2505.10774</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10774v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yueyang Yao, Jiajun Li, Xingyuan Dai, MengMeng Zhang, Xiaoyan Gong, Fei-Yue Wang, Yisheng Lv</dc:creator>
        <description><![CDATA[
            时间序列预测在多个领域至关重要，但现有方法难以有效整合外部文本并与大语言模型的概率特性对齐。为此，本文提出CAPTime方法，先使用预训练时间序列编码器编码时间模式，再通过可学习交互将其与文本上下文对齐以生成联合多模态表示，结合分布专家混合体与冻结的大语言模型实现上下文感知概率预测。实验表明，该方法在多模态场景下预测准确性和泛化性更优，在数据稀缺场景下通过混合概率解码也展现出较强鲁棒性。
            arXiv:2505.10774v1 Announce Type: new 
Abstract: Time series forecasting is important for applications spanning energy markets, climate analysis, and traffic management. However, existing methods struggle to effectively integrate exogenous texts and align them with the probabilistic nature of large language models (LLMs). Current approaches either employ shallow text-time series fusion via basic prompts or rely on deterministic numerical decoding that conflict with LLMs' token-generation paradigm, which limits contextual awareness and distribution modeling. To address these limitations, we propose CAPTime, a context-aware probabilistic multimodal time series forecasting method that leverages text-informed abstraction and autoregressive LLM decoding. Our method first encodes temporal patterns using a pretrained time series encoder, then aligns them with textual contexts via learnable interactions to produce joint multimodal representations. By combining a mixture of distribution experts with frozen LLMs, we enable context-aware probabilistic forecasting while preserving LLMs' inherent distribution modeling capabilities. Experiments on diverse time series forecasting tasks demonstrate the superior accuracy and generalization of CAPTime, particularly in multimodal scenarios. Additional analysis highlights its robustness in data-scarce scenarios through hybrid probabilistic decoding.
        ]]></description>
    </item>
    <item>
        <title>Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL</title>
        <link>https://arxiv.org/abs/2505.10832</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10832v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao</dc:creator>
        <description><![CDATA[
            大推理模型生成详细推理序列时会产生大量计算开销和延迟，尤其对简单问题。为解决过度推理问题，本文基于R1风格蒸馏模型，发现插入省略号可随机触发推理或不推理模式。提出AutoThink多阶段强化学习框架，通过分阶段奖励塑造优化推理策略，必要时调用显式推理，简单任务则简洁作答。在五个数学基准测试中，相比现有方法，AutoThink实现了精度与效率的良好平衡，在DeepSeek - R1 - Distill - Qwen - 1.5B上相对精度提升6.4%，减少52%的token使用。
            arXiv:2505.10832v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis ("...") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs.
        ]]></description>
    </item>
    <item>
        <title>Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate</title>
        <link>https://arxiv.org/abs/2505.10870</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10870v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ziyang Huang, Wangtao Sun, Jun Zhao, Kang Liu</dc:creator>
        <description><![CDATA[
            背景：规则检索是关键但研究不足的领域，传统检索方法因查询实例事实与规则抽象表示间语义差距大，导致检索准确率低、影响推理性能。方法：提出Self - Induction Augmented Retrieval（SIAR），用大语言模型从查询中抽象知识和逻辑结构，归纳潜在推理规则用于查询增强；引入Rule Relevance ReEstimate（R³），重新评估检索规则与查询事实的相关性及对推理的帮助。效果：多场景实验证明了方法的有效性和通用性。
            arXiv:2505.10870v1 Announce Type: new 
Abstract: This paper systematically addresses the challenges of rule retrieval, a crucial yet underexplored area. Vanilla retrieval methods using sparse or dense retrievers to directly search for relevant rules to support downstream reasoning, often suffer from low accuracy. This is primarily due to a significant semantic gap between the instantiated facts in the queries and the abstract representations of the rules. Such misalignment results in suboptimal retrieval quality, which in turn negatively impacts reasoning performance. To overcome these challenges, we propose Self-Induction Augmented Retrieval (SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce potential inferential rules that might offer benefits for reasoning by abstracting the underlying knowledge and logical structure in queries. These induced rules are then used for query augmentation to improve retrieval effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a method that re-estimates the relevance of retrieved rules by assessing whether the abstract knowledge they contain can be instantiated to align with the facts in the queries and the helpfulness for reasoning. Extensive experiments across various settings demonstrate the effectiveness and versatility of our proposed methods.
        ]]></description>
    </item>
    <item>
        <title>Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations</title>
        <link>https://arxiv.org/abs/2505.10877</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10877v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mathieu Alain, So Takao, Xiaowen Dong, Bastian Rieck, Emmanuel Noutahi</dc:creator>
        <description><![CDATA[
            背景：预测图结构数据标签在科学应用中至关重要，常用图神经网络（GNNs）实现，但数据稀缺时GNNs易过拟合、性能差，近期提出以图为输入的高斯过程（GPs）作为替代。方法：将高斯过程框架扩展到单纯复形（SCs），处理边级属性和高阶单纯形上的属性，通过考虑霍奇分解增强SC表示，纳入同调信息。效果：该框架在多种应用中提升了预测效果，为GPs在图和SC级预测的更广泛应用奠定基础。
            arXiv:2505.10877v1 Announce Type: new 
Abstract: Predicting the labels of graph-structured data is crucial in scientific applications and is often achieved using graph neural networks (GNNs). However, when data is scarce, GNNs suffer from overfitting, leading to poor performance. Recently, Gaussian processes (GPs) with graph-level inputs have been proposed as an alternative. In this work, we extend the Gaussian process framework to simplicial complexes (SCs), enabling the handling of edge-level attributes and attributes supported on higher-order simplices. We further augment the resulting SC representations by considering their Hodge decompositions, allowing us to account for homological information, such as the number of holes, in the SC. We demonstrate that our framework enhances the predictions across various applications, paving the way for GPs to be more widely used for graph and SC-level predictions.
        ]]></description>
    </item>
    <item>
        <title>Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents</title>
        <link>https://arxiv.org/abs/2505.10936</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10936v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiaxing Zhao, Hongbin Xie, Yuzhen Lei, Xuan Song, Zhuoran Shi, Lianxin Li, Shuangxue Liu, Haoran Zhang</dc:creator>
        <description><![CDATA[
            背景：大语言模型在复杂推理任务中表现出色，但思维链单智能体面临跨领域提示设计难题，多智能体系统消耗大量令牌且易稀释核心问题。方法：提出协作提示框架Cochain，构建包含多阶段知识的集成知识图谱，维护和检索提示树获取业务流程其他阶段提示信息。效果：在多个数据集上评估，Cochain在提示工程和多智能体大语言模型方面均优于所有基线，专家评估显示小模型结合Cochain效果超GPT - 4。
            arXiv:2505.10936v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.
        ]]></description>
    </item>
    <item>
        <title>Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations</title>
        <link>https://arxiv.org/abs/2505.10937</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10937v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang</dc:creator>
        <description><![CDATA[
            背景：大型推理模型借助思维链（CoT）处理复杂任务，但缺乏全面的CoT数据集。方法：提出OmniThought，这是一个含200万个CoT过程的大规模数据集，由两个强大的大型推理模型生成和验证，每个CoT过程标注了推理详尽度（RV）和认知难度（CD）分数，并建立自主流程来管理数据集。效果：使用不同大小的Qwen2.5模型实验表明，所提分数对大型推理模型训练效果有积极影响，基于该数据集训练并发布的系列模型推理能力更强。 
            arXiv:2505.10937v1 Announce Type: new 
Abstract: The emergence of large reasoning models (LRMs) has transformed Natural Language Processing by excelling in complex tasks such as mathematical problem-solving and code generation. These models leverage chain-of-thought (CoT) processes, enabling them to emulate human-like reasoning strategies. However, the advancement of LRMs is hindered by the lack of comprehensive CoT datasets. Current resources often fail to provide extensive reasoning problems with coherent CoT processes distilled from multiple teacher models and do not account for multifaceted properties describing the internal characteristics of CoTs. To address these challenges, we introduce OmniThought, a large-scale dataset featuring 2 million CoT processes generated and validated by two powerful LRMs as teacher models. Each CoT process in OmniThought is annotated with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which describe the appropriateness of CoT verbosity and cognitive difficulty level for models to comprehend these reasoning processes. We further establish a self-reliant pipeline to curate this dataset. Extensive experiments using Qwen2.5 models of various sizes demonstrate the positive impact of our proposed scores on LRM training effectiveness. Based on the proposed OmniThought dataset, we further train and release a series of high-performing LRMs, specifically equipped with stronger reasoning abilities and optimal CoT output length and difficulty level. Our contributions significantly enhance the development and training of LRMs for solving complex tasks.
        ]]></description>
    </item>
    <item>
        <title>SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache</title>
        <link>https://arxiv.org/abs/2505.10951</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10951v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qiuyu Zhu, Liang Zhang, Qianxiong Xu, Cheng Long, Jie Zhang</dc:creator>
        <description><![CDATA[
            背景：基于图的检索增强生成（RAG）可让大语言模型通过图检索结合结构化知识，但不同查询可能检索到相似子图。方法：提出SubGCache，基于子图嵌入对查询聚类，为每个簇构建代表性子图并预计算其键值（KV）缓存，查询复用簇内代表性子图的预计算KV缓存以节省计算量。效果：在两个新数据集、多个大模型和图RAG框架上实验表明，SubGCache能降低推理延迟，生成质量相当甚至提升，首令牌生成时间最多减少6.68倍。
            arXiv:2505.10951v1 Announce Type: new 
Abstract: Graph-based retrieval-augmented generation (RAG) enables large language models (LLMs) to incorporate structured knowledge via graph retrieval as contextual input, enhancing more accurate and context-aware reasoning. We observe that for different queries, it could retrieve similar subgraphs as prompts, and thus we propose SubGCache, which aims to reduce inference latency by reusing computation across queries with similar structural prompts (i.e., subgraphs). Specifically, SubGCache clusters queries based on subgraph embeddings, constructs a representative subgraph for each cluster, and pre-computes the key-value (KV) cache of the representative subgraph. For each query with its retrieved subgraph within a cluster, it reuses the pre-computed KV cache of the representative subgraph of the cluster without computing the KV tensors again for saving computation. Experiments on two new datasets across multiple LLM backbones and graph-based RAG frameworks demonstrate that SubGCache consistently reduces inference latency with comparable and even improved generation quality, achieving up to 6.68$\times$ reduction in time-to-first-token (TTFT).
        ]]></description>
    </item>
    <item>
        <title>Relational Graph Transformer</title>
        <link>https://arxiv.org/abs/2505.10960</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.10960v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico L\'opez, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec</dc:creator>
        <description><![CDATA[
            背景：关系深度学习将多表关系数据表示为异质时态图来构建预测模型，但常用图神经网络在捕捉关系数据复杂结构模式和长程依赖上有局限，现有图变换器用于关系实体图也面临挑战。方法：提出关系图变换器RelGT，采用多元素分词策略，将节点分解为五组件，结合子图局部注意力和可学习质心全局注意力。效果：在RelBench基准21个任务中，RelGT始终与GNN基线持平或最高超越18%。 
            arXiv:2505.10960v1 Announce Type: new 
Abstract: Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.
        ]]></description>
    </item>
    <item>
        <title>WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?</title>
        <link>https://arxiv.org/abs/2505.11015</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11015v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>An-Lan Wang, Jingqun Tang, Liao Lei, Hao Feng, Qi Liu, Xiang Fei, Jinghui Lu, Han Wang, Weiwei Liu, Hao Liu, Yuliang Liu, Xiang Bai, Can Huang</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型提升了文档理解能力，但现有基准不能反映现实场景挑战。方法：本文推出首个自然环境文档理解评估基准WildDoc，纳入多种真实文档图像，从已有基准获取文档源便于对比，且每份文档在不同条件下拍摄四次。效果：在WildDoc上评估先进MLLMs，发现相比传统基准模型性能大幅下降、鲁棒性不足，凸显现实文档理解的独特挑战。
            arXiv:2505.11015v1 Announce Type: new 
Abstract: The rapid advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced capabilities in Document Understanding. However, prevailing benchmarks like DocVQA and ChartQA predominantly comprise \textit{scanned or digital} documents, inadequately reflecting the intricate challenges posed by diverse real-world scenarios, such as variable illumination and physical distortions. This paper introduces WildDoc, the inaugural benchmark designed specifically for assessing document understanding in natural environments. WildDoc incorporates a diverse set of manually captured document images reflecting real-world conditions and leverages document sources from established benchmarks to facilitate comprehensive comparisons with digital or scanned documents. Further, to rigorously evaluate model robustness, each document is captured four times under different conditions. Evaluations of state-of-the-art MLLMs on WildDoc expose substantial performance declines and underscore the models' inadequate robustness compared to traditional benchmarks, highlighting the unique challenges posed by real-world document understanding. Our project homepage is available at https://bytedance.github.io/WildDoc.
        ]]></description>
    </item>
    <item>
        <title>Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting</title>
        <link>https://arxiv.org/abs/2505.11017</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11017v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wenjie Ou, Zhishuo Zhao, Dongyue Guo, Yi Lin</dc:creator>
        <description><![CDATA[
            时间序列预测在多领域至关重要，现有基于大语言模型（LLM）的方法将其作为黑盒编码器，忽视时间序列短期局部变化，未充分利用分层表示。为此提出Logo - LLM框架，从预训练LLM不同层显式提取和建模多尺度时间特征，发现浅层捕捉局部动态、深层编码全局趋势，还引入轻量级模块跨层对齐和整合特征。大量实验表明，该框架在多基准测试中表现出色，少样本和零样本泛化能力强，计算开销低。
            arXiv:2505.11017v1 Announce Type: new 
Abstract: Time series forecasting is critical across multiple domains, where time series data exhibits both local patterns and global dependencies. While Transformer-based methods effectively capture global dependencies, they often overlook short-term local variations in time series. Recent methods that adapt large language models (LLMs) into time series forecasting inherit this limitation by treating LLMs as black-box encoders, relying solely on the final-layer output and underutilizing hierarchical representations. To address this limitation, we propose Logo-LLM, a novel LLM-based framework that explicitly extracts and models multi-scale temporal features from different layers of a pre-trained LLM. Through empirical analysis, we show that shallow layers of LLMs capture local dynamics in time series, while deeper layers encode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and Global-Mixer modules to align and integrate features with the temporal input across layers. Extensive experiments demonstrate that Logo-LLM achieves superior performance across diverse benchmarks, with strong generalization in few-shot and zero-shot settings while maintaining low computational overhead.
        ]]></description>
    </item>
    <item>
        <title>Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs</title>
        <link>https://arxiv.org/abs/2505.11023</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11023v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kutalm{\i}\c{s} Co\c{s}kun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan L\"udtke, Martin Becker</dc:creator>
        <description><![CDATA[
            背景：在生物医学等复杂低数据领域，将背景知识（BK）图融入图机器学习有前景，但BK实际贡献及不完美知识的影响不明。方法：以癌症亚型分类任务研究BK作用，引入评估框架，用合成设置和扰动模拟测试模型鲁棒性。效果：发现使用BK的先进GNN不比线性回归等无信息模型好，BK图受严重扰动时性能变化不大，表明需仔细对齐GNN架构与BK特征以提升性能。
            arXiv:2505.11023v1 Announce Type: new 
Abstract: In complex and low-data domains such as biomedical research, incorporating background knowledge (BK) graphs, such as protein-protein interaction (PPI) networks, into graph-based machine learning pipelines is a promising research direction. However, while BK is often assumed to improve model performance, its actual contribution and the impact of imperfect knowledge remain poorly understood. In this work, we investigate the role of BK in an important real-world task: cancer subtype classification. Surprisingly, we find that (i) state-of-the-art GNNs using BK perform no better than uninformed models like linear regression, and (ii) their performance remains largely unchanged even when the BK graph is heavily perturbed. To understand these unexpected results, we introduce an evaluation framework, which employs (i) a synthetic setting where the BK is clearly informative and (ii) a set of perturbations that simulate various imperfections in BK graphs. With this, we test the robustness of BK-aware models in both synthetic and real-world biomedical settings. Our findings reveal that careful alignment of GNN architectures and BK characteristics is necessary but holds the potential for significant performance improvements.
        ]]></description>
    </item>
    <item>
        <title>OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning</title>
        <link>https://arxiv.org/abs/2505.11031</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11031v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiao Zhang, Huiyuan Lai, Qianru Meng, Johan Bos</dc:creator>
        <description><![CDATA[
            背景：大语言模型在自然语言处理任务表现出色，但处理结构化符号知识能力待探索。方法：提出大语言模型本体能力分类法，引入OntoURL基准，基于分类法从理解、推理和学习三个维度，通过15个任务、58981个问题，对来自8个领域40个本体进行系统评估。效果：对20个开源大模型实验显示，不同模型、任务和领域表现差异大，当前大模型理解本体知识较熟练，但推理和学习任务有明显不足。
            arXiv:2505.11031v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing tasks, yet their ability to process structured symbolic knowledge remains underexplored. To address this gap, we propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the first comprehensive benchmark designed to systematically evaluate LLMs' proficiency in handling ontologies -- formal, symbolic representations of domain knowledge through concepts, relationships, and instances. Based on the proposed taxonomy, OntoURL systematically assesses three dimensions: understanding, reasoning, and learning through 15 distinct tasks comprising 58,981 questions derived from 40 ontologies across 8 domains. Experiments with 20 open-source LLMs reveal significant performance differences across models, tasks, and domains, with current LLMs showing proficiency in understanding ontological knowledge but substantial weaknesses in reasoning and learning tasks. These findings highlight fundamental limitations in LLMs' capability to process symbolic knowledge and establish OntoURL as a critical benchmark for advancing the integration of LLMs with formal knowledge representations.
        ]]></description>
    </item>
    <item>
        <title>Halting Recurrent GNNs and the Graded $\mu$-Calculus</title>
        <link>https://arxiv.org/abs/2505.11050</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11050v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jeroen Bollen, Jan Van den Bussche, Stijn Vansummeren, Jonni Virtema</dc:creator>
        <description><![CDATA[
            背景：图神经网络（GNN）处理图结构数据，其表达能力与特定逻辑相关，现有循环GNN存在需给定图大小或无终止保证的问题。方法：提出循环GNN的停止机制，开发分级μ - 演算的近似语义和与图大小无关的计数算法，并在停止循环GNN上实现该算法。效果：证明停止模型能表达分级模态μ - 演算中可定义的所有节点分类器，且在特定逻辑下，循环GNN只能表达分级模态μ - 演算中可定义的节点分类器。
            arXiv:2505.11050v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) are a class of machine-learning models that operate on graph-structured data. Their expressive power is intimately related to logics that are invariant under graded bisimilarity. Current proposals for recurrent GNNs either assume that the graph size is given to the model, or suffer from a lack of termination guarantees. In this paper, we propose a halting mechanism for recurrent GNNs. We prove that our halting model can express all node classifiers definable in graded modal mu-calculus, even for the standard GNN variant that is oblivious to the graph size. A recent breakthrough in the study of the expressivity of graded modal mu-calculus in the finite suggests that conversely, restricted to node classifiers definable in monadic second-order logic, recurrent GNNs can express only node classifiers definable in graded modal mu-calculus. To prove our main result, we develop a new approximate semantics for graded mu-calculus, which we believe to be of independent interest. We leverage this new semantics into a new model-checking algorithm, called the counting algorithm, which is oblivious to the graph size. In a final step we show that the counting algorithm can be implemented on a halting recurrent GNN.
        ]]></description>
    </item>
    <item>
        <title>GraphOracle: A Foundation Model for Knowledge Graph Reasoning</title>
        <link>https://arxiv.org/abs/2505.11125</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11125v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Enjun Du, Siyi Liu, Yongqi Zhang</dc:creator>
        <description><![CDATA[
            背景：基础模型在多领域表现出色，但知识图谱因动态性和跨领域推理需求，开发类似模型面临挑战。方法：提出以关系为中心的基础模型GraphOracle，将知识图谱转换为关系依赖图，用更少边显式编码组合模式，开发查询依赖注意力机制学习关系和实体的归纳表示，在多样知识图谱上预训练并微调。效果：在31个基准测试中表现达最优，与最强基线相比，预测性能最高提升35%。
            arXiv:2505.11125v1 Announce Type: new 
Abstract: Foundation models have demonstrated remarkable capabilities across various domains, but developing analogous models for knowledge graphs presents unique challenges due to their dynamic nature and the need for cross-domain reasoning. To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a relation-centric foundation model that unifies reasoning across knowledge graphs by converting them into Relation-Dependency Graphs (RDG), explicitly encoding compositional patterns with fewer edges than prior methods. A query-dependent attention mechanism is further developed to learn inductive representations for both relations and entities. Pre-training on diverse knowledge graphs, followed by minutes-level fine-tuning, enables effective generalization to unseen entities, relations, and entire graphs. Through comprehensive experiments on 31 diverse benchmarks spanning transductive, inductive, and cross-domain settings, we demonstrate consistent state-of-the-art performance with minimal adaptation, improving the prediction performance by up to 35\% compared to the strongest baselines.
        ]]></description>
    </item>
    <item>
        <title>Scaling Reasoning can Improve Factuality in Large Language Models</title>
        <link>https://arxiv.org/abs/2505.11140</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11140v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mike Zhang, Johannes Bjerva, Russa Biswas</dc:creator>
        <description><![CDATA[
            背景：此前研究表明长推理过程和额外计算资源可提升大语言模型在数学推理任务中的表现，但长推理链能否提升事实准确性存疑。方法：在复杂开放域问答场景中，从先进推理模型提取推理轨迹，对不同规模模型微调，将知识图的路径形式事实信息融入推理轨迹。效果：小规模推理模型单次运行时事实准确性有明显提升；增加测试时计算和令牌预算，事实准确性持续提升2 - 8%，证实测试时扩展对开放域问答推理准确性的有效性。
            arXiv:2505.11140v1 Announce Type: new 
Abstract: Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.
        ]]></description>
    </item>
    <item>
        <title>Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans</title>
        <link>https://arxiv.org/abs/2505.11141</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11141v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yansheng Qiu, Li Xiao, Zhaopan Xu, Pengfei Zhou, Zheng Wang, Kaipeng Zhang</dc:creator>
        <description><![CDATA[
            背景：实现通用人工智能目标需模仿并超越人类，含类人推理能力的大语言模型被融入多模态大模型，但在推理任务上是否达人类水平尚不明确。方法：提出Human - Aligned Bench基准，收集9794个仅依赖上下文推理的多模态问题，涵盖四种题型，且每题配有人类成功率和易错选项。效果：实验显示当前多模态大模型在多模态推理表现与人类有显著差异，为下一代模型发展提供见解。
            arXiv:2505.11141v1 Announce Type: new 
Abstract: The goal of achieving Artificial General Intelligence (AGI) is to imitate humans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have demonstrated that large language models (LLMs) with human-like reasoning capabilities exhibit exceptional performance and are being gradually integrated into multimodal large language models (MLLMs). However, whether these models possess capabilities comparable to humans in handling reasoning tasks remains unclear at present. In this paper, we propose Human-Aligned Bench, a benchmark for fine-grained alignment of multimodal reasoning with human performance. Specifically, we collected 9,794 multimodal questions that solely rely on contextual reasoning, including bilingual (Chinese and English) multimodal questions and pure text-based questions, encompassing four question types: visual reasoning, definition judgment, analogical reasoning, and logical judgment. More importantly, each question is accompanied by human success rates and options that humans are prone to choosing incorrectly. Extensive experiments on the Human-Aligned Bench reveal notable differences between the performance of current MLLMs in multimodal reasoning and human performance. The findings on our benchmark provide insights into the development of the next-generation models.
        ]]></description>
    </item>
    <item>
        <title>GeoMM: On Geodesic Perspective for Multi-modal Learning</title>
        <link>https://arxiv.org/abs/2505.11216</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11216v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shibin Mei, Hang Wang, Bingbing Ni</dc:creator>
        <description><![CDATA[
            背景：在多模态学习的非线性流形中，传统距离度量难以区分语义不同但相似度高的样本。方法：首次引入测地距离作为多模态学习的新度量，构建图结构表示样本邻接关系，用最短路径算法获取测地距离，还提出分层图结构和增量更新策略以高效计算。效果：在多个下游任务的大量实验验证了该方法的有效性，能捕捉样本复杂关系，提升多模态学习模型性能。
            arXiv:2505.11216v1 Announce Type: new 
Abstract: Geodesic distance serves as a reliable means of measuring distance in nonlinear spaces, and such nonlinear manifolds are prevalent in the current multimodal learning. In these scenarios, some samples may exhibit high similarity, yet they convey different semantics, making traditional distance metrics inadequate for distinguishing between positive and negative samples. This paper introduces geodesic distance as a novel distance metric in multi-modal learning for the first time, to mine correlations between samples, aiming to address the limitations of common distance metric. Our approach incorporates a comprehensive series of strategies to adapt geodesic distance for the current multimodal learning. Specifically, we construct a graph structure to represent the adjacency relationships among samples by thresholding distances between them and then apply the shortest-path algorithm to obtain geodesic distance within this graph. To facilitate efficient computation, we further propose a hierarchical graph structure through clustering and combined with incremental update strategies for dynamic status updates. Extensive experiments across various downstream tasks validate the effectiveness of our proposed method, demonstrating its capability to capture complex relationships between samples and improve the performance of multimodal learning models.
        ]]></description>
    </item>
    <item>
        <title>A Set-Sequence Model for Time Series</title>
        <link>https://arxiv.org/abs/2505.11243</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11243v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Elliot L. Epstein, Apaar Sadhwani, Kay Giesecke</dc:creator>
        <description><![CDATA[
            背景：金融预测问题中，个体行为受多种因素及潜在截面效应影响，传统方法靠手工特征捕捉潜在效应。方法：提出无需手工特征的集 - 序列模型，集模型学习各时期共享截面摘要，序列模型独立处理各单元的摘要增强时间序列以预测结果，两部分在训练时联合学习。效果：该模型利用截面集特性，计算高效，能灵活使用现有序列模型。在股票回报预测和抵押贷款行为任务中显著优于基准模型。
            arXiv:2505.11243v1 Announce Type: new 
Abstract: In many financial prediction problems, the behavior of individual units (such as loans, bonds, or stocks) is influenced by observable unit-level factors and macroeconomic variables, as well as by latent cross-sectional effects. Traditional approaches attempt to capture these latent effects via handcrafted summary features. We propose a Set-Sequence model that eliminates the need for handcrafted features. The Set model first learns a shared cross-sectional summary at each period. The Sequence model then ingests the summary-augmented time series for each unit independently to predict its outcome. Both components are learned jointly over arbitrary sets sampled during training. Our approach harnesses the set nature of the cross-section and is computationally efficient, generating set summaries in linear time relative to the number of units. It is also flexible, allowing the use of existing sequence models and accommodating a variable number of units at inference. Empirical evaluations demonstrate that our Set-Sequence model significantly outperforms benchmarks on stock return prediction and mortgage behavior tasks. Code will be released.
        ]]></description>
    </item>
    <item>
        <title>Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline</title>
        <link>https://arxiv.org/abs/2505.11250</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11250v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, Bin Yang</dc:creator>
        <description><![CDATA[
            不规则多变量时间序列（IMTS）预测在医疗、气候等领域至关重要，但因序列不规则、数据缺失及现有方法复杂资源消耗大，实现准确实用预测颇具挑战。为此，研究提出通用框架APN。设计时间感知补丁聚合（TAPA）模块实现自适应补丁，以通道独立方式将不规则序列转化为高质量正则化表示，用简单查询模块整合历史信息，最后用浅多层感知机预测。在多真实数据集实验显示，APN在效率和准确性上均超现有最优方法。
            arXiv:2505.11250v1 Announce Type: new 
Abstract: The forecasting of irregular multivariate time series (IMTS) is crucial in key areas such as healthcare, biomechanics, climate science, and astronomy. However, achieving accurate and practical predictions is challenging due to two main factors. First, the inherent irregularity and data missingness in irregular time series make modeling difficult. Second, most existing methods are typically complex and resource-intensive. In this study, we propose a general framework called APN to address these challenges. Specifically, we design a novel Time-Aware Patch Aggregation (TAPA) module that achieves adaptive patching. By learning dynamically adjustable patch boundaries and a time-aware weighted averaging strategy, TAPA transforms the original irregular sequences into high-quality, regularized representations in a channel-independent manner. Additionally, we use a simple query module to effectively integrate historical information while maintaining the model's efficiency. Finally, predictions are made by a shallow MLP. Experimental results on multiple real-world datasets show that APN outperforms existing state-of-the-art methods in both efficiency and accuracy.
        ]]></description>
    </item>
    <item>
        <title>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs</title>
        <link>https://arxiv.org/abs/2505.11277</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11277v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang</dc:creator>
        <description><![CDATA[
            背景：大语言模型推理能力强，但受知识储备限制，现有检索增强推理方法常获取无关或噪声信息，影响推理准确性。方法：提出AutoRefine强化学习后训练框架，采用“思考时搜索并提炼”范式，在连续搜索调用间增加知识提炼步骤，结合定制检索奖励与答案正确性奖励。效果：在单跳和多跳问答基准测试中显著优于现有方法，尤其在复杂多跳推理场景，能更频繁高质量搜索并有效整合证据。
            arXiv:2505.11277v1 Announce Type: new 
Abstract: Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.
        ]]></description>
    </item>
    <item>
        <title>Graph Representational Learning: When Does More Expressivity Hurt Generalization?</title>
        <link>https://arxiv.org/abs/2505.11298</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11298v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Sohir Maskey, Raffaele Paolino, Fabian Jogl, Gitta Kutyniok, Johannes F. Lutzeyer</dc:creator>
        <description><![CDATA[
            背景：图神经网络（GNNs）是处理结构化数据的有力工具，但表达能力与预测性能的关系尚不明确。方法：引入一系列预度量来捕捉图之间不同程度的结构相似性，并将其与泛化能力及表达性GNN的性能相关联，在图标签与结构特征相关的设定下推导泛化边界。效果：研究表明，除非通过足够大的训练集或缩小训练与测试图之间的距离来平衡复杂度，否则表达性更强的GNN泛化效果可能更差，理论结果得到了实证支持。
            arXiv:2505.11298v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) are powerful tools for learning on structured data, yet the relationship between their expressivity and predictive performance remains unclear. We introduce a family of premetrics that capture different degrees of structural similarity between graphs and relate these similarities to generalization, and consequently, the performance of expressive GNNs. By considering a setting where graph labels are correlated with structural features, we derive generalization bounds that depend on the distance between training and test graphs, model complexity, and training set size. These bounds reveal that more expressive GNNs may generalize worse unless their increased complexity is balanced by a sufficiently large training set or reduced distance between training and test graphs. Our findings relate expressivity and generalization, offering theoretical insights supported by empirical results.
        ]]></description>
    </item>
    <item>
        <title>Effective Probabilistic Time Series Forecasting with Fourier Adaptive Noise-Separated Diffusion</title>
        <link>https://arxiv.org/abs/2505.11306</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11306v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xinyan Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu</dc:creator>
        <description><![CDATA[
            背景：时间序列预测需有效概率框架。方法：提出FALDA概率框架，引入DMRR统一扩散概率回归方法，利用傅里叶分解结合特定组件架构，用条件扩散模型估计未来噪声项，提出轻量级去噪器DEMA增强去噪性能。效果：通过数学分析和实验验证，FALDA有效降低认知不确定性，专注于偶然不确定性。在六个真实基准测试中，长时预测多数数据集上超现有方法，计算效率提升且不损精度，相比SOTA点预测方法，整体性能最高提升9%。
            arXiv:2505.11306v1 Announce Type: new 
Abstract: We propose the Fourier Adaptive Lite Diffusion Architecture (FALDA), a novel probabilistic framework for time series forecasting. First, we introduce the Diffusion Model for Residual Regression (DMRR) framework, which unifies diffusion-based probabilistic regression methods. Within this framework, FALDA leverages Fourier-based decomposition to incorporate a component-specific architecture, enabling tailored modeling of individual temporal components. A conditional diffusion model is utilized to estimate the future noise term, while our proposed lightweight denoiser, DEMA (Decomposition MLP with AdaLN), conditions on the historical noise term to enhance denoising performance. Through mathematical analysis and empirical validation, we demonstrate that FALDA effectively reduces epistemic uncertainty, allowing probabilistic learning to primarily focus on aleatoric uncertainty. Experiments on six real-world benchmarks demonstrate that FALDA consistently outperforms existing probabilistic forecasting approaches across most datasets for long-term time series forecasting while achieving enhanced computational efficiency without compromising accuracy. Notably, FALDA also achieves superior overall performance compared to state-of-the-art (SOTA) point forecasting approaches, with improvements of up to 9%.
        ]]></description>
    </item>
    <item>
        <title>Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning</title>
        <link>https://arxiv.org/abs/2505.11349</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11349v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuanzhao Zhang, William Gilpin</dc:creator>
        <description><![CDATA[
            背景：科学机器学习中的时间序列基础模型有预测物理系统的能力，但可能未形成对物理原理的有效表征。方法：研究指出基础模型常通过‘上下文模仿’（直接从上下文中复制）进行预测，并构建了简单的直接上下文模仿模型。效果：该模型在预测多种动力系统时得分高于最先进的时间序列基础模型，且计算成本极低。此研究为未来时间序列基础模型提供了简单却难超越的基线。
            arXiv:2505.11349v1 Announce Type: new 
Abstract: Recently-developed time series foundation models for scientific machine learning exhibit emergent abilities to predict physical systems. These abilities include zero-shot forecasting, in which a model forecasts future states of a system given only a short trajectory as context. Here, we show that foundation models applied to physical systems can give accurate predictions, but that they fail to develop meaningful representations of the underlying physics. Instead, foundation models often forecast by context parroting, a simple zero-shot forecasting strategy that copies directly from the context. As a result, a naive direct context parroting model scores higher than state-of-the-art time-series foundation models on predicting a diverse range of dynamical systems, at a tiny fraction of the computational cost. We draw a parallel between context parroting and induction heads, which explains why large language models trained on text can be repurposed for time series forecasting. Our dynamical systems perspective also ties the scaling between forecast accuracy and context length to the fractal dimension of the attractor, providing insight into the previously observed in-context neural scaling laws. Context parroting thus serves as a simple but tough-to-beat baseline for future time-series foundation models and can help identify in-context learning strategies beyond parroting.
        ]]></description>
    </item>
    <item>
        <title>Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner</title>
        <link>https://arxiv.org/abs/2505.11404</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11404v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu</dc:creator>
        <description><![CDATA[
            背景：当前病理领域特定的视觉语言模型在诊断准确性和推理合理性上存在局限，主要因现有数据集缺乏深度和结构化诊断范式。方法：利用病理教科书和专家构建高质量、面向推理的数据集，提出基于多模态强化学习的病理推理器Patho - R1，经三阶段训练，还提出PathoCLIP评估数据集对齐质量。效果：PathoCLIP和Patho - R1在零样本分类、跨模态检索等多项病理相关任务中表现出色。
            arXiv:2505.11404v1 Announce Type: new 
Abstract: Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.
        ]]></description>
    </item>
    <item>
        <title>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs</title>
        <link>https://arxiv.org/abs/2505.11423</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11423v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal</dc:creator>
        <description><![CDATA[
            背景：推理增强的大语言模型在复杂推理任务上表现出色，但显式思维链推理对指令遵循准确性的影响未被充分研究。方法：在两个基准上评估15个模型，通过案例研究和注意力分析，识别推理的利弊，提出约束注意力指标，还引入四种策略减轻推理负面影响。效果：研究发现思维链推理会降低指令遵循准确性，选择性推理策略，尤其是分类器选择性推理，能大幅挽回损失的性能。
            arXiv:2505.11423v1 Announce Type: new 
Abstract: Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.
        ]]></description>
    </item>
    <item>
        <title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
        <link>https://arxiv.org/abs/2505.11484</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11484v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao</dc:creator>
        <description><![CDATA[
            背景：Test - Time Scaling（TTS）可在不改变模型参数下提升推理性能，现有TTS在离散标记空间操作，连续潜在空间推理虽有提升但探索路径受限。方法：提出SoftCoT++，通过多个专门初始标记扰动潜在思维，并应用对比学习促进软思维表示的多样性。效果：在五个推理基准和两种大语言模型架构实验表明，SoftCoT++显著提升SoftCoT性能，优于带自一致性扩展的SoftCoT，且与传统扩展技术兼容性强。代码见https://github.com/xuyige/SoftCoT 。
            arXiv:2505.11484v1 Announce Type: new 
Abstract: Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.
        ]]></description>
    </item>
    <item>
        <title>LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios</title>
        <link>https://arxiv.org/abs/2505.11247</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11247v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mingxing Peng, Yuting Xie, Xusen Guo, Ruoyu Yao, Hai Yang, Jun Ma</dc:creator>
        <description><![CDATA[
            背景：评估自动驾驶系统安全性需在关键场景中进行，但此类场景数据难收集，现有方法可控性有限且需专业知识。方法：提出LD - Scene框架，将大语言模型（LLMs）与潜在扩散模型（LDMs）结合，通过自然语言实现用户可控的对抗场景生成，含捕捉轨迹分布的LDM和将用户查询转化为损失函数的LLM引导模块，还集成思维链代码生成器和代码调试器。效果：在nuScenes数据集上达先进水平，能生成多样有效场景，可精细控制对抗行为。
            arXiv:2505.11247v1 Announce Type: cross 
Abstract: Ensuring the safety and robustness of autonomous driving systems necessitates a comprehensive evaluation in safety-critical scenarios. However, these safety-critical scenarios are rare and difficult to collect from real-world driving data, posing significant challenges to effectively assessing the performance of autonomous vehicles. Typical existing methods often suffer from limited controllability and lack user-friendliness, as extensive expert knowledge is essentially required. To address these challenges, we propose LD-Scene, a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) for user-controllable adversarial scenario generation through natural language. Our approach comprises an LDM that captures realistic driving trajectory distributions and an LLM-based guidance module that translates user queries into adversarial loss functions, facilitating the generation of scenarios aligned with user queries. The guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator and an LLM-based code debugger, enhancing the controllability and robustness in generating guidance functions. Extensive experiments conducted on the nuScenes dataset demonstrate that LD-Scene achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios. Furthermore, our framework provides fine-grained control over adversarial behaviors, thereby facilitating more effective testing tailored to specific driving scenarios.
        ]]></description>
    </item>
    <item>
        <title>Degree-Conscious Spiking Graph for Cross-Domain Adaptation</title>
        <link>https://arxiv.org/abs/2410.06883</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.06883v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yingxu Wang, Mengzhu Wang, Siwei Liu, Houcheng Su, Nan Yin, James Kwok</dc:creator>
        <description><![CDATA[
            背景：尖峰图网络（SGNs）在图分类中潜力大，但现有SGNs局限于分布内场景，难应对分布偏移。方法：提出SGNs的跨域适应问题，引入Degree - Consicious Spiking Graph for Cross - Domain Adaptation框架，包含度感知尖峰表示模块、时域分布对齐及提取一致预测生成伪标签三部分，还建立了泛化界。效果：在基准数据集上实验表明，该方法在分类准确率和能源效率上均超现有方法。 
            arXiv:2410.06883v4 Announce Type: replace 
Abstract: Spiking Graph Networks (SGNs) have demonstrated significant potential in graph classification by emulating brain-inspired neural dynamics to achieve energy-efficient computation. However, existing SGNs are generally constrained to in-distribution scenarios and struggle with distribution shifts. In this paper, we first propose the domain adaptation problem in SGNs, and introduce a novel framework named Degree-Consicious Spiking Graph for Cross-Domain Adaptation. DeSGraDA enhances generalization across domains with three key components. First, we introduce the degree-conscious spiking representation module by adapting spike thresholds based on node degrees, enabling more expressive and structure-aware signal encoding. Then, we perform temporal distribution alignment by adversarially matching membrane potentials between domains, ensuring effective performance under domain shift while preserving energy efficiency. Additionally, we extract consistent predictions across two spaces to create reliable pseudo-labels, effectively leveraging unlabeled data to enhance graph classification performance. Furthermore, we establish the first generalization bound for SGDA, providing theoretical insights into its adaptation performance. Extensive experiments on benchmark datasets validate that DeSGraDA consistently outperforms state-of-the-art methods in both classification accuracy and energy efficiency.
        ]]></description>
    </item>
    <item>
        <title>UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction</title>
        <link>https://arxiv.org/abs/2411.07019</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.07019v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhiqiang Liu, Yin Hua, Mingyang Chen, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang</dc:creator>
        <description><![CDATA[
            背景：超关系事实、时间事实和嵌套事实等非三元组事实表示受关注，但现有链接预测模型难实现分层事实建模和模块泛化。方法：提出统一分层表示学习框架UniHR，包含统一分层数据表示模块HiDR和统一分层结构学习模块HiSL，HiDR将多种知识图谱统一为基于三元组的表示，HiSL结合事实内和事实间消息传递。效果：实验证明了UniHR的有效性和统一表示的潜力。
            arXiv:2411.07019v3 Announce Type: replace 
Abstract: Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, constrained by complex fact representation forms, existing link prediction models for beyond-triple facts have difficulty achieving hierarchical fact modeling and generalizing the modules for one specific facts to other fact types. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Empirical results demonstrate the effectiveness of UniHR and highlight the strong potential of unified representations. Code and data are available at https://github.com/Lza12a/UniHR.
        ]]></description>
    </item>
    <item>
        <title>NeuroLifting: Neural Inference on Markov Random Fields at Scale</title>
        <link>https://arxiv.org/abs/2411.18954</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.18954v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yaomin Wang, Chaolong Ying, Xiaodong Luo, Tianshu Yu</dc:creator>
        <description><![CDATA[
            大规模马尔可夫随机场（MRF）推理是关键且具挑战性的任务，传统近似或精确方法难在效率与解质量间达最优平衡。本文提出NeuroLifting技术，利用图神经网络（GNN）对MRF决策变量重新参数化，以标准梯度下降优化。该方法将传统提升技术拓展到非参数神经网络框架，借助神经网络平滑损失面实现高效并行优化。实验表明，在中等规模上解质量接近精确求解器Toulbar2，超现有近似方法；大规模下解质量超所有基线，且计算复杂度线性增长。
            arXiv:2411.18954v2 Announce Type: replace 
Abstract: Inference in large-scale Markov Random Fields (MRFs) is a critical yet challenging task, traditionally approached through approximate methods like belief propagation and mean field, or exact methods such as the Toulbar2 solver. These strategies often fail to strike an optimal balance between efficiency and solution quality, particularly as the problem scale increases. This paper introduces NeuroLifting, a novel technique that leverages Graph Neural Networks (GNNs) to reparameterize decision variables in MRFs, facilitating the use of standard gradient descent optimization. By extending traditional lifting techniques into a non-parametric neural network framework, NeuroLifting benefits from the smooth loss landscape of neural networks, enabling efficient and parallelizable optimization. Empirical results demonstrate that, on moderate scales, NeuroLifting performs very close to the exact solver Toulbar2 in terms of solution quality, significantly surpassing existing approximate methods. Notably, on large-scale MRFs, NeuroLifting delivers superior solution quality against all baselines, as well as exhibiting linear computational complexity growth. This work presents a significant advancement in MRF inference, offering a scalable and effective solution for large-scale problems.
        ]]></description>
    </item>
    <item>
        <title>Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals</title>
        <link>https://arxiv.org/abs/2412.09758</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.09758v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yunfei Luo, Yuliang Chen, Asif Salekin, Tauhidur Rahman</dc:creator>
        <description><![CDATA[
            背景：时间序列基础模型在预测任务表现出色，但可穿戴传感数据模式和频带多变，构建可泛化表征是挑战。方法：提出首个多模态通用基础模型NormWear，设计通道感知注意力机制检测信号模式，在多种生理信号上预训练。效果：在11个公开数据集、18个应用场景表现出卓越泛化性，在零样本、部分样本和全样本设置下均优于竞争基线，适用于现实健康应用。
            arXiv:2412.09758v2 Announce Type: replace 
Abstract: Time-series foundation models excel at tasks like forecasting across diverse data types by leveraging informative waveform representations. Wearable sensing data, however, pose unique challenges due to their variability in patterns and frequency bands, especially for healthcare-related outcomes. The main obstacle lies in crafting generalizable representations that adapt efficiently across heterogeneous sensing configurations and applications. To address this, we propose NormWear, the first multi-modal and ubiquitous foundation model designed to extract generalized and informative representations from wearable sensing data. Specifically, we design a channel-aware attention mechanism with a shared special liaison [CLS] token to detect signal patterns in both intra-sensor and inter-sensors. This helps the model to extract more meaningful information considering both time series themselves and the relationships between input sensors. This helps the model to be widely compatible with various sensors settings. NormWear is pretrained on a diverse set of physiological signals, including PPG, ECG, EEG, GSR, and IMU, from various public datasets. Our model shows exceptional generalizability across 11 public wearable sensing datasets, spanning 18 applications in mental health, body state inference, vital sign estimation, and disease risk evaluation. It consistently outperforms competitive baselines under zero-shot, partial-shot, and full-shot settings, indicating broad applicability in real-world health applications.
        ]]></description>
    </item>
    <item>
        <title>Leveraging Large Language Models for Effective Label-free Node Classification in Text-Attributed Graphs</title>
        <link>https://arxiv.org/abs/2412.11983</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.11983v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Taiyan Zhang, Renchi Yang, Yurui Lai, Mingyu Yan, Xiaochun Ye, Dongrui Fan</dc:creator>
        <description><![CDATA[
            背景：图神经网络用于图数据节点分类依赖大量高质量标注数据，获取成本高。利用大语言模型零样本能力和知识进行节点标注存在需大量查询或标签噪声问题。方法：提出Locle，一种主动自训练框架，用图神经网络迭代识别“关键”样本，结合大语言模型和图神经网络提取信息伪标签辅助训练，含有效节点选择、“关键”节点识别、标签细化模块。效果：在五个基准数据集上显著优于现有方法，如在DBLP数据集上准确率提升8.08%，成本不到1美分。
            arXiv:2412.11983v3 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have become the preferred models for node classification in graph data due to their robust capabilities in integrating graph structures and attributes. However, these models heavily depend on a substantial amount of high-quality labeled data for training, which is often costly to obtain. With the rise of large language models (LLMs), a promising approach is to utilize their exceptional zero-shot capabilities and extensive knowledge for node labeling. Despite encouraging results, this approach either requires numerous queries to LLMs or suffers from reduced performance due to noisy labels generated by LLMs. To address these challenges, we introduce Locle, an active self-training framework that does Label-free node Classification with LLMs cost-Effectively. Locle iteratively identifies small sets of "critical" samples using GNNs and extracts informative pseudo-labels for them with both LLMs and GNNs, serving as additional supervision signals to enhance model training. Specifically, Locle comprises three key components: (i) an effective active node selection strategy for initial annotations; (ii) a careful sample selection scheme to identify "critical" nodes based on label disharmonicity and entropy; and (iii) a label refinement module that combines LLMs and GNNs with a rewired topology. Extensive experiments on five benchmark text-attributed graph datasets demonstrate that Locle significantly outperforms state-of-the-art methods under the same query budget to LLMs in terms of label-free node classification. Notably, on the DBLP dataset with 14.3k nodes, Locle achieves an 8.08% improvement in accuracy over the state-of-the-art at a cost of less than one cent. Our code is available at https://github.com/HKBU-LAGAS/Locle.
        ]]></description>
    </item>
    <item>
        <title>What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context</title>
        <link>https://arxiv.org/abs/2412.12632</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.12632v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Qing Wang, Yihao Huang, Yang Liu</dc:creator>
        <description><![CDATA[
            背景：将外部知识融入大语言模型可缓解其知识陈旧和幻觉问题，但外部知识常不完美，含无关或错误信息。方法：受刑事诉讼法证据链启发，提出LLMs偏好的知识应与问题相关且知识间相互支持，还提出自动证据链判别方法，并评估其在检索增强生成（RAG）中的效果。效果：在五个LLMs上测试表明，证据链能提高生成准确率、答案忠实度、对知识冲突的鲁棒性，还提升三种实际RAG场景下现有方法的性能。
            arXiv:2412.12632v2 Announce Type: replace 
Abstract: Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs. However, external knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses. This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and evaluate LLMs' effectiveness, faithfulness and robustness with CoE, including its application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs show CoE improves generation accuracy, answer faithfulness, robustness to knowledge conflicts, and boosts the performance of existing approaches in three practical RAG scenarios.
        ]]></description>
    </item>
    <item>
        <title>XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2412.15529</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.15529v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Qianren Mao, Yangyifei Luo, Qili Zhang, Yashuo Luo, Zhilong Cao, Jinlong Zhang, HanWen Hao, Zhijun Chen, Weifeng Jiang, Junnan Liu, Xiaolong Wang, Zhenting Huang, Zhixing Tan, Sun Jie, Bo Li, Xudong Liu, Richong Zhang, Jianxin Li</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）结合数据检索与大语言模型生成能力，但系统复杂度不断提升。方法：提出开源模块化代码库XRAG，将RAG基础组件分为预检索、检索、后检索和生成四个核心阶段，在重新配置的数据集上系统分析，制定实验方法和诊断测试协议剖析故障点并给出解决方案。效果：全面评估RAG系统中高级核心组件性能，为常见故障点优化提供见解。
            arXiv:2412.15529v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. As the complexity of RAG systems continues to escalate, we underscore the critical need to identify potential failure points in RAG systems. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed at bolstering the overall performance of these modules. Our work thoroughly evaluates the performance of advanced core components in RAG systems, providing insights into optimizations for prevalent failure points.
        ]]></description>
    </item>
    <item>
        <title>TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations</title>
        <link>https://arxiv.org/abs/2504.12721</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.12721v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yihang Lu, Yangyang Xu, Qitao Qing, Xianwei Meng</dc:creator>
        <description><![CDATA[
            背景：当前深度学习模型用于长期时间序列预测（LTSF）常依赖复杂设计，而简单模型表现更好。方法：本文梳理先进LTSF模型常用技术核心思想，提出基于高维信息压缩原理的TimeCapsule模型，将时间序列建模为3D张量，利用模式积捕捉多模式依赖并降维，还提出压缩表示域内的内部预测，并以JEPA监测预测表示学习。效果：在挑战性基准测试中，该模型展现出通用性，达到了当前最优性能。
            arXiv:2504.12721v2 Announce Type: replace 
Abstract: Recent deep learning models for Long-term Time Series Forecasting (LTSF) often emphasize complex, handcrafted designs, while simpler architectures like linear models or MLPs have often outperformed these intricate solutions. In this paper, we revisit and organize the core ideas behind several key techniques, such as redundancy reduction and multi-scale modeling, which are frequently employed in advanced LTSF models. Our goal is to streamline these ideas for more efficient deep learning utilization. To this end, we introduce TimeCapsule, a model built around the principle of high-dimensional information compression that unifies these techniques in a generalized yet simplified framework. Specifically, we model time series as a 3D tensor, incorporating temporal, variate, and level dimensions, and leverage mode production to capture multi-mode dependencies while achieving dimensionality compression. We propose an internal forecast within the compressed representation domain, supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the learning of predictive representations. Extensive experiments on challenging benchmarks demonstrate the versatility of our method, showing that TimeCapsule can achieve state-of-the-art performance.
        ]]></description>
    </item>
    <item>
        <title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
        <link>https://arxiv.org/abs/2505.00254</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.00254v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu</dc:creator>
        <description><![CDATA[
            背景：AI视频分析受限特定任务，视频语言模型（VLM）虽有潜力，但处理超长视频时因上下文窗口有限面临挑战。方法：提出AVAS系统，包含近实时构建事件知识图（EKG）对长视频高效索引，以及利用EKG的智能检索生成机制处理复杂查询。效果：在LVBench和VideoMME - Long上准确率达62.3%和64.1%，超现有VLM和视频RAG系统；在新基准AVAS - 100上准确率达75.8%，表现优异。
            arXiv:2505.00254v3 Announce Type: replace 
Abstract: AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVAS, a VLM-powered system designed for open-ended, advanced video analytics. AVAS incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an accuracy of 75.8%.
        ]]></description>
    </item>
    <item>
        <title>34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery</title>
        <link>https://arxiv.org/abs/2505.03049</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.03049v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yoel Zimmermann, Adib Bazgir, Alexander Al-Feghali, Mehrad Ansari, Joshua Bocarsly, L. Catherine Brinson, Yuan Chiang, Defne Circi, Min-Hsueh Chiu, Nathan Daelman, Matthew L. Evans, Abhijeet S. Gangan, Janine George, Hassan Harb, Ghazal Khalighinejad, Sartaaj Takrim Khan, Sascha Klawohn, Magdalena Lederbauer, Soroush Mahjoubi, Bernadette Mohr, Seyed Mohamad Moosavi, Aakash Naik, Aleyna Beste Ozhan, Dieter Plessers, Aritra Roy, Fabian Sch\"oppach, Philippe Schwaller, Carla Terboven, Katharina Ueltzen, Yue Wu, Shang Zhu, Jan Janssen, Calvin Li, Ian Foster, Ben Blaiszik</dc:creator>
        <description><![CDATA[
            背景：大语言模型（LLMs）正重塑材料科学与化学研究多方面。方法：通过回顾第二届全球材料科学与化学大语言模型应用黑客松的34个项目，探索LLMs在研究全生命周期的能力边界，这些项目涵盖分子和材料性质预测等七个研究领域。效果：LLMs可作为多功能预测模型等，通过添加推理、训练数据和新技术，开源和专有LLMs性能提升，尤其在低数据环境和跨学科研究中更有效，但融入科研工作流也带来新挑战。 
            arXiv:2505.03049v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more. Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows. To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 34 total projects developed during the second annual Large Language Model Hackathon for Applications in Materials Science and Chemistry, a global hybrid event. These projects spanned seven key research areas: (1) molecular and material property prediction, (2) molecular and material design, (3) automation and novel interfaces, (4) scientific communication and education, (5) research data management and automation, (6) hypothesis generation and evaluation, and (7) knowledge extraction and reasoning from the scientific literature. Collectively, these applications illustrate how LLMs serve as versatile predictive models, platforms for rapid prototyping of domain-specific tools, and much more. In particular, improvements in both open source and proprietary LLM performance through the addition of reasoning, additional training data, and new techniques have expanded effectiveness, particularly in low-data environments and interdisciplinary research. As LLMs continue to improve, their integration into scientific workflows presents both new opportunities and new challenges, requiring ongoing exploration, continued refinement, and further research to address reliability, interpretability, and reproducibility.
        ]]></description>
    </item>
    <item>
        <title>Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions</title>
        <link>https://arxiv.org/abs/2505.05755</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.05755v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Dhruvesh Patel, Aishwarya Sahoo, Avinash Amballa, Tahira Naseem, Tim G. J. Rudner, Andrew McCallum</dc:creator>
        <description><![CDATA[
            背景：自回归模型在序列生成任务取得成功，但处理复杂约束或乱序依赖序列较困难，掩码扩散模型虽能解决部分问题，但有不连贯和无法处理未知填充数量等局限。方法：提出插入语言模型（ILMs），可在序列任意位置插入标记，通过一次插入一个标记来表示标记间强依赖，采用定制网络参数化和简单去噪目标进行训练。效果：在常见规划任务上优于自回归和掩码扩散模型，无条件文本生成任务中与自回归模型表现相当且比掩码扩散模型更灵活。
            arXiv:2505.05755v2 Announce Type: replace 
Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one ``from left to right,'' have achieved significant success across a wide range of sequence generation tasks. However, they struggle to accurately represent sequences that require satisfying sophisticated constraints or whose sequential dependencies are better addressed by out-of-order generation. Masked Diffusion Models (MDMs) address some of these limitations, but the process of unmasking multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs cannot handle arbitrary infilling constraints when the number of tokens to be filled in is not known in advance. In this work, we introduce Insertion Language Models (ILMs), which learn to insert tokens at arbitrary positions in a sequence -- that is, they select jointly both the position and the vocabulary element to be inserted. By inserting tokens one at a time, ILMs can represent strong dependencies between tokens, and their ability to generate sequences in arbitrary order allows them to accurately model sequences where token dependencies do not follow a left-to-right sequential structure. To train ILMs, we propose a tailored network parameterization and use a simple denoising objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs and MDMs on common planning tasks. Furthermore, we show that ILMs outperform MDMs and perform on par with ARMs in an unconditional text generation task while offering greater flexibility than MDMs in arbitrary-length text infilling.
        ]]></description>
    </item>
    <item>
        <title>DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2505.07233</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.07233v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, Jiawei Han</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）系统结合大语言模型与外部知识检索，但确定重排器应选择的文档数量较难。方法：提出DynamicRAG框架，将重排器建模为通过强化学习优化的智能体，根据查询动态调整检索文档的顺序和数量，利用大语言模型输出质量作为奖励。效果：在七个知识密集型数据集上表现出色，在相同参数规模模型中取得了最先进的结果。代码等可在https://github.com/GasolSun36/DynamicRAG获取。
            arXiv:2505.07233v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker. Since irrelevant documents in RAG systems can mislead the generator, the reranker plays a vital role in refining retrieved documents to enhance generation quality and explainability. However, it is challenging to determine the appropriate number of documents ($k$) that the reranker should select: too few may result in missing critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results among models of same parameter sizes. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG.
        ]]></description>
    </item>
    <item>
        <title>Large Language Model Enhancers for Graph Neural Networks: An Analysis from the Perspective of Causal Mechanism Identification</title>
        <link>https://arxiv.org/abs/2505.08265</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.08265v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu</dc:creator>
        <description><![CDATA[
            背景：用大语言模型（LLMs）作为特征增强器优化节点表示，再输入图神经网络（GNNs），在图表示学习中有很大潜力，但该方法基本特性待深入研究。方法：基于互换干预法深入分析，构建有可控因果关系的合成图数据集，进行互换干预研究LLM增强器和GNN的深层特性，据此设计即插即用优化模块。效果：多数据集和模型实验验证了该模块的有效性。
            arXiv:2505.08265v2 Announce Type: replace 
Abstract: The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module.
        ]]></description>
    </item>
    <item>
        <title>Item-Language Model for Conversational Recommendation</title>
        <link>https://arxiv.org/abs/2406.02844</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.02844v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Li Yang, Anushya Subbiah, Hardik Patel, Judith Yue Li, Yanwei Song, Reza Mirghaderi, Vikram Aggarwal, Qifan Wang</dc:creator>
        <description><![CDATA[
            背景：大语言模型（LLMs）在多模态能力拓展方面取得进展，但将其应用于推荐系统存在问题，如未在推荐系统数据上训练、用户交互信号与自然语言模式不同、难针对不同用例训练多个LLMs。方法：提出物品语言模型（ILM），由物品编码器生成编码用户交互信号的文本对齐物品表示，以及能理解这些表示并保留预训练知识的冻结LLM。效果：实验证明了物品编码器中语言对齐和用户交互知识的重要性。
            arXiv:2406.02844v2 Announce Type: replace-cross 
Abstract: Large-language Models (LLMs) have been extremely successful at tasks like complex dialogue understanding, reasoning and coding due to their emergent abilities. These emergent abilities have been extended with multi-modality to include image, audio, and video capabilities. Recommender systems, on the other hand, have been critical for information seeking and item discovery needs. Recently, there have been attempts to apply LLMs for recommendations. One difficulty of current attempts is that the underlying LLM is usually not trained on the recommender system data, which largely contains user interaction signals and is often not publicly available. Another difficulty is user interaction signals often have a different pattern from natural language text, and it is currently unclear if the LLM training setup can learn more non-trivial knowledge from interaction signals compared with traditional recommender system methods. Finally, it is difficult to train multiple LLMs for different use-cases, and to retain the original language and reasoning abilities when learning from recommender system data. To address these three limitations, we propose an Item-Language Model (ILM), which is composed of an item encoder to produce text-aligned item representations that encode user interaction signals, and a frozen LLM that can understand those item representations with preserved pretrained knowledge. We conduct extensive experiments which demonstrate both the importance of the language-alignment and of user interaction knowledge in the item encoder.
        ]]></description>
    </item>
    <item>
        <title>Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese</title>
        <link>https://arxiv.org/abs/2505.11200</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11200v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xihuai Wang, Ziyi Zhao, Siyu Ren, Shao Zhang, Song Li, Xiaoyu Li, Ziwen Wang, Lin Qiu, Guanglu Wan, Xuezhi Cao, Xunliang Cai, Weinan Zhang</dc:creator>
        <description><![CDATA[
            背景：大语言模型提升了中文文本转语音（TTS）系统性能，但传统平均意见得分（MOS）评估有主观性等问题，现有评估数据集缺乏多维度设计。方法：提出音频图灵测试（ATT），构建多维度中文语料库ATT - Corpus及评估协议，让评估者判断语音是否像人类；用人类判断数据微调Qwen2 - Audio - Instruct得到自动评估工具Auto - ATT。效果：ATT能有效区分模型特定能力维度，Auto - ATT与人类评估高度一致，是快速可靠的评估工具。
            arXiv:2505.11200v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly improved text-to-speech (TTS) systems, enhancing control over speech style, naturalness, and emotional expression, which brings TTS Systems closer to human-level performance. Although the Mean Opinion Score (MOS) remains the standard for TTS System evaluation, it suffers from subjectivity, environmental inconsistencies, and limited interpretability. Existing evaluation datasets also lack a multi-dimensional design, often neglecting factors such as speaking styles, context diversity, and trap utterances, which is particularly evident in Chinese TTS evaluation. To address these challenges, we introduce the Audio Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on complex MOS scales or direct model comparisons, ATT asks evaluators to judge whether a voice sounds human. This simplification reduces rating bias and improves evaluation robustness. To further support rapid model development, we also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for automatic evaluation. Experimental results show that ATT effectively differentiates models across specific capability dimensions using its multi-dimensional design. Auto-ATT also demonstrates strong alignment with human evaluations, confirming its value as a fast and reliable assessment tool. The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face Collection (https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).
        ]]></description>
    </item>
    <item>
        <title>Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior</title>
        <link>https://arxiv.org/abs/2505.11315</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11315v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chin-Yun Yu, Marco A. Mart\'inez-Ram\'irez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, Gy\"orgy Fazekas</dc:creator>
        <description><![CDATA[
            背景：推理时优化的风格迁移（ST - ITO）方法在音频效果迁移中存在将所有配置等同看待、仅依赖嵌入空间而导致结果不真实或有偏差的问题。方法：引入源自人声预设数据集DiffVox的高斯先验到参数空间，优化过程等价于最大后验估计。效果：在MedleyDB数据集的人声效果迁移评估中，与多个基线方法相比，各项指标显著提升，参数均方误差最多降低33%，更匹配参考风格，主观评价也证实该方法优越性，尤其在数据有限时。
            arXiv:2505.11315v1 Announce Type: new 
Abstract: Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach for transferring the applied effects of a reference audio to a raw audio track. It optimises the effect parameters to minimise the distance between the style embeddings of the processed audio and the reference. However, this method treats all possible configurations equally and relies solely on the embedding space, which can lead to unrealistic or biased results. We address this pitfall by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox, over the parameter space. The resulting optimisation is equivalent to maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the MedleyDB dataset show significant improvements across metrics compared to baselines, including a blind audio effects estimator, nearest-neighbour approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter mean squared error by up to 33% and matches the reference style better. Subjective evaluations with 16 participants confirm our method's superiority, especially in limited data regimes. This work demonstrates how incorporating prior knowledge in inference time enhances audio effects transfer, paving the way for more effective and realistic audio processing systems.
        ]]></description>
    </item>
    <item>
        <title>Machine Learning Approaches to Vocal Register Classification in Contemporary Male Pop Music</title>
        <link>https://arxiv.org/abs/2505.11378</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11378v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Alexander Kim, Charlotte Botha</dc:creator>
        <description><![CDATA[
            背景：学习声乐技巧时，把握换声点附近的发声位置和音域是一大挑战，流行音乐中识别歌手使用的音域也较困难。方法：本文通过分析男性流行音乐音频信号的梅尔频谱图纹理特征，提出两种音域分类方法，还探讨了将模型集成到声乐分析工具中，并介绍了自动音域分析软件AVRA。效果：支持向量机（SVM）和卷积神经网络（CNN）模型均实现了音域的稳定分类，为更多声部和演唱风格的分类提供了可能。
            arXiv:2505.11378v1 Announce Type: new 
Abstract: For singers of all experience levels, one of the most daunting challenges in learning technical repertoire is navigating placement and vocal register in and around the passagio (passage between chest voice and head voice registers). Particularly in pop music, where a single artist may use a variety of timbre's and textures to achieve a desired quality, it can be difficult to identify what vocal register within the vocal range a singer is using. This paper presents two methods for classifying vocal registers in an audio signal of male pop music through the analysis of textural features of mel-spectrogram images. Additionally, we will discuss the practical integration of these models for vocal analysis tools, and introduce a concurrently developed software called AVRA which stands for Automatic Vocal Register Analysis. Our proposed methods achieved consistent classification of vocal register through both Support Vector Machine (SVM) and Convolutional Neural Network (CNN) models, which supports the promise of more robust classification possibilities across more voice types and genres of singing.
        ]]></description>
    </item>
    <item>
        <title>LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models</title>
        <link>https://arxiv.org/abs/2505.11391</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11391v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Danilo de Oliveira, Julius Richter, Tal Peer, Timo Germann</dc:creator>
        <description><![CDATA[
            背景：需要从无声视频记录中合成自然且可理解的语音。方法：提出LipDiffuser，这是一种用于唇语到语音生成的条件扩散模型，利用MP - ADM架构作为去噪器模型，通过MP - FiLM结合视觉特征和说话人嵌入来有效调节模型，再用神经声码器从生成的梅尔频谱图重建语音波形。效果：在LRS3和TCD - TIMIT数据集上评估显示，其在感知语音质量和说话人相似度上优于现有基线，在下游自动语音识别中也具竞争力，消融研究和跨数据集评估证实了方法有效性和泛化能力。
            arXiv:2505.11391v1 Announce Type: new 
Abstract: We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 and TCD-TIMIT demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition (ASR). These findings are also supported by a formal listening experiment. Extensive ablation studies and cross-dataset evaluation confirm the effectiveness and generalization capabilities of our approach.
        ]]></description>
    </item>
    <item>
        <title>LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors</title>
        <link>https://arxiv.org/abs/2505.11352</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.11352v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Rao Ma, Tongzhou Chen, Kartik Audhkhasi, Bhuvana Ramabhadran</dc:creator>
        <description><![CDATA[
            背景：现有结合预训练语音编码器和大语言模型（LLM）的方法易性能不佳或缺乏灵活性。方法：提出LegoSLM范式，利用自动语音识别（ASR）后验矩阵连接语音编码器和LLM，语音编码器生成基于LLM词汇表的连接主义时序分类（CTC）后验，重构伪音频嵌入并与文本嵌入拼接。效果：在ASR和语音翻译任务表现良好，连接USM和Gemma模型，在8个MLS测试集上平均字错误率（WERR）比USM - CTC基线降低49%，模型具模块化特点，控制解码时间影响的方法在领域适应有效。
            arXiv:2505.11352v1 Announce Type: cross 
Abstract: Recently, large-scale pre-trained speech encoders and Large Language Models (LLMs) have been released, which show state-of-the-art performance on a range of spoken language processing tasks including Automatic Speech Recognition (ASR). To effectively combine both models for better performance, continuous speech prompts, and ASR error correction have been adopted. However, these methods are prone to suboptimal performance or are inflexible. In this paper, we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using the ASR posterior matrices. The speech encoder is trained to generate Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary, which are used to reconstruct pseudo-audio embeddings by computing a weighted sum of the LLM input embeddings. These embeddings are concatenated with text embeddings in the LLM input space. Using the well-performing USM and Gemma models as an example, we demonstrate that our proposed LegoSLM method yields good performance on both ASR and speech translation tasks. By connecting USM with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline on 8 MLS testsets. The trained model also exhibits modularity in a range of settings -- after fine-tuning the Gemma model weights, the speech encoder can be switched and combined with the LLM in a zero-shot fashion. Additionally, we propose to control the decode-time influence of the USM and LLM using a softmax temperature, which shows effectiveness in domain adaptation.
        ]]></description>
    </item>
    <item>
        <title>SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech System</title>
        <link>https://arxiv.org/abs/2503.23108</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.23108v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee</dc:creator>
        <description><![CDATA[
            背景：为提升语音合成的可扩展性和效率，提出新的文本转语音（TTS）系统。方法：SupertonicTTS包含语音自编码器、文本到隐空间映射模块和时长预测器，采用低维隐空间、隐变量时间压缩和ConvNeXt块，直接处理原始字符级文本并使用交叉注意力进行文本 - 语音对齐，还引入上下文共享批量扩展。效果：与现有TTS模型相比，在显著降低架构复杂度和计算开销的同时，取得了有竞争力的性能。
            arXiv:2503.23108v2 Announce Type: replace 
Abstract: We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for improved scalability and efficiency in speech synthesis. SupertonicTTS comprises three components: a speech autoencoder for continuous latent representation, a text-to-latent module leveraging flow-matching for text-to-latent mapping, and an utterance-level duration predictor. To enable a lightweight architecture, we employ a low-dimensional latent space, temporal compression of latents, and ConvNeXt blocks. We further simplify the TTS pipeline by operating directly on raw character-level text and employing cross-attention for text-speech alignment, thus eliminating the need for grapheme-to-phoneme (G2P) modules and external aligners. In addition, we introduce context-sharing batch expansion that accelerates loss convergence and stabilizes text-speech alignment. Experimental results demonstrate that SupertonicTTS achieves competitive performance while significantly reducing architectural complexity and computational overhead compared to contemporary TTS models. Audio samples demonstrating the capabilities of SupertonicTTS are available at: https://supertonictts.github.io/.
        ]]></description>
    </item>
    <item>
        <title>SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified Flow</title>
        <link>https://arxiv.org/abs/2504.07776</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.07776v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kaidi Wang, Wenhao Guan, Shenghui Lu, Jianglong Yao, Lin Li, Qingyang Hong</dc:creator>
        <description><![CDATA[
            背景：基于流匹配的语音合成提升了合成语音质量并减少推理步骤。方法：提出基于整流流的轻量级高效语音合成系统SlimSpeech，在现有整流流模型语音合成方法基础上修改结构以减少参数作教师模型，改进回流操作从大模型直接导出采样轨迹更直接的小模型，用蒸馏技术提升性能。效果：大幅减少模型参数，通过单步采样达到与大模型相当的性能。
            arXiv:2504.07776v2 Announce Type: replace 
Abstract: Recently, flow matching based speech synthesis has significantly enhanced the quality of synthesized speech while reducing the number of inference steps. In this paper, we introduce SlimSpeech, a lightweight and efficient speech synthesis system based on rectified flow. We have built upon the existing speech synthesis method utilizing the rectified flow model, modifying its structure to reduce parameters and serve as a teacher model. By refining the reflow operation, we directly derive a smaller model with a more straight sampling trajectory from the larger model, while utilizing distillation techniques to further enhance the model performance. Experimental results demonstrate that our proposed method, with significantly reduced model parameters, achieves comparable performance to larger models through one-step sampling.
        ]]></description>
    </item>
</channel>
</rss>