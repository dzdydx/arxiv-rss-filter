<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 28 Apr 2025 12:20:56 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Mon, 28 Apr 2025 12:20:56 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>Token Sequence Compression for Efficient Multimodal Computing</title>
        <link>https://arxiv.org/abs/2504.17892</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17892v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yasmine Omri, Parth Shroff, Thierry Tambe</dc:creator>
        <description><![CDATA[
            背景：大型多模态模型（LMMs）虽推动跨模态推理进步，但计算成本高昂，且当前视觉编码器存在冗余和低效问题。方法：聚焦视觉语言模型，通过基准测试和定性分析，研究多种视觉令牌选择和合并方法，证明简单的聚类级令牌聚合效果更优。效果：该方法在令牌选择和合并上超越先前的先进工作，为高维数据的高效编码和处理做出初步探索，为可扩展和可持续的多模态系统奠定基础。
            arXiv:2504.17892v1 Announce Type: new 
Abstract: The exponential growth of Large Multimodal Models (LMMs) has driven advancements in cross-modal reasoning but at significant computational costs. In this work, we focus on visual language models. We highlight the redundancy and inefficiency in current vision encoders, and seek to construct an adaptive compression method for multimodal data. In this work, we characterize a panoply of visual token selection and merging approaches through both benchmarking and qualitative analysis. In particular, we demonstrate that simple cluster-level token aggregation outperforms prior state-of-the-art works in token selection and merging, including merging at the vision encoder level and attention-based approaches. We underline the redundancy in current vision encoders, and shed light on several puzzling trends regarding principles of visual token selection through cross-modal attention visualizations. This work is a first effort towards more effective encoding and processing of high-dimensional data, and paves the way for more scalable and sustainable multimodal systems.
        ]]></description>
    </item>
    <item>
        <title>PropRAG: Guiding Retrieval with Beam Search over Proposition Paths</title>
        <link>https://arxiv.org/abs/2504.18070</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18070v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jingjin Wang</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）是为大语言模型提供最新知识的标准非参数方法，但标准RAG无法捕捉人类记忆的关联性，结构化RAG存在上下文损失。方法：提出PropRAG框架，利用上下文丰富的命题和新的束搜索算法发现多步推理链，在线检索依靠图遍历和预计算嵌入，离线用大模型提取命题、检索后生成答案。效果：在多个数据集上取得了最先进的零样本Recall@5结果，如PopQA达55.3%，还获得了顶尖的F1分数。
            arXiv:2504.18070v1 Announce Type: new 
Abstract: Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to capture the interconnected nature of human memory crucial for complex reasoning (associativity) and contextual understanding (sense-making). While structured RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples, the inherent context loss limits fidelity. We introduce PropRAG, a framework leveraging contextually rich propositions and a novel beam search algorithm over proposition paths to explicitly discover multi-step reasoning chains. Crucially, PropRAG's online retrieval process operates entirely without invoking generative LLMs, relying instead on efficient graph traversal and pre-computed embeddings. This avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are used effectively offline for high-quality proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.
        ]]></description>
    </item>
    <item>
        <title>A Generative Graph Contrastive Learning Model with Global Signal</title>
        <link>https://arxiv.org/abs/2504.18148</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18148v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaofan Wei, Binyan Zhang</dc:creator>
        <description><![CDATA[
            背景：图对比学习（GCL）可通过自监督学习从图中学习复杂结构信息，但现有模型因对比信号不当导致性能下降。方法：提出准确图学习的对比信号生成框架（CSG2L），一是构建奇异值分解（SVD）导向的增强模块（SVD - aug）获取全局交互并避免随机噪声干扰；二是设计带自适应重加权策略的局部 - 全局依赖学习模块（LGDL）区分难易样本对影响。效果：在基准数据集上实验表明，CSG2L优于现有基线模型，且与多种图神经网络兼容。
            arXiv:2504.18148v1 Announce Type: new 
Abstract: Graph contrastive learning (GCL) has garnered significant attention recently since it learns complex structural information from graphs through self-supervised learning manner. However, prevalent GCL models may suffer from performance degradation due to inappropriate contrastive signals. Concretely, they commonly generate augmented views based on random perturbation, which leads to biased essential structures due to the introduction of noise. In addition, they assign equal weight to both hard and easy sample pairs, thereby ignoring the difference in importance of the sample pairs. To address these issues, this study proposes a novel Contrastive Signal Generative Framework for Accurate Graph Learning (CSG2L) with the following two-fold ideas: a) building a singular value decomposition (SVD)-directed augmented module (SVD-aug) to obtain the global interactions as well as avoiding the random noise perturbation; b) designing a local-global dependency learning module (LGDL) with an adaptive reweighting strategy which can differentiate the effects of hard and easy sample pairs. Extensive experiments on benchmark datasets demonstrate that the proposed CSG2L outperforms the state-of-art baselines. Moreover, CSG2L is compatible with a variety of GNNs.
        ]]></description>
    </item>
    <item>
        <title>Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family</title>
        <link>https://arxiv.org/abs/2504.18225</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18225v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Pierre-Carl Langlais, Pavel Chizhov, Mattia Nee, Carlos Rosas Hinostroza, Matthieu Delsart, Ir\`ene Girard, Othman Hicheur, Anastasia Stasenko, Ivan P. Yamshchikov</dc:creator>
        <description><![CDATA[
            背景：为RAG、搜索和源摘要任务开发新型小推理模型。方法：提出Pleias - RAG - 350m和Pleias - RAG - 1B，在模拟从通用语料库检索多种多语言开源数据的大型合成数据集上进行中间训练，支持引用和溯源，集成RAG工作流的多种特征。效果：在标准化RAG基准测试中，优于40亿参数以下的单语言模型，与Qwen - 2.5 - 7B等大模型有竞争力，能在欧洲主要语言中保持稳定性能，确保陈述的系统参考溯源。
            arXiv:2504.18225v1 Announce Type: new 
Abstract: We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.
        ]]></description>
    </item>
    <item>
        <title>Efficient Single-Pass Training for Multi-Turn Reasoning</title>
        <link>https://arxiv.org/abs/2504.18246</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18246v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ritesh Goru, Shanay Mehta, Prateek Jain</dc:creator>
        <description><![CDATA[
            背景：训练大语言模型在给出答案前生成显式推理能提升其在数学、编码等任务中的表现，但在多轮推理数据集上微调大模型时，推理标记会影响后续输入，无法单轮前向处理。方法：提出通过响应标记复制和自定义注意力掩码克服此局限的新方法。效果：显著减少训练时间，可在多轮推理数据集上高效微调。
            arXiv:2504.18246v1 Announce Type: new 
Abstract: Training Large Language Models ( LLMs) to generate explicit reasoning before they produce an answer has been shown to improve their performance across various tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets presents a unique challenge: LLMs must generate reasoning tokens that are excluded from subsequent inputs to the LLM. This discrepancy prevents us from processing an entire conversation in a single forward pass-an optimization readily available when we fine-tune on a multi-turn non-reasoning dataset. This paper proposes a novel approach that overcomes this limitation through response token duplication and a custom attention mask that enforces appropriate visibility constraints. Our approach significantly reduces the training time and allows efficient fine-tuning on multi-turn reasoning datasets.
        ]]></description>
    </item>
    <item>
        <title>Testing Individual Fairness in Graph Neural Networks</title>
        <link>https://arxiv.org/abs/2504.18353</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18353v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Roya Nasiri</dc:creator>
        <description><![CDATA[
            背景：人工智能模型的偏差会导致基于敏感属性的歧视，图神经网络（GNNs）在个体公平性方面研究较少，其节点互联结构使偏差传播，增加个体公平性检测和缓解难度。方法：先系统回顾个体公平性文献，创建分类法；再通过调整和扩展现有技术，开发评估和确保GNNs个体公平性的框架。效果：将通过基于图的大语言模型的工业案例研究评估框架。
            arXiv:2504.18353v1 Announce Type: new 
Abstract: The biases in artificial intelligence (AI) models can lead to automated decision-making processes that discriminate against groups and/or individuals based on sensitive properties such as gender and race. While there are many studies on diagnosing and mitigating biases in various AI models, there is little research on individual fairness in Graph Neural Networks (GNNs). Unlike traditional models, which treat data features independently and overlook their inter-relationships, GNNs are designed to capture graph-based structure where nodes are interconnected. This relational approach enables GNNs to model complex dependencies, but it also means that biases can propagate through these connections, complicating the detection and mitigation of individual fairness violations. This PhD project aims to develop a testing framework to assess and ensure individual fairness in GNNs. It first systematically reviews the literature on individual fairness, categorizing existing approaches to define, measure, test, and mitigate model biases, creating a taxonomy of individual fairness. Next, the project will develop a framework for testing and ensuring fairness in GNNs by adapting and extending current fairness testing and mitigation techniques. The framework will be evaluated through industrial case studies, focusing on graph-based large language models.
        ]]></description>
    </item>
    <item>
        <title>Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization</title>
        <link>https://arxiv.org/abs/2504.18397</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18397v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kesen Zhao, Beier Zhu, Qianru Sun, Hanwang Zhang</dc:creator>
        <description><![CDATA[
            思维链推理能提升多模态大语言模型的可解释性与问题解决能力，但现有方法多聚焦文本思维链，视觉思维链研究不足且依赖有监督微调。本文提出无监督视觉思维链（UV - CoT）框架，通过偏好优化实现图像级思维链推理。该框架对模型生成的边界框进行偏好比较，借助自动数据生成流程获取偏好数据。实验表明，在六个数据集上，UV - CoT优于现有文本和视觉思维链方法，在四个未见数据集的零样本测试中展现出强泛化能力。
            arXiv:2504.18397v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perception--identifying key regions and reasoning based on them--UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in https://github.com/kesenzhao/UV-CoT.
        ]]></description>
    </item>
    <item>
        <title>Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models</title>
        <link>https://arxiv.org/abs/2504.17807</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17807v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ze Yang, Yihong Jin, Juntian Liu, Xinhe Xu, Yihan Zhang, Shuyang Ji</dc:creator>
        <description><![CDATA[
            背景：云平台发展与网络流量复杂性增加，需有效进行网络流量监测与异常检测。方法：构建基于大语言模型的系统，利用大模型处理网络流量序列数据，结合transformer注意力机制到监督学习框架，添加考虑时间和上下文的异常检测层，采用基于迁移学习的方法。效果：该模型在检测准确率和计算效率上优于传统方法，能有效识别零日攻击、流量拥塞等异常，显著降低误报率。
            arXiv:2504.17807v1 Announce Type: cross 
Abstract: The rapidly evolving cloud platforms and the escalating complexity of network traffic demand proper network traffic monitoring and anomaly detection to ensure network security and performance. This paper introduces a large language model (LLM)-based network traffic monitoring and anomaly detection system. In addition to existing models such as autoencoders and decision trees, we harness the power of large language models for processing sequence data from network traffic, which allows us a better capture of underlying complex patterns, as well as slight fluctuations in the dataset. We show for a given detection task, the need for a hybrid model that incorporates the attention mechanism of the transformer architecture into a supervised learning framework in order to achieve better accuracy. A pre-trained large language model analyzes and predicts the probable network traffic, and an anomaly detection layer that considers temporality and context is added. Moreover, we present a novel transfer learning-based methodology to enhance the model's effectiveness to quickly adapt to unknown network structures and adversarial conditions without requiring extensive labeled datasets. Actual results show that the designed model outperforms traditional methods in detection accuracy and computational efficiency, effectively identify various network anomalies such as zero-day attacks and traffic congestion pattern, and significantly reduce the false positive rate.
        ]]></description>
    </item>
    <item>
        <title>OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation Learning</title>
        <link>https://arxiv.org/abs/2504.17811</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.17811v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Anirudhan Badrinath, Alex Yang, Kousik Rajesh, Prabhat Agarwal, Jaewon Yang, Haoyu Chen, Jiajing Xu, Charles Rosenberg</dc:creator>
        <description><![CDATA[
            背景：在网页应用中，开发整合多种技术以支持多应用的统一表征学习框架是一大挑战。方法：本文提出OmniSage，它是一个大规模表征框架，通过多个对比学习任务将图神经网络与基于内容的模型、用户序列模型集成，有效处理图数据、用户序列数据和内容信号，还开发了能支持数十亿节点图的高效基础设施。效果：该框架生成的通用表征显著提升了Pinterest用户体验，使五个应用的全站转存率约提高2.5%。
            arXiv:2504.17811v1 Announce Type: cross 
Abstract: Representation learning, a task of learning latent vectors to represent entities, is a key task in improving search and recommender systems in web applications. Various representation learning methods have been developed, including graph-based approaches for relationships among entities, sequence-based methods for capturing the temporal evolution of user activities, and content-based models for leveraging text and visual content. However, the development of a unifying framework that integrates these diverse techniques to support multiple applications remains a significant challenge. This paper presents OmniSage, a large-scale representation framework that learns universal representations for a variety of applications at Pinterest. OmniSage integrates graph neural networks with content-based models and user sequence models by employing multiple contrastive learning tasks to effectively process graph data, user sequence data, and content signals. To support the training and inference of OmniSage, we developed an efficient infrastructure capable of supporting Pinterest graphs with billions of nodes. The universal representations generated by OmniSage have significantly enhanced user experiences on Pinterest, leading to an approximate 2.5% increase in sitewide repins (saves) across five applications. This paper highlights the impact of unifying representation learning methods, and we will open source the OmniSage code by the time of publication.
        ]]></description>
    </item>
    <item>
        <title>PreGSU-A Generalized Traffic Scene Understanding Model for Autonomous Driving based on Pre-trained Graph Attention Network</title>
        <link>https://arxiv.org/abs/2404.10263</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2404.10263v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuning Wang, Zhiyuan Liu, Haotian Lin, Junkai Jiang, Shaobing Xu, Jianqiang Wang</dc:creator>
        <description><![CDATA[
            背景：当前交通场景理解方法泛化能力不足，难以适应复杂交通和多样下游需求。方法：提出基于图注意力网络的广义预训练场景理解模型PreGSU，经特征工程和子图模块将元素嵌入为节点形成动态加权图，用四层图注意力层学习关系，预训练阶段在两个自监督任务上训练，微调时加载预训练参数。效果：在三个数据集和两个下游任务验证，相比单任务基线，PreGSU表现优异，消融实验证明预训练任务设计有效。
            arXiv:2404.10263v2 Announce Type: replace 
Abstract: Scene understanding, defined as learning, extraction, and representation of interactions among traffic elements, is one of the critical challenges toward high-level autonomous driving (AD). Current scene understanding methods mainly focus on one concrete single task, such as trajectory prediction and risk level evaluation. Although they perform well on specific metrics, the generalization ability is insufficient to adapt to the real traffic complexity and downstream demand diversity. In this study, we propose PreGSU, a generalized pre-trained scene understanding model based on graph attention network to learn the universal interaction and reasoning of traffic scenes to support various downstream tasks. After the feature engineering and sub-graph module, all elements are embedded as nodes to form a dynamic weighted graph. Then, four graph attention layers are applied to learn the relationships among agents and lanes. In the pre-train phase, the understanding model is trained on two self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road Modeling (MRM). Based on the artificial potential field theory, VIF modeling enables PreGSU to capture the agent-to-agent interactions while MRM extracts agent-to-road connections. In the fine-tuning process, the pre-trained parameters are loaded to derive detailed understanding outputs. We conduct validation experiments on three datasets and two downstream tasks, i.e., trajectory prediction in urban scenario and intention recognition in highway scenario, to verify the model's generalization and understanding capabilities. Results show that compared with single-task-driven baselines, PreGSU achieves competitive performance on all datasets and downstream tasks, indicating its potential to be generalized to various scenes and targets. Ablation study shows the effectiveness of pre-train task design.
        ]]></description>
    </item>
    <item>
        <title>AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction</title>
        <link>https://arxiv.org/abs/2406.10432</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2406.10432v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Peitao Han, Lis Kanashiro Pereira, Fei Cheng, Wan Jou She, Eiji Aramaki</dc:creator>
        <description><![CDATA[
            背景：现有关系抽取的上下文学习方法常重语言相似性而轻结构相似性，易忽略实体关系。方法：提出一种基于抽象意义表示（AMR）增强的基于检索的上下文学习方法，该模型根据任务输入和训练样本间的语义结构相似性检索上下文示例。效果：在四个标准英文关系抽取数据集上评估，无监督设置下均优于基线模型；有监督设置下，三个数据集达最优，第四个数据集表现有竞争力。
            arXiv:2406.10432v3 Announce Type: replace 
Abstract: Existing in-context learning (ICL) methods for relation extraction (RE) often prioritize language similarity over structural similarity, which can lead to overlooking entity relationships. To address this, we propose an AMR-enhanced retrieval-based ICL method for RE. Our model retrieves in-context examples based on semantic structure similarity between task inputs and training samples. Evaluations on four standard English RE datasets show that our model outperforms baselines in the unsupervised setting across all datasets. In the supervised setting, it achieves state-of-the-art results on three datasets and competitive results on the fourth.
        ]]></description>
    </item>
    <item>
        <title>GOFA: A Generative One-For-All Model for Joint Graph Language Modeling</title>
        <link>https://arxiv.org/abs/2407.09709</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2407.09709v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang</dc:creator>
        <description><![CDATA[
            背景：基础模型在文本、图像领域表现强大，但图数据结构不明确，开发图基础模型面临挑战，现有方法难以兼顾处理无限任务和捕捉图结构。方法：提出生成式图语言模型GOFA，将随机初始化的GNN层插入预训练LLM，结合语义和结构建模能力，在新提出的图级任务上预训练，再在下游任务微调。效果：在各种下游任务评估中，模型在零样本场景展现出解决结构和上下文问题的强大能力。
            arXiv:2407.09709v2 Announce Type: replace 
Abstract: Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better -- yet, no existing work can achieve both simultaneously. In this paper, we identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA to solve the problem. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, and structural tasks to obtain the above GFM properties. The pre-trained model is further fine-tuned on downstream tasks to obtain task-solving ability. The fine-tuned model is evaluated on various downstream tasks, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.
        ]]></description>
    </item>
    <item>
        <title>MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning</title>
        <link>https://arxiv.org/abs/2409.12059</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.12059v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Yue Zhao, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji</dc:creator>
        <description><![CDATA[
            背景：大语言模型能理解和生成人类表达，但缺乏深度思考和推理机制，现有提升思考能力的研究多非数据驱动或基于训练。方法：受自然认知机制启发，设计TaS模型架构，先思考再作答；设计流程从提示 - 响应样本中标注或生成思考内容，在中间层添加语言头作为思考层，用增强数据训练模型。效果：定性示例和定量结果验证了TaS的有效性和性能。
            arXiv:2409.12059v4 Announce Type: replace 
Abstract: Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE.
        ]]></description>
    </item>
    <item>
        <title>A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization</title>
        <link>https://arxiv.org/abs/2411.06018</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.06018v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haoxin Liu, Chenghao Liu, B. Aditya Prakash</dc:creator>
        <description><![CDATA[
            背景：大语言模型在多领域展现推理能力，但在现实中普遍存在的时间序列推理（TsR）方面研究较少。方法：提出首个评估大语言模型TsR性能的综合测试平台TimerBed，识别出数据数值建模可能是初始推理失败的原因，提出基于提示的解决方案VL - Time，利用可视化建模数据和语言引导推理。效果：实验表明VL - Time使多模态大语言模型在时间序列推理中，零样本学习表现显著，少样本上下文学习能力强大，平均性能提升约140%，平均令牌成本降低99%。
            arXiv:2411.06018v2 Announce Type: replace 
Abstract: Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, are largely underexplored for time-series reasoning (TsR), which is ubiquitous in the real world. In this work, we propose TimerBed, the first comprehensive testbed for evaluating LLMs' TsR performance. Specifically, TimerBed includes stratified reasoning patterns with real-world tasks, comprehensive combinations of LLMs and reasoning strategies, and various supervised models as comparison anchors. We perform extensive experiments with TimerBed, test multiple current beliefs, and verify the initial failures of LLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and performance degradation of few shot in-context learning (ICL). Further, we identify one possible root cause: the numerical modeling of data. To address this, we propose a prompt-based solution VL-Time, using visualization-modeled data and language-guided reasoning. Experimental results demonstrate that Vl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL reasoners for time series, achieving about 140% average performance improvement and 99% average token costs reduction.
        ]]></description>
    </item>
    <item>
        <title>Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models</title>
        <link>https://arxiv.org/abs/2411.07611</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.07611v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang</dc:creator>
        <description><![CDATA[
            疾病诊断中解释性很关键，但现有模型难平衡预测准确性与解释合理性。大语言模型推理强但计算成本高、多模态推理受限，小语言模型高效却缺乏多模态数据推理能力，且两者都缺领域知识。为此提出ClinRaGen，通过原理蒸馏和领域知识注入提升小语言模型。采用顺序原理蒸馏框架赋予小模型类似大模型的多模态推理能力，用知识增强注意力机制统一时间序列和文本数据的多模态表示。实验显示，ClinRaGen在疾病诊断和原理生成上达最优。
            arXiv:2411.07611v2 Announce Type: replace 
Abstract: Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable mutlimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in a same encoding space, enabling it naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.
        ]]></description>
    </item>
    <item>
        <title>UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs</title>
        <link>https://arxiv.org/abs/2502.00806</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.00806v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yufei He, Yuan Sui, Xiaoxin He, Yue Liu, Yifei Sun, Bryan Hooi</dc:creator>
        <description><![CDATA[
            现有基础模型常忽略多模态数据集中的图结构，而现有图基础模型难以处理多模态图（MMGs）的复杂性。为此，本文提出UniGraph2，一种跨领域图基础模型。它使用特定模态编码器和图神经网络学习统一低维嵌入空间，提出跨领域多图预训练算法，并采用专家混合组件对齐不同领域和模态特征。大量实验表明，在表示学习、迁移学习和多模态生成任务等方面，UniGraph2显著优于现有模型。 
            arXiv:2502.00806v2 Announce Type: replace 
Abstract: Existing foundation models, such as CLIP, aim to learn a unified embedding space for multimodal data, enabling a wide range of downstream web-based applications like search, recommendation, and content classification. However, these models often overlook the inherent graph structures in multimodal datasets, where entities and their relationships are crucial. Multimodal graphs (MMGs) represent such graphs where each node is associated with features from different modalities, while the edges capture the relationships between these entities. On the other hand, existing graph foundation models primarily focus on text-attributed graphs (TAGs) and are not designed to handle the complexities of MMGs. To address these limitations, we propose UniGraph2, a novel cross-domain graph foundation model that enables general representation learning on MMGs, providing a unified embedding space. UniGraph2 employs modality-specific encoders alongside a graph neural network (GNN) to learn a unified low-dimensional embedding space that captures both the multimodal information and the underlying graph structure. We propose a new cross-domain multi-graph pre-training algorithm at scale to ensure effective transfer learning across diverse graph domains and modalities. Additionally, we adopt a Mixture of Experts (MoE) component to align features from different domains and modalities, ensuring coherent and robust embeddings that unify the information across modalities. Extensive experiments on a variety of multimodal graph tasks demonstrate that UniGraph2 significantly outperforms state-of-the-art models in tasks such as representation learning, transfer learning, and multimodal generative tasks, offering a scalable and flexible solution for learning on MMGs.
        ]]></description>
    </item>
    <item>
        <title>ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion</title>
        <link>https://arxiv.org/abs/2504.15920</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15920v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiang Li, Haobing Liu, Jianpeng Qi, Yuan Cao, Guoqing Chao, Yanwei Yu</dc:creator>
        <description><![CDATA[
            背景：图神经网络（GNNs）在图任务中表现出色，但存在过平滑和可扩展性问题。方法：提出ScaleGNN框架，通过自适应高阶特征融合模块学习各阶邻居矩阵相对信息，引入基于局部贡献分数的高阶冗余特征掩码机制，保留相关邻居，采用低阶增强特征聚合自适应整合高低阶特征。效果：在真实数据集实验中，该方法在准确性和计算效率上均优于现有GNN模型。
            arXiv:2504.15920v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across various graph-based tasks by effectively capturing relational information between nodes. These models rely on iterative message passing to propagate node features, enabling nodes to aggregate information from their neighbors. Recent research has significantly improved the message-passing mechanism, enhancing GNN scalability on large-scale graphs. However, GNNs still face two main challenges: over-smoothing, where excessive message passing results in indistinguishable node representations, especially in deep networks incorporating high-order neighbors; and scalability issues, as traditional architectures suffer from high model complexity and increased inference time due to redundant information aggregation. This paper proposes a novel framework for large-scale graphs named ScaleGNN that simultaneously addresses both challenges by adaptively fusing multi-level graph features. We first construct neighbor matrices for each order, learning their relative information through trainable weights through an adaptive high-order feature fusion module. This allows the model to selectively emphasize informative high-order neighbors while reducing unnecessary computational costs. Additionally, we introduce a High-order redundant feature masking mechanism based on a Local Contribution Score (LCS), which enables the model to retain only the most relevant neighbors at each order, preventing redundant information propagation. Furthermore, low-order enhanced feature aggregation adaptively integrates low-order and high-order features based on task relevance, ensuring effective capture of both local and global structural information without excessive complexity. Extensive experiments on real-world datasets demonstrate that our approach consistently outperforms state-of-the-art GNN models in both accuracy and computational efficiency.
        ]]></description>
    </item>
    <item>
        <title>Instant Policy: In-Context Imitation Learning via Graph Diffusion</title>
        <link>https://arxiv.org/abs/2411.12633</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.12633v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Vitalis Vosylius, Edward Johns</dc:creator>
        <description><![CDATA[
            背景：大模型上下文学习能力突出，上下文模仿学习（ICIL）为机器人领域带来机遇。方法：提出Instant Policy，通过图表示引入归纳偏置，将ICIL建模为图生成问题并结合扩散过程，实现对演示、观察和动作的结构化推理；用模拟生成的伪演示作为训练数据。效果：模拟和真实实验表明，Instant Policy能快速学习各类日常机器人任务，还可作为跨实体和零样本迁移到语言定义任务的基础。
            arXiv:2411.12633v2 Announce Type: replace-cross 
Abstract: Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly (without further training) from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem with a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations - arbitrary trajectories generated in simulation - as a virtually infinite pool of training data. Simulated and real experiments show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. Code and videos are available at https://www.robot-learning.uk/instant-policy.
        ]]></description>
    </item>
    <item>
        <title>DOSE : Drum One-Shot Extraction from Music Mixture</title>
        <link>https://arxiv.org/abs/2504.18157</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18157v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Suntae Hwang, Seonghyeon Kang, Kyungsu Kim, Semin Ahn, Kyogu Lee</dc:creator>
        <description><![CDATA[
            鼓的单音样本对音乐制作至关重要。本文提出鼓单音提取任务，即从音乐混合信号中提取鼓单音。为此构建了随机混合单音数据集（RMOD），并提出鼓单音提取模型（DOSE），利用神经音频编解码语言模型实现端到端提取，还引入新的起始损失以准确预测鼓单音初始瞬态。与基于源分离的提取方法对比，用弗雷歇音频距离（FAD）和多尺度频谱损失（MSS）评估，结果显示带起始损失的DOSE表现更优，能从音乐混合信号中提取更准确、高质量的鼓单音。
            arXiv:2504.18157v1 Announce Type: new 
Abstract: Drum one-shot samples are crucial for music production, particularly in sound design and electronic music. This paper introduces Drum One-Shot Extraction, a task in which the goal is to extract drum one-shots that are present in the music mixture. To facilitate this, we propose the Random Mixture One-shot Dataset (RMOD), comprising large-scale, randomly arranged music mixtures paired with corresponding drum one-shot samples. Our proposed model, Drum One- Shot Extractor (DOSE), leverages neural audio codec language models for end-to-end extraction, bypassing traditional source separation steps. Additionally, we introduce a novel onset loss, designed to encourage accurate prediction of the initial transient of drum one-shots, which is essential for capturing timbral characteristics. We compare this approach against a source separation-based extraction method as a baseline. The results, evaluated using Frechet Audio Distance (FAD) and Multi-Scale Spectral loss (MSS), demonstrate that DOSE, enhanced with onset loss, outperforms the baseline, providing more accurate and higher-quality drum one-shots from music mixtures. The code, model checkpoint, and audio examples are available at https://github.com/HSUNEH/DOSE
        ]]></description>
    </item>
    <item>
        <title>Kimi-Audio Technical Report</title>
        <link>https://arxiv.org/abs/2504.18425</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.18425v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>KimiTeam, Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, Zhengtao Wang, Chu Wei, Yifei Xin, Xinran Xu, Jianwei Yu, Yutao Zhang, Xinyu Zhou, Y. Charles, Jun Chen, Yanru Chen, Yulun Du, Weiran He, Zhenxing Hu, Guokun Lai, Qingcheng Li, Yangyang Liu, Weidong Sun, Jianzhou Wang, Yuzhi Wang, Yuefeng Wu, Yuxin Wu, Dongchao Yang, Hao Yang, Ying Yang, Zhilin Yang, Aoxiong Yin, Ruibin Yuan, Yutong Zhang, Zaida Zhou</dc:creator>
        <description><![CDATA[
            背景：音频领域需强大的基础模型。方法：提出开源音频基础模型Kimi - Audio，采用12.5Hz音频分词器，设计基于大语言模型（LLM）架构，以连续特征为输入、离散令牌为输出，开发基于流匹配的分块流式去分词器；构建含超1300万小时音频的预训练数据集及后训练数据管道；经多任务持续预训练和微调。效果：在语音识别、音频理解等多个音频基准测试中达最优，代码等已开源。
            arXiv:2504.18425v1 Announce Type: new 
Abstract: We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.
        ]]></description>
    </item>
    <item>
        <title>EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing</title>
        <link>https://arxiv.org/abs/2412.08988</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2412.08988v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van den Hengel, Jian Yang, Qingming Huang</dc:creator>
        <description><![CDATA[
            电影配音任务需生成与视频同步且克隆所需声音的语音，但现有方法存在难以兼顾音画同步与清晰发音、缺乏情感表达能力的问题。为此，研究者提出EmoDubber，一种可控制情感的配音架构。该架构设计了LPA、PE策略、说话人身份适配模块和FUEC。其中，FUEC通过正负引导机制根据用户情感指令确定梯度方向和引导规模。在三个基准数据集上的实验显示，该方法优于多个现有先进方法。
            arXiv:2412.08988v3 Announce Type: replace 
Abstract: Given a piece of text, a video clip, and a reference audio, the movie dubbing task aims to generate speech that aligns with the video while cloning the desired voice. The existing methods have two primary deficiencies: (1) They struggle to simultaneously hold audio-visual sync and achieve clear pronunciation; (2) They lack the capacity to express user-defined emotions. To address these problems, we propose EmoDubber, an emotion-controllable dubbing architecture that allows users to specify emotion type and emotional intensity while satisfying high-quality lip sync and pronunciation. Specifically, we first design Lip-related Prosody Aligning (LPA), which focuses on learning the inherent consistency between lip motion and prosody variation by duration level contrastive learning to incorporate reasonable alignment. Then, we design Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences by efficient conformer to improve speech intelligibility. Next, the speaker identity adapting module aims to decode acoustics prior and inject the speaker style embedding. After that, the proposed Flow-based User Emotion Controlling (FUEC) is used to synthesize waveform by flow matching prediction network conditioned on acoustics prior. In this process, the FUEC determines the gradient direction and guidance scale based on the user's emotion instructions by the positive and negative guidance mechanism, which focuses on amplifying the desired emotion while suppressing others. Extensive experimental results on three benchmark datasets demonstrate favorable performance compared to several state-of-the-art methods.
        ]]></description>
    </item>
    <item>
        <title>Spatial Audio Processing with Large Language Model on Wearable Devices</title>
        <link>https://arxiv.org/abs/2504.08907</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08907v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ayushi Mishra, Yang Bai, Priyadarshan Narayanasamy, Nakul Garg, Nirupam Roy</dc:creator>
        <description><![CDATA[
            背景：将空间上下文融入大语言模型有望革新可穿戴设备的人机交互。方法：提出将空间语音理解融入大语言模型的系统架构，利用单声道麦克风基于微结构空间传感提取精确到达方向信息，合成OmniTalk数据集，将空间信息与Whisper模型的语言嵌入融合，与LLaMA - 3.2 3B模型输入空间对齐并微调。效果：在空间感知自动语音识别中，平均误差25.72°，优于现有工作的88.52°，词错误率5.3；声景处理可推断最多5人说话及方向，中位到达方向误差16°。
            arXiv:2504.08907v2 Announce Type: replace 
Abstract: Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of $25.72^\circ$-a substantial improvement compared to the 88.52$^\circ$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16$^\circ$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences.
        ]]></description>
    </item>
    <item>
        <title>Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio</title>
        <link>https://arxiv.org/abs/1711.08058</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:1711.08058v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ahmad AbdulKader, Kareem Nassar, Mohamed El-Geish, Daniel Galvez, Chetan Patil</dc:creator>
        <description><![CDATA[
            背景：在非独立同分布环境下对8kHz窄带音频进行关键词检测（KWS），比多数现有KWS系统面临的任务更具挑战性。方法：提出使用级联分类器，模型结合深度神经网络（DNNs）、级联、多特征表示和多实例学习，级联分类器处理类别不平衡问题，并通过提前终止减少计算受限设备的功耗。效果：该KWS系统在每小时误报率为0.75的情况下，漏报率达6%。
            arXiv:1711.08058v2 Announce Type: replace-cross 
Abstract: We propose using cascaded classifiers for a keyword spotting (KWS) task on narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more challenging task than most state-of-the-art KWS systems face. We present a model that incorporates Deep Neural Networks (DNNs), cascading, multiple-feature representations, and multiple-instance learning. The cascaded classifiers handle the task's class imbalance and reduce power consumption on computationally-constrained devices via early termination. The KWS system achieves a false negative rate of 6% at an hourly false positive rate of 0.75
        ]]></description>
    </item>
</channel>
</rss>