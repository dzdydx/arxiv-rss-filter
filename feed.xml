<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 06 Aug 2025 12:47:47 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Wed, 06 Aug 2025 12:47:47 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>DMSC: Dynamic Multi-Scale Coordination Framework for Time Series Forecasting</title>
        <link>https://arxiv.org/abs/2508.02753</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02753v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haonan Yang, Jianchao Tang, Zhuo Li, Long Lan</dc:creator>
        <description><![CDATA[
            时间序列预测在建模不同尺度的复杂时间依赖关系方面存在挑战，现有方法受静态分解策略、碎片化依赖建模和不灵活融合机制限制。为此，本文提出动态多尺度协调框架DMSC，包含多尺度补丁分解块、三元交互块和自适应尺度路由混合专家块。通过输入自适应补丁调整消除预定义尺度约束，联合建模多种依赖关系，利用专家动态融合多尺度预测。在13个真实基准上的实验表明，DMSC在时间序列预测任务中保持了最优性能和高计算效率。
            arXiv:2508.02753v1 Announce Type: new 
Abstract: Time Series Forecasting (TSF) faces persistent challenges in modeling intricate temporal dependencies across different scales. Despite recent advances leveraging different decomposition operations and novel architectures based on CNN, MLP or Transformer, existing methods still struggle with static decomposition strategies, fragmented dependency modeling, and inflexible fusion mechanisms, limiting their ability to model intricate temporal dependencies. To explicitly solve the mentioned three problems respectively, we propose a novel Dynamic Multi-Scale Coordination Framework (DMSC) with Multi-Scale Patch Decomposition block (EMPD), Triad Interaction Block (TIB) and Adaptive Scale Routing MoE block (ASR-MoE). Specifically, EMPD is designed as a built-in component to dynamically segment sequences into hierarchical patches with exponentially scaled granularities, eliminating predefined scale constraints through input-adaptive patch adjustment. TIB then jointly models intra-patch, inter-patch, and cross-variable dependencies within each layer's decomposed representations. EMPD and TIB are jointly integrated into layers forming a multi-layer progressive cascade architecture, where coarse-grained representations from earlier layers adaptively guide fine-grained feature extraction in subsequent layers via gated pathways. And ASR-MoE dynamically fuses multi-scale predictions by leveraging specialized global and local experts with temporal-aware weighting. Comprehensive experiments on thirteen real-world benchmarks demonstrate that DMSC consistently maintains state-of-the-art (SOTA) performance and superior computational efficiency for TSF tasks. Code is available at https://github.com/1327679995/DMSC.
        ]]></description>
    </item>
    <item>
        <title>Context-Adaptive Multi-Prompt LLM Embedding for Vision-Language Alignment</title>
        <link>https://arxiv.org/abs/2508.02762</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02762v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Dahun Kim, Anelia Angelova</dc:creator>
        <description><![CDATA[
            背景：标准CLIP风格模型在视觉语言对比学习中依赖单一文本嵌入。方法：提出上下文自适应多提示嵌入方法，引入多个结构化提示，每个含自适应标记以捕捉输入文本不同语义，在单次前向传播中联合处理提示，将提示嵌入组合成统一文本表示，还加入多样性正则化损失和否定感知损失。效果：在图像 - 文本和视频 - 文本检索基准上均取得持续改进。
            arXiv:2508.02762v1 Announce Type: new 
Abstract: We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to enrich semantic representations in vision-language contrastive learning. Unlike standard CLIP-style models that rely on a single text embedding, our method introduces multiple structured prompts, each containing a distinct adaptive token that captures diverse semantic aspects of the input text. We process all prompts jointly in a single forward pass. The resulting prompt embeddings are combined into a unified text representation, enabling semantically richer alignment with visual features. To further promote semantic diversity and representation quality, we incorporate a diversity regularization loss and a negation-aware loss, encouraging specialization across prompts and improving contrastive discrimination. Our method achieves consistent improvements on both image-text and video-text retrieval benchmarks.
        ]]></description>
    </item>
    <item>
        <title>Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation</title>
        <link>https://arxiv.org/abs/2508.02835</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02835v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kennedy Edemacu, Vinay M. Shashidhar, Micheal Tuape, Dan Abudu, Beakcheol Jang, Jong Wook Kim</dc:creator>
        <description><![CDATA[
            检索增强生成（RAG）可结合外部知识源提升大语言模型能力，但易遭受知识投毒攻击。攻击者可篡改知识源误导生成模型，如PoisonedRAG攻击。为此，本文提出FilterRAG和ML - FilterRAG防御方法。先确定可区分对抗文本和干净文本的特性，再用此特性过滤对抗文本。在基准数据集上的评估显示这些方法有效，性能接近原始RAG系统。
            arXiv:2508.02835v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to boost the capabilities of large language models (LLMs) by incorporating external, up-to-date knowledge sources. However, this introduces a potential vulnerability to knowledge poisoning attacks, where attackers can compromise the knowledge source to mislead the generation model. One such attack is the PoisonedRAG in which the injected adversarial texts steer the model to generate an attacker-chosen response to a target question. In this work, we propose novel defense methods, FilterRAG and ML-FilterRAG, to mitigate the PoisonedRAG attack. First, we propose a new property to uncover distinct properties to differentiate between adversarial and clean texts in the knowledge data source. Next, we employ this property to filter out adversarial texts from clean ones in the design of our proposed approaches. Evaluation of these methods using benchmark datasets demonstrate their effectiveness, with performances close to those of the original RAG systems.
        ]]></description>
    </item>
    <item>
        <title>CauKer: classification time series foundation models can be pretrained on synthetic data only</title>
        <link>https://arxiv.org/abs/2508.02879</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02879v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shifeng Xie, Vasilii Feofanov, Marius Alonso, Ambroise Odonnat, Jianfeng Zhang, Themis Palpanas, Ievgen Redko</dc:creator>
        <description><![CDATA[
            背景：时间序列基础模型（TSFMs）因零样本能力强且应用广泛而受关注，但预训练需对大规模精心整理的真实序列进行高计算成本的操作。方法：提出CauKer算法，结合高斯过程（GP）核组成与结构因果模型（SCM），生成具有现实趋势、季节性和非线性交互的多样化、因果连贯的合成时间序列，用于不同架构和预训练方法的分类TSFMs样本高效预训练。效果：实验显示CauKer生成的数据集在规模和模型容量上有明确缩放规律，而真实数据集缩放行为不规则。
            arXiv:2508.02879v1 Announce Type: new 
Abstract: Time series foundation models (TSFMs) have recently gained significant attention due to their strong zero-shot capabilities and widespread real-world applications. Such models typically require a computationally costly pretraining on large-scale, carefully curated collections of real-world sequences. To allow for a sample-efficient pretraining of TSFMs, we propose CauKer, a novel algorithm designed to generate diverse, causally coherent synthetic time series with realistic trends, seasonality, and nonlinear interactions. CauKer combines Gaussian Process (GP) kernel composition with Structural Causal Models (SCM) to produce data for sample-efficient pretraining of state-of-the-art classification TSFMs having different architectures and following different pretraining approaches. Additionally, our experiments reveal that CauKer-generated datasets exhibit clear scaling laws for both dataset size (10K to 10M samples) and model capacity (1M to 783M parameters), unlike real-world datasets, which display irregular scaling behavior.
        ]]></description>
    </item>
    <item>
        <title>VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction</title>
        <link>https://arxiv.org/abs/2508.02890</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02890v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Rongxin Jiang, Robert Long, Chenghao Gu, Mingrui Yan</dc:creator>
        <description><![CDATA[
            现有大视觉语言模型在复杂视觉引导的创意内容生成中，存在视觉保真度、创造性及指令遵循度不足的问题。本文提出VisuCraft框架，通过集成多模态结构化信息提取器（E）和动态提示生成模块（G）解决上述问题。提取器将图像视觉属性转化为结构化表示，动态提示模块结合用户指令生成优化提示。在自建数据集上评估，该框架在故事生成、诗歌创作等任务中，各项指标均优于基线模型，尤其在创造性和指令遵循度上提升显著，证明其有效性。
            arXiv:2508.02890v1 Announce Type: new 
Abstract: This paper introduces VisuCraft, a novel framework designed to significantly enhance the capabilities of Large Vision-Language Models (LVLMs) in complex visual-guided creative content generation. Existing LVLMs often exhibit limitations in maintaining high visual fidelity, genuine creativity, and precise adherence to nuanced user instructions when generating long-form texts. VisuCraft addresses these challenges by integrating a multimodal structured information extractor (E) and a dynamic prompt generation module (G). The extractor distills fine-grained visual attributes from input images into a rich, structured representation, which the dynamic prompt module then combines with user instructions to create highly optimized prompts for underlying LVLMs (e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed ImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity, and Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs across tasks like story generation and poetry composition. Our results demonstrate remarkable improvements, particularly in creativity and instruction adherence, validating VisuCraft's effectiveness in producing imaginative, visually grounded, and user-aligned long-form creative text. This work unlocks new potential for LVLMs in sophisticated creative AI applications.
        ]]></description>
    </item>
    <item>
        <title>HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation</title>
        <link>https://arxiv.org/abs/2508.03104</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03104v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mengting Pan, Fan Li, Xiaoyang Wang, Wenjie Zhang, Xuemin Lin</dc:creator>
        <description><![CDATA[
            背景：现有基于对比学习的方法应用于文本属性超图时存在忽视图拓扑与文本关联、依赖随机数据增强引入噪声、难以捕捉长程依赖等问题。方法：提出HiTeC，两阶段分层对比学习框架。第一阶段用结构感知对比目标预训练文本编码器；第二阶段引入两种语义感知增强策略；还提出多尺度对比损失。效果：解耦文本编码器预训练和超图对比学习，提升可扩展性且不降低表征质量，大量实验证实其有效性。
            arXiv:2508.03104v1 Announce Type: new 
Abstract: Contrastive learning (CL) has become a dominant paradigm for self-supervised hypergraph learning, enabling effective training without costly labels. However, node entities in real-world hypergraphs are often associated with rich textual information, which is overlooked in prior works. Directly applying existing CL-based methods to such text-attributed hypergraphs (TAHGs) leads to three key limitations: (1) The common use of graph-agnostic text encoders overlooks the correlations between textual content and hypergraph topology, resulting in suboptimal representations. (2) Their reliance on random data augmentations introduces noise and weakens the contrastive objective. (3) The primary focus on node- and hyperedge-level contrastive signals limits the ability to capture long-range dependencies, which is essential for expressive representation learning. Although HyperBERT pioneers CL on TAHGs, its co-training paradigm suffers from poor scalability. To fill the research gap, we introduce HiTeC, a two-stage hierarchical contrastive learning framework with semantic-aware augmentation for scalable and effective self-supervised learning on TAHGs. In the first stage, we pre-train the text encoder with a structure-aware contrastive objective to overcome the graph-agnostic nature of conventional methods. In the second stage, we introduce two semantic-aware augmentation strategies, including prompt-enhanced text augmentation and semantic-aware hyperedge drop, to facilitate informative view generation. Furthermore, we propose a multi-scale contrastive loss that extends existing objectives with an $s$-walk-based subgraph-level contrast to better capture long-range dependencies. By decoupling text encoder pretraining from hypergraph contrastive learning, this two-stage design enhances scalability without compromising representation quality. Extensive experiments confirm the effectiveness of HiTeC.
        ]]></description>
    </item>
    <item>
        <title>Long Story Generation via Knowledge Graph and Literary Theory</title>
        <link>https://arxiv.org/abs/2508.03137</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03137v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ge Shi, Kaiyu Huang, Guochen Feng</dc:creator>
        <description><![CDATA[
            长故事生成是长文本生成领域的子任务，以往基于大纲的生成方法存在主题漂移和情节乏味逻辑不连贯的问题。本文提出多智能体故事生成器结构改进多阶段方法，以大语言模型为智能体核心组件。引入含长期和短期记忆存储的记忆存储模型避免主题漂移；基于文学叙事学理论设计故事主题障碍框架，构建知识图谱融入新节点内容提升故事吸引力；建立多智能体交互阶段模拟作者读者互动并修订文本。评估显示该方法能生成更高质量长故事。
            arXiv:2508.03137v1 Announce Type: new 
Abstract: The generation of a long story consisting of several thousand words is a sub-task in the field of long text generation~(LTG). Previous research has addressed this challenge through outline-based generation, which employs a multi-stage method for generating outlines into stories. However, this approach suffers from two common issues: almost inevitable theme drift caused by the loss of memory of previous outlines, and tedious plots with incoherent logic that are less appealing to human readers.
  In this paper, we propose the multi-agent Story Generator structure to improve the multi-stage method, using large language models~(LLMs) as the core components of agents. To avoid theme drift, we introduce a memory storage model comprising two components: a long-term memory storage that identifies the most important memories, thereby preventing theme drift; and a short-term memory storage that retains the latest outlines from each generation round. To incorporate engaging elements into the story, we design a story theme obstacle framework based on literary narratology theory that introduces uncertain factors and evaluation criteria to generate outline. This framework calculates the similarity of the former storyline and enhances the appeal of the story by building a knowledge graph and integrating new node content. Additionally, we establish a multi-agent interaction stage to simulate writer-reader interaction through dialogue and revise the story text according to feedback, to ensure it remains consistent and logical. Evaluations against previous methods demonstrate that our approach can generate higher-quality long stories.
        ]]></description>
    </item>
    <item>
        <title>RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior</title>
        <link>https://arxiv.org/abs/2508.03140</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03140v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junyao Yang, Jianwei Wang, Huiping Zhuang, Cen Chen, Ziqian Zeng</dc:creator>
        <description><![CDATA[
            背景：具有长思维链能力的大语言模型在复杂问题解决上表现出色，模型融合是创建兼具长思维链能力和特定领域知识模型的高效方法，但现有融合方法存在推理能力下降等问题。方法：提出RCP - Merging框架，将推理模型权重作为基础先验，利用推理能力指标保留核心长思维链能力模型权重，选择性融合关键特定领域权重。效果：在多个模型和领域实验表明，相比现有方法，领域任务性能提升9.5%和9.2%，且不显著损害原长思维链推理能力。
            arXiv:2508.03140v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with long chain-of-thought (CoT) capability, termed Reasoning Models, demonstrate superior intricate problem-solving abilities through multi-step long CoT reasoning. To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, model merging emerges as a highly resource-efficient method. However, significant challenges lie in merging domain-specific LLMs with long CoT ones since nowadays merging methods suffer from reasoning capability degradation, even gibberish output and output collapse. To overcome this, we introduce RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, meanwhile maintaining model performance in the original domain. Treating reasoning model weights as foundational prior, our method utilizes a reasoning capability indicator to preserve core long CoT capability model weights while selectively merging essential domain-specific weights. We conducted extensive experiments on Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance domains. Our results show that RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.
        ]]></description>
    </item>
    <item>
        <title>Rethinking Selectivity in State Space Models: A Minimal Predictive Sufficiency Approach</title>
        <link>https://arxiv.org/abs/2508.03158</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03158v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yiyi Wang, Jian'an Zhang, Hongyi Duan, Haoyang Liu, Qingyang Li</dc:creator>
        <description><![CDATA[
            背景：状态空间模型（SSMs）是序列建模的主流架构，但现有模型依赖启发式选择机制，缺乏理论推导，其最优性和鲁棒性存疑。方法：引入预测充分性原则，提出最小预测充分性状态空间模型（MPS - SSM），通过优化基于该原则的目标函数指导选择机制，使模型压缩历史信息且保留预测能力。效果：在多个基准数据集上，MPS - SSM达到了最优性能，在长期预测和噪声场景中显著优于现有模型，鲁棒性强，且该原则可扩展为通用正则框架提升其他架构。  
            arXiv:2508.03158v1 Announce Type: new 
Abstract: State Space Models (SSMs), particularly recent selective variants like Mamba, have emerged as a leading architecture for sequence modeling, challenging the dominance of Transformers. However, the success of these state-of-the-art models largely relies on heuristically designed selective mechanisms, which lack a rigorous first-principle derivation. This theoretical gap raises questions about their optimality and robustness against spurious correlations. To address this, we introduce the Principle of Predictive Sufficiency, a novel information-theoretic criterion stipulating that an ideal hidden state should be a minimal sufficient statistic of the past for predicting the future. Based on this principle, we propose the Minimal Predictive Sufficiency State Space Model (MPS-SSM), a new framework where the selective mechanism is guided by optimizing an objective function derived from our principle. This approach encourages the model to maximally compress historical information without losing predictive power, thereby learning to ignore non-causal noise and spurious patterns. Extensive experiments on a wide range of benchmark datasets demonstrate that MPS-SSM not only achieves state-of-the-art performance, significantly outperforming existing models in long-term forecasting and noisy scenarios, but also exhibits superior robustness. Furthermore, we show that the MPS principle can be extended as a general regularization framework to enhance other popular architectures, highlighting its broad potential.
        ]]></description>
    </item>
    <item>
        <title>CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction</title>
        <link>https://arxiv.org/abs/2508.03159</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03159v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang</dc:creator>
        <description><![CDATA[
            药物毒性是药物研发的重大挑战，现有机器学习模型在预测时依赖标注数据且缺乏可解释性。为解决此问题，研究提出一种名为CoTox的框架，将大语言模型与思维链推理结合用于多毒性预测。它整合化学结构数据、生物途径和基因本体术语，通过逐步推理生成可解释的毒性预测。实验表明，该框架在GPT - 4o上表现优于传统机器学习和深度学习模型，使用IUPAC名称表征化学结构可增强推理能力和预测性能，还能结合生物背景模拟人体生理响应进行预测。
            arXiv:2508.03159v1 Announce Type: new 
Abstract: Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.
        ]]></description>
    </item>
    <item>
        <title>ChartCap: Mitigating Hallucination of Dense Chart Captioning</title>
        <link>https://arxiv.org/abs/2508.03164</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03164v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junyoung Lim, Jaewoo Ahn, Gunhee Kim</dc:creator>
        <description><![CDATA[
            当下视觉语言模型为图表生成准确、无幻觉的说明仍有挑战，原因是缺乏大规模、高质量真实图表数据集，且现有数据集存在含无关信息、未充分捕捉结构元素和关键信息的问题。为此，研究团队引入含56.5万张真实图表及对应说明的ChartCap数据集，设计四阶段流程并采用基于循环一致性的人工验证。此外，提出视觉一致性得分新指标。实验表明，基于该数据集微调的模型生成说明更准确、详细且幻觉更少，超越开源、专有模型及人工标注说明。
            arXiv:2508.03164v1 Announce Type: new 
Abstract: Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.
        ]]></description>
    </item>
    <item>
        <title>Towards Interpretable Concept Learning over Time Series via Temporal Logic Semantics</title>
        <link>https://arxiv.org/abs/2508.03269</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03269v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Irene Ferfoglia, Simone Silvetti, Gaia Saveri, Laura Nenzi, Luca Bortolussi</dc:creator>
        <description><![CDATA[
            时间序列分类在安全关键应用中至关重要，但常用黑盒深度学习方法，难以理解输出逻辑。为此提出神经符号框架，将轨迹直接嵌入信号时态逻辑（STL）概念空间，统一分类与解释。引入受STL启发的核函数，将原始时间序列映射到与预定义STL公式的对齐状态，联合优化准确性与可解释性。能基于人类可解释的时间模式分类，给出局部和全局符号解释。早期结果显示性能良好，还能为模型决策提供高质量逻辑依据。
            arXiv:2508.03269v1 Announce Type: new 
Abstract: Time series classification is a task of paramount importance, as this kind of data often arises in safety-critical applications. However, it is typically tackled with black-box deep learning methods, making it hard for humans to understand the rationale behind their output. To take on this challenge, we propose a neuro-symbolic framework that unifies classification and explanation through direct embedding of trajectories into a space of Signal Temporal Logic (STL) concepts. By introducing a novel STL-inspired kernel that maps raw time series to their alignment with predefined STL formulae, our model jointly optimises for accuracy and interpretability, as each prediction is accompanied by the most relevant logical concepts that characterise it. This enables classification grounded in human-interpretable temporal patterns and produces both local and global symbolic explanations. Early results show competitive performance while offering high-quality logical justifications for model decisions.
        ]]></description>
    </item>
    <item>
        <title>Understanding the Embedding Models on Hyper-relational Knowledge Graph</title>
        <link>https://arxiv.org/abs/2508.03280</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03280v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yubo Wang, Shimin Di, Zhili Wang, Haoyang Li, Fei Teng, Hao Xin, Lei Chen</dc:creator>
        <description><![CDATA[
            背景：超关系知识图（HKGs）作为传统知识图的扩展被提出，研究者设计额外模块让经典知识图嵌入（KGE）模型适配HKGs，但不清楚超关系KGE（HKGE）模型性能优势来源。方法：用三种分解方法将HKGs转换为KG格式，评估经典KGE模型表现，提出FormerGNN框架，通过限定符整合器保留拓扑，用GNN编码器捕捉长程依赖，改进信息整合方法。效果：实验表明部分KGE模型可达到与HKGE模型相当表现，FormerGNN优于现有HKGE模型。
            arXiv:2508.03280v1 Announce Type: new 
Abstract: Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an extension of traditional Knowledge Graphs (KGs) to better represent real-world facts with additional qualifiers. As a result, researchers have attempted to adapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing extra qualifier processing modules. However, it remains unclear whether the superior performance of Hyper-relational KGE (HKGE) models arises from their base KGE model or the specially designed extension module. Hence, in this paper, we data-wise convert HKGs to KG format using three decomposition methods and then evaluate the performance of several classical KGE models on HKGs. Our results show that some KGE models achieve performance comparable to that of HKGE models. Upon further analysis, we find that the decomposition methods alter the original HKG topology and fail to fully preserve HKG information. Moreover, we observe that current HKGE models are either insufficient in capturing the graph's long-range dependency or struggle to integrate main-triple and qualifier information due to the information compression issue. To further justify our findings and offer a potential direction for future HKGE research, we propose the FormerGNN framework. This framework employs a qualifier integrator to preserve the original HKG topology, and a GNN-based graph encoder to capture the graph's long-range dependencies, followed by an improved approach for integrating main-triple and qualifier information to mitigate compression issues. Our experimental results demonstrate that FormerGNN outperforms existing HKGE models.
        ]]></description>
    </item>
    <item>
        <title>Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</title>
        <link>https://arxiv.org/abs/2508.03337</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03337v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shaoguang Wang (The Hong Kong University of Science and Technology), Jianxiang He (The Hong Kong University of Science and Technology), Yijie Xu (The Hong Kong University of Science and Technology), Ziyang Chen (The Hong Kong University of Science and Technology), Weiyu Guo (The Hong Kong University of Science and Technology), Hui Xiong (The Hong Kong University of Science and Technology)</dc:creator>
        <description><![CDATA[
            多模态大语言模型处理视频问答时，处理大量视频帧的高token成本阻碍其实际应用，且过多帧会因上下文稀释降低性能，现有关键帧选择方法存在时间冗余。为此，提出自适应帧剪枝（AFP）方法，在融合特征空间采用自适应层次聚类算法，识别并合并冗余帧。还引入轻量级文本语义图，以极小token开销提供关键上下文。在多个基准测试中，该方法最多减少86.9%的所需帧数和83.2%的总输入token，不仅提升效率，还常比使用更多帧的基线方法提高准确率。
            arXiv:2508.03337v1 Announce Type: new 
Abstract: The practical application of Multimodal Large Language Models (MLLMs) to Video Question Answering (Video-QA) is severely hindered by the high token cost of processing numerous video frames. While increasing the number of sampled frames is a common strategy, we observe a "less is more" phenomenon where excessive frames can paradoxically degrade performance due to context dilution. Concurrently, state-of-the-art keyframe selection methods, while effective, still yield significant temporal redundancy, which we term 'visual echoes'. To address these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel post-processing method that intelligently prunes the selected keyframes. AFP employs an adaptive hierarchical clustering algorithm on a fused ResNet-50 and CLIP feature space to identify and merge these echoes into single representatives. To compensate for information loss, we then introduce a lightweight, text-based semantic graph that provides critical context with minimal token overhead. Conducting extensive experiments on the LongVideoBench and VideoMME benchmarks across multiple leading MLLMs, our full approach demonstrates a drastic reduction in required frames by up to 86.9% and total input tokens by up to 83.2%. Crucially, by providing a concise, high-quality set of frames, our method not only enhances efficiency but often improves accuracy over baselines that use more frames. The code will be released upon publication.
        ]]></description>
    </item>
    <item>
        <title>WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval</title>
        <link>https://arxiv.org/abs/2508.03343</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03343v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Junlong Ren, Gangjian Zhang, Honghao Fu, Pengcheng Wu, Hao Wang</dc:creator>
        <description><![CDATA[
            文本-动作检索（TMR）旨在检索与文本描述语义相关的3D动作序列，但实现3D动作与文本的匹配具有挑战，现有方法难以区分身体各部分及其动态，限制了精确语义对齐。为此提出WaMo，一种基于小波的多频特征提取框架。它包含轨迹小波分解、轨迹小波重构和无序动作序列预测三个关键组件来提取动作特征，实现与文本的细粒度对齐。实验表明，WaMo在HumanML3D和KIT - ML数据集上的$Rsum$分别提升17.0%和18.2%，优于现有SOTA方法。
            arXiv:2508.03343v1 Announce Type: new 
Abstract: Text-Motion Retrieval (TMR) aims to retrieve 3D motion sequences semantically relevant to text descriptions. However, matching 3D motions with text remains highly challenging, primarily due to the intricate structure of human body and its spatial-temporal dynamics. Existing approaches often overlook these complexities, relying on general encoding methods that fail to distinguish different body parts and their dynamics, limiting precise semantic alignment. To address this, we propose WaMo, a novel wavelet-based multi-frequency feature extraction framework. It fully captures part-specific and time-varying motion details across multiple resolutions on body joints, extracting discriminative motion features to achieve fine-grained alignment with texts. WaMo has three key components: (1) Trajectory Wavelet Decomposition decomposes motion signals into frequency components that preserve both local kinematic details and global motion semantics. (2) Trajectory Wavelet Reconstruction uses learnable inverse wavelet transforms to reconstruct original joint trajectories from extracted features, ensuring the preservation of essential spatial-temporal information. (3) Disordered Motion Sequence Prediction reorders shuffled motion sequences to improve the learning of inherent temporal coherence, enhancing motion-text alignment. Extensive experiments demonstrate WaMo's superiority, achieving 17.0\% and 18.2\% improvements in $Rsum$ on HumanML3D and KIT-ML datasets, respectively, outperforming existing state-of-the-art (SOTA) methods.
        ]]></description>
    </item>
    <item>
        <title>Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models</title>
        <link>https://arxiv.org/abs/2508.03363</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03363v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haotian Wu, Bo Xu, Yao Shu, Menglin Yang, Chengwei Qin</dc:creator>
        <description><![CDATA[
            推理大语言模型虽有结构化和多步推理能力，但上下文学习潜力待挖掘。为此提出“Thinking with Nothinking Calibration”新范式，利用两种推理模式差异提升推理准确率。让模型并行生成两种模式答案，不一致时触发第二轮思考。因不一致情况少，多数仅一轮推理，延迟开销小。实验表明，该方法显著优于少样本思维链和多数投票，在分布内表现与基于训练的最优方法相当，分布外任务表现更佳，还体现出强可扩展性。
            arXiv:2508.03363v1 Announce Type: new 
Abstract: Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that leverages the structured difference between two reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy. Specifically, our method prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt that incorporates the original question and both candidate answers. Since such disagreement occurs infrequently (e.g., only 6\% in GSM8K), our method performs just one round of reasoning in most cases, resulting in minimal latency overhead. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT) and majority voting with improved answer robustness. Moreover, It achieves comparable in-distribution performance to training-based SOTA method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing that leveraging different reasoning modes consistently lowers the error rate and highlights the value of structural thinking diversity. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second round of thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs.
        ]]></description>
    </item>
    <item>
        <title>Marito: Structuring and Building Open Multilingual Terminologies for South African NLP</title>
        <link>https://arxiv.org/abs/2508.03529</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03529v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Vukosi Marivate, Isheanesu Dzingirai, Fiskani Banda, Richard Lastrucci, Thapelo Sindane, Keabetswe Madumo, Kayode Olaleye, Abiodun Modupe, Unarine Netshifhefhe, Herkulaas Combrink, Mohlatlego Nakeng, Matome Ledwaba</dc:creator>
        <description><![CDATA[
            背景：南非官方语言缺乏结构化术语数据，现有术语列表分散且格式不利于计算研究。方法：Marito 系统地聚合、清理和标准化零散资源，形成开放、可互操作的数据集，并在以非洲为中心的 NOODL 框架下发布，还将术语集成到检索增强生成（RAG）管道。效果：实验表明，这使大语言模型的英语到文达语机器翻译的准确性和特定领域一致性大幅提高，为开发强大公平的 NLP 技术奠定基础。
            arXiv:2508.03529v1 Announce Type: new 
Abstract: The critical lack of structured terminological data for South Africa's official languages hampers progress in multilingual NLP, despite the existence of numerous government and academic terminology lists. These valuable assets remain fragmented and locked in non-machine-readable formats, rendering them unusable for computational research and development. \emph{Marito} addresses this challenge by systematically aggregating, cleaning, and standardising these scattered resources into open, interoperable datasets. We introduce the foundational \emph{Marito} dataset, released under the equitable, Africa-centered NOODL framework. To demonstrate its immediate utility, we integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline. Experiments show substantial improvements in the accuracy and domain-specific consistency of English-to-Tshivenda machine translation for large language models. \emph{Marito} provides a scalable foundation for developing robust and equitable NLP technologies, ensuring South Africa's rich linguistic diversity is represented in the digital age.
        ]]></description>
    </item>
    <item>
        <title>Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation</title>
        <link>https://arxiv.org/abs/2508.03571</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03571v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Iing Muttakhiroh, Thomas Fevens</dc:creator>
        <description><![CDATA[
            大语言模型在面临领域转移时，因灾难性遗忘常出现性能下降问题。为此，本文提出KILO框架，将动态知识图谱与指令调优结合。训练时利用检索到的特定领域知识作指导，增强模型对新领域的适应性和旧知识的保留能力。在WikiText - 103上预训练，在四个目标领域评估。实验表明，KILO在多项指标上优于多个强基线模型，证明结合结构化知识检索与指令提示能有效应对持续学习中的领域转移挑战。
            arXiv:2508.03571v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often suffer from performance degradation when faced with domain shifts, primarily due to catastrophic forgetting. In this work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation), a novel continual learning framework that integrates dynamic knowledge graphs with instruction tuning. By leveraging retrieved domain-specific knowledge as guidance during training, KILO enhances both adaptability to new domains and retention of previously acquired knowledge. We pretrain our model on WikiText-103 and evaluate sequential adaptation across four diverse target domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that KILO consistently outperforms strong baselines, including continual fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward transfer, F1 score, retention rate, and training efficiency. These results highlight the effectiveness of combining structured knowledge retrieval and instruction prompting to overcome domain shift challenges in continual learning scenarios.
        ]]></description>
    </item>
    <item>
        <title>Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?</title>
        <link>https://arxiv.org/abs/2508.03644</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03644v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin</dc:creator>
        <description><![CDATA[
            背景：多模态大语言模型的检索增强生成（RAG）系统在复杂文档理解方面前景广阔，但现有评估不足制约其发展，当前基准存在缺陷。方法：引入新的大规模、多语言、多模态评估系统Double - Bench，涵盖多种语言和文档类型。效果：实验表明文本和视觉嵌入模型差距缩小，揭示当前文档RAG框架过度自信问题，该开源系统为未来研究提供基础，还计划每年更新基准。 
            arXiv:2508.03644v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.
        ]]></description>
    </item>
    <item>
        <title>CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction</title>
        <link>https://arxiv.org/abs/2508.03668</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03668v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zixuan Li, Binzong Geng, Jing Xiong, Yong He, Yuxuan Hu, Jian Chen, Dingwei Chen, Xiyu Chang, Liang Zhang, Linjian Mo, Chengming Li, Chuan Yuan, Zhenan Sun</dc:creator>
        <description><![CDATA[
            背景：点击率（CTR）预测是推荐系统的核心任务，用语言模型（LM）处理用户行为序列时，因序列结构与预训练的自然语言不同，导致语义碎片化，影响预测性能。方法：提出CTR - Sink框架，引入行为级注意力汇聚点，插入汇聚令牌，结合推荐特定信号；设计两阶段训练策略和注意力汇聚机制，引导LM关注汇聚令牌、增强汇聚点间依赖以捕捉行为关联。效果：在工业和开源数据集上实验及可视化结果验证了该方法的有效性。
            arXiv:2508.03668v1 Announce Type: new 
Abstract: Click-Through Rate (CTR) prediction, a core task in recommendation systems, estimates user click likelihood using historical behavioral data. Modeling user behavior sequences as text to leverage Language Models (LMs) for this task has gained traction, owing to LMs' strong semantic understanding and contextual modeling capabilities. However, a critical structural gap exists: user behavior sequences consist of discrete actions connected by semantically empty separators, differing fundamentally from the coherent natural language in LM pre-training. This mismatch causes semantic fragmentation, where LM attention scatters across irrelevant tokens instead of focusing on meaningful behavior boundaries and inter-behavior relationships, degrading prediction performance. To address this, we propose $\textit{CTR-Sink}$, a novel framework introducing behavior-level attention sinks tailored for recommendation scenarios. Inspired by attention sink theory, it constructs attention focus sinks and dynamically regulates attention aggregation via external information. Specifically, we insert sink tokens between consecutive behaviors, incorporating recommendation-specific signals such as temporal distance to serve as stable attention sinks. To enhance generality, we design a two-stage training strategy that explicitly guides LM attention toward sink tokens and a attention sink mechanism that amplifies inter-sink dependencies to better capture behavioral correlations. Experiments on one industrial dataset and two open-source datasets (MovieLens, Kuairec), alongside visualization results, validate the method's effectiveness across scenarios.
        ]]></description>
    </item>
    <item>
        <title>La La LiDAR: Large-Scale Layout Generation from LiDAR Data</title>
        <link>https://arxiv.org/abs/2508.03691</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03691v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Youquan Liu, Lingdong Kong, Weidong Yang, Xin Li, Ao Liang, Runnan Chen, Ben Fei, Tongliang Liu</dc:creator>
        <description><![CDATA[
            背景：可控生成逼真的激光雷达场景对自动驾驶和机器人技术等应用至关重要，但现有基于扩散的模型缺乏对前景物体和空间关系的明确控制。方法：提出大规模布局引导的激光雷达生成模型La La LiDAR，包括语义增强的场景图扩散和关系感知上下文调节用于结构化激光雷达布局生成，以及前景感知控制注入用于完整场景生成；引入两个大规模激光雷达场景图数据集和新的布局合成评估指标。效果：大幅实验表明，该模型在激光雷达生成和下游感知任务中达到了最先进水平。
            arXiv:2508.03691v1 Announce Type: new 
Abstract: Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model ("La La LiDAR"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.
        ]]></description>
    </item>
    <item>
        <title>LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</title>
        <link>https://arxiv.org/abs/2508.03692</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03692v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi</dc:creator>
        <description><![CDATA[
            生成式世界模型是自动驾驶重要数据引擎，但现有研究多关注视频或占用网格，忽略激光雷达特性。将激光雷达生成拓展到动态4D世界建模存在可控性、时间连贯性和评估标准化等挑战。为此，研究团队提出LiDARCrafter框架，将自然语言指令解析为以自我为中心的场景图，借助三分支扩散网络生成对象结构、运动轨迹和几何图形，还通过自回归模块生成时间连贯的4D激光雷达序列。在nuScenes数据集上实验显示，该框架在各层面的保真度、可控性和时间一致性上达最优，代码和基准已开源。
            arXiv:2508.03692v1 Announce Type: new 
Abstract: Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.
        ]]></description>
    </item>
    <item>
        <title>Kronos: A Foundation Model for the Language of Financial Markets</title>
        <link>https://arxiv.org/abs/2508.02739</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02739v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yu Shi, Zongliang Fu, Shuo Chen, Bohan Zhao, Wei Xu, Changshui Zhang, Jian Li</dc:creator>
        <description><![CDATA[
            背景：大语言模型预训练范式推动了时间序列基础模型发展，但在金融K线数据应用有限，且现有模型忽视部分下游任务。方法：提出Kronos框架，引入专门分词器将连续市场信息离散为令牌序列，在超120亿条K线记录的多市场语料库上自回归预训练，学习细致的时间和跨资产表征。效果：在零样本金融任务中表现出色，在基准数据集上，价格序列预测RankIC比领先TSFM提升93%、比非预训练基线提升87%，波动率预测MAE降低9%，合成K线序列生成保真度提升22%。
            arXiv:2508.02739v1 Announce Type: cross 
Abstract: The success of large-scale pre-training paradigm, exemplified by Large Language Models (LLMs), has inspired the development of Time Series Foundation Models (TSFMs). However, their application to financial candlestick (K-line) data remains limited, often underperforming non-pre-trained architectures. Moreover, existing TSFMs often overlook crucial downstream tasks such as volatility prediction and synthetic data generation. To address these limitations, we propose Kronos, a unified, scalable pre-training framework tailored to financial K-line modeling. Kronos introduces a specialized tokenizer that discretizes continuous market information into token sequences, preserving both price dynamics and trade activity patterns. We pre-train Kronos using an autoregressive objective on a massive, multi-market corpus of over 12 billion K-line records from 45 global exchanges, enabling it to learn nuanced temporal and cross-asset representations. Kronos excels in a zero-shot setting across a diverse set of financial tasks. On benchmark datasets, Kronos boosts price series forecasting RankIC by 93% over the leading TSFM and 87% over the best non-pre-trained baseline. It also achieves a 9% lower MAE in volatility forecasting and a 22% improvement in generative fidelity for synthetic K-line sequences. These results establish Kronos as a robust, versatile foundation model for end-to-end financial time series analysis. Our pre-trained model is publicly available at https://github.com/shiyu-coder/Kronos.
        ]]></description>
    </item>
    <item>
        <title>Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow</title>
        <link>https://arxiv.org/abs/2508.02959</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02959v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chia-Tung Ho, Jing Gong, Xufeng Yao, Yunsheng Bai, Abhishek B Akkur, Haoxing Ren</dc:creator>
        <description><![CDATA[
            背景：大语言模型虽可通过执行含详细指令与结构化操作的代理工作流解决复杂任务，但手动嵌入基础模型构建通用代理存在可扩展性和效率问题，现有基于代码表示自动化生成和优化工作流的方法依赖标注数据，对无标注的动态问题效果不佳。方法：提出自优化代理Polymath，利用任务流图灵活性和代码表示工作流的表达能力，将多网格启发的图优化与自我反思引导的进化算法结合进行无标注数据的工作流优化。效果：在六个基准数据集上平均比现有基线提升8.1%。
            arXiv:2508.02959v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at solving complex tasks by executing agentic workflows composed of detailed instructions and structured operations. Yet, building general-purpose agents by manually embedding foundation models into agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT through text interfaces limits scalability and efficiency. Recently, many researchers have sought to automate the generation and optimization of these workflows through code-based representations. However, existing methods often rely on labeled datasets to train and optimize workflows, making them ineffective and inflexible for solving real-world, dynamic problems where labeled data is unavailable. To address this challenge, we introduce Polymath, a self-optimizing agent with dynamic hierarchical workflow that leverages the flexibility of task flow graphs and the expressiveness of code-represented workflows to solve a wide range of real-world, dynamic problems. The proposed optimization methodology integrates multi-grid-inspired graph optimization with a self-reflection-guided evolutionary algorithm to refine workflows without labeled data. Experimental results on six benchmark datasets across coding, math, and multi-turn QA tasks show that Polymath achieves 8.1% average improvement over state-of-the-art baselines.
        ]]></description>
    </item>
    <item>
        <title>AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots</title>
        <link>https://arxiv.org/abs/2508.02999</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02999v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xinjie Zhao, Moritz Blum, Fan Gao, Yingjian Chen, Boming Yang, Luis Marquez-Carpintero, M\'onica Pina-Navarro, Yanran Fu, So Morikawa, Yusuke Iwasawa, Yutaka Matsuo, Chanjun Park, Irene Li</dc:creator>
        <description><![CDATA[
            背景：需要一种能让非技术用户通过自然语言操作知识图谱来交互和管理特定领域数据的系统。方法：提出AGENTiGraph，它是基于多智能体的知识图谱框架，具有意图分类、任务规划和自动知识整合等灵活设计，可实现多轮对话和动态更新。效果：在教育场景3500个查询基准测试中，分类准确率达95.12%，执行成功率达90.45%，优于零样本基线，有望应用于法律和医疗等领域。
            arXiv:2508.02999v1 Announce Type: cross 
Abstract: AGENTiGraph is a user-friendly, agent-driven system that enables intuitive interaction and management of domain-specific data through the manipulation of knowledge graphs in natural language. It gives non-technical users a complete, visual solution to incrementally build and refine their knowledge bases, allowing multi-round dialogues and dynamic updates without specialized query languages. The flexible design of AGENTiGraph, including intent classification, task planning, and automatic knowledge integration, ensures seamless reasoning between diverse tasks. Evaluated on a 3,500-query benchmark within an educational scenario, the system outperforms strong zero-shot baselines (achieving 95.12% classification accuracy, 90.45% execution success), indicating potential scalability to compliance-critical or multi-step queries in legal and medical domains, e.g., incorporating new statutes or research on the fly. Our open-source demo offers a powerful new paradigm for multi-turn enterprise knowledge management that bridges LLMs and structured graphs.
        ]]></description>
    </item>
    <item>
        <title>MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation</title>
        <link>https://arxiv.org/abs/2508.03553</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03553v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang</dc:creator>
        <description><![CDATA[
            检索增强生成（RAG）可解决大语言模型幻觉问题，但多源检索集成会带来新挑战，如数据分布稀疏、源间信息冲突。为此提出MultiRAG框架，一是用多源线图构建知识模块，聚合不同知识源逻辑关系，解决数据分布问题；二是检索模块采用多级置信度计算机制，评估并消除不可靠信息节点，减少源间不一致导致的幻觉。在多个数据集实验表明，该框架显著提升复杂多源场景下知识检索的可靠性与效率。
            arXiv:2508.03553v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG.
        ]]></description>
    </item>
    <item>
        <title>Efficient Time Series Processing for Transformers and State-Space Models through Token Merging</title>
        <link>https://arxiv.org/abs/2405.17951</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2405.17951v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Leon G\"otz, Marcel Kollovieh, Stephan G\"unnemann, Leo Schwinn</dc:creator>
        <description><![CDATA[
            背景：处理长令牌序列计算需求大，令牌合并可提高计算效率。方法：首次在时间序列分析中对Transformer和状态空间模型进行令牌合并研究，引入局部合并算法，可根据邻域大小调整计算复杂度，且是首个支持Transformer解码器中令牌合并的因果合并方案，还能识别输入数据的频谱特性以预测其潜在益处。效果：在准确性影响极小的情况下大幅提高效率，在Chronos基础模型上实现高达5400%的加速。
            arXiv:2405.17951v4 Announce Type: replace 
Abstract: Despite recent advances in subquadratic attention mechanisms or state-space models, processing long token sequences still imposes significant computational requirements. Token merging has emerged as a solution to increase computational efficiency in computer vision architectures. In this work, we perform the first investigations of token merging in time series analysis on both transformers and state-space models. We further introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, achieving two major benefits: a) Local merging can adjust its computational complexity from quadratic to linear based on the neighborhood size to effectively scale to long sequences; b) Local merging is the first causal merging scheme enabling token merging in transformer decoders. Further, we identify spectral properties of the input data that reliably predict the potential benefits of local merging without requiring evaluation on downstream tasks. Our comprehensive empirical evaluation demonstrates that local merging offers substantial efficiency gains with minimal impact on accuracy, achieving up to 5400% acceleration on the recently proposed Chronos foundation model.
        ]]></description>
    </item>
    <item>
        <title>Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions</title>
        <link>https://arxiv.org/abs/2408.06787</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2408.06787v4</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Bo Xue, Yi Xu, Yunchong Song, Jiaxin Ding, Luoyi Fu, Xinbing Wang</dc:creator>
        <description><![CDATA[
            传统知识图谱补全（KGC）方法仅依赖结构信息，难以应对图谱稀疏性问题，而微调大语言模型（LLMs）虽有效但开销大，非微调LLMs效率高但性能不佳。为此提出新框架，从LLMs中间层提取知识三元组的上下文隐藏状态，训练适用于KGC任务的分类器，利用子图采样生成实体描述以弥合语义差距，采用切片互信息量化信息。实验表明，该方法比非微调LLMs方法相对提升47%，内存效率提高188倍，训练和推理加速26.11倍。
            arXiv:2408.06787v4 Announce Type: replace 
Abstract: Traditional knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). By contrast, Large Language Models (LLMs) encapsulate extensive world knowledge and exhibit powerful context modeling capabilities, making them promising for mitigating the limitations of traditional methods. However, direct fine-tuning of LLMs for KGC, though effective, imposes substantial computational and memory overheads, while utilizing non-fine-tuned LLMs is efficient but yields suboptimal performance. In this work, we propose a novel framework that synergizes the strengths of LLMs with robust knowledge representation to enable effective and efficient KGC. We extract the context-aware hidden states of knowledge triples from the intermediate layers of LLMs, thereby capturing rich semantic and relational nuances. These representations are then utilized to train a data-efficient classifier tailored specifically for KGC tasks. To bridge the semantic gaps between LLMs and KGs, we employ subgraph sampling on KGs to generate model-friendly entity descriptions. We further adopt sliced mutual information (SMI) as a principled metric to quantify the task-specific information encoded in these representations. Extensive experiments on standard benchmarks validate the efficiency and effectiveness of our approach. We achieve a 47\% relative improvement over previous methods based on non-fine-tuned LLMs and, to our knowledge, are the first to achieve classification performance comparable to fine-tuned LLMs while enhancing GPU memory efficiency by $188\times$ and accelerating training and inference by $26.11\times$.
        ]]></description>
    </item>
    <item>
        <title>Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data</title>
        <link>https://arxiv.org/abs/2409.16647</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.16647v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kota Dohi, Aoi Ito, Harsh Purohit, Tomoya Nishida, Takashi Endo, Yohei Kawaguchi</dc:creator>
        <description><![CDATA[
            背景：因带描述文本注释的时间序列数据稀缺，训练生成时间序列描述文本的模型颇具挑战。方法：提出系统性从时间序列数据生成与领域无关描述文本的方法，确定正向和反向两种创建数据与描述文本对的途径，通过新的反向途径创建TACO数据集。效果：实验表明，使用TACO数据集训练的基于对比学习的模型，能为新领域的时间序列数据生成描述性文本。
            arXiv:2409.16647v2 Announce Type: replace 
Abstract: Due to scarcity of time-series data annotated with descriptive texts, training a model to generate descriptive texts for time-series data is challenging. In this study, we propose a method to systematically generate domain-independent descriptive texts from time-series data. We identify two distinct approaches for creating pairs of time-series data and descriptive texts: the forward approach and the backward approach. By implementing the novel backward approach, we create the Temporal Automated Captions for Observations (TACO) dataset. Experimental results demonstrate that a contrastive learning based model trained using the TACO dataset is capable of generating descriptive texts for time-series data in novel domains.
        ]]></description>
    </item>
    <item>
        <title>CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM</title>
        <link>https://arxiv.org/abs/2411.04954</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2411.04954v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, Shenghua Gao</dc:creator>
        <description><![CDATA[
            背景：现有缺乏能根据多模态输入生成CAD模型的统一系统。方法：提出CAD - MLLM系统，利用CAD模型命令序列，用大语言模型对齐多模态数据与CAD模型向量表示的特征空间，设计数据构建与标注流程，构建Omni - CAD数据集；引入新评估指标。效果：该系统显著优于现有条件生成方法，对噪声和缺失点有高鲁棒性，数据集含约450K实例及其CAD构建序列。
            arXiv:2411.04954v3 Announce Type: replace 
Abstract: This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user's inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models' vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: https://cad-mllm.github.io/
        ]]></description>
    </item>
    <item>
        <title>AdaMCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Multilingual Chain-of-Thought</title>
        <link>https://arxiv.org/abs/2501.16154</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2501.16154v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Weihua Zheng, Xin Huang, Zhengyuan Liu, Tarun Kumar Vangani, Bowei Zou, Xiyan Tao, Yuhao Wu, Ai Ti Aw, Nancy F. Chen, Roy Ka-Wei Lee</dc:creator>
        <description><![CDATA[
            背景：大语言模型虽有强大多语言能力，但因训练数据分布不均，不同语言推理性能差异大，现有方法存在扩展性问题且难捕捉跨语言推理过程。方法：提出AdaMCOT框架，在生成目标语言响应前，通过中间“思考语言”动态路由思维过程，利用无语言依赖核心并结合基于奖励的自适应机制选择最优推理路径，无需额外预训练。效果：多基准评估显示，在事实推理质量和跨语言一致性上显著提升，资源少的语言设置中性能提升尤为明显。
            arXiv:2501.16154v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown impressive multilingual capabilities through pretraining on diverse corpora. Although these models show strong reasoning abilities, their performance varies significantly between languages due to the imbalanced distribution of training data. Existing approaches using sample-level translation for extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages. In this paper, we introduce AdaMCOT (Adaptive Multilingual Chain-of-Thought), a framework that enhances multilingual factual reasoning by dynamically routing thought processes in intermediary "thinking languages" before generating target-language responses. AdaMCOT leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining. Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings. An in-depth analysis of the model's hidden states and semantic space further elucidates the underlying mechanism of our method. The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances.
        ]]></description>
    </item>
    <item>
        <title>CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements</title>
        <link>https://arxiv.org/abs/2502.04592</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.04592v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yang Zhang, Wenbo Yang, Jun Wang, Qiang Ma, Jie Xiong</dc:creator>
        <description><![CDATA[
            准确预测宏观经济事件影响对投资者和政策制定者至关重要，现有预测方法难以捕捉金融市场多模态特性及事件与价格走势的因果关系。为此，提出CAMEF多模态框架，通过因果学习机制和基于大语言模型的反事实事件增强技术，有效整合文本与时间序列数据进行金融预测。贡献包括捕捉政策文本与历史价格数据因果关系、构建新金融数据集、提出基于大语言模型的增强策略。经对比和消融实验验证了方法有效性。
            arXiv:2502.04592v2 Announce Type: replace 
Abstract: Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types.
        ]]></description>
    </item>
    <item>
        <title>Information Bottleneck-Guided Heterogeneous Graph Learning for Interpretable Neurodevelopmental Disorder Diagnosis</title>
        <link>https://arxiv.org/abs/2502.20769</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.20769v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yueyang Li, Lei Chen, Wenhao Dong, Shengyu Gong, Zijian Kang, Boyang Wei, Weiming Zeng, Hongjie Yan, Lingbin Bian, Zhiguo Zhang, Wai Ting Siok, Nizhuan Wang</dc:creator>
        <description><![CDATA[
            背景：开发可解释的神经发育障碍（NDDs）诊断模型，在多模态神经影像数据的编码、解码和整合上存在挑战，现有方法在可解释性、特征提取及数据融合方面有局限。方法：提出可解释信息瓶颈异构图神经网络（I2B - HGNN）框架，包含信息瓶颈图变换器（IBGraphFormer）和信息瓶颈异构图注意力网络（IB - HGAN），分别用于识别生物标志物和实现数据融合。效果：在NDDs诊断中表现出色，分类准确率高，能识别可解释的生物标志物，还可有效分析非影像数据。
            arXiv:2502.20769v2 Announce Type: replace 
Abstract: Developing interpretable models for neurodevelopmental disorders (NDDs) diagnosis presents significant challenges in effectively encoding, decoding, and integrating multimodal neuroimaging data. While many existing machine learning approaches have shown promise in brain network analysis, they typically suffer from limited interpretability, particularly in extracting meaningful biomarkers from functional magnetic resonance imaging (fMRI) data and establishing clear relationships between imaging features and demographic characteristics. Besides, current graph neural network methodologies face limitations in capturing both local and global functional connectivity patterns while simultaneously achieving theoretically principled multimodal data fusion. To address these challenges, we propose the Interpretable Information Bottleneck Heterogeneous Graph Neural Network (I2B-HGNN), a unified framework that applies information bottleneck principles to guide both brain connectivity modeling and cross-modal feature integration. This framework comprises two complementary components. The first is the Information Bottleneck Graph Transformer (IBGraphFormer), which combines transformer-based global attention mechanisms with graph neural networks through information bottleneck-guided pooling to identify sufficient biomarkers. The second is the Information Bottleneck Heterogeneous Graph Attention Network (IB-HGAN), which employs meta-path-based heterogeneous graph learning with structural consistency constraints to achieve interpretable fusion of neuroimaging and demographic data. The experimental results demonstrate that I2B-HGNN achieves superior performance in diagnosing NDDs, exhibiting both high classification accuracy and the ability to provide interpretable biomarker identification while effectively analyzing non-imaging data.
        ]]></description>
    </item>
    <item>
        <title>Out-of-Context Relational Reasoning in Large Language Models</title>
        <link>https://arxiv.org/abs/2503.10408</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2503.10408v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jonathan Shaki, Emanuele La Malfa, Michael Wooldridge, Sarit Kraus</dc:creator>
        <description><![CDATA[
            背景：二元关系是大语言模型（LLM）基准测试中常见的概念，当前研究多在无上下文学习场景下对LLM进行基准测试，且现有工作多关注高阶任务，难以解读成败。方法：研究LLM仅通过学习新引入标记的表示来对二元关系进行无上下文推理的能力，实验聚焦于相等、不等和包含关系及其性质。效果：LLM的准确率高于随机水平，但即使在相对简单的二元关系推理任务上也远未达到完美，且模型能根据任务排列嵌入，直接编码有用信息。
            arXiv:2503.10408v2 Announce Type: replace 
Abstract: Binary relations, such as equality, are basic mathematical concepts that appear, implicitly or explicitly, in most benchmarks for Large Language Models (LLM). A recent trend in the literature is benchmarking LLMs on out-of-context learning, where the data is not presented in the prompt, but only during the model's training. However, existing works mostly focus on higher-order tasks, making it hard to interpret success or failure. In this work, we study how well can LLMs reason out-of-context on binary relations by only learning the representations of newly introduced tokens. Our experiments focus on equality ($=$), inequality ($<$), and inclusion ($\subset$) and the properties they satisfy, such as reflexivity, symmetry, transitivity, and logical complexity (e.g., the number of reasoning "hops"). We show that LLMs achieve better than random accuracy, but are still far from perfect, even on relatively simple reasoning tasks involving binary relations. We analyse the learned representations and show that LLMs encode useful information directly, arranging the embeddings according to the task.
        ]]></description>
    </item>
    <item>
        <title>How Can Objects Help Video-Language Understanding?</title>
        <link>https://arxiv.org/abs/2504.07454</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.07454v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zitian Tang, Shijie Wang, Junho Cho, Jaewook Yoo, Chen Sun</dc:creator>
        <description><![CDATA[
            背景：在多模态大语言模型中是否还需显式表示对象存在疑问。方法：提出ObjectMLLM框架，能利用任意计算机视觉算法提取并整合结构化视觉表征，采用量化连续结构化对象信息并以纯文本表示的简单方法。效果：在六个视频问答基准测试中证实，显式整合以对象为中心的表征仍有必要，且上述简单方法表现最佳，为将其他视觉感知模块集成到MLLM设计提供了数据高效的方法。
            arXiv:2504.07454v2 Announce Type: replace 
Abstract: Do we still need to represent objects explicitly in multimodal large language models (MLLMs)? To one extreme, pre-trained encoders convert images into visual tokens, with which objects and spatiotemporal relationships may be implicitly modeled. To the other extreme, image captions by themselves provide strong empirical performances for understanding tasks, despite missing fine-grained spatiotemporal information. To answer this question, we introduce ObjectMLLM, a framework capable of leveraging arbitrary computer vision algorithm to extract and integrate structured visual representation. Through extensive evaluations on six video question answering benchmarks, we confirm that explicit integration of object-centric representation remains necessary. Surprisingly, we observe that the simple approach of quantizing the continuous, structured object information and representing them as plain text performs the best, offering a data-efficient approach to integrate other visual perception modules into MLLM design. Our code and models are released at https://github.com/brown-palm/ObjectMLLM.
        ]]></description>
    </item>
    <item>
        <title>The Multi-Round Diagnostic RAG Framework for Emulating Clinical Reasoning</title>
        <link>https://arxiv.org/abs/2504.07724</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.07724v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Penglei Sun, Yixiang Chen, Xiang Li, Xiaowen Chu</dc:creator>
        <description><![CDATA[
            近年来，准确快速部署医疗大语言模型成趋势，检索增强生成（RAG）因快速部署和隐私保护受关注，但患者描述与医学知识库专业术语的语义鸿沟阻碍其在医疗诊断中的实际应用。从数据和方法两方面应对挑战，数据上构建涵盖中西医的通用知识图谱DiagnosGraph，含876种常见疾病，引入1908份病历；方法上提出多轮诊断RAG框架，利用多轮对话细化诊断可能，模拟医生临床推理。在四个医疗基准测试中实验，经医生评估，该框架提升了大语言模型诊断性能，使自动诊断更准确、贴合人类。
            arXiv:2504.07724v2 Announce Type: replace 
Abstract: In recent years, accurately and quickly deploying medical large language models (LLMs) has become a trend. Among these, retrieval-augmented generation (RAG) has garnered attention due to rapid deployment and privacy protection. However, the challenge hinder the practical deployment of RAG for medical diagnosis: the semantic gap between colloquial patient descriptions and the professional terminology within medical knowledge bases. We try to address the challenge from the data perspective and the method perspective. First, to address the semantic gap in existing knowledge bases, we construct DiagnosGraph, a generalist knowledge graph covering both modern medicine and Traditional Chinese Medicine. It contains 876 common diseases with the graph of 7,997 nodes and 37,201 triples. To bridge the gap between colloquial patient narratives and academic medical knowledge, DiagnosGraph also introduces $1,908$ medical record by formalizing the patient chief complaint and proposing a medical diagnosis. Second, we introduce the Multi-Round Diagnostic RAG (MRD-RAG) framework. It utilizes a multi-round dialogue to refine diagnostic possibilities, emulating the clinical reasoning of a physician. Experiments conducted on four medical benchmarks, with evaluations by human physicians, demonstrate that MRD-RAG enhances the diagnostic performance of LLMs, highlighting its potential to make automated diagnosis more accurate and human-aligned.
        ]]></description>
    </item>
    <item>
        <title>Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis</title>
        <link>https://arxiv.org/abs/2504.12326</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.12326v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shahriar Noroozizadeh, Jeremy C. Weiss</dc:creator>
        <description><![CDATA[
            背景：临床病例报告和出院总结完整准确，但时间标注滞后，结构化数据虽获取早却不完整。方法：构建利用大语言模型对病例报告进行表型分析、提取和标注时间定位结果的流程，应用该流程生成含2139份病例报告的脓毒症文本时间序列语料库，并在相关数据上验证系统。效果：临床发现恢复率高（O1-preview为0.755、Llama 3.3 70B Instruct为0.753），时间顺序一致性强（O1-preview为0.932、Llama 3.3 70B Instruct为0.932），为多模态集成提供改进方向。
            arXiv:2504.12326v2 Announce Type: replace 
Abstract: Clinical case reports and discharge summaries may be the most complete and accurate summarization of patient encounters, yet they are finalized, i.e., timestamped after the encounter. Complementary data structured streams become available sooner but suffer from incompleteness. To train models and algorithms on more complete and temporally fine-grained data, we construct a pipeline to phenotype, extract, and annotate time-localized findings within case reports using large language models. We apply our pipeline to generate an open-access textual time series corpus for Sepsis-3 comprising 2,139 case reports from the Pubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA and timeline annotations from I2B2/MIMIC-IV and compare the results to physician-expert annotations. We show high recovery rates of clinical findings (event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and strong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B Instruct--0.932). Our work characterizes the ability of LLMs to time-localize clinical findings in text, illustrating the limitations of LLM use for temporal reconstruction and providing several potential avenues of improvement via multimodal integration.
        ]]></description>
    </item>
    <item>
        <title>Byte Pair Encoding for Efficient Time Series Forecasting</title>
        <link>https://arxiv.org/abs/2505.14411</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.14411v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Leon G\"otz, Marcel Kollovieh, Stephan G\"unnemann, Leo Schwinn</dc:creator>
        <description><![CDATA[
            现有时间序列标记化方法存在灵活性不足、计算开销大的问题。为此，受字节对编码启发，研究提出首个以模式为中心的时间序列标记化方案，基于频繁模式的离散词汇表，将有潜在模式的样本合并为标记，自适应压缩时间序列；还引入条件解码作为后验优化方法。实验表明，基于模式的标记化使预测性能平均提升36%，效率提升1990%，条件解码使均方误差最多降低44%，且该方法具有适应性、泛化性，标记表示有意义。
            arXiv:2505.14411v2 Announce Type: replace 
Abstract: Existing time series tokenization methods predominantly encode a constant number of samples into individual tokens. This inflexible approach can generate excessive tokens for even simple patterns like extended constant values, resulting in substantial computational overhead. Inspired by the success of byte pair encoding, we propose the first pattern-centric tokenization scheme for time series analysis. Based on a discrete vocabulary of frequent motifs, our method merges samples with underlying patterns into tokens, compressing time series adaptively. Exploiting our finite set of motifs and the continuous properties of time series, we further introduce conditional decoding as a lightweight yet powerful post-hoc optimization method, which requires no gradient computation and adds no computational overhead. On recent time series foundation models, our motif-based tokenization improves forecasting performance by 36% and boosts efficiency by 1990% on average. Conditional decoding further reduces MSE by up to 44%. In an extensive analysis, we demonstrate the adaptiveness of our tokenization to diverse temporal patterns, its generalization to unseen data, and its meaningful token representations capturing distinct time series properties, including statistical moments and trends.
        ]]></description>
    </item>
    <item>
        <title>MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering</title>
        <link>https://arxiv.org/abs/2505.18247</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.18247v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kunal Sawarkar, Shivam R. Solanki, Abhilasha Mangal</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）在处理特定领域企业数据集时存在问题，不同领域语义差异影响上下文精度，微调方案成本高、效率低且缺乏泛化性。方法：提出“MetaGen Blended RAG”，通过元数据生成管道和混合查询索引增强语义检索器，利用关键概念等创建元数据丰富的语义索引和增强混合查询。效果：在生物医学PubMedQA数据集上，检索准确率达82%，RAG准确率达77%，超越所有先前零样本RAG基准，在多数据集表现出色，泛化能力强。
            arXiv:2505.18247v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.
        ]]></description>
    </item>
    <item>
        <title>Enhancing Spectral Graph Neural Networks with LLM-Predicted Homophily</title>
        <link>https://arxiv.org/abs/2506.14220</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.14220v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Kangkang Lu, Yanhua Yu, Zhiyong Huang, Tat-Seng Chua</dc:creator>
        <description><![CDATA[
            背景：谱图神经网络（SGNNs）在节点分类等任务中表现出色，但在标记数据有限的场景，尤其是异质性图中，难以捕捉最优滤波器形状，导致性能下降。大语言模型（LLMs）的发展为图学习带来新可能。方法：提出利用LLMs估计图的同质性水平，用此全局结构先验指导谱滤波器构建的框架，设计轻量级、即插即用的流程，将少量标记节点对转为自然语言提示让LLM预测同质性比率。效果：在多个基准数据集上，该框架持续提升性能，且计算和金钱成本可忽略不计。
            arXiv:2506.14220v2 Announce Type: replace 
Abstract: Spectral Graph Neural Networks (SGNNs) have achieved remarkable performance in tasks such as node classification due to their ability to learn flexible filters. Typically, these filters are learned under the supervision of downstream tasks, enabling SGNNs to adapt to diverse structural patterns. However, in scenarios with limited labeled data, SGNNs often struggle to capture the optimal filter shapes, resulting in degraded performance, especially on graphs with heterophily. Meanwhile, the rapid progress of Large Language Models (LLMs) has opened new possibilities for enhancing graph learning without modifying graph structure or requiring task-specific training. In this work, we propose a novel framework that leverages LLMs to estimate the homophily level of a graph and uses this global structural prior to guide the construction of spectral filters. Specifically, we design a lightweight and plug-and-play pipeline where a small set of labeled node pairs is formatted as natural language prompts for the LLM, which then predicts the graph's homophily ratio. This estimated value informs the spectral filter basis, enabling SGNNs to adapt more effectively to both homophilic and heterophilic structures. Extensive experiments on multiple benchmark datasets demonstrate that our LLM-assisted spectral framework consistently improves performance over strong SGNN baselines. Importantly, this enhancement incurs negligible computational and monetary cost, making it a practical solution for real-world graph applications.
        ]]></description>
    </item>
    <item>
        <title>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study</title>
        <link>https://arxiv.org/abs/2506.19794</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.19794v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Ningyu Zhang, Huajun Chen</dc:creator>
        <description><![CDATA[
            背景：大语言模型在自动化数据分析任务上有潜力，但开源模型在推理密集场景存在局限。方法：通过策划多样化、真实场景的种子数据集，从数据理解、代码生成和战略规划三个核心维度评估模型行为，分析得出关键结论，并据此开发数据合成方法。效果：显著提升了开源大语言模型的分析推理能力，代码可在https://github.com/zjunlp/DataMind获取。
            arXiv:2506.19794v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.
        ]]></description>
    </item>
    <item>
        <title>STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking</title>
        <link>https://arxiv.org/abs/2507.03674</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.03674v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tek Raj Chhetri, Yibei Chen, Puja Trivedi, Dorota Jarecka, Saif Haobsh, Patrick Ray, Lydia Ng, Satrajit S. Ghosh</dc:creator>
        <description><![CDATA[
            从非结构化源中提取结构化信息对科学发现和知识合成至关重要，大语言模型虽有相关能力，但在特定领域效果不佳且跨任务和领域可迁移性差。为此，本文提出基于大语言模型的模块化、与任务无关的开源框架StructSense。它借助本体中编码的特定领域符号知识，结合自我评估判断器的反馈循环和人工参与机制。通过在神经科学信息提取任务中的应用表明，该框架能克服领域敏感性和跨任务泛化不足的局限。
            arXiv:2507.03674v2 Announce Type: replace 
Abstract: The ability to extract structured information from unstructured sources-such as free-text documents and scientific literature-is critical for accelerating scientific discovery and knowledge synthesis. Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, including structured information extraction. However, their effectiveness often diminishes in specialized, domain-specific contexts that require nuanced understanding and expert-level domain knowledge. In addition, existing LLM-based approaches frequently exhibit poor transferability across tasks and domains, limiting their scalability and adaptability. To address these challenges, we introduce StructSense, a modular, task-agnostic, open-source framework for structured information extraction built on LLMs. StructSense is guided by domain-specific symbolic knowledge encoded in ontologies, enabling it to navigate complex domain content more effectively. It further incorporates agentic capabilities through self-evaluative judges that form a feedback loop for iterative refinement, and includes human-in-the-loop mechanisms to ensure quality and validation. We demonstrate that StructSense can overcome both the limitations of domain sensitivity and the lack of cross-task generalizability, as shown through its application to diverse neuroscience information extraction tasks.
        ]]></description>
    </item>
    <item>
        <title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
        <link>https://arxiv.org/abs/2507.17307</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.17307v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang</dc:creator>
        <description><![CDATA[
            思维链（CoT）推理能增强大语言模型解决问题的能力，但会带来较大计算开销。现有加速策略存在局限性，如投机解码在大小模型一致性低时加速有限。本文提出R-Stitch，一种基于置信度的混合解码框架，默认用小语言模型（SLM）生成标记，仅在其置信度低于阈值时调用大语言模型（LLM）。该方法与模型无关、无需训练且兼容标准解码流程。实验表明，在数学推理基准测试中，R - Stitch可将推理延迟降低达85%，且准确率下降可忽略不计。
            arXiv:2507.17307v3 Announce Type: replace 
Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing acceleration strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the LLM only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the LLM on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.
        ]]></description>
    </item>
    <item>
        <title>Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback</title>
        <link>https://arxiv.org/abs/2507.20766</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.20766v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yang Chen, Yufan Shen, Wenxuan Huang, Sheng Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Jiajun Bu, Botian Shi, Yu Qiao</dc:creator>
        <description><![CDATA[
            多模态大语言模型在视觉任务中表现出色，但向深度视觉推理发展时，严重依赖图像 - 文本监督是瓶颈。为此提出“推理 - 渲染 - 视觉反馈”（RRVF）框架，基于“验证不对称性”原理，以相对容易的验证作为强化学习的奖励信号，减少对图像 - 文本监督的依赖。该框架通过推理、渲染和视觉反馈的闭环迭代过程，用GRPO算法端到端优化。在数据图表和网页界面的图像到代码生成任务评估中，RRVF训练的模型优于现有同规模开源模型和监督微调基线，泛化性更好。
            arXiv:2507.20766v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) exhibit impressive performance across various visual tasks. Subsequent investigations into enhancing their visual reasoning abilities have significantly expanded their performance envelope. However, a critical bottleneck in the advancement of MLLMs toward deep visual reasoning is their heavy reliance on curated image-text supervision. To solve this problem, we introduce a novel framework, ``Reasoning-Rendering-Visual-Feedback'' (RRVF), that enables MLLMs to learn complex visual reasoning from only raw images. This framework builds on the ``Asymmetry of Verification'' principle, i.e., verifying the rendered output against the source image is substantially easier than performing deep visual reasoning to generate a faithful, structured representation such as code. We demonstrate that this relative ease provides an ideal reward signal for optimization via Reinforcement Learning (RL), thereby reducing reliance on image-text supervision. RRVF implements a closed-loop iterative process encompassing reasoning, rendering, and visual feedback components, enabling the model to perform complex reasoning, including self-correction through multi-turn interactions. This process is optimized end-to-end using the GRPO algorithm. Extensive evaluations are conducted on image-to-code generation across two diverse domains: data charts and web interfaces. The RRVF-trained model not only outperforms existing similarly sized open-source MLLMs and supervised fine-tuning baselines but also exhibits superior generalization. Notably, the model outperforms the more advanced MLLM used to generate visual feedback during training. %only in arxiv Code is available at https://github.com/L-O-I/RRVF.
        ]]></description>
    </item>
    <item>
        <title>Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs</title>
        <link>https://arxiv.org/abs/2507.21893</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.21893v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Saeed Ghorbani</dc:creator>
        <description><![CDATA[
            背景：现有顺序文本到视觉的管道存在局限。方法：提出Aether Weaver框架，通过紧密集成的协同生成机制，同时合成文本叙述、动态场景图表示、视觉场景和情感音景。核心的大语言模型“讲述者”生成叙述文本和多模态提示，“导演”管理动态场景图，还有叙述弧控制器和情感基调映射器辅助。效果：在多种类型的叙述提示上的定性评估显示，与级联基线方法相比，显著提升了叙述深度、视觉保真度和情感共鸣。
            arXiv:2507.21893v2 Announce Type: replace 
Abstract: We introduce Aether Weaver, a novel, integrated framework for multimodal narrative co-generation that overcomes limitations of sequential text-to-visual pipelines. Our system concurrently synthesizes textual narratives, dynamic scene graph representations, visual scenes, and affective soundscapes, driven by a tightly integrated, co-generation mechanism. At its core, the Narrator, a large language model, generates narrative text and multimodal prompts, while the Director acts as a dynamic scene graph manager, and analyzes the text to build and maintain a structured representation of the story's world, ensuring spatio-temporal and relational consistency for visual rendering and subsequent narrative generation. Additionally, a Narrative Arc Controller guides the high-level story structure, influencing multimodal affective consistency, further complemented by an Affective Tone Mapper that ensures congruent emotional expression across all modalities. Through qualitative evaluations on a diverse set of narrative prompts encompassing various genres, we demonstrate that Aether Weaver significantly enhances narrative depth, visual fidelity, and emotional resonance compared to cascaded baseline approaches. This integrated framework provides a robust platform for rapid creative prototyping and immersive storytelling experiences.
        ]]></description>
    </item>
    <item>
        <title>HGCN(O): A Self-Tuning GCN HyperModel Toolkit for Outcome Prediction in Event-Sequence Data</title>
        <link>https://arxiv.org/abs/2507.22524</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.22524v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Fang Wang, Paolo Ceravolo, Ernesto Damiani</dc:creator>
        <description><![CDATA[
            背景：现有方法在事件序列预测上存在不足。方法：提出HGCN(O)自调优工具包，采用图卷积网络（GCN）模型进行事件序列预测，包含四种GCN架构，通过边权重整合事件序列的多种图表示、节点和图级属性及时间依赖。效果：实验表明GCNConv模型在不平衡数据上表现出色，所有模型在平衡数据上表现稳定，HGCN(O)性能优于传统方法，可应用于预测性业务流程监控等场景。
            arXiv:2507.22524v2 Announce Type: replace 
Abstract: We propose HGCN(O), a self-tuning toolkit using Graph Convolutional Network (GCN) models for event sequence prediction. Featuring four GCN architectures (O-GCN, T-GCN, TP-GCN, TE-GCN) across the GCNConv and GraphConv layers, our toolkit integrates multiple graph representations of event sequences with different choices of node- and graph-level attributes and in temporal dependencies via edge weights, optimising prediction accuracy and stability for balanced and unbalanced datasets. Extensive experiments show that GCNConv models excel on unbalanced data, while all models perform consistently on balanced data. Experiments also confirm the superior performance of HGCN(O) over traditional approaches. Applications include Predictive Business Process Monitoring (PBPM), which predicts future events or states of a business process based on event logs.
        ]]></description>
    </item>
    <item>
        <title>SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity</title>
        <link>https://arxiv.org/abs/2507.23095</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2507.23095v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Ishani Mondal, Meera Bharadwaj, Ayush Roy, Aparna Garimella, Jordan Lee Boyd-Graber</dc:creator>
        <description><![CDATA[
            背景：现有模型在编辑时多进行局部编辑，难以保证全局一致性。方法：提出SMART - Editor框架用于结构化（海报、网站）和非结构化（自然图像）领域的布局与内容编辑，采用Reward - Refine推理时奖励引导细化方法和RewardDPO训练时偏好优化方法保证全局连贯，还引入SMARTEdit - Bench进行评估。效果：优于InstructPix2Pix和HIVE等基线模型，RewardDPO在结构化场景增益达15%，Reward - Refine在自然图像上有优势，经自动和人工评估证明有效。
            arXiv:2507.23095v2 Announce Type: replace 
Abstract: We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits.
        ]]></description>
    </item>
    <item>
        <title>ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network</title>
        <link>https://arxiv.org/abs/2508.00429</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.00429v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang</dc:creator>
        <description><![CDATA[
            背景：图神经网络（GNNs）通过预定义聚合机制在节点间传播信息，但存在无法处理节点信息不平衡、忽略图全局语义关系的局限。方法：提出检索增强图智能网络（ReaGAN），赋予每个节点自主决策能力，节点作为智能体基于内部记忆规划行动，实现节点级规划和自适应消息传播，还利用检索增强生成（RAG）让节点获取语义相关内容，构建全局关系。效果：在少样本上下文设置中，使用冻结大语言模型骨干且无需微调就取得有竞争力的性能。
            arXiv:2508.00429v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.
        ]]></description>
    </item>
    <item>
        <title>FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models</title>
        <link>https://arxiv.org/abs/2508.01055</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.01055v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xuan Liu, Siru Ouyang, Xianrui Zhong, Jiawei Han, Huimin Zhao</dc:creator>
        <description><![CDATA[
            大语言模型在化学领域受关注，但现有数据集多关注分子级属性预测，忽视细粒度官能团（FG）信息。本文引入含62.5万条带官能团信息的分子属性推理问题的数据集FGBench，官能团精准标注定位，涵盖回归和分类任务。基准测试显示，现有大语言模型在FG级属性推理上表现不佳。该数据集构建方法有望为生成新问答对提供框架，提升大语言模型对分子结构 - 属性关系的理解，代码可在指定链接获取。
            arXiv:2508.01055v2 Announce Type: replace 
Abstract: Large language models (LLMs) have gained significant attention in chemistry. However, most existing datasets center on molecular-level property prediction and overlook the role of fine-grained functional group (FG) information. Incorporating FG-level data can provide valuable prior knowledge that links molecular structures with textual descriptions, which can be used to build more interpretable, structure-aware LLMs for reasoning on molecule-related tasks. Moreover, LLMs can learn from such fine-grained information to uncover hidden relationships between specific functional groups and molecular properties, thereby advancing molecular design and drug discovery. Here, we introduce FGBench, a dataset comprising 625K molecular property reasoning problems with functional group information. Functional groups are precisely annotated and localized within the molecule, which ensures the dataset's interoperability thereby facilitating further multimodal applications. FGBench includes both regression and classification tasks on 245 different functional groups across three categories for molecular property reasoning: (1) single functional group impacts, (2) multiple functional group interactions, and (3) direct molecular comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the results indicate that current LLMs struggle with FG-level property reasoning, highlighting the need to enhance reasoning capabilities in LLMs for chemistry tasks. We anticipate that the methodology employed in FGBench to construct datasets with functional group-level information will serve as a foundational framework for generating new question-answer pairs, enabling LLMs to better understand fine-grained molecular structure-property relationships. The dataset and evaluation code are available at https://github.com/xuanliugit/FGBench.
        ]]></description>
    </item>
    <item>
        <title>Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</title>
        <link>https://arxiv.org/abs/2508.01696</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.01696v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）可提升大语言模型能力，但当前RAG方法在生成时难以充分利用知识，模型内部参数知识与外部检索知识协同有限。方法：提出协作智能体链框架，先引入CoCoA - zero多智能体RAG框架进行条件知识归纳和答案推理，在此基础上开发CoCoA长链训练策略微调大语言模型。效果：实验表明CoCoA - zero和CoCoA在开放领域和多跳问答任务上表现优异。
            arXiv:2508.01696v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.
        ]]></description>
    </item>
    <item>
        <title>Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</title>
        <link>https://arxiv.org/abs/2508.01191</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.01191v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chengshuai Zhao, Zhen Tan, Pingchuan Ma, Dawei Li, Bohan Jiang, Yancheng Wang, Yingzhen Yang, Huan Liu</dc:creator>
        <description><![CDATA[
            背景：思维链（CoT）提示能提升大语言模型（LLM）在多任务中的表现，但有研究认为其推理可能较表面。方法：从数据分布视角研究CoT推理，通过任务、长度和格式三个维度剖析，设计DataAlchemy环境从零训练LLM并在不同分布条件下测试。效果：研究发现当超出训练分布时，CoT推理会失效，这有助于深入理解其失败原因，凸显实现真正且可泛化推理仍面临挑战。
            arXiv:2508.01191v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.
        ]]></description>
    </item>
    <item>
        <title>MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh</title>
        <link>https://arxiv.org/abs/2508.01242</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.01242v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shuangkang Fang, I-Chao Shen, Yufeng Wang, Yi-Hsuan Tsai, Yi Yang, Shuchang Zhou, Wenrui Ding, Takeo Igarashi, Ming-Hsuan Yang</dc:creator>
        <description><![CDATA[
            现有方法在处理3D网格数据时存在数据集规模受限、序列化时丢失结构信息等问题。为此，本文提出MeshLLM框架，采用Primitive - Mesh分解策略，将3D网格划分为有结构意义的子单元，创建了包含1500k+样本的大规模数据集，比以往方法大近50倍。还提出从顶点推断面连接和局部网格组装训练策略，增强大语言模型捕捉网格拓扑和空间结构的能力。实验表明，MeshLLM在网格生成质量和形状理解上优于LLaMA - Mesh，在处理文本序列化3D网格方面潜力巨大。
            arXiv:2508.01242v2 Announce Type: replace-cross 
Abstract: We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50 times larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes.
        ]]></description>
    </item>
    <item>
        <title>Adaptive Knowledge Distillation for Device-Directed Speech Detection</title>
        <link>https://arxiv.org/abs/2508.02801</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02801v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Hyung Gun Chi, Florian Pesce, Wonil Chang, Oggi Rudovic, Arturo Argueta, Stefan Braun, Vineet Garg, Ahmed Hussen Abdelaziz</dc:creator>
        <description><![CDATA[
            这是一篇关于音频分类的论文。设备定向语音检测（DDSD）是将用户对语音助手的查询与背景语音或旁侧对话分离的二分类任务，对实现自然的用户体验很重要。为此，提出一种自适应知识蒸馏（KD）方法，从ASR大型预训练声学编码器（教师）的通用表示中转移知识，在（冻结的）教师编码器上应用特定任务适配器，并与学生模型在DDSD上联合训练。该方法在关键词和无关键词调用方面分别使等错误率提升了26%和19%，且在基于Transformer和Conformer的模型架构上具有泛化性。
            arXiv:2508.02801v1 Announce Type: new 
Abstract: Device-directed speech detection (DDSD) is a binary classification task that separates the user's queries to a voice assistant (VA) from background speech or side conversations. This is important for achieving naturalistic user experience. To this end, we propose knowledge distillation (KD) to enhance DDSD accuracy while ensuring efficient deployment. Specifically, we introduce a novel adaptive KD method that transfers knowledge from general representations of an ASR large pre-trained acoustic encoder (teacher). We apply task-specific adapters, on top of the (frozen) teacher encoder, trained jointly with the student model on DDSD. We demonstrate that the proposed adaptive KD outperforms the student model without distillation in the keyword and keyword-free (follow-up) invocations, with an improvement of +26% and +19% in terms of Equal Error Rate, respectively. We also show that this approach generalizes across the transformer and conformer-based model architectures.
        ]]></description>
    </item>
    <item>
        <title>SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec</title>
        <link>https://arxiv.org/abs/2508.02849</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02849v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chunyu Qiang, Haoyu Wang, Cheng Gong, Tianrui Wang, Ruibo Fu, Tao Wang, Ruilong Chen, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Longbiao Wang, Jianwu Dang, Jianhua Tao</dc:creator>
        <description><![CDATA[
            语音编解码器是统一语音和文本语言模型的关键桥梁，现有编解码方法在语义编码上存在诸多挑战。为此，本文提出SecoustiCodec，一种跨模态对齐的低比特率流式语音编解码器。它在单码本空间中分离语义和副语言信息，引入副语言编码确保语义完整和重建保真度，提出基于VAE和FSQ的仅语义高效量化方法、基于对比学习的语义分离方法及声学约束多阶段优化策略。该编解码器在0.27/1 kbps下实现了1.77/2.58的SOTA重建质量（PESQ）。
            arXiv:2508.02849v1 Announce Type: new 
Abstract: Speech codecs serve as a crucial bridge in unifying speech and text language models. Existing codec methods face several challenges in semantic encoding, such as residual paralinguistic information (e.g., timbre, emotion), insufficient semantic completeness, limited reconstruction capability, and lack of support for streaming. To address these challenges, we propose SecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that disentangles semantic and paralinguistic information in a single-codebook space. To ensure semantic completeness and reconstruction fidelity, paralinguistic encoding is introduced to bridge the information gap between semantic and acoustic encoding. A semantic-only efficient quantization method based on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is proposed. This approach alleviates the long-tail distribution problem of tokens while maintaining high codebook utilization. A semantic disentanglement method based on contrastive learning is proposed, which aligns text and speech in a joint multimodal frame-level space, effectively removing paralinguistic information from semantic encoding. An acoustic-constrained multi-stage optimization strategy is proposed to ensure robust and stable convergence. Figure~\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA (state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps. The code and model weights for SecoustiCodec will be open-sourced upon the completion of the peer-review process. We've open-sourced SecoustiCodec's demo, code, and model weights.
        ]]></description>
    </item>
    <item>
        <title>Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback</title>
        <link>https://arxiv.org/abs/2508.03123</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03123v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jingyi Chen, Ju Seung Byun, Micha Elsner, Pichao Wang, Andrew Perrault</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成中基础音频生成模型的研究。背景是扩散模型生成语音保真度高，但因去噪步骤长、难以建模语调节奏，用于实时场景效率低。方法是提出Diffusion Loss-Guided Policy Optimization（DLPO），将原训练损失融入奖励函数，以自然度分数为反馈优化奖励。效果显著，在WaveGrad 2上评估，客观指标（UTMOS 3.65，NISQA 4.02）和主观评价均提升，67%的情况下用户更偏好DLPO生成的音频。
            arXiv:2508.03123v1 Announce Type: new 
Abstract: Diffusion models produce high-fidelity speech but are inefficient for real-time use due to long denoising steps and challenges in modeling intonation and rhythm. To improve this, we propose Diffusion Loss-Guided Policy Optimization (DLPO), an RLHF framework for TTS diffusion models. DLPO integrates the original training loss into the reward function, preserving generative capabilities while reducing inefficiencies. Using naturalness scores as feedback, DLPO aligns reward optimization with the diffusion model's structure, improving speech quality. We evaluate DLPO on WaveGrad 2, a non-autoregressive diffusion-based TTS model. Results show significant improvements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective evaluations, with DLPO audio preferred 67\% of the time. These findings demonstrate DLPO's potential for efficient, high-quality diffusion TTS in real-time, resource-limited settings.
        ]]></description>
    </item>
    <item>
        <title>SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering</title>
        <link>https://arxiv.org/abs/2508.03448</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03448v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jan Melechovsky, Ambuj Mehrish, Dorien Herremans</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。背景是音乐录音常存在音质问题，传统需用专门工具手动校正。方法是提出首个统一生成模型SonicMaster，可通过文本控制解决多种音频伪影，还构建SonicMaster数据集，利用流匹配生成训练范式，依据文本提示学习音频转换。效果是客观音频质量指标显示其显著提升音质，主观听力测试表明听众更青睐增强后的输出。 
            arXiv:2508.03448v1 Announce Type: new 
Abstract: Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach.
        ]]></description>
    </item>
    <item>
        <title>EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable Text-to-Speech via Activation Steering</title>
        <link>https://arxiv.org/abs/2508.03543</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03543v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Tianxin Xie, Shan Yang, Chenxing Li, Dong Yu, Li Liu</dc:creator>
        <description><![CDATA[
            近年来文本到语音（TTS）取得进展，但现有TTS系统情感控制粗糙、不稳定且需大量数据训练。为此提出EmoSteer - TTS，一种免训练方法。先发现修改基于流匹配的TTS模型内部激活子集可改变合成语音情感，进而开发免训练算法，包括激活提取、情感令牌搜索和推理时引导，并构建情感语音数据集。实验表明，该方法能对语音情感进行细粒度、可解释和连续控制，超越现有技术，是首个在TTS中实现免训练和连续细粒度情感控制的方法。
            arXiv:2508.03543v1 Announce Type: new 
Abstract: Text-to-speech (TTS) has shown great progress in recent years. However, most existing TTS systems offer only coarse and rigid emotion control, typically via discrete emotion labels or a carefully crafted and detailed emotional text prompt, making fine-grained emotion manipulation either inaccessible or unstable. These models also require extensive, high-quality datasets for training. To address these limitations, we propose EmoSteer-TTS, a novel training-free approach, to achieve fine-grained speech emotion control (conversion, interpolation, erasure) by activation steering. We first empirically observe that modifying a subset of the internal activations within a flow matching-based TTS model can effectively alter the emotional tone of synthesized speech. Building on this insight, we then develop a training-free and efficient algorithm, including activation extraction, emotional token searching, and inference-time steering, which can be seamlessly integrated into a wide range of pretrained models (e.g., F5-TTS, CosyVoice2, and E2-TTS). In addition, to derive effective steering vectors, we construct a curated emotional speech dataset with diverse speakers. Extensive experiments demonstrate that EmoSteer-TTS enables fine-grained, interpretable, and continuous control over speech emotion, outperforming the state-of-the-art (SOTA). To the best of our knowledge, this is the first method that achieves training-free and continuous fine-grained emotion control in TTS.
        ]]></description>
    </item>
    <item>
        <title>How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes</title>
        <link>https://arxiv.org/abs/2508.02905</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02905v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Mahnoor Fatima Saad, Ziad Al-Halah</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。背景是探究室内场景材质变化对声音的影响。方法上，提出了一种基于编码器 - 解码器的新方法，从视听观察中编码场景关键属性，根据用户指定的材质配置生成目标房间脉冲响应（RIR），还创建了新基准数据集“声学仙境数据集”。效果方面，该模型能根据不同材质配置生成多样 RIR，有效编码材质信息，生成高保真 RIR，优于多个基线和先进方法。
            arXiv:2508.02905v1 Announce Type: cross 
Abstract: How would the sound in a studio change with a carpeted floor and acoustic tiles on the walls? We introduce the task of material-controlled acoustic profile generation, where, given an indoor scene with specific audio-visual characteristics, the goal is to generate a target acoustic profile based on a user-defined material configuration at inference time. We address this task with a novel encoder-decoder approach that encodes the scene's key properties from an audio-visual observation and generates the target Room Impulse Response (RIR) conditioned on the material specifications provided by the user. Our model enables the generation of diverse RIRs based on various material configurations defined dynamically at inference time. To support this task, we create a new benchmark, the Acoustic Wonderland Dataset, designed for developing and evaluating material-aware RIR prediction methods under diverse and challenging settings. Our results demonstrate that the proposed model effectively encodes material information and generates high-fidelity RIRs, outperforming several baselines and state-of-the-art methods.
        ]]></description>
    </item>
    <item>
        <title>READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</title>
        <link>https://arxiv.org/abs/2508.03457</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.03457v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haotian Wang, Yuzhe Weng, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Jianqing Gao, Qingfeng Liu</dc:creator>
        <description><![CDATA[
            这是一篇关于音频驱动的会说话头部生成的研究。背景是扩散模型虽推动了该领域发展，但推理速度慢限制了其实用性。方法上，提出READ框架，先通过时间VAE学习时空高度压缩的视频潜在空间，用预训练的语音自动编码器生成对应语音潜在代码，再用A2V - DiT骨干网络建模，还提出异步噪声调度器。效果表明，READ能显著减少运行时间，生成有竞争力的会说话头部视频，在质量和速度间取得平衡，长时间生成时指标稳定。
            arXiv:2508.03457v1 Announce Type: cross 
Abstract: The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, the first real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference process of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation.
        ]]></description>
    </item>
    <item>
        <title>Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis</title>
        <link>https://arxiv.org/abs/2504.10352</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.10352v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yifan Yang, Shujie Liu, Jinyu Li, Yuxuan Hu, Haibin Wu, Hui Wang, Jianwei Yu, Lingwei Meng, Haiyang Sun, Yanqing Liu, Yan Lu, Kai Yu, Xie Chen</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。现有零样本文本转语音系统中，自回归模型生成慢且缺乏时长可控性，非自回归模型缺乏时序建模且设计复杂。为此，论文提出伪自回归编解码器语言建模方法，结合自回归和非自回归建模优势。在此基础上，提出两阶段TTS系统PALLE。实验表明，在LibriTTS上训练的PALLE在语音质量、说话人相似度和可懂度上优于在大规模数据上训练的先进系统，推理速度快达10倍。
            arXiv:2504.10352v3 Announce Type: replace 
Abstract: Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.
        ]]></description>
    </item>
    <item>
        <title>AudioGenie: A Training-Free Multi-Agent Framework for Diverse Multimodality-to-Multiaudio Generation</title>
        <link>https://arxiv.org/abs/2505.22053</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2505.22053v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yan Rong, Jinting Wang, Guangzhi Lei, Shan Yang, Li Liu</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。多模态到多音频（MM2MA）生成面临高质量配对数据集稀缺、缺乏鲁棒多任务学习框架等问题，现有多智能体系统应用于该任务存在理解不足等挑战。为此，论文提出训练无关的多智能体系统AudioGenie，采用双层架构，生成团队设计了细粒度任务分解等模块，监督团队确保时空一致性。此外还构建了首个MM2MA任务基准MA - Bench。实验表明，AudioGenie在8个任务的9个指标上达到或接近最优，用户研究也验证了其有效性。
            arXiv:2505.22053v2 Announce Type: replace 
Abstract: Multimodality-to-Multiaudio (MM2MA) generation faces significant challenges in synthesizing diverse and contextually aligned audio types (e.g., sound effects, speech, music, and songs) from multimodal inputs (e.g., video, text, images), owing to the scarcity of high-quality paired datasets and the lack of robust multi-task learning frameworks. Recently, multi-agent system shows great potential in tackling the above issues. However, directly applying it to MM2MA task presents three critical challenges: (1) inadequate fine-grained understanding of multimodal inputs (especially for video), (2) the inability of single models to handle diverse audio events, and (3) the absence of self-correction mechanisms for reliable outputs. To this end, we propose AudioGenie, a novel training-free multi-agent system featuring a dual-layer architecture with a generation team and a supervisor team. For the generation team, a fine-grained task decomposition and an adaptive Mixture-of-Experts (MoE) collaborative entity are designed for detailed comprehensive multimodal understanding and dynamic model selection, and a trial-and-error iterative refinement module is designed for self-correction. The supervisor team ensures temporal-spatial consistency and verifies outputs through feedback loops. Moreover, we build MA-Bench, the first benchmark for MM2MA tasks, comprising 198 annotated videos with multi-type audios. Experiments demonstrate that our AudioGenie achieves state-of-the-art (SOTA) or comparable performance across 9 metrics in 8 tasks. User study further validates the effectiveness of our method in terms of quality, accuracy, alignment, and aesthetic. The project website with audio samples can be found at https://audiogenie.github.io/.
        ]]></description>
    </item>
    <item>
        <title>AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation</title>
        <link>https://arxiv.org/abs/2508.00733</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.00733v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Le Wang, Jun Wang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai</dc:creator>
        <description><![CDATA[
            这是一篇关于音频生成的论文。背景是缺乏能与视频同步生成音频等的统一方法。方法上，提出基于多模态扩散变压器的AudioGen - Omni，采用新的联合训练范式整合大规模视频 - 文本 - 音频语料，用统一编码器编码输入，以AdaLN联合注意力机制融合表征。效果显著，提升了音频质量、语义对齐和唇同步准确性，在文本到音频等任务上达最优，8秒音频推理时间1.91秒，效率和通用性俱佳。
            arXiv:2508.00733v3 Announce Type: replace 
Abstract: We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and songs coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both sung and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality.
        ]]></description>
    </item>
    <item>
        <title>Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers</title>
        <link>https://arxiv.org/abs/2508.02175</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2508.02175v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Liang Lin, Miao Yu, Kaiwen Luo, Yibo Zhang, Lilan Peng, Dexian Wang, Xuehai Tang, Yuanhe Zhang, Xikang Yang, Zhenhong Zhou, Kun Wang, Yang Liu</dc:creator>
        <description><![CDATA[
            随着音频大语言模型（ALLMs）成为语音处理有力工具，其安全问题亟待关注。现有研究多聚焦文本和视觉安全，音频特性带来新挑战。本文提出“Hidden in the Noise（HIN）”后门攻击框架，通过修改原始音频波形、注入特定噪声等引入触发模式。同时开发AudioSafe基准评估ALLMs鲁棒性。实验表明，现有ALLMs存在关键漏洞，如环境噪声等音频特征攻击成功率超90%，对不同声学特征敏感度不同，且攻击隐蔽性强。
            arXiv:2508.02175v2 Announce Type: replace 
Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech processing, their safety implications demand urgent attention. While considerable research has explored textual and vision safety, audio's distinct characteristics present significant challenges. This paper first investigates: Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In response to this issue, we introduce Hidden in the Noise (HIN), a novel backdoor attack framework designed to exploit subtle, audio-specific features. HIN applies acoustic modifications to raw audio waveforms, such as alterations to temporal dynamics and strategic injection of spectrally tailored noise. These changes introduce consistent patterns that an ALLM's acoustic feature encoder captures, embedding robust triggers within the audio stream. To evaluate ALLM robustness against audio-feature-based triggers, we develop the AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments on AudioSafe and three established safety datasets reveal critical vulnerabilities in existing ALLMs: (I) audio features like environment noise and speech rate variations achieve over 90% average attack success rate. (II) ALLMs exhibit significant sensitivity differences across acoustic features, particularly showing minimal response to volume as a trigger, and (III) poisoned sample inclusion causes only marginal loss curve fluctuations, highlighting the attack's stealth.
        ]]></description>
    </item>
    <item>
        <title>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</title>
        <link>https://arxiv.org/abs/2506.04134</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.04134v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jinting Wang, Shan Yang, Chenxing Li, Dong Yu, Li Liu</dc:creator>
        <description><![CDATA[
            这是一篇关于中文提示语视频到语音生成的论文。现有方法在提示语视频转语音任务中存在误差传播、时空不对齐等问题，直接生成音频又面临多模态复杂和数据有限的挑战。为此，研究者提出了UniCUE框架，它无需中间文本，通过整合理解任务，利用细粒度视觉语义线索指导语音生成。同时构建了大规模数据集UniCUE - HI。实验表明，该框架在多个评估指标上达到了最优性能。
            arXiv:2506.04134v3 Announce Type: replace-cross 
Abstract: Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.
        ]]></description>
    </item>
    <item>
        <title>What Makes a Good Speech Tokenizer for LLM-Centric Speech Generation? A Systematic Study</title>
        <link>https://arxiv.org/abs/2506.12537</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2506.12537v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaoran Fan, Zhichao Sun, Yangfan Gao, Jingfei Xiong, Hang Yan, Yifei Cao, Jiajun Sun, Shuo Li, Zhihao Zhang, Zhiheng Xi, Yuhao Zhou, Senjie Jin, Changhao Jiang, Junjie Ye, Ming Zhang, Rui Zheng, Zhenhua Han, Yunke Zhang, Demei Yan, Shaokang Dong, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang</dc:creator>
        <description><![CDATA[
            语音语言模型在统一语音与文本理解和生成方面有前景，但在跨模态对齐和高质量语音生成上存在挑战。本文系统研究以大语言模型为中心的语音语言模型中语音分词器设计的作用，对比不同类型分词器，发现解耦分词显著提升对齐和合成质量。为解决语音与文本信息密度不匹配问题，引入多令牌预测，使解码速度提升至12倍，字错误率从6.07降至3.01。还提出说话人感知生成范式并引入新基准，实验表明方法提升了知识理解和说话人一致性。
            arXiv:2506.12537v2 Announce Type: replace-cross 
Abstract: Speech-language models (SLMs) offer a promising path toward unifying speech and text understanding and generation. However, challenges remain in achieving effective cross-modal alignment and high-quality speech generation. In this work, we systematically investigate the role of speech tokenizer designs in LLM-centric SLMs, augmented by speech heads and speaker modeling. We compare coupled, semi-decoupled, and fully decoupled speech tokenizers under a fair SLM framework and find that decoupled tokenization significantly improves alignment and synthesis quality. To address the information density mismatch between speech and text, we introduce multi-token prediction (MTP) into SLMs, enabling each hidden state to decode multiple speech tokens. This leads to up to 12$\times$ faster decoding and a substantial drop in word error rate (from 6.07 to 3.01). Furthermore, we propose a speaker-aware generation paradigm and introduce RoleTriviaQA, a large-scale role-playing knowledge QA benchmark with diverse speaker identities. Experiments demonstrate that our methods enhance both knowledge understanding and speaker consistency.
        ]]></description>
    </item>
</channel>
</rss>