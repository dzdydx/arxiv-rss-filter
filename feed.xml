<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
<channel>
    <title>arXiv Paper Filter</title>
    <link>https://github.com/None/arxiv_filter</link>
    <description>根据研究兴趣筛选的arXiv论文</description>
    <atom:link href="https://raw.githubusercontent.com/None/arxiv_filter/main/feed.xml" rel="self" type="application/rss+xml" />
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 23 Apr 2025 12:14:39 +0800</lastBuildDate>
    <managingEditor>None@github.com</managingEditor>
    <pubDate>Wed, 23 Apr 2025 12:14:39 +0800</pubDate>
    <skipDays>
        <day>Saturday</day>
        <day>Sunday</day>
    </skipDays>

    <item>
        <title>Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)</title>
        <link>https://arxiv.org/abs/2504.15349</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15349v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>William Bruns</dc:creator>
        <description><![CDATA[
            背景：人类具备组合泛化能力，而Transformer模型在COGS基准测试的一些结构泛化任务上准确率为0%。方法：使用受限访问序列处理（RASP）这一与Transformer等价的编程语言，构建Transformer编解码器模型，采用词级标记及“嵌入”层，应用19条注意力头兼容的扁平模式匹配规则，结合介词短语和句子补语处理逻辑。效果：该模型在ReCOGS测试集上语义完全匹配度达100%，除obj_pp_to_subj_pp泛化分割为92%外，其他分割SEM达100%，在递归任务上语义和字符串完全匹配度也达100%。
            arXiv:2504.15349v1 Announce Type: new 
Abstract: Humans understand new combinations of words encountered if they are combinations of words recognized from different contexts, an ability called Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020) arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted Access Sequence Processing (RASP), a Transformer-equivalent programming language, to prove by construction that a Transformer encoder-decoder can perform the semantically equivalent ReCOGS_pos (Wu et al., 2024) arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore, our RASP model shows the ReCOGS_pos task does not require a hierarchical or tree-structured solution: we use word-level tokens with an "embedding" layer that tags with possible parts of speech, applying just once per encoder pass 19 attention-head compatible flat pattern-matching rules, shown using grammar coverage (Zeller et al., 2023) to be learnable from the training data, plus general prepositional phrase (pp) handling and sentential complement (cp) handling logic, and output the next logical form (LF) token (repeating until the LF is complete). The model does not apply recursive, tree-structured rules like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact match on pp recursion, cp recursion using the decoder loop.
        ]]></description>
    </item>
    <item>
        <title>LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception</title>
        <link>https://arxiv.org/abs/2504.15362</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15362v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuan-Hong Liao, Sven Elflein, Liu He, Laura Leal-Taix\'e, Yejin Choi, Sanja Fidler, David Acuna</dc:creator>
        <description><![CDATA[
            背景：长思维链在数学和代码等推理任务中能提升性能，但在感知任务中的作用较少探索。方法：提出LongPerceptualThoughts，创建含3万条长思维轨迹的合成数据集，采用三阶段数据合成框架，从图像描述生成问题，从视觉语言模型提取简单思维链，再扩展为长思维链。效果：在与强指令调优的7B模型对照实验中，优于现有视觉推理数据生成方法，训练后的模型在5个视觉基准测试中平均提升3.4分，在V* Bench提升11.8分，在文本推理基准MMLU - Pro提升2分。
            arXiv:2504.15362v1 Announce Type: new 
Abstract: Recent reasoning models through test-time scaling have demonstrated that long chain-of-thoughts can unlock substantial performance boosts in hard reasoning tasks such as math and code. However, the benefit of such long thoughts for system-2 reasoning is relatively less explored in other domains such as perceptual tasks where shallower, system-1 reasoning seems sufficient. In this paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K long-thought traces for perceptual tasks. The key challenges in synthesizing elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models are not yet equipped with such thinking behavior and that it is not straightforward to build a reliable process verifier for perceptual tasks. Thus, we propose a novel three-stage data synthesis framework that first synthesizes verifiable multiple-choice questions from dense image descriptions, then extracts simple CoTs from VLMs for those verifiable problems, and finally expands those simple thoughts to elaborate long thoughts via frontier reasoning models. In controlled experiments with a strong instruction-tuned 7B model, we demonstrate notable improvements over existing visual reasoning data-generation methods. Our model, trained on the generated dataset, achieves an average +3.4 points improvement over 5 vision-centric benchmarks, including +11.8 points on V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves performance on the text reasoning benchmark, MMLU-Pro, by +2 points.
        ]]></description>
    </item>
    <item>
        <title>Event2Vec: Processing neuromorphic events directly by representations in vector space</title>
        <link>https://arxiv.org/abs/2504.15371</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15371v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Wei Fang, Priyadarshini Panda</dc:creator>
        <description><![CDATA[
            背景：神经形态事件相机相比传统相机有诸多优势，但输出的事件与主流方法不兼容，现有解决方法存在不足。方法：受词向量成功的启发，总结单词和事件的相似性，提出首个事件向量（event2vec）表示。效果：在ASL - DVS数据集分类任务上验证，相比以往基于图、图像、体素的表示，在参数效率、准确性和速度上表现出色，还能将事件与自然语言处理领域对齐，为融入大语言和多模态模型带来前景。
            arXiv:2504.15371v1 Announce Type: new 
Abstract: The neuromorphic event cameras have overwhelming advantages in temporal resolution, power efficiency, and dynamic range compared to traditional cameras. However, the event cameras output asynchronous, sparse, and irregular events, which are not compatible with mainstream computer vision and deep learning methods. Various methods have been proposed to solve this issue but at the cost of long preprocessing procedures, losing temporal resolutions, or being incompatible with massively parallel computation. Inspired by the great success of the word to vector, we summarize the similarities between words and events, then propose the first event to vector (event2vec) representation. We validate event2vec on classifying the ASL-DVS dataset, showing impressive parameter efficiency, accuracy, and speed than previous graph/image/voxel-based representations. Beyond task performance, the most attractive advantage of event2vec is that it aligns events to the domain of natural language processing, showing the promising prospect of integrating events into large language and multimodal models. Our codes, models, and training logs are available at https://github.com/fangwei123456/event2vec.
        ]]></description>
    </item>
    <item>
        <title>Dynamic Early Exit in Reasoning Models</title>
        <link>https://arxiv.org/abs/2504.15895</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15895v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, Weiping Wang</dc:creator>
        <description><![CDATA[
            背景：大型推理语言模型依赖测试时扩展长思维链解决复杂任务，但长思维链过度推理会降低效率且有精度损失风险。方法：提出一种简单有效的方法，让大语言模型在生成时通过提前退出自截断思维链序列，在潜在推理转换点监测模型行为，当模型对试验答案有高置信度时动态终止下一推理链生成，无需额外训练。效果：在多个推理基准测试中，该方法在deepseek系列推理大模型上平均将思维链序列长度减少31% - 43%，精度提升1.7% - 5.7%。
            arXiv:2504.15895v1 Announce Type: new 
Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,"Wait" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024 show that the proposed method is consistently effective on deepseek-series reasoning LLMs, reducing the length of CoT sequences by an average of 31% to 43% while improving accuracy by 1.7% to 5.7%.
        ]]></description>
    </item>
    <item>
        <title>SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning</title>
        <link>https://arxiv.org/abs/2504.15900</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15900v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li</dc:creator>
        <description><![CDATA[
            背景：强化学习可提升大语言模型推理能力，但在音频 - 语言推理中的应用待探索。方法：将GRPO框架扩展到大型音频 - 语言模型，构建32k样本选择题语料库，采用两阶段策略，先在结构化和非结构化思维链上进行监督微调，再进行课程引导的GRPO，比较不同推理形式。效果：结构化音频推理模型SARI较基础模型Qwen2 - Audio - 7B - Instruct平均准确率提升16.35%，基于Qwen2.5 - Omni的变体在MMAU测试 - 迷你基准上达67.08%的先进水平。
            arXiv:2504.15900v1 Announce Type: new 
Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to "think before answering." Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.
        ]]></description>
    </item>
    <item>
        <title>ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion</title>
        <link>https://arxiv.org/abs/2504.15920</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15920v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiang Li, Haobing Liu, Jianpeng Qi, Yuan Cao, Guoqing Chao, Yanwei Yu</dc:creator>
        <description><![CDATA[
            背景：图神经网络（GNNs）在图任务中表现出色，但存在过平滑和可扩展性问题。方法：提出ScaleGNN框架，通过自适应高阶特征融合模块学习各阶邻居矩阵相对信息，引入基于局部贡献得分的高阶冗余特征掩码机制，保留相关邻居，还采用低阶增强特征聚合自适应整合高低阶特征。效果：在真实数据集上实验表明，该方法在准确率和计算效率上均优于现有GNN模型。
            arXiv:2504.15920v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across various graph-based tasks by effectively capturing relational information between nodes. These models rely on iterative message passing to propagate node features, enabling nodes to aggregate information from their neighbors. Recent research has significantly improved the message-passing mechanism, enhancing GNN scalability on large-scale graphs. However, GNNs still face two main challenges: over-smoothing, where excessive message passing results in indistinguishable node representations, especially in deep networks incorporating high-order neighbors; and scalability issues, as traditional architectures suffer from high model complexity and increased inference time due to redundant information aggregation. This paper proposes a novel framework for large-scale graphs named ScaleGNN that simultaneously addresses both challenges by adaptively fusing multi-level graph features. We first construct neighbor matrices for each order, learning their relative information through trainable weights through an adaptive high-order feature fusion module. This allows the model to selectively emphasize informative high-order neighbors while reducing unnecessary computational costs. Additionally, we introduce a High-order redundant feature masking mechanism based on a Local Contribution Score (LCS), which enables the model to retain only the most relevant neighbors at each order, preventing redundant information propagation. Furthermore, low-order enhanced feature aggregation adaptively integrates low-order and high-order features based on task relevance, ensuring effective capture of both local and global structural information without excessive complexity. Extensive experiments on real-world datasets demonstrate that our approach consistently outperforms state-of-the-art GNN models in both accuracy and computational efficiency.
        ]]></description>
    </item>
    <item>
        <title>Learning Adaptive Parallel Reasoning with Language Models</title>
        <link>https://arxiv.org/abs/2504.15466</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15466v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, Alane Suhr</dc:creator>
        <description><![CDATA[
            背景：现有语言模型推理方法存在局限，序列化思维链输出长、延迟高，并行方法如自一致性协调不足、计算冗余。方法：提出自适应并行推理（APR）框架，用spawn()和join()操作实现自适应多线程推理，采用端到端强化学习策略优化推理线程。效果：在倒计时推理任务中表现出色，相同上下文窗口下性能更高（4k上下文时83.4% vs. 60.0%），计算量增加时扩展性更好（20k总令牌时80.1% vs. 66.6%），相同延迟下准确率更高（约5000ms时75.2% vs. 57.3%）。
            arXiv:2504.15466v1 Announce Type: cross 
Abstract: Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.
        ]]></description>
    </item>
    <item>
        <title>CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction</title>
        <link>https://arxiv.org/abs/2504.15629</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15629v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Harsh Maheshwari, Srikanth Tenneti, Alwarappan Nakkiran</dc:creator>
        <description><![CDATA[
            背景：检索增强生成（RAG）结合传统搜索与大语言模型，但大模型常存在源归因问题，流行生成搜索引擎引用准确率仅约74%。方法：提出高效后处理算法，通过关键词+语义匹配、基于BERTScore微调模型和轻量级大模型技术，对生成引用与检索文章进行交叉核对。效果：RAG系统整体准确率指标相对提升15.46%，可改用约12倍成本效益更高、推理速度快3倍的较小模型，性能相当。
            arXiv:2504.15629v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of Large Language Models (LLMs), revolutionizing information search and consumption. RAG systems combine traditional search capabilities with LLMs to generate comprehensive answers to user queries, ideally with accurate citations. However, in our experience of developing a RAG product, LLMs often struggle with source attribution, aligning with other industry studies reporting citation accuracy rates of only about 74% for popular generative search engines. To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches cross-check generated citations against retrieved articles using methods including keyword + semantic matching, fine tuned model with BERTScore, and a lightweight LLM-based technique. Our experimental results demonstrate a relative improvement of 15.46% in the overall accuracy metrics of our RAG system. This significant enhancement potentially enables a shift from our current larger language model to a relatively smaller model that is approximately 12x more cost-effective and 3x faster in inference time, while maintaining comparable performance. This research contributes to enhancing the reliability and trustworthiness of AI-generated content in information retrieval and summarization tasks which is critical to gain customer trust especially in commercial products.
        ]]></description>
    </item>
    <item>
        <title>TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving</title>
        <link>https://arxiv.org/abs/2504.15780</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15780v1</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Daocheng Fu, Zijun Chen, Renqiu Xia, Qi Liu, Yuan Feng, Hongbin Zhou, Renrui Zhang, Shiyang Feng, Peng Gao, Junchi Yan, Botian Shi, Bo Zhang, Yu Qiao</dc:creator>
        <description><![CDATA[
            背景：数学几何问题求解需有效整合多模态信息与可验证的逻辑连贯性，现有合成基准存在不自验证、含噪声和矛盾信息等问题。方法：提出可扩展数据引擎TrustGeoGen用于问题生成，通过多模态对齐生成、形式验证、自举机制和GeoExplore系列算法合成几何数据。效果：生成GeoTrust - 200K数据集和测试集，现有模型在测试集上准确率仅49.17%，基于该数据集训练的模型在GeoQA上实现OOD泛化，显著减少逻辑不一致性。
            arXiv:2504.15780v1 Announce Type: cross 
Abstract: Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen
        ]]></description>
    </item>
    <item>
        <title>LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision</title>
        <link>https://arxiv.org/abs/2304.07647</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2304.07647v5</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim</dc:creator>
        <description><![CDATA[
            现有从视频学习时空场景图（STSG）的监督方法因依赖标注视频，大规模构建成本高而受限。为此提出LASER神经符号框架，仅用视频字幕训练STSG生成器。先利用大语言模型从字幕提取含丰富时空语义信息的逻辑规范，再训练STSG生成器使预测的STSG与规范对齐，通过可微符号推理器及多种损失函数克服弱监督挑战。在三个视频数据集上评估，相比全监督基线有显著提升，如在OpenPVSG上一元谓词预测准确率达27.78%（提升12.65%）等。
            arXiv:2304.07647v5 Announce Type: replace 
Abstract: Supervised approaches for learning spatio-temporal scene graphs (STSG) from video are greatly hindered due to their reliance on STSG-annotated videos, which are labor-intensive to construct at scale. Is it feasible to instead use readily available video captions as weak supervision? To address this question, we propose LASER, a neuro-symbolic framework to enable training STSG generators using only video captions. LASER employs large language models to first extract logical specifications with rich spatio-temporal semantic information from video captions. LASER then trains the underlying STSG generator to align the predicted STSG with the specification. The alignment algorithm overcomes the challenges of weak supervision by leveraging a differentiable symbolic reasoner and using a combination of contrastive, temporal, and semantics losses. The overall approach efficiently trains low-level perception models to extract a fine-grained STSG that conforms to the video caption. In doing so, it enables a novel methodology for learning STSGs without tedious annotations. We evaluate our method on three video datasets: OpenPVSG, 20BN, and MUGEN. Our approach demonstrates substantial improvements over fully-supervised baselines, achieving a unary predicate prediction accuracy of 27.78% (+12.65%) and a binary recall@5 of 0.42 (+0.22) on OpenPVSG. Additionally, LASER exceeds baselines by 7% on 20BN and 5.2% on MUGEN in terms of overall predicate prediction accuracy.
        ]]></description>
    </item>
    <item>
        <title>LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content</title>
        <link>https://arxiv.org/abs/2410.10783</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2410.10783v3</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Nimrod Shabtay, Felipe Maia Polo, Sivan Doveh, Wei Lin, M. Jehanzeb Mirza, Leshem Chosen, Mikhail Yurochkin, Yuekai Sun, Assaf Arbelle, Leonid Karlinsky, Raja Giryes</dc:creator>
        <description><![CDATA[
            背景：多模态模型在网页数据上大规模训练虽有成效，但网页数据可能影响模型评估基准。方法：提出基于arXiv论文的可扩展动态基准LiveXiv，自动利用论文多模态内容生成视觉问答对，还引入高效评估方法。效果：在首个版本基准上对多个多模态大模型进行测试，展现其挑战性并避免污染，手动验证子集与自动标注结果性能方差极小（<2.5%），数据集已在HuggingFace发布。
            arXiv:2410.10783v3 Announce Type: replace 
Abstract: The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.
        ]]></description>
    </item>
    <item>
        <title>SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions</title>
        <link>https://arxiv.org/abs/2504.02698</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.02698v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Shengrui XU, Tianchi Lu, Zikun Wang, Jixiu Zhai, Jingwan Wang</dc:creator>
        <description><![CDATA[
            背景：蛋白质 - 蛋白质相互作用（PPI）预测对解读细胞功能和疾病机制至关重要，传统方法在跨模态特征融合和抑制假阴性方面有局限。方法：提出SCMPPI，将基于序列的特征与网络拓扑结构有效整合，结合负样本过滤的增强对比学习策略。效果：在八个基准数据集上实验，准确率达98.13%、AUC为99.69%，跨物种泛化AUC>99%，在多个生物网络分析中有成功应用，是多模态生物数据分析有力工具。
            arXiv:2504.02698v2 Announce Type: replace 
Abstract: Protein-protein interaction (PPI) prediction plays a pivotal role in deciphering cellular functions and disease mechanisms. To address the limitations of traditional experimental methods and existing computational approaches in cross-modal feature fusion and false-negative suppression, we propose SCMPPI-a novel supervised contrastive multimodal framework. By effectively integrating sequence-based features (AAC, DPC, ESMC-CKSAAP) with network topology (Node2Vec embeddings) and incorporating an enhanced contrastive learning strategy with negative sample filtering, SCMPPI achieves superior prediction performance. Extensive experiments on eight benchmark datasets demonstrate its state-of-the-art accuracy(98.13%) and AUC(99.69%), along with excellent cross-species generalization (AUC>99%). Successful applications in CD9 networks, Wnt pathway analysis, and cancer-specific networks further highlight its potential for disease target discovery, establishing SCMPPI as a powerful tool for multimodal biological data analysis.
        ]]></description>
    </item>
    <item>
        <title>HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation with User History Encoding and Compression</title>
        <link>https://arxiv.org/abs/2504.10150</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.10150v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Chen Zhang, Bo Hu, Weidong Chen, Zhendong Mao</dc:creator>
        <description><![CDATA[
            背景：大语言模型在多模态推荐任务中的应用尚待深入，用长提示表示用户历史交互会降低训练和推理效率，影响推荐性能。方法：提出HistLLM框架，通过用户历史编码模块整合文本和视觉特征，将多模态用户历史交互压缩成单个令牌表示，便于大语言模型处理用户偏好。效果：大量实验证明了该机制的有效性和高效性。
            arXiv:2504.10150v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) have proven effective in leveraging textual data for recommendations, their application to multimodal recommendation tasks remains relatively underexplored. Although LLMs can process multimodal information through projection functions that map visual features into their semantic space, recommendation tasks often require representing users' history interactions through lengthy prompts combining text and visual elements, which not only hampers training and inference efficiency but also makes it difficult for the model to accurately capture user preferences from complex and extended prompts, leading to reduced recommendation performance. To address this challenge, we introduce HistLLM, an innovative multimodal recommendation framework that integrates textual and visual features through a User History Encoding Module (UHEM), compressing multimodal user history interactions into a single token representation, effectively facilitating LLMs in processing user preferences. Extensive experiments demonstrate the effectiveness and efficiency of our proposed mechanism.
        ]]></description>
    </item>
    <item>
        <title>TALES: Text Adventure Learning Environment Suite</title>
        <link>https://arxiv.org/abs/2504.14128</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.14128v2</guid>
        <category>我关注的研究话题是多模态大模型，尤其是与结构化数据（图、序列等）相关的研究，包括大模型结构化数据特征对齐、大模型结构化数据RAG、大模型思维链。具体的研究问题有：（1）如何有效地将结构化的信息（图、序列数据）与自然语言的语义空间进行对齐，使得模型能够同时理解数据结构和语义信息；（2）如何用适当的指令使得大模型理解结构化数据中的结构信息；（3）如何赋予大语言模型图学习下游任务的逐步推理能力，从而逐步推断出更复杂的关系和属性。（4）对于下游任务的推理能力，目前的研究比较少，针对序列数据的推理能力研究非常少。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre C\^ot\'e</dc:creator>
        <description><![CDATA[
            背景：推理是大语言模型与世界交互的关键技能，复杂任务对序列决策的推理能力要求提高。方法：提出TALES，包含合成和人工编写的文本冒险游戏集合，用于挑战和评估不同推理能力，并对多种大语言模型进行实验和定性分析。效果：即便表现最佳的大语言模型驱动的智能体，在为人类设计的游戏中成功率也未达15%。实验代码和可视化见https://microsoft.github.io/tales 。
            arXiv:2504.14128v2 Announce Type: replace-cross 
Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tales.
        ]]></description>
    </item>
    <item>
        <title>Exploring the User Experience of AI-Assisted Sound Searching Systems for Creative Workflows</title>
        <link>https://arxiv.org/abs/2504.15575</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.15575v1</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Haohe Liu, Thomas Deacon, Wenwu Wang, Matt Paradis, Mark D. Plumbley</dc:creator>
        <description><![CDATA[
            在音频制作中，高效查找合适音效是重要且具挑战的问题。当前多数音效搜索系统依赖人工预标注音频标签，耗时且易出错，限制了音频制作效率。为此，研究人员探索了基于对比语言 - 音频预训练（CLAP）模型、无需人工标注的音效搜索系统（CLAP - UI）。通过与BBC音效库对比实验，基于专业音效搜索流程的有效任务评估用户表现、认知负荷和满意度。结果显示，CLAP - UI显著提升了生产力、减少了挫败感，同时认知需求相当。此外，还对用户反馈进行定性分析，为未来设计提供参考。
            arXiv:2504.15575v1 Announce Type: new 
Abstract: Locating the right sound effect efficiently is an important yet challenging topic for audio production. Most current sound-searching systems rely on pre-annotated audio labels created by humans, which can be time-consuming to produce and prone to inaccuracies, limiting the efficiency of audio production. Following the recent advancement of contrastive language-audio pre-training (CLAP) models, we explore an alternative CLAP-based sound-searching system (CLAP-UI) that does not rely on human annotations. To evaluate the effectiveness of CLAP-UI, we conducted comparative experiments with a widely used sound effect searching platform, the BBC Sound Effect Library. Our study evaluates user performance, cognitive load, and satisfaction through ecologically valid tasks based on professional sound-searching workflows. Our result shows that CLAP-UI demonstrated significantly enhanced productivity and reduced frustration while maintaining comparable cognitive demands. We also qualitatively analyzed the participants' feedback, which offered valuable perspectives on the design of future AI-assisted sound search systems.
        ]]></description>
    </item>
    <item>
        <title>Listenable Maps for Zero-Shot Audio Classifiers</title>
        <link>https://arxiv.org/abs/2405.17615</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2405.17615v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Francesco Paissan, Luca Della Libera, Mirco Ravanelli, Cem Subakan</dc:creator>
        <description><![CDATA[
            背景：解释深度学习模型（如音频分类器）的决策，对确保技术透明性和可信度至关重要。方法：提出LMAC - ZS，这是首个基于解码器的事后解释方法，用于解释零样本音频分类器的决策，采用新损失函数最大化给定文本 - 音频对原始相似度。效果：用CLAP模型评估表明，该解释器在零样本分类中忠实于决策，且定性展示其能产生与不同文本提示相关性好的有意义解释。
            arXiv:2405.17615v2 Announce Type: replace 
Abstract: Interpreting the decisions of deep learning models, including audio classifiers, is crucial for ensuring the transparency and trustworthiness of this technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Audio Classifiers in the Zero-Shot context), which, to the best of our knowledge, is the first decoder-based post-hoc interpretation method for explaining the decisions of zero-shot audio classifiers. The proposed method utilizes a novel loss function that maximizes the faithfulness to the original similarity between a given text-and-audio pair. We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our interpreter remains faithful to the decisions in a zero-shot classification context. Moreover, we qualitatively show that our method produces meaningful explanations that correlate well with different text prompts.
        ]]></description>
    </item>
    <item>
        <title>Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Fold Paralysis</title>
        <link>https://arxiv.org/abs/2409.03597</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2409.03597v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Juan Liu, Faya Liang, Ming Li</dc:creator>
        <description><![CDATA[
            背景：为辅助临床评估声带麻痹，需从喉镜视频中提取关键信息。方法：提出多模态喉镜视频分析系统（MLVAS），融合视频声门检测与音频关键词识别方法分析音视频数据；用预训练音频编码器提取音频特征，通过测量声门左右两侧与中线的角度偏差获取视觉特征，还引入基于扩散的方法优化分割结果。效果：在公开分割数据集上验证了分割模块有效性，在临床数据集上，MLVAS能为声带麻痹分类提供可靠客观指标和可视化辅助诊断。
            arXiv:2409.03597v3 Announce Type: replace 
Abstract: This paper presents the Multimodal Laryngoscopic Video Analyzing System (MLVAS), a novel system that leverages both audio and video data to automatically extract key video segments and metrics from raw laryngeal videostroboscopic videos for assisted clinical assessment. The system integrates video-based glottis detection with an audio keyword spotting method to analyze both video and audio data, identifying patient vocalizations and refining video highlights to ensure optimal inspection of vocal fold movements. Beyond key video segment extraction from the raw laryngeal videos, MLVAS is able to generate effective audio and visual features for Vocal Fold Paralysis (VFP) detection. Pre-trained audio encoders are utilized to encode the patient voice to get the audio features. Visual features are generated by measuring the angle deviation of both the left and right vocal folds to the estimated glottal midline on the segmented glottis masks. To get better masks, we introduce a diffusion-based refinement that follows traditional U-Net segmentation to reduce false positives. We conducted several ablation studies to demonstrate the effectiveness of each module and modalities in the proposed MLVAS. The experimental results on a public segmentation dataset show the effectiveness of our proposed segmentation module. In addition, unilateral VFP classification results on a real-world clinic dataset demonstrate MLVAS's ability of providing reliable and objective metrics as well as visualization for assisted clinical diagnosis.
        ]]></description>
    </item>
    <item>
        <title>F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group Relative Policy Optimization</title>
        <link>https://arxiv.org/abs/2504.02407</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.02407v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xiaohui Sun, Ruitong Xiao, Jianye Mo, Bowen Wu, Qun Yu, Baoxun Wang</dc:creator>
        <description><![CDATA[
            背景：现有基于流匹配的文本到语音（TTS）系统存在提升空间。方法：提出F5R - TTS系统，将组相对策略优化（GRPO）集成到基于流匹配的架构中，把流匹配TTS的确定性输出重新表述为概率高斯分布，在预训练阶段用开源数据集训练模型，强化学习阶段采用GRPO驱动，利用自动语音识别计算的字错误率（WER）和验证模型评估的说话人相似度（SIM）作为双奖励指标。效果：在零样本语音克隆实验中，相比传统系统，WER相对降低29.5%，SIM得分相对提高4.6%。
            arXiv:2504.02407v3 Announce Type: replace 
Abstract: We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Group Relative Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (a 29.5% relative reduction in WER) and speaker similarity (a 4.6% relative increase in SIM score) compared to conventional flow-matching based TTS systems. Audio samples are available at https://frontierlabs.github.io/F5R.
        ]]></description>
    </item>
    <item>
        <title>Location-Oriented Sound Event Localization and Detection with Spatial Mapping and Regression Localization</title>
        <link>https://arxiv.org/abs/2504.08365</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.08365v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Xueping Zhang, Yaxiong Chen, Ruilin Yao, Yunfei Zi, Shengwu Xiong</dc:creator>
        <description><![CDATA[
            背景：当前面向事件的多轨道方法用于声音事件定位与检测（SELD）时，因轨道数量限制，在多音环境中泛化性不佳。方法：提出空间映射和回归定位的SELD方法（SMRL - SELD），将3D空间分割并映射到2D平面，提出新的回归定位损失帮助结果向对应事件位置收敛，该方法以位置为导向让模型基于方向学习事件特征。效果：在STARSS23和STARSS22数据集上实验，SMRL - SELD在整体评估和多音环境中优于现有SELD方法。
            arXiv:2504.08365v2 Announce Type: replace 
Abstract: Sound Event Localization and Detection (SELD) combines the Sound Event Detection (SED) with the corresponding Direction Of Arrival (DOA). Recently, adopted event oriented multi-track methods affect the generality in polyphonic environments due to the limitation of the number of tracks. To enhance the generality in polyphonic environments, we propose Spatial Mapping and Regression Localization for SELD (SMRL-SELD). SMRL-SELD segments the 3D spatial space, mapping it to a 2D plane, and a new regression localization loss is proposed to help the results converge toward the location of the corresponding event. SMRL-SELD is location-oriented, allowing the model to learn event features based on orientation. Thus, the method enables the model to process polyphonic sounds regardless of the number of overlapping events. We conducted experiments on STARSS23 and STARSS22 datasets and our proposed SMRL-SELD outperforms the existing SELD methods in overall evaluation and polyphony environments.
        ]]></description>
    </item>
    <item>
        <title>EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting</title>
        <link>https://arxiv.org/abs/2504.12867</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.12867v3</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Guanrou Yang, Chen Yang, Qian Chen, Ziyang Ma, Wenxi Chen, Wen Wang, Tianrui Wang, Yifan Yang, Zhikang Niu, Wenrui Liu, Fan Yu, Zhihao Du, Zhifu Gao, ShiLiang Zhang, Xie Chen</dc:creator>
        <description><![CDATA[
            背景：现有文本转语音（TTS）模型在控制生成语音情感表达上存在挑战。方法：提出EmoVoice模型，利用大语言模型（LLMs）实现细粒度自由式自然语言情感控制，采用音素增强变体设计使模型并行输出音素和音频令牌以增强内容一致性；还引入40小时高质量英语情感数据集EmoVoice - DB。效果：仅用合成训练数据就在英语EmoVoice - DB测试集和中文Secap测试集达最优性能，还探究了情感评估指标可靠性及与人类感知偏好的一致性。
            arXiv:2504.12867v3 Announce Type: replace 
Abstract: Human speech goes beyond the mere transfer of information; it is a profound exchange of emotions and a connection between individuals. While Text-to-Speech (TTS) models have made huge progress, they still face challenges in controlling the emotional expression in the generated speech. In this work, we propose EmoVoice, a novel emotion-controllable TTS model that exploits large language models (LLMs) to enable fine-grained freestyle natural language emotion control, and a phoneme boost variant design that makes the model output phoneme tokens and audio tokens in parallel to enhance content consistency, inspired by chain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring expressive speech and fine-grained emotion labels with natural language descriptions. EmoVoice achieves state-of-the-art performance on the English EmoVoice-DB test set using only synthetic training data, and on the Chinese Secap test set using our in-house data. We further investigate the reliability of existing emotion evaluation metrics and their alignment with human perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and Gemini to assess emotional speech. Demo samples are available at https://yanghaha0908.github.io/EmoVoice/. Dataset, code, and checkpoints will be released.
        ]]></description>
    </item>
    <item>
        <title>EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with the Musical Feature Template</title>
        <link>https://arxiv.org/abs/2309.13259</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2309.13259v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Monan Zhou, Xiaobing Li, Feng Yu, Wei Li</dc:creator>
        <description><![CDATA[
            背景：缺乏结构良好且有情感标签的乐谱。方法：从小规模情感符号音乐数据集和音乐心理学结论中得出音乐特征与情感标签的统计相关性，设计控制情感旋律生成的模板；用模板为大型乐谱集标注粗略情感标签，转换为ABC记谱法，通过数据增强减少标签不平衡，得到Rough4Q数据集。效果：系统骨干在Rough4Q上预训练后，music21解析率达99%，模板生成的旋律在盲听测试中情感表达一致性达91%。
            arXiv:2309.13259v2 Announce Type: replace-cross 
Abstract: The EMelodyGen system focuses on emotional melody generation in ABC notation controlled by the musical feature template. Owing to the scarcity of well-structured and emotionally labeled sheet music, we designed a template for controlling emotional melody generation by statistical correlations between musical features and emotion labels derived from small-scale emotional symbolic music datasets and music psychology conclusions. We then automatically annotated a large, well-structured sheet music collection with rough emotional labels by the template, converted them into ABC notation, and reduced label imbalance by data augmentation, resulting in a dataset named Rough4Q. Our system backbone pre-trained on Rough4Q can achieve up to 99% music21 parsing rate and melodies generated by our template can lead to a 91% alignment on emotional expressions in blind listening tests. Ablation studies further validated the effectiveness of the feature controls in the template. Available code and demos are at https://github.com/monetjoe/EMelodyGen.
        ]]></description>
    </item>
    <item>
        <title>Audio signal interpolation using optimal transportation of spectrograms</title>
        <link>https://arxiv.org/abs/2502.15430</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2502.15430v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>David Valdivia, Marien Renaud, Elsa Cazelles, C\'edric F\'evotte</dc:creator>
        <description><![CDATA[
            背景：需要生成能在给定源声音和目标声音之间进行插值的人工音频信号。方法：提出一种新方法，先计算源和目标频谱图的Wasserstein重心，再进行相位重建和反转；该方法全局考虑频谱图，不逐帧操作，还赋予传输成本矩阵特定结构以避免能量沿时间轴远距离移动，并利用非平衡传输框架实现最优传输。效果：通过合成音符和真实环境声音实验展示了该方法的潜力，还降低了计算负载。
            arXiv:2502.15430v2 Announce Type: replace-cross 
Abstract: We present a novel approach for generating an artificial audio signal that interpolates between given source and target sounds. Our approach relies on the computation of Wasserstein barycenters of the source and target spectrograms, followed by phase reconstruction and inversion. In contrast with previous works, our new method considers the spectrograms globally and does not operate on a temporal frame-to-frame basis. Another contribution is to endow the transportation cost matrix with a specific structure that prohibits remote displacements of energy along the time axis, and for which optimal transport is made possible by leveraging the unbalanced transport framework. The proposed cost matrix makes sense from the audio perspective and also allows to reduce the computation load. Results with synthetic musical notes and real environmental sounds illustrate the potential of our novel approach.
        ]]></description>
    </item>
    <item>
        <title>VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation</title>
        <link>https://arxiv.org/abs/2504.04060</link>
        <guid isPermaLink="false">oai:arXiv.org:oai:arXiv.org:2504.04060v2</guid>
        <category>我关注的研究话题是：1) 音频分类，包括Audioset/DCASE等数据集上有关audio tagging/sound event detection的工作，尤其是与Audioset Ontology相关的研究。2) 音频生成，包括：a）基础音频生成模型，包括Diffusion、Flow、VQGAN等；b）能够处理音频的LLM，包括音频的tokenization、音频的prompt、音频的上下文学习等。</category>
        <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
        <arxiv:announce_type>new</arxiv:announce_type>
        <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
        <dc:creator>Yuhao Wang, Heyang Liu, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</dc:creator>
        <description><![CDATA[
            语音大语言模型是语音处理领域的研究热点。本文提出VocalNet - 1B和VocalNet - 8B，借助为实时语音交互设计的可扩展、与模型无关的训练框架实现高性能、低延迟。首次将多令牌预测（MTP）应用于语音大语言模型，与标准的下一令牌预测不同，可同时提升生成速度和质量。通过分析MTP对语音生成的影响及实验对比，设计出高效的MTP实现方法。实验表明，VocalNet在有限训练数据下与主流全功能大语言模型表现相当，远超现有开源语音大语言模型。
            arXiv:2504.04060v2 Announce Type: replace-cross 
Abstract: Speech large language models (LLMs) have emerged as a prominent research focus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series of high-performance, low-latency speech LLMs enabled by a scalable and model-agnostic training framework designed for real-time voice interaction. Central to our contribution is the first application of multi-token prediction (MTP) to speech LLMs. This approach represents a paradigm shift from standard next-token prediction (NTP), offering simultaneous improvements in generation speed and quality. Informed by analysis of MTP's effect on speech generation and experimental comparisons, we designed a straightforward and highly effective MTP implementation. Experiments demonstrate that VocalNet performs on par with mainstream Omni LLMs even with limited training data, and significantly surpasses existing open-source speech LLMs. To foster reproducibility and community advancement, all model weights, inference code, training data, and framework implementations have been made publicly available at https://github.com/SJTU-OmniAgent/VocalNet
        ]]></description>
    </item>
</channel>
</rss>