{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Wed, 04 Jun 2025 04:13:41 +0000",
      "published": "Wed, 04 Jun 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2506.01959v1",
        "title": "Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes",
        "link": "https://arxiv.org/abs/2506.01959",
        "author": "Irmi Schneider",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01959v1 Announce Type: new \nAbstract: Symmetry plays a crucial role in understanding the properties of mathematical structures and optimization problems. Recent work has explored this phenomenon in the context of neural networks, where the loss function is invariant under column and row permutations of the network weights. It has been observed that local minima exhibit significant symmetry with respect to the network weights (invariance to row and column permutations). And moreover no critical point was found that lacked symmetry. We extend this line of inquiry by investigating symmetry phenomena in real-valued loss functions defined on a broader class of spaces. We will introduce four more cases: the projective case over a finite field, the octahedral graph case, the perfect matching case, and the particle attraction case. We show that as in the neural network case, all the critical points observed have non-trivial symmetry. Finally we introduce a new measure of symmetry in the system and show that it reveals additional symmetry structures not captured by the previous measure."
      },
      {
        "id": "oai:arXiv.org:2506.01961v1",
        "title": "Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System",
        "link": "https://arxiv.org/abs/2506.01961",
        "author": "Jinzhu Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01961v1 Announce Type: new \nAbstract: This study is dedicated to exploring the application of prompt learning methods to advance Named Entity Recognition (NER) within the medical domain. In recent years, the emergence of large-scale models has driven significant progress in NER tasks, particularly with the introduction of the BioBERT language model, which has greatly enhanced NER capabilities in medical texts. Our research introduces the Prompt-bioMRC model, which integrates both hard template and soft prompt designs aimed at refining the precision and efficiency of medical entity recognition. Through extensive experimentation across diverse medical datasets, our findings consistently demonstrate that our approach surpasses traditional models. This enhancement not only validates the efficacy of our methodology but also highlights its potential to provide reliable technological support for applications like intelligent diagnosis systems. By leveraging advanced NER techniques, this study contributes to advancing automated medical data processing, facilitating more accurate medical information extraction, and supporting efficient healthcare decision-making processes."
      },
      {
        "id": "oai:arXiv.org:2506.01962v1",
        "title": "Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition",
        "link": "https://arxiv.org/abs/2506.01962",
        "author": "Xiaozhou Ye, Kevin I-Kai Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01962v1 Announce Type: new \nAbstract: Cross-user variability poses a significant challenge in sensor-based Human Activity Recognition (HAR) systems, as traditional models struggle to generalize across users due to differences in behavior, sensor placement, and data distribution. To address this, we propose GNN-ADG (Graph Neural Network with Adversarial Domain Generalization), a novel method that leverages both the strength from both the Graph Neural Networks (GNNs) and adversarial learning to achieve robust cross-user generalization. GNN-ADG models spatial relationships between sensors on different anatomical body parts, extracting three types of Anatomical Units: (1) Interconnected Units, capturing inter-relations between neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or functionally similar body parts; and (3) Lateral Units, connecting sensors based on their position to capture region-specific coordination. These units information are fused into an unified graph structure with a cyclic training strategy, dynamically integrating spatial, functional, and lateral correlations to facilitate a holistic, user-invariant representation. Information fusion mechanism of GNN-ADG occurs by iteratively cycling through edge topologies during training, allowing the model to refine its understanding of inter-sensor relationships across diverse perspectives. By representing the spatial configuration of sensors as an unified graph and incorporating adversarial learning, Information Fusion GNN-ADG effectively learns features that generalize well to unseen users without requiring target user data during training, making it practical for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.01963v1",
        "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons",
        "link": "https://arxiv.org/abs/2506.01963",
        "author": "Andrew Kiruluta, Preethi Raju, Priscilla Burity",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01963v1 Announce Type: new \nAbstract: We present a novel non attention based architecture for large language models (LLMs) that efficiently handles very long context windows, on the order of hundreds of thousands to potentially millions of tokens. Unlike traditional Transformer designs, which suffer from quadratic memory and computation overload due to the nature of the self attention mechanism, our model avoids token to token attention entirely. Instead, it combines the following complementary components: State Space blocks (inspired by S4) that learn continuous time convolution kernels and scale near linearly with sequence length, Multi Resolution Convolution layers that capture local context at different dilation levels, a lightweight Recurrent Supervisor to maintain a global hidden state across sequential chunks, and Retrieval Augmented External Memory that stores and retrieves high-level chunk embeddings without reintroducing quadratic operations."
      },
      {
        "id": "oai:arXiv.org:2506.01964v1",
        "title": "A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction",
        "link": "https://arxiv.org/abs/2506.01964",
        "author": "Kamal Acharya, Mehul Lad, Liang Sun, Houbing Song",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01964v1 Announce Type: new \nAbstract: Accurate prediction of trips between zones is critical for transportation planning, as it supports resource allocation and infrastructure development across various modes of transport. Although the gravity model has been widely used due to its simplicity, it often inadequately represents the complex factors influencing modern travel behavior. This study introduces a data-driven approach to enhance the gravity model by integrating geographical, economic, social, and travel data from the counties in Tennessee and New York state. Using machine learning techniques, we extend the capabilities of the traditional model to handle more complex interactions between variables. Our experiments demonstrate that machine learning-enhanced models significantly outperform the traditional model. Our results show a 51.48% improvement in R-squared, indicating a substantial enhancement in the model's explanatory power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a significant increase in prediction accuracy. Furthermore, a 44.32% increase in Common Part of Commuters (CPC) demonstrates improved prediction reliability. These findings highlight the substantial benefits of integrating diverse datasets and advanced algorithms into transportation models. They provide urban planners and policymakers with more reliable forecasting and decision-making tools."
      },
      {
        "id": "oai:arXiv.org:2506.01965v1",
        "title": "TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition",
        "link": "https://arxiv.org/abs/2506.01965",
        "author": "Bonpagna Kann, Sandra Castellanos-Paez, Romain Rombourg, Philippe Lalanda",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01965v1 Announce Type: new \nAbstract: As machine learning based systems become more integrated into daily life, they unlock new opportunities but face the challenge of adapting to dynamic data environments. Various forms of data shift-gradual, abrupt, or cyclic-threaten model accuracy, making continual adaptation essential. Continual Learning (CL) enables models to learn from evolving data streams while minimizing forgetting of prior knowledge. Among CL strategies, replay-based methods have proven effective, but their success relies on balancing memory constraints and retaining old class accuracy while learning new classes. This paper presents TaskVAE, a framework for replay-based CL in class-incremental settings. TaskVAE employs task-specific Variational Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which are then used to train the classifier alongside new task data. In contrast to traditional methods that require prior knowledge of the total class count or rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks without such constraints. We focus on Human Activity Recognition (HAR) using IMU sensor-equipped devices. Unlike previous HAR studies that combine data across all users, our approach focuses on individual user data, better reflecting real-world scenarios where a person progressively learns new activities. Extensive experiments on 5 different HAR datasets show that TaskVAE outperforms experience replay methods, particularly with limited data, and exhibits robust performance as dataset size increases. Additionally, memory footprint of TaskVAE is minimal, being equivalent to only 60 samples per task, while still being able to generate an unlimited number of synthetic samples. The contributions lie in balancing memory constraints, task-specific generation, and long-term stability, making it a reliable solution for real-world applications in domains like HAR."
      },
      {
        "id": "oai:arXiv.org:2506.01966v1",
        "title": "Matrix Is All You Need",
        "link": "https://arxiv.org/abs/2506.01966",
        "author": "Yuzhou Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01966v1 Announce Type: new \nAbstract: Deep neural networks employ specialized architectures for vision, sequential and language tasks, yet this proliferation obscures their underlying commonalities. We introduce a unified matrix-order framework that casts convolutional, recurrent and self-attention operations as sparse matrix multiplications. Convolution is realized via an upper-triangular weight matrix performing first-order transformations; recurrence emerges from a lower-triangular matrix encoding stepwise updates; attention arises naturally as a third-order tensor factorization. We prove algebraic isomorphism with standard CNN, RNN and Transformer layers under mild assumptions. Empirical evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet), time-series forecasting (ETTh1, Electricity Load Diagrams) and language modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that sparse-matrix formulations match or exceed native model performance while converging in comparable or fewer epochs. By reducing architecture design to sparse pattern selection, our matrix perspective aligns with GPU parallelism and leverages mature algebraic optimization tools. This work establishes a mathematically rigorous substrate for diverse neural architectures and opens avenues for principled, hardware-aware network design."
      },
      {
        "id": "oai:arXiv.org:2506.01967v1",
        "title": "Turning LLM Activations Quantization-Friendly",
        "link": "https://arxiv.org/abs/2506.01967",
        "author": "Patrik Czak\\'o, G\\'abor Kert\\'esz, S\\'andor Sz\\'en\\'asi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01967v1 Announce Type: new \nAbstract: Quantization effectively reduces the serving costs of Large Language Models (LLMs) by speeding up data movement through compressed parameters and enabling faster operations via integer arithmetic. However, activating integer arithmetic requires quantizing both weights and activations, which poses challenges due to the significant outliers in LLMs that increase quantization error. In this work, we investigate these outliers with an emphasis on their effect on layer-wise quantization error, then examine how smoothing and rotation transform the observed values. Our primary contributions include introducing a new metric to measure and visualize quantization difficulty based on channel magnitudes, as well as proposing a hybrid approach that applies channel-wise scaling before rotation, supported by a mathematical formulation of its benefits."
      },
      {
        "id": "oai:arXiv.org:2506.01968v1",
        "title": "Efficient ANN-SNN Conversion with Error Compensation Learning",
        "link": "https://arxiv.org/abs/2506.01968",
        "author": "Chang Liu, Jiangrong Shen, Xuming Ran, Mingkun Xu, Qi Xu, Yi Xu, Gang Pan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01968v1 Announce Type: new \nAbstract: Artificial neural networks (ANNs) have demonstrated outstanding performance in numerous tasks, but deployment in resource-constrained environments remains a challenge due to their high computational and memory requirements. Spiking neural networks (SNNs) operate through discrete spike events and offer superior energy efficiency, providing a bio-inspired alternative. However, current ANN-to-SNN conversion often results in significant accuracy loss and increased inference time due to conversion errors such as clipping, quantization, and uneven activation. This paper proposes a novel ANN-to-SNN conversion framework based on error compensation learning. We introduce a learnable threshold clipping function, dual-threshold neurons, and an optimized membrane potential initialization strategy to mitigate the conversion error. Together, these techniques address the clipping error through adaptive thresholds, dynamically reduce the quantization error through dual-threshold neurons, and minimize the non-uniformity error by effectively managing the membrane potential. Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our method achieves high-precision and ultra-low latency among existing conversion methods. Using only two time steps, our method significantly reduces the inference time while maintains competitive accuracy of 94.75% on CIFAR-10 dataset under ResNet-18 structure. This research promotes the practical application of SNNs on low-power hardware, making efficient real-time processing possible."
      },
      {
        "id": "oai:arXiv.org:2506.01970v1",
        "title": "Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability",
        "link": "https://arxiv.org/abs/2506.01970",
        "author": "Ruizhuo Song, Beiming Yuan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01970v1 Announce Type: new \nAbstract: This paper thoroughly investigates the challenges of enhancing AI's abstract reasoning capabilities, with a particular focus on Raven's Progressive Matrices (RPM) tasks involving complex human-like concepts. Firstly, it dissects the empirical reality that traditional end-to-end RPM-solving models heavily rely on option pool configurations, highlighting that this dependency constrains the model's reasoning capabilities. To address this limitation, the paper proposes the Johnny architecture - a novel representation space-based framework for RPM-solving. Through the synergistic operation of its Representation Extraction Module and Reasoning Module, Johnny significantly enhances reasoning performance by supplementing primitive negative option configurations with a learned representation space. Furthermore, to strengthen the model's capacity for capturing positional relationships among local features, the paper introduces the Spin-Transformer network architecture, accompanied by a lightweight Straw Spin-Transformer variant that reduces computational overhead through parameter sharing and attention mechanism optimization. Experimental evaluations demonstrate that both Johnny and Spin-Transformer achieve superior performance on RPM tasks, offering innovative methodologies for advancing AI's abstract reasoning capabilities."
      },
      {
        "id": "oai:arXiv.org:2506.01974v1",
        "title": "Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh",
        "link": "https://arxiv.org/abs/2506.01974",
        "author": "Kanwal Aalijah",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01974v1 Announce Type: new \nAbstract: Urban planning plays a very important role in development modern cities. It effects the economic growth, quality of life, and environmental sustainability. Modern cities face challenges in managing traffic congestion. These challenges arise to due to rapid urbanization. In this study we will explore how AI can be used to understand the traffic and mobility related issues and its effects on the residents sentiment. The approach combines real-time traffic data with geo-located sentiment analysis, offering a comprehensive and dynamic approach to urban mobility planning. AI models and exploratory data analysis was used to predict traffic congestion patterns, analyze commuter behaviors, and identify congestion hotspots and dissatisfaction zones. The findings offer actionable recommendations for optimizing traffic flow, enhancing commuter experiences, and addressing city specific mobility challenges in the Middle East and beyond."
      },
      {
        "id": "oai:arXiv.org:2506.01975v1",
        "title": "An empirical study of task and feature correlations in the reuse of pre-trained models",
        "link": "https://arxiv.org/abs/2506.01975",
        "author": "Jama Hussein Mohamud",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01975v1 Announce Type: new \nAbstract: Pre-trained neural networks are commonly used and reused in the machine learning community. Alice trains a model for a particular task, and a part of her neural network is reused by Bob for a different task, often to great effect. To what can we ascribe Bob's success? This paper introduces an experimental setup through which factors contributing to Bob's empirical success could be studied in silico. As a result, we demonstrate that Bob might just be lucky: his task accuracy increases monotonically with the correlation between his task and Alice's. Even when Bob has provably uncorrelated tasks and input features from Alice's pre-trained network, he can achieve significantly better than random performance due to Alice's choice of network and optimizer. When there is little correlation between tasks, only reusing lower pre-trained layers is preferable, and we hypothesize the converse: that the optimal number of retrained layers is indicative of task and feature correlation. Finally, we show in controlled real-world scenarios that Bob can effectively reuse Alice's pre-trained network if there are semantic correlations between his and Alice's task."
      },
      {
        "id": "oai:arXiv.org:2506.01976v1",
        "title": "Crack Path Prediction with Operator Learning using Discrete Particle System data Generation",
        "link": "https://arxiv.org/abs/2506.01976",
        "author": "Elham Kiyani, Venkatesh Ananchaperumal, Ahmad Peyvan, Mahendaran Uchimali, Gang Li, George Em Karniadakis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01976v1 Announce Type: new \nAbstract: Accurately modeling crack propagation is critical for predicting failure in engineering materials and structures, where small cracks can rapidly evolve and cause catastrophic damage. The interaction of cracks with discontinuities, such as holes, significantly affects crack deflection and arrest. Recent developments in discrete particle systems with multibody interactions based on constitutive behavior have demonstrated the ability to capture crack nucleation and evolution without relying on continuum assumptions. In this work, we use data from Constitutively Informed Particle Dynamics (CPD) simulations to train operator learning models, specifically Deep Operator Networks (DeepONets), which learn mappings between function spaces instead of finite-dimensional vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for predicting time-evolving crack propagation in specimens with varying geometries. Three representative cases are studied: (i) varying notch height without active fracture; and (ii) and (iii) combinations of notch height and hole radius where dynamic fracture occurs on irregular discrete meshes. The models are trained on 32 to 45 samples, using geometric inputs in the branch network and spatial-temporal coordinates in the trunk network. Results show that Fusion DeepONet consistently outperforms the vanilla variant, with more accurate predictions especially in non-fracturing cases. Fracture-driven scenarios involving displacement and crack evolution remain more challenging. These findings highlight the potential of Fusion DeepONet to generalize across complex, geometry-varying, and time-dependent crack propagation phenomena."
      },
      {
        "id": "oai:arXiv.org:2506.01977v1",
        "title": "Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN",
        "link": "https://arxiv.org/abs/2506.01977",
        "author": "Wei Huang, Hanchen Wang, Dong Wen, Shaozhen Ma, Wenjie Zhang, Xuemin Lin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01977v1 Announce Type: new \nAbstract: Graph Edit Distance (GED) is a fundamental graph similarity metric widely used in various applications. However, computing GED is an NP-hard problem. Recent state-of-the-art hybrid GED solver has shown promising performance by formulating GED as a bipartite graph matching problem, then leveraging a generative diffusion model to predict node matching between two graphs, from which both the GED and its corresponding edit path can be extracted using a traditional algorithm. However, such methods typically rely heavily on ground-truth supervision, where the ground-truth labels are often costly to obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel unsupervised GAN-based framework for GED computation. Specifically, GEDRanker consists of a matching-based GED solver and introduces an interpretable preference-aware discriminator with an effective training strategy to guide the matching-based GED solver toward generating high-quality node matching without the need for ground-truth labels. Extensive experiments on benchmark datasets demonstrate that our GEDRanker enables the matching-based GED solver to achieve near-optimal solution quality without any ground-truth supervision."
      },
      {
        "id": "oai:arXiv.org:2506.01983v1",
        "title": "Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification",
        "link": "https://arxiv.org/abs/2506.01983",
        "author": "Reyhaneh Keshavarzpour, Eghbal Mansoori",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01983v1 Announce Type: new \nAbstract: Identification of antimicrobial peptides is an important and necessary issue in today's era. Antimicrobial peptides are essential as an alternative to antibiotics for biomedical applications and many other practical applications. These oligopeptides are useful in drug design and cause innate immunity against microorganisms. Artificial intelligence algorithms have played a significant role in the ease of identifying these peptides.This research is improved by improving proposed method in the field of antimicrobial peptides prediction. Suggested method is improved by combining the best coding method from different perspectives, In the following a deep neural network to balance the imbalanced combined datasets. The results of this research show that the proposed method have a significant improvement in the accuracy and efficiency of the prediction of antimicrobial peptides and are able to provide the best results compared to the existing methods. These development in the field of prediction and classification of antimicrobial peptides, basically in the fields of medicine and pharmaceutical industries, have high effectiveness and application."
      },
      {
        "id": "oai:arXiv.org:2506.01986v1",
        "title": "SpecMemo: Speculative Decoding is in Your Pocket",
        "link": "https://arxiv.org/abs/2506.01986",
        "author": "Selin Yildirim, Deming Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01986v1 Announce Type: new \nAbstract: Recent advancements in speculative decoding have demonstrated considerable speedup across a wide array of large language model (LLM) tasks. Speculative decoding inherently relies on sacrificing extra memory allocations to generate several candidate tokens, of which acceptance rate drives the speedup. However, deploying speculative decoding on memory-constrained devices, such as mobile GPUs, remains as a significant challenge in real-world scenarios. In this work, we present a device-aware inference engine named SpecMemo that can smartly control memory allocations at finer levels to enable multi-turn chatbots with speculative decoding on such limited memory devices. Our methodology stems from theoretically modeling memory footprint of speculative decoding to determine a lower bound on the required memory budget while retaining speedup. SpecMemo empirically acquires a careful balance between minimizing redundant memory allocations for rejected candidate tokens and maintaining competitive performance gains from speculation. Notably, with SpecMemo's memory management, we maintain 96% of overall throughput from speculative decoding on MT-Bench, with reduced generation-memory by 65% on single Nvidia Titan RTX. Given multiple constrained GPUs, we build on top of previous speculative decoding architectures to facilitate big-model inference by distributing Llama-2-70B-Chat model, on which we provide novel batched speculative decoding to increase usability of multiple small server GPUs. This novel framework demonstrates 2x speedup over distributed and batched vanilla decoding with the base model on eight AMD MI250 GPUs. Moreover, inference throughput increases remarkably 8x with batch size 10. Our work contributes to democratized LLM applications in resource-constrained environments, providing a pathway for faster and cheaper deployment of real-world LLM applications with robust performance."
      },
      {
        "id": "oai:arXiv.org:2506.01987v1",
        "title": "Equally Critical: Samples, Targets, and Their Mappings in Datasets",
        "link": "https://arxiv.org/abs/2506.01987",
        "author": "Runkang Yang, Peng Sun, Xinyi Shang, Yi Tang, Tao Lin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01987v1 Announce Type: new \nAbstract: Data inherently possesses dual attributes: samples and targets. For targets, knowledge distillation has been widely employed to accelerate model convergence, primarily relying on teacher-generated soft target supervision. Conversely, recent advancements in data-efficient learning have emphasized sample optimization techniques, such as dataset distillation, while neglected the critical role of target. This dichotomy motivates our investigation into understanding how both sample and target collectively influence training dynamic. To address this gap, we first establish a taxonomy of existing paradigms through the lens of sample-target interactions, categorizing them into distinct sample-to-target mapping strategies. Building upon this foundation, we then propose a novel unified loss framework to assess their impact on training efficiency. Through extensive empirical studies on our proposed strategies, we comprehensively analyze how variations in target and sample types, quantities, and qualities influence model training, providing six key insights to enhance training efficacy."
      },
      {
        "id": "oai:arXiv.org:2506.01988v1",
        "title": "Surrogate Interpretable Graph for Random Decision Forests",
        "link": "https://arxiv.org/abs/2506.01988",
        "author": "Akshat Dubey, Aleksandar An\\v{z}el, Georges Hattab",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01988v1 Announce Type: new \nAbstract: The field of health informatics has been profoundly influenced by the development of random forest models, which have led to significant advances in the interpretability of feature interactions. These models are characterized by their robustness to overfitting and parallelization, making them particularly useful in this domain. However, the increasing number of features and estimators in random forests can prevent domain experts from accurately interpreting global feature interactions, thereby compromising trust and regulatory compliance. A method called the surrogate interpretability graph has been developed to address this issue. It uses graphs and mixed-integer linear programming to analyze and visualize feature interactions. This improves their interpretability by visualizing the feature usage per decision-feature-interaction table and the most dominant hierarchical decision feature interactions for predictions. The implementation of a surrogate interpretable graph enhances global interpretability, which is critical for such a high-stakes domain."
      },
      {
        "id": "oai:arXiv.org:2506.01989v1",
        "title": "Coded Robust Aggregation for Distributed Learning under Byzantine Attacks",
        "link": "https://arxiv.org/abs/2506.01989",
        "author": "Chengxi Li, Ming Xiao, Mikael Skoglund",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01989v1 Announce Type: new \nAbstract: In this paper, we investigate the problem of distributed learning (DL) in the presence of Byzantine attacks. For this problem, various robust bounded aggregation (RBA) rules have been proposed at the central server to mitigate the impact of Byzantine attacks. However, current DL methods apply RBA rules for the local gradients from the honest devices and the disruptive information from Byzantine devices, and the learning performance degrades significantly when the local gradients of different devices vary considerably from each other. To overcome this limitation, we propose a new DL method to cope with Byzantine attacks based on coded robust aggregation (CRA-DL). Before training begins, the training data are allocated to the devices redundantly. During training, in each iteration, the honest devices transmit coded gradients to the server computed from the allocated training data, and the server then aggregates the information received from both honest and Byzantine devices using RBA rules. In this way, the global gradient can be approximately recovered at the server to update the global model. Compared with current DL methods applying RBA rules, the improvement of CRA-DL is attributed to the fact that the coded gradients sent by the honest devices are closer to each other. This closeness enhances the robustness of the aggregation against Byzantine attacks, since Byzantine messages tend to be significantly different from those of honest devices in this case. We theoretically analyze the convergence performance of CRA-DL. Finally, we present numerical results to verify the superiority of the proposed method over existing baselines, showing its enhanced learning performance under Byzantine attacks."
      },
      {
        "id": "oai:arXiv.org:2506.01992v1",
        "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success",
        "link": "https://arxiv.org/abs/2506.01992",
        "author": "Lukas Rauch, Moritz Wirth, Denis Huseljic, Marek Herde, Bernhard Sick, Matthias A{\\ss}enmacher",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01992v1 Announce Type: new \nAbstract: The advent of large language models (LLMs) capable of producing general-purpose representations lets us revisit the practicality of deep active learning (AL): By leveraging frozen LLM embeddings, we can mitigate the computational costs of iteratively fine-tuning large backbones. This study establishes a benchmark and systematically investigates the influence of LLM embedding quality on query strategies in deep AL. We employ five top-performing models from the massive text embedding benchmark (MTEB) leaderboard and two baselines for ten diverse text classification tasks. Our findings reveal key insights: First, initializing the labeled pool using diversity-based sampling synergizes with high-quality embeddings, boosting performance in early AL iterations. Second, the choice of the optimal query strategy is sensitive to embedding quality. While the computationally inexpensive Margin sampling can achieve performance spikes on specific datasets, we find that strategies like Badge exhibit greater robustness across tasks. Importantly, their effectiveness is often enhanced when paired with higher-quality embeddings. Our results emphasize the need for context-specific evaluation of AL strategies, as performance heavily depends on embedding quality and the target task."
      },
      {
        "id": "oai:arXiv.org:2506.02000v1",
        "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts",
        "link": "https://arxiv.org/abs/2506.02000",
        "author": "Abhay Gupta, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02000v1 Announce Type: new \nAbstract: Current large language models (LLMs) struggle to answer questions that span tens of thousands of tokens, especially when multi-hop reasoning is involved. While prior benchmarks explore long-context comprehension or multi-hop reasoning in isolation, none jointly vary context length and reasoning depth in natural narrative settings. We introduce NovelHopQA, the first benchmark to evaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length public-domain novels. A keyword-guided pipeline builds hop-separated chains grounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models and apply oracle-context filtering to ensure all questions are genuinely answerable. Human annotators validate both alignment and hop depth. We noticed consistent accuracy drops with increased hops and context length, even in frontier models-revealing that sheer scale does not guarantee robust reasoning. Our failure mode analysis highlights common breakdowns, such as missed final-hop integration and long-range drift. NovelHopQA offers a controlled diagnostic setting to stress-test multi-hop reasoning at scale."
      },
      {
        "id": "oai:arXiv.org:2506.02005v1",
        "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT",
        "link": "https://arxiv.org/abs/2506.02005",
        "author": "Timothy Do, Pranav Saran, Harshita Poojary, Pranav Prabhu, Sean O'Brien, Vasu Sharma, Kevin Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02005v1 Announce Type: new \nAbstract: In this paper, we address the persistent challenges that figurative language expressions pose for natural language processing (NLP) systems, particularly in low-resource languages such as Konkani. We present a hybrid model that integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM and a linear classifier. This architecture is fine-tuned on a newly introduced annotated dataset for metaphor classification, developed as part of this work. To improve the model's efficiency, we implement a gradient-based attention head pruning strategy. For metaphor classification, the pruned model achieves an accuracy of 78%. We also applied our pruning approach to expand on an existing idiom classification task, achieving 83% accuracy. These results demonstrate the effectiveness of attention head pruning for building efficient NLP tools in underrepresented languages."
      },
      {
        "id": "oai:arXiv.org:2506.02010v1",
        "title": "CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge",
        "link": "https://arxiv.org/abs/2506.02010",
        "author": "Zehua Liu, Xiaolou Li, Chen Chen, Lantian Li, Dong Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02010v1 Announce Type: new \nAbstract: This paper presents the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in Chinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The challenge evaluates two test scenarios: reading in recording studios and Internet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC 2023, which involves CN-CVS for training and CNVSRC-Single/Multi for development and evaluation. However, CNVSRC 2024 introduced two key improvements: (1) a stronger baseline system, and (2) an additional dataset, CN-CVS2-P1, for open tracks to improve data volume and diversity. The new challenge has demonstrated several important innovations in data preprocessing, feature extraction, model design, and training strategies, further pushing the state-of-the-art in Chinese LVC-VSR. More details and resources are available at the official website."
      },
      {
        "id": "oai:arXiv.org:2506.02011v1",
        "title": "OASIS: Online Sample Selection for Continual Visual Instruction Tuning",
        "link": "https://arxiv.org/abs/2506.02011",
        "author": "Minjae Lee, Minhyuk Seo, Tingyu Qu, Tinne Tuytelaars, Jonghyun Choi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02011v1 Announce Type: new \nAbstract: In continual visual instruction tuning (CVIT) scenarios, where multi-modal data continuously arrive in an online streaming manner, training delays from large-scale data significantly hinder real-time adaptation. While existing data selection strategies reduce training overheads, they rely on pre-trained reference models, which are impractical in CVIT setups due to unknown future data. Recent reference model-free online sample selection methods address this issue but typically select a fixed number of samples per batch (e.g., top-k), causing them to suffer from distribution shifts where informativeness varies across batches. To address these limitations, we propose OASIS, an adaptive online sample selection approach for CVIT that: (1) dynamically adjusts selected samples per batch based on relative inter-batch informativeness, and (2) minimizes redundancy of selected samples through iterative selection score updates. Empirical results across various MLLMs, such as LLaVA-1.5 and Qwen-VL-2.5, show that OASIS achieves comparable performance to full-data training using only 25% of the data and outperforms the state-of-the-art."
      },
      {
        "id": "oai:arXiv.org:2506.02012v1",
        "title": "Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing",
        "link": "https://arxiv.org/abs/2506.02012",
        "author": "Zehua Liu, Xiaolou Li, Li Guo, Lantian Li, Dong Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02012v1 Announce Type: new \nAbstract: Visual Speech Recognition (VSR) transcribes speech by analyzing lip movements. Recently, Large Language Models (LLMs) have been integrated into VSR systems, leading to notable performance improvements. However, the potential of LLMs has not been extensively studied, and how to effectively utilize LLMs in VSR tasks remains unexplored. This paper systematically explores how to better leverage LLMs for VSR tasks and provides three key contributions: (1) Scaling Test: We study how the LLM size affects VSR performance, confirming a scaling law in the VSR task. (2) Context-Aware Decoding: We add contextual text to guide the LLM decoding, improving recognition accuracy. (3) Iterative Polishing: We propose iteratively refining LLM outputs, progressively reducing recognition errors. Extensive experiments demonstrate that by these designs, the great potential of LLMs can be largely harnessed, leading to significant VSR performance improvement."
      },
      {
        "id": "oai:arXiv.org:2506.02014v1",
        "title": "Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization",
        "link": "https://arxiv.org/abs/2506.02014",
        "author": "Wang Mengjie, Zhu Huiping, Li Jian, Shi Wenxiu, Zhang Song",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02014v1 Announce Type: new \nAbstract: With the advancement of autonomous and assisted driving technologies, higher demands are placed on the ability to understand complex driving scenarios. Multimodal general large models have emerged as a solution for this challenge. However, applying these models in vertical domains involves difficulties such as data collection, model training, and deployment optimization. This paper proposes a comprehensive method for optimizing multimodal models in driving scenarios, including cone detection, traffic light recognition, speed limit recommendation, and intersection alerts. The method covers key aspects such as dynamic prompt optimization, dataset construction, model training, and deployment. Specifically, the dynamic prompt optimization adjusts the prompts based on the input image content to focus on objects affecting the ego vehicle, enhancing the model's task-specific focus and judgment capabilities. The dataset is constructed by combining real and synthetic data to create a high-quality and diverse multimodal training dataset, improving the model's generalization in complex driving environments. In model training, advanced techniques like knowledge distillation, dynamic fine-tuning, and quantization are integrated to reduce storage and computational costs while boosting performance. Experimental results show that this systematic optimization method not only significantly improves the model's accuracy in key tasks but also achieves efficient resource utilization, providing strong support for the practical application of driving scenario perception technologies."
      },
      {
        "id": "oai:arXiv.org:2506.02015v1",
        "title": "Object-centric Self-improving Preference Optimization for Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2506.02015",
        "author": "Yoonjin Oh, Yongjin Kim, Hyomin Kim, Donghwan Chi, Sungwoong Kim",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02015v1 Announce Type: new \nAbstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly improved both image understanding and generation capabilities. Despite these improvements, MLLMs still struggle with fine-grained visual comprehension, particularly in text-to-image generation tasks. While preference optimization methods have been explored to address these limitations in image understanding tasks, their application to image generation remains largely underexplored. To address this gap, we propose an Object-centric Self-improving Preference Optimization (OSPO) framework designed for text-to-image generation by MLLMs. OSPO leverages the intrinsic reasoning abilities of MLLMs without requiring any external datasets or models. OSPO emphasizes the importance of high-quality preference pair data, which is critical for effective preference optimization. To achieve this, it introduces a self-improving mechanism that autonomously constructs object-level contrastive preference pairs through object-centric prompt perturbation, densification and VQA scoring. This process eliminates ambiguous or disproportionate variations commonly found in naively generated preference pairs, thereby enhancing the effectiveness of preference optimization. We validate OSPO on three representative compositional text-to-image benchmarks, demonstrating substantial performance gains over baseline models."
      },
      {
        "id": "oai:arXiv.org:2506.02016v1",
        "title": "Are classical deep neural networks weakly adversarially robust?",
        "link": "https://arxiv.org/abs/2506.02016",
        "author": "Nuolin Sun, Linyuan Wang, Dongyang Li, Bin Yan, Lei Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02016v1 Announce Type: new \nAbstract: Adversarial attacks have received increasing attention and it has been widely recognized that classical DNNs have weak adversarial robustness. The most commonly used adversarial defense method, adversarial training, improves the adversarial accuracy of DNNs by generating adversarial examples and retraining the model. However, adversarial training requires a significant computational overhead. In this paper, inspired by existing studies focusing on the clustering properties of DNN output features at each layer and the Progressive Feedforward Collapse phenomenon, we propose a method for adversarial example detection and image recognition that uses layer-wise features to construct feature paths and computes the correlation between the examples feature paths and the class-centered feature paths. Experimental results show that the recognition method achieves 82.77% clean accuracy and 44.17% adversarial accuracy on the ResNet-20 with PFC. Compared to the adversarial training method with 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits a trade-off without relying on computationally expensive defense strategies. Furthermore, on the standard ResNet-18, our method maintains this advantage with respective metrics of 80.01% and 46.1%. This result reveals inherent adversarial robustness in DNNs, challenging the conventional understanding of the weak adversarial robustness in DNNs."
      },
      {
        "id": "oai:arXiv.org:2506.02017v1",
        "title": "Fairness through Feedback: Addressing Algorithmic Misgendering in Automatic Gender Recognition",
        "link": "https://arxiv.org/abs/2506.02017",
        "author": "Camilla Quaresmini, Giacomo Zanotti",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02017v1 Announce Type: new \nAbstract: Automatic Gender Recognition (AGR) systems are an increasingly widespread application in the Machine Learning (ML) landscape. While these systems are typically understood as detecting gender, they often classify datapoints based on observable features correlated at best with either male or female sex. In addition to questionable binary assumptions, from an epistemological point of view, this is problematic for two reasons. First, there exists a gap between the categories the system is meant to predict (woman versus man) and those onto which their output reasonably maps (female versus male). What is more, gender cannot be inferred on the basis of such observable features. This makes AGR tools often unreliable, especially in the case of non-binary and gender non-conforming people. We suggest a theoretical and practical rethinking of AGR systems. To begin, distinctions are made between sex, gender, and gender expression. Then, we build upon the observation that, unlike algorithmic misgendering, human-human misgendering is open to the possibility of re-evaluation and correction. We suggest that analogous dynamics should be recreated in AGR, giving users the possibility to correct the system's output. While implementing such a feedback mechanism could be regarded as diminishing the system's autonomy, it represents a way to significantly increase fairness levels in AGR. This is consistent with the conceptual change of paradigm that we advocate for AGR systems, which should be understood as tools respecting individuals' rights and capabilities of self-expression and determination."
      },
      {
        "id": "oai:arXiv.org:2506.02018v1",
        "title": "Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data",
        "link": "https://arxiv.org/abs/2506.02018",
        "author": "Christopher Lee L\\\"ubbers",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02018v1 Announce Type: new \nAbstract: Paraphrasing re-expresses meaning to enhance applications like text simplification, machine translation, and question-answering. Specific paraphrase types facilitate accurate semantic analysis and robust language models. However, existing paraphrase-type generation methods often misalign with human preferences due to reliance on automated metrics and limited human-annotated training data, obscuring crucial aspects of semantic fidelity and linguistic transformations.\n  This study addresses this gap by leveraging a human-ranked paraphrase-type dataset and integrating Direct Preference Optimization (DPO) to align model outputs directly with human judgments. DPO-based training increases paraphrase-type generation accuracy by 3 percentage points over a supervised baseline and raises human preference ratings by 7 percentage points. A newly created human-annotated dataset supports more rigorous future evaluations. Additionally, a paraphrase-type detection model achieves F1 scores of 0.91 for addition/deletion, 0.78 for same polarity substitution, and 0.70 for punctuation changes.\n  These findings demonstrate that preference data and DPO training produce more reliable, semantically accurate paraphrases, enabling downstream applications such as improved summarization and more robust question-answering. The PTD model surpasses automated metrics and provides a more reliable framework for evaluating paraphrase quality, advancing paraphrase-type research toward richer, user-aligned language generation and establishing a stronger foundation for future evaluations grounded in human-centric criteria."
      },
      {
        "id": "oai:arXiv.org:2506.02019v1",
        "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking",
        "link": "https://arxiv.org/abs/2506.02019",
        "author": "E Fan, Weizong Wang, Tianhan Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02019v1 Announce Type: new \nAbstract: Computational Fluid Dynamics (CFD) is essential for scientific and engineering advancements but is limited by operational complexity and the need for extensive expertise. This paper presents ChatCFD, a large language model-driven pipeline that automates CFD workflows within the OpenFOAM framework. It enables users to configure and execute complex simulations from natural language prompts or published literature with minimal expertise. The innovation is its structured approach to database construction, configuration validation, and error reflection, integrating CFD and OpenFOAM knowledge with general language models to improve accuracy and adaptability. Validation shows ChatCFD can autonomously reproduce published CFD results, handling complex, unseen configurations beyond basic examples, a task challenging for general language models."
      },
      {
        "id": "oai:arXiv.org:2506.02020v1",
        "title": "Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying",
        "link": "https://arxiv.org/abs/2506.02020",
        "author": "Youze Xue, Dian Li, Gang Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02020v1 Announce Type: new \nAbstract: With the rapid advancement of multi-modal large language models (MLLMs) in recent years, the foundational Contrastive Language-Image Pretraining (CLIP) framework has been successfully extended to MLLMs, enabling more powerful and universal multi-modal embeddings for a wide range of retrieval tasks. Despite these developments, the core contrastive learning paradigm remains largely unchanged from CLIP-style models to MLLMs. Within this framework, the effective mining of hard negative samples continues to be a critical factor for enhancing performance. Prior works have introduced both offline and online strategies for hard negative mining to improve the efficiency of contrastive learning. While these approaches have led to improved multi-modal embeddings, the specific contribution of each hard negative sample to the learning process has not been thoroughly investigated. In this work, we conduct a detailed analysis of the gradients of the info-NCE loss with respect to the query, positive, and negative samples, elucidating the role of hard negatives in updating model parameters. Building upon this analysis, we propose to explicitly amplify the gradients associated with hard negative samples, thereby encouraging the model to learn more discriminative embeddings. Our multi-modal embedding model, trained with the proposed Explicit Gradient Amplifier and based on the LLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the MMEB benchmark compared to previous methods utilizing the same MLLM backbone. Furthermore, when integrated with our self-developed MLLM, QQMM, our approach attains the top rank on the MMEB leaderboard. Code and models are released on https://github.com/QQ-MM/QQMM-embed."
      },
      {
        "id": "oai:arXiv.org:2506.02021v1",
        "title": "Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics",
        "link": "https://arxiv.org/abs/2506.02021",
        "author": "Yinjie Zhao, Heng Zhao, Bihan Wen, Yew-Soon Ong, Joey Tianyi Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02021v1 Announce Type: new \nAbstract: With the rapid development of vision tasks and the scaling on datasets and models, redundancy reduction in vision datasets has become a key area of research. To address this issue, dataset distillation (DD) has emerged as a promising approach to generating highly compact synthetic datasets with significantly less redundancy while preserving essential information. However, while DD has been extensively studied for image datasets, DD on video datasets remains underexplored. Video datasets present unique challenges due to the presence of temporal information and varying levels of redundancy across different classes. Existing DD approaches assume a uniform level of temporal redundancy across all different video semantics, which limits their effectiveness on video datasets. In this work, we propose Dynamic-Aware Video Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop reward function is proposed to update the RL agent policy. To the best of our knowledge, this is the first study to introduce adaptive temporal resolution based on video semantics in video dataset distillation. Our approach significantly outperforms existing DD methods, demonstrating substantial improvements in performance. This work paves the way for future research on more efficient and semantic-adaptive video dataset distillation research."
      },
      {
        "id": "oai:arXiv.org:2506.02022v1",
        "title": "Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs",
        "link": "https://arxiv.org/abs/2506.02022",
        "author": "Aditya Kanade, Tanuja Ganu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02022v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) show reasoning promise, yet their visual perception is a critical bottleneck. Strikingly, MLLMs can produce correct answers even while misinterpreting crucial visual elements, masking these underlying failures. Our preliminary study on a joint perception-reasoning dataset revealed that for one leading MLLM, 29% of its correct answers to reasoning questions still exhibited visual perception errors. To systematically address this, we introduce \"Do You See Me\", a scalable benchmark with 1,758 images and 2,612 questions. It spans seven human-psychology inspired subtasks in 2D and 3D, featuring controllable complexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading closed-source and 5 major open-source models reveal a stark deficit: humans achieve 96.49% accuracy, while top MLLMs average below 50%. This performance gap widens rapidly with increased task complexity (e.g., from 12% to 45% in the visual form constancy subtask). Further analysis into the root causes suggests that failures stem from challenges like misallocated visual attention and the instability of internal representations for fine-grained details, especially at or below encoder patch resolution. This underscores an urgent need for MLLMs with truly robust visual perception. The benchmark dataset, source code and evaluation scripts are available at https://github.com/microsoft/Do-You-See-Me."
      },
      {
        "id": "oai:arXiv.org:2506.02037v1",
        "title": "FinS-Pilot: A Benchmark for Online Financial System",
        "link": "https://arxiv.org/abs/2506.02037",
        "author": "Feng Wang, Yiding Sun, Jiaxin Mao, Wei Xue, Danqing Xu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02037v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated remarkable capabilities across various professional domains, with their performance typically evaluated through standardized benchmarks. However, the development of financial RAG benchmarks has been constrained by data confidentiality issues and the lack of dynamic data integration. To address this issue, we introduces FinS-Pilot, a novel benchmark for evaluating RAG systems in online financial applications. Constructed from real-world financial assistant interactions, our benchmark incorporates both real-time API data and structured text sources, organized through an intent classification framework covering critical financial domains such as equity analysis and macroeconomic forecasting. The benchmark enables comprehensive evaluation of financial assistants' capabilities in handling both static knowledge and time-sensitive market information. Through systematic experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying models suitable for financial applications while addressing the current gap in specialized evaluation tools for the financial domain. Our work contributes both a practical evaluation framework and a curated dataset to advance research in financial NLP systems. The code and dataset are accessible on GitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}."
      },
      {
        "id": "oai:arXiv.org:2506.02041v1",
        "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA",
        "link": "https://arxiv.org/abs/2506.02041",
        "author": "Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, Jinfeng Bai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02041v1 Announce Type: new \nAbstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes."
      },
      {
        "id": "oai:arXiv.org:2506.02050v1",
        "title": "Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids",
        "link": "https://arxiv.org/abs/2506.02050",
        "author": "Qingyu Xiao, Yuanlin Chang, Youtian Du",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02050v1 Announce Type: new \nAbstract: Effective agent exploration remains a core challenge in reinforcement learning (RL) for complex discrete state-space environments, particularly under partial observability. This paper presents a decoupled hierarchical RL framework integrating state abstraction (DcHRL-SA) to address this issue. The proposed method employs a dual-level architecture, consisting of a high level RL-based actor and a low-level rule-based policy, to promote effective exploration. Additionally, state abstraction method is incorporated to cluster discrete states, effectively lowering state dimensionality. Experiments conducted in two discrete customized grid environments demonstrate that the proposed approach consistently outperforms PPO in terms of exploration efficiency, convergence speed, cumulative reward, and policy stability. These results demonstrate a practical approach for integrating decoupled hierarchical policies and state abstraction in discrete grids with large-scale exploration space. Code will be available at https://github.com/XQY169/DcHRL-SA."
      },
      {
        "id": "oai:arXiv.org:2506.02053v1",
        "title": "Generalization Performance of Ensemble Clustering: From Theory to Algorithm",
        "link": "https://arxiv.org/abs/2506.02053",
        "author": "Xu Zhang, Haoye Qiu, Weixuan Liang, Hui Liu, Junhui Hou, Yuheng Jia",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02053v1 Announce Type: new \nAbstract: Ensemble clustering has demonstrated great success in practice; however, its theoretical foundations remain underexplored. This paper examines the generalization performance of ensemble clustering, focusing on generalization error, excess risk and consistency. We derive a convergence rate of generalization error bound and excess risk bound both of $\\mathcal{O}(\\sqrt{\\frac{\\log n}{m}}+\\frac{1}{\\sqrt{n}})$, with $n$ and $m$ being the numbers of samples and base clusterings. Based on this, we prove that when $m$ and $n$ approach infinity and $m$ is significantly larger than log $n$, i.e., $m,n\\to \\infty, m\\gg \\log n$, ensemble clustering is consistent. Furthermore, recognizing that $n$ and $m$ are finite in practice, the generalization error cannot be reduced to zero. Thus, by assigning varying weights to finite clusterings, we minimize the error between the empirical average clusterings and their expectation. From this, we theoretically demonstrate that to achieve better clustering performance, we should minimize the deviation (bias) of base clustering from its expectation and maximize the differences (diversity) among various base clusterings. Additionally, we derive that maximizing diversity is nearly equivalent to a robust (min-max) optimization model. Finally, we instantiate our theory to develop a new ensemble clustering algorithm. Compared with SOTA methods, our approach achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t. NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC."
      },
      {
        "id": "oai:arXiv.org:2506.02058v1",
        "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
        "link": "https://arxiv.org/abs/2506.02058",
        "author": "Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02058v1 Announce Type: new \nAbstract: Accurate evaluation of large language models (LLMs) is crucial for understanding their capabilities and guiding their development. However, current evaluations often inconsistently reflect the actual capacities of these models. In this paper, we demonstrate that one of many contributing factors to this \\textit{evaluation crisis} is the oversight of unseen knowledge -- information encoded by LLMs but not directly observed or not yet observed during evaluations. We introduce KnowSum, a statistical framework designed to provide a more comprehensive assessment by quantifying the unseen knowledge for a class of evaluation tasks. KnowSum estimates the unobserved portion by extrapolating from the appearance frequencies of observed knowledge instances. We demonstrate the effectiveness and utility of KnowSum across three critical applications: estimating total knowledge, evaluating information retrieval effectiveness, and measuring output diversity. Our experiments reveal that a substantial volume of knowledge is omitted when relying solely on observed LLM performance. Importantly, KnowSum yields significantly different comparative rankings for several common LLMs based on their internal knowledge."
      },
      {
        "id": "oai:arXiv.org:2506.02062v1",
        "title": "Predicting Blood Type: Assessing Model Performance with ROC Analysis",
        "link": "https://arxiv.org/abs/2506.02062",
        "author": "Malik A. Altayar, Muhyeeddin Alqaraleh, Mowafaq Salem Alzboon, Wesam T. Almagharbeh",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02062v1 Announce Type: new \nAbstract: Introduction: Personal identification is a critical aspect of forensic sciences, security, and healthcare. While conventional biometrics systems such as DNA profiling and iris scanning offer high accuracy, they are time-consuming and costly. Objectives: This study investigates the relationship between fingerprint patterns and ABO blood group classification to explore potential correlations between these two traits. Methods: The study analyzed 200 individuals, categorizing their fingerprints into three types: loops, whorls, and arches. Blood group classification was also recorded. Statistical analysis, including chi-square and Pearson correlation tests, was used to assess associations between fingerprint patterns and blood groups. Results: Loops were the most common fingerprint pattern, while blood group O+ was the most prevalent among the participants. Statistical analysis revealed no significant correlation between fingerprint patterns and blood groups (p > 0.05), suggesting that these traits are independent. Conclusions: Although the study showed limited correlation between fingerprint patterns and ABO blood groups, it highlights the importance of future research using larger and more diverse populations, incorporating machine learning approaches, and integrating multiple biometric signals. This study contributes to forensic science by emphasizing the need for rigorous protocols and comprehensive investigations in personal identification."
      },
      {
        "id": "oai:arXiv.org:2506.02065v1",
        "title": "EWGN: Elastic Weight Generation and Context Switching in Deep Learning",
        "link": "https://arxiv.org/abs/2506.02065",
        "author": "Shriraj P. Sawant, Krishna P. Miyapuram",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02065v1 Announce Type: new \nAbstract: The ability to learn and retain a wide variety of tasks is a hallmark of human intelligence that has inspired research in artificial general intelligence. Continual learning approaches provide a significant step towards achieving this goal. It has been known that task variability and context switching are challenging for learning in neural networks. Catastrophic forgetting refers to the poor performance on retention of a previously learned task when a new task is being learned. Switching between different task contexts can be a useful approach to mitigate the same by preventing the interference between the varying task weights of the network. This paper introduces Elastic Weight Generative Networks (EWGN) as an idea for context switching between two different tasks. The proposed EWGN architecture uses an additional network that generates the weights of the primary network dynamically while consolidating the weights learned. The weight generation is input-dependent and thus enables context switching. Using standard computer vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of previously learned task representations in Fully Connected Networks, Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient Descent and Elastic Weight Consolidation learning algorithms. Understanding dynamic weight generation and context-switching ability can be useful in enabling continual learning for improved performance."
      },
      {
        "id": "oai:arXiv.org:2506.02070v1",
        "title": "An Introduction to Flow Matching and Diffusion Models",
        "link": "https://arxiv.org/abs/2506.02070",
        "author": "Peter Holderrieth, Ezra Erives",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02070v1 Announce Type: new \nAbstract: Diffusion and flow-based models have become the state of the art for generative AI across a wide range of data modalities, including images, videos, shapes, molecules, music, and more! These notes are originally from https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter) term, and are intended to accompany other course content, including lectures and labs. Overall, they function as a self-contained introduction to both flow matching and diffusion models, starting with ordinary and stochastic differential equations, and culminating in flow matching, score matching, classifier-free guidance, and the inner workings of modern, state-of-the-art models for image and video. These notes, and the accompanying course, are ideal for students and practitioners alike who want to develop a principled understanding of the theory and practice of generative AI."
      },
      {
        "id": "oai:arXiv.org:2506.02077v1",
        "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition",
        "link": "https://arxiv.org/abs/2506.02077",
        "author": "Yoonjun Cho, Soeun Kim, Dongjae Jeon, Kyelim Lee, Beomsoo Lee, Albert No",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02077v1 Announce Type: new \nAbstract: Decomposing weight matrices into quantization and low-rank components ($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used technique for compressing large language models (LLMs). Existing joint optimization methods iteratively alternate between quantization and low-rank approximation. However, these methods tend to prioritize one component at the expense of the other, resulting in suboptimal decompositions that fail to leverage each component's unique strengths. In this work, we introduce Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank components the specific role of capturing activation-sensitive weights. This structured decomposition mitigates outliers' negative impact on quantization, enabling more effective balance between quantization and low-rank approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B demonstrate that incorporating ODLRI into the joint optimization framework consistently reduces activation-aware error, minimizes quantization scale, and improves perplexity and zero-shot accuracy in low-bit settings."
      },
      {
        "id": "oai:arXiv.org:2506.02079v1",
        "title": "Robust Federated Learning against Noisy Clients via Masked Optimization",
        "link": "https://arxiv.org/abs/2506.02079",
        "author": "Xuefeng Jiang, Tian Wen, Zhiqin Yang, Lvhua Wu, Yufeng Chen, Sheng Sun, Yuwei Wang, Min Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02079v1 Announce Type: new \nAbstract: In recent years, federated learning (FL) has made significant advance in privacy-sensitive applications. However, it can be hard to ensure that FL participants provide well-annotated data for training. The corresponding annotations from different clients often contain complex label noise at varying levels. This label noise issue has a substantial impact on the performance of the trained models, and clients with greater noise levels can be largely attributed for this degradation. To this end, it is necessary to develop an effective optimization strategy to alleviate the adverse effects of these noisy clients.In this study, we present a two-stage optimization framework, MaskedOptim, to address this intricate label noise problem. The first stage is designed to facilitate the detection of noisy clients with higher label noise rates. The second stage focuses on rectifying the labels of the noisy clients' data through an end-to-end label correction mechanism, aiming to mitigate the negative impacts caused by misinformation within datasets. This is achieved by learning the potential ground-truth labels of the noisy clients' datasets via backpropagation. To further enhance the training robustness, we apply the geometric median based model aggregation instead of the commonly-used vanilla averaged model aggregation. We implement sixteen related methods and conduct evaluations on three image datasets and one text dataset with diverse label noise patterns for a comprehensive comparison. Extensive experimental results indicate that our proposed framework shows its robustness in different scenarios. Additionally, our label correction framework effectively enhances the data quality of the detected noisy clients' local datasets. % Our codes will be open-sourced to facilitate related research communities. Our codes are available via https://github.com/Sprinter1999/MaskedOptim ."
      },
      {
        "id": "oai:arXiv.org:2506.02081v1",
        "title": "RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection",
        "link": "https://arxiv.org/abs/2506.02081",
        "author": "Chihiro Maru, Shoetsu Sato",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02081v1 Announce Type: new \nAbstract: Inspired by the success of large language models (LLMs) in natural language processing, recent research has explored the building of time series foundation models and applied them to tasks such as forecasting, classification, and anomaly detection. However, their performances vary between different domains and tasks. In LLM-based approaches, test-time adaptation using example-based prompting has become common, owing to the high cost of retraining. In the context of anomaly detection, which is the focus of this study, providing normal examples from the target domain can also be effective. However, time series foundation models do not naturally acquire the ability to interpret or utilize examples or instructions, because the nature of time series data used during training does not encourage such capabilities. To address this limitation, we propose a retrieval augmented time series foundation model (RATFM), which enables pretrained time series foundation models to incorporate examples of test-time adaptation. We show that RATFM achieves a performance comparable to that of in-domain fine-tuning while avoiding domain-dependent fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset including nine domains, confirms the effectiveness of the proposed approach."
      },
      {
        "id": "oai:arXiv.org:2506.02084v1",
        "title": "Temporal Causal-based Simulation for Realistic Time-series Generation",
        "link": "https://arxiv.org/abs/2506.02084",
        "author": "Nikolaos Gkorgkolis, Nikolaos Kougioulis, MingXue Wang, Bora Caglayan, Andrea Tonon, Dario Simionato, Ioannis Tsamardinos",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02084v1 Announce Type: new \nAbstract: Causal Discovery plays a pivotal role in revealing relationships among observed variables, particularly in the temporal setup. While the majority of CD methods rely on synthetic data for evaluation, and recently for training, these fall short in accurately mirroring real-world scenarios; an effect even more evident in temporal data. Generation techniques depending on simplified assumptions on causal structure, effects and time, limit the quality and diversity of the simulated data. In this work, we introduce Temporal Causal-based Simulation (TCS), a robust framework for generating realistic time-series data and their associated temporal causal graphs. The approach is structured in three phases: estimating the true lagged causal structure of the data, approximating the functional dependencies between variables and learning the noise distribution of the corresponding causal model, each part of which can be explicitly tailored based on data assumptions and characteristics. Through an extensive evaluation process, we highlight that single detection methods for generated data discrimination prove inadequate, accentuating it as a multifaceted challenge. For this, we detail a Min-max optimization phase that draws on AutoML techniques. Our contributions include a flexible, model-agnostic pipeline for generating realistic temporal causal data, a thorough evaluation setup which enhances the validity of the generated datasets and insights into the challenges posed by realistic data generation. Through experiments involving not only real but also semi-synthetic and purely synthetic datasets, we demonstrate that while sampling realistic causal data remains a complex task, our method enriches the domain of generating sensible causal-based temporal data."
      },
      {
        "id": "oai:arXiv.org:2506.02089v1",
        "title": "SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design",
        "link": "https://arxiv.org/abs/2506.02089",
        "author": "Zeng Wang, Minghao Shao, Rupesh Karn, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02089v1 Announce Type: new \nAbstract: Large Language Models (LLMs) offer transformative capabilities for hardware design automation, particularly in Verilog code generation. However, they also pose significant data security challenges, including Verilog evaluation data contamination, intellectual property (IP) design leakage, and the risk of malicious Verilog generation. We introduce SALAD, a comprehensive assessment that leverages machine unlearning to mitigate these threats. Our approach enables the selective removal of contaminated benchmarks, sensitive IP and design artifacts, or malicious code patterns from pre-trained LLMs, all without requiring full retraining. Through detailed case studies, we demonstrate how machine unlearning techniques effectively reduce data security risks in LLM-aided hardware design."
      },
      {
        "id": "oai:arXiv.org:2506.02092v1",
        "title": "Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models",
        "link": "https://arxiv.org/abs/2506.02092",
        "author": "Francesco De Santis, Philippe Bich, Gabriele Ciravegna, Pietro Barbiero, Danilo Giordano, Tania Cerquitelli",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02092v1 Announce Type: new \nAbstract: To increase the trustworthiness of deep neural networks, it is critical to improve the understanding of how they make decisions. This paper introduces a novel unsupervised concept-based model for image classification, named Learnable Concept-Based Model (LCBM) which models concepts as random variables within a Bernoulli latent space. Unlike traditional methods that either require extensive human supervision or suffer from limited scalability, our approach employs a reduced number of concepts without sacrificing performance. We demonstrate that LCBM surpasses existing unsupervised concept-based models in generalization capability and nearly matches the performance of black-box models. The proposed concept representation enhances information retention and aligns more closely with human understanding. A user study demonstrates the discovered concepts are also more intuitive for humans to interpret. Finally, despite the use of concept embeddings, we maintain model interpretability by means of a local linear combination of concepts."
      },
      {
        "id": "oai:arXiv.org:2506.02095v1",
        "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences",
        "link": "https://arxiv.org/abs/2506.02095",
        "author": "Hyojin Bahng, Caroline Chan, Fredo Durand, Phillip Isola",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02095v1 Announce Type: new \nAbstract: Learning alignment between language and vision is a fundamental challenge, especially as multimodal data becomes increasingly detailed and complex. Existing methods often rely on collecting human or AI preferences, which can be costly and time-intensive. We propose an alternative approach that leverages cycle consistency as a supervisory signal. Given an image and generated text, we map the text back to image space using a text-to-image model and compute the similarity between the original image and its reconstruction. Analogously, for text-to-image generation, we measure the textual similarity between an input caption and its reconstruction through the cycle. We use the cycle consistency score to rank candidates and construct a preference dataset of 866K comparison pairs. The reward model trained on our dataset outperforms state-of-the-art alignment metrics on detailed captioning, with superior inference-time scalability when used as a verifier for Best-of-N sampling. Furthermore, performing DPO and Diffusion DPO using our dataset enhances performance across a wide range of vision-language tasks and text-to-image generation. Our dataset, model, and code are at https://cyclereward.github.io"
      },
      {
        "id": "oai:arXiv.org:2506.02096v1",
        "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
        "link": "https://arxiv.org/abs/2506.02096",
        "author": "Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, Michael Qizhe Shieh",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02096v1 Announce Type: new \nAbstract: Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns."
      },
      {
        "id": "oai:arXiv.org:2506.02098v1",
        "title": "LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale",
        "link": "https://arxiv.org/abs/2506.02098",
        "author": "Miran \\\"Ozdogan, Gilad Landau, Gereon Elvers, Dulhan Jayalath, Pratik Somaiya, Francesco Mantegna, Mark Woolrich, Oiwi Parker Jones",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02098v1 Announce Type: new \nAbstract: LibriBrain represents the largest single-subject MEG dataset to date for speech decoding, with over 50 hours of recordings -- 5$\\times$ larger than the next comparable dataset and 50$\\times$ larger than most. This unprecedented `depth' of within-subject data enables exploration of neural representations at a scale previously unavailable with non-invasive methods. LibriBrain comprises high-quality MEG recordings together with detailed annotations from a single participant listening to naturalistic spoken English, covering nearly the full Sherlock Holmes canon. Designed to support advances in neural decoding, LibriBrain comes with a Python library for streamlined integration with deep learning frameworks, standard data splits for reproducibility, and baseline results for three foundational decoding tasks: speech detection, phoneme classification, and word classification. Baseline experiments demonstrate that increasing training data yields substantial improvements in decoding performance, highlighting the value of scaling up deep, within-subject datasets. By releasing this dataset, we aim to empower the research community to advance speech decoding methodologies and accelerate the development of safe, effective clinical brain-computer interfaces."
      },
      {
        "id": "oai:arXiv.org:2506.02112v1",
        "title": "SAB3R: Semantic-Augmented Backbone in 3D Reconstruction",
        "link": "https://arxiv.org/abs/2506.02112",
        "author": "Xuweiyi Chen, Tian Xia, Sihan Xu, Jianing Yang, Joyce Chai, Zezhou Cheng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02112v1 Announce Type: new \nAbstract: We introduce a new task, Map and Locate, which unifies the traditionally distinct objectives of open-vocabulary segmentation - detecting and segmenting object instances based on natural language queries - and 3D reconstruction, the process of estimating a scene's 3D structure from visual inputs. Specifically, Map and Locate involves generating a point cloud from an unposed video and segmenting object instances based on open-vocabulary queries. This task serves as a critical step toward real-world embodied AI applications and introduces a practical task that bridges reconstruction, recognition and reorganization. To tackle this task, we introduce a simple yet effective baseline, which we denote as SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer vision, and incorporates a lightweight distillation strategy. This method transfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP and DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary frozen networks, our model generates per-pixel semantic features and constructs cohesive point maps in a single forward pass. Compared to separately deploying MASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the Map and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic segmentation and 3D tasks to comprehensively validate its effectiveness."
      },
      {
        "id": "oai:arXiv.org:2506.02126v1",
        "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains",
        "link": "https://arxiv.org/abs/2506.02126",
        "author": "Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, Yuyin Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02126v1 Announce Type: new \nAbstract: Recent advances in reasoning-enhanced Large Language Models such as OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex tasks. However, the quality and transparency of their internal reasoning processes remain underexplored. This work moves beyond the final-answer accuracy and investigates step-by-step reasoning in the medical and mathematical domains by explicitly decomposing the thinking trajectories into two parts: knowledge and reasoning. Specifically, we introduce a fine-grained evaluation framework that judges: (1) the correctness of knowledge used (measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured by Information Gain (InfoGain)). Using this framework, we study R1-distilled and base Qwen models trained with supervised fine-tuning (SFT) and/or reinforcement learning (RL) in the medical and math domains. Three intriguing findings emerge: (1) The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain through either SFT or RL. (2) SFT raises final-answer accuracy in both domains, but often at the cost of reasoning quality: InfoGain drops by 38.9% on average compared with untrained models; In the medical domain, however, SFT remains crucial because domain knowledge is indispensable. (3) RL enhances medical reasoning by pruning inaccurate or irrelevant knowledge from reasoning paths, thereby improving both reasoning accuracy and knowledge correctness."
      },
      {
        "id": "oai:arXiv.org:2506.02132v1",
        "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models",
        "link": "https://arxiv.org/abs/2506.02132",
        "author": "Michael Li, Nishant Subramani",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02132v1 Announce Type: new \nAbstract: Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand today's language models, we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) represent lexical identity and inflectional morphology. We train linear and nonlinear classifiers on layer-wise activations to predict word lemmas and inflectional features. We discover that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout the layers. Further analysis reveals that these models encode inflectional morphology through generalizable abstractions, but rely predominantly on memorization to encode lexical identity. Remarkably, these patterns emerge across all 16 models we test, despite differences in architecture, size, and training regime (including pretrained and instruction-tuned variants). This consistency suggests that, despite substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties could be fundamental for next token prediction and are learned early during pretraining. Our code is available at https://github.com/ml5885/model_internal_sleuthing."
      },
      {
        "id": "oai:arXiv.org:2506.02134v1",
        "title": "ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels",
        "link": "https://arxiv.org/abs/2506.02134",
        "author": "Rishi Raj Sahoo, Rucha Bhalchandra Joshi, Subhankar Mishra",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02134v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) achieve high performance across many applications but function as black-box models, limiting their use in critical domains like healthcare and criminal justice. Explainability methods address this by providing feature-level explanations that identify important node attributes for predictions. These explanations create privacy risks. Combined with auxiliary information, feature explanations can enable adversaries to reconstruct graph structure, exposing sensitive relationships. Existing graph reconstruction attacks assume access to original auxiliary data, but practical systems use differential privacy to protect node features and labels while providing explanations for transparency. We study a threat model where adversaries access public feature explanations along with privatized node features and labels. We show that existing explanation-based attacks like GSEF perform poorly with privatized data due to noise from differential privacy mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios with public explanations and privatized auxiliary data. Our method adapts explanation-based frameworks by incorporating denoising mechanisms that handle differential privacy noise while exploiting structural signals in explanations. Experiments across multiple datasets show ReconXF outperforms SoTA methods in privatized settings, with improvements in AUC and average precision. Results indicate that public explanations combined with denoising enable graph structure recovery even under the privacy protection of auxiliary data. Code is available at (link to be made public after acceptance)."
      },
      {
        "id": "oai:arXiv.org:2506.02138v1",
        "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability",
        "link": "https://arxiv.org/abs/2506.02138",
        "author": "Yarden Bakish, Itamar Zimerman, Hila Chefer, Lior Wolf",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02138v1 Announce Type: new \nAbstract: The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available."
      },
      {
        "id": "oai:arXiv.org:2506.02147v1",
        "title": "BabyLM's First Constructions: Causal interventions provide a signal of learning",
        "link": "https://arxiv.org/abs/2506.02147",
        "author": "Joshua Rozner, Leonie Weissweiler, Cory Shain",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02147v1 Announce Type: new \nAbstract: Construction grammar posits that children acquire constructions (form-meaning pairings) from the statistics of their environment. Recent work supports this hypothesis by showing sensitivity to constructions in pretrained language models (PLMs), including one recent study (Rozner et al., 2025) demonstrating that constructions shape the PLM's output distribution. However, models under study have generally been trained on developmentally implausible amounts of data, casting doubt on their relevance to human language learning. Here we use Rozner et al.'s methods to evaluate constructional learning in models from the 2024 BabyLM challenge. Our results show that even when trained on developmentally plausible quantities of data, models represent diverse constructions, even hard cases that are superficially indistinguishable. We further find correlational evidence that constructional performance may be functionally relevant: models that better represent constructions perform better on the BabyLM benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.02150v1",
        "title": "Implicit Deformable Medical Image Registration with Learnable Kernels",
        "link": "https://arxiv.org/abs/2506.02150",
        "author": "Stefano Fogarollo, Gregor Laimer, Reto Bale, Matthias Harders",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02150v1 Announce Type: new \nAbstract: Deformable medical image registration is an essential task in computer-assisted interventions. This problem is particularly relevant to oncological treatments, where precise image alignment is necessary for tracking tumor growth, assessing treatment response, and ensuring accurate delivery of therapies. Recent AI methods can outperform traditional techniques in accuracy and speed, yet they often produce unreliable deformations that limit their clinical adoption. In this work, we address this challenge and introduce a novel implicit registration framework that can predict accurate and reliable deformations. Our insight is to reformulate image registration as a signal reconstruction problem: we learn a kernel function that can recover the dense displacement field from sparse keypoint correspondences. We integrate our method in a novel hierarchical architecture, and estimate the displacement field in a coarse-to-fine manner. Our formulation also allows for efficient refinement at test time, permitting clinicians to easily adjust registrations when needed. We validate our method on challenging intra-patient thoracic and abdominal zero-shot registration tasks, using public and internal datasets from the local University Hospital. Our method not only shows competitive accuracy to state-of-the-art approaches, but also bridges the generalization gap between implicit and explicit registration techniques. In particular, our method generates deformations that better preserve anatomical relationships and matches the performance of specialized commercial systems, underscoring its potential for clinical adoption."
      },
      {
        "id": "oai:arXiv.org:2506.02154v1",
        "title": "Z-Error Loss for Training Neural Networks",
        "link": "https://arxiv.org/abs/2506.02154",
        "author": "Guillaume Godin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02154v1 Announce Type: new \nAbstract: Outliers introduce significant training challenges in neural networks by propagating erroneous gradients, which can degrade model performance and generalization. We propose the Z-Error Loss, a statistically principled approach that minimizes outlier influence during training by masking the contribution of data points identified as out-of-distribution within each batch. This method leverages batch-level statistics to automatically detect and exclude anomalous samples, allowing the model to focus its learning on the true underlying data structure. Our approach is robust, adaptive to data quality, and provides valuable diagnostics for data curation and cleaning."
      },
      {
        "id": "oai:arXiv.org:2506.02157v1",
        "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation",
        "link": "https://arxiv.org/abs/2506.02157",
        "author": "Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02157v1 Announce Type: new \nAbstract: Neural transducers (NT) provide an effective framework for speech streaming, demonstrating strong performance in automatic speech recognition (ASR). However, the application of NT to speech translation (ST) remains challenging, as existing approaches struggle with word reordering and performance degradation when jointly modeling ASR and ST, resulting in a gap with attention-based encoder-decoder (AED) models. Existing NT-based ST approaches also suffer from high computational training costs. To address these issues, we propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech Recognition and Translation), a novel framework that factorizes ASR and translation tasks to better handle reordering. To ensure robust ST while preserving ASR performance, we use self-distillation with CTC consistency regularization. Moreover, we improve computational efficiency by incorporating best practices from ASR transducers, including a down-sampled hierarchical encoder, a stateless predictor, and a pruned transducer loss to reduce training complexity. Finally, we introduce a blank penalty during decoding, reducing deletions and improving translation quality. Our approach is evaluated on three conversational datasets Arabic, Spanish, and Mandarin achieving new state-of-the-art performance among NT models and substantially narrowing the gap with AED-based systems."
      },
      {
        "id": "oai:arXiv.org:2506.02161v1",
        "title": "TIIF-Bench: How Does Your T2I Model Follow Your Instructions?",
        "link": "https://arxiv.org/abs/2506.02161",
        "author": "Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, Lei Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02161v1 Announce Type: new \nAbstract: The rapid advancements of Text-to-Image (T2I) models have ushered in a new phase of AI-generated content, marked by their growing ability to interpret and follow user instructions. However, existing T2I model evaluation benchmarks fall short in limited prompt diversity and complexity, as well as coarse evaluation metrics, making it difficult to evaluate the fine-grained alignment performance between textual instructions and generated images. In this paper, we present TIIF-Bench (Text-to-Image Instruction Following Benchmark), aiming to systematically assess T2I models' ability in interpreting and following intricate textual instructions. TIIF-Bench comprises a set of 5000 prompts organized along multiple dimensions, which are categorized into three levels of difficulties and complexities. To rigorously evaluate model robustness to varying prompt lengths, we provide a short and a long version for each prompt with identical core semantics. Two critical attributes, i.e., text rendering and style control, are introduced to evaluate the precision of text synthesis and the aesthetic coherence of T2I models. In addition, we collect 100 high-quality designer level prompts that encompass various scenarios to comprehensively assess model performance. Leveraging the world knowledge encoded in large vision language models, we propose a novel computable framework to discern subtle variations in T2I model outputs. Through meticulous benchmarking of mainstream T2I models on TIIF-Bench, we analyze the pros and cons of current T2I models and reveal the limitations of current T2I benchmarks. Project Page: https://a113n-w3i.github.io/TIIF_Bench/."
      },
      {
        "id": "oai:arXiv.org:2506.02164v1",
        "title": "Quantifying task-relevant representational similarity using decision variable correlation",
        "link": "https://arxiv.org/abs/2506.02164",
        "author": "Yu (Eric),  Qian, Wilson S. Geisler, Xue-Xin Wei",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02164v1 Announce Type: new \nAbstract: Previous studies have compared the brain and deep neural networks trained on image classification. Intriguingly, while some suggest that their representations are highly similar, others argued the opposite. Here, we propose a new approach to characterize the similarity of the decision strategies of two observers (models or brains) using decision variable correlation (DVC). DVC quantifies the correlation between decoded decisions on individual samples in a classification task and thus can capture task-relevant information rather than general representational alignment. We evaluate this method using monkey V4/IT recordings and models trained on image classification tasks.\n  We find that model--model similarity is comparable to monkey--monkey similarity, whereas model--monkey similarity is consistently lower and, surprisingly, decreases with increasing ImageNet-1k performance. While adversarial training enhances robustness, it does not improve model--monkey similarity in task-relevant dimensions; however, it markedly increases model--model similarity. Similarly, pre-training on larger datasets does not improve model--monkey similarity. These results suggest a fundamental divergence between the task-relevant representations in monkey V4/IT and those learned by models trained on image classification tasks."
      },
      {
        "id": "oai:arXiv.org:2506.02167v1",
        "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos",
        "link": "https://arxiv.org/abs/2506.02167",
        "author": "Aditi Tiwari, Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft, Heng Ji, Klara Nahrstedt",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02167v1 Announce Type: new \nAbstract: Modern AI systems struggle most in environments where reliability is critical - scenes with smoke, poor visibility, and structural deformation. Each year, tens of thousands of firefighters are injured on duty, often due to breakdowns in situational perception. We introduce Fire360, a benchmark for evaluating perception and reasoning in safety-critical firefighting scenarios. The dataset includes 228 360-degree videos from professional training sessions under diverse conditions (e.g., low light, thermal distortion), annotated with action segments, object locations, and degradation metadata. Fire360 supports five tasks: Visual Question Answering, Temporal Action Captioning, Object Localization, Safety-Critical Reasoning, and Transformed Object Retrieval (TOR). TOR tests whether models can match pristine exemplars to fire-damaged counterparts in unpaired scenes, evaluating transformation-invariant recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag significantly, exposing failures in reasoning under degradation. By releasing Fire360 and its evaluation suite, we aim to advance models that not only see, but also remember, reason, and act under uncertainty. The dataset is available at: https://uofi.box.com/v/fire360dataset."
      },
      {
        "id": "oai:arXiv.org:2506.02168v1",
        "title": "An Approximation Theory Perspective on Machine Learning",
        "link": "https://arxiv.org/abs/2506.02168",
        "author": "Hrushikesh N. Mhaskar, Efstratios Tsoukanis, Ameya D. Jagtap",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02168v1 Announce Type: new \nAbstract: A central problem in machine learning is often formulated as follows: Given a dataset $\\{(x_j, y_j)\\}_{j=1}^M$, which is a sample drawn from an unknown probability distribution, the goal is to construct a functional model $f$ such that $f(x) \\approx y$ for any $(x, y)$ drawn from the same distribution. Neural networks and kernel-based methods are commonly employed for this task due to their capacity for fast and parallel computation. The approximation capabilities, or expressive power, of these methods have been extensively studied over the past 35 years. In this paper, we will present examples of key ideas in this area found in the literature. We will discuss emerging trends in machine learning including the role of shallow/deep networks, approximation on manifolds, physics-informed neural surrogates, neural operators, and transformer architectures. Despite function approximation being a fundamental problem in machine learning, approximation theory does not play a central role in the theoretical foundations of the field. One unfortunate consequence of this disconnect is that it is often unclear how well trained models will generalize to unseen or unlabeled data. In this review, we examine some of the shortcomings of the current machine learning framework and explore the reasons for the gap between approximation theory and machine learning practice. We will then introduce our novel research to achieve function approximation on unknown manifolds without the need to learn specific manifold features, such as the eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In many machine learning problems, particularly classification tasks, the labels $y_j$ are drawn from a finite set of values."
      },
      {
        "id": "oai:arXiv.org:2506.02172v1",
        "title": "Different Speech Translation Models Encode and Translate Speaker Gender Differently",
        "link": "https://arxiv.org/abs/2506.02172",
        "author": "Dennis Fucci, Marco Gaido, Matteo Negri, Luisa Bentivogli, Andre Martins, Giuseppe Attanasio",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02172v1 Announce Type: new \nAbstract: Recent studies on interpreting the hidden states of speech models have shown their ability to capture speaker-specific features, including gender. Does this finding also hold for speech translation (ST) models? If so, what are the implications for the speaker's gender assignment in translation? We address these questions from an interpretability perspective, using probing methods to assess gender encoding across diverse ST models. Results on three language directions (English-French/Italian/Spanish) indicate that while traditional encoder-decoder models capture gender information, newer architectures -- integrating a speech encoder with a machine translation system via adapters -- do not. We also demonstrate that low gender encoding capabilities result in systems' tendency toward a masculine default, a translation bias that is more pronounced in newer architectures."
      },
      {
        "id": "oai:arXiv.org:2506.02175v1",
        "title": "AI Debate Aids Assessment of Controversial Claims",
        "link": "https://arxiv.org/abs/2506.02175",
        "author": "Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02175v1 Announce Type: new \nAbstract: As AI grows more powerful, it will increasingly shape how we understand the world. But with this influence comes the risk of amplifying misinformation and deepening social divides-especially on consequential topics like public health where factual accuracy directly impacts well-being. Scalable Oversight aims to ensure AI truthfulness by enabling humans to supervise systems that may exceed human capabilities--yet humans themselves hold different beliefs and biases that impair their judgment. We study whether AI debate can guide biased judges toward the truth by having two AI systems debate opposing sides of controversial COVID-19 factuality claims where people hold strong prior beliefs. We conduct two studies: one with human judges holding either mainstream or skeptical beliefs evaluating factuality claims through AI-assisted debate or consultancy protocols, and a second examining the same problem with personalized AI judges designed to mimic these different human belief systems. In our human study, we find that debate-where two AI advisor systems present opposing evidence-based arguments-consistently improves judgment accuracy and confidence calibration, outperforming consultancy with a single-advisor system by 10% overall. The improvement is most significant for judges with mainstream beliefs (+15.2% accuracy), though debate also helps skeptical judges who initially misjudge claims move toward accurate views (+4.7% accuracy). In our AI judge study, we find that AI judges with human-like personas achieve even higher accuracy (78.5%) than human judges (70.1%) and default AI judges without personas (69.8%), suggesting their potential for supervising frontier AI models. These findings highlight AI debate as a promising path toward scalable, bias-resilient oversight--leveraging both diverse human and AI judgments to move closer to truth in contested domains."
      },
      {
        "id": "oai:arXiv.org:2506.02181v1",
        "title": "Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution",
        "link": "https://arxiv.org/abs/2506.02181",
        "author": "Dennis Fucci, Marco Gaido, Matteo Negri, Mauro Cettolo, Luisa Bentivogli",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02181v1 Announce Type: new \nAbstract: Despite significant advances in ASR, the specific acoustic cues models rely on remain unclear. Prior studies have examined such cues on a limited set of phonemes and outdated models. In this work, we apply a feature attribution technique to identify the relevant acoustic cues for a modern Conformer-based ASR system. By analyzing plosives, fricatives, and vowels, we assess how feature attributions align with their acoustic properties in the time and frequency domains, also essential for human speech perception. Our findings show that the ASR model relies on vowels' full time spans, particularly their first two formants, with greater saliency in male speech. It also better captures the spectral characteristics of sibilant fricatives than non-sibilants and prioritizes the release phase in plosives, especially burst characteristics. These insights enhance the interpretability of ASR models and highlight areas for future research to uncover potential gaps in model robustness."
      },
      {
        "id": "oai:arXiv.org:2506.02200v1",
        "title": "Learning Treatment Representations for Downstream Instrumental Variable Regression",
        "link": "https://arxiv.org/abs/2506.02200",
        "author": "Shiangyi Lin, Hui Lan, Vasilis Syrgkanis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02200v1 Announce Type: new \nAbstract: Traditional instrumental variable (IV) estimators face a fundamental constraint: they can only accommodate as many endogenous treatment variables as available instruments. This limitation becomes particularly challenging in settings where the treatment is presented in a high-dimensional and unstructured manner (e.g. descriptions of patient treatment pathways in a hospital). In such settings, researchers typically resort to applying unsupervised dimension reduction techniques to learn a low-dimensional treatment representation prior to implementing IV regression analysis. We show that such methods can suffer from substantial omitted variable bias due to implicit regularization in the representation learning step. We propose a novel approach to construct treatment representations by explicitly incorporating instrumental variables during the representation learning process. Our approach provides a framework for handling high-dimensional endogenous variables with limited instruments. We demonstrate both theoretically and empirically that fitting IV models on these instrument-informed representations ensures identification of directions that optimize outcome prediction. Our experiments show that our proposed methodology improves upon the conventional two-stage approaches that perform dimension reduction without incorporating instrument information."
      },
      {
        "id": "oai:arXiv.org:2506.02203v1",
        "title": "Constrained Sliced Wasserstein Embedding",
        "link": "https://arxiv.org/abs/2506.02203",
        "author": "Navid NaderiAlizadeh, Darian Salehi, Xinran Liu, Soheil Kolouri",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02203v1 Announce Type: new \nAbstract: Sliced Wasserstein (SW) distances offer an efficient method for comparing high-dimensional probability measures by projecting them onto multiple 1-dimensional probability distributions. However, identifying informative slicing directions has proven challenging, often necessitating a large number of slices to achieve desirable performance and thereby increasing computational complexity. We introduce a constrained learning approach to optimize the slicing directions for SW distances. Specifically, we constrain the 1D transport plans to approximate the optimal plan in the original space, ensuring meaningful slicing directions. By leveraging continuous relaxations of these transport plans, we enable a gradient-based primal-dual approach to train the slicer parameters, alongside the remaining model parameters. We demonstrate how this constrained slicing approach can be applied to pool high-dimensional embeddings into fixed-length permutation-invariant representations. Numerical results on foundation models trained on images, point clouds, and protein sequences showcase the efficacy of the proposed constrained learning approach in learning more informative slicing directions. Our implementation code can be found at https://github.com/Stranja572/constrainedswe."
      },
      {
        "id": "oai:arXiv.org:2506.02204v1",
        "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models",
        "link": "https://arxiv.org/abs/2506.02204",
        "author": "Lindia Tjuatja, Graham Neubig",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02204v1 Announce Type: new \nAbstract: Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\" and \"exclamation marks after emotional statements\", where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone."
      },
      {
        "id": "oai:arXiv.org:2506.02205v1",
        "title": "Bregman Centroid Guided Cross-Entropy Method",
        "link": "https://arxiv.org/abs/2506.02205",
        "author": "Yuliang Gu, Hongpeng Cao, Marco Caccamo, Naira Hovakimyan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02205v1 Announce Type: new \nAbstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in model-based reinforcement learning (MBRL), but its unimodal sampling strategy often leads to premature convergence in multimodal landscapes. In this work, we propose Bregman Centroid Guided CEM ($\\mathcal{BC}$-EvoCEM), a lightweight enhancement to ensemble CEM that leverages $\\textit{Bregman centroids}$ for principled information aggregation and diversity control. $\\textbf{$\\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman centroid across CEM workers and updates the least contributing ones by sampling within a trust region around the centroid. Leveraging the duality between Bregman divergences and exponential family distributions, we show that $\\textbf{$\\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM pipelines with negligible overhead. Empirical results on synthetic benchmarks, a cluttered navigation task, and full MBRL pipelines demonstrate that $\\textbf{$\\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution quality, providing a simple yet effective upgrade for CEM."
      },
      {
        "id": "oai:arXiv.org:2506.02208v1",
        "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.02208",
        "author": "Hongling Xu, Qi Zhu, Heyuan Deng, Jinpeng Li, Lu Hou, Yasheng Wang, Lifeng Shang, Ruifeng Xu, Fei Mi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02208v1 Announce Type: new \nAbstract: Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs."
      },
      {
        "id": "oai:arXiv.org:2506.02210v1",
        "title": "Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning",
        "link": "https://arxiv.org/abs/2506.02210",
        "author": "Pu (Luke),  Yi, Tianlang Chen, Yifan Yang, Sara Achour",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02210v1 Announce Type: new \nAbstract: Neural networks (NNs) are equipped with increasingly many parameters and require more and more resource for deployment. Researchers have explored various ways to improve the efficiency of NNs by identifying and reducing the redundancy, such as pruning or quantizing unimportant weights. Symmetry in the NN architectures has been identified by prior work as a possible type of redundancy, but exploiting it for efficient inference is not yet explored. In this work, we formalize the symmetry of parameters and intermediate values in NNs using the statistical property of exchangeablility. We identify that exchangeable values in NN computation may contain overlapping information, leading to redundancy. Exploiting the insight, we derive a principled general dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a per-input basis. We also provide an instantiation of ExPrune that performs neuron-level dynamic pruning by predicting negative inputs to ReLU activations. We evaluate ExPrune on two computer vision models, one graph model and one language model. ExPrune provides 10.98--26.3% reduction in FLOPs with negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1% accuracy drop. We also demonstrate that ExPrune composes with static pruning. On models that have been aggressively pruned statically, ExPrune provides additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and 13.91--14.39% reduction in FLOPs with at most 1% accuracy drop."
      },
      {
        "id": "oai:arXiv.org:2506.02212v1",
        "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics",
        "link": "https://arxiv.org/abs/2506.02212",
        "author": "Ella Rannon, David Burstein",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02212v1 Announce Type: new \nAbstract: Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life."
      },
      {
        "id": "oai:arXiv.org:2506.02213v1",
        "title": "Quantum Ensembling Methods for Healthcare and Life Science",
        "link": "https://arxiv.org/abs/2506.02213",
        "author": "Kahn Rhrissorrakrai, Kathleen E. Hamilton, Prerana Bangalore Parthsarathy, Aldo Guzman-Saenz, Tyler Alban, Filippo Utro, Laxmi Parida",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02213v1 Announce Type: new \nAbstract: Learning on small data is a challenge frequently encountered in many real-world applications. In this work we study how effective quantum ensemble models are when trained on small data problems in healthcare and life sciences. We constructed multiple types of quantum ensembles for binary classification using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our ensemble designs use minimal trainable parameters but require long-range connections between qubits. We tested these quantum ensembles on synthetic datasets and gene expression data from renal cell carcinoma patients with the task of predicting patient response to immunotherapy. From the performance observed in simulation and initial hardware experiments, we demonstrate how quantum embedding structure affects performance and discuss how to extract informative features and build models that can learn and generalize effectively. We present these exploratory results in order to assist other researchers in the design of effective learning on small data using ensembles. Incorporating quantum computing in these data constrained problems offers hope for a wide range of studies in healthcare and life sciences where biological samples are relatively scarce given the feature space to be explored."
      },
      {
        "id": "oai:arXiv.org:2506.02221v1",
        "title": "Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment",
        "link": "https://arxiv.org/abs/2506.02221",
        "author": "Johannes Schusterbauer, Ming Gui, Frank Fundel, Bj\\\"orn Ommer",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02221v1 Announce Type: new \nAbstract: Diffusion models have revolutionized generative tasks through high-fidelity outputs, yet flow matching (FM) offers faster inference and empirical performance gains. However, current foundation FM models are computationally prohibitive for finetuning, while diffusion models like Stable Diffusion benefit from efficient architectures and ecosystem support. This work addresses the critical challenge of efficiently transferring knowledge from pre-trained diffusion models to flow matching. We propose Diff2Flow, a novel framework that systematically bridges diffusion and FM paradigms by rescaling timesteps, aligning interpolants, and deriving FM-compatible velocity fields from diffusion predictions. This alignment enables direct and efficient FM finetuning of diffusion priors with no extra computation overhead. Our experiments demonstrate that Diff2Flow outperforms na\\\"ive FM and diffusion finetuning particularly under parameter-efficient constraints, while achieving superior or competitive performance across diverse downstream tasks compared to state-of-the-art methods. We will release our code at https://github.com/CompVis/diff2flow."
      },
      {
        "id": "oai:arXiv.org:2506.02229v1",
        "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis",
        "link": "https://arxiv.org/abs/2506.02229",
        "author": "Manas Mehta, Yimu Pan, Kelly Gallagher, Alison D. Gernand, Jeffery A. Goldstein, Delia Mwinyelle, Leena Mithal, James Z. Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02229v1 Announce Type: new \nAbstract: Pathological examination of the placenta is an effective method for detecting and mitigating health risks associated with childbirth. Recent advancements in AI have enabled the use of photographs of the placenta and pathology reports for detecting and classifying signs of childbirth-related pathologies. However, existing automated methods are computationally extensive, which limits their deployability. We propose two modifications to vision-language contrastive learning (VLC) frameworks to enhance their accuracy and efficiency: (1) text-anchored vision-language contrastive knowledge distillation (VLCD)-a new knowledge distillation strategy for medical VLC pretraining, and (2) unsupervised predistillation using a large natural images dataset for improved initialization. Our approach distills efficient neural networks that match or surpass the teacher model in performance while achieving model compression and acceleration. Our results showcase the value of unsupervised predistillation in improving the performance and robustness of our approach, specifically for lower-quality images. VLCD serves as an effective way to improve the efficiency and deployability of medical VLC approaches, making AI-based healthcare solutions more accessible, especially in resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2506.02239v1",
        "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition",
        "link": "https://arxiv.org/abs/2506.02239",
        "author": "Sofoklis Kakouros",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02239v1 Announce Type: new \nAbstract: In emotion recognition from speech, a key challenge lies in identifying speech signal segments that carry the most relevant acoustic variations for discerning specific emotions. Traditional approaches compute functionals for features such as energy and F0 over entire sentences or longer speech portions, potentially missing essential fine-grained variation in the long-form statistics. This research investigates the use of word informativeness, derived from a pre-trained language model, to identify semantically important segments. Acoustic features are then computed exclusively for these identified segments, enhancing emotion recognition accuracy. The methodology utilizes standard acoustic prosodic features, their functionals, and self-supervised representations. Results indicate a notable improvement in recognition performance when features are computed on segments selected based on word informativeness, underscoring the effectiveness of this approach."
      },
      {
        "id": "oai:arXiv.org:2506.02242v1",
        "title": "From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models",
        "link": "https://arxiv.org/abs/2506.02242",
        "author": "Yihong Tang, Ao Qu, Xujing Yu, Weipeng Deng, Jun Ma, Jinhua Zhao, Lijun Sun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02242v1 Announce Type: new \nAbstract: Urban and transportation research has long sought to uncover statistically meaningful relationships between key variables and societal outcomes such as road safety, to generate actionable insights that guide the planning, development, and renewal of urban and transportation systems. However, traditional workflows face several key challenges: (1) reliance on human experts to propose hypotheses, which is time-consuming and prone to confirmation bias; (2) limited interpretability, particularly in deep learning approaches; and (3) underutilization of unstructured data that can encode critical urban context. Given these limitations, we propose a Multimodal Large Language Model (MLLM)-based approach for interpretable hypothesis inference, enabling the automated generation, evaluation, and refinement of hypotheses concerning urban context and road safety outcomes. Our method leverages MLLMs to craft safety-relevant questions for street view images (SVIs), extract interpretable embeddings from their responses, and apply them in regression-based statistical models. UrbanX supports iterative hypothesis testing and refinement, guided by statistical evidence such as coefficient significance, thereby enabling rigorous scientific discovery of previously overlooked correlations between urban design and safety. Experimental evaluations on Manhattan street segments demonstrate that our approach outperforms pretrained deep learning models while offering full interpretability. Beyond road safety, UrbanX can serve as a general-purpose framework for urban scientific discovery, extracting structured insights from unstructured urban data across diverse socioeconomic and environmental outcomes. This approach enhances model trustworthiness for policy applications and establishes a scalable, statistically grounded pathway for interpretable knowledge discovery in urban and transportation studies."
      },
      {
        "id": "oai:arXiv.org:2506.02243v1",
        "title": "From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs",
        "link": "https://arxiv.org/abs/2506.02243",
        "author": "Tamara Cucumides, Floris Geerts",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02243v1 Announce Type: new \nAbstract: Tabular and relational data remain the most ubiquitous formats in real-world machine learning applications, spanning domains from finance to healthcare. Although both formats offer structured representations, they pose distinct challenges for modern deep learning methods, which typically assume flat, feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a promising solution by capturing structural dependencies within and between tables. However, existing GNN-based approaches often rely on rigid, schema-derived graphs -- such as those based on primary-foreign key links -- thereby underutilizing rich, predictive signals in non key attributes. In this work, we introduce auGraph, a unified framework for task-aware graph augmentation that applies to both tabular and relational data. auGraph enhances base graph structures by selectively promoting attributes into nodes, guided by scoring functions that quantify their relevance to the downstream prediction task. This augmentation preserves the original data schema while injecting task-relevant structural signal. Empirically, auGraph outperforms schema-based and heuristic graph construction methods by producing graphs that better support learning for relational and tabular prediction tasks."
      },
      {
        "id": "oai:arXiv.org:2506.02244v1",
        "title": "Motion aware video generative model",
        "link": "https://arxiv.org/abs/2506.02244",
        "author": "Bowen Xue, Giuseppe Claudio Guarnera, Shuang Zhao, Zahra Montazeri",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02244v1 Announce Type: new \nAbstract: Recent advances in diffusion-based video generation have yielded unprecedented quality in visual content and semantic coherence. However, current approaches predominantly rely on statistical learning from vast datasets without explicitly modeling the underlying physics of motion, resulting in subtle yet perceptible non-physical artifacts that diminish the realism of generated videos. This paper introduces a physics-informed frequency domain approach to enhance the physical plausibility of generated videos. We first conduct a systematic analysis of the frequency-domain characteristics of diverse physical motions (translation, rotation, scaling), revealing that each motion type exhibits distinctive and identifiable spectral signatures. Building on this theoretical foundation, we propose two complementary components: (1) a physical motion loss function that quantifies and optimizes the conformity of generated videos to ideal frequency-domain motion patterns, and (2) a frequency domain enhancement module that progressively learns to adjust video features to conform to physical motion constraints while preserving original network functionality through a zero-initialization strategy. Experiments across multiple video diffusion architectures demonstrate that our approach significantly enhances motion quality and physical plausibility without compromising visual quality or semantic alignment. Our frequency-domain physical motion framework generalizes effectively across different video generation architectures, offering a principled approach to incorporating physical constraints into deep learning-based video synthesis pipelines. This work seeks to establish connections between data-driven models and physics-based motion models."
      },
      {
        "id": "oai:arXiv.org:2506.02247v1",
        "title": "PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss",
        "link": "https://arxiv.org/abs/2506.02247",
        "author": "Yu Wang, Juhyung Ha, David J. Crandall",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02247v1 Announce Type: new \nAbstract: Active speaker detection (ASD) in egocentric videos presents unique challenges due to unstable viewpoints, motion blur, and off-screen speech sources - conditions under which traditional visual-centric methods degrade significantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with Regularization Network), an effective model that integrates a partially frozen Whisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly fuse cross-modal cues. To counteract modality imbalance, we introduce an inter-modal alignment loss that synchronizes audio and visual representations, enabling more consistent convergence across modalities. Without relying on multi-speaker context or ideal frontal views, PAIR-Net achieves state-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP, surpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results highlight the value of pretrained audio priors and alignment-based fusion for robust ASD under real-world egocentric conditions."
      },
      {
        "id": "oai:arXiv.org:2506.02255v1",
        "title": "SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems",
        "link": "https://arxiv.org/abs/2506.02255",
        "author": "Asha Ramanujam (Davidson School of Chemical Engineering, Purdue University, West Lafayette, IN), Adam Elyoumi (Davidson School of Chemical Engineering, Purdue University, West Lafayette, IN), Hao Chen (Davidson School of Chemical Engineering, Purdue University, West Lafayette, IN), Sai Madhukiran Kompalli (Davidson School of Chemical Engineering, Purdue University, West Lafayette, IN), Akshdeep Singh Ahluwalia (Davidson School of Chemical Engineering, Purdue University, West Lafayette, IN), Shraman Pal (Davidson School of Chemical Engineering, Purdue University, West Lafayette, IN), Dimitri J. Papageorgiou (Energy Sciences, ExxonMobil Technology and Engineering Company, Annandale, NJ), Can Li (Davidson School of Chemical Engineering, Purdue University, West Lafayette, IN)",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02255v1 Announce Type: new \nAbstract: Most existing safe reinforcement learning (RL) benchmarks focus on robotics and control tasks, offering limited relevance to high-stakes domains that involve structured constraints, mixed-integer decisions, and industrial complexity. This gap hinders the advancement and deployment of safe RL in critical areas such as energy systems, manufacturing, and supply chains. To address this limitation, we present SafeOR-Gym, a benchmark suite of nine operations research (OR) environments tailored for safe RL under complex constraints. Each environment captures a realistic planning, scheduling, or control problems characterized by cost-based constraint violations, planning horizons, and hybrid discrete-continuous action spaces. The suite integrates seamlessly with the Constrained Markov Decision Process (CMDP) interface provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms across these environments, revealing a wide range of performance: while some tasks are tractable, others expose fundamental limitations in current approaches. SafeOR-Gym provides a challenging and practical testbed that aims to catalyze future research in safe RL for real-world decision-making problems. The SafeOR-Gym framework and all accompanying code are available at: https://github.com/li-group/SafeOR-Gym."
      },
      {
        "id": "oai:arXiv.org:2506.02256v1",
        "title": "Human Heterogeneity Invariant Stress Sensing",
        "link": "https://arxiv.org/abs/2506.02256",
        "author": "Yi Xiao, Harshit Sharma, Sawinder Kaur, Dessa Bergen-Cico, Asif Salekin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02256v1 Announce Type: new \nAbstract: Stress affects physical and mental health, and wearable devices have been widely used to detect daily stress through physiological signals. However, these signals vary due to factors such as individual differences and health conditions, making generalizing machine learning models difficult. To address these challenges, we present Human Heterogeneity Invariant Stress Sensing (HHISS), a domain generalization approach designed to find consistent patterns in stress signals by removing person-specific differences. This helps the model perform more accurately across new people, environments, and stress types not seen during training. Its novelty lies in proposing a novel technique called person-wise sub-network pruning intersection to focus on shared features across individuals, alongside preventing overfitting by leveraging continuous labels while training. The study focuses especially on people with opioid use disorder (OUD)-a group where stress responses can change dramatically depending on their time of daily medication taking. Since stress often triggers cravings, a model that can adapt well to these changes could support better OUD rehabilitation and recovery. We tested HHISS on seven different stress datasets-four of which we collected ourselves and three public ones. Four are from lab setups, one from a controlled real-world setting, driving, and two are from real-world in-the-wild field datasets without any constraints. This is the first study to evaluate how well a stress detection model works across such a wide range of data. Results show HHISS consistently outperformed state-of-the-art baseline methods, proving both effective and practical for real-world use. Ablation studies, empirical justifications, and runtime evaluations confirm HHISS's feasibility and scalability for mobile stress sensing in sensitive real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.02264v1",
        "title": "CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment",
        "link": "https://arxiv.org/abs/2506.02264",
        "author": "Radin Shayanfar, Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02264v1 Announce Type: new \nAbstract: It is often challenging to teach specialized, unseen tasks to dialogue systems due to the high cost of expert knowledge, training data, and high technical difficulty. To support domain-specific applications - such as law, medicine, or finance - it is essential to build frameworks that enable non-technical experts to define, test, and refine system behaviour with minimal effort. Achieving this requires cross-disciplinary collaboration between developers and domain specialists. In this work, we introduce a novel framework, CoDial (Code for Dialogue), that converts expert knowledge, represented as a novel structured heterogeneous graph, into executable conversation logic. CoDial can be easily implemented in existing guardrailing languages, such as Colang, to enable interpretable, modifiable, and true zero-shot specification of task-oriented dialogue systems. Empirically, CoDial achieves state-of-the-art performance on the STAR dataset for inference-based models and is competitive with similar baselines on the well-known MultiWOZ dataset. We also demonstrate CoDial's iterative improvement via manual and LLM-aided feedback, making it a practical tool for expert-guided alignment of LLMs in high-stakes domains."
      },
      {
        "id": "oai:arXiv.org:2506.02265v1",
        "title": "Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction",
        "link": "https://arxiv.org/abs/2506.02265",
        "author": "Samuel Li, Pujith Kachana, Prajwal Chidananda, Saurabh Nair, Yasutaka Furukawa, Matthew Brown",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02265v1 Announce Type: new \nAbstract: Estimating agent pose and 3D scene structure from multi-camera rigs is a central task in embodied AI applications such as autonomous driving. Recent learned approaches such as DUSt3R have shown impressive performance in multiview settings. However, these models treat images as unstructured collections, limiting effectiveness in scenarios where frames are captured from synchronized rigs with known or inferable structure.\n  To this end, we introduce Rig3R, a generalization of prior multiview reconstruction models that incorporates rig structure when available, and learns to infer it when not. Rig3R conditions on optional rig metadata including camera ID, time, and rig poses to develop a rig-aware latent space that remains robust to missing information. It jointly predicts pointmaps and two types of raymaps: a pose raymap relative to a global frame, and a rig raymap relative to a rig-centric frame consistent across time. Rig raymaps allow the model to infer rig structure directly from input images when metadata is missing.\n  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose estimation, and rig discovery, outperforming both traditional and learned methods by 17-45% mAA across diverse real-world rig datasets, all in a single forward pass without post-processing or iterative refinement."
      },
      {
        "id": "oai:arXiv.org:2506.02269v1",
        "title": "A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models",
        "link": "https://arxiv.org/abs/2506.02269",
        "author": "YuQing Xie, Tess Smidt",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02269v1 Announce Type: new \nAbstract: Equivariant neural networks have proven to be effective for tasks with known underlying symmetries. However, optimizing equivariant networks can be tricky and best training practices are less established than for standard networks. In particular, recent works have found small training benefits from relaxing equivariance constraints. This raises the question: do equivariance constraints introduce fundamental obstacles to optimization? Or do they simply require different hyperparameter tuning? In this work, we investigate this question through a theoretical analysis of the loss landscape geometry. We focus on networks built using permutation representations, which we can view as a subset of unconstrained MLPs. Importantly, we show that the parameter symmetries of the unconstrained model has nontrivial effects on the loss landscape of the equivariant subspace and under certain conditions can provably prevent learning of the global minima. Further, we empirically demonstrate in such cases, relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly, the weights eventually found via relaxation corresponds to a different choice of group representation in the hidden layer. From this, we draw 3 key takeaways. (1) Viewing any class of networks in the context of larger unconstrained function space can give important insights on loss landscape structure. (2) Within the unconstrained function space, equivariant networks form a complicated union of linear hyperplanes, each associated with a specific choice of internal group representation. (3) Effective relaxation of equivariance may require not only adding nonequivariant degrees of freedom, but also rethinking the fixed choice of group representations in hidden layers."
      },
      {
        "id": "oai:arXiv.org:2506.02276v1",
        "title": "Latent Stochastic Interpolants",
        "link": "https://arxiv.org/abs/2506.02276",
        "author": "Saurabh Singh, Dmitry Lagun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02276v1 Announce Type: new \nAbstract: Stochastic Interpolants (SI) are a powerful framework for generative modeling, capable of flexibly transforming between two probability distributions. However, their use in jointly optimized latent variable models remains unexplored as they require direct access to the samples from the two distributions. This work presents Latent Stochastic Interpolants (LSI) enabling joint learning in a latent space with end-to-end optimized encoder, decoder and latent SI models. We achieve this by developing a principled Evidence Lower Bound (ELBO) objective derived directly in continuous time. The joint optimization allows LSI to learn effective latent representations along with a generative process that transforms an arbitrary prior distribution into the encoder-defined aggregated posterior. LSI sidesteps the simple priors of the normal diffusion models and mitigates the computational demands of applying SI directly in high-dimensional observation spaces, while preserving the generative flexibility of the SI framework. We demonstrate the efficacy of LSI through comprehensive experiments on the standard large scale ImageNet generation benchmark."
      },
      {
        "id": "oai:arXiv.org:2506.02279v1",
        "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries",
        "link": "https://arxiv.org/abs/2506.02279",
        "author": "Wenzheng Zhang, Xi Victoria Lin, Karl Stratos, Wen-tau Yih, Mingda Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02279v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval and generation as separate processes, requiring explicit textual queries to connect them. This separation can limit the ability of models to generalize across diverse tasks. In this work, we propose a query-free RAG system, named ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG allows models to implicitly express their information needs, eliminating the need for human-specified queries. By dividing pretrained decoder-only language models into specialized layer groups, ImpRAG optimizes retrieval and generation tasks simultaneously. Our approach employs a two-stage inference process, using the same model parameters and forward pass for both retrieval and generation, thereby minimizing the disparity between retrievers and language models. Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves 3.6-11.5 improvements in exact match scores on unseen tasks with diverse formats, highlighting its effectiveness in enabling models to articulate their own information needs and generalize across tasks. Our analysis underscores the importance of balancing retrieval and generation parameters and leveraging generation perplexities as retrieval training objectives for enhanced performance."
      },
      {
        "id": "oai:arXiv.org:2506.02281v1",
        "title": "Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals",
        "link": "https://arxiv.org/abs/2506.02281",
        "author": "Qinsi Wang, Jinghan Ke, Hancheng Ye, Yueqian Lin, Yuzhe Fu, Jianyi Zhang, Kurt Keutzer, Chenfeng Xu, Yiran Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02281v1 Announce Type: new \nAbstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main."
      },
      {
        "id": "oai:arXiv.org:2506.02283v1",
        "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews",
        "link": "https://arxiv.org/abs/2506.02283",
        "author": "Sofoklis Kakouros, Haoyu Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02283v1 Announce Type: new \nAbstract: This study examines the prosodic characteristics associated with winning and losing in post-match tennis interviews. Additionally, this research explores the potential to classify match outcomes solely based on post-match interview recordings using prosodic features and self-supervised learning (SSL) representations. By analyzing prosodic elements such as pitch and intensity, alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine whether an athlete has won or lost their match. Traditional acoustic features and deep speech representations are extracted from the data, and machine learning classifiers are employed to distinguish between winning and losing players. Results indicate that SSL representations effectively differentiate between winning and losing outcomes, capturing subtle speech patterns linked to emotional states. At the same time, prosodic cues -- such as pitch variability -- remain strong indicators of victory."
      },
      {
        "id": "oai:arXiv.org:2506.02285v1",
        "title": "Why Gradients Rapidly Increase Near the End of Training",
        "link": "https://arxiv.org/abs/2506.02285",
        "author": "Aaron Defazio",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02285v1 Announce Type: new \nAbstract: During long-duration Large Language Model (LLM) training runs the gradient norm increases rapidly near the end of training. In this short note, we show that this increase is due to an unintended interaction between weight decay, normalization layers, and the learning rate schedule. We propose a simple correction that fixes this behavior while also resulting in lower loss values throughout training."
      },
      {
        "id": "oai:arXiv.org:2506.02291v1",
        "title": "Entity Image and Mixed-Modal Image Retrieval Datasets",
        "link": "https://arxiv.org/abs/2506.02291",
        "author": "Cristian-Ioan Blaga, Paul Suganthan, Sahil Dua, Krishna Srinivasan, Enrique Alfonseca, Peter Dornbach, Tom Duerig, Imed Zitouni, Zhe Dong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02291v1 Announce Type: new \nAbstract: Despite advances in multimodal learning, challenging benchmarks for mixed-modal image retrieval that combines visual and textual information are lacking. This paper introduces a novel benchmark to rigorously evaluate image retrieval that demands deep cross-modal contextual understanding. We present two new datasets: the Entity Image Dataset (EI), providing canonical images for Wikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived from the WIT dataset. The MMIR benchmark features two challenging query types requiring models to ground textual descriptions in the context of provided visual entities: single entity-image queries (one entity image with descriptive text) and multi-entity-image queries (multiple entity images with relational text). We empirically validate the benchmark's utility as both a training corpus and an evaluation set for mixed-modal retrieval. The quality of both datasets is further affirmed through crowd-sourced human annotations. The datasets are accessible through the GitHub page: https://github.com/google-research-datasets/wit-retrieval."
      },
      {
        "id": "oai:arXiv.org:2506.02293v1",
        "title": "On Universality Classes of Equivariant Networks",
        "link": "https://arxiv.org/abs/2506.02293",
        "author": "Marco Pacini, Gabriele Santin, Bruno Lepri, Shubhendu Trivedi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02293v1 Announce Type: new \nAbstract: Equivariant neural networks provide a principled framework for incorporating symmetry into learning architectures and have been extensively analyzed through the lens of their separation power, that is, the ability to distinguish inputs modulo symmetry. This notion plays a central role in settings such as graph learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In contrast, the universality of equivariant models-their capacity to approximate target functions-remains comparatively underexplored. In this work, we investigate the approximation power of equivariant neural networks beyond separation constraints. We show that separation power does not fully capture expressivity: models with identical separation power may differ in their approximation ability. To demonstrate this, we characterize the universality classes of shallow invariant networks, providing a general framework for understanding which functions these architectures can approximate. Since equivariant models reduce to invariant ones under projection, this analysis yields sufficient conditions under which shallow equivariant networks fail to be universal. Conversely, we identify settings where shallow models do achieve separation-constrained universality. These positive results, however, depend critically on structural properties of the symmetry group, such as the existence of adequate normal subgroups, which may not hold in important cases like permutation symmetry."
      },
      {
        "id": "oai:arXiv.org:2506.02294v1",
        "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation",
        "link": "https://arxiv.org/abs/2506.02294",
        "author": "Niclas Popp, Kevin Alexander Laube, Matthias Hein, Lukas Schott",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02294v1 Announce Type: new \nAbstract: Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines"
      },
      {
        "id": "oai:arXiv.org:2506.02295v1",
        "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation",
        "link": "https://arxiv.org/abs/2506.02295",
        "author": "Ahmed Wasfy, Omer Nacar, Abdelakreem Elkhateb, Mahmoud Reda, Omar Elshehy, Adel Ammar, Wadii Boulila",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02295v1 Announce Type: new \nAbstract: The inherent complexities of Arabic script; its cursive nature, diacritical marks (tashkeel), and varied typography, pose persistent challenges for Optical Character Recognition (OCR). We present Qari-OCR, a series of vision-language models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets. Our leading model, QARI v0.2, establishes a new open-source state-of-the-art with a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling of tashkeel, diverse fonts, and document layouts, alongside impressive performance on low-resolution images. Further explorations (QARI v0.3) showcase strong potential for structural document understanding and handwritten text. This work delivers a marked improvement in Arabic OCR accuracy and efficiency, with all models and datasets released to foster further research."
      },
      {
        "id": "oai:arXiv.org:2506.02298v1",
        "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback",
        "link": "https://arxiv.org/abs/2506.02298",
        "author": "Thai Hoang, Kung-Hsiang Huang, Shirley Kokane, Jianguo Zhang, Zuxin Liu, Ming Zhu, Jake Grigsby, Tian Lan, Michael S Ryoo, Chien-Sheng Wu, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02298v1 Announce Type: new \nAbstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR's efficiency and effectiveness in speeding up development of AI agents."
      },
      {
        "id": "oai:arXiv.org:2506.02300v1",
        "title": "Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation",
        "link": "https://arxiv.org/abs/2506.02300",
        "author": "Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02300v1 Announce Type: new \nAbstract: Understanding the internal representations and decision mechanisms of deep neural networks remains a critical open challenge. While existing interpretability methods often identify influential input regions, they may not elucidate how a model distinguishes between classes or what specific changes would transition an input from one category to another. To address these limitations, we propose a novel framework that visualizes the implicit path between classes by treating the network gradient as a form of infinitesimal motion. Drawing inspiration from phase-based motion magnification, we first decompose images using invertible transforms-specifically the Complex Steerable Pyramid-then compute class-conditional gradients in the transformed space. Rather than iteratively integrating the gradient to trace a full path, we amplify the one-step gradient to the input and perform a linear extrapolation to expose how the model moves from source to target class. By operating in the steerable pyramid domain, these amplified gradients produce semantically meaningful, spatially coherent morphs that highlight the classifier's most sensitive directions, giving insight into the geometry of its decision boundaries. Experiments on both synthetic and real-world datasets demonstrate that our phase-focused extrapolation yields perceptually aligned, semantically meaningful transformations, offering a novel, interpretable lens into neural classifiers' internal representations."
      },
      {
        "id": "oai:arXiv.org:2506.02302v1",
        "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments",
        "link": "https://arxiv.org/abs/2506.02302",
        "author": "Russell Scheinberg, Ameeta Agrawal, Amber Shore, So Young Lee",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02302v1 Announce Type: new \nAbstract: Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present \"grammar prompting\", an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the target model -- either an LLM or a smaller language model (SLM) -- before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across many syntactic phenomena. Feeding an LLM's metalinguistic explanation back to the target model bridges the gap between knowing a rule and using it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by 56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings."
      },
      {
        "id": "oai:arXiv.org:2506.02306v1",
        "title": "CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation",
        "link": "https://arxiv.org/abs/2506.02306",
        "author": "Aditya Gorla, Ryan Wang, Zhengtong Liu, Ulzee An, Sriram Sankararaman",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02306v1 Announce Type: new \nAbstract: We present CACTI, a masked autoencoding approach for imputing tabular data that leverages the structure in missingness patterns and contextual information. Our approach employs a novel median truncated copy masking training strategy that encourages the model to learn from empirical patterns of missingness while incorporating semantic relationships between features - captured by column names and text descriptions - to better represent feature dependence. These dual sources of inductive bias enable CACTI to outperform state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best method (13.4%, 6.1%, and 5.3% under missing not at random, at random and completely at random, respectively) - across a diverse range of datasets and missingness conditions. Our results highlight the value of leveraging dataset-specific contextual information and missingness patterns to enhance imputation performance."
      },
      {
        "id": "oai:arXiv.org:2506.02308v1",
        "title": "MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping",
        "link": "https://arxiv.org/abs/2506.02308",
        "author": "Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02308v1 Announce Type: new \nAbstract: Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization."
      },
      {
        "id": "oai:arXiv.org:2506.02315v1",
        "title": "A Data-Based Architecture for Flight Test without Test Points",
        "link": "https://arxiv.org/abs/2506.02315",
        "author": "D. Isaiah Harp, Joshua Ott, John Alora, Dylan Asmar",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02315v1 Announce Type: new \nAbstract: The justification for the \"test point\" derives from the test pilot's obligation to reproduce faithfully the pre-specified conditions of some model prediction. Pilot deviation from those conditions invalidates the model assumptions. Flight test aids have been proposed to increase accuracy on more challenging test points. However, the very existence of databands and tolerances is the problem more fundamental than inadequate pilot skill. We propose a novel approach, which eliminates test points. We start with a high-fidelity digital model of an air vehicle. Instead of using this model to generate a point prediction, we use a machine learning method to produce a reduced-order model (ROM). The ROM has two important properties. First, it can generate a prediction based on any set of conditions the pilot flies. Second, if the test result at those conditions differ from the prediction, the ROM can be updated using the new data. The outcome of flight test is thus a refined ROM at whatever conditions were flown. This ROM in turn updates and validates the high-fidelity model. We present a single example of this \"point-less\" architecture, using T-38C flight test data. We first use a generic aircraft model to build a ROM of longitudinal pitching motion as a hypersurface. We then ingest unconstrained flight test data and use Gaussian Process Regression to update and condition the hypersurface. By proposing a second-order equivalent system for the T-38C, this hypersurface then generates parameters necessary to assess MIL-STD-1797B compliance for longitudinal dynamics."
      },
      {
        "id": "oai:arXiv.org:2506.02318v1",
        "title": "Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models",
        "link": "https://arxiv.org/abs/2506.02318",
        "author": "Yuchen Liang, Renxiang Huang, Lifeng Lai, Ness Shroff, Yingbin Liang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02318v1 Announce Type: new \nAbstract: Discrete state space diffusion models have shown significant advantages in applications involving discrete data, such as text and image generation. It has also been observed that their performance is highly sensitive to the choice of rate matrices, particularly between uniform and absorbing rate matrices. While empirical results suggest that absorbing rate matrices often yield better generation quality compared to uniform rate matrices, existing theoretical works have largely focused on the uniform rate matrices case. Notably, convergence guarantees and error analyses for absorbing diffusion models are still missing. In this work, we provide the first finite-time error bounds and convergence rate analysis for discrete diffusion models using absorbing rate matrices. We begin by deriving an upper bound on the KL divergence of the forward process, introducing a surrogate initialization distribution to address the challenge posed by the absorbing stationary distribution, which is a singleton and causes the KL divergence to be ill-defined. We then establish the first convergence guarantees for both the $\\tau$-leaping and uniformization samplers under absorbing rate matrices, demonstrating improved rates over their counterparts using uniform rate matrices. Furthermore, under suitable assumptions, we provide convergence guarantees without early stopping. Our analysis introduces several new technical tools to address challenges unique to absorbing rate matrices. These include a Jensen-type argument for bounding forward process convergence, novel techniques for bounding absorbing score functions, and a non-divergent upper bound on the score near initialization that removes the need of early-stopping."
      },
      {
        "id": "oai:arXiv.org:2506.02321v1",
        "title": "Quantifying Misattribution Unfairness in Authorship Attribution",
        "link": "https://arxiv.org/abs/2506.02321",
        "author": "Pegah Alipoormolabashi, Ajay Patel, Niranjan Balasubramanian",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02321v1 Announce Type: new \nAbstract: Authorship misattribution can have profound consequences in real life. In forensic settings simply being considered as one of the potential authors of an evidential piece of text or communication can result in undesirable scrutiny. This raises a fairness question: Is every author in the candidate pool at equal risk of misattribution? Standard evaluation measures for authorship attribution systems do not explicitly account for this notion of fairness. We introduce a simple measure, Misattribution Unfairness Index (MAUIk), which is based on how often authors are ranked in the top k for texts they did not write. Using this measure we quantify the unfairness of five models on two different datasets. All models exhibit high levels of unfairness with increased risks for some authors. Furthermore, we find that this unfairness relates to how the models embed the authors as vectors in the latent search space. In particular, we observe that the risk of misattribution is higher for authors closer to the centroid (or center) of the embedded authors in the haystack. These results indicate the potential for harm and the need for communicating with and calibrating end users on misattribution risk when building and providing such models for downstream use."
      },
      {
        "id": "oai:arXiv.org:2506.02323v1",
        "title": "Sensitivity-Aware Density Estimation in Multiple Dimensions",
        "link": "https://arxiv.org/abs/2506.02323",
        "author": "Aleix Boquet-Pujadas, Pol del Aguila Pla, Michael Unser",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02323v1 Announce Type: new \nAbstract: We formulate an optimization problem to estimate probability densities in the context of multidimensional problems that are sampled with uneven probability. It considers detector sensitivity as an heterogeneous density and takes advantage of the computational speed and flexible boundary conditions offered by splines on a grid. We choose to regularize the Hessian of the spline via the nuclear norm to promote sparsity. As a result, the method is spatially adaptive and stable against the choice of the regularization parameter, which plays the role of the bandwidth. We test our computational pipeline on standard densities and provide software. We also present a new approach to PET rebinning as an application of our framework."
      },
      {
        "id": "oai:arXiv.org:2506.02326v1",
        "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target",
        "link": "https://arxiv.org/abs/2506.02326",
        "author": "Berk Atil, Namrata Sureddy, Rebecca J. Passonneau",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02326v1 Announce Type: new \nAbstract: Toxicity in online content, including content generated by language models, has become a critical concern due to its potential for negative psychological and social impact. This paper introduces TRuST, a comprehensive dataset designed to improve toxicity detection that merges existing datasets, and has labels for toxicity, target social group, and toxic spans. It includes a diverse range of target groups such as ethnicity, gender, religion, disability, and politics, with both human/machine-annotated and human machine-generated data. We benchmark state-of-the-art large language models (LLMs) on toxicity detection, target group identification, and toxic span extraction. We find that fine-tuned models consistently outperform zero-shot and few-shot prompting, though performance remains low for certain social groups. Further, reasoning capabilities do not significantly improve performance, indicating that LLMs have weak social reasoning skills."
      },
      {
        "id": "oai:arXiv.org:2506.02327v1",
        "title": "Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning",
        "link": "https://arxiv.org/abs/2506.02327",
        "author": "Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02327v1 Announce Type: new \nAbstract: Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As a result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13%, paving the way for future integration of medical world models as the second readers."
      },
      {
        "id": "oai:arXiv.org:2506.02334v1",
        "title": "Generalized Category Discovery via Reciprocal Learning and Class-Wise Distribution Regularization",
        "link": "https://arxiv.org/abs/2506.02334",
        "author": "Duo Liu, Zhiquan Tan, Linglan Zhao, Zhongqiang Zhang, Xiangzhong Fang, Weiran Huang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02334v1 Announce Type: new \nAbstract: Generalized Category Discovery (GCD) aims to identify unlabeled samples by leveraging the base knowledge from labeled ones, where the unlabeled set consists of both base and novel classes. Since clustering methods are time-consuming at inference, parametric-based approaches have become more popular. However, recent parametric-based methods suffer from inferior base discrimination due to unreliable self-supervision. To address this issue, we propose a Reciprocal Learning Framework (RLF) that introduces an auxiliary branch devoted to base classification. During training, the main branch filters the pseudo-base samples to the auxiliary branch. In response, the auxiliary branch provides more reliable soft labels for the main branch, leading to a virtuous cycle. Furthermore, we introduce Class-wise Distribution Regularization (CDR) to mitigate the learning bias towards base classes. CDR essentially increases the prediction confidence of the unlabeled data and boosts the novel class performance. Combined with both components, our proposed method, RLCD, achieves superior performance in all classes with negligible extra computation. Comprehensive experiments across seven GCD datasets validate its superiority. Our codes are available at https://github.com/APORduo/RLCD."
      },
      {
        "id": "oai:arXiv.org:2506.02337v1",
        "title": "Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs",
        "link": "https://arxiv.org/abs/2506.02337",
        "author": "Adrienne M. Propp, Jonas A. Actor, Elise Walker, Houman Owhadi, Nathaniel Trask, Daniel M. Tartakovsky",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02337v1 Announce Type: new \nAbstract: Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations across computational subdomains by ensuring continuity of state variables and fluxes at artificial interfaces. We present a novel method for learning Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for problems where the data obey a conservation constraint from an underlying partial differential equation. Our approach combines discrete exterior calculus and nonlinear optimal recovery to infer relationships between vertex and edge values. This framework yields data-driven predictions with uncertainty quantification across the entire graph, even when observations are limited to a subset of vertices and edges. By optimizing over the reproducing kernel Hilbert space norm while applying a maximum likelihood estimation penalty on kernel complexity, our method ensures that the resulting surrogate strictly enforces conservation laws without overfitting. We demonstrate our method on two representative applications: subsurface fracture networks and arterial blood flow. Our results show that the method maintains high accuracy and well-calibrated uncertainty estimates even under severe data scarcity, highlighting its potential for scientific applications where limited data and reliable uncertainty quantification are critical."
      },
      {
        "id": "oai:arXiv.org:2506.02338v1",
        "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL",
        "link": "https://arxiv.org/abs/2506.02338",
        "author": "Hyungjoo Chae, Dongjin Kang, Jihyuk Kim, Beong-woo Kwak, Sunghyun Park, Haeju Park, Jinyoung Yeo, Moontae Lee, Kyungjae Lee",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02338v1 Announce Type: new \nAbstract: With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR."
      },
      {
        "id": "oai:arXiv.org:2506.02347v1",
        "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation",
        "link": "https://arxiv.org/abs/2506.02347",
        "author": "Jiaming Li, Yukun Chen, Ziqiang Liu, Minghuan Tan, Lei Zhang, Yunshui Li, Run Luo, Longze Chen, Jing Luo, Ahmadreza Argha, Hamid Alinejad-Rokny, Wei Zhou, Min Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02347v1 Announce Type: new \nAbstract: Stories are central to human culture, serving to share ideas, preserve traditions, and foster connections. Automatic story generation, a key advancement in artificial intelligence (AI), offers new possibilities for creating personalized content, exploring creative ideas, and enhancing interactive experiences. However, existing methods struggle to maintain narrative coherence and logical consistency. This disconnect compromises the overall storytelling experience, underscoring the need for substantial improvements. Inspired by human cognitive processes, we introduce Storyteller, a novel approach that systemically improves the coherence and consistency of automatically generated stories. Storyteller introduces a plot node structure based on linguistically grounded subject verb object (SVO) triplets, which capture essential story events and ensure a consistent logical flow. Unlike previous methods, Storyteller integrates two dynamic modules, the STORYLINE and narrative entity knowledge graph (NEKG),that continuously interact with the story generation process. This integration produces structurally sound, cohesive and immersive narratives. Extensive experiments demonstrate that Storyteller significantly outperforms existing approaches, achieving an 84.33% average win rate through human preference evaluation. At the same time, it is also far ahead in other aspects including creativity, coherence, engagement, and relevance."
      },
      {
        "id": "oai:arXiv.org:2506.02350v1",
        "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection",
        "link": "https://arxiv.org/abs/2506.02350",
        "author": "Herun Wan, Jiaying Wu, Minnan Luo, Zhi Zeng, Zhixiong Su",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02350v1 Announce Type: new \nAbstract: Misinformation detection models often rely on superficial cues (i.e., \\emph{shortcuts}) that correlate with misinformation in training data but fail to generalize to the diverse and evolving nature of real-world misinformation. This issue is exacerbated by large language models (LLMs), which can easily generate convincing misinformation through simple prompts. We introduce TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning in misinformation detection. TruthOverTricks categorizes shortcut behaviors into intrinsic shortcut induction and extrinsic shortcut injection, and evaluates seven representative detectors across 14 popular benchmarks, along with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo. Empirical results reveal that existing detectors suffer severe performance degradation when exposed to both naturally occurring and adversarially crafted shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation framework that mitigates shortcut reliance through paraphrasing, factual summarization, and sentiment normalization. SMF consistently enhances robustness across 16 benchmarks, encouraging models to rely on deeper semantic understanding rather than shortcut cues. To promote the development of misinformation detectors, we have published the resources publicly at https://github.com/whr000001/TruthOverTricks."
      },
      {
        "id": "oai:arXiv.org:2506.02351v1",
        "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization",
        "link": "https://arxiv.org/abs/2506.02351",
        "author": "Jeonghun Kang, Soonmok Kwon, Joonseok Lee, Byung-Hak Kim",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02351v1 Announce Type: new \nAbstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond."
      },
      {
        "id": "oai:arXiv.org:2506.02354v1",
        "title": "RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models",
        "link": "https://arxiv.org/abs/2506.02354",
        "author": "Junjie Li, Nan Zhang, Xiaoyang Qu, Kai Lu, Guokuan Li, Jiguang Wan, Jianzong Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02354v1 Announce Type: new \nAbstract: Object Navigation (ObjectNav) is a fundamental task in embodied artificial intelligence. Although significant progress has been made in semantic map construction and target direction prediction in current research, redundant exploration and exploration failures remain inevitable. A critical but underexplored direction is the timely termination of exploration to overcome these challenges. We observe a diminishing marginal effect between exploration steps and exploration rates and analyze the cost-benefit relationship of exploration. Inspired by this, we propose RATE-Nav, a Region-Aware Termination-Enhanced method. It includes a geometric predictive region segmentation algorithm and region-Based exploration estimation algorithm for exploration rate calculation. By leveraging the visual question answering capabilities of visual language models (VLMs) and exploration rates enables efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of 31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav shows approximately 10% improvement over previous zero-shot methods."
      },
      {
        "id": "oai:arXiv.org:2506.02355v1",
        "title": "Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening",
        "link": "https://arxiv.org/abs/2506.02355",
        "author": "Andre He, Daniel Fried, Sean Welleck",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02355v1 Announce Type: new \nAbstract: Reinforcement learning has emerged as an effective framework for training large language models on structured language-conditioned tasks. We identify a critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL algorithm in this setting. For tasks that require multi-sample performance, such as formal theorem proving, GRPO biasedly reinforces already probable solutions and neglects rare but correct proofs. This implicit bias impairs performance on pass@$N$ metrics at large sample sizes, limiting its practicality for training theorem provers. To address this, we introduce the unlikeliness reward, a straightforward method that explicitly encourages reinforcing rare correct solutions. Additionally, we find that increasing the number of PPO epochs further mitigates this bias. Our experiments confirm that incorporating the unlikeliness reward significantly improves pass@$N$ across a large range of N, outperforming standard GRPO and substantially increasing sample diversity. Applying our revised recipe to Lean, we achieve competitive performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We release our implementation, providing a simple yet effective recipe for training formal theorem provers with RL."
      },
      {
        "id": "oai:arXiv.org:2506.02356v1",
        "title": "InterRVOS: Interaction-aware Referring Video Object Segmentation",
        "link": "https://arxiv.org/abs/2506.02356",
        "author": "Woojeong Jin, Seongchan Kim, Seungryong Kim",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02356v1 Announce Type: new \nAbstract: Referring video object segmentation aims to segment the object in a video corresponding to a given natural language expression. While prior works have explored various referring scenarios, including motion-centric or multi-instance expressions, most approaches still focus on localizing a single target object in isolation. However, in comprehensive video understanding, an object's role is often defined by its interactions with other entities, which are largely overlooked in existing datasets and models. In this work, we introduce Interaction-aware referring video object sgementation (InterRVOS), a new task that requires segmenting both actor and target entities involved in an interaction. Each interactoin is described through a pair of complementary expressions from different semantic perspectives, enabling fine-grained modeling of inter-object relationships. To tackle this task, we propose InterRVOS-8K, the large-scale and automatically constructed dataset containing diverse interaction-aware expressions with corresponding masks, including challenging cases such as motion-only multi-instance expressions. We also present a baseline architecture, ReVIOSa, designed to handle actor-target segmentation from a single expression, achieving strong performance in both standard and interaction-focused settings. Furthermore, we introduce an actor-target-aware evalaution setting that enables a more targeted assessment of interaction understanding. Experimental results demonstrate that our approach outperforms prior methods in modeling complex object interactions for referring video object segmentation task, establishing a strong foundation for future research in interaction-centric video understanding. Our project page is available at \\href{https://cvlab-kaist.github.io/InterRVOS}{https://cvlab-kaist.github.io/InterRVOS}."
      },
      {
        "id": "oai:arXiv.org:2506.02357v1",
        "title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components",
        "link": "https://arxiv.org/abs/2506.02357",
        "author": "Ram Potham (Independent Researcher)",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02357v1 Announce Type: new \nAbstract: Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. Failure to prioritize such principles indicates a potential basic control failure. This paper introduces a lightweight, interpretable benchmark methodology using a simple grid world to evaluate an LLM agent's ability to uphold a predefined, high-level safety principle (e.g., \"never enter hazardous zones\") when faced with conflicting lower-level task instructions. We probe whether the agent reliably prioritizes the inviolable directive, testing a foundational controllability aspect of LLMs. This pilot study demonstrates the methodology's feasibility, offers preliminary insights into agent behavior under principle conflict, and discusses how such benchmarks can contribute empirical evidence for assessing controllability. We argue that evaluating adherence to hierarchical principles is a crucial early step in understanding our capacity to build governable AI systems."
      },
      {
        "id": "oai:arXiv.org:2506.02358v1",
        "title": "RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving",
        "link": "https://arxiv.org/abs/2506.02358",
        "author": "Tianze Wang, Zhang Zhang, Chao Sun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02358v1 Announce Type: new \nAbstract: The classification of the type of road surface (RSC) aims to utilize pavement features to identify the roughness, wet and dry conditions, and material information of the road surface. Due to its ability to effectively enhance road safety and traffic management, it has received widespread attention in recent years. In autonomous driving, accurate RSC allows vehicles to better understand the road environment, adjust driving strategies, and ensure a safer and more efficient driving experience. For a long time, vision-based RSC has been favored. However, existing visual classification methods have overlooked the exploration of fine-grained classification of pavement types (such as similar pavement textures). In this work, we propose a pure vision-based fine-grained RSC method for autonomous driving scenarios, which fuses local and global feature information through the stacking of convolutional and transformer modules. We further explore the stacking strategies of local and global feature extraction modules to find the optimal feature extraction strategy. In addition, since fine-grained tasks also face the challenge of relatively large intra-class differences and relatively small inter-class differences, we propose a Foreground-Background Module (FBM) that effectively extracts fine-grained context features of the pavement, enhancing the classification ability for complex pavements. Experiments conducted on a large-scale pavement dataset containing one million samples and a simplified dataset reorganized from this dataset achieved Top-1 classification accuracies of 92.52% and 96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods. These results demonstrate that RoadFormer outperforms existing methods in RSC tasks, providing significant progress in improving the reliability of pavement perception in autonomous driving systems."
      },
      {
        "id": "oai:arXiv.org:2506.02359v1",
        "title": "Auto-Labeling Data for Object Detection",
        "link": "https://arxiv.org/abs/2506.02359",
        "author": "Brent A. Griffin, Manushree Gangwar, Jacob Sela, Jason J. Corso",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02359v1 Announce Type: new \nAbstract: Great labels make great models. However, traditional labeling approaches for tasks like object detection have substantial costs at scale. Furthermore, alternatives to fully-supervised object detection either lose functionality or require larger models with prohibitive computational costs for inference at scale. To that end, this paper addresses the problem of training standard object detection models without any ground truth labels. Instead, we configure previously-trained vision-language foundation models to generate application-specific pseudo \"ground truth\" labels. These auto-generated labels directly integrate with existing model training frameworks, and we subsequently train lightweight detection models that are computationally efficient. In this way, we avoid the costs of traditional labeling, leverage the knowledge of vision-language models, and keep the efficiency of lightweight models for practical application. We perform exhaustive experiments across multiple labeling configurations, downstream inference models, and datasets to establish best practices and set an extensive auto-labeling benchmark. From our results, we find that our approach is a viable alternative to standard labeling in that it maintains competitive performance on multiple datasets and substantially reduces labeling time and costs."
      },
      {
        "id": "oai:arXiv.org:2506.02364v1",
        "title": "A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer",
        "link": "https://arxiv.org/abs/2506.02364",
        "author": "Liang Li, Jianli Zhao, Sheng Fang, Siyu Chen, Hui Sun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02364v1 Announce Type: new \nAbstract: Hyperspectral images (HSIs) are often degraded by complex mixed noise during acquisition and transmission, making effective denoising essential for subsequent analysis. Recent hybrid approaches that bridge model-driven and data-driven paradigms have shown great promise. However, most of these approaches lack effective alternation between different priors or modules, resulting in loosely coupled regularization and insufficient exploitation of their complementary strengths. Inspired by tensor robust principal component analysis (TRPCA), we propose a novel deep unfolding network (DU-TRPCA) that enforces stage-wise alternation between two tightly integrated modules: low-rank and sparse. The low-rank module employs thresholded tensor singular value decomposition (t-SVD), providing a widely adopted convex surrogate for tensor low-rankness and has been demonstrated to effectively capture the global spatial-spectral structure of HSIs. The Top-K sparse transformer module adaptively imposes sparse constraints, directly matching the sparse regularization in TRPCA and enabling effective removal of localized outliers and complex noise. This tightly coupled architecture preserves the stage-wise alternation between low-rank approximation and sparse refinement inherent in TRPCA, while enhancing representational capacity through attention mechanisms. Extensive experiments on synthetic and real-world HSIs demonstrate that DU-TRPCA surpasses state-of-the-art methods under severe mixed noise, while offering interpretability benefits and stable denoising dynamics inspired by iterative optimization. Code is available at https://github.com/liangli97/TRPCA-Deep-Unfolding-HSI-Denoising."
      },
      {
        "id": "oai:arXiv.org:2506.02366v1",
        "title": "Approximate Borderline Sampling using Granular-Ball for Classification Tasks",
        "link": "https://arxiv.org/abs/2506.02366",
        "author": "Qin Xie, Qinghua Zhang, Shuyin Xia",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02366v1 Announce Type: new \nAbstract: Data sampling enhances classifier efficiency and robustness through data compression and quality improvement. Recently, the sampling method based on granular-ball (GB) has shown promising performance in generality and noisy classification tasks. However, some limitations remain, including the absence of borderline sampling strategies and issues with class boundary blurring or shrinking due to overlap between GBs. In this paper, an approximate borderline sampling method using GBs is proposed for classification tasks. First, a restricted diffusion-based GB generation (RD-GBG) method is proposed, which prevents GB overlaps by constrained expansion, preserving precise geometric representation of GBs via redefined ones. Second, based on the concept of heterogeneous nearest neighbor, a GB-based approximate borderline sampling (GBABS) method is proposed, which is the first general sampling method capable of both borderline sampling and improving the quality of class noise datasets. Additionally, since RD-GBG incorporates noise detection and GBABS focuses on borderline samples, GBABS performs outstandingly on class noise datasets without the need for an optimal purity threshold. Experimental results demonstrate that the proposed methods outperform the GB-based sampling method and several representative sampling methods. Our source code is publicly available at https://github.com/CherylTse/GBABS."
      },
      {
        "id": "oai:arXiv.org:2506.02367v1",
        "title": "ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery",
        "link": "https://arxiv.org/abs/2506.02367",
        "author": "Jiayi Su, Dequan Jin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02367v1 Announce Type: new \nAbstract: Generalized category discovery (GCD) is a highly popular task in open-world recognition, aiming to identify unknown class samples using known class data. By leveraging pre-training, meta-training, and fine-tuning, ViT achieves excellent few-shot learning capabilities. Its MLP head is a feedforward network, trained synchronously with the entire network in the same process, increasing the training cost and difficulty without fully leveraging the power of the feature extractor. This paper proposes a new architecture by replacing the MLP head with a neural field-based one. We first present a new static neural field function to describe the activity distribution of the neural field and then use two static neural field functions to build an efficient few-shot classifier. This neural field-based (NF) classifier consists of two coupled static neural fields. It stores the feature information of support samples by its elementary field, the known categories by its high-level field, and the category information of support samples by its cross-field connections. We replace the MLP head with the proposed NF classifier, resulting in a novel architecture ViTNF, and simplify the three-stage training mode by pre-training the feature extractor on source tasks and training the NF classifier with support samples in meta-testing separately, significantly reducing ViT's demand for training samples and the difficulty of model training. To enhance the model's capability in identifying new categories, we provide an effective algorithm to determine the lateral interaction scale of the elementary field. Experimental results demonstrate that our model surpasses existing state-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard Cars, achieving dramatic accuracy improvements of 19\\% and 16\\% in new and all classes, respectively, indicating a notable advantage in GCD."
      },
      {
        "id": "oai:arXiv.org:2506.02370v1",
        "title": "Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning",
        "link": "https://arxiv.org/abs/2506.02370",
        "author": "Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02370v1 Announce Type: new \nAbstract: Recent dimension-free communication frameworks in Federated Learning (FL), such as DeComFL, significantly reduce per-round communication by transmitting only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method is particularly advantageous for federated fine-tuning of Large Language Models (LLMs). Yet, the high variance in ZO gradient estimation typically leads to slow convergence. Although leveraging Hessian information is known to enhance optimization speed, integrating this into FL presents significant challenges. These include clients' restrictions on local data and the critical need to maintain the dimension-free communication property. To overcome this limitation, we first introduce a generalized scalar-only communication FL framework that decouples dimension-free communication from standard ZO-SGD, enabling the integration of more advanced optimization strategies. Building on this framework, we propose HiSo, a fast federated fine-tuning method via Hessian-informed zeroth-order optimization and Scalar-only communication. Specifically, it leverages global curvature information to accelerate convergence while preserving the same minimal communication cost per round. Theoretically, we establish convergence guarantees that are independent of the global Lipschitz constant, and further show that HiSo achieves faster rates when the global Hessian exhibits a low effective rank -- a common phenomenon in LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks confirm that HiSo significantly outperforms existing ZO-based FL methods in both convergence speed and communication efficiency."
      },
      {
        "id": "oai:arXiv.org:2506.02371v1",
        "title": "SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples",
        "link": "https://arxiv.org/abs/2506.02371",
        "author": "Haoye Lu, Darren Lo, Yaoliang Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02371v1 Announce Type: new \nAbstract: Diffusion models achieve strong generative performance but often rely on large datasets that may include sensitive content. This challenge is compounded by the models' tendency to memorize training data, raising privacy concerns. SFBD (Lu et al., 2025) addresses this by training on corrupted data and using limited clean samples to capture local structure and improve convergence. However, its iterative denoising and fine-tuning loop requires manual coordination, making it burdensome to implement. We reinterpret SFBD as an alternating projection algorithm and introduce a continuous variant, SFBD flow, that removes the need for alternating steps. We further show its connection to consistency constraint-based methods, and demonstrate that its practical instantiation, Online SFBD, consistently outperforms strong baselines across benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.02372v1",
        "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output",
        "link": "https://arxiv.org/abs/2506.02372",
        "author": "Hisami Suzuki, Satoru Katsumata, Takashi Kodama, Tetsuro Takahashi, Kouta Nakayama, Satoshi Sekine",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02372v1 Announce Type: new \nAbstract: In this paper we present AnswerCarefully, a dataset for promoting the safety and appropriateness of Japanese LLM outputs. The dataset consists of 1,800 pairs of questions and reference answers, where the questions require special attention in answering. It covers a wide range of risk categories established in prior English-language datasets, but the data samples are original in that they are manually created to reflect the socio-cultural context of LLM usage in Japan. We show that using this dataset for instruction to fine-tune a Japanese LLM led to improved output safety without compromising the utility of general responses. We also report the results of a safety evaluation of 12 Japanese LLMs using this dataset as a benchmark. Finally, we describe the latest update on the dataset which provides English translations and annotations of the questions, aimed at facilitating the derivation of similar datasets in different languages and regions."
      },
      {
        "id": "oai:arXiv.org:2506.02378v1",
        "title": "Exploring Explanations Improves the Robustness of In-Context Learning",
        "link": "https://arxiv.org/abs/2506.02378",
        "author": "Ukyo Honda, Tatsushi Oka",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02378v1 Announce Type: new \nAbstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs). However, it often struggles to generalize beyond the distribution of the provided demonstrations. A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels. Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and robust decision-making. Experimental results on multiple natural language understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches."
      },
      {
        "id": "oai:arXiv.org:2506.02380v1",
        "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR",
        "link": "https://arxiv.org/abs/2506.02380",
        "author": "Zihao Ding, Cheng-Tse Lee, Mufeng Zhu, Tao Guan, Yuan-Chun Sun, Cheng-Hsin Hsu, Yao Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02380v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) is an emerging media representation that reconstructs real-world 3D scenes in high fidelity, enabling 6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However, developing and evaluating 3DGS-enabled applications and optimizing their rendering performance, require realistic user navigation data. Such data is currently unavailable for photorealistic 3DGS reconstructions of real-world scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available 6-DoF navigation dataset featuring traces from 46 participants exploring twelve diverse, real-world 3DGS scenes. The dataset was collected at two sites, using the Meta Quest Pro headsets, recording the head pose and eye gaze data for each rendered frame during free world standing 6-DoF navigation. For each of the twelve scenes, we performed careful scene initialization to correct for scene tilt and scale, ensuring a perceptually-comfortable VR experience. We also release our open-source SIBR viewer software fork with record-and-replay functionalities and a suite of utility tools for data processing, conversion, and visualization. The EyeNavGS dataset and its accompanying software tools provide valuable resources for advancing research in 6-DoF viewport prediction, adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/."
      },
      {
        "id": "oai:arXiv.org:2506.02382v1",
        "title": "Multi-level and Multi-modal Action Anticipation",
        "link": "https://arxiv.org/abs/2506.02382",
        "author": "Seulgi Kim, Ghazal Kaviani, Mohit Prabhushankar, Ghassan AlRegib",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02382v1 Announce Type: new \nAbstract: Action anticipation, the task of predicting future actions from partially observed videos, is crucial for advancing intelligent systems. Unlike action recognition, which operates on fully observed videos, action anticipation must handle incomplete information. Hence, it requires temporal reasoning, and inherent uncertainty handling. While recent advances have been made, traditional methods often focus solely on visual modalities, neglecting the potential of integrating multiple sources of information. Drawing inspiration from human behavior, we introduce \\textit{Multi-level and Multi-modal Action Anticipation (m\\&amp;m-Ant)}, a novel multi-modal action anticipation approach that combines both visual and textual cues, while explicitly modeling hierarchical semantic information for more accurate predictions. To address the challenge of inaccurate coarse action labels, we propose a fine-grained label generator paired with a specialized temporal consistency loss function to optimize performance. Extensive experiments on widely used datasets, including Breakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach, achieving state-of-the-art results with an average anticipation accuracy improvement of 3.08\\% over existing methods. This work underscores the potential of multi-modal and hierarchical modeling in advancing action anticipation and establishes a new benchmark for future research in the field. Our code is available at: https://github.com/olivesgatech/mM-ant."
      },
      {
        "id": "oai:arXiv.org:2506.02385v1",
        "title": "Multi-agent Markov Entanglement",
        "link": "https://arxiv.org/abs/2506.02385",
        "author": "Shuze Chen, Tianyi Peng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02385v1 Announce Type: new \nAbstract: Value decomposition has long been a fundamental technique in multi-agent dynamic programming and reinforcement learning (RL). Specifically, the value function of a global state $(s_1,s_2,\\ldots,s_N)$ is often approximated as the sum of local functions: $V(s_1,s_2,\\ldots,s_N)\\approx\\sum_{i=1}^N V_i(s_i)$. This approach traces back to the index policy in restless multi-armed bandit problems and has found various applications in modern RL systems. However, the theoretical justification for why this decomposition works so effectively remains underexplored.\n  In this paper, we uncover the underlying mathematical structure that enables value decomposition. We demonstrate that a multi-agent Markov decision process (MDP) permits value decomposition if and only if its transition matrix is not \"entangled\" -- a concept analogous to quantum entanglement in quantum physics. Drawing inspiration from how physicists measure quantum entanglement, we introduce how to measure the \"Markov entanglement\" for multi-agent MDPs and show that this measure can be used to bound the decomposition error in general multi-agent MDPs.\n  Using the concept of Markov entanglement, we proved that a widely-used class of index policies is weakly entangled and enjoys a sublinear $\\mathcal O(\\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we show how Markov entanglement can be efficiently estimated in practice, providing practitioners with an empirical proxy for the quality of value decomposition."
      },
      {
        "id": "oai:arXiv.org:2506.02386v1",
        "title": "Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget",
        "link": "https://arxiv.org/abs/2506.02386",
        "author": "Jie Bian, Vincent Y. F. Tan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02386v1 Announce Type: new \nAbstract: The challenge of identifying the best feasible arm within a fixed budget has attracted considerable interest in recent years. However, a notable gap remains in the literature: the exact exponential rate at which the error probability approaches zero has yet to be established, even in the relatively simple setting of $K$-armed bandits with Gaussian noise. In this paper, we address this gap by examining the problem within the context of linear bandits. We introduce a novel algorithm for best feasible arm identification that guarantees an exponential decay in the error probability. Remarkably, the decay rate -- characterized by the exponent -- matches the theoretical lower bound derived using information-theoretic principles. Our approach leverages a posterior sampling framework embedded within a game-based sampling rule involving a min-learner and a max-learner. This strategy shares its foundations with Thompson sampling, but is specifically tailored to optimize the identification process under fixed-budget constraints. Furthermore, we validate the effectiveness of our algorithm through comprehensive empirical evaluations across various problem instances with different levels of complexity. The results corroborate our theoretical findings and demonstrate that our method outperforms several benchmark algorithms in terms of both accuracy and efficiency."
      },
      {
        "id": "oai:arXiv.org:2506.02389v1",
        "title": "Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting",
        "link": "https://arxiv.org/abs/2506.02389",
        "author": "Chamara Madarasingha, Nasrin Sohrabi, Zahir Tari",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02389v1 Announce Type: new \nAbstract: Time-series prediction or forecasting is critical across many real-world dynamic systems, and recent studies have proposed using Large Language Models (LLMs) for this task due to their strong generalization capabilities and ability to perform well without extensive pre-training. However, their effectiveness in handling complex, noisy, and multivariate time-series data remains underexplored. To address this, we propose LLMPred which enhances LLM-based time-series prediction by converting time-series sequences into text and feeding them to LLMs for zero shot prediction along with two main data pre-processing techniques. First, we apply time-series sequence decomposition to facilitate accurate prediction on complex and noisy univariate sequences. Second, we extend this univariate prediction capability to multivariate data using a lightweight prompt-processing strategy. Extensive experiments with smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B demonstrate that LLMPred achieves competitive or superior performance compared to state-of-the-art baselines. Additionally, a thorough ablation study highlights the importance of the key components proposed in LLMPred."
      },
      {
        "id": "oai:arXiv.org:2506.02390v1",
        "title": "GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure",
        "link": "https://arxiv.org/abs/2506.02390",
        "author": "Qin Xie, Qinghua Zhang, Shuyin Xia, Xinran Zhou, Guoyin Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02390v1 Announce Type: new \nAbstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label noise, especially in multiclass classification tasks. Existing methods either lack mechanisms to handle label noise effectively or suffer from high computational costs due to redundant data usage. Inspired by granular computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel two-stage framework comprising a data granulation stage and an adaptive boosting stage, to enhance efficiency and robustness under noisy conditions. To validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is proposed. Specifically, first, a granular-ball generation method is designed to compress data while preserving diversity and mitigating label noise. Second, the granular ball-based SAMME algorithm focuses on granular balls rather than individual samples, improving efficiency and reducing sensitivity to noise. Experimental results on some noisy datasets show that the proposed approach achieves superior robustness and efficiency compared with existing methods, demonstrating that this work effectively extends AdaBoost and SAMME."
      },
      {
        "id": "oai:arXiv.org:2506.02391v1",
        "title": "Consultant Decoding: Yet Another Synergistic Mechanism",
        "link": "https://arxiv.org/abs/2506.02391",
        "author": "Chuanghao Ding, Jiaping Wang, Ziqing Yang, Xiaoliang Wang, Dahua Lin, Cam-Tu Nguyen, Fei Tan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02391v1 Announce Type: new \nAbstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered considerable attention as a simple yet effective approach for accelerating the inference of large language models (LLMs). Nonetheless, the high rejection rates require repeated LLMs calls to validate draft tokens, undermining the overall efficiency gain of SD. In this work, we revisit existing verification mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD). Unlike SD, which relies on a metric derived from importance sampling for verification, CD verifies candidate drafts using token-level likelihoods computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining comparable generation quality (around 100% of the target model's performance). Interestingly, this is achieved by combining models whose parameter sizes differ by two orders of magnitude. In addition, CD reduces the call frequency of the large target model to below 10%, particularly in more demanding tasks. CD's performance was even found to surpass that of the large target model, which theoretically represents the upper bound for speculative decoding."
      },
      {
        "id": "oai:arXiv.org:2506.02392v1",
        "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning",
        "link": "https://arxiv.org/abs/2506.02392",
        "author": "Yuanyao Chen, Rongsheng Chen, Fu Luo, Zhenkun Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02392v1 Announce Type: new \nAbstract: Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions."
      },
      {
        "id": "oai:arXiv.org:2506.02393v1",
        "title": "RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection",
        "link": "https://arxiv.org/abs/2506.02393",
        "author": "Yongxian Liu, Boyang Li, Ting Liu, Zaiping Lin, Wei An",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02393v1 Announce Type: new \nAbstract: Infrared small target detection is a challenging task due to its unique characteristics (e.g., small, dim, shapeless and changeable). Recently published CNN-based methods have achieved promising performance with heavy feature extraction and fusion modules. To achieve efficient and effective detection, we propose a recurrent reusable-convolution attention network (RRCA-Net) for infrared small target detection. Specifically, RRCA-Net incorporates reusable-convolution block (RuCB) in a recurrent manner without introducing extra parameters. With the help of the repetitive iteration in RuCB, the high-level information of small targets in the deep layers can be well maintained and further refined. Then, a dual interactive attention aggregation module (DIAAM) is proposed to promote the mutual enhancement and fusion of refined information. In this way, RRCA-Net can both achieve high-level feature refinement and enhance the correlation of contextual information between adjacent layers. Moreover, to achieve steady convergence, we design a target characteristic inspired loss function (DpT-k loss) by integrating physical and mathematical constraints. Experimental results on three benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate that our RRCA-Net can achieve comparable performance to the state-of-the-art methods while maintaining a small number of parameters, and act as a plug and play module to introduce consistent performance improvement for several popular IRSTD methods. Our code will be available at https://github.com/yongxianLiu/ soon."
      },
      {
        "id": "oai:arXiv.org:2506.02395v1",
        "title": "The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception",
        "link": "https://arxiv.org/abs/2506.02395",
        "author": "Xiaofeng Cong, Yu-Xin Zhang, Haoran Wei, Yeying Jin, Junming Hou, Jie Gui, Jing Zhang, Dacheng Tao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02395v1 Announce Type: new \nAbstract: While nighttime image dehazing has been extensively studied, converting nighttime hazy images to daytime-equivalent brightness remains largely unaddressed. Existing methods face two critical limitations: (1) datasets overlook the brightness relationship between day and night, resulting in the brightness mapping being inconsistent with the real world during image synthesis; and (2) models do not explicitly incorporate daytime brightness knowledge, limiting their ability to reconstruct realistic lighting. To address these challenges, we introduce the Diffusion-Based Nighttime Dehazing (DiffND) framework, which excels in both data synthesis and lighting reconstruction. Our approach starts with a data synthesis pipeline that simulates severe distortions while enforcing brightness consistency between synthetic and real-world scenes, providing a strong foundation for learning night-to-day brightness mapping. Next, we propose a restoration model that integrates a pre-trained diffusion model guided by a brightness perception network. This design harnesses the diffusion model's generative ability while adapting it to nighttime dehazing through brightness-aware optimization. Experiments validate our dataset's utility and the model's superior performance in joint haze removal and brightness mapping."
      },
      {
        "id": "oai:arXiv.org:2506.02396v1",
        "title": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather",
        "link": "https://arxiv.org/abs/2506.02396",
        "author": "Longyu Yang, Ping Hu, Shangbo Yuan, Lu Zhang, Jun Liu, Hengtao Shen, Xiaofeng Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02396v1 Announce Type: new \nAbstract: Existing LiDAR semantic segmentation models often suffer from decreased accuracy when exposed to adverse weather conditions. Recent methods addressing this issue focus on enhancing training data through weather simulation or universal augmentation techniques. However, few works have studied the negative impacts caused by the heterogeneous domain shifts in the geometric structure and reflectance intensity of point clouds. In this paper, we delve into this challenge and address it with a novel Geometry-Reflectance Collaboration (GRC) framework that explicitly separates feature extraction for geometry and reflectance. Specifically, GRC employs a dual-branch architecture designed to independently process geometric and reflectance features initially, thereby capitalizing on their distinct characteristic. Then, GRC adopts a robust multi-level feature collaboration module to suppress redundant and unreliable information from both branches. Consequently, without complex simulation or augmentation, our method effectively extracts intrinsic information about the scene while suppressing interference, thus achieving better robustness and generalization in adverse weather conditions. We demonstrate the effectiveness of GRC through comprehensive experiments on challenging benchmarks, showing that our method outperforms previous approaches and establishes new state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2506.02404v1",
        "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2506.02404",
        "author": "Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qianwen Zhang, Di Yin, Xing Sun, Xiao Huang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02404v1 Announce Type: new \nAbstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \\((ii)\\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community."
      },
      {
        "id": "oai:arXiv.org:2506.02405v1",
        "title": "Modelship Attribution: Tracing Multi-Stage Manipulations Across Generative Models",
        "link": "https://arxiv.org/abs/2506.02405",
        "author": "Zhiya Tan, Xin Zhang, Joey Tianyi Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02405v1 Announce Type: new \nAbstract: As generative techniques become increasingly accessible, authentic visuals are frequently subjected to iterative alterations by various individuals employing a variety of tools. Currently, to avoid misinformation and ensure accountability, a lot of research on detection and attribution is emerging. Although these methods demonstrate promise in single-stage manipulation scenarios, they fall short when addressing complex real-world iterative manipulation. In this paper, we are the first, to the best of our knowledge, to systematically model this real-world challenge and introduce a novel method to solve it. We define a task called \"Modelship Attribution\", which aims to trace the evolution of manipulated images by identifying the generative models involved and reconstructing the sequence of edits they performed. To realistically simulate this scenario, we utilize three generative models, StyleMapGAN, DiffSwap, and FacePartsSwap, that sequentially modify distinct regions of the same image. This process leads to the creation of the first modelship dataset, comprising 83,700 images (16,740 images*5). Given that later edits often overwrite the fingerprints of earlier models, the focus shifts from extracting blended fingerprints to characterizing each model's distinctive editing patterns. To tackle this challenge, we introduce the modelship attribution transformer (MAT), a purpose-built framework designed to effectively recognize and attribute the contributions of various models within complex, multi-stage manipulation workflows. Through extensive experiments and comparative analysis with other related methods, our results, including comprehensive ablation studies, demonstrate that the proposed approach is a highly effective solution for modelship attribution."
      },
      {
        "id": "oai:arXiv.org:2506.02406v1",
        "title": "Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL",
        "link": "https://arxiv.org/abs/2506.02406",
        "author": "Renat Sergazinov, Jing Wu, Shao-An Yin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02406v1 Announce Type: new \nAbstract: While random Fourier features are a classic tool in kernel methods, their utility as a pre-processing step for deep learning on tabular data has been largely overlooked. Motivated by shortcomings in tabular deep learning pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit and repurpose random Fourier mappings as a parameter-free, architecture-agnostic transformation. By projecting each input into a fixed feature space via sine and cosine projections with frequencies drawn once at initialization, this approach circumvents the need for ad hoc normalization or additional learnable embeddings. We show within the NTK framework that this mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii) introduces a bias that shortens the optimization trajectory, thereby accelerating gradient-based training. These effects pre-condition the network with a stable kernel from the outset. Empirically, we demonstrate that deep networks trained on Fourier-transformed inputs converge more rapidly and consistently achieve strong final performance, often with fewer epochs and less hyperparameter tuning. Our findings establish random Fourier pre-processing as a theoretically motivated, plug-and-play enhancement for tabular deep learning."
      },
      {
        "id": "oai:arXiv.org:2506.02408v1",
        "title": "Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology",
        "link": "https://arxiv.org/abs/2506.02408",
        "author": "Wenhao Tang, Rong Qin, Heng Fang, Fengtao Zhou, Hao Chen, Xiang Li, Ming-Ming Cheng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02408v1 Announce Type: new \nAbstract: Pre-trained encoders for offline feature extraction followed by multiple instance learning (MIL) aggregators have become the dominant paradigm in computational pathology (CPath), benefiting cancer diagnosis and prognosis. However, performance limitations arise from the absence of encoder fine-tuning for downstream tasks and disjoint optimization with MIL. While slide-level supervised end-to-end (E2E) learning is an intuitive solution to this issue, it faces challenges such as high computational demands and suboptimal results. These limitations motivate us to revisit E2E learning. We argue that prior work neglects inherent E2E optimization challenges, leading to performance disparities compared to traditional two-stage methods. In this paper, we pioneer the elucidation of optimization challenge caused by sparse-attention MIL and propose a novel MIL called ABMILX. It mitigates this problem through global correlation-based attention refinement and multi-head mechanisms. With the efficient multi-scale random patch sampling strategy, an E2E trained ResNet with ABMILX surpasses SOTA foundation models under the two-stage paradigm across multiple challenging benchmarks, while remaining computationally efficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath and calls for greater research focus in this area. The code is https://github.com/DearCaat/E2E-WSI-ABMILX."
      },
      {
        "id": "oai:arXiv.org:2506.02412v1",
        "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning",
        "link": "https://arxiv.org/abs/2506.02412",
        "author": "Zhengyuan Liu, Geyu Lin, Hui Li Tan, Huayun Zhang, Yanfeng Lu, Xiaoxue Gao, Stella Xin Yin, He Sun, Hock Huan Goh, Lung Hsiang Wong, Nancy F. Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02412v1 Announce Type: new \nAbstract: The integration of generative artificial intelligence into educational applications has enhanced personalized and interactive learning experiences, and it shows strong potential to promote young learners language acquisition. However, it is still challenging to ensure consistent and robust performance across different languages and cultural contexts, and kids-friendly design requires simplified instructions, engaging interactions, and age-appropriate scaffolding to maintain motivation and optimize learning outcomes. In this work, we introduce SingaKids, a dialogic tutor designed to facilitate language learning through picture description tasks. Our system integrates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation to create an immersive learning environment in four languages: English, Mandarin, Malay, and Tamil. We further improve the system through multilingual pre-training, task-specific tuning, and scaffolding optimization. Empirical studies with elementary school students demonstrate that SingaKids provides effective dialogic teaching, benefiting learners at different performance levels."
      },
      {
        "id": "oai:arXiv.org:2506.02414v1",
        "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion",
        "link": "https://arxiv.org/abs/2506.02414",
        "author": "Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02414v1 Announce Type: new \nAbstract: Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content. Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content. Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance. However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling. We propose StarVC, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features. The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found at: https://thuhcsi.github.io/StarVC/."
      },
      {
        "id": "oai:arXiv.org:2506.02415v1",
        "title": "AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting",
        "link": "https://arxiv.org/abs/2506.02415",
        "author": "Karthikeyan Vaiapury",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02415v1 Announce Type: new \nAbstract: Optimization remains a fundamental pillar of machine learning, yet existing methods often struggle to maintain stability and adaptability in dynamic, non linear systems, especially under uncertainty. We introduce AERO (Adversarial Energy-based Redirection Optimization), a novel framework inspired by the redirection principle in Judo, where external disturbances are leveraged rather than resisted. AERO reimagines optimization as a redirection process guided by 15 interrelated axioms encompassing adversarial correction, energy conservation, and disturbance-aware learning. By projecting gradients, integrating uncertainty driven dynamics, and managing learning energy, AERO offers a principled approach to stable and robust model updates. Applied to probabilistic solar energy forecasting, AERO demonstrates substantial gains in predictive accuracy, reliability, and adaptability, especially in noisy and uncertain environments. Our findings highlight AERO as a compelling new direction in the theoretical and practical landscape of optimization."
      },
      {
        "id": "oai:arXiv.org:2506.02419v1",
        "title": "Guiding Registration with Emergent Similarity from Pre-Trained Diffusion Models",
        "link": "https://arxiv.org/abs/2506.02419",
        "author": "Nurislam Tursynbek, Hastings Greer, Basar Demir, Marc Niethammer",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02419v1 Announce Type: new \nAbstract: Diffusion models, while trained for image generation, have emerged as powerful foundational feature extractors for downstream tasks. We find that off-the-shelf diffusion models, trained exclusively to generate natural RGB images, can identify semantically meaningful correspondences in medical images. Building on this observation, we propose to leverage diffusion model features as a similarity measure to guide deformable image registration networks. We show that common intensity-based similarity losses often fail in challenging scenarios, such as when certain anatomies are visible in one image but absent in another, leading to anatomically inaccurate alignments. In contrast, our method identifies true semantic correspondences, aligning meaningful structures while disregarding those not present across images. We demonstrate superior performance of our approach on two tasks: multimodal 2D registration (DXA to X-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted MRI). Code: https://github.com/uncbiag/dgir"
      },
      {
        "id": "oai:arXiv.org:2506.02425v1",
        "title": "Gender Inequality in English Textbooks Around the World: an NLP Approach",
        "link": "https://arxiv.org/abs/2506.02425",
        "author": "Tairan Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02425v1 Announce Type: new \nAbstract: Textbooks play a critical role in shaping children's understanding of the world. While previous studies have identified gender inequality in individual countries' textbooks, few have examined the issue cross-culturally. This study applies natural language processing methods to quantify gender inequality in English textbooks from 22 countries across 7 cultural spheres. Metrics include character count, firstness (which gender is mentioned first), and TF-IDF word associations by gender. The analysis also identifies gender patterns in proper names appearing in TF-IDF word lists, tests whether large language models can distinguish between gendered word lists, and uses GloVe embeddings to examine how closely keywords associate with each gender. Results show consistent overrepresentation of male characters in terms of count, firstness, and named entities. All regions exhibit gender inequality, with the Latin cultural sphere showing the least disparity."
      },
      {
        "id": "oai:arXiv.org:2506.02426v1",
        "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification",
        "link": "https://arxiv.org/abs/2506.02426",
        "author": "Maryam Berijanian, Kuldeep Singh, Amin Sehati",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02426v1 Announce Type: new \nAbstract: Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at \\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}."
      },
      {
        "id": "oai:arXiv.org:2506.02431v1",
        "title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models",
        "link": "https://arxiv.org/abs/2506.02431",
        "author": "Mahammed Kamruzzaman, Abdullah Al Monsur, Gene Louis Kim, Anshuman Chhabra",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02431v1 Announce Type: new \nAbstract: Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs."
      },
      {
        "id": "oai:arXiv.org:2506.02433v1",
        "title": "Empowering Functional Neuroimaging: A Pre-trained Generative Framework for Unified Representation of Neural Signals",
        "link": "https://arxiv.org/abs/2506.02433",
        "author": "Weiheng Yao, Xuhang Chen, Shuqiang Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02433v1 Announce Type: new \nAbstract: Multimodal functional neuroimaging enables systematic analysis of brain mechanisms and provides discriminative representations for brain-computer interface (BCI) decoding. However, its acquisition is constrained by high costs and feasibility limitations. Moreover, underrepresentation of specific groups undermines fairness of BCI decoding model. To address these challenges, we propose a unified representation framework for multimodal functional neuroimaging via generative artificial intelligence (AI). By mapping multimodal functional neuroimaging into a unified representation space, the proposed framework is capable of generating data for acquisition-constrained modalities and underrepresented groups. Experiments show that the framework can generate data consistent with real brain activity patterns, provide insights into brain mechanisms, and improve performance on downstream tasks. More importantly, it can enhance model fairness by augmenting data for underrepresented groups. Overall, the framework offers a new paradigm for decreasing the cost of acquiring multimodal functional neuroimages and enhancing the fairness of BCI decoding models."
      },
      {
        "id": "oai:arXiv.org:2506.02439v1",
        "title": "Video-Level Language-Driven Video-Based Visible-Infrared Person Re-Identification",
        "link": "https://arxiv.org/abs/2506.02439",
        "author": "Shuang Li, Jiaxu Leng, Changjiang Kuang, Mingpi Tan, Xinbo Gao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02439v1 Announce Type: new \nAbstract: Video-based Visible-Infrared Person Re-Identification (VVI-ReID) aims to match pedestrian sequences across modalities by extracting modality-invariant sequence-level features. As a high-level semantic representation, language provides a consistent description of pedestrian characteristics in both infrared and visible modalities. Leveraging the Contrastive Language-Image Pre-training (CLIP) model to generate video-level language prompts and guide the learning of modality-invariant sequence-level features is theoretically feasible. However, the challenge of generating and utilizing modality-shared video-level language prompts to address modality gaps remains a critical problem. To address this problem, we propose a simple yet powerful framework, video-level language-driven VVI-ReID (VLD), which consists of two core modules: invariant-modality language prompting (IMLP) and spatial-temporal prompting (STP). IMLP employs a joint fine-tuning strategy for the visual encoder and the prompt learner to effectively generate modality-shared text prompts and align them with visual features from different modalities in CLIP's multimodal space, thereby mitigating modality differences. Additionally, STP models spatiotemporal information through two submodules, the spatial-temporal hub (STH) and spatial-temporal aggregation (STA), which further enhance IMLP by incorporating spatiotemporal information into text prompts. The STH aggregates and diffuses spatiotemporal information into the [CLS] token of each frame across the vision transformer (ViT) layers, whereas STA introduces dedicated identity-level loss and specialized multihead attention to ensure that the STH focuses on identity-relevant spatiotemporal feature aggregation. The VLD framework achieves state-of-the-art results on two VVI-ReID benchmarks. The code will be released at https://github.com/Visuang/VLD."
      },
      {
        "id": "oai:arXiv.org:2506.02442v1",
        "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?",
        "link": "https://arxiv.org/abs/2506.02442",
        "author": "Utsav Maskey, Mark Dras, Usman Naseem",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02442v1 Announce Type: new \nAbstract: This paper presents a systematic evaluation of Large Language Models' (LLMs) behavior on long-tail distributed (encrypted) texts and their safety implications. We introduce a two-dimensional framework for assessing LLM safety: (1) instruction refusal-the ability to reject harmful obfuscated instructions, and (2) generation safety-the suppression of generating harmful responses. Through comprehensive experiments, we demonstrate that models that possess capabilities to decrypt ciphers may be susceptible to mismatched-generalization attacks: their safety mechanisms fail on at least one safety dimension, leading to unsafe responses or over-refusal. Based on these findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss their strengths and limitations. This work contributes to understanding the safety of LLM in long-tail text scenarios and provides directions for developing robust safety mechanisms."
      },
      {
        "id": "oai:arXiv.org:2506.02444v1",
        "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios",
        "link": "https://arxiv.org/abs/2506.02444",
        "author": "Lingwei Dang, Ruizhi Shao, Hongwen Zhang, Wei Min, Yebin Liu, Qingyao Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02444v1 Announce Type: new \nAbstract: Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at \\href{https://github.com/Droliven}{https://github.com/Droliven}."
      },
      {
        "id": "oai:arXiv.org:2506.02448v1",
        "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos",
        "link": "https://arxiv.org/abs/2506.02448",
        "author": "Baoyu Liang, Qile Su, Shoutai Zhu, Yuchen Liang, Chao Tong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02448v1 Announce Type: new \nAbstract: Despite the significant impact of visual events on human cognition, understanding events in videos remains a challenging task for AI due to their complex structures, semantic hierarchies, and dynamic evolution. To address this, we propose the task of video event understanding that extracts event scripts and makes predictions with these scripts from videos. To support this task, we introduce VidEvent, a large-scale dataset containing over 23,000 well-labeled events, featuring detailed event structures, broad hierarchies, and logical relations extracted from movie recap videos. The dataset was created through a meticulous annotation process, ensuring high-quality and reliable event data. We also provide comprehensive baseline models offering detailed descriptions of their architecture and performance metrics. These models serve as benchmarks for future research, facilitating comparisons and improvements. Our analysis of VidEvent and the baseline models highlights the dataset's potential to advance video event understanding and encourages the exploration of innovative algorithms and models. The dataset and related resources are publicly available at www.videvent.top."
      },
      {
        "id": "oai:arXiv.org:2506.02449v1",
        "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data",
        "link": "https://arxiv.org/abs/2506.02449",
        "author": "Bo Peng, Zhiheng Wang, Heyang Gong, Chaochao Lu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02449v1 Announce Type: new \nAbstract: In modern dialogue systems, the ability to implicitly infer user backgrounds from conversations and leverage this information for personalized assistance is crucial. However, the scarcity of high-quality data remains a fundamental challenge to evaluating and improving this capability. Traditional dataset construction methods are labor-intensive, resource-demanding, and raise privacy concerns. To address these issues, we propose a novel approach for automatic synthetic data generation and introduce the Implicit Personalized Dialogue (IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12 user attribute types. Additionally, we develop a systematic evaluation framework with four metrics to assess both attribute awareness and reasoning capabilities. We further propose five causal graphs to elucidate models' reasoning pathways during implicit personalization. Extensive experiments yield insightful observations and prove the reliability of our dataset."
      },
      {
        "id": "oai:arXiv.org:2506.02451v1",
        "title": "Weak Supervision for Real World Graphs",
        "link": "https://arxiv.org/abs/2506.02451",
        "author": "Pratheeksha Nair, Reihaneh Rabbany",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02451v1 Announce Type: new \nAbstract: Node classification in real world graphs often suffers from label scarcity and noise, especially in high stakes domains like human trafficking detection and misinformation monitoring. While direct supervision is limited, such graphs frequently contain weak signals, noisy or indirect cues, that can still inform learning. We propose WSNET, a novel weakly supervised graph contrastive learning framework that leverages these weak signals to guide robust representation learning. WSNET integrates graph structure, node features, and multiple noisy supervision sources through a contrastive objective tailored for weakly labeled data. Across three real world datasets and synthetic benchmarks with controlled noise, WSNET consistently outperforms state of the art contrastive and noisy label learning methods by up to 15% in F1 score. Our results highlight the effectiveness of contrastive learning under weak supervision and the promise of exploiting imperfect labels in graph based settings."
      },
      {
        "id": "oai:arXiv.org:2506.02452v1",
        "title": "ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model",
        "link": "https://arxiv.org/abs/2506.02452",
        "author": "Wenshuo Chen, Kuimou Yu, Haozhe Jia, Kaishen Yuan, Bowen Tian, Songning Lai, Hongru Xiao, Erhang Zhang, Lei Wang, Yutao Yue",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02452v1 Announce Type: new \nAbstract: While diffusion models advance text-to-motion generation, their static semantic conditioning ignores temporal-frequency demands: early denoising requires structural semantics for motion foundations while later stages need localized details for text alignment. This mismatch mirrors biological morphogenesis where developmental phases demand distinct genetic programs. Inspired by epigenetic regulation governing morphological specialization, we propose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture. ANT orchestrates semantic granularity through: **(i) Semantic Temporally Adaptive (STA) Module:** Automatically partitions denoising into low-frequency structural planning and high-frequency refinement via spectral analysis. **(ii) Dynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts conditional to unconditional ratio enhancing efficiency while maintaining fidelity. **(iii) Temporal-semantic reweighting:** Quantitatively aligns text influence with phase requirements. Extensive experiments show that ANT can be applied to various baselines, significantly improving model performance, and achieving state-of-the-art semantic alignment on StableMoFusion."
      },
      {
        "id": "oai:arXiv.org:2506.02453v1",
        "title": "PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation",
        "link": "https://arxiv.org/abs/2506.02453",
        "author": "Kunyu Wang, Xueyang Fu, Yunfei Bao, Chengjie Ge, Chengzhi Cao, Wei Zhai, Zheng-Jun Zha",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02453v1 Announce Type: new \nAbstract: Continual Test-Time Adaptation (CTTA) aims to online adapt a pre-trained model to changing environments during inference. Most existing methods focus on exploiting target data, while overlooking another crucial source of information, the pre-trained weights, which encode underutilized domain-invariant priors. This paper takes the geometric attributes of pre-trained weights as a starting point, systematically analyzing three key components: magnitude, absolute angle, and pairwise angular structure. We find that the pairwise angular structure remains stable across diverse corrupted domains and encodes domain-invariant semantic information, suggesting it should be preserved during adaptation. Based on this insight, we propose PAID (Pairwise Angular-Invariant Decomposition), a prior-driven CTTA method that decomposes weight into magnitude and direction, and introduces a learnable orthogonal matrix via Householder reflections to globally rotate direction while preserving the pairwise angular structure. During adaptation, only the magnitudes and the orthogonal matrices are updated. PAID achieves consistent improvements over recent SOTA methods on four widely used CTTA benchmarks, demonstrating that preserving pairwise angular structure offers a simple yet effective principle for CTTA."
      },
      {
        "id": "oai:arXiv.org:2506.02454v1",
        "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework",
        "link": "https://arxiv.org/abs/2506.02454",
        "author": "Zhaorui Yang, Bo Pan, Han Wang, Yiyao Wang, Xingyu Liu, Minfeng Zhu, Bo Zhang, Wei Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02454v1 Announce Type: new \nAbstract: Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method."
      },
      {
        "id": "oai:arXiv.org:2506.02459v1",
        "title": "ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment",
        "link": "https://arxiv.org/abs/2506.02459",
        "author": "Martin JJ. Bucher, Iro Armeni",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02459v1 Announce Type: new \nAbstract: Scene synthesis and editing has emerged as a promising direction in computer graphics. Current trained approaches for 3D indoor scenes either oversimplify object semantics through one-hot class encodings (e.g., 'chair' or 'table'), require masked diffusion for editing, ignore room boundaries, or rely on floor plan renderings that fail to capture complex layouts. In contrast, LLM-based methods enable richer semantics via natural language (e.g., 'modern studio with light wood furniture') but do not support editing, remain limited to rectangular layouts or rely on weak spatial reasoning from implicit world models. We introduce ReSpace, a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models. Our approach features a compact structured scene representation with explicit room boundaries that frames scene editing as a next-token prediction task. We leverage a dual-stage training approach combining supervised fine-tuning and preference alignment, enabling a specially trained language model for object addition that accounts for user instructions, spatial geometry, object semantics, and scene-level composition. For scene editing, we employ a zero-shot LLM to handle object removal and prompts for addition. We further introduce a novel voxelization-based evaluation that captures fine-grained geometry beyond 3D bounding boxes. Experimental results surpass state-of-the-art on object addition while maintaining competitive results on full scene synthesis."
      },
      {
        "id": "oai:arXiv.org:2506.02460v1",
        "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework",
        "link": "https://arxiv.org/abs/2506.02460",
        "author": "Yupeng Qi, Ziyu Lyu, Min Yang, Yanlin Wang, Lu Bai, Lixin Cui",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02460v1 Announce Type: new \nAbstract: As large language models (LLMs) are increasingly applied across various domains, enhancing safety while maintaining the helpfulness of LLMs has become a critical challenge. Recent studies solve this problem through safety-constrained online preference optimization or safety-constrained offline preference optimization. However, the safety-constrained online methods often suffer from excessive safety, which might reduce helpfulness, while the safety-constrained offline methods perform poorly in adaptively balancing safety and helpfulness. To address these limitations, we propose MidPO, a \\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness \\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference \\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference enhanced direct preference optimization approach to transform the base model into two independent experts, termed safety and helpfulness experts, and fine-tunes the two independent experts for optimal safety or helpfulness performance. Secondly, to achieve an effective balance between safety and helpfulness, MidPO incorporates the two experts into the MoE framework and designs a dynamic routing mechanism to allocate contributions from each expert adaptively. We conduct quantitative and qualitative experiments on three popular datasets to demonstrate the proposed MidPO significantly outperforms state-of-the-art approaches in both safety and helpfulness. The code and models will be released."
      },
      {
        "id": "oai:arXiv.org:2506.02461v1",
        "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models",
        "link": "https://arxiv.org/abs/2506.02461",
        "author": "Chunkit Chan, Yauwai Yim, Hongchuan Zeng, Zhiying Zou, Xinyuan Cheng, Zhifan Sun, Zheye Deng, Kawai Chung, Yuzhuo Ao, Yixiang Fan, Cheng Jiayang, Ercong Nie, Ginny Y. Wong, Helmut Schmid, Hinrich Sch\\\"utze, Simon See, Yangqiu Song",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02461v1 Announce Type: new \nAbstract: Theory of Mind (ToM), the ability to infer mental states in others, is pivotal for human social cognition. Existing evaluations of ToM in LLMs are largely limited to English, neglecting the linguistic diversity that shapes human cognition. This limitation raises a critical question: can LLMs exhibit Multilingual Theory of Mind, which is the capacity to reason about mental states across diverse linguistic contexts? To address this gap, we present XToM, a rigorously validated multilingual benchmark that evaluates ToM across five languages and incorporates diverse, contextually rich task scenarios. Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a pronounced dissonance: while models excel in multilingual language understanding, their ToM performance varies across languages. Our findings expose limitations in LLMs' ability to replicate human-like mentalizing across linguistic contexts."
      },
      {
        "id": "oai:arXiv.org:2506.02462v1",
        "title": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning",
        "link": "https://arxiv.org/abs/2506.02462",
        "author": "Kunyu Wang, Xueyang Fu, Xin Lu, Chengjie Ge, Chengzhi Cao, Wei Zhai, Zheng-Jun Zha",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02462v1 Announce Type: new \nAbstract: Continual test-time adaptive object detection (CTTA-OD) aims to online adapt a source pre-trained detector to ever-changing environments during inference under continuous domain shifts. Most existing CTTA-OD methods prioritize effectiveness while overlooking computational efficiency, which is crucial for resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD method via pruning. Our motivation stems from the observation that not all learned source features are beneficial; certain domain-sensitive feature channels can adversely affect target domain performance. Inspired by this, we introduce a sensitivity-guided channel pruning strategy that quantifies each channel based on its sensitivity to domain discrepancies at both image and instance levels. We apply weighted sparsity regularization to selectively suppress and prune these sensitive channels, focusing adaptation efforts on invariant ones. Additionally, we introduce a stochastic channel reactivation mechanism to restore pruned channels, enabling recovery of potentially useful features and mitigating the risks of early pruning. Extensive experiments on three benchmarks show that our method achieves superior adaptation performance while reducing computational overhead by 12% in FLOPs compared to the recent SOTA method."
      },
      {
        "id": "oai:arXiv.org:2506.02472v1",
        "title": "HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation",
        "link": "https://arxiv.org/abs/2506.02472",
        "author": "Halil Ismail Helvaci, Justin Philip Huber, Jihye Bae, Sen-ching Samson Cheung",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02472v1 Announce Type: new \nAbstract: Stroke rehabilitation often demands precise tracking of patient movements to monitor progress, with complexities of rehabilitation exercises presenting two critical challenges: fine-grained and sub-second (under one-second) action detection. In this work, we propose the High Resolution Temporal Transformer (HRTR), to time-localize and classify high-resolution (fine-grained), sub-second actions in a single-stage transformer, eliminating the need for multi-stage methods and post-processing. Without any refinements, HRTR outperforms state-of-the-art systems on both stroke related and general datasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on StrokeRehab IMU, and 88.4 on 50Salads."
      },
      {
        "id": "oai:arXiv.org:2506.02473v1",
        "title": "Generative Perception of Shape and Material from Differential Motion",
        "link": "https://arxiv.org/abs/2506.02473",
        "author": "Xinran Nicole Han, Ko Nishino, Todd Zickler",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02473v1 Announce Type: new \nAbstract: Perceiving the shape and material of an object from a single image is inherently ambiguous, especially when lighting is unknown and unconstrained. Despite this, humans can often disentangle shape and material, and when they are uncertain, they often move their head slightly or rotate the object to help resolve the ambiguities. Inspired by this behavior, we introduce a novel conditional denoising-diffusion model that generates samples of shape-and-material maps from a short video of an object undergoing differential motions. Our parameter-efficient architecture allows training directly in pixel-space, and it generates many disentangled attributes of an object simultaneously. Trained on a modest number of synthetic object-motion videos with supervision on shape and material, the model exhibits compelling emergent behavior: For static observations, it produces diverse, multimodal predictions of plausible shape-and-material maps that capture the inherent ambiguities; and when objects move, the distributions quickly converge to more accurate explanations. The model also produces high-quality shape-and-material estimates for less ambiguous, real-world objects. By moving beyond single-view to continuous motion observations, our work suggests a generative perception approach for improving visual reasoning in physically-embodied systems."
      },
      {
        "id": "oai:arXiv.org:2506.02475v1",
        "title": "Comba: Improving Nonlinear RNNs with Closed-loop Control",
        "link": "https://arxiv.org/abs/2506.02475",
        "author": "Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, Weigao Sun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02475v1 Announce Type: new \nAbstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and RWKV-7 have achieved performance improvements by supervising the recurrent memory management through Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, resulting in a nonlinear recursive structure. In this paper, we first introduce the concept of Nonlinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then, based on closed-loop control theory, we propose a novel Nonlinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on large-scale corpus. Comba demonstrates its superior performance and computation efficiency in both language and vision modeling."
      },
      {
        "id": "oai:arXiv.org:2506.02477v1",
        "title": "Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay",
        "link": "https://arxiv.org/abs/2506.02477",
        "author": "Kunyu Wang, Xueyang Fu, Chengzhi Cao, Chengjie Ge, Wei Zhai, Zheng-Jun Zha",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02477v1 Announce Type: new \nAbstract: Current image de-raining methods primarily learn from a limited dataset, leading to inadequate performance in varied real-world rainy conditions. To tackle this, we introduce a new framework that enables networks to progressively expand their de-raining knowledge base by tapping into a growing pool of datasets, significantly boosting their adaptability. Drawing inspiration from the human brain's ability to continuously absorb and generalize from ongoing experiences, our approach borrow the mechanism of the complementary learning system. Specifically, we first deploy Generative Adversarial Networks (GANs) to capture and retain the unique features of new data, mirroring the hippocampus's role in learning and memory. Then, the de-raining network is trained with both existing and GAN-synthesized data, mimicking the process of hippocampal replay and interleaved learning. Furthermore, we employ knowledge distillation with the replayed data to replicate the synergy between the neocortex's activity patterns triggered by hippocampal replays and the pre-existing neocortical knowledge. This comprehensive framework empowers the de-raining network to amass knowledge from various datasets, continually enhancing its performance on previously unseen rainy scenes. Our testing on three benchmark de-raining networks confirms the framework's effectiveness. It not only facilitates continuous knowledge accumulation across six datasets but also surpasses state-of-the-art methods in generalizing to new real-world scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.02478v1",
        "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging",
        "link": "https://arxiv.org/abs/2506.02478",
        "author": "Zijian Li, Xiaocheng Feng, Huixin Liu, Yichong Huang, Ting Liu, Bing Qin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02478v1 Announce Type: new \nAbstract: With the development of large language models, fine-tuning has emerged as an effective method to enhance performance in specific scenarios by injecting domain-specific knowledge. In this context, model merging techniques provide a solution for fusing knowledge from multiple fine-tuning models by combining their parameters. However, traditional methods often encounter task interference when merging full fine-tuning models, and this problem becomes even more evident in parameter-efficient fine-tuning scenarios. In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem."
      },
      {
        "id": "oai:arXiv.org:2506.02480v1",
        "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities",
        "link": "https://arxiv.org/abs/2506.02480",
        "author": "Yifan Duan, Yihong Tang, Kehai Chen, Liqiang Nie, Min Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02480v1 Announce Type: new \nAbstract: High-quality prompts are crucial for eliciting outstanding performance from large language models (LLMs) on complex tasks. Existing research has explored model-driven strategies for prompt optimization. However, these methods often suffer from high computational overhead or require strong optimization capabilities from the model itself, which limits their broad applicability.To address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a framework that enhances model performance by optimizing and generating role-playing prompts. The core idea of ORPP is to confine the prompt search space to role-playing scenarios, thereby fully activating the model's intrinsic capabilities through carefully crafted, high-quality role-playing prompts. Specifically, ORPP first performs iterative optimization on a small subset of training samples to generate high-quality role-playing prompts. Then, leveraging the model's few-shot learning capability, it transfers the optimization experience to efficiently generate suitable prompts for the remaining samples.Our experimental results show that ORPP not only matches but in most cases surpasses existing mainstream prompt optimization methods in terms of performance. Notably, ORPP demonstrates superior \"plug-and-play\" capability. In most cases, it can be integrated with various other prompt methods and further enhance their effectiveness."
      },
      {
        "id": "oai:arXiv.org:2506.02481v1",
        "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths",
        "link": "https://arxiv.org/abs/2506.02481",
        "author": "Inderjeet Nair, Lu Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02481v1 Announce Type: new \nAbstract: Evaluations of LLMs' ethical risks and value inclinations often rely on short-form surveys and psychometric tests, yet real-world use involves long-form, open-ended responses -- leaving value-related risks and preferences in practical settings largely underexplored. In this work, we ask: Do value preferences inferred from short-form tests align with those expressed in long-form outputs? To address this question, we compare value preferences elicited from short-form reactions and long-form responses, varying the number of arguments in the latter to capture users' differing verbosity preferences. Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b), we find (1) a weak correlation between value preferences inferred from short-form and long-form responses across varying argument counts, and (2) similarly weak correlation between preferences derived from any two distinct long-form generation settings. (3) Alignment yields only modest gains in the consistency of value expression. Further, we examine how long-form generation attributes relate to value preferences, finding that argument specificity negatively correlates with preference strength, while representation across scenarios shows a positive correlation. Our findings underscore the need for more robust methods to ensure consistent value expression across diverse applications."
      },
      {
        "id": "oai:arXiv.org:2506.02482v1",
        "title": "Building a Recommendation System Using Amazon Product Co-Purchasing Network",
        "link": "https://arxiv.org/abs/2506.02482",
        "author": "Minghao Liu, Catherine Zhao, Nathan Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02482v1 Announce Type: new \nAbstract: This project develops an online, inductive recommendation system for newly listed products on e-commerce platforms, focusing on suggesting relevant new items to customers as they purchase other products. Using the Amazon Product Co-Purchasing Network Metadata dataset, we construct a co-purchasing graph where nodes represent products and edges capture co-purchasing relationships. To address the challenge of recommending new products with limited information, we apply a modified GraphSAGE method for link prediction. This inductive approach leverages both product features and the existing co-purchasing graph structure to predict potential co-purchasing relationships, enabling the model to generalize to unseen products. As an online method, it updates in real time, making it scalable and adaptive to evolving product catalogs. Experimental results demonstrate that our approach outperforms baseline algorithms in predicting relevant product links, offering a promising solution for enhancing the relevance of new product recommendations in e-commerce environments. All code is available at https://github.com/cse416a-fl24/final-project-l-minghao_z-catherine_z-nathan.git."
      },
      {
        "id": "oai:arXiv.org:2506.02483v1",
        "title": "Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks",
        "link": "https://arxiv.org/abs/2506.02483",
        "author": "Sina Bagheri Nezhad, Ameeta Agrawal",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02483v1 Announce Type: new \nAbstract: Large language models (LLMs) often struggle to perform multi-target reasoning in long-context scenarios where relevant information is scattered across extensive documents. To address this challenge, we introduce NeuroSymbolic Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic reasoning during inference. NSAR explicitly extracts symbolic facts from text and generates executable Python code to handle complex reasoning steps. Through extensive experiments across seven languages and diverse context lengths, we demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and advanced prompting strategies in accurately identifying and synthesizing multiple pieces of information. Our results highlight the effectiveness of combining explicit symbolic operations with neural inference for robust, interpretable, and scalable reasoning in multilingual settings."
      },
      {
        "id": "oai:arXiv.org:2506.02488v1",
        "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models",
        "link": "https://arxiv.org/abs/2506.02488",
        "author": "Hongtao Huang, Xiaojun Chang, Lina Yao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02488v1 Announce Type: new \nAbstract: Diffusion models (DMs) are powerful generative models capable of producing high-fidelity images but are constrained by high computational costs due to iterative multi-step inference. While Neural Architecture Search (NAS) can optimize DMs, existing methods are hindered by retraining requirements, exponential search complexity from step-wise optimization, and slow evaluation relying on massive image generation. To address these challenges, we propose Flexiffusion, a training-free NAS framework that jointly optimizes generation schedules and model architectures without modifying pre-trained parameters. Our key insight is to decompose the generation process into flexible segments of equal length, where each segment dynamically combines three step types: full (complete computation), partial (cache-reused computation), and null (skipped computation). This segment-wise search space reduces the candidate pool exponentially compared to step-wise NAS while preserving architectural diversity. Further, we introduce relative FID (rFID), a lightweight evaluation metric for NAS that measures divergence from a teacher model's outputs instead of ground truth, slashing evaluation time by over $90\\%$. In practice, Flexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable Diffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$, outperforming prior NAS and caching methods. Notably, it attains $5.1\\times$ speedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers a resource-efficient paradigm for searching high-speed DMs without sacrificing quality."
      },
      {
        "id": "oai:arXiv.org:2506.02492v1",
        "title": "Co-Evidential Fusion with Information Volume for Medical Image Segmentation",
        "link": "https://arxiv.org/abs/2506.02492",
        "author": "Yuanpeng He, Lijian Li, Tianxiang Zhan, Chi-Man Pun, Wenpin Jiao, Zhi Jin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02492v1 Announce Type: new \nAbstract: Although existing semi-supervised image segmentation methods have achieved good performance, they cannot effectively utilize multiple sources of voxel-level uncertainty for targeted learning. Therefore, we propose two main improvements. First, we introduce a novel pignistic co-evidential fusion strategy using generalized evidential deep learning, extended by traditional D-S evidence theory, to obtain a more precise uncertainty measure for each voxel in medical samples. This assists the model in learning mixed labeled information and establishing semantic associations between labeled and unlabeled data. Second, we introduce the concept of information volume of mass function (IVUM) to evaluate the constructed evidence, implementing two evidential learning schemes. One optimizes evidential deep learning by combining the information volume of the mass function with original uncertainty measures. The other integrates the learning pattern based on the co-evidential fusion strategy, using IVUM to design a new optimization objective. Experiments on four datasets demonstrate the competitive performance of our method."
      },
      {
        "id": "oai:arXiv.org:2506.02493v1",
        "title": "Towards In-the-wild 3D Plane Reconstruction from a Single Image",
        "link": "https://arxiv.org/abs/2506.02493",
        "author": "Jiachen Liu, Rui Yu, Sili Chen, Sharon X. Huang, Hengkai Guo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02493v1 Announce Type: new \nAbstract: 3D plane reconstruction from a single image is a crucial yet challenging topic in 3D computer vision. Previous state-of-the-art (SOTA) methods have focused on training their system on a single dataset from either indoor or outdoor domain, limiting their generalizability across diverse testing data. In this work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based model targeting zero-shot 3D plane detection and reconstruction from a single image, over diverse domains and environments. To enable data-driven models across multiple domains, we have curated a large-scale planar benchmark, comprising over 14 datasets and 560,000 high-resolution, dense planar annotations for diverse indoor and outdoor scenes. To address the challenge of achieving desirable planar geometry on multi-dataset training, we propose to disentangle the representation of plane normal and offset, and employ an exemplar-guided, classification-then-regression paradigm to learn plane and offset respectively. Additionally, we employ advanced backbones as image encoder, and present an effective pixel-geometry-enhanced plane embedding module to further facilitate planar reconstruction. Extensive experiments across multiple zero-shot evaluation datasets have demonstrated that our approach significantly outperforms previous methods on both reconstruction accuracy and generalizability, especially over in-the-wild data. Our code and data are available at: https://github.com/jcliu0428/ZeroPlane."
      },
      {
        "id": "oai:arXiv.org:2506.02494v1",
        "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text",
        "link": "https://arxiv.org/abs/2506.02494",
        "author": "Junzhe Zhang, Huixuan Zhang, Xinyu Hu, Li Lin, Mingqi Gao, Shi Qiu, Xiaojun Wan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02494v1 Announce Type: new \nAbstract: Evaluation is important for multimodal generation tasks. With the rapid progress of MLLMs, there is growing interest in applying MLLMs to build general evaluation systems. However, existing work overlooks two aspects: (1) the development of evaluation capabilities for text-to-image (T2I) generation task, and (2) the incorporation of large-scale human evaluation data. In this paper, we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that combines evaluation data from both human and GPT. The corpus contains evaluation data across both image-to-text(I2T) and T2I generation tasks. Based on this corpus, we propose Data Selection and Balance, Mix-SFT training methods, and apply DPO to develop Minos, a multimodal evaluation model built upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among all open-source evaluation models of similar scale on the average of evaluation performance on all tasks, and outperforms all open-source and closed-source models on evaluation of T2I generation task. Extensive experiments demonstrate the importance of leveraging high-quality human evaluation data and jointly training on evaluation data from both I2T and T2I generation tasks."
      },
      {
        "id": "oai:arXiv.org:2506.02497v1",
        "title": "LumosFlow: Motion-Guided Long Video Generation",
        "link": "https://arxiv.org/abs/2506.02497",
        "author": "Jiahao Chen, Hangjie Yuan, Yichen Qian, Jingyun Liang, Jiazheng Xing, Pengwei Liu, Weihua Chen, Fan Wang, Bing Su",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02497v1 Announce Type: new \nAbstract: Long video generation has gained increasing attention due to its widespread applications in fields such as entertainment and simulation. Despite advances, synthesizing temporally coherent and visually compelling long sequences remains a formidable challenge. Conventional approaches often synthesize long videos by sequentially generating and concatenating short clips, or generating key frames and then interpolate the intermediate frames in a hierarchical manner. However, both of them still remain significant challenges, leading to issues such as temporal repetition or unnatural transitions. In this paper, we revisit the hierarchical long video generation pipeline and introduce LumosFlow, a framework introduce motion guidance explicitly. Specifically, we first employ the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames with larger motion intervals, thereby ensuring content diversity in the generated long videos. Given the complexity of interpolating contextual transitions between key frames, we further decompose the intermediate frame interpolation into motion generation and post-hoc refinement. For each pair of key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes complex and large-motion optical flows, while MotionControlNet subsequently refines the warped results to enhance quality and guide intermediate frame generation. Compared with traditional video frame interpolation, we achieve 15x interpolation, ensuring reasonable and continuous motion between adjacent frames. Experiments show that our method can generate long videos with consistent motion and appearance. Code and models will be made publicly available upon acceptance. Our project page: https://jiahaochen1.github.io/LumosFlow/"
      },
      {
        "id": "oai:arXiv.org:2506.02503v1",
        "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG",
        "link": "https://arxiv.org/abs/2506.02503",
        "author": "Yongjian Li, HaoCheng Chu, Yukun Yan, Zhenghao Liu, Shi Yu, Zheni Zeng, Ruobing Wang, Sen Song, Zhiyuan Liu, Maosong Sun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02503v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access broader knowledge sources, yet factual inconsistencies persist due to noise in retrieved documents-even with advanced retrieval methods. We demonstrate that enhancing generative models' capacity to process noisy content is equally critical for robust performance. In this paper, we present KARE-RAG (Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge utilization through three key innovations: (1) structured knowledge representations that facilitate error detection during training, (2) Dense Direct Preference Optimization (DDPO)-a refined training objective that prioritizes correction of critical errors, and (3) a contrastive data generation pipeline that maintains semantic consistency while rectifying factual inaccuracies. Experiments show our method significantly enhances standard RAG pipelines across model scales, improving both in-domain and out-of-domain task performance without compromising general capabilities. Notably, these gains are achieved with modest training data, suggesting data-efficient optimization is possible through targeted learning strategies. Our findings establish a new direction for RAG improvement: by improving how models learn to process retrieved content, we can enhance performance across diverse inference paradigms. All data and code will be publicly available on Github."
      },
      {
        "id": "oai:arXiv.org:2506.02504v1",
        "title": "Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization",
        "link": "https://arxiv.org/abs/2506.02504",
        "author": "Xingyu Chen, Bokun Wang, Ming Yang, Quanqi Hu, Qihang Lin, Tianbao Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02504v1 Announce Type: new \nAbstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its coupled compositional objective structure, emerges as an important optimization paradigm for addressing a wide range of machine learning problems. In this paper, we focus on a challenging class of non-convex non-smooth FCCO, where the outer functions are non-smooth weakly convex or convex and the inner functions are smooth or weakly convex. Existing state-of-the-art result face two key limitations: (1) a high iteration complexity of $O(1/\\epsilon^6)$ under the assumption that the stochastic inner functions are Lipschitz continuous in expectation; (2) reliance on vanilla SGD-type updates, which are not suitable for deep learning applications. Our main contributions are two fold: (i) We propose stochastic momentum methods tailored for non-smooth FCCO that come with provable convergence guarantees; (ii) We establish a new state-of-the-art iteration complexity of $O(1/\\epsilon^5)$. Moreover, we apply our algorithms to multiple inequality constrained non-convex optimization problems involving smooth or weakly convex functional inequality constraints. By optimizing a smoothed hinge penalty based formulation, we achieve a new state-of-the-art complexity of $O(1/\\epsilon^5)$ for finding an (nearly) $\\epsilon$-level KKT solution. Experiments on three tasks demonstrate the effectiveness of the proposed algorithms."
      },
      {
        "id": "oai:arXiv.org:2506.02510v1",
        "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset",
        "link": "https://arxiv.org/abs/2506.02510",
        "author": "Jie Zhu, Junhui Li, Yalong Wen, Xiandong Li, Lifan Guo, Feng Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02510v1 Announce Type: new \nAbstract: Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, $\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of $\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting comprehension skills."
      },
      {
        "id": "oai:arXiv.org:2506.02515v1",
        "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning",
        "link": "https://arxiv.org/abs/2506.02515",
        "author": "Zhuohan Xie, Dhruv Sahnan, Debopriyo Banerjee, Georgi Georgiev, Rushil Thareja, Hachem Madmoun, Jinyan Su, Aaryamonvikram Singh, Yuxia Wang, Rui Xing, Fajri Koto, Haonan Li, Ivan Koychev, Tanmoy Chakraborty, Salem Lahlou, Veselin Stoyanov, Preslav Nakov",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02515v1 Announce Type: new \nAbstract: Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain."
      },
      {
        "id": "oai:arXiv.org:2506.02519v1",
        "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning",
        "link": "https://arxiv.org/abs/2506.02519",
        "author": "Sohan Patnaik, Milan Aggarwal, Sumit Bhatia, Balaji Krishnamurthy",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02519v1 Announce Type: new \nAbstract: LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions by generating step-by-step rationales. Prior works have utilized this capability to improve smaller and cheaper LMs (say, with 7B parameters). However, various practical constraints, such as copyright and legal issues, owing to lack of transparency in the pre-training data of large (often closed) models, prevent their use in commercial settings. Little focus has been given to improving the innate reasoning ability of smaller models without distilling information from larger LLMs. To address this, we propose COLLATE, a trainable framework that tunes a (small) LLM to generate those outputs from a pool of diverse rationales that selectively improves the downstream task. COLLATE enforces multiple instances of the same LLM to exhibit distinct behavior and employs them to generate rationales to obtain diverse outputs. The LLM is then tuned via preference optimization to choose the candidate rationale which maximizes the likelihood of ground-truth answer. COLLATE outperforms several trainable and prompting baselines on 5 datasets across 3 domains: maths problem solving, natural language inference, and commonsense reasoning. We show the eff icacy of COLLATE on LLMs from different model families across varying parameter scales (1B to 8B) and demonstrate the benefit of multiple rationale providers guided by the end task through ablations. Code is released here (https://github.com/Sohanpatnaik106/collate)."
      },
      {
        "id": "oai:arXiv.org:2506.02527v1",
        "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base",
        "link": "https://arxiv.org/abs/2506.02527",
        "author": "Yingying Zhuang, Aman Gupta, Anurag Beniwal",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02527v1 Announce Type: new \nAbstract: Multilingual information retrieval has emerged as powerful tools for expanding knowledge sharing across languages. On the other hand, resources on high quality knowledge base are often scarce and in limited languages, therefore an effective embedding model to transform sentences from different languages into a feature vector space same as the knowledge base language becomes the key ingredient for cross language knowledge sharing, especially to transfer knowledge available in high-resource languages to low-resource ones. In this paper we propose a novel strategy to fine-tune multilingual embedding models with weighted sampling for contrastive learning, enabling multilingual information retrieval with a monolingual knowledge base. We demonstrate that the weighted sampling strategy produces performance gains compared to standard ones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our proposed methodology is language agnostic and applicable for both multilingual and code switching use cases."
      },
      {
        "id": "oai:arXiv.org:2506.02528v1",
        "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers",
        "link": "https://arxiv.org/abs/2506.02528",
        "author": "Yan Gong, Yiren Song, Yicheng Li, Chenglin Li, Yin Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02528v1 Announce Type: new \nAbstract: Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance."
      },
      {
        "id": "oai:arXiv.org:2506.02532v1",
        "title": "ReasoningFlow: Semantic Structure of Complex Reasoning Traces",
        "link": "https://arxiv.org/abs/2506.02532",
        "author": "Jinu Lee, Sagnik Mukherjee, Dilek Hakkani-Tur, Julia Hockenmaier",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02532v1 Announce Type: new \nAbstract: Large reasoning models (LRMs) generate complex reasoning traces with planning, reflection, verification, and backtracking. In this work, we introduce ReasoningFlow, a unified schema for analyzing the semantic structures of these complex traces. ReasoningFlow parses traces into directed acyclic graphs, enabling the characterization of distinct reasoning patterns as subgraph structures. This human-interpretable representation offers promising applications in understanding, evaluating, and enhancing the reasoning processes of LRMs."
      },
      {
        "id": "oai:arXiv.org:2506.02533v1",
        "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey",
        "link": "https://arxiv.org/abs/2506.02533",
        "author": "Maike Behrendt, Stefan Sylvius Wagner, Carina Weinmann, Marike Bormann, Mira Warne, Stefan Harmeling",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02533v1 Announce Type: new \nAbstract: Political online participation in the form of discussing political issues and exchanging opinions among citizens is gaining importance with more and more formats being held digitally. To come to a decision, a careful discussion and consideration of opinions and a civil exchange of arguments, which is defined as the act of deliberation, is desirable. The quality of discussions and participation processes in terms of their deliberativeness highly depends on the design of platforms and processes. To facilitate online communication for both participants and initiators, machine learning methods offer a lot of potential. In this work we want to showcase which issues occur in political online discussions and how machine learning can be used to counteract these issues and enhance deliberation."
      },
      {
        "id": "oai:arXiv.org:2506.02534v1",
        "title": "Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels",
        "link": "https://arxiv.org/abs/2506.02534",
        "author": "Sining Chen, Yilei Shi, Xiao Xiang Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02534v1 Announce Type: new \nAbstract: Monocular height estimation is considered the most efficient and cost-effective means of 3D perception in remote sensing, and it has attracted much attention since the emergence of deep learning. While training neural networks requires a large amount of data, data with perfect labels are scarce and only available within developed regions. The trained models therefore lack generalizability, which limits the potential for large-scale application of existing methods. We tackle this problem for the first time, by introducing data with imperfect labels into training pixel-wise height estimation networks, including labels that are incomplete, inexact, and inaccurate compared to high-quality labels. We propose an ensemble-based pipeline compatible with any monocular height estimation network. Taking the challenges of noisy labels, domain shift, and long-tailed distribution of height values into consideration, we carefully design the architecture and loss functions to leverage the information concealed in imperfect labels using weak supervision through balanced soft losses and ordinal constraints. We conduct extensive experiments on two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m). The results indicate that the proposed pipeline outperforms baselines by achieving more balanced performance across various domains, leading to improvements of average root mean square errors up to 22.94 %, and 18.62 % on DFC23 and GBH, respectively. The efficacy of each design component is validated through ablation studies. Code is available at https://github.com/zhu-xlab/weakim2h."
      },
      {
        "id": "oai:arXiv.org:2506.02535v1",
        "title": "MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection",
        "link": "https://arxiv.org/abs/2506.02535",
        "author": "Juntong Li, Lingwei Dang, Yukun Su, Yun Hao, Qingxin Xiao, Yongwei Nie, Qingyao Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02535v1 Announce Type: new \nAbstract: Video Anomaly Detection (VAD) methods based on reconstruction or prediction face two critical challenges: (1) strong generalization capability often results in accurate reconstruction or prediction of abnormal events, making it difficult to distinguish normal from abnormal patterns; (2) reliance only on low-level appearance and motion cues limits their ability to identify high-level semantic in abnormal events from complex scenes. To address these limitations, we propose a novel VAD framework with two key innovations. First, to suppress excessive generalization, we introduce the Sparse Feature Filtering Module (SFFM) that employs bottleneck filters to dynamically and adaptively remove abnormal information from features. Unlike traditional memory modules, it does not need to memorize the normal prototypes across the training dataset. Further, we design the Mixture of Experts (MoE) architecture for SFFM. Each expert is responsible for extracting specialized principal features during running time, and different experts are selectively activated to ensure the diversity of the learned principal features. Second, to overcome the neglect of semantics in existing methods, we integrate a Vision-Language Model (VLM) to generate textual descriptions for video clips, enabling comprehensive joint modeling of semantic, appearance, and motion cues. Additionally, we enforce modality consistency through semantic similarity constraints and motion frame-difference contrastive loss. Extensive experiments on multiple public datasets validate the effectiveness of our multimodal joint modeling framework and sparse feature filtering paradigm. Project page at https://qzfm.github.io/sfn_vad_project_page/."
      },
      {
        "id": "oai:arXiv.org:2506.02536v1",
        "title": "Answer Convergence as a Signal for Early Stopping in Reasoning",
        "link": "https://arxiv.org/abs/2506.02536",
        "author": "Xin Liu, Lu Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02536v1 Announce Type: new \nAbstract: Chain-of-thought (CoT) prompting enhances reasoning in large language models (LLMs) but often leads to verbose and redundant outputs, thus increasing inference cost. We hypothesize that many reasoning steps are unnecessary for producing correct answers. To investigate this, we start with a systematic study to examine what is the minimum reasoning required for a model to reach a stable decision. We find that on math reasoning tasks like math, models typically converge to their final answers after 60\\% of the reasoning steps, suggesting substantial redundancy in the remaining content. Based on these insights, we propose three inference-time strategies to improve efficiency: (1) early stopping via answer consistency, (2) boosting the probability of generating end-of-reasoning signals, and (3) a supervised method that learns when to stop based on internal activations. Experiments across five benchmarks and five open-weights LLMs show that our methods significantly reduce token usage with little or no accuracy drop. In particular, on NaturalQuestions, Answer Consistency reduces tokens by over 40\\% while further improving accuracy. Our work underscores the importance of cost-effective reasoning methods that operate at inference time, offering practical benefits for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.02537v1",
        "title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning",
        "link": "https://arxiv.org/abs/2506.02537",
        "author": "Hao Yan, Handong Zheng, Hao Wang, Liang Yin, Xingchen Liu, Zhenbiao Cao, Xinxing Su, Zihao Chen, Jihao Wu, Minghui Liao, Chao Weng, Wei Chen, Yuliang Liu, Xiang Bai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02537v1 Announce Type: new \nAbstract: Recent strides in multimodal large language models (MLLMs) have significantly advanced their performance in many reasoning tasks. However, Abstract Visual Reasoning (AVR) remains a critical challenge, primarily due to limitations in perceiving abstract graphics. To tackle this issue, we investigate the bottlenecks in current MLLMs and synthesize training data to improve their abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR, featuring tasks meticulously constructed to assess models' reasoning capacities across five core dimensions and two high-level reasoning categories. Second, we introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for generating riddles with fine-grained perceptual descriptions. PRS not only generates valuable training data for abstract graphics but also provides fine-grained perceptual description, crucially allowing for supervision over intermediate reasoning stages and thereby improving both training efficacy and model interpretability. Our extensive experimental results on VisuRiddles empirically validate that fine-grained visual perception is the principal bottleneck and our synthesis framework markedly enhances the performance of contemporary MLLMs on these challenging tasks. Our code and dataset will be released at https://github.com/yh-hust/VisuRiddles"
      },
      {
        "id": "oai:arXiv.org:2506.02539v1",
        "title": "VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning",
        "link": "https://arxiv.org/abs/2506.02539",
        "author": "Thong Q. Nguyen, Shubhang Desai, Yash Jain, Tanvir Aumi, Vishal Chowdhary",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02539v1 Announce Type: new \nAbstract: Continual memory augmentation allows computer-use agents (CUAs) to learn from past interactions and refine their task-solving strategies over time. However, unchecked memory accumulation can introduce spurious or hallucinated \"learnings\" that degrade agent performance, particularly in domain-specific workflows such as productivity software. We present a novel framework, VerificAgent, that effectively manages memory for CUAs through (1) an expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory refinement during training, and (3) a post-hoc fact-checking pass by human experts to sanitize accumulated memory before deployment. On OSWorld productivity tasks, VerificAgent achieves a 111.1% relative improvement in success rate over baseline CUA without any additional fine-tuning."
      },
      {
        "id": "oai:arXiv.org:2506.02541v1",
        "title": "Rethinking Post-Unlearning Behavior of Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2506.02541",
        "author": "Minsung Kim, Nakyeong Yang, Kyomin Jung",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02541v1 Announce Type: new \nAbstract: Machine unlearning is used to mitigate the privacy risks of Large Vision-Language Models (LVLMs) arising from training on large-scale web data. However, existing unlearning methods often fail to carefully select substitute outputs for forget targets, resulting in Unlearning Aftermaths-undesirable behaviors such as degenerate, hallucinated, or excessively refused responses. We highlight that, especially for generative LVLMs, it is crucial to consider the quality and informativeness of post-unlearning responses rather than relying solely on naive suppression. To address this, we introduce a new unlearning task for LVLMs that requires models to provide privacy-preserving yet informative and visually grounded responses. We also propose PUBG, a novel unlearning method that explicitly guides post-unlearning behavior toward a desirable output distribution. Experiments show that, while existing methods suffer from Unlearning Aftermaths despite successfully preventing privacy violations, PUBG effectively mitigates these issues, generating visually grounded and informative responses without privacy leakage for forgotten targets."
      },
      {
        "id": "oai:arXiv.org:2506.02542v1",
        "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification",
        "link": "https://arxiv.org/abs/2506.02542",
        "author": "Niklas Kormann, Masoud Ramuz, Zeeshan Nisar, Nadine S. Schaadt, Hendrik Annuth, Benjamin Doerr, Friedrich Feuerhake, Thomas Lampert, Johannes F. Lutzeyer",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02542v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have recently been found to excel in histopathology. However, an important histopathological task, where GNNs have not been extensively explored, is the classification of glomeruli health as an important indicator in nephropathology. This task presents unique difficulties, particularly for the graph construction, i.e., the identification of nodes, edges, and informative features. In this work, we propose a pipeline composed of different traditional and machine learning-based computer vision techniques to identify nodes, edges, and their corresponding features to form a heterogeneous graph. We then proceed to propose a novel heterogeneous GNN architecture for glomeruli classification, called HIEGNet, that integrates both glomeruli and their surrounding immune cells. Hence, HIEGNet is able to consider the immune environment of each glomerulus in its classification. Our HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney transplant patients. Experimental results demonstrate that HIEGNet outperforms several baseline models and generalises best between patients among all baseline models. Our implementation is publicly available at https://github.com/nklsKrmnn/HIEGNet.git."
      },
      {
        "id": "oai:arXiv.org:2506.02544v1",
        "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG",
        "link": "https://arxiv.org/abs/2506.02544",
        "author": "Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02544v1 Announce Type: new \nAbstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge \\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG} (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6\\% and 9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We release code and data at \\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}."
      },
      {
        "id": "oai:arXiv.org:2506.02547v1",
        "title": "Probabilistic Online Event Downsampling",
        "link": "https://arxiv.org/abs/2506.02547",
        "author": "Andreu Girbau-Xalabarder, Jun Nagata, Shinichi Sumiyoshi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02547v1 Announce Type: new \nAbstract: Event cameras capture scene changes asynchronously on a per-pixel basis, enabling extremely high temporal resolution. However, this advantage comes at the cost of high bandwidth, memory, and computational demands. To address this, prior work has explored event downsampling, but most approaches rely on fixed heuristics or threshold-based strategies, limiting their adaptability. Instead, we propose a probabilistic framework, POLED, that models event importance through an event-importance probability density function (ePDF), which can be arbitrarily defined and adapted to different applications. Our approach operates in a purely online setting, estimating event importance on-the-fly from raw event streams, enabling scene-specific adaptation. Additionally, we introduce zero-shot event downsampling, where downsampled events must remain usable for models trained on the original event stream, without task-specific adaptation. We design a contour-preserving ePDF that prioritizes structurally important events and evaluate our method across four datasets and tasks--object classification, image interpolation, surface normal estimation, and object detection--demonstrating that intelligent sampling is crucial for maintaining performance under event-budget constraints."
      },
      {
        "id": "oai:arXiv.org:2506.02550v1",
        "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025",
        "link": "https://arxiv.org/abs/2506.02550",
        "author": "Qiaohui Chu, Haoyu Zhang, Yisen Feng, Meng Liu, Weili Guan, Yaowei Wang, Liqiang Nie",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02550v1 Announce Type: new \nAbstract: In this report, we present a novel three-stage framework developed for the Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in foundation models, our method consists of three stages: feature extraction, action recognition, and long-term action anticipation. First, visual features are extracted using a high-performance visual encoder. The features are then fed into a Transformer to predict verbs and nouns, with a verb-noun co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the predicted verb-noun pairs are formatted as textual prompts and input into a fine-tuned large language model (LLM) to anticipate future action sequences. Our framework achieves first place in this challenge at CVPR 2025, establishing a new state-of-the-art in long-term action prediction. Our code will be released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025."
      },
      {
        "id": "oai:arXiv.org:2506.02553v1",
        "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective",
        "link": "https://arxiv.org/abs/2506.02553",
        "author": "Shenghua He, Tian Xia, Xuan Zhou, Hui Wei",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02553v1 Announce Type: new \nAbstract: We study a common challenge in reinforcement learning for large language models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e., intermediate token generations) receive zero task-specific immediate reward, while only the final token receives a reward for the entire response. This assumption arises frequently in practice, as precise token-level rewards are often difficult or infeasible to obtain in LLM applications. In this work, we provide a unifying theoretical perspective. We introduce the Trajectory Policy Gradient Theorem, which shows that the policy gradient based on true, unknown token-level rewards can be unbiasedly estimated using only a response-level reward model, regardless of whether the Zero-Reward Assumption holds or not, for algorithms in the REINFORCE and Actor-Critic families. This result reveals that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess the capacity to model token-level reward signals, offering a theoretical justification for response-level reward approaches. Our findings pave the way for more practical, efficient LLM fine-tuning, allowing developers to treat training algorithms as black boxes and focus on improving the response-level reward model with auxiliary sub-models. We also offer a detailed analysis of popular RL and non-RL methods, comparing their theoretical foundations and practical advantages across common LLM tasks. Finally, we propose a new algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically grounded method that is simpler than PPO, matches GRPO in memory efficiency, and holds promise for broad applicability."
      },
      {
        "id": "oai:arXiv.org:2506.02555v1",
        "title": "SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence",
        "link": "https://arxiv.org/abs/2506.02555",
        "author": "Zhitao Zeng, Zhu Zhuo, Xiaojun Jia, Erli Zhang, Junde Wu, Jiaan Zhang, Yuxuan Wang, Chang Han Low, Jian Jiang, Zilong Zheng, Xiaochun Cao, Yutong Ban, Qi Dou, Yang Liu, Yueming Jin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02555v1 Announce Type: new \nAbstract: Foundation models have achieved transformative success across biomedical domains by enabling holistic understanding of multimodal data. However, their application in surgery remains underexplored. Surgical intelligence presents unique challenges - requiring surgical visual perception, temporal analysis, and reasoning. Existing general-purpose vision-language models fail to address these needs due to insufficient domain-specific supervision and the lack of a large-scale high-quality surgical database. To bridge this gap, we propose SurgVLM, one of the first large vision-language foundation models for surgical intelligence, where this single universal model can tackle versatile surgical tasks. To enable this, we construct a large-scale multimodal surgical database, SurgVLM-DB, comprising over 1.81 million frames with 7.79 million conversations, spanning more than 16 surgical types and 18 anatomical structures. We unify and reorganize 23 public datasets across 10 surgical tasks, followed by standardizing labels and doing hierarchical vision-language alignment to facilitate comprehensive coverage of gradually finer-grained surgical tasks, from visual perception, temporal analysis, to high-level reasoning. Building upon this comprehensive dataset, we propose SurgVLM, which is built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical tasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for method evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets in surgical domain, covering several crucial downstream tasks. Based on SurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants: SurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive comparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash, Qwen2.5-Max)."
      },
      {
        "id": "oai:arXiv.org:2506.02557v1",
        "title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models",
        "link": "https://arxiv.org/abs/2506.02557",
        "author": "Shizhan Gong, Yankai Jiang, Qi Dou, Farzan Farnia",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02557v1 Announce Type: new \nAbstract: Vision-language models, such as CLIP, have achieved significant success in aligning visual and textual representations, becoming essential components of many multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo. However, numerous studies have identified CLIP's limited fine-grained perception as a critical drawback, leading to substantial failures in downstream MLLMs. In contrast, vision-centric foundation models like DINOv2 demonstrate remarkable capabilities in capturing fine details from images. In this work, we propose a novel kernel-based method to align CLIP's visual representation with that of DINOv2, ensuring that the resulting embeddings maintain compatibility with text embeddings while enhancing perceptual capabilities. Our alignment objective is designed for efficient stochastic optimization. Following this image-only alignment fine-tuning, the visual encoder retains compatibility with the frozen text encoder and exhibits significant improvements in zero-shot object recognition, fine-grained spatial reasoning, and localization. By integrating the aligned visual encoder, downstream MLLMs also demonstrate enhanced performance."
      },
      {
        "id": "oai:arXiv.org:2506.02560v1",
        "title": "DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing",
        "link": "https://arxiv.org/abs/2506.02560",
        "author": "Zixiang Li, Haoyu Wang, Wei Wang, Chuangchuang Tan, Yunchao Wei, Yao Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02560v1 Announce Type: new \nAbstract: Diffusion models have achieved remarkable success in image generation and editing tasks. Inversion within these models aims to recover the latent noise representation for a real or generated image, enabling reconstruction, editing, and other downstream tasks. However, to date, most inversion approaches suffer from an intrinsic trade-off between reconstruction accuracy and editing flexibility. This limitation arises from the difficulty of maintaining both semantic alignment and structural consistency during the inversion process. In this work, we introduce Dual-Conditional Inversion (DCI), a novel framework that jointly conditions on the source prompt and reference image to guide the inversion process. Specifically, DCI formulates the inversion process as a dual-condition fixed-point optimization problem, minimizing both the latent noise gap and the reconstruction error under the joint guidance. This design anchors the inversion trajectory in both semantic and visual space, leading to more accurate and editable latent representations. Our novel setup brings new understanding to the inversion process. Extensive experiments demonstrate that DCI achieves state-of-the-art performance across multiple editing tasks, significantly improving both reconstruction quality and editing precision. Furthermore, we also demonstrate that our method achieves strong results in reconstruction tasks, implying a degree of robustness and generalizability approaching the ultimate goal of the inversion process."
      },
      {
        "id": "oai:arXiv.org:2506.02561v1",
        "title": "Pruning General Large Language Models into Customized Expert Models",
        "link": "https://arxiv.org/abs/2506.02561",
        "author": "Yirao Zhao, Guizhen Chen, Kenji Kawaguchi, Lidong Bing, Wenxuan Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02561v1 Announce Type: new \nAbstract: Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes."
      },
      {
        "id": "oai:arXiv.org:2506.02563v1",
        "title": "Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation",
        "link": "https://arxiv.org/abs/2506.02563",
        "author": "Roie Reshef, Kfir Yehuda Levy",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02563v1 Announce Type: new \nAbstract: This paper tackles the challenge of achieving Differential Privacy (DP) in Federated Learning (FL) under partial-participation, where only a subset of the machines participate in each time-step. While previous work achieved optimal performance in full-participation settings, these methods struggled to extend to partial-participation scenarios. Our approach fills this gap by introducing a novel noise-cancellation mechanism that preserves privacy without sacrificing convergence rates or computational efficiency. We analyze our method within the Stochastic Convex Optimization (SCO) framework and show that it delivers optimal performance for both homogeneous and heterogeneous data distributions. This work expands the applicability of DP in FL, offering an efficient and practical solution for privacy-preserving learning in distributed systems with partial participation."
      },
      {
        "id": "oai:arXiv.org:2506.02571v1",
        "title": "Contrast & Compress: Learning Lightweight Embeddings for Short Trajectories",
        "link": "https://arxiv.org/abs/2506.02571",
        "author": "Abhishek Vivekanandan, Christian Hubschneider, J. Marius Z\\\"ollner",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02571v1 Announce Type: new \nAbstract: The ability to retrieve semantically and directionally similar short-range trajectories with both accuracy and efficiency is foundational for downstream applications such as motion forecasting and autonomous navigation. However, prevailing approaches often depend on computationally intensive heuristics or latent anchor representations that lack interpretability and controllability. In this work, we propose a novel framework for learning fixed-dimensional embeddings for short trajectories by leveraging a Transformer encoder trained with a contrastive triplet loss that emphasize the importance of discriminative feature spaces for trajectory data. We analyze the influence of Cosine and FFT-based similarity metrics within the contrastive learning paradigm, with a focus on capturing the nuanced directional intent that characterizes short-term maneuvers. Our empirical evaluation on the Argoverse 2 dataset demonstrates that embeddings shaped by Cosine similarity objectives yield superior clustering of trajectories by both semantic and directional attributes, outperforming FFT-based baselines in retrieval tasks. Notably, we show that compact Transformer architectures, even with low-dimensional embeddings (e.g., 16 dimensions, but qualitatively down to 4), achieve a compelling balance between retrieval performance (minADE, minFDE) and computational overhead, aligning with the growing demand for scalable and interpretable motion priors in real-time systems. The resulting embeddings provide a compact, semantically meaningful, and efficient representation of trajectory data, offering a robust alternative to heuristic similarity measures and paving the way for more transparent and controllable motion forecasting pipelines."
      },
      {
        "id": "oai:arXiv.org:2506.02572v1",
        "title": "HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference",
        "link": "https://arxiv.org/abs/2506.02572",
        "author": "Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02572v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA."
      },
      {
        "id": "oai:arXiv.org:2506.02573v1",
        "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages",
        "link": "https://arxiv.org/abs/2506.02573",
        "author": "Muhammad Falensi Azmi, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02573v1 Announce Type: new \nAbstract: Although region-specific large language models (LLMs) are increasingly developed, their safety remains underexplored, particularly in culturally diverse settings like Indonesia, where sensitivity to local norms is essential and highly valued by the community. In this work, we present IndoSafety, the first high-quality, human-verified safety evaluation dataset tailored for the Indonesian context, covering five language varieties: formal and colloquial Indonesian, along with three major local languages: Javanese, Sundanese, and Minangkabau. IndoSafety is constructed by extending prior safety frameworks to develop a taxonomy that captures Indonesia's sociocultural context. We find that existing Indonesian-centric LLMs often generate unsafe outputs, particularly in colloquial and local language settings, while fine-tuning on IndoSafety significantly improves safety while preserving task performance. Our work highlights the critical need for culturally grounded safety evaluation and provides a concrete step toward responsible LLM deployment in multilingual settings. Warning: This paper contains example data that may be offensive, harmful, or biased."
      },
      {
        "id": "oai:arXiv.org:2506.02577v1",
        "title": "Reachability Weighted Offline Goal-conditioned Resampling",
        "link": "https://arxiv.org/abs/2506.02577",
        "author": "Wenyan Yang, Joni Pajarinen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02577v1 Announce Type: new \nAbstract: Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets where many potential goals share the same state and action spaces. However, these potential goals are not explicitly represented in the collected trajectories. To learn a generalizable goal-conditioned policy, it is common to sample goals and state-action pairs uniformly using dynamic programming methods such as Q-learning. Uniform sampling, however, requires an intractably large dataset to cover all possible combinations and creates many unreachable state-goal-action pairs that degrade policy performance. Our key insight is that sampling should favor transitions that enable goal achievement. To this end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability classifier trained via positive-unlabeled (PU) learning on goal-conditioned state-action values. The classifier maps these values to a reachability score, which is then used as a sampling priority. RWS is a plug-and-play module that integrates seamlessly with standard offline RL algorithms. Experiments on six complex simulated robotic manipulation tasks, including those with a robot arm and a dexterous hand, show that RWS significantly improves performance. In one notable case, performance on the HandBlock-Z task improved by nearly 50 percent relative to the baseline. These results indicate the effectiveness of reachability-weighted sampling."
      },
      {
        "id": "oai:arXiv.org:2506.02584v1",
        "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning",
        "link": "https://arxiv.org/abs/2506.02584",
        "author": "Sarenne Wallbridge, Christoph Minixhofer, Catherine Lai, Peter Bell",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02584v1 Announce Type: new \nAbstract: People exploit the predictability of lexical structures during text comprehension. Though predictable structure is also present in speech, the degree to which prosody, e.g. intonation, tempo, and loudness, contributes to such structure independently of the lexical content is unclear. This study leverages self-supervised learning (SSL) to examine the temporal granularity of structures in the acoustic correlates of prosody. Representations from our proposed Masked Prosody Model can predict perceptual labels dependent on local information, such as word boundaries, but provide the most value for labels involving longer-term structures, like emotion recognition. Probing experiments across various perceptual labels show strong relative gains over untransformed pitch, energy, and voice activity features. Our results reveal the importance of SSL training objective timescale and highlight the value of complex SSL-encoded structures compared to more constrained classical structures."
      },
      {
        "id": "oai:arXiv.org:2506.02587v1",
        "title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations",
        "link": "https://arxiv.org/abs/2506.02587",
        "author": "Weiduo Yuan, Jerry Li, Justin Yue, Divyank Shah, Konstantinos Karydis, Hang Qiu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02587v1 Announce Type: new \nAbstract: Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometric information from the BEV feature, we introduce a novel feature selector to filter the most important features in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations on KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a new state of the art. Under various noise conditions, BEVCALIB outperforms the best baseline in the literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation), respectively. In the open-source domain, it improves the best reproducible baseline by one order of magnitude. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib."
      },
      {
        "id": "oai:arXiv.org:2506.02589v1",
        "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM",
        "link": "https://arxiv.org/abs/2506.02589",
        "author": "Maria Levchenko",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02589v1 Announce Type: new \nAbstract: This paper addresses the challenge of Named Entity Recognition (NER) for person names within the specialized domain of Russian news texts concerning cultural events. The study utilizes the unique SPbLitGuide dataset, a collection of event announcements from Saint Petersburg spanning 1999 to 2019. A comparative evaluation of diverse NER models is presented, encompassing established transformer-based architectures such as DeepPavlov, RoBERTa, and SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4, and GPT-4o. Key findings highlight the superior performance of GPT-4o when provided with specific prompting for JSON output, achieving an F1 score of 0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The research contributes to a deeper understanding of current NER model capabilities and limitations when applied to morphologically rich languages like Russian within the cultural heritage domain, offering insights for researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025) achieves F1=0.94 for both simple and structured prompts, demonstrating rapid progress across model families and simplified deployment requirements."
      },
      {
        "id": "oai:arXiv.org:2506.02591v1",
        "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures",
        "link": "https://arxiv.org/abs/2506.02591",
        "author": "Minh Duc Bui, Kyung Eun Park, Goran Glava\\v{s}, Fabian David Schmidt, Katharina von der Wense",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02591v1 Announce Type: new \nAbstract: Measurement systems (e.g., currencies) differ across cultures, but the conversions between them are well defined so that humans can state facts using any measurement system of their choice. Being available to users from diverse cultural backgrounds, large language models (LLMs) should also be able to provide accurate information irrespective of the measurement system at hand. Using newly compiled datasets we test if this is the case for seven open-source LLMs, addressing three key research questions: (RQ1) What is the default system used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate potential challenges w.r.t. underrepresented systems via reasoning? Our findings show that LLMs default to the measurement system predominantly used in the data. Additionally, we observe considerable instability and variance in performance across different measurement systems. While this instability can in part be mitigated by employing reasoning methods such as chain-of-thought (CoT), this implies longer responses and thereby significantly increases test-time compute (and inference costs), marginalizing users from cultural backgrounds that use underrepresented measurement systems."
      },
      {
        "id": "oai:arXiv.org:2506.02592v1",
        "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
        "link": "https://arxiv.org/abs/2506.02592",
        "author": "Zhi-Yuan Chen, Hao Wang, Xinyu Zhang, Enrui Hu, Yankai Lin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02592v1 Announce Type: new \nAbstract: Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference."
      },
      {
        "id": "oai:arXiv.org:2506.02596v1",
        "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing",
        "link": "https://arxiv.org/abs/2506.02596",
        "author": "Fan Gao, Dongyuan Li, Ding Xia, Fei Mi, Yasheng Wang, Lifeng Shang, Baojun Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02596v1 Announce Type: new \nAbstract: Chinese essay writing and its evaluation are critical in educational contexts, yet the capabilities of Large Language Models (LLMs) in this domain remain largely underexplored. Existing benchmarks often rely on coarse-grained text quality metrics, largely overlooking the structural and rhetorical complexities of Chinese essays, particularly across diverse genres. To address this gap, we propose \\benchName, a multi-genre benchmark specifically designed for Chinese essay writing across four major genres: Argumentative, Narrative, Descriptive, and Expository. We curate and refine a total of 728 real-world prompts to ensure authenticity and meticulously categorize them into the \\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing scenarios. To reliably evaluate generated essays, we develop a fine-grained, genre-specific scoring framework that hierarchically aggregates scores. We further validate our evaluation protocol through a comprehensive human agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their strengths and limitations across genres and instruction types. With \\benchName, we aim to advance LLM-based Chinese essay evaluation and inspire future research on improving essay generation in educational settings."
      },
      {
        "id": "oai:arXiv.org:2506.02599v1",
        "title": "Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis",
        "link": "https://arxiv.org/abs/2506.02599",
        "author": "Niklas Ro{\\ss}berg, Marion Neumeier, Sinan Hasirlioglu, Mohamed Essayed Bouzouraa, Michael Botsch",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02599v1 Announce Type: new \nAbstract: The ability to operate safely in increasingly complex traffic scenarios is a fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe release of ADS functions necessitates a precise understanding of the occurring traffic scenarios. To support this objective, this work introduces a pipeline for traffic scenario clustering and the analysis of scenario category completeness. The Clustering Vector Quantized - Variational Autoencoder (CVQ-VAE) is employed for the clustering of highway traffic scenarios and utilized to create various catalogs with differing numbers of traffic scenario categories. Subsequently, the impact of the number of categories on the completeness considerations of the traffic scenario categories is analyzed. The results show an outperforming clustering performance compared to previous work. The trade-off between cluster quality and the amount of required data to maintain completeness is discussed based on the publicly available highD dataset."
      },
      {
        "id": "oai:arXiv.org:2506.02601v1",
        "title": "Hyperspectral Image Generation with Unmixing Guided Diffusion Model",
        "link": "https://arxiv.org/abs/2506.02601",
        "author": "Shiyu Shen, Bin Pan, Ziye Zhang, Zhenwei Shi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02601v1 Announce Type: new \nAbstract: Recently, hyperspectral image generation has received increasing attention, but existing generative models rely on conditional generation schemes, which limits the diversity of generated images. Diffusion models are popular for their ability to generate high-quality samples, but adapting these models from RGB to hyperspectral data presents the challenge of high dimensionality and physical constraints. To address these challenges, we propose a novel diffusion model guided by hyperspectral unmixing. Our model comprises two key modules: an unmixing autoencoder module and an abundance diffusion module. The unmixing autoencoder module leverages unmixing guidance to shift the generative task from the image space to the low-dimensional abundance space, significantly reducing computational complexity while preserving high fidelity. The abundance diffusion module generates samples that satisfy the constraints of non-negativity and unity, ensuring the physical consistency of the reconstructed HSIs. Additionally, we introduce two evaluation metrics tailored to hyperspectral data. Empirical results, evaluated using both traditional metrics and our proposed metrics, indicate that our model is capable of generating high-quality and diverse hyperspectral images, offering an advancement in hyperspectral data generation."
      },
      {
        "id": "oai:arXiv.org:2506.02604v1",
        "title": "Application of convolutional neural networks in image super-resolution",
        "link": "https://arxiv.org/abs/2506.02604",
        "author": "Tian Chunwei, Song Mingjian, Zuo Wangmeng, Du Bo, Zhang Yanning, Zhang Shichao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02604v1 Announce Type: new \nAbstract: Due to strong learning abilities of convolutional neural networks (CNNs), they have become mainstream methods for image super-resolution. However, there are big differences of different deep learning methods with different types. There is little literature to summarize relations and differences of different methods in image super-resolution. Thus, summarizing these literatures are important, according to loading capacity and execution speed of devices. This paper first introduces principles of CNNs in image super-resolution, then introduces CNNs based bicubic interpolation, nearest neighbor interpolation, bilinear interpolation, transposed convolution, sub-pixel layer, meta up-sampling for image super-resolution to analyze differences and relations of different CNNs based interpolations and modules, and compare performance of these methods by experiments. Finally, this paper gives potential research points and drawbacks and summarizes the whole paper, which can facilitate developments of CNNs in image super-resolution."
      },
      {
        "id": "oai:arXiv.org:2506.02605v1",
        "title": "One-Step Diffusion-based Real-World Image Super-Resolution with Visual Perception Distillation",
        "link": "https://arxiv.org/abs/2506.02605",
        "author": "Xue Wu, Jingwei Xin, Zhijun Tu, Jie Hu, Jie Li, Nannan Wang, Xinbo Gao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02605v1 Announce Type: new \nAbstract: Diffusion-based models have been widely used in various visual generation tasks, showing promising results in image super-resolution (SR), while typically being limited by dozens or even hundreds of sampling steps. Although existing methods aim to accelerate the inference speed of multi-step diffusion-based SR methods through knowledge distillation, their generated images exhibit insufficient semantic alignment with real images, resulting in suboptimal perceptual quality reconstruction, specifically reflected in the CLIPIQA score. These methods still have many challenges in perceptual quality and semantic fidelity. Based on the challenges, we propose VPD-SR, a novel visual perception diffusion distillation framework specifically designed for SR, aiming to construct an effective and efficient one-step SR model. Specifically, VPD-SR consists of two components: Explicit Semantic-aware Supervision (ESS) and High-Frequency Perception (HFP) loss. Firstly, the ESS leverages the powerful visual perceptual understanding capabilities of the CLIP model to extract explicit semantic supervision, thereby enhancing semantic consistency. Then, Considering that high-frequency information contributes to the visual perception quality of images, in addition to the vanilla distillation loss, the HFP loss guides the student model to restore the missing high-frequency details in degraded images that are critical for enhancing perceptual quality. Lastly, we expand VPD-SR in adversarial training manner to further enhance the authenticity of the generated content. Extensive experiments conducted on synthetic and real-world datasets demonstrate that the proposed VPD-SR achieves superior performance compared to both previous state-of-the-art methods and the teacher model with just one-step sampling."
      },
      {
        "id": "oai:arXiv.org:2506.02612v1",
        "title": "Simple, Good, Fast: Self-Supervised World Models Free of Baggage",
        "link": "https://arxiv.org/abs/2506.02612",
        "author": "Jan Robine, Marc H\\\"oftmann, Stefan Harmeling",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02612v1 Announce Type: new \nAbstract: What are the essential components of world models? How far do we get with world models that are not employing RNNs, transformers, discrete representations, and image reconstructions? This paper introduces SGF, a Simple, Good, and Fast world model that uses self-supervised representation learning, captures short-time dependencies through frame and action stacking, and enhances robustness against model errors through data augmentation. We extensively discuss SGF's connections to established world models, evaluate the building blocks in ablation studies, and demonstrate good performance through quantitative comparisons on the Atari 100k benchmark."
      },
      {
        "id": "oai:arXiv.org:2506.02614v1",
        "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset",
        "link": "https://arxiv.org/abs/2506.02614",
        "author": "Guohang Zhuang, Weixi Song, Jinyang Huang, Chenwei Yang, Yan Lu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02614v1 Announce Type: new \nAbstract: With the rapid development of space exploration, space debris has attracted more attention due to its potential extreme threat, leading to the need for real-time and accurate debris tracking. However, existing methods are mainly based on traditional signal processing, which cannot effectively process the complex background and dense space debris. In this paper, we propose a deep learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly accurate debris tracking. SDT-Net effectively represents the feature of debris, enhancing the efficiency and stability of end-to-end model learning. To train and evaluate this model effectively, we also produce a large-scale dataset Space Debris Tracking Dataset (SDTD) by a novel observation-based data simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562 frames and covers 250,000 synthetic space debris. Extensive experiments validate the effectiveness of our model and the challenging of our dataset. Furthermore, we test our model on real data from the Antarctic Station, achieving a MOTA score of 70.6%, which demonstrates its strong transferability to real-world scenarios. Our dataset and code will be released soon."
      },
      {
        "id": "oai:arXiv.org:2506.02615v1",
        "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models",
        "link": "https://arxiv.org/abs/2506.02615",
        "author": "Safaa Abdullahi Moallim Mohamud, Minjin Baek, Dong Seog Han",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02615v1 Announce Type: new \nAbstract: In this paper, we present a hierarchical question-answering (QA) approach for scene understanding in autonomous vehicles, balancing cost-efficiency with detailed visual interpretation. The method fine-tunes a compact vision-language model (VLM) on a custom dataset specific to the geographical area in which the vehicle operates to capture key driving-related visual elements. At the inference stage, the hierarchical QA strategy decomposes the scene understanding task into high-level and detailed sub-questions. Instead of generating lengthy descriptions, the VLM navigates a structured question tree, where answering high-level questions (e.g., \"Is it possible for the ego vehicle to turn left at the intersection?\") triggers more detailed sub-questions (e.g., \"Is there a vehicle approaching the intersection from the opposite direction?\"). To optimize inference time, questions are dynamically skipped based on previous answers, minimizing computational overhead. The extracted answers are then synthesized using handcrafted templates to ensure coherent, contextually accurate scene descriptions. We evaluate the proposed approach on the custom dataset using GPT reference-free scoring, demonstrating its competitiveness with state-of-the-art methods like GPT-4o in capturing key scene details while achieving significantly lower inference time. Moreover, qualitative results from real-time deployment highlight the proposed approach's capacity to capture key driving elements with minimal latency."
      },
      {
        "id": "oai:arXiv.org:2506.02616v1",
        "title": "Compositional Learning for Modular Multi-Agent Self-Organizing Networks",
        "link": "https://arxiv.org/abs/2506.02616",
        "author": "Qi Liao, Parijat Bhattacharjee",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02616v1 Announce Type: new \nAbstract: Self-organizing networks face challenges from complex parameter interdependencies and conflicting objectives. This study introduces two compositional learning approaches-Compositional Deep Reinforcement Learning (CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their performance under training time and safety constraints in multi-agent systems. We propose a modular, two-tier framework with cell-level and cell-pair-level agents to manage heterogeneous agent granularities while reducing model complexity. Numerical simulations reveal a significant reduction in handover failures, along with improved throughput and latency, outperforming conventional multi-agent deep reinforcement learning approaches. The approach also demonstrates superior scalability, faster convergence, higher sample efficiency, and safer training in large-scale self-organizing networks."
      },
      {
        "id": "oai:arXiv.org:2506.02619v1",
        "title": "HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport",
        "link": "https://arxiv.org/abs/2506.02619",
        "author": "Yanbei Liu, Chongxu Wang, Zhitao Xiao, Lei Geng, Yanwei Pang, Xiao Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02619v1 Announce Type: new \nAbstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent capabilities in processing heterogeneous information networks. Self-supervised learning on heterogeneous graphs, especially contrastive self-supervised strategy, shows great potential when there are no labels. However, this approach requires the use of carefully designed graph augmentation strategies and the selection of positive and negative samples. Determining the exact level of similarity between sample pairs is non-trivial.To solve this problem, we propose a novel self-supervised Heterogeneous graph neural network with Optimal Transport (HGOT) method which is designed to facilitate self-supervised learning for heterogeneous graphs without graph augmentation strategies. Different from traditional contrastive self-supervised learning, HGOT employs the optimal transport mechanism to relieve the laborious sampling process of positive and negative samples. Specifically, we design an aggregating view (central view) to integrate the semantic information contained in the views represented by different meta-paths (branch views). Then, we introduce an optimal transport plan to identify the transport relationship between the semantics contained in the branch view and the central view. This allows the optimal transport plan between graphs to align with the representations, forcing the encoder to learn node representations that are more similar to the graph space and of higher quality. Extensive experiments on four real-world datasets demonstrate that our proposed HGOT model can achieve state-of-the-art performance on various downstream tasks. In particular, in the node classification task, HGOT achieves an average of more than 6% improvement in accuracy compared with state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2506.02623v1",
        "title": "SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search",
        "link": "https://arxiv.org/abs/2506.02623",
        "author": "Yuyang Zhou, Ferrante Neri, Yew-Soon Ong, Ruibin Bai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02623v1 Announce Type: new \nAbstract: Modern neural architecture search (NAS) is inherently multi-objective, balancing trade-offs such as accuracy, parameter count, and computational cost. This complexity makes NAS computationally expensive and nearly impossible to solve without efficient approximations. To address this, we propose a novel surrogate modelling approach that leverages an ensemble of Siamese network blocks to predict dominance relationships between candidate architectures. Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces the crowding distance calculation in the survivor selection strategy with a heuristic rule based on model size. Integrated into a framework termed SiamNAS, this design eliminates costly evaluations during the search process. Experiments on NAS-Bench-201 demonstrate the framework's ability to identify Pareto-optimal solutions with significantly reduced computational costs. The proposed SiamNAS identified a final non-dominated set containing the best architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in terms of test error rate, within 0.01 GPU days. This proof-of-concept study highlights the potential of the proposed Siamese network surrogate model to generalise to multi-tasking optimisation, enabling simultaneous optimisation across tasks. Additionally, it offers opportunities to extend the approach for generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal solutions for heterogeneous task settings."
      },
      {
        "id": "oai:arXiv.org:2506.02626v1",
        "title": "Synthetic Iris Image Databases and Identity Leakage: Risks and Mitigation Strategies",
        "link": "https://arxiv.org/abs/2506.02626",
        "author": "Ada Sawilska, Mateusz Trokielewicz",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02626v1 Announce Type: new \nAbstract: This paper presents a comprehensive overview of iris image synthesis methods, which can alleviate the issues associated with gathering large, diverse datasets of biometric data from living individuals, which are considered pivotal for biometric methods development. These methods for synthesizing iris data range from traditional, hand crafted image processing-based techniques, through various iterations of GAN-based image generators, variational autoencoders (VAEs), as well as diffusion models. The potential and fidelity in iris image generation of each method is discussed and examples of inferred predictions are provided. Furthermore, the risks of individual biometric features leakage from the training sets are considered, together with possible strategies for preventing them, which have to be implemented should these generative methods be considered a valid replacement of real-world biometric datasets."
      },
      {
        "id": "oai:arXiv.org:2506.02627v1",
        "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning",
        "link": "https://arxiv.org/abs/2506.02627",
        "author": "\\\"Omer Tarik \\\"Ozyilmaz, Matt Coler, Matias Valdenegro-Toro",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02627v1 Announce Type: new \nAbstract: Although commercial Arabic automatic speech recognition (ASR) systems support Modern Standard Arabic (MSA), they struggle with dialectal speech. We investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA training size effects, benefits of pre-training on MSA data, and dialect-specific versus dialect-pooled models. We find that small amounts of MSA fine-tuning data yield substantial improvements for smaller models, matching larger non-fine-tuned models. While MSA pre-training shows minimal benefit, suggesting limited shared features between MSA and dialects, our dialect-pooled models perform comparably to dialect-specific ones. This indicates that pooling dialectal data, when properly balanced, can help address data scarcity in low-resource ASR without significant performance loss."
      },
      {
        "id": "oai:arXiv.org:2506.02630v1",
        "title": "HAM: A Hyperbolic Step to Regulate Implicit Bias",
        "link": "https://arxiv.org/abs/2506.02630",
        "author": "Tom Jacobs, Advait Gadhikar, Celia Rubio-Madrigal, Rebekka Burkholz",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02630v1 Announce Type: new \nAbstract: Understanding the implicit bias of optimization algorithms has become central to explaining the generalization behavior of deep learning models. For instance, the hyperbolic implicit bias induced by the overparameterization $m \\odot w$--though effective in promoting sparsity--can result in a small effective learning rate, which slows down convergence. To overcome this obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates between an optimizer step and a new hyperbolic mirror step. We derive the Riemannian gradient flow for its combination with gradient descent, leading to improved convergence and a similar beneficial hyperbolic geometry as $m \\odot w$ for feature learning. We provide an interpretation of the the algorithm by relating it to natural gradient descent, and an exact characterization of its implicit bias for underdetermined linear regression. HAM's implicit bias consistently boosts performance--even of dense training, as we demonstrate in experiments across diverse tasks, including vision, graph and node classification, and large language model fine-tuning. HAM is especially effective in combination with different sparsification methods, improving upon the state of the art. The hyperbolic step requires minimal computational and memory overhead, it succeeds even with small batch sizes, and its implementation integrates smoothly with existing optimizers."
      },
      {
        "id": "oai:arXiv.org:2506.02633v1",
        "title": "ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration",
        "link": "https://arxiv.org/abs/2506.02633",
        "author": "Cheng Yang, Lijing Liang, Zhixun Su",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02633v1 Announce Type: new \nAbstract: This paper proposes ControlMambaIR, a novel image restoration method designed to address perceptual challenges in image deraining, deblurring, and denoising tasks. By integrating the Mamba network architecture with the diffusion model, the condition network achieves refined conditional control, thereby enhancing the control and optimization of the image generation process. To evaluate the robustness and generalization capability of our method across various image degradation conditions, extensive experiments were conducted on several benchmark datasets, including Rain100H, Rain100L, GoPro, and SSID. The results demonstrate that our proposed approach consistently surpasses existing methods in perceptual quality metrics, such as LPIPS and FID, while maintaining comparable performance in image distortion metrics, including PSNR and SSIM, highlighting its effectiveness and adaptability. Notably, ablation experiments reveal that directly noise prediction in the diffusion process achieves better performance, effectively balancing noise suppression and detail preservation. Furthermore, the findings indicate that the Mamba architecture is particularly well-suited as a conditional control network for diffusion models, outperforming both CNN- and Attention-based approaches in this context. Overall, these results highlight the flexibility and effectiveness of ControlMambaIR in addressing a range of image restoration perceptual challenges."
      },
      {
        "id": "oai:arXiv.org:2506.02654v1",
        "title": "A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction",
        "link": "https://arxiv.org/abs/2506.02654",
        "author": "Shiyu Shen, Bin Pan, Guirong Xue",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02654v1 Announce Type: new \nAbstract: City-scale traffic volume prediction plays a pivotal role in intelligent transportation systems, yet remains a challenge due to the inherent incompleteness and bias in observational data. Although deep learning-based methods have shown considerable promise, most existing approaches produce deterministic point estimates, thereby neglecting the uncertainty arising from unobserved traffic flows. Furthermore, current models are typically trained in a city-specific manner, which hinders their generalizability and limits scalability across diverse urban contexts. To overcome these limitations, we introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model traffic volume as a distributional aggregation of trajectories. Our framework fuses heterogeneous data sources-including real-time observations, historical trajectory data, and road network topology-enabling robust and uncertainty-aware traffic inference. TrafficPPT is initially pretrained on large-scale simulated data spanning multiple urban scenarios, and later fine-tuned on target cities to ensure effective domain adaptation. Experiments on real-world datasets show that TrafficPPT consistently surpasses state-of-the-art baselines, particularly under conditions of extreme data sparsity. Code will be open."
      },
      {
        "id": "oai:arXiv.org:2506.02659v1",
        "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs",
        "link": "https://arxiv.org/abs/2506.02659",
        "author": "Manon Reusens, Bart Baesens, David Jurgens",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02659v1 Announce Type: new \nAbstract: Personalized Large Language Models (LLMs) are increasingly used in diverse applications, where they are assigned a specific persona - such as a happy high school teacher - to guide their responses. While prior research has examined how well LLMs adhere to predefined personas in writing style, a comprehensive analysis of consistency across different personas and task types is lacking. In this paper, we introduce a new standardized framework to analyze consistency in persona-assigned LLMs. We define consistency as the extent to which a model maintains coherent responses when assigned the same persona across different tasks and runs. Our framework evaluates personas across four different categories (happiness, occupation, personality, and political stance) spanning multiple task dimensions (survey writing, essay generation, social media post generation, single turn, and multi-turn conversations). Our findings reveal that consistency is influenced by multiple factors, including the assigned persona, stereotypes, and model design choices. Consistency also varies across tasks, increasing with more structured tasks and additional context. All code is available on GitHub."
      },
      {
        "id": "oai:arXiv.org:2506.02665v1",
        "title": "Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection",
        "link": "https://arxiv.org/abs/2506.02665",
        "author": "Tianci Liu, Tong Yang, Quan Zhang, Qi Lei",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02665v1 Announce Type: new \nAbstract: As AI advances, copyrighted content faces growing risk of unauthorized use, whether through model training or direct misuse. Building upon invisible adversarial perturbation, recent works developed copyright protections against specific AI techniques such as unauthorized personalization through DreamBooth that are misused. However, these methods offer only short-term security, as they require retraining whenever the underlying model architectures change. To establish long-term protection aiming at better robustness, we go beyond invisible perturbation, and propose a universal approach that embeds \\textit{visible} watermarks that are \\textit{hard-to-remove} into images. Grounded in a new probabilistic and inverse problem-based formulation, our framework maximizes the discrepancy between the \\textit{optimal} reconstruction and the original content. We develop an effective and efficient approximation algorithm to circumvent a intractable bi-level optimization. Experimental results demonstrate superiority of our approach across diverse scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.02671v1",
        "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet",
        "link": "https://arxiv.org/abs/2506.02671",
        "author": "Xiao Chen, Jiazhen Huang, Qinting Jiang, Fanding Huang, Xianghua Fu, Jingyan Jiang, Zhi Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02671v1 Announce Type: new \nAbstract: Test-time adaptation (TTA) has emerged as a critical technique for enhancing the generalization capability of vision-language models (VLMs) during inference. However, existing approaches often incur substantial computational costs and exhibit poor scalability, primarily due to sample-wise adaptation granularity and reliance on costly auxiliary designs such as data augmentation. To address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel adapter-based TTA framework that leverages a lightweight, learnable AdaptNet to enable efficient and scalable model adaptation. As SAIL's core, a frozen pre-trained VLM collaborates with AdaptNet through a confidence-based interpolation weight, generating robust predictions during inference. These predictions serve as self-supervised targets to align AdaptNet's outputs through efficient batch-wise processing, dramatically reducing computational costs without modifying the VLM or requiring memory caches. To mitigate catastrophic forgetting during continual adaptation, we propose a gradient-aware reset strategy driven by a gradient drift indicator (GDI), which dynamically detects domain transitions and strategically resets AdaptNet for stable adaptation. Extensive experiments across diverse benchmarks on two scenarios demonstrate that SAIL achieves state-of-the-art performance while maintaining low computational costs. These results highlight SAIL's effectiveness, efficiency and scalability for real-world deployment. The code will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2506.02672v1",
        "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving",
        "link": "https://arxiv.org/abs/2506.02672",
        "author": "Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, Cheng Zhong, Zongzhang Zhang, Tao Gui, Chao Xin, Wei Chengzhi, Lin Yan, Qi Zhang, Xuanjing Huang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02672v1 Announce Type: new \nAbstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available at the GitHub repository."
      },
      {
        "id": "oai:arXiv.org:2506.02677v1",
        "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation",
        "link": "https://arxiv.org/abs/2506.02677",
        "author": "Jintao Tong, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02677v1 Announce Type: new \nAbstract: Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a source-domain dataset to unseen target-domain datasets with limited annotations. Current methods typically compare the distance between training and testing samples for mask prediction. However, we find an entanglement problem exists in this widely adopted method, which tends to bind sourcedomain patterns together and make each of them hard to transfer. In this paper, we aim to address this problem for the CD-FSS task. We first find a natural decomposition of the ViT structure, based on which we delve into the entanglement problem for an interpretation. We find the decomposed ViT components are crossly compared between images in distance calculation, where the rational comparisons are entangled with those meaningless ones by their equal importance, leading to the entanglement problem. Based on this interpretation, we further propose to address the entanglement problem by learning to weigh for all comparisons of ViT components, which learn disentangled features and re-compose them for the CD-FSS task, benefiting both the generalization and finetuning. Experiments show that our model outperforms the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under 1-shot and 5-shot settings, respectively."
      },
      {
        "id": "oai:arXiv.org:2506.02678v1",
        "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
        "link": "https://arxiv.org/abs/2506.02678",
        "author": "Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Xing W, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo, Xiao Liu, Fei Yin, Cheng-Lin Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02678v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon."
      },
      {
        "id": "oai:arXiv.org:2506.02680v1",
        "title": "Solving Inverse Problems with FLAIR",
        "link": "https://arxiv.org/abs/2506.02680",
        "author": "Julius Erbach, Dominik Narnhofer, Andreas Dombos, Bernt Schiele, Jan Eric Lenssen, Konrad Schindler",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02680v1 Announce Type: new \nAbstract: Flow-based latent generative models such as Stable Diffusion 3 are able to generate images with remarkable quality, even enabling photorealistic text-to-image generation. Their impressive performance suggests that these models should also constitute powerful priors for inverse imaging problems, but that approach has not yet led to comparable fidelity. There are several key obstacles: (i) the encoding into a lower-dimensional latent space makes the underlying (forward) mapping non-linear; (ii) the data likelihood term is usually intractable; and (iii) learned generative models struggle to recover rare, atypical data modes during inference. We present FLAIR, a novel training free variational framework that leverages flow-based generative models as a prior for inverse problems. To that end, we introduce a variational objective for flow matching that is agnostic to the type of degradation, and combine it with deterministic trajectory adjustments to recover atypical modes. To enforce exact consistency with the observed data, we decouple the optimization of the data fidelity and regularization terms. Moreover, we introduce a time-dependent calibration scheme in which the strength of the regularization is modulated according to off-line accuracy estimates. Results on standard imaging benchmarks demonstrate that FLAIR consistently outperforms existing diffusion- and flow-based methods in terms of reconstruction quality and sample diversity."
      },
      {
        "id": "oai:arXiv.org:2506.02683v1",
        "title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints",
        "link": "https://arxiv.org/abs/2506.02683",
        "author": "Zhengdong Lu, Weikai Lu, Yiling Tao, Yun Dai, ZiXuan Chen, Huiping Zhuang, Cen Chen, Hao Peng, Ziqian Zeng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02683v1 Announce Type: new \nAbstract: Despite significant advances in Large Language Models (LLMs), planning tasks still present challenges for LLM-based agents. Existing planning methods face two key limitations: heavy constraints and cascading errors. To address these limitations, we propose a novel parallel planning paradigm, which Decomposes, Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM). Specifically, DPPM decomposes the complex task based on constraints into subtasks, generates the subplan for each subtask in parallel, and merges them into a global plan. In addition, our approach incorporates a verification and refinement module, enabling error correction and conflict resolution. Experimental results demonstrate that DPPM significantly outperforms existing methods in travel planning tasks."
      },
      {
        "id": "oai:arXiv.org:2506.02686v1",
        "title": "Random Hyperbolic Graphs with Arbitrary Mesoscale Structures",
        "link": "https://arxiv.org/abs/2506.02686",
        "author": "Stefano Guarino, Davide Torre, Enrico Mastrostefano",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02686v1 Announce Type: new \nAbstract: Real-world networks exhibit universal structural properties such as sparsity, small-worldness, heterogeneous degree distributions, high clustering, and community structures. Geometric network models, particularly Random Hyperbolic Graphs (RHGs), effectively capture many of these features by embedding nodes in a latent similarity space. However, networks are often characterized by specific connectivity patterns between groups of nodes -- i.e. communities -- that are not geometric, in the sense that the dissimilarity between groups do not obey the triangle inequality. Structuring connections only based on the interplay of similarity and popularity thus poses fundamental limitations on the mesoscale structure of the networks that RHGs can generate. To address this limitation, we introduce the Random Hyperbolic Block Model (RHBM), which extends RHGs by incorporating block structures within a maximum-entropy framework. We demonstrate the advantages of the RHBM through synthetic network analyses, highlighting its ability to preserve community structures where purely geometric models fail. Our findings emphasize the importance of latent geometry in network modeling while addressing its limitations in controlling mesoscale mixing patterns."
      },
      {
        "id": "oai:arXiv.org:2506.02689v1",
        "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching",
        "link": "https://arxiv.org/abs/2506.02689",
        "author": "Liang Yue, Yihong Tang, Kehai Chen, Jie Liu, Min Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02689v1 Announce Type: new \nAbstract: Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models' instruction-following capabilities and task-specific performance. However, obtaining high-quality fine-tuning data for large models is challenging due to data collection difficulties and high production costs. To address this, we propose MASTER, a novel data augmentation method that enriches original data through interactions among multiple agents with varying cognitive levels. We simulate three pedagogically grounded teaching scenarios, leveraging multi-agent conversations to generate high-quality teacher-student interaction data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5. Experiments show that models fine-tuned with BOOST-QA perform excellently across multiple benchmarks, demonstrating strong multitask generalization. Notably, MASTER significantly improves models' reasoning abilities in complex tasks, providing valuable insights for future research."
      },
      {
        "id": "oai:arXiv.org:2506.02690v1",
        "title": "Towards Geometry Problem Solving in the Large Model Era: A Survey",
        "link": "https://arxiv.org/abs/2506.02690",
        "author": "Yurui Zhao, Xiang Wang, Jiahong Liu, Irwin King, Zhitao Huang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02690v1 Announce Type: new \nAbstract: Geometry problem solving (GPS) represents a critical frontier in artificial intelligence, with profound applications in education, computer-aided design, and computational graphics. Despite its significance, automating GPS remains challenging due to the dual demands of spatial understanding and rigorous logical reasoning. Recent advances in large models have enabled notable breakthroughs, particularly for SAT-level problems, yet the field remains fragmented across methodologies, benchmarks, and evaluation frameworks. This survey systematically synthesizes GPS advancements through three core dimensions: (1) benchmark construction, (2) textual and diagrammatic parsing, and (3) reasoning paradigms. We further propose a unified analytical paradigm, assess current limitations, and identify emerging opportunities to guide future research toward human-level geometric reasoning, including automated benchmark generation and interpretable neuro-symbolic integration."
      },
      {
        "id": "oai:arXiv.org:2506.02692v1",
        "title": "Large-scale Self-supervised Video Foundation Model for Intelligent Surgery",
        "link": "https://arxiv.org/abs/2506.02692",
        "author": "Shu Yang, Fengtao Zhou, Leon Mayer, Fuxiang Huang, Yiliang Chen, Yihui Wang, Sunan He, Yuxiang Nie, Xi Wang, \\\"Omer S\\\"umer, Yueming Jin, Huihui Sun, Shuchang Xu, Alex Qinyang Liu, Zheng Li, Jing Qin, Jeremy YuenChun Teoh, Lena Maier-Hein, Hao Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02692v1 Announce Type: new \nAbstract: Computer-Assisted Intervention (CAI) has the potential to revolutionize modern surgery, with surgical scene understanding serving as a critical component in supporting decision-making, improving procedural efficacy, and ensuring intraoperative safety. While existing AI-driven approaches alleviate annotation burdens via self-supervised spatial representation learning, their lack of explicit temporal modeling during pre-training fundamentally restricts the capture of dynamic surgical contexts, resulting in incomplete spatiotemporal understanding. In this work, we introduce the first video-level surgical pre-training framework that enables joint spatiotemporal representation learning from large-scale surgical video data. To achieve this, we constructed a large-scale surgical video dataset comprising 3,650 videos and approximately 3.55 million frames, spanning more than 20 surgical procedures and over 10 anatomical structures. Building upon this dataset, we propose SurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a reconstruction-based pre-training method that captures intricate spatial structures and temporal dynamics through joint spatiotemporal modeling. Additionally, SurgVISTA incorporates image-level knowledge distillation guided by a surgery-specific expert to enhance the learning of fine-grained anatomical and semantic features. To validate its effectiveness, we established a comprehensive benchmark comprising 13 video-level datasets spanning six surgical procedures across four tasks. Extensive experiments demonstrate that SurgVISTA consistently outperforms both natural- and surgical-domain pre-trained models, demonstrating strong potential to advance intelligent surgical systems in clinically meaningful scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.02694v1",
        "title": "XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation",
        "link": "https://arxiv.org/abs/2506.02694",
        "author": "Daichi Kimura, Tomonori Izumitani, Hisashi Kashima",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02694v1 Announce Type: new \nAbstract: Various Transformer-based models have been proposed for time series forecasting. These models leverage the self-attention mechanism to capture long-term temporal or variate dependencies in sequences. Existing methods can be divided into two approaches: (1) reducing computational cost of attention by making the calculations sparse, and (2) reshaping the input data to aggregate temporal features. However, existing attention mechanisms may not adequately capture inherent nonlinear dependencies present in time series data, leaving room for improvement. In this study, we propose a novel attention mechanism based on Chatterjee's rank correlation coefficient, which measures nonlinear dependencies between variables. Specifically, we replace the matrix multiplication in standard attention mechanisms with this rank coefficient to measure the query-key relationship. Since computing Chatterjee's correlation coefficient involves sorting and ranking operations, we introduce a differentiable approximation employing SoftSort and SoftRank. Our proposed mechanism, ``XicorAttention,'' integrates it into several state-of-the-art Transformer models. Experimental results on real-world datasets demonstrate that incorporating nonlinear correlation into the attention improves forecasting accuracy by up to approximately 9.1\\% compared to existing models."
      },
      {
        "id": "oai:arXiv.org:2506.02695v1",
        "title": "FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition",
        "link": "https://arxiv.org/abs/2506.02695",
        "author": "Linquan Wu, Tianxiang Jiang, Wenhao Duan, Yini Fang, Jacky Keung",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02695v1 Announce Type: new \nAbstract: Micro-expression recognition (MER) demands models that can amplify millisecond-level, low-amplitude facial motions while suppressing identity-specific appearance. We introduce FaceSleuth, a dual-stream architecture that (1) enhances motion along the empirically dominant vertical axix through a Continuously Vertical Attention (CVA) block, (2) localises the resulting signals with a Facial Position Focalizer built on hierarchical cross-window attention, and (3) steers feature learning toward physiologically meaningful regions via lightweight Action-Unit embeddings. To examine whether the hand-chosen vertical axis is indeed optimal, we further propose a Single-Orientation Attention (SOA) module that learns its own pooling direction end-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses to CVA when the learned angle converges to {\\Pi}/2. In practice, SOA reliably drifts to 88{\\deg}, confirming the effectiveness of the vertical prior while delivering consistent gains. On three standard MER benchmarks, FaceSleuth with CVA already surpasses previous state-of-the-art methods; plugging in SOA lifts accuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840 on SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness. These results establish a new state of the art and, for the first time, provide empirical evidence that the vertical attention bias is the most discriminative orientation for MER."
      },
      {
        "id": "oai:arXiv.org:2506.02697v1",
        "title": "LayoutRAG: Retrieval-Augmented Model for Content-agnostic Conditional Layout Generation",
        "link": "https://arxiv.org/abs/2506.02697",
        "author": "Yuxuan Wu, Le Wang, Sanping Zhou, Mengnan Liu, Gang Hua, Haoxiang Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02697v1 Announce Type: new \nAbstract: Controllable layout generation aims to create plausible visual arrangements of element bounding boxes within a graphic design according to certain optional constraints, such as the type or position of a specific component. While recent diffusion or flow-matching models have achieved considerable advances in multifarious conditional generation tasks, there remains considerable room for generating optimal arrangements under given conditions. In this work, we propose to carry out layout generation through retrieving by conditions and reference-guided generation. Specifically, we retrieve appropriate layout templates according to given conditions as references. The references are then utilized to guide the denoising or flow-based transport process. By retrieving layouts compatible with the given conditions, we can uncover the potential information not explicitly provided in the given condition. Such an approach offers more effective guidance to the model during the generation process, in contrast to previous models that feed the condition to the model and let the model infer the unprovided layout attributes directly. Meanwhile, we design a condition-modulated attention that selectively absorbs retrieval knowledge, adapting to the difference between retrieved templates and given conditions. Extensive experiment results show that our method successfully produces high-quality layouts that meet the given conditions and outperforms existing state-of-the-art models. Code will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2506.02698v1",
        "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences",
        "link": "https://arxiv.org/abs/2506.02698",
        "author": "Yunhong Lu, Qichao Wang, Hengyuan Cao, Xiaoyin Xu, Min Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02698v1 Announce Type: new \nAbstract: Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation models with human preferences using pairwise preference data. Although substantial resources are expended in collecting and labeling datasets, a critical aspect is often neglected: \\textit{preferences vary across individuals and should be represented with more granularity.} To address this, we propose SmPO-Diffusion, a novel method for modeling preference distributions to improve the DPO objective, along with a numerical upper bound estimation for the diffusion optimization objective. First, we introduce a smoothed preference distribution to replace the original binary distribution. We employ a reward model to simulate human preferences and apply preference likelihood averaging to improve the DPO loss, such that the loss function approaches zero when preferences are similar. Furthermore, we utilize an inversion technique to simulate the trajectory preference distribution of the diffusion model, enabling more accurate alignment with the optimization objective. Our approach effectively mitigates issues of excessive optimization and objective misalignment present in existing methods through straightforward modifications. Our SmPO-Diffusion achieves state-of-the-art performance in preference evaluation, outperforming baselines across metrics with lower training costs. The project page is https://jaydenlyh.github.io/SmPO-project-page/."
      },
      {
        "id": "oai:arXiv.org:2506.02701v1",
        "title": "On Entity Identification in Language Models",
        "link": "https://arxiv.org/abs/2506.02701",
        "author": "Masaki Sakata, Sho Yokoi, Benjamin Heinzerling, Takumi Ito, Kentaro Inui",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02701v1 Announce Type: new \nAbstract: We analyze the extent to which internal representations of language models (LMs) identify and distinguish mentions of named entities, focusing on the many-to-many correspondence between entities and their mentions. We first formulate two problems of entity mentions -- ambiguity and variability -- and propose a framework analogous to clustering quality metrics. Specifically, we quantify through cluster analysis of LM internal representations the extent to which mentions of the same entity cluster together and mentions of different entities remain separated. Our experiments examine five Transformer-based autoregressive models, showing that they effectively identify and distinguish entities with metrics analogous to precision and recall ranging from 0.66 to 0.9. Further analysis reveals that entity-related information is compactly represented in a low-dimensional linear subspace at early LM layers. Additionally, we clarify how the characteristics of entity representations influence word prediction performance. These findings are interpreted through the lens of isomorphism between LM representations and entity-centric knowledge structures in the real world, providing insights into how LMs internally organize and use entity information."
      },
      {
        "id": "oai:arXiv.org:2506.02702v1",
        "title": "ToothForge: Automatic Dental Shape Generation using Synchronized Spectral Embeddings",
        "link": "https://arxiv.org/abs/2506.02702",
        "author": "Tibor Kub\\'ik, Fran\\c{c}ois Guibault, Michal \\v{S}pan\\v{e}l, Herv\\'e Lombaert",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02702v1 Announce Type: new \nAbstract: We introduce ToothForge, a spectral approach for automatically generating novel 3D teeth, effectively addressing the sparsity of dental shape datasets. By operating in the spectral domain, our method enables compact machine learning modeling, allowing the generation of high-resolution tooth meshes in milliseconds. However, generating shape spectra comes with the instability of the decomposed harmonics. To address this, we propose modeling the latent manifold on synchronized frequential embeddings. Spectra of all data samples are aligned to a common basis prior to the training procedure, effectively eliminating biases introduced by the decomposition instability. Furthermore, synchronized modeling removes the limiting factor imposed by previous methods, which require all shapes to share a common fixed connectivity. Using a private dataset of real dental crowns, we observe a greater reconstruction quality of the synthetized shapes, exceeding those of models trained on unaligned embeddings. We also explore additional applications of spectral analysis in digital dentistry, such as shape compression and interpolation. ToothForge facilitates a range of approaches at the intersection of spectral analysis and machine learning, with fewer restrictions on mesh structure. This makes it applicable for shape analysis not only in dentistry, but also in broader medical applications, where guaranteeing consistent connectivity across shapes from various clinics is unrealistic. The code is available at https://github.com/tiborkubik/toothForge."
      },
      {
        "id": "oai:arXiv.org:2506.02703v1",
        "title": "Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies",
        "link": "https://arxiv.org/abs/2506.02703",
        "author": "Khizar Hayat, Baptiste Magnier",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02703v1 Announce Type: new \nAbstract: This study critically examines the methodological rigor in credit card fraud detection research, revealing how fundamental evaluation flaws can overshadow algorithmic sophistication. Through deliberate experimentation with improper evaluation protocols, we demonstrate that even simple models can achieve deceptively impressive results when basic methodological principles are violated. Our analysis identifies four critical issues plaguing current approaches: (1) pervasive data leakage from improper preprocessing sequences, (2) intentional vagueness in methodological reporting, (3) inadequate temporal validation for transaction data, and (4) metric manipulation through recall optimization at precision's expense. We present a case study showing how a minimal neural network architecture with data leakage outperforms many sophisticated methods reported in literature, achieving 99.9\\% recall despite fundamental evaluation flaws. These findings underscore that proper evaluation methodology matters more than model complexity in fraud detection research. The study serves as a cautionary example of how methodological rigor must precede architectural sophistication, with implications for improving research practices across machine learning applications."
      },
      {
        "id": "oai:arXiv.org:2506.02706v1",
        "title": "Collective Intelligence Outperforms Individual Talent: A Case Study in League of Legends",
        "link": "https://arxiv.org/abs/2506.02706",
        "author": "Angelo Josey Caldeira, Sajan Maharjan, Srijoni Majumdar, Evangelos Pournaras",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02706v1 Announce Type: new \nAbstract: Gaming environments are popular testbeds for studying human interactions and behaviors in complex artificial intelligence systems. Particularly, in multiplayer online battle arena (MOBA) games, individuals collaborate in virtual environments of high realism that involves real-time strategic decision-making and trade-offs on resource management, information collection and sharing, team synergy and collective dynamics. This paper explores whether collective intelligence, emerging from cooperative behaviours exhibited by a group of individuals, who are not necessarily skillful but effectively engage in collaborative problem-solving tasks, exceeds individual intelligence observed within skillful individuals. This is shown via a case study in League of Legends, using machine learning algorithms and statistical methods applied to large-scale data collected for the same purpose. By modelling systematically game-specific metrics but also new game-agnostic topological and graph spectra measures of cooperative interactions, we demonstrate compelling insights about the superior performance of collective intelligence."
      },
      {
        "id": "oai:arXiv.org:2506.02708v1",
        "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation",
        "link": "https://arxiv.org/abs/2506.02708",
        "author": "Naoto Tanji, Toshihiko Yamasaki",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02708v1 Announce Type: new \nAbstract: Image scoring is a crucial task in numerous real-world applications. To trust a model's judgment, understanding its rationale is essential. This paper proposes a novel training method for Vision Language Models (VLMs) to generate not only image scores but also corresponding justifications in natural language. Leveraging only an image scoring dataset and an instruction-tuned VLM, our method enables self-training, utilizing the VLM's generated text without relying on external data or models. In addition, we introduce a simple method for creating a dataset designed to improve alignment between predicted scores and their textual justifications. By iteratively training the model with Direct Preference Optimization on two distinct datasets and merging them, we can improve both scoring accuracy and the coherence of generated explanations."
      },
      {
        "id": "oai:arXiv.org:2506.02712v1",
        "title": "Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport",
        "link": "https://arxiv.org/abs/2506.02712",
        "author": "Jayadev Naram, Fredrik Hellstr\\\"om, Ziming Wang, Rebecka J\\\"ornsten, Giuseppe Durisi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02712v1 Announce Type: new \nAbstract: In many scenarios of practical interest, labeled data from a target distribution are scarce while labeled data from a related source distribution are abundant. One particular setting of interest arises when the target label space is a subset of the source label space, leading to the framework of partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a domain alignment term and a weighted empirical loss on the source data, with the aim of transferring knowledge between domains. However, a theoretical basis for this procedure is lacking, and in particular, most existing weighting schemes are heuristic. In this work, we derive generalization bounds for the PDA problem based on partial optimal transport. These bounds corroborate the use of the partial Wasserstein distance as a domain alignment term, and lead to theoretically motivated explicit expressions for the empirical source loss weights. Inspired by these bounds, we devise a practical algorithm for PDA, termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT is competitive with recent approaches, and that our proposed weights improve on existing schemes."
      },
      {
        "id": "oai:arXiv.org:2506.02718v1",
        "title": "Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems",
        "link": "https://arxiv.org/abs/2506.02718",
        "author": "Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02718v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, yet their deployment in real-world applications is hindered by fixed knowledge cutoffs and difficulties in generating controllable, accurate outputs in a single inference. Multi-agent systems (MAS) built from specialized LLM agents offer a promising solution, enabling dynamic collaboration and iterative reasoning. However, optimizing these systems remains a challenge, as conventional methods such as prompt engineering and supervised fine-tuning entail high engineering overhead and limited adaptability. Reinforcement learning (RL), particularly multi-agent reinforcement learning (MARL), provides a scalable framework by refining agent policies based on system-level feedback. Nevertheless, existing MARL algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on Critic networks, which can cause training instability and increase computational burden. To address these limitations and target the prototypical Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy updates by estimating relative reward advantages across heterogeneous groups of rollouts. MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead. Additionally, we introduce three group rollout sampling strategies that trade off between efficiency and effectiveness. Experiments on a multi-agent LLM-based search system demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, without requiring warm-up, underscoring its potential for stable and scalable optimization of complex LLM-based MAS."
      },
      {
        "id": "oai:arXiv.org:2506.02724v1",
        "title": "WeightLoRA: Keep Only Necessary Adapters",
        "link": "https://arxiv.org/abs/2506.02724",
        "author": "Andrey Veprikov, Vladimir Solodkin, Alexander Zyl, Andrey Savchenko, Aleksandr Beznosikov",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02724v1 Announce Type: new \nAbstract: The widespread utilization of language models in modern applications is inconceivable without Parameter-Efficient Fine-Tuning techniques, such as low-rank adaptation ($\\texttt{LoRA}$), which adds trainable adapters to selected layers. Although $\\texttt{LoRA}$ may obtain accurate solutions, it requires significant memory to train large models and intuition on which layers to add adapters. In this paper, we propose a novel method, $\\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the most critical $\\texttt{LoRA}$ heads throughout the optimization process. As a result, we can significantly reduce the number of trainable parameters while maintaining the capability to obtain consistent or even superior metric values. We conduct experiments for a series of competitive benchmarks and DeBERTa, BART, and Llama models, comparing our method with different adaptive approaches. The experimental results demonstrate the efficacy of $\\texttt{WeightLoRA}$ and the superior performance of $\\texttt{WeightLoRA+}$ in almost all cases."
      },
      {
        "id": "oai:arXiv.org:2506.02726v1",
        "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models",
        "link": "https://arxiv.org/abs/2506.02726",
        "author": "Qihang Yan, Xinyu Zhang, Luming Guo, Qi Zhang, Feifan Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02726v1 Announce Type: new \nAbstract: Large Language Models (LLMs) struggle with accuracy, domain-specific reasoning, and interpretability in vertical domains. Traditional preference alignment methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) often overlook the underlying knowledge sources and reasoning logic. This paper introduces RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel framework designed to address these limitations. RACE-Align systematically constructs a binary preference dataset incorporating external knowledge support and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO algorithm. The core innovation lies in its preference data construction strategy: it integrates AI-driven retrieval for factual grounding, enhancing knowledgeability and accuracy, and emphasizes the optimization of domain-specific CoT, treating the reasoning process itself as a key preference dimension. A multi-stage, AI-driven refinement pipeline cost-effectively generates these preference pairs. Experimental validation in Traditional Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that RACE-Align significantly outperforms the original base model and a model fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed across multiple dimensions, including answer accuracy, information richness, application of TCM thinking patterns, logicality and depth of reasoning, and interpretability. These findings suggest RACE-Align offers an effective pathway to enhance LLMs' knowledge application, reasoning reliability, and process transparency in complex vertical domains."
      },
      {
        "id": "oai:arXiv.org:2506.02733v1",
        "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering",
        "link": "https://arxiv.org/abs/2506.02733",
        "author": "Xiaoyi Feng, Kaifeng Zou, Caichun Cen, Tao Huang, Hui Guo, Zizhou Huang, Yingli Zhao, Mingqing Zhang, Diwei Wang, Yuntao Zou, Dagang Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02733v1 Announce Type: new \nAbstract: Existing optical flow datasets focus primarily on real-world simulation or synthetic human motion, but few are tailored to Celluloid(cel) anime character motion: a domain with unique visual and motion characteristics. To bridge this gap and facilitate research in optical flow estimation and downstream tasks such as anime video generation and line drawing colorization, we introduce LinkTo-Anime, the first high-quality dataset specifically designed for cel anime character motion generated with 3D model rendering. LinkTo-Anime provides rich annotations including forward and backward optical flow, occlusion masks, and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230 training frames, 720 validation frames, and 4,320 test frames. Furthermore, a comprehensive benchmark is constructed with various optical flow estimation methods to analyze the shortcomings and limitations across multiple datasets."
      },
      {
        "id": "oai:arXiv.org:2506.02736v1",
        "title": "GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal",
        "link": "https://arxiv.org/abs/2506.02736",
        "author": "Shufan Qing, Anzhen Li, Qiandi Wang, Yuefeng Niu, Mingchen Feng, Guoliang Hu, Jinqiao Wu, Fengtao Nan, Yingchun Fan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02736v1 Announce Type: new \nAbstract: Existing semantic SLAM in dynamic environments mainly identify dynamic regions through object detection or semantic segmentation methods. However, in certain highly dynamic scenarios, the detection boxes or segmentation masks cannot fully cover dynamic regions. Therefore, this paper proposes a robust and efficient GeneA-SLAM2 system that leverages depth variance constraints to handle dynamic scenes. Our method extracts dynamic pixels via depth variance and creates precise depth masks to guide the removal of dynamic objects. Simultaneously, an autoencoder is used to reconstruct keypoints, improving the genetic resampling keypoint algorithm to obtain more uniformly distributed keypoints and enhance the accuracy of pose estimation. Our system was evaluated on multiple highly dynamic sequences. The results demonstrate that GeneA-SLAM2 maintains high accuracy in dynamic scenes compared to current methods. Code is available at: https://github.com/qingshufan/GeneA-SLAM2."
      },
      {
        "id": "oai:arXiv.org:2506.02738v1",
        "title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning",
        "link": "https://arxiv.org/abs/2506.02738",
        "author": "Negin Baghbanzadeh, Sajad Ashkezari, Elham Dolatabadi, Arash Afkanpour",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02738v1 Announce Type: new \nAbstract: Compound figures, which are multi-panel composites containing diverse subfigures, are ubiquitous in biomedical literature, yet large-scale subfigure extraction remains largely unaddressed. Prior work on subfigure extraction has been limited in both dataset size and generalizability, leaving a critical open question: How does high-fidelity image-text alignment via large-scale subfigure extraction impact representation learning in vision-language models? We address this gap by introducing a scalable subfigure extraction pipeline based on transformer-based object detection, trained on a synthetic corpus of 500,000 compound figures, and achieving state-of-the-art performance on both ImageCLEF 2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a large-scale high quality biomedical vision-language dataset comprising 18 million clinically relevant subfigure-caption pairs spanning radiology, microscopy, and visible light photography. We train and evaluate vision-language models on our curated datasets and show improved performance across retrieval, zero-shot classification, and robustness benchmarks, outperforming existing baselines. We release our dataset, models, and code to support reproducible benchmarks and further study into biomedical vision-language modeling and representation learning."
      },
      {
        "id": "oai:arXiv.org:2506.02740v1",
        "title": "Stereotypical gender actions can be extracted from Web text",
        "link": "https://arxiv.org/abs/2506.02740",
        "author": "Ama\\c{c} Herda\\u{g}delen, Marco Baroni",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02740v1 Announce Type: new \nAbstract: We extracted gender-specific actions from text corpora and Twitter, and compared them to stereotypical expectations of people. We used Open Mind Common Sense (OMCS), a commonsense knowledge repository, to focus on actions that are pertinent to common sense and daily life of humans. We use the gender information of Twitter users and Web-corpus-based pronoun/name gender heuristics to compute the gender bias of the actions. With high recall, we obtained a Spearman correlation of 0.47 between corpus-based predictions and a human gold standard, and an area under the ROC curve of 0.76 when predicting the polarity of the gold standard. We conclude that it is feasible to use natural text (and a Twitter-derived corpus in particular) in order to augment commonsense repositories with the stereotypical gender expectations of actions. We also present a dataset of 441 commonsense actions with human judges' ratings on whether the action is typically/slightly masculine/feminine (or neutral), and another larger dataset of 21,442 actions automatically rated by the methods we investigate in this study."
      },
      {
        "id": "oai:arXiv.org:2506.02741v1",
        "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians",
        "link": "https://arxiv.org/abs/2506.02741",
        "author": "Pengchong Hu, Zhizhong Han",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02741v1 Announce Type: new \nAbstract: Jointly estimating camera poses and mapping scenes from RGBD images is a fundamental task in simultaneous localization and mapping (SLAM). State-of-the-art methods employ 3D Gaussians to represent a scene, and render these Gaussians through splatting for higher efficiency and better rendering. However, these methods cannot scale up to extremely large scenes, due to the inefficient tracking and mapping strategies that need to optimize all 3D Gaussians in the limited GPU memories throughout the training to maintain the geometry and color consistency to previous RGBD observations. To resolve this issue, we propose novel tracking and mapping strategies to work with a novel 3D representation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied 3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels, without needing to learn locations, rotations, and multi-dimensional variances. Tying Gaussians to views not only significantly saves storage but also allows us to employ many more Gaussians to represent local details in the limited GPU memory. Moreover, our strategies remove the need of maintaining all Gaussians learnable throughout the training, while improving rendering quality, and tracking accuracy. We justify the effectiveness of these designs, and report better performance over the latest methods on the widely used benchmarks in terms of rendering and tracking accuracy and scalability. Please see our project page for code and videos at https://machineperceptionlab.github.io/VTGaussian-SLAM-Project ."
      },
      {
        "id": "oai:arXiv.org:2506.02749v1",
        "title": "Knowledge Graph Completion by Intermediate Variables Regularization",
        "link": "https://arxiv.org/abs/2506.02749",
        "author": "Changyi Xiao, Yixin Cao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02749v1 Announce Type: new \nAbstract: Knowledge graph completion (KGC) can be framed as a 3-order binary tensor completion task. Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC. In this paper, we provide a summary of existing TDB models and derive a general form for them, serving as a foundation for further exploration of TDB models. Despite the expressiveness of TDB models, they are prone to overfitting. Existing regularization methods merely minimize the norms of embeddings to regularize the model, leading to suboptimal performance. Therefore, we propose a novel regularization method for TDB models that addresses this limitation. The regularization is applicable to most TDB models and ensures tractable computation. Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor. To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting. Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis. The code is available at https://github.com/changyi7231/IVR."
      },
      {
        "id": "oai:arXiv.org:2506.02751v1",
        "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS",
        "link": "https://arxiv.org/abs/2506.02751",
        "author": "Chuanyu Fu, Yuqi Zhang, Kunbin Yao, Guanying Chen, Yuan Xiong, Chuan Huang, Shuguang Cui, Xiaochun Cao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02751v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/."
      },
      {
        "id": "oai:arXiv.org:2506.02753v1",
        "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection",
        "link": "https://arxiv.org/abs/2506.02753",
        "author": "Aisha Alansari, Hamzah Luqman",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02753v1 Announce Type: new \nAbstract: The rapid growth of social media has amplified the spread of offensive, violent, and vulgar speech, which poses serious societal and cybersecurity concerns. Detecting such content in Arabic text is particularly complex due to limited labeled data, dialectal variations, and the language's inherent complexity. This paper proposes a novel framework that integrates multi-task learning (MTL) with active learning to enhance offensive speech detection in Arabic social media text. By jointly training on two auxiliary tasks, violent and vulgar speech, the model leverages shared representations to improve the detection accuracy of the offensive speech. Our approach dynamically adjusts task weights during training to balance the contribution of each task and optimize performance. To address the scarcity of labeled data, we employ an active learning strategy through several uncertainty sampling techniques to iteratively select the most informative samples for model training. We also introduce weighted emoji handling to better capture semantic cues. Experimental results on the OSACT2022 dataset show that the proposed framework achieves a state-of-the-art macro F1-score of 85.42%, outperforming existing methods while using significantly fewer fine-tuning samples. The findings of this study highlight the potential of integrating MTL with active learning for efficient and accurate offensive language detection in resource-constrained settings."
      },
      {
        "id": "oai:arXiv.org:2506.02757v1",
        "title": "Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection",
        "link": "https://arxiv.org/abs/2506.02757",
        "author": "Ruiying Lu, Jinhan Liu, Chuan Du, Dandan Guo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02757v1 Announce Type: new \nAbstract: Tabular anomaly detection, which aims at identifying deviant samples, has been crucial in a variety of real-world applications, such as medical disease identification, financial fraud detection, intrusion monitoring, etc. Although recent deep learning-based methods have achieved competitive performances, these methods suffer from representation entanglement and the lack of global correlation modeling, which hinders anomaly detection performance. To tackle the problem, we incorporate mask modeling and prototype learning into tabular anomaly detection. The core idea is to design learnable masks by disentangled representation learning within a projection space and extracting normal dependencies as explicit global prototypes. Specifically, the overall model involves two parts: (i) During encoding, we perform mask modeling in both the data space and projection space with orthogonal basis vectors for learning shared disentangled normal patterns; (ii) During decoding, we decode multiple masked representations in parallel for reconstruction and learn association prototypes to extract normal characteristic correlations. Our proposal derives from a distribution-matching perspective, where both projection space learning and association prototype learning are formulated as optimal transport problems, and the calibration distances are utilized to refine the anomaly scores. Quantitative and qualitative experiments on 20 tabular benchmarks demonstrate the effectiveness and interpretability of our model."
      },
      {
        "id": "oai:arXiv.org:2506.02758v1",
        "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs",
        "link": "https://arxiv.org/abs/2506.02758",
        "author": "Stefano Bann\\`o, Kate Knill, Mark Gales",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02758v1 Announce Type: new \nAbstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency. To date, its assessment by automated systems has typically examined the context-independent, or part-of-speech (PoS) related use of words. This paper introduces a novel approach to enable fine-grained vocabulary evaluation exploiting the precise use of words within a sentence. The scheme combines large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP is a standard lexical resource that enables in-context vocabulary use to be linked with proficiency level. We evaluate the ability of LLMs to assign proficiency levels to individual words as they appear in L2 learner writing, addressing key challenges such as polysemy, contextual variation, and multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to exploit additional semantic information that yields improved performance. We also explore correlations between word-level proficiency and essay-level proficiency. Finally, the approach is applied to examine the consistency of the EVP proficiency levels. Results show that LLMs are well-suited for the task of vocabulary assessment."
      },
      {
        "id": "oai:arXiv.org:2506.02764v1",
        "title": "Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations",
        "link": "https://arxiv.org/abs/2506.02764",
        "author": "Fatma Youssef Mohammed, Kostas Alexis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02764v1 Announce Type: new \nAbstract: Computational human attention modeling in free-viewing and task-specific settings is often studied separately, with limited exploration of whether a common representation exists between them. This work investigates this question and proposes a neural network architecture that builds upon the Human Attention transformer (HAT) to test the hypothesis. Our results demonstrate that free-viewing and visual search can efficiently share a common representation, allowing a model trained in free-viewing attention to transfer its knowledge to task-driven visual search with a performance drop of only 3.86% in the predicted fixation scanpaths, measured by the semantic sequence score (SemSS) metric which reflects the similarity between predicted and human scanpaths. This transfer reduces computational costs by 92.29% in terms of GFLOPs and 31.23% in terms of trainable parameters."
      },
      {
        "id": "oai:arXiv.org:2506.02765v1",
        "title": "A Dynamic Transformer Network for Vehicle Detection",
        "link": "https://arxiv.org/abs/2506.02765",
        "author": "Chunwei Tian, Kai Liu, Bob Zhang, Zhixiang Huang, Chia-Wen Lin, David Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02765v1 Announce Type: new \nAbstract: Stable consumer electronic systems can assist traffic better. Good traffic consumer electronic systems require collaborative work between traffic algorithms and hardware. However, performance of popular traffic algorithms containing vehicle detection methods based on deep networks via learning data relation rather than learning differences in different lighting and occlusions is limited. In this paper, we present a dynamic Transformer network for vehicle detection (DTNet). DTNet utilizes a dynamic convolution to guide a deep network to dynamically generate weights to enhance adaptability of an obtained detector. Taking into relations of different information account, a mixed attention mechanism based channel attention and Transformer is exploited to strengthen relations of channels and pixels to extract more salient information for vehicle detection. To overcome the drawback of difference in an image account, a translation-variant convolution relies on spatial location information to refine obtained structural information for vehicle detection. Experimental results illustrate that our DTNet is competitive for vehicle detection. Code of the proposed DTNet can be obtained at https://github.com/hellloxiaotian/DTNet."
      },
      {
        "id": "oai:arXiv.org:2506.02767v1",
        "title": "Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization",
        "link": "https://arxiv.org/abs/2506.02767",
        "author": "Marco Cal\\`i, Giulio Giacomuzzo, Ruggero Carli, Alberto Dalla Libera",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02767v1 Announce Type: new \nAbstract: This paper addresses the slow policy optimization convergence of Monte Carlo Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art model-based reinforcement learning (MBRL) algorithm, by integrating it with iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization method suitable for nonlinear systems. The proposed method, Exploration-Boosted MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory trajectories and initialize the policy, significantly reducing the number of required optimization steps. Experiments on the cart-pole task demonstrate that EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up to $\\bm{45.9\\%}$ reduction in execution time when both methods solve the task in four trials. EB-MC-PILCO also maintains a $\\bm{100\\%}$ success rate across trials while solving the task faster, even in cases where MC-PILCO converges in fewer iterations."
      },
      {
        "id": "oai:arXiv.org:2506.02781v1",
        "title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts",
        "link": "https://arxiv.org/abs/2506.02781",
        "author": "Tongyuan Bai, Wangyuanfan Bai, Dong Chen, Tieru Wu, Manyi Li, Rui Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02781v1 Announce Type: new \nAbstract: Controllability plays a crucial role in the practical applications of 3D indoor scene synthesis. Existing works either allow rough language-based control, that is convenient but lacks fine-grained scene customization, or employ graph based control, which offers better controllability but demands considerable knowledge for the cumbersome graph design process. To address these challenges, we present FreeScene, a user-friendly framework that enables both convenient and effective control for indoor scene synthesis.Specifically, FreeScene supports free-form user inputs including text description and/or reference images, allowing users to express versatile design intentions. The user inputs are adequately analyzed and integrated into a graph representation by a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion Transformer, which performs graph-aware denoising to enhance scene generation. Our MG-DiT not only excels at preserving graph structure but also offers broad applicability to various tasks, including, but not limited to, text-to-scene, graph-to-scene, and rearrangement, all within a single model. Extensive experiments demonstrate that FreeScene provides an efficient and user-friendly solution that unifies text-based and graph based scene synthesis, outperforming state-of-the-art methods in terms of both generation quality and controllability in a range of applications."
      },
      {
        "id": "oai:arXiv.org:2506.02783v1",
        "title": "SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model",
        "link": "https://arxiv.org/abs/2506.02783",
        "author": "Carlos Garcia-Lopez-de-Haro, Caterina Fuster-Barcelo, Curtis T. Rueden, Jonathan Heras, Vladimir Ulman, Daniel Franco-Barranco, Adrian Ines, Kevin W. Eliceiri, Jean-Christophe Olivo-Marin, Jean-Yves Tinevez, Daniel Sage, Arrate Munoz-Barrutia",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02783v1 Announce Type: new \nAbstract: Mask annotation remains a significant bottleneck in AI-driven biomedical image analysis due to its labor-intensive nature. To address this challenge, we introduce SAMJ, a user-friendly ImageJ/Fiji plugin leveraging the Segment Anything Model (SAM). SAMJ enables seamless, interactive annotations with one-click installation on standard computers. Designed for real-time object delineation in large scientific images, SAMJ is an easy-to-use solution that simplifies and accelerates the creation of labeled image datasets."
      },
      {
        "id": "oai:arXiv.org:2506.02789v1",
        "title": "Automated Measurement of Optic Nerve Sheath Diameter Using Ocular Ultrasound Video",
        "link": "https://arxiv.org/abs/2506.02789",
        "author": "Renxing Li, Weiyi Tang, Peiqi Li, Qiming Huang, Jiayuan She, Shengkai Li, Haoran Xu, Yeyun Wan, Jing Liu, Hailong Fu, Xiang Li, Jiangang Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02789v1 Announce Type: new \nAbstract: Objective. Elevated intracranial pressure (ICP) is recognized as a biomarker of secondary brain injury, with a significant linear correlation observed between optic nerve sheath diameter (ONSD) and ICP. Frequent monitoring of ONSD could effectively support dynamic evaluation of ICP. However, ONSD measurement is heavily reliant on the operator's experience and skill, particularly in manually selecting the optimal frame from ultrasound sequences and measuring ONSD. Approach. This paper presents a novel method to automatically identify the optimal frame from video sequences for ONSD measurement by employing the Kernel Correlation Filter (KCF) tracking algorithm and Simple Linear Iterative Clustering (SLIC) segmentation algorithm. The optic nerve sheath is mapped and measured using a Gaussian Mixture Model (GMM) combined with a KL-divergence-based method. Results. When compared with the average measurements of two expert clinicians, the proposed method achieved a mean error, mean squared deviation, and intraclass correlation coefficient (ICC) of 0.04, 0.054, and 0.782, respectively. Significance. The findings suggest that this method provides highly accurate automated ONSD measurements, showing potential for clinical application."
      },
      {
        "id": "oai:arXiv.org:2506.02803v1",
        "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking",
        "link": "https://arxiv.org/abs/2506.02803",
        "author": "Sifan Li, Yujun Cai, Yiwei Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02803v1 Announce Type: new \nAbstract: Vision-language models (VLMs) excel in semantic tasks but falter at a core human capability: detecting hidden content in optical illusions or AI-generated images through perceptual adjustments like zooming. We introduce HC-Bench, a benchmark of 112 images with hidden text, objects, and illusions, revealing that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to an overreliance on high-level semantics. Strikingly, we propose SemVink (Semantic Visual Thinking) by simply scaling images to low resolutions (32-128 pixels), which unlocks >99% accuracy by eliminating redundant visual noise. This exposes a critical architectural flaw: VLMs prioritize abstract reasoning over low-level visual operations crucial for real-world robustness. Our work urges a shift toward hybrid models integrating multi-scale processing, bridging the gap between computational vision and human cognition for applications in medical imaging, security, and beyond."
      },
      {
        "id": "oai:arXiv.org:2506.02811v1",
        "title": "CART-based Synthetic Tabular Data Generation for Imbalanced Regression",
        "link": "https://arxiv.org/abs/2506.02811",
        "author": "Ant\\'onio Pedro Pinheiro, Rita P. Ribeiro",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02811v1 Announce Type: new \nAbstract: Handling imbalanced target distributions in regression tasks remains a significant challenge in tabular data settings where underrepresented regions can hinder model performance. Among data-level solutions, some proposals, such as random sampling and SMOTE-based approaches, propose adapting classification techniques to regression tasks. However, these methods typically rely on crisp, artificial thresholds over the target variable, a limitation inherited from classification settings that can introduce arbitrariness, often leading to non-intuitive and potentially misleading problem formulations. While recent generative models, such as GANs and VAEs, provide flexible sample synthesis, they come with high computational costs and limited interpretability. In this study, we propose adapting an existing CART-based synthetic data generation method, tailoring it for imbalanced regression. The new method integrates relevance and density-based mechanisms to guide sampling in sparse regions of the target space and employs a threshold-free, feature-driven generation process. Our experimental study focuses on the prediction of extreme target values across benchmark datasets. The results indicate that the proposed method is competitive with other resampling and generative strategies in terms of performance, while offering faster execution and greater transparency. These results highlight the method's potential as a transparent, scalable data-level strategy for improving regression models in imbalanced domains."
      },
      {
        "id": "oai:arXiv.org:2506.02818v1",
        "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations",
        "link": "https://arxiv.org/abs/2506.02818",
        "author": "Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02818v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate impressive results in natural language processing tasks but require a significant amount of computational and memory resources. Structured matrix representations are a promising way for reducing the number of parameters of these models. However, it seems unrealistic to expect that weight matrices of pretrained models can be accurately represented by structured matrices without any fine-tuning. To overcome this issue, we utilize the fact that LLM output is invariant under certain orthogonal transformations of weight matrices. This insight can be leveraged to identify transformations that significantly improve the compressibility of weights within structured classes. The proposed approach is applicable to various types of structured matrices that support efficient projection operations. Code is available at https://github.com/GrishKate/ProcrustesGPT"
      },
      {
        "id": "oai:arXiv.org:2506.02827v1",
        "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference",
        "link": "https://arxiv.org/abs/2506.02827",
        "author": "Yulin Dou, Jiangming Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02827v1 Announce Type: new \nAbstract: Large language models (LLMs) can effectively elicit human preferences through multi-turn dialogue. Complex tasks can be accomplished through iterative clarifying questions and final responses generated by an LLM acting as a questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches based on self-taught reasoning struggle to identify optimal dialogue trajectories and avoid irrelevant questions to the tasks. To address this limitation, we propose TO-GATE, a novel framework that enhances question generation through trajectory optimization, which consists of two key components: a clarification resolver that generates optimal questioning trajectories, and a summarizer that ensures task-aligned final responses. The trajectory optimization enables the model to produce effective elicitation questions and summary responses tailored to specific tasks. Experimental results demonstrate that TO-GATE significantly outperforms baseline methods, achieving a 9.32% improvement on standard preference elicitation tasks."
      },
      {
        "id": "oai:arXiv.org:2506.02842v1",
        "title": "Sheaves Reloaded: A Directional Awakening",
        "link": "https://arxiv.org/abs/2506.02842",
        "author": "Stefano Fiorini, Hakan Aktas, Iulia Duta, Stefano Coniglio, Pietro Morerio, Alessio Del Bue, Pietro Li\\`o",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02842v1 Announce Type: new \nAbstract: Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph Neural Networks (GNNs) that significantly improve our ability to model complex relational data. While directionality has been shown to substantially boost performance in graph learning tasks and is key to many real-world applications, existing SNNs fall short in representing it. To address this limitation, we introduce the Directed Cellular Sheaf, a special type of cellular sheaf designed to explicitly account for edge orientation. Building on this structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which captures both the graph's topology and its directional information. This operator serves as the backbone of the Directed Sheaf Neural Network (DSNN), the first SNN model to embed a directional bias into its architecture. Extensive experiments on nine real-world benchmarks show that DSNN consistently outperforms baseline methods."
      },
      {
        "id": "oai:arXiv.org:2506.02843v1",
        "title": "Random Registers for Cross-Domain Few-Shot Learning",
        "link": "https://arxiv.org/abs/2506.02843",
        "author": "Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02843v1 Announce Type: new \nAbstract: Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a data-sufficient source domain to data-scarce target domains. Although Vision Transformer (ViT) has shown superior capability in many vision tasks, its transferability against huge domain gaps in CDFSL is still under-explored. In this paper, we find an intriguing phenomenon: during the source-domain training, prompt tuning, as a common way to train ViT, could be harmful for the generalization of ViT in target domains, but setting them to random noises (i.e., random registers) could consistently improve target-domain performance. We then delve into this phenomenon for an interpretation. We find that learnable prompts capture domain information during the training on the source dataset, which views irrelevant visual patterns as vital cues for recognition. This can be viewed as a kind of overfitting and increases the sharpness of the loss landscapes. In contrast, random registers are essentially a novel way of perturbing attention for the sharpness-aware minimization, which helps the model find a flattened minimum in loss landscapes, increasing the transferability. Based on this phenomenon and interpretation, we further propose a simple but effective approach for CDFSL to enhance the perturbation on attention maps by adding random registers on the semantic regions of image tokens, improving the effectiveness and efficiency of random registers. Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance. Codes and models are available at https://github.com/shuaiyi308/REAP."
      },
      {
        "id": "oai:arXiv.org:2506.02845v1",
        "title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments",
        "link": "https://arxiv.org/abs/2506.02845",
        "author": "Di Wen, Lei Qi, Kunyu Peng, Kailun Yang, Fei Teng, Ao Luo, Jia Fu, Yufan Chen, Ruiping Liu, Yitian Shi, M. Saquib Sarfraz, Rainer Stiefelhagen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02845v1 Announce Type: new \nAbstract: Despite substantial progress in video understanding, most existing datasets are limited to Earth's gravitational conditions. However, microgravity alters human motion, interactions, and visual semantics, revealing a critical gap for real-world vision systems. This presents a challenge for domain-robust video understanding in safety-critical space applications. To address this, we introduce MicroG-4M, the first benchmark for spatio-temporal and semantic understanding of human activities in microgravity. Constructed from real-world space missions and cinematic simulations, the dataset includes 4,759 clips covering 50 actions, 1,238 context-rich captions, and over 7,000 question-answer pairs on astronaut activities and scene understanding. MicroG-4M supports three core tasks: fine-grained multi-label action recognition, temporal video captioning, and visual question answering, enabling a comprehensive evaluation of both spatial localization and semantic reasoning in microgravity contexts. We establish baselines using state-of-the-art models. All data, annotations, and code are available at https://github.com/LEI-QI-233/HAR-in-Space."
      },
      {
        "id": "oai:arXiv.org:2506.02846v1",
        "title": "PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors",
        "link": "https://arxiv.org/abs/2506.02846",
        "author": "Yujin Chen, Yinyu Nie, Benjamin Ummenhofer, Reiner Birkl, Michael Paulitsch, Matthias Nie{\\ss}ner",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02846v1 Announce Type: new \nAbstract: We present PBR-SR, a novel method for physically based rendering (PBR) texture super resolution (SR). It outputs high-resolution, high-quality PBR textures from low-resolution (LR) PBR input in a zero-shot manner. PBR-SR leverages an off-the-shelf super-resolution model trained on natural images, and iteratively minimizes the deviations between super-resolution priors and differentiable renderings. These enhancements are then back-projected into the PBR map space in a differentiable manner to produce refined, high-resolution textures. To mitigate view inconsistencies and lighting sensitivity, which is common in view-based super-resolution, our method applies 2D prior constraints across multi-view renderings, iteratively refining the shared, upscaled textures. In parallel, we incorporate identity constraints directly in the PBR texture domain to ensure the upscaled textures remain faithful to the LR input. PBR-SR operates without any additional training or data requirements, relying entirely on pretrained image priors. We demonstrate that our approach produces high-fidelity PBR textures for both artist-designed and AI-generated meshes, outperforming both direct SR models application and prior texture optimization methods. Our results show high-quality outputs in both PBR and rendering evaluations, supporting advanced applications such as relighting."
      },
      {
        "id": "oai:arXiv.org:2506.02850v1",
        "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding",
        "link": "https://arxiv.org/abs/2506.02850",
        "author": "Mengyue Wang, Shuo Chen, Kristian Kersting, Volker Tresp, Yunpu Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02850v1 Announce Type: new \nAbstract: Recent advances in Video Large Language Models (VLLMs) have significantly enhanced their ability to understand video content. Nonetheless, processing long videos remains challenging due to high computational demands and the redundancy present in the visual data. In this work, we propose METok, a training-free, Multi-stage Event-based Token compression framework designed to accelerate VLLMs' inference while preserving accuracy. METok progressively eliminates redundant visual tokens across three critical stages: (1) event-aware compression during vision encoding, (2) hierarchical token pruning in the prefilling stage based on semantic alignment and event importance, and (3) a decoding-stage KV Cache optimization that further reduces memory consumption. Our experiments on diverse video benchmarks demonstrate that METok achieves an optimal trade-off between efficiency and accuracy by dynamically selecting informative visual tokens. For instance, equipping LongVA-7B with METok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all while maintaining comparable or even superior accuracy."
      },
      {
        "id": "oai:arXiv.org:2506.02853v1",
        "title": "Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation",
        "link": "https://arxiv.org/abs/2506.02853",
        "author": "Mingjie Wei, Xuemei Xie, Yutong Zhong, Guangming Shi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02853v1 Announce Type: new \nAbstract: Action coordination in human structure is indispensable for the spatial constraints of 2D joints to recover 3D pose. Usually, action coordination is represented as a long-range dependence among body parts. However, there are two main challenges in modeling long-range dependencies. First, joints should not only be constrained by other individual joints but also be modulated by the body parts. Second, existing methods make networks deeper to learn dependencies between non-linked parts. They introduce uncorrelated noise and increase the model size. In this paper, we utilize a pyramid structure to better learn potential long-range dependencies. It can capture the correlation across joints and groups, which complements the context of the human sub-structure. In an effective cross-scale way, it captures the pyramid-structured long-range dependence. Specifically, we propose a novel Pyramid Graph Attention (PGA) module to capture long-range cross-scale dependencies. It concatenates information from various scales into a compact sequence, and then computes the correlation between scales in parallel. Combining PGA with graph convolution modules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose estimation, which is a lightweight multi-scale transformer architecture. It encapsulates human sub-structures into self-attention by pooling. Extensive experiments show that our approach achieves lower error and smaller model size than state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code is available at https://github.com/MingjieWe/PGFormer."
      },
      {
        "id": "oai:arXiv.org:2506.02854v1",
        "title": "Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image Segmentation Framework",
        "link": "https://arxiv.org/abs/2506.02854",
        "author": "Mengmeng Zhang, Xingyuan Dai, Yicheng Sun, Jing Wang, Yueyang Yao, Xiaoyan Gong, Fuze Cong, Feiyue Wang, Yisheng Lv",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02854v1 Announce Type: new \nAbstract: Although the Segment Anything Model (SAM) is highly effective in natural image segmentation, it requires dependencies on prompts, which limits its applicability to medical imaging where manual prompts are often unavailable. Existing efforts to fine-tune SAM for medical segmentation typically struggle to remove this dependency. We propose Hierarchical Self-Prompting SAM (HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong performance in prompt-free medical image segmentation. Unlike previous self-prompting methods that remain limited to positional prompts similar to vanilla SAM, we are the first to introduce learning abstract prompts during the self-prompting process. This simple and intuitive self-prompting framework achieves superior performance on classic segmentation tasks such as polyp and skin lesion segmentation, while maintaining robustness across diverse medical imaging modalities. Furthermore, it exhibits strong generalization to unseen datasets, achieving improvements of up to 14.04% over previous state-of-the-art methods on some challenging benchmarks. These results suggest that abstract prompts encapsulate richer and higher-dimensional semantic information compared to positional prompts, thereby enhancing the model's robustness and generalization performance. All models and codes will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2506.02857v1",
        "title": "Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection",
        "link": "https://arxiv.org/abs/2506.02857",
        "author": "Luca Maiano, Fabrizio Casadei, Irene Amerini",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02857v1 Announce Type: new \nAbstract: Detecting deepfakes has become a critical challenge in Computer Vision and Artificial Intelligence. Despite significant progress in detection techniques, generalizing them to open-set scenarios continues to be a persistent difficulty. Neural networks are often trained on the closed-world assumption, but with new generative models constantly evolving, it is inevitable to encounter data generated by models that are not part of the training distribution. To address these challenges, in this paper, we propose two novel Out-Of-Distribution (OOD) detection approaches. The first approach is trained to reconstruct the input image, while the second incorporates an attention mechanism for detecting OODs. Our experiments validate the effectiveness of the proposed approaches compared to existing state-of-the-art techniques. Our method achieves promising results in deepfake detection and ranks among the top-performing configurations on the benchmark, demonstrating their potential for robust, adaptable solutions in dynamic, real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.02864v1",
        "title": "BNPO: Beta Normalization Policy Optimization",
        "link": "https://arxiv.org/abs/2506.02864",
        "author": "Changyi Xiao, Mengdi Zhang, Yixin Cao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02864v1 Announce Type: new \nAbstract: Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that reinforcement learning with rule-based, binary-valued reward functions can significantly enhance the reasoning capabilities of large language models. These models primarily utilize REINFORCE-based policy optimization techniques, such as REINFORCE with baseline and group relative policy optimization (GRPO). However, a key limitation remains: current policy optimization methods either neglect reward normalization or employ static normalization strategies, which fail to adapt to the dynamic nature of policy updates during training. This may result in unstable gradient estimates and hinder training stability. To address this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel policy optimization method that adaptively normalizes rewards using a Beta distribution with dynamically updated parameters. BNPO aligns the normalization with the changing policy distribution, enabling more precise and lower-variance gradient estimation, which in turn promotes stable training dynamics. We provide theoretical analysis demonstrating BNPO's variance-reducing properties and show that it generalizes both REINFORCE and GRPO under binary-valued reward settings. Furthermore, we introduce an advantage decomposition mechanism to extend BNPO's applicability to more complex reward systems. Experimental results confirm that BNPO achieves state-of-the-art performance among policy optimization methods on reasoning tasks. The code is available at https://github.com/changyi7231/BNPO."
      },
      {
        "id": "oai:arXiv.org:2506.02866v1",
        "title": "MVTD: A Benchmark Dataset for Maritime Visual Object Tracking",
        "link": "https://arxiv.org/abs/2506.02866",
        "author": "Ahsan Baidar Bakht, Muhayy Ud Din, Sajid Javed, Irfan Hussain",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02866v1 Announce Type: new \nAbstract: Visual Object Tracking (VOT) is a fundamental task with widespread applications in autonomous navigation, surveillance, and maritime robotics. Despite significant advances in generic object tracking, maritime environments continue to present unique challenges, including specular water reflections, low-contrast targets, dynamically changing backgrounds, and frequent occlusions. These complexities significantly degrade the performance of state-of-the-art tracking algorithms, highlighting the need for domain-specific datasets. To address this gap, we introduce the Maritime Visual Tracking Dataset (MVTD), a comprehensive and publicly available benchmark specifically designed for maritime VOT. MVTD comprises 182 high-resolution video sequences, totaling approximately 150,000 frames, and includes four representative object classes: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset captures a diverse range of operational conditions and maritime scenarios, reflecting the real-world complexities of maritime environments. We evaluated 14 recent SOTA tracking algorithms on the MVTD benchmark and observed substantial performance degradation compared to their performance on general-purpose datasets. However, when fine-tuned on MVTD, these models demonstrate significant performance gains, underscoring the effectiveness of domain adaptation and the importance of transfer learning in specialized tracking contexts. The MVTD dataset fills a critical gap in the visual tracking community by providing a realistic and challenging benchmark for maritime scenarios. Dataset and Source Code can be accessed here \"https://github.com/AhsanBaidar/MVTD\"."
      },
      {
        "id": "oai:arXiv.org:2506.02868v1",
        "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings",
        "link": "https://arxiv.org/abs/2506.02868",
        "author": "Amal S. Perera, David Fernandez, Chandi Witharana, Elias Manos, Michael Pimenta, Anna K. Liljedahl, Ingmar Nitze, Yili Yang, Todd Nicholson, Chia-Yu Hsu, Wenwen Li, Guido Grosse",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02868v1 Announce Type: new \nAbstract: Accurate mapping of permafrost landforms, thaw disturbances, and human-built infrastructure at pan-Arctic scale using sub-meter satellite imagery is increasingly critical. Handling petabyte-scale image data requires high-performance computing and robust feature detection models. While convolutional neural network (CNN)-based deep learning approaches are widely used for remote sensing (RS),similar to the success in transformer based large language models, Vision Transformers (ViTs) offer advantages in capturing long-range dependencies and global context via attention mechanisms. ViTs support pretraining via self-supervised learning-addressing the common limitation of labeled data in Arctic feature detection and outperform CNNs on benchmark datasets. Arctic also poses challenges for model generalization, especially when features with the same semantic class exhibit diverse spectral characteristics. To address these issues for Arctic feature detection, we integrate geospatial location embeddings into ViTs to improve adaptation across regions. This work investigates: (1) the suitability of pre-trained ViTs as feature extractors for high-resolution Arctic remote sensing tasks, and (2) the benefit of combining image and location embeddings. Using previously published datasets for Arctic feature detection, we evaluate our models on three tasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and human-built infrastructure. We empirically explore multiple configurations to fuse image embeddings and location embeddings. Results show that ViTs with location embeddings outperform prior CNN-based models on two of the three tasks including F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating the potential of transformer-based models with spatial awareness for Arctic RS applications."
      },
      {
        "id": "oai:arXiv.org:2506.02872v1",
        "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias",
        "link": "https://arxiv.org/abs/2506.02872",
        "author": "Ludovic Moncla, H\\'edi Zeghidi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02872v1 Announce Type: new \nAbstract: Named Entity Recognition (NER) in historical texts presents unique challenges due to non-standardized language, archaic orthography, and nested or overlapping entities. This study benchmarks a diverse set of NER approaches, ranging from classical Conditional Random Fields (CRFs) and spaCy-based models to transformer-based architectures such as CamemBERT and sequence-labeling models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly annotated corpus derived from 18th-century French encyclopedias. We propose framing NER as both token-level and span-level classification to accommodate complex nested entity structures typical of historical documents. Additionally, we evaluate the emerging potential of few-shot prompting with generative language models for low-resource scenarios. Our results demonstrate that while transformer-based models achieve state-of-the-art performance, especially on nested entities, generative models offer promising alternatives when labeled data are scarce. The study highlights ongoing challenges in historical NER and suggests avenues for hybrid approaches combining symbolic and neural methods to better capture the intricacies of early modern French text."
      },
      {
        "id": "oai:arXiv.org:2506.02875v1",
        "title": "NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results",
        "link": "https://arxiv.org/abs/2506.02875",
        "author": "Xiaohong Liu, Xiongkuo Min, Qiang Hu, Xiaoyun Zhang, Jie Guo, Guangtao Zhai, Shushi Wang, Yingjie Zhou, Lu Liu, Jingxin Li, Liu Yang, Farong Wen, Li Xu, Yanwei Jiang, Xilei Zhu, Chunyi Li, Zicheng Zhang, Huiyu Duan, Xiele Wu, Yixuan Gao, Yuqin Cao, Jun Jia, Wei Sun, Jiezhang Cao, Radu Timofte, Baojun Li, Jiamian Huang, Dan Luo, Tao Liu, Weixia Zhang, Bingkun Zheng, Junlin Chen, Ruikai Zhou, Meiya Chen, Yu Wang, Hao Jiang, Xiantao Li, Yuxiang Jiang, Jun Tang, Yimeng Zhao, Bo Hu, Zelu Qi, Chaoyang Zhang, Fei Zhao, Ping Shi, Lingzhi Fu, Heng Cong, Shuai He, Rongyu Zhang, Jiarong He, Zongyao Hu, Wei Luo, Zihao Yu, Fengbin Guan, Yiting Lu, Xin Li, Zhibo Chen, Mengjing Su, Yi Wang, Tuo Chen, Chunxiao Li, Shuaiyu Zhao, Jiaxin Wen, Chuyi Lin, Sitong Liu, Ningxin Chu, Jing Wan, Yu Zhou, Baoying Chen, Jishen Zeng, Jiarui Liu, Xianjin Liu, Xin Chen, Lanzhi Zhou, Hangyu Li, You Han, Bibo Xiang, Zhenjie Liu, Jianzhang Lu, Jialin Gui, Renjie Lu, Shangfei Wang, Donghao Zhou, Jingyu Lin, Quanjian Song, Jiancheng Huang, Yufeng Yang, Changwei Wang, Shupeng Zhong, Yang Yang, Lihuo He, Jia Liu, Yuting Xing, Tida Fang, Yuchun Jin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02875v1 Announce Type: new \nAbstract: This paper reports on the NTIRE 2025 XGC Quality Assessment Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025. This challenge is to address a major challenge in the field of video and talking head processing. The challenge is divided into three tracks, including user generated video, AI generated video and talking head. The user-generated video track uses the FineVD-GC, which contains 6,284 user generated videos. The user-generated video track has a total of 125 registered participants. A total of 242 submissions are received in the development phase, and 136 submissions are received in the test phase. Finally, 5 participating teams submitted their models and fact sheets. The AI generated video track uses the Q-Eval-Video, which contains 34,029 AI-Generated Videos (AIGVs) generated by 11 popular Text-to-Video (T2V) models. A total of 133 participants have registered in this track. A total of 396 submissions are received in the development phase, and 226 submissions are received in the test phase. Finally, 6 participating teams submitted their models and fact sheets. The talking head track uses the THQA-NTIRE, which contains 12,247 2D and 3D talking heads. A total of 89 participants have registered in this track. A total of 225 submissions are received in the development phase, and 118 submissions are received in the test phase. Finally, 8 participating teams submitted their models and fact sheets. Each participating team in every track has proposed a method that outperforms the baseline, which has contributed to the development of fields in three tracks."
      },
      {
        "id": "oai:arXiv.org:2506.02878v1",
        "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective",
        "link": "https://arxiv.org/abs/2506.02878",
        "author": "Jintian Shao, Yiming Cheng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02878v1 Announce Type: new \nAbstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes. Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes."
      },
      {
        "id": "oai:arXiv.org:2506.02882v1",
        "title": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation",
        "link": "https://arxiv.org/abs/2506.02882",
        "author": "Sohyun Lee, Yeho Kwon, Lukas Hoyer, Suha Kwak",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02882v1 Announce Type: new \nAbstract: Improving robustness of the Segment Anything Model (SAM) to input degradations is critical for its deployment in high-stakes applications such as autonomous driving and robotics. Our approach to this challenge prioritizes three key aspects: first, parameter efficiency to maintain the inherent generalization capability of SAM; second, fine-grained and input-aware robustification to precisely address the input corruption; and third, adherence to standard training protocols for ease of training. To this end, we propose gated-rank adaptation (GaRA). GaRA introduces lightweight adapters into intermediate layers of the frozen SAM, where each adapter dynamically adjusts the effective rank of its weight matrix based on the input by selectively activating (rank-1) components of the matrix using a learned gating module. This adjustment enables fine-grained and input-aware robustification without compromising the generalization capability of SAM. Our model, GaRA-SAM, significantly outperforms prior work on all robust segmentation benchmarks. In particular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC, a challenging real corrupted image dataset."
      },
      {
        "id": "oai:arXiv.org:2506.02883v1",
        "title": "A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks",
        "link": "https://arxiv.org/abs/2506.02883",
        "author": "Anthony Kobanda, Odalric-Ambrym Maillard, R\\'emy Portelas",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02883v1 Announce Type: new \nAbstract: Autonomous agents operating in domains such as robotics or video game simulations must adapt to changing tasks without forgetting about the previous ones. This process called Continual Reinforcement Learning poses non-trivial difficulties, from preventing catastrophic forgetting to ensuring the scalability of the approaches considered. Building on recent advances, we introduce a benchmark providing a suite of video-game navigation scenarios, thus filling a gap in the literature and capturing key challenges : catastrophic forgetting, task adaptation, and memory efficiency. We define a set of various tasks and datasets, evaluation protocols, and metrics to assess the performance of algorithms, including state-of-the-art baselines. Our benchmark is designed not only to foster reproducible research and to accelerate progress in continual reinforcement learning for gaming, but also to provide a reproducible framework for production pipelines -- helping practitioners to identify and to apply effective approaches."
      },
      {
        "id": "oai:arXiv.org:2506.02887v1",
        "title": "Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review",
        "link": "https://arxiv.org/abs/2506.02887",
        "author": "Mrinmay Sen, Shruti Aparna, Rohit Agarwal, Chalavadi Krishna Mohan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02887v1 Announce Type: new \nAbstract: Federated Learning (FL) is a learning mechanism that falls under the distributed training umbrella, which collaboratively trains a shared global model without disclosing the raw data from different clients. This paper presents an extensive survey on the impact of partial client participation in federated learning. While much of the existing research focuses on addressing issues such as generalization, robustness, and fairness caused by data heterogeneity under the assumption of full client participation, limited attention has been given to the practical and theoretical challenges arising from partial client participation, which is common in real-world scenarios. This survey provides an in-depth review of existing FL methods designed to cope with partial client participation. We offer a comprehensive analysis supported by theoretical insights and empirical findings, along with a structured categorization of these methods, highlighting their respective advantages and disadvantages."
      },
      {
        "id": "oai:arXiv.org:2506.02890v1",
        "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights",
        "link": "https://arxiv.org/abs/2506.02890",
        "author": "Jakub Krajewski, Marcin Chochowski, Daniel Korzekwa",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02890v1 Announce Type: new \nAbstract: Mixture of Experts (MoE) architectures have emerged as pivotal for scaling Large Language Models (LLMs) efficiently. Fine-grained MoE approaches - utilizing more numerous, smaller experts - have demonstrated potential in improving model convergence and quality. This work proposes a set of training recipes and provides a comprehensive empirical evaluation of fine-grained MoE, directly comparing its scaling properties against standard MoE configurations for models with up to 56B total (17B active) parameters. We investigate convergence speed, model performance on downstream benchmarks, and practical training considerations across various setups. Overall, at the largest scale we show that fine-grained MoE achieves better validation loss and higher accuracy across a set of downstream benchmarks. This study offers empirical grounding and practical insights for leveraging fine-grained MoE in the development of future large-scale models."
      },
      {
        "id": "oai:arXiv.org:2506.02891v1",
        "title": "OpenFace 3.0: A Lightweight Multitask System for Comprehensive Facial Behavior Analysis",
        "link": "https://arxiv.org/abs/2506.02891",
        "author": "Jiewen Hu, Leena Mathur, Paul Pu Liang, Louis-Philippe Morency",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02891v1 Announce Type: new \nAbstract: In recent years, there has been increasing interest in automatic facial behavior analysis systems from computing communities such as vision, multimodal interaction, robotics, and affective computing. Building upon the widespread utility of prior open-source facial analysis systems, we introduce OpenFace 3.0, an open-source toolkit capable of facial landmark detection, facial action unit detection, eye-gaze estimation, and facial emotion recognition. OpenFace 3.0 contributes a lightweight unified model for facial analysis, trained with a multi-task architecture across diverse populations, head poses, lighting conditions, video resolutions, and facial analysis tasks. By leveraging the benefits of parameter sharing through a unified model and training paradigm, OpenFace 3.0 exhibits improvements in prediction performance, inference speed, and memory efficiency over similar toolkits and rivals state-of-the-art models. OpenFace 3.0 can be installed and run with a single line of code and operate in real-time without specialized hardware. OpenFace 3.0 code for training models and running the system is freely available for research purposes and supports contributions from the community."
      },
      {
        "id": "oai:arXiv.org:2506.02893v1",
        "title": "Dense Match Summarization for Faster Two-view Estimation",
        "link": "https://arxiv.org/abs/2506.02893",
        "author": "Jonathan Astermark, Anders Heyden, Viktor Larsson",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02893v1 Announce Type: new \nAbstract: In this paper, we speed up robust two-view relative pose from dense correspondences. Previous work has shown that dense matchers can significantly improve both accuracy and robustness in the resulting pose. However, the large number of matches comes with a significantly increased runtime during robust estimation in RANSAC. To avoid this, we propose an efficient match summarization scheme which provides comparable accuracy to using the full set of dense matches, while having 10-100x faster runtime. We validate our approach on standard benchmark datasets together with multiple state-of-the-art dense matchers."
      },
      {
        "id": "oai:arXiv.org:2506.02894v1",
        "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation",
        "link": "https://arxiv.org/abs/2506.02894",
        "author": "Verena Blaschke, Miriam Winkler, Constantin F\\\"orster, Gabriele Wenger-Glemser, Barbara Plank",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02894v1 Announce Type: new \nAbstract: Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, an evaluation dataset containing four hours of read speech in three dialect groups spoken in Southeast Germany (Franconian, Bavarian, Alemannic), and half an hour of Standard German speech. We provide both dialectal and Standard German transcriptions, and analyze the linguistic differences between them. We benchmark several multilingual state-of-the-art ASR models on speech translation into Standard German, and find differences between how much the output resembles the dialectal vs. standardized transcriptions. Qualitative error analyses of the best ASR model reveal that it sometimes normalizes grammatical differences, but often stays closer to the dialectal constructions."
      },
      {
        "id": "oai:arXiv.org:2506.02896v1",
        "title": "FlySearch: Exploring how vision-language models explore",
        "link": "https://arxiv.org/abs/2506.02896",
        "author": "Adam Pardyl, Dominik Matuszek, Mateusz Przebieracz, Marek Cygan, Bartosz Zieli\\'nski, Maciej Wo{\\l}czyk",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02896v1 Announce Type: new \nAbstract: The real world is messy and unstructured. Uncovering critical information often requires active, goal-driven exploration. It remains to be seen whether Vision-Language Models (VLMs), which recently emerged as a popular zero-shot tool in many difficult tasks, can operate effectively in such conditions. In this paper, we answer this question by introducing FlySearch, a 3D, outdoor, photorealistic environment for searching and navigating to objects in complex scenes. We define three sets of scenarios with varying difficulty and observe that state-of-the-art VLMs cannot reliably solve even the simplest exploration tasks, with the gap to human performance increasing as the tasks get harder. We identify a set of central causes, ranging from vision hallucination, through context misunderstanding, to task planning failures, and we show that some of them can be addressed by finetuning. We publicly release the benchmark, scenarios, and the underlying codebase."
      },
      {
        "id": "oai:arXiv.org:2506.02897v1",
        "title": "Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning",
        "link": "https://arxiv.org/abs/2506.02897",
        "author": "Alessandro Licciardi, Roberta Raineri, Anton Proskurnikov, Lamberto Rondoni, Lorenzo Zino",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02897v1 Announce Type: new \nAbstract: Federated Learning (FL) enables privacy-preserving collaborative model training, yet its practical strength is often undermined by client data heterogeneity, which severely degrades model performance. This paper proposes that data heterogeneity across clients' distributions can be effectively addressed by adopting an approach inspired by opinion dynamics over temporal social networks. We introduce \\shortname (Federated Coalition Variance Reduction with Boltzmann Exploration), a variance-reducing selection algorithm in which (1) clients dynamically organize into non-overlapping clusters based on asymptotic agreements, and (2) from each cluster, one client is selected to minimize the expected variance of its model update. Our experiments show that in heterogeneous scenarios our algorithm outperforms existing FL algorithms, yielding more accurate results and faster convergence, validating the efficacy of our approach."
      },
      {
        "id": "oai:arXiv.org:2506.02899v1",
        "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator",
        "link": "https://arxiv.org/abs/2506.02899",
        "author": "Yusuke Sakai, Takumi Goto, Taro Watanabe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02899v1 Announce Type: new \nAbstract: We propose IMPARA-GED, a novel reference-free automatic grammatical error correction (GEC) evaluation method with grammatical error detection (GED) capabilities. We focus on the quality estimator of IMPARA, an existing automatic GEC evaluation method, and construct that of IMPARA-GED using a pre-trained language model with enhanced GED capabilities. Experimental results on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods, demonstrate that IMPARA-GED achieves the highest correlation with human sentence-level evaluations."
      },
      {
        "id": "oai:arXiv.org:2506.02911v1",
        "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.02911",
        "author": "Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, Zhiyong Lu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02911v1 Announce Type: new \nAbstract: Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1."
      },
      {
        "id": "oai:arXiv.org:2506.02914v1",
        "title": "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection",
        "link": "https://arxiv.org/abs/2506.02914",
        "author": "Yechi Ma, Wei Hua, Shu Kong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02914v1 Announce Type: new \nAbstract: A crucial yet under-appreciated prerequisite in machine learning solutions for real-applications is data annotation: human annotators are hired to manually label data according to detailed, expert-crafted guidelines. This is often a laborious, tedious, and costly process. To study methods for facilitating data annotation, we introduce a new benchmark AnnoGuide: Auto-Annotation from Annotation Guidelines. It aims to evaluate automated methods for data annotation directly from expert-defined annotation guidelines, eliminating the need for manual labeling. As a case study, we repurpose the well-established nuScenes dataset, commonly used in autonomous driving research, which provides comprehensive annotation guidelines for labeling LiDAR point clouds with 3D cuboids across 18 object classes. These guidelines include a few visual examples and textual descriptions, but no labeled 3D cuboids in LiDAR data, making this a novel task of multi-modal few-shot 3D detection without 3D annotations. The advances of powerful foundation models (FMs) make AnnoGuide especially timely, as FMs offer promising tools to tackle its challenges. We employ a conceptually straightforward pipeline that (1) utilizes open-source FMs for object detection and segmentation in RGB images, (2) projects 2D detections into 3D using known camera poses, and (3) clusters LiDAR points within the frustum of each 2D detection to generate a 3D cuboid. Starting with a non-learned solution that leverages off-the-shelf FMs, we progressively refine key components and achieve significant performance improvements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our results highlight that AnnoGuide remains an open and challenging problem, underscoring the urgent need for developing LiDAR-based FMs. We release our code and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark"
      },
      {
        "id": "oai:arXiv.org:2506.02921v1",
        "title": "A Controllable Examination for Long-Context Language Models",
        "link": "https://arxiv.org/abs/2506.02921",
        "author": "Yijun Yang, Zeyu Huang, Wenhao Zhu, Zihan Qiu, Fei Yuan, Jeff Z. Pan, Ivan Titov",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02921v1 Announce Type: new \nAbstract: Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the \"needle\" and the \"haystack\" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: $\\textit{seamless context}$, $\\textit{controllable setting}$, and $\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of $\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$. Our experimental evaluation, which includes $\\textbf{18}$ LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable."
      },
      {
        "id": "oai:arXiv.org:2506.02924v1",
        "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification",
        "link": "https://arxiv.org/abs/2506.02924",
        "author": "Diogo A. P. Nunes, Eug\\'enio Ribeiro",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02924v1 Announce Type: new \nAbstract: In this work, we describe our team's approach to eRisk's 2025 Task 1: Search for Symptoms of Depression. Given a set of sentences and the Beck's Depression Inventory - II (BDI) questionnaire, participants were tasked with submitting up to 1,000 sentences per depression symptom in the BDI, sorted by relevance. Participant submissions were evaluated according to standard Information Retrieval (IR) metrics, including Average Precision (AP) and R-Precision (R-PREC). The provided training data, however, consisted of sentences labeled as to whether a given sentence was relevant or not w.r.t. one of BDI's symptoms. Due to this labeling limitation, we framed our development as a binary classification task for each BDI symptom, and evaluated accordingly. To that end, we split the available labeled data into training and validation sets, and explored foundation model fine-tuning, sentence similarity, Large Language Model (LLM) prompting, and ensemble techniques. The validation results revealed that fine-tuning foundation models yielded the best performance, particularly when enhanced with synthetic data to mitigate class imbalance. We also observed that the optimal approach varied by symptom. Based on these insights, we devised five independent test runs, two of which used ensemble methods. These runs achieved the highest scores in the official IR evaluation, outperforming submissions from 16 other teams."
      },
      {
        "id": "oai:arXiv.org:2506.02933v1",
        "title": "From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation",
        "link": "https://arxiv.org/abs/2506.02933",
        "author": "Junyi Fang, Yuxun Chen, Yuxin Chen, Chen Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02933v1 Announce Type: new \nAbstract: The Multi-Armed Bandit (MAB) problem is challenging in non-stationary environments where reward distributions evolve dynamically. We introduce RAVEN-UCB, a novel algorithm that combines theoretical rigor with practical efficiency via variance-aware adaptation. It achieves tighter regret bounds than UCB1 and UCB-V, with gap-dependent regret of order $K \\sigma_{\\max}^2 \\log T / \\Delta$ and gap-independent regret of order $\\sqrt{K T \\log T}$. RAVEN-UCB incorporates three innovations: (1) variance-driven exploration using $\\sqrt{\\hat{\\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive control via $\\alpha_t = \\alpha_0 / \\log(t + \\epsilon)$, and (3) constant-time recursive updates for efficiency. Experiments across non-stationary patterns - distributional changes, periodic shifts, and temporary fluctuations - in synthetic and logistics scenarios demonstrate its superiority over state-of-the-art baselines, confirming theoretical and practical robustness."
      },
      {
        "id": "oai:arXiv.org:2506.02935v1",
        "title": "MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver",
        "link": "https://arxiv.org/abs/2506.02935",
        "author": "Yuepeng Zheng, Fu Luo, Zhenkun Wang, Yaoxin Wu, Yu Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02935v1 Announce Type: new \nAbstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a promising approach to train a unified model capable of solving multiple Vehicle Routing Problem (VRP) variants. However, existing Reinforcement Learning (RL)-based multi-task methods can only train light decoder models on small-scale problems, exhibiting limited generalization ability when solving large-scale problems. To overcome this limitation, this work introduces a novel multi-task learning method driven by knowledge distillation (MTL-KD), which enables the efficient training of heavy decoder models with strong generalization ability. The proposed MTL-KD method transfers policy knowledge from multiple distinct RL-based single-task models to a single heavy decoder model, facilitating label-free training and effectively improving the model's generalization ability across diverse tasks. In addition, we introduce a flexible inference strategy termed Random Reordering Re-Construction (R3C), which is specifically adapted for diverse VRP tasks and further boosts the performance of the multi-task model. Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that our proposed method consistently achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities."
      },
      {
        "id": "oai:arXiv.org:2506.02938v1",
        "title": "MIND: Material Interface Generation from UDFs for Non-Manifold Surface Reconstruction",
        "link": "https://arxiv.org/abs/2506.02938",
        "author": "Xuhui Chen, Fei Hou, Wencheng Wang, Hong Qin, Ying He",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02938v1 Announce Type: new \nAbstract: Unsigned distance fields (UDFs) are widely used in 3D deep learning due to their ability to represent shapes with arbitrary topology. While prior work has largely focused on learning UDFs from point clouds or multi-view images, extracting meshes from UDFs remains challenging, as the learned fields rarely attain exact zero distances. A common workaround is to reconstruct signed distance fields (SDFs) locally from UDFs to enable surface extraction via Marching Cubes. However, this often introduces topological artifacts such as holes or spurious components. Moreover, local SDFs are inherently incapable of representing non-manifold geometry, leading to complete failure in such cases. To address this gap, we propose MIND (Material Interface from Non-manifold Distance fields), a novel algorithm for generating material interfaces directly from UDFs, enabling non-manifold mesh extraction from a global perspective. The core of our method lies in deriving a meaningful spatial partitioning from the UDF, where the target surface emerges as the interface between distinct regions. We begin by computing a two-signed local field to distinguish the two sides of manifold patches, and then extend this to a multi-labeled global field capable of separating all sides of a non-manifold structure. By combining this multi-labeled field with the input UDF, we construct material interfaces that support non-manifold mesh extraction via a multi-labeled Marching Cubes algorithm. Extensive experiments on UDFs generated from diverse data sources, including point cloud reconstruction, multi-view reconstruction, and medial axis transforms, demonstrate that our approach robustly handles complex non-manifold surfaces and significantly outperforms existing methods."
      },
      {
        "id": "oai:arXiv.org:2506.02939v1",
        "title": "QKV Projections Require a Fraction of Their Memory",
        "link": "https://arxiv.org/abs/2506.02939",
        "author": "Malik Khalf, Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02939v1 Announce Type: new \nAbstract: The Multi-Head Attention mechanism is central to LLM operation, and multiple works target its compute and memory efficiency during training. While most works focus on approximating the scaled dot product, the memory consumption of the linear projections that compute the $Q$, $K$, and $V$ tensors from the input $x$ is often overlooked. To address this, we propose Point-Approximate Matrix Multiplication (PAMM), a novel tensor compression technique that reduces memory consumption of the $Q,K,V$ projections in attention layers by a factor of up to $\\times 512$, effectively erasing their memory footprint, while achieving similar or better final perplexity. PAMM is fully composable with efficient attention techniques such as FlashAttention, making it a practical and complementary method for memory-efficient LLM training."
      },
      {
        "id": "oai:arXiv.org:2506.02945v1",
        "title": "Quantitative LLM Judges",
        "link": "https://arxiv.org/abs/2506.02945",
        "author": "Aishwarya Sahoo, Jeevana Kruthi Karnuthala, Tushar Parmanand Budhwani, Pranchal Agarwal, Sankaran Vaidyanathan, Alexa Siu, Franck Dernoncourt, Jennifer Healey, Nedim Lipka, Ryan Rossi, Uttaran Bhattacharya, Branislav Kveton",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02945v1 Announce Type: new \nAbstract: LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling."
      },
      {
        "id": "oai:arXiv.org:2506.02946v1",
        "title": "Abstract Counterfactuals for Language Model Agents",
        "link": "https://arxiv.org/abs/2506.02946",
        "author": "Edoardo Pona, Milad Kazemi, Yali Du, David Watson, Nicola Paoletti",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02946v1 Announce Type: new \nAbstract: Counterfactual inference is a powerful tool for analysing and evaluating autonomous agents, but its application to language model (LM) agents remains challenging. Existing work on counterfactuals in LMs has primarily focused on token-level counterfactuals, which are often inadequate for LM agents due to their open-ended action spaces. Unlike traditional agents with fixed, clearly defined action spaces, the actions of LM agents are often implicit in the strings they output, making their action spaces difficult to define and interpret. Furthermore, the meanings of individual tokens can shift depending on the context, adding complexity to token-level reasoning and sometimes leading to biased or meaningless counterfactuals. We introduce \\emph{Abstract Counterfactuals}, a framework that emphasises high-level characteristics of actions and interactions within an environment, enabling counterfactual reasoning tailored to user-relevant features. Our experiments demonstrate that the approach produces consistent and meaningful counterfactuals while minimising the undesired side effects of token-level methods. We conduct experiments on text-based games and counterfactual text generation, while considering both token-level and latent-space interventions."
      },
      {
        "id": "oai:arXiv.org:2506.02950v1",
        "title": "Interaction Field Matching: Overcoming Limitations of Electrostatic Models",
        "link": "https://arxiv.org/abs/2506.02950",
        "author": "Stepan I. Manukhov, Alexander Kolesov, Vladimir V. Palyulin, Alexander Korotin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02950v1 Announce Type: new \nAbstract: Electrostatic field matching (EFM) has recently appeared as a novel physics-inspired paradigm for data generation and transfer using the idea of an electric capacitor. However, it requires modeling electrostatic fields using neural networks, which is non-trivial because of the necessity to take into account the complex field outside the capacitor plates. In this paper, we propose Interaction Field Matching (IFM), a generalization of EFM which allows using general interaction fields beyond the electrostatic one. Furthermore, inspired by strong interactions between quarks and antiquarks in physics, we design a particular interaction field realization which solves the problems which arise when modeling electrostatic fields in EFM. We show the performance on a series of toy and image data transfer problems."
      },
      {
        "id": "oai:arXiv.org:2506.02951v1",
        "title": "Adaptive Graph Pruning for Multi-Agent Communication",
        "link": "https://arxiv.org/abs/2506.02951",
        "author": "Boyi Li, Zhonghan Zhao, Der-Horng Lee, Gaoang Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02951v1 Announce Type: new \nAbstract: Large Language Model (LLM) based multi-agent systems have shown remarkable performance in various tasks, especially when enhanced through collaborative communication. However, current methods often rely on a fixed number of agents and static communication structures, limiting their ability to adapt to varying task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a novel task-adaptive multi-agent collaboration framework that jointly optimizes agent quantity (hard-pruning) and communication topology (soft-pruning). Specifically, our method employs a two-stage training strategy: firstly, independently training soft-pruning networks for different agent quantities to determine optimal agent-quantity-specific complete graphs and positional masks across specific tasks; and then jointly optimizing hard-pruning and soft-pruning within a maximum complete graph to dynamically configure the number of agents and their communication topologies per task. Extensive experiments demonstrate that our approach is: (1) High-performing, achieving state-of-the-art results across six benchmarks and consistently generalizes across multiple mainstream LLM architectures, with a increase in performance of $2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized communication topologies tailored to specific tasks, with an extremely high performance in all three task categories (general reasoning, mathematical reasoning, and code generation); (3) Token-economical, having fewer training steps and token consumption at the same time, with a decrease in token consumption of $90\\%+$; and (4) Training-efficient, achieving high performance with very few training steps compared with other methods. The performance will surpass the existing baselines after about ten steps of training under six benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.02959v1",
        "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring",
        "link": "https://arxiv.org/abs/2506.02959",
        "author": "Zhixiong Su, Yichen Wang, Herun Wan, Zhaohan Zhang, Minnan Luo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02959v1 Announce Type: new \nAbstract: The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring. We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio. Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection. Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks. Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved. We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement."
      },
      {
        "id": "oai:arXiv.org:2506.02961v1",
        "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models",
        "link": "https://arxiv.org/abs/2506.02961",
        "author": "Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, Daoyuan Chen, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02961v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have achieved state-of-the-art results across diverse domains, yet their development remains reliant on vast amounts of publicly available data, raising concerns about data scarcity and the lack of access to domain-specific, sensitive information. Federated Learning (FL) presents a compelling framework to address these challenges by enabling decentralized fine-tuning on pre-trained LLMs without sharing raw data. However, the compatibility and performance of pre-trained LLMs in FL settings remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning of LLMs across four diverse domains: general NLP, finance, medical, and coding. Each domain includes federated instruction-tuning datasets and domain-specific evaluation metrics. Our results, obtained through a collaborative, open-source and community-driven approach, provide the first comprehensive comparison across 26 pre-trained LLMs with different aggregation and fine-tuning strategies under federated settings, offering actionable insights into model performance, resource constraints, and domain adaptation. This work lays the foundation for developing privacy-preserving, domain-specialized LLMs for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.02964v1",
        "title": "FORLA:Federated Object-centric Representation Learning with Slot Attention",
        "link": "https://arxiv.org/abs/2506.02964",
        "author": "Guiqiu Liao, Matjaz Jogan, Eric Eaton, Daniel A. Hashimoto",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02964v1 Announce Type: new \nAbstract: Learning efficient visual representations across heterogeneous unlabeled datasets remains a central challenge in federated learning. Effective federated representations require features that are jointly informative across clients while disentangling domain-specific factors without supervision. We introduce FORLA, a novel framework for federated object-centric representation learning and feature adaptation across clients using unsupervised slot attention. At the core of our method is a shared feature adapter, trained collaboratively across clients to adapt features from foundation models, and a shared slot attention module that learns to reconstruct the adapted features. To optimize this adapter, we design a two-branch student-teacher architecture. In each client, a student decoder learns to reconstruct full features from foundation models, while a teacher decoder reconstructs their adapted, low-dimensional counterpart. The shared slot attention module bridges cross-domain learning by aligning object-level representations across clients. Experiments in multiple real-world datasets show that our framework not only outperforms centralized baselines on object discovery but also learns a compact, universal representation that generalizes well across domains. This work highlights federated slot attention as an effective tool for scalable, unsupervised visual representation learning from cross-domain data with distributed concepts."
      },
      {
        "id": "oai:arXiv.org:2506.02965v1",
        "title": "Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs",
        "link": "https://arxiv.org/abs/2506.02965",
        "author": "Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02965v1 Announce Type: new \nAbstract: Mixture-of-Experts (MoE) has been gaining popularity due to its successful adaptation to large language models (LLMs). In this work, we introduce Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages the sparsity of the MoE architecture for memory-efficient decentralized collaborative LLM training, enabling multiple parties with limited GPU-memory and data resources to collectively train more capable LLMs than they could achieve individually. At the same time, this approach protects training data privacy of each participant by keeping training data, as well as parts of the forward pass signal and gradients locally within each party. By design, PC-MoE synergistically combines the strengths of distributed computation with strong confidentiality assurances. Unlike most privacy-preserving schemes, which pay for confidentiality with lower task accuracy, our framework breaks that trade-off: across seven popular LLM benchmarks, it almost matches (and sometimes exceeds) the performance and convergence rate of a fully centralized model, enjoys near 70% peak GPU RAM reduction, while being fully robust against reconstruction attacks."
      },
      {
        "id": "oai:arXiv.org:2506.02972v1",
        "title": "Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles",
        "link": "https://arxiv.org/abs/2506.02972",
        "author": "Md-Ferdous Pervej, Richeng Jin, Md Moin Uddin Chowdhury, Simran Singh, \\.Ismail G\\\"uven\\c{c}, Huaiyu Dai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02972v1 Announce Type: new \nAbstract: Privacy-preserving distributed machine learning (ML) and aerial connected vehicle (ACV)-assisted edge computing have drawn significant attention lately. Since the onboard sensors of ACVs can capture new data as they move along their trajectories, the continual arrival of such 'newly' sensed data leads to online learning and demands carefully crafting the trajectories. Besides, as typical ACVs are inherently resource-constrained, computation- and communication-efficient ML solutions are needed. Therefore, we propose a computation- and communication-efficient online aerial federated learning (2CEOAFL) algorithm to take the benefits of continual sensed data and limited onboard resources of the ACVs. In particular, considering independently owned ACVs act as selfish data collectors, we first model their trajectories according to their respective time-varying data distributions. We then propose a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense ML model to make it shallow, (b) train the pruned model, and (c) probabilistically quantize and offload their trained accumulated gradients to the central server (CS). Our extensive simulation results show that the proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned and nonquantized, hence, computation- and communication-inefficient counterparts."
      },
      {
        "id": "oai:arXiv.org:2506.02973v1",
        "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation",
        "link": "https://arxiv.org/abs/2506.02973",
        "author": "Dingwei Chen, Ziqiang Liu, Feiteng Fang, Chak Tou Leong, Shiwen Ni, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02973v1 Announce Type: new \nAbstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text understanding and generation. However, their tendency to produce factually inconsistent outputs, commonly referred to as ''hallucinations'', remains a critical challenge. Existing approaches, such as retrieval-based and inference-time correction methods, primarily address this issue at the input or output level, often overlooking the intrinsic information refinement process and the role of premature layers. Meanwhile, alignment- and fine-tuning-based methods are resource-intensive. In this paper, we propose PLI (Premature Layers Interpolation), a novel, training-free, and plug-and-play intervention designed to enhance factuality. PLI mitigates hallucinations by inserting premature layers formed through mathematical interpolation with adjacent layers. Inspired by stable diffusion and sampling steps, PLI extends the depth of information processing and transmission in LLMs, improving factual coherence. Experiments on four publicly available datasets demonstrate that PLI effectively reduces hallucinations while outperforming existing baselines in most cases. Further analysis suggests that the success of layer interpolation is closely linked to LLMs' internal mechanisms. To promote reproducibility, we will release our code and data upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2506.02975v1",
        "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation",
        "link": "https://arxiv.org/abs/2506.02975",
        "author": "Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zunnan Xu, Zhaoyang Zhang, Yixiao Ge, Xiu Li, Ying Shan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02975v1 Announce Type: new \nAbstract: With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at https://github.com/Tencent/HaploVLM."
      },
      {
        "id": "oai:arXiv.org:2506.02976v1",
        "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge",
        "link": "https://arxiv.org/abs/2506.02976",
        "author": "Rachid Zeghlache, Ikram Brahim, Pierre-Henri Conze, Mathieu Lamard, Mohammed El Amine Lazouni, Zineb Aziza Elaouaber, Leila Ryma Lazouni, Christopher Nielsen, Ahmad O. Ahsan, Matthias Wilms, Nils D. Forkert, Lovre Antonio Budimir, Ivana Matovinovi\\'c, Donik Vr\\v{s}nak, Sven Lon\\v{c}ari\\'c, Philippe Zhang, Weili Jiang, Yihao Li, Yiding Hao, Markus Frohmann, Patrick Binder, Marcel Huber, Taha Emre, Teresa Finisterra Ara\\'ujo, Marzieh Oghbaie, Hrvoje Bogunovi\\'c, Amerens A. Bekkers, Nina M. van Liebergen, Hugo J. Kuijf, Abdul Qayyum, Moona Mazher, Steven A. Niederer, Alberto J. Beltr\\'an-Carrero, Juan J. G\\'omez-Valverde, Javier Torresano-Rodr\\'iquez, \\'Alvaro Caballero-Sastre, Mar\\'ia J. Ledesma Carbayo, Yosuke Yamagishi, Yi Ding, Robin Peretzke, Alexandra Ertl, Maximilian Fischer, Jessica K\\\"achele, Sofiane Zehar, Karim Boukli Hacene, Thomas Monfort, B\\'eatrice Cochener, Mostafa El Habib Daho, Anas-Alexis Benyoussef, Gwenol\\'e Quellec",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02976v1 Announce Type: new \nAbstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated detection and monitoring of age-related macular degeneration (AMD) through the analysis of optical coherence tomography (OCT) images. Designed to evaluate algorithmic performance in detecting neovascular activity changes within AMD, the challenge incorporated unique multi-modal datasets. The primary dataset, sourced from Brest, France, was used by participating teams to train and test their models. The final ranking was determined based on performance on this dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate population and device shifts from submitted solutions. Two tasks were involved in the MARIO challenge. The first one was the classification of evolution between two consecutive 2D OCT B-scans. The second one was the prediction of future AMD evolution over three months for patients undergoing anti-vascular endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with the top 12 finalists presenting their methods. This paper outlines the challenge's structure, tasks, data characteristics, and winning methodologies, setting a benchmark for AMD monitoring using OCT, infrared imaging, and clinical data (such as the number of visits, age, gender, etc.). The results of this challenge indicate that artificial intelligence (AI) performs as well as a physician in measuring AMD progression (Task 1) but is not yet able of predicting future evolution (Task 2)."
      },
      {
        "id": "oai:arXiv.org:2506.02978v1",
        "title": "On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses",
        "link": "https://arxiv.org/abs/2506.02978",
        "author": "Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul R\\'ecamier, Salah Ghamizi, Maxime Cordy, Mike Papadakis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02978v1 Announce Type: new \nAbstract: Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage in-context learning to achieve strong performance without gradient updates or fine-tuning. However, their robustness to adversarial manipulation remains largely unexplored. In this work, we present a comprehensive study of the adversarial vulnerabilities of tabular FM, focusing on both their fragility to targeted test-time attacks and their potential misuse as adversarial tools. We show on three benchmarks in finance, cybersecurity and healthcare, that small, structured perturbations to test inputs can significantly degrade prediction accuracy, even when training context remain fixed. Additionally, we demonstrate that tabular FM can be repurposed to generate transferable evasion to conventional models such as random forests and XGBoost, and on a lesser extent to deep tabular models. To improve tabular FM, we formulate the robustification problem as an optimization of the weights (adversarial fine-tuning), or the context (adversarial in-context learning). We introduce an in-context adversarial training strategy that incrementally replaces the context with adversarial perturbed instances, without updating model weights. Our approach improves robustness across multiple tabular benchmarks. Together, these findings position tabular FM as both a target and a source of adversarial threats, highlighting the urgent need for robust training and evaluation practices in this emerging paradigm."
      },
      {
        "id": "oai:arXiv.org:2506.02979v1",
        "title": "Towards a Japanese Full-duplex Spoken Dialogue System",
        "link": "https://arxiv.org/abs/2506.02979",
        "author": "Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02979v1 Announce Type: new \nAbstract: Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model's performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness."
      },
      {
        "id": "oai:arXiv.org:2506.02981v1",
        "title": "Astrophotography turbulence mitigation via generative models",
        "link": "https://arxiv.org/abs/2506.02981",
        "author": "Joonyeoup Kim, Yu Yuan, Xingguang Zhang, Xijun Wang, Stanley Chan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02981v1 Announce Type: new \nAbstract: Photography is the cornerstone of modern astronomical and space research. However, most astronomical images captured by ground-based telescopes suffer from atmospheric turbulence, resulting in degraded imaging quality. While multi-frame strategies like lucky imaging can mitigate some effects, they involve intensive data acquisition and complex manual processing. In this paper, we propose AstroDiff, a generative restoration method that leverages both the high-quality generative priors and restoration capabilities of diffusion models to mitigate atmospheric turbulence. Extensive experiments demonstrate that AstroDiff outperforms existing state-of-the-art learning-based methods in astronomical image turbulence mitigation, providing higher perceptual quality and better structural fidelity under severe turbulence conditions. Our code and additional results are available at https://web-six-kappa-66.vercel.app/"
      },
      {
        "id": "oai:arXiv.org:2506.02986v1",
        "title": "Implicit Regularization of the Deep Inverse Prior Trained with Inertia",
        "link": "https://arxiv.org/abs/2506.02986",
        "author": "Nathan Buskulic, Jalal Fadil, Yvain Qu\\'eau",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02986v1 Announce Type: new \nAbstract: Solving inverse problems with neural networks benefits from very few theoretical guarantees when it comes to the recovery guarantees. We provide in this work convergence and recovery guarantees for self-supervised neural networks applied to inverse problems, such as Deep Image/Inverse Prior, and trained with inertia featuring both viscous and geometric Hessian-driven dampings. We study both the continuous-time case, i.e., the trajectory of a dynamical system, and the discrete case leading to an inertial algorithm with an adaptive step-size. We show in the continuous-time case that the network can be trained with an optimal accelerated exponential convergence rate compared to the rate obtained with gradient flow. We also show that training a network with our inertial algorithm enjoys similar recovery guarantees though with a less sharp linear convergence rate."
      },
      {
        "id": "oai:arXiv.org:2506.02987v1",
        "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis",
        "link": "https://arxiv.org/abs/2506.02987",
        "author": "Richard Armitage",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02987v1 Announce Type: new \nAbstract: Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data."
      },
      {
        "id": "oai:arXiv.org:2506.02995v1",
        "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems",
        "link": "https://arxiv.org/abs/2506.02995",
        "author": "Iuliia Zaitova, Badr M. Abdullah, Wei Xue, Dietrich Klakow, Bernd M\\\"obius, Tania Avgustinova",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02995v1 Announce Type: new \nAbstract: Idioms are defined as a group of words with a figurative meaning not deducible from their individual components. Although modern machine translation systems have made remarkable progress, translating idioms remains a major challenge, especially for speech-to-text systems, where research on this topic is notably sparse. In this paper, we systematically evaluate idiom translation as compared to conventional news translation in both text-to-text machine translation (MT) and speech-to-text translation (SLT) systems across two language pairs (German to English, Russian to English). We compare state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal that SLT systems experience a pronounced performance drop on idiomatic data, often reverting to literal translations even in higher layers, whereas MT systems and Large Language Models demonstrate better handling of idioms. These findings underscore the need for idiom-specific strategies and improved internal representations in SLT architectures."
      },
      {
        "id": "oai:arXiv.org:2506.02997v1",
        "title": "Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich Representation",
        "link": "https://arxiv.org/abs/2506.02997",
        "author": "Yongqi Wang, Chunlei Zhang, Hangting Chen, Zhou Zhao, Dong Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02997v1 Announce Type: new \nAbstract: Controllable TTS models with natural language prompts often lack the ability for fine-grained control and face a scarcity of high-quality data. We propose a two-stage style-controllable TTS system with language models, utilizing a quantized masked-autoencoded style-rich representation as an intermediary. In the first stage, an autoregressive transformer is used for the conditional generation of these style-rich tokens from text and control signals. The second stage generates codec tokens from both text and sampled style-rich tokens. Experiments show that training the first-stage model on extensive datasets enhances the content robustness of the two-stage model as well as control capabilities over multiple attributes. By selectively combining discrete labels and speaker embeddings, we explore fully controlling the speaker's timbre and other stylistic information, and adjusting attributes like emotion for a specified speaker. Audio samples are available at https://style-ar-tts.github.io."
      },
      {
        "id": "oai:arXiv.org:2506.02998v1",
        "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems",
        "link": "https://arxiv.org/abs/2506.02998",
        "author": "{\\DJ}or{\\dj}e Klisura, Astrid R Bernaga Torres, Anna Karen G\\'arate-Escamilla, Rajesh Roshan Biswal, Ke Yang, Hilal Pataci, Anthony Rios",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02998v1 Announce Type: new \nAbstract: Privacy policies inform users about data collection and usage, yet their complexity limits accessibility for diverse populations. Existing Privacy Policy Question Answering (QA) systems exhibit performance disparities across English dialects, disadvantaging speakers of non-standard varieties. We propose a novel multi-agent framework inspired by human-centered design principles to mitigate dialectal biases. Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent, and a Privacy Policy Agent, which refines predictions using domain expertise. Unlike prior approaches, our method does not require retraining or dialect-specific fine-tuning, making it broadly applicable across models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without additional training data. These results highlight the effectiveness of structured agent collaboration in mitigating dialect biases and underscore the importance of designing NLP systems that account for linguistic diversity to ensure equitable access to privacy information."
      },
      {
        "id": "oai:arXiv.org:2506.03007v1",
        "title": "DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models",
        "link": "https://arxiv.org/abs/2506.03007",
        "author": "Jiarui Wang, Huiyu Duan, Juntong Wang, Ziheng Jia, Woo Yi Yang, Xiaorong Zhu, Yu Zhao, Jiaying Qian, Yuke Xing, Guangtao Zhai, Xiongkuo Min",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03007v1 Announce Type: new \nAbstract: With the rapid advancement of generative models, the realism of AI-generated images has significantly improved, posing critical challenges for verifying digital content authenticity. Current deepfake detection methods often depend on datasets with limited generation models and content diversity that fail to keep pace with the evolving complexity and increasing realism of the AI-generated content. Large multimodal models (LMMs), widely adopted in various vision tasks, have demonstrated strong zero-shot capabilities, yet their potential in deepfake detection remains largely unexplored. To bridge this gap, we present \\textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i) broad diversity, including 540,000 images across real, AI-edited, and AI-generated content, (ii) latest model, the fake images are generated by 12 state-of-the-art generation models, and (iii) bidirectional benchmarking and evaluating for both the detection accuracy of deepfake detectors and the evasion capability of generative models. Based on DFBench, we propose \\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a combined probability strategy from multiple LMMs. MoA-DF achieves state-of-the-art performance, further proving the effectiveness of leveraging LMMs for deepfake detection. Database and codes are publicly available at https://github.com/IntMeGroup/DFBench."
      },
      {
        "id": "oai:arXiv.org:2506.03009v1",
        "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech",
        "link": "https://arxiv.org/abs/2506.03009",
        "author": "Florian Ludwig, Torsten Zesch, Frederike Zufall",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03009v1 Announce Type: new \nAbstract: The assessment of legal problems requires the consideration of a specific legal system and its levels of abstraction, from constitutional law to statutory law to case law. The extent to which Large Language Models (LLMs) internalize such legal systems is unknown. In this paper, we propose and investigate different approaches to condition LLMs at different levels of abstraction in legal systems. This paper examines different approaches to conditioning LLMs at multiple levels of abstraction in legal systems to detect potentially punishable hate speech. We focus on the task of classifying whether a specific social media posts falls under the criminal offense of incitement to hatred as prescribed by the German Criminal Code. The results show that there is still a significant performance gap between models and legal experts in the legal assessment of hate speech, regardless of the level of abstraction with which the models were conditioned. Our analysis revealed, that models conditioned on abstract legal knowledge lacked deep task understanding, often contradicting themselves and hallucinating answers, while models using concrete legal knowledge performed reasonably well in identifying relevant target groups, but struggled with classifying target conducts."
      },
      {
        "id": "oai:arXiv.org:2506.03011v1",
        "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers",
        "link": "https://arxiv.org/abs/2506.03011",
        "author": "Aditya Bharat Soni, Boxuan Li, Xingyao Wang, Valerie Chen, Graham Neubig",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03011v1 Announce Type: new \nAbstract: Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. In addition, AI agents have been specialized for domains such as software engineering, web navigation, and workflow automation. However, this results in agents that are good for one thing but fail to generalize beyond their intended scope. One reason for this is that agent developers provide a highly specialized set of tools or make architectural decisions optimized for a specific use case or benchmark. In this work, we ask the question: what is the minimal set of general tools that can be used to achieve high performance across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist agent built with a modest number of general tools: code editing and execution, web search, as well as multimodal web browsing and file access. Importantly, OpenHands-Versa demonstrates superior or competitive performance over leading specialized agents across three diverse and challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, outperforming the best-performing previously published results with absolute improvements in success rate of 9.1, 1.3, and 9.1 points respectively. Further, we show how existing state-of-the-art multi-agent systems fail to generalize beyond their target domains. These results demonstrate the feasibility of developing a generalist agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline for future research."
      },
      {
        "id": "oai:arXiv.org:2506.03022v1",
        "title": "Smartflow: Enabling Scalable Spatiotemporal Geospatial Research",
        "link": "https://arxiv.org/abs/2506.03022",
        "author": "David McVicar, Brian Avant, Adrian Gould, Diego Torrejon, Charles Della Porta, Ryan Mukherjee",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03022v1 Announce Type: new \nAbstract: BlackSky introduces Smartflow, a cloud-based framework enabling scalable spatiotemporal geospatial research built on open-source tools and technologies. Using STAC-compliant catalogs as a common input, heterogeneous geospatial data can be processed into standardized datacubes for analysis and model training. Model experimentation is managed using a combination of tools, including ClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is Kubernetes, which orchestrates the provisioning and execution of workflows to support both horizontal and vertical scalability. This combination of features makes Smartflow well-suited for geospatial model development and analysis over large geographic areas, time scales, and expansive image archives.\n  We also present a novel neural architecture, built using Smartflow, to monitor large geographic areas for heavy construction. Qualitative results based on data from the IARPA Space-based Machine Automated Recognition Technique (SMART) program are presented that show the model is capable of detecting heavy construction throughout all major phases of development."
      },
      {
        "id": "oai:arXiv.org:2506.03028v1",
        "title": "Protein Inverse Folding From Structure Feedback",
        "link": "https://arxiv.org/abs/2506.03028",
        "author": "Junde Xu, Zijun Gao, Xinyi Zhou, Jie Hu, Xingyi Cheng, Le Song, Guangyong Chen, Pheng-Ann Heng, Jiezhong Qiu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03028v1 Announce Type: new \nAbstract: The inverse folding problem, aiming to design amino acid sequences that fold into desired three-dimensional structures, is pivotal for various biotechnological applications. Here, we introduce a novel approach leveraging Direct Preference Optimization (DPO) to fine-tune an inverse folding model using feedback from a protein folding model. Given a target protein structure, we begin by sampling candidate sequences from the inverse-folding model, then predict the three-dimensional structure of each sequence with the folding model to generate pairwise structural-preference labels. These labels are used to fine-tune the inverse-folding model under the DPO objective. Our results on the CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence recovery of baseline models but also leads to a significant improvement in average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity. Furthermore, iterative application of our DPO-based method on challenging protein structures yields substantial gains, with an average TM-Score increase of 79.5\\% with regard to the baseline model. This work establishes a promising direction for enhancing protein sequence design ability from structure feedback by effectively utilizing preference optimization."
      },
      {
        "id": "oai:arXiv.org:2506.03035v1",
        "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning",
        "link": "https://arxiv.org/abs/2506.03035",
        "author": "Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03035v1 Announce Type: new \nAbstract: Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length."
      },
      {
        "id": "oai:arXiv.org:2506.03037v1",
        "title": "On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning",
        "link": "https://arxiv.org/abs/2506.03037",
        "author": "Shubhendu Trivedi, Brian D. Nord",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03037v1 Announce Type: new \nAbstract: Quantifying uncertainties for machine learning (ML) models is a foundational challenge in modern data analysis. This challenge is compounded by at least two key aspects of the field: (a) inconsistent terminology surrounding uncertainty and estimation across disciplines, and (b) the varying technical requirements for establishing trustworthy uncertainties in diverse problem contexts. In this position paper, we aim to clarify the depth of these challenges by identifying these inconsistencies and articulating how different contexts impose distinct epistemic demands. We examine the current landscape of estimation targets (e.g., prediction, inference, simulation-based inference), uncertainty constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to map between them. Drawing on the literature, we highlight and explain examples of problematic mappings. To help address these issues, we advocate for standards that promote alignment between the \\textit{intent} and \\textit{implementation} of uncertainty quantification (UQ) approaches. We discuss several axes of trustworthiness that are necessary (if not sufficient) for reliable UQ in ML models, and show how these axes can inform the design and evaluation of uncertainty-aware ML systems. Our practical recommendations focus on scientific ML, offering illustrative cases and use scenarios, particularly in the context of simulation-based inference (SBI)."
      },
      {
        "id": "oai:arXiv.org:2506.03038v1",
        "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective",
        "link": "https://arxiv.org/abs/2506.03038",
        "author": "Jintian Shao, Yiming Cheng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03038v1 Announce Type: new \nAbstract: Reinforcement learning (RL) enhances large language models (LLMs) in complex, long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework, despite sophisticated mechanisms like Decoupled GAE, theoretically faces fundamental limitations in comprehensively modeling and leveraging deep, long-term value for fine-grained, step-by-step policy guidance in extended reasoning chains. We argue these limitations stem from inherent difficulties in credit assignment, value function representational capacity with temporally abstracted goals, and translating global value signals into local policy improvements, especially with sparse rewards. Our theoretical analysis examines these aspects to illuminate VAPO's boundaries in long-term value modeling, aiming to deepen understanding of current RL for advanced reasoning and suggest future research for more robust LLM agents."
      },
      {
        "id": "oai:arXiv.org:2506.03043v1",
        "title": "Sample complexity of Schr\\\"odinger potential estimation",
        "link": "https://arxiv.org/abs/2506.03043",
        "author": "Nikita Puchkin, Iurii Pustovalov, Yuri Sapronov, Denis Suchkov, Alexey Naumov, Denis Belomestny",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03043v1 Announce Type: new \nAbstract: We address the problem of Schr\\\"odinger potential estimation, which plays a crucial role in modern generative modelling approaches based on Schr\\\"odinger bridges and stochastic optimal control for SDEs. Given a simple prior diffusion process, these methods search for a path between two given distributions $\\rho_0$ and $\\rho_T^*$ requiring minimal efforts. The optimal drift in this case can be expressed through a Schr\\\"odinger potential. In the present paper, we study generalization ability of an empirical Kullback-Leibler (KL) risk minimizer over a class of admissible log-potentials aimed at fitting the marginal distribution at time $T$. Under reasonable assumptions on the target distribution $\\rho_T^*$ and the prior process, we derive a non-asymptotic high-probability upper bound on the KL-divergence between $\\rho_T^*$ and the terminal density corresponding to the estimated log-potential. In particular, we show that the excess KL-risk may decrease as fast as $O(\\log^2 n / n)$ when the sample size $n$ tends to infinity even if both $\\rho_0$ and $\\rho_T^*$ have unbounded supports."
      },
      {
        "id": "oai:arXiv.org:2506.03051v1",
        "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs",
        "link": "https://arxiv.org/abs/2506.03051",
        "author": "Yuval Kansal, Shmuel Berman, Lydia Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03051v1 Announce Type: new \nAbstract: Factuality is a necessary precursor to useful educational tools. As adoption of Large Language Models (LLMs) in education continues of grow, ensuring correctness in all settings is paramount. Despite their strong English capabilities, LLM performance in other languages is largely untested. In this work, we evaluate the correctness of the Llama3.1 family of models in answering factual questions appropriate for middle and high school students. We demonstrate that LLMs not only provide extraneous and less truthful information, but also exacerbate existing biases against rare languages."
      },
      {
        "id": "oai:arXiv.org:2506.03062v1",
        "title": "Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation",
        "link": "https://arxiv.org/abs/2506.03062",
        "author": "Qining Zhang, Tanner Fiez, Yi Liu, Wenyang Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03062v1 Announce Type: new \nAbstract: Standard A/B tests in online experiments face statistical power challenges when testing multiple candidates simultaneously, while adaptive experimental designs (AED) alone fall short in inferring experiment statistics such as the average treatment effect, especially with many metrics (e.g., revenue, safety) and heterogeneous variances. This paper proposes a fixed-budget multi-metric AED framework with a two-phase structure: an adaptive exploration phase to identify the best treatment, and a validation phase with an A/B test to verify the treatment's quality and infer statistics. We propose SHRVar, which generalizes sequential halving (SH) (Karnin et al., 2013) with a novel relative-variance-based sampling and an elimination strategy built on reward z-values. It achieves a provable error probability that decreases exponentially, where the exponent generalizes the complexity measure for SH (Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and heterogeneous variances, respectively. Numerical experiments verify our analysis and demonstrate the superior performance of this new framework."
      },
      {
        "id": "oai:arXiv.org:2506.03065v1",
        "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers",
        "link": "https://arxiv.org/abs/2506.03065",
        "author": "Pengtao Chen, Xianfang Zeng, Maosen Zhao, Peng Ye, Mingzhu Shen, Wei Cheng, Gang Yu, Tao Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03065v1 Announce Type: new \nAbstract: While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical FLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$, and 1.58$\\times$, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis."
      },
      {
        "id": "oai:arXiv.org:2506.03066v1",
        "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function",
        "link": "https://arxiv.org/abs/2506.03066",
        "author": "Qining Zhang, Lei Ying",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03066v1 Announce Type: new \nAbstract: Link functions, which characterize how human preferences are generated from the value function of an RL problem, are a crucial component in designing RLHF algorithms. Almost all RLHF algorithms, including state-of-the-art ones in empirical studies such as DPO and PPO, assume the link function is known to the agent (e.g., a logistic function according to the Bradley-Terry model), which is arguably unrealistic considering the complex nature of human preferences. To avoid link function mis-specification, this paper studies general RLHF problems with unknown link functions. We propose a novel policy optimization algorithm called ZSPO based on a new zeroth-order policy optimization method, where the key is to use human preference to construct a parameter update direction that is positively correlated with the true policy gradient direction. ZSPO achieves it by estimating the sign of the value function difference instead of estimating the gradient from the value function difference, so it does not require knowing the link function. Under mild conditions, ZSPO converges to a stationary policy with a polynomial convergence rate depending on the number of policy iterations and trajectories per iteration. Numerical results also show the superiority of ZSPO under link function mismatch."
      },
      {
        "id": "oai:arXiv.org:2506.03067v1",
        "title": "EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models",
        "link": "https://arxiv.org/abs/2506.03067",
        "author": "Mingzhe Li, Gehao Zhang, Zhenting Wang, Shiqing Ma, Siqi Pan, Richard Cartwright, Juan Zhai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03067v1 Announce Type: new \nAbstract: Text-to-image generation models~(e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual large language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called \\sys for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation."
      },
      {
        "id": "oai:arXiv.org:2506.03073v1",
        "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM",
        "link": "https://arxiv.org/abs/2506.03073",
        "author": "Roman Titkov, Egor Zubkov, Dmitry Yudin, Jaafar Mahmoud, Malik Mohrat, Gennady Sidorov",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03073v1 Announce Type: new \nAbstract: Modern Gaussian Splatting methods have proven highly effective for real-time photorealistic rendering of 3D scenes. However, integrating semantic information into this representation remains a significant challenge, especially in maintaining real-time performance for SLAM (Simultaneous Localization and Mapping) applications. In this work, we introduce LEG-SLAM -- a novel approach that fuses an optimized Gaussian Splatting implementation with visual-language feature extraction using DINOv2 followed by a learnable feature compressor based on Principal Component Analysis, while enabling an online dense SLAM. Our method simultaneously generates high-quality photorealistic images and semantically labeled scene maps, achieving real-time scene reconstruction with more than 10 fps on the Replica dataset and 18 fps on ScanNet. Experimental results show that our approach significantly outperforms state-of-the-art methods in reconstruction speed while achieving competitive rendering quality. The proposed system eliminates the need for prior data preparation such as camera's ego motion or pre-computed static semantic maps. With its potential applications in autonomous robotics, augmented reality, and other interactive domains, LEG-SLAM represents a significant step forward in real-time semantic 3D Gaussian-based SLAM. Project page: https://titrom025.github.io/LEG-SLAM/"
      },
      {
        "id": "oai:arXiv.org:2506.03075v1",
        "title": "Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness",
        "link": "https://arxiv.org/abs/2506.03075",
        "author": "Bogdan Chornomaz, Yonatan Koren, Shay Moran, Tom Waknine",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03075v1 Announce Type: new \nAbstract: We study the problem of learning in the presence of an adversary that can corrupt an $\\eta$ fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work established that the optimal error under such instance-targeted poisoning attacks scales as $\\Theta(d\\eta)$, where $d$ is the VC dimension of the hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding question in the agnostic setting. We show that the optimal excess error is $\\tilde{\\Theta}(\\sqrt{d\\eta})$, answering one of the main open problems left by Hanneke et al. To achieve this rate, it is necessary to use randomized learners: Hanneke et al. showed that deterministic learners can be forced to suffer error close to 1, even under small amounts of poisoning. Perhaps surprisingly, our upper bound remains valid even when the learner's random bits are fully visible to the adversary . In the other direction, our lower bound is stronger than standard PAC-style bounds: instead of tailoring a hard distribution separately for each sample size, we exhibit a single fixed distribution under which the adversary can enforce an excess error of $\\Omega(\\sqrt{d\\eta})$ infinitely often."
      },
      {
        "id": "oai:arXiv.org:2506.03077v1",
        "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
        "link": "https://arxiv.org/abs/2506.03077",
        "author": "Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03077v1 Announce Type: new \nAbstract: Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP."
      },
      {
        "id": "oai:arXiv.org:2506.03079v1",
        "title": "ORV: 4D Occupancy-centric Robot Video Generation",
        "link": "https://arxiv.org/abs/2506.03079",
        "author": "Xiuyu Yang, Bohan Li, Shaocong Xu, Nan Wang, Chongjie Ye, Zhaoxi Chen, Minghan Qin, Yikang Ding, Xin Jin, Hang Zhao, Hao Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03079v1 Announce Type: new \nAbstract: Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV"
      },
      {
        "id": "oai:arXiv.org:2506.03082v1",
        "title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis",
        "link": "https://arxiv.org/abs/2506.03082",
        "author": "Ssharvien Kumar Sivakumar, Yannik Frisch, Ghazal Ghazaei, Anirban Mukhopadhyay",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03082v1 Announce Type: new \nAbstract: Surgical simulation plays a pivotal role in training novice surgeons, accelerating their learning curve and reducing intra-operative errors. However, conventional simulation tools fall short in providing the necessary photorealism and the variability of human anatomy. In response, current methods are shifting towards generative model-based simulators. Yet, these approaches primarily focus on using increasingly complex conditioning for precise synthesis while neglecting the fine-grained human control aspect. To address this gap, we introduce SG2VID, the first diffusion-based video model that leverages Scene Graphs for both precise video synthesis and fine-grained human control. We demonstrate SG2VID's capabilities across three public datasets featuring cataract and cholecystectomy surgery. While SG2VID outperforms previous methods both qualitatively and quantitatively, it also enables precise synthesis, providing accurate control over tool and anatomy's size and movement, entrance of new tools, as well as the overall scene layout. We qualitatively motivate how SG2VID can be used for generative augmentation and present an experiment demonstrating its ability to improve a downstream phase detection task when the training set is extended with our synthetic videos. Finally, to showcase SG2VID's ability to retain human control, we interact with the Scene Graphs to generate new video samples depicting major yet rare intra-operative irregularities."
      },
      {
        "id": "oai:arXiv.org:2506.03084v1",
        "title": "InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba",
        "link": "https://arxiv.org/abs/2506.03084",
        "author": "Zizhao Wu, Yingying Sun, Yiming Chen, Xiaoling Gu, Ruyu Liu, Jiazhou Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03084v1 Announce Type: new \nAbstract: Human-human interaction generation has garnered significant attention in motion synthesis due to its vital role in understanding humans as social beings. However, existing methods typically rely on transformer-based architectures, which often face challenges related to scalability and efficiency. To address these issues, we propose a novel, efficient human-human interaction generation method based on the Mamba framework, designed to meet the demands of effectively capturing long-sequence dependencies while providing real-time feedback. Specifically, we introduce an adaptive spatio-temporal Mamba framework that utilizes two parallel SSM branches with an adaptive mechanism to integrate the spatial and temporal features of motion sequences. To further enhance the model's ability to capture dependencies within individual motion sequences and the interactions between different individual sequences, we develop two key modules: the self-adaptive spatio-temporal Mamba module and the cross-adaptive spatio-temporal Mamba module, enabling efficient feature learning. Extensive experiments demonstrate that our method achieves state-of-the-art results on two interaction datasets with remarkable quality and efficiency. Compared to the baseline method InterGen, our approach not only improves accuracy but also requires a minimal parameter size of just 66M ,only 36% of InterGen's, while achieving an average inference speed of 0.57 seconds, which is 46% of InterGen's execution time."
      },
      {
        "id": "oai:arXiv.org:2506.03085v1",
        "title": "Non-Asymptotic Length Generalization",
        "link": "https://arxiv.org/abs/2506.03085",
        "author": "Thomas Chen, Tengyu Ma, Zhiyuan Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03085v1 Announce Type: new \nAbstract: Length generalization is the ability of a learning algorithm to learn a hypothesis which generalizes to longer inputs than the inputs in the training set. In this paper, we provide provable guarantees of length generalization for various classes of functions in an idealized setting. First, we formalize the framework of non-asymptotic length generalization, which requires a computable upper bound for the minimum input length that guarantees length generalization, as a function of the complexity of ground-truth function under some given complexity measure. We refer to this minimum input length to length generalize as length complexity. We show the Minimum-Complexity Interpolator learning algorithm achieves optimal length complexity. We further show that whether a function class admits non-asymptotic length generalization is equivalent to the decidability of its language equivalence problem, which implies that there is no computable upper bound for the length complexity of Context-Free Grammars. On the positive side, we show that the length complexity of Deterministic Finite Automata is $2n - 2$ where $n$ is the number of states of the ground-truth automaton. Our main results are upper bounds of length complexity for a subset of a transformer-related function class called C-RASP (Yang & Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions is $O(T^2)$ when the ground-truth function has precision $T$, and that the length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the ground-truth function has precision $T$ and $K$ heads."
      },
      {
        "id": "oai:arXiv.org:2506.03087v1",
        "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment",
        "link": "https://arxiv.org/abs/2506.03087",
        "author": "Bin Ma, Yuyuan Feng, Minhua Lin, Enyan Dai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03087v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have become essential tools for analyzing graph-structured data in domains such as drug discovery and financial analysis, leading to growing demands for model transparency. Recent advances in explainable GNNs have addressed this need by revealing important subgraphs that influence predictions, but these explanation mechanisms may inadvertently expose models to security risks. This paper investigates how such explanations potentially leak critical decision logic that can be exploited for model stealing. We propose {\\method}, a novel stealing framework that integrates explanation alignment for capturing decision logic with guided data augmentation for efficient training under limited queries, enabling effective replication of both the predictive behavior and underlying reasoning patterns of target models. Experiments on molecular graph datasets demonstrate that our approach shows advantages over conventional methods in model stealing. This work highlights important security considerations for the deployment of explainable GNNs in sensitive domains and suggests the need for protective measures against explanation-based attacks. Our code is available at https://github.com/beanmah/EGSteal."
      },
      {
        "id": "oai:arXiv.org:2506.03089v1",
        "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness",
        "link": "https://arxiv.org/abs/2506.03089",
        "author": "Lucas Piper, Arlindo L. Oliveira, Tiago Marques",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03089v1 Announce Type: new \nAbstract: Convolutional neural networks (CNNs) trained on object recognition achieve high task performance but continue to exhibit vulnerability under a range of visual perturbations and out-of-domain images, when compared with biological vision. Prior work has demonstrated that coupling a standard CNN with a front-end block (VOneBlock) that mimics the primate primary visual cortex (V1) can improve overall model robustness. Expanding on this, we introduce Early Vision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock with a novel SubcorticalBlock, whose architecture draws from computational models in neuroscience and is parameterized to maximize alignment with subcortical responses reported across multiple experimental studies. Without being optimized to do so, the assembly of the SubcorticalBlock with the VOneBlock improved V1 alignment across most standard V1 benchmarks, and better modeled extra-classical receptive field phenomena. In addition, EVNets exhibit stronger emergent shape bias and overperform the base CNN architecture by 8.5% on an aggregate benchmark of robustness evaluations, including adversarial perturbations, common corruptions, and domain shifts. Finally, we show that EVNets can be further improved when paired with a state-of-the-art data augmentation technique, surpassing the performance of the isolated data augmentation approach by 7.3% on our robustness benchmark. This result reveals complementary benefits between changes in architecture to better mimic biology and training-based machine learning approaches."
      },
      {
        "id": "oai:arXiv.org:2506.03090v1",
        "title": "Literary Evidence Retrieval via Long-Context Language Models",
        "link": "https://arxiv.org/abs/2506.03090",
        "author": "Katherine Thai, Mohit Iyyer",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03090v1 Announce Type: new \nAbstract: How well do modern long-context language models understand literary fiction? We explore this question via the task of literary evidence retrieval, repurposing the RELiC dataset of That et al. (2022) to construct a benchmark where the entire text of a primary source (e.g., The Great Gatsby) is provided to an LLM alongside literary criticism with a missing quotation from that work. This setting, in which the model must generate the missing quotation, mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination. We curate a high-quality subset of 292 examples through extensive filtering and human verification. Our experiments show that recent reasoning models, such as Gemini Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In contrast, the best open-weight model achieves only 29.1% accuracy, highlighting a wide gap in interpretive reasoning between open and closed-weight models. Despite their speed and apparent accuracy, even the strongest models struggle with nuanced literary signals and overgeneration, signaling open challenges for applying LLMs to literary analysis. We release our dataset and evaluation code to encourage future work in this direction."
      },
      {
        "id": "oai:arXiv.org:2506.03093v1",
        "title": "From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit",
        "link": "https://arxiv.org/abs/2506.03093",
        "author": "Val\\'erie Costa, Thomas Fel, Ekdeep Singh Lubana, Bahareh Tolooshams, Demba Ba",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03093v1 Announce Type: new \nAbstract: Motivated by the hypothesis that neural network representations encode abstract, interpretable features as linearly accessible, approximately orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in interpretability. However, recent work has demonstrated phenomenology of model representations that lies outside the scope of this hypothesis, showing signatures of hierarchical, nonlinear, and multi-dimensional features. This raises the question: do SAEs represent features that possess structure at odds with their motivating hypothesis? If not, does avoiding this mismatch help identify said features and gain further insights into neural network representations? To answer these questions, we take a construction-based approach and re-contextualize the popular matching pursuits (MP) algorithm from sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a sequence of residual-guided steps, allowing it to capture hierarchical and nonlinearly accessible features. Comparing this architecture with existing SAEs on a mixture of synthetic and natural data settings, we show: (i) hierarchical concepts induce conditionally orthogonal features, which existing SAEs are unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE recovers highly meaningful features, helping us unravel shared structure in the seemingly dichotomous representation spaces of different modalities in a vision-language model, hence demonstrating the assumption that useful features are solely linearly accessible is insufficient. We also show that the sequential encoder principle of MP-SAE affords an additional benefit of adaptive sparsity at inference time, which may be of independent interest. Overall, we argue our results provide credence to the idea that interpretability should begin with the phenomenology of representations, with methods emerging from assumptions that fit it."
      },
      {
        "id": "oai:arXiv.org:2506.03096v1",
        "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
        "link": "https://arxiv.org/abs/2506.03096",
        "author": "Christian Schlarmann, Francesco Croce, Nicolas Flammarion, Matthias Hein",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03096v1 Announce Type: new \nAbstract: Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into a single feature vector. As a remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use a single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks."
      },
      {
        "id": "oai:arXiv.org:2506.03097v1",
        "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding",
        "link": "https://arxiv.org/abs/2506.03097",
        "author": "Ashwin Vinod, Shrey Pandit, Aditya Vavre, Linshen Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03097v1 Announce Type: new \nAbstract: Emerging embodied AI applications, such as wearable cameras and autonomous agents, have underscored the need for robust reasoning from first person video streams. We introduce EgoVLM, a vision-language model specifically designed to integrate visual comprehension and spatial-temporal reasoning within egocentric video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method adapted to align model outputs with human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly tune using RL without any supervised fine-tuning phase on chain-of-thought (CoT) data. We evaluate EgoVLM on egocentric video question answering benchmarks and show that domain-specific training substantially improves performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By explicitly generating reasoning traces, EgoVLM enhances interpretability, making it well-suited for downstream applications. Furthermore, we introduce a novel keyframe-based reward that incorporates salient frame selection to guide reinforcement learning optimization. This reward formulation opens a promising avenue for future exploration in temporally grounded egocentric reasoning."
      },
      {
        "id": "oai:arXiv.org:2506.03100v1",
        "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds",
        "link": "https://arxiv.org/abs/2506.03100",
        "author": "Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03100v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA."
      },
      {
        "id": "oai:arXiv.org:2506.03101v1",
        "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales",
        "link": "https://arxiv.org/abs/2506.03101",
        "author": "Jonas F. Lotz, Ant\\'onio V. Lopes, Stephan Peitz, Hendra Setiawan, Leonardo Emili",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03101v1 Announce Type: new \nAbstract: The choice of tokenizer can profoundly impact language model performance, yet accessible and reliable evaluations of tokenizer quality remain an open challenge. Inspired by scaling consistency, we show that smaller models can accurately predict significant differences in tokenizer impact on larger models at a fraction of the compute cost. By systematically evaluating both English-centric and multilingual tokenizers, we find that tokenizer choice has negligible effects on tasks in English but results in consistent performance differences in multilingual settings. We propose new intrinsic tokenizer metrics inspired by Zipf's law that correlate more strongly with downstream performance than text compression when modeling unseen languages. By combining several metrics to capture multiple aspects of tokenizer behavior, we develop a reliable framework for intrinsic tokenizer evaluations. Our work offers a more efficient path to informed tokenizer selection in future language model development."
      },
      {
        "id": "oai:arXiv.org:2506.03103v1",
        "title": "DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation",
        "link": "https://arxiv.org/abs/2506.03103",
        "author": "Xiaoyan Cong, Angela Xing, Chandradeep Pokhariya, Rao Fu, Srinath Sridhar",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03103v1 Announce Type: new \nAbstract: Reconstructing dynamic hand-object contacts is essential for realistic manipulation in AI character animation, XR, and robotics, yet it remains challenging due to heavy occlusions, complex surface details, and limitations in existing capture techniques. In this paper, we introduce DyTact, a markerless capture method for accurately capturing dynamic contact in hand-object manipulations in a non-intrusive manner. Our approach leverages a dynamic, articulated representation based on 2D Gaussian surfels to model complex manipulations. By binding these surfels to MANO meshes, DyTact harnesses the inductive bias of template models to stabilize and accelerate optimization. A refinement module addresses time-dependent high-frequency deformations, while a contact-guided adaptive sampling strategy selectively increases surfel density in contact regions to handle heavy occlusion. Extensive experiments demonstrate that DyTact not only achieves state-of-the-art dynamic contact estimation accuracy but also significantly improves novel view synthesis quality, all while operating with fast optimization and efficient memory usage. Project Page: https://oliver-cong02.github.io/DyTact.github.io/ ."
      },
      {
        "id": "oai:arXiv.org:2506.03105v1",
        "title": "Detecting Patterns of Interaction in Temporal Hypergraphs via Edge Clustering",
        "link": "https://arxiv.org/abs/2506.03105",
        "author": "Ryan DeWolfe, Fran\\c{c}ois Th\\'eberge",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03105v1 Announce Type: new \nAbstract: Finding densely connected subsets of vertices in an unsupervised setting, called clustering or community detection, is one of the fundamental problems in network science. The edge clustering approach instead detects communities by clustering the edges of the graph and then assigning a vertex to a community if it has at least one edge in that community, thereby allowing for overlapping clusters of vertices. We apply the idea behind edge clustering to temporal hypergraphs, an extension of a graph where a single edge can contain any number of vertices and each edge has a timestamp. Extending to hypergraphs allows for many different patterns of interaction between edges, and by defining a suitable structural similarity function, our edge clustering algorithm can find clusters of these patterns. We test the algorithm with three structural similarity functions on a large collaboration hypergraph, and find intuitive cluster structures that could prove useful for downstream tasks."
      },
      {
        "id": "oai:arXiv.org:2506.03106v1",
        "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
        "link": "https://arxiv.org/abs/2506.03106",
        "author": "Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03106v1 Announce Type: new \nAbstract: Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration."
      },
      {
        "id": "oai:arXiv.org:2506.03107v1",
        "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions",
        "link": "https://arxiv.org/abs/2506.03107",
        "author": "Di Chang, Mingdeng Cao, Yichun Shi, Bo Liu, Shengqu Cai, Shijie Zhou, Weilin Huang, Gordon Wetzstein, Mohammad Soleymani, Peng Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03107v1 Announce Type: new \nAbstract: Editing images with instructions to reflect non-rigid motions, camera viewpoint shifts, object deformations, human articulations, and complex interactions, poses a challenging yet underexplored problem in computer vision. Existing approaches and datasets predominantly focus on static scenes or rigid transformations, limiting their capacity to handle expressive edits involving dynamic motion. To address this gap, we introduce ByteMorph, a comprehensive framework for instruction-based image editing with an emphasis on non-rigid motions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong baseline model built upon the Diffusion Transformer (DiT), named ByteMorpher. ByteMorph-6M includes over 6 million high-resolution image editing pairs for training, along with a carefully curated evaluation benchmark ByteMorph-Bench. Both capture a wide variety of non-rigid motion types across diverse environments, human figures, and object categories. The dataset is constructed using motion-guided data generation, layered compositing techniques, and automated captioning to ensure diversity, realism, and semantic coherence. We further conduct a comprehensive evaluation of recent instruction-based image editing methods from both academic and commercial domains."
      },
      {
        "id": "oai:arXiv.org:2506.03109v1",
        "title": "On Weak-to-Strong Generalization and f-Divergence",
        "link": "https://arxiv.org/abs/2506.03109",
        "author": "Wei Yao, Gengze Xu, Huayi Tang, Wenkai Yang, Donglin Di, Ziqiao Wang, Yong Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03109v1 Announce Type: new \nAbstract: Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for stimulating the capabilities of strong pre-trained models by leveraging supervision from weaker supervisors. To improve the performance of the strong model, existing methods often require additional weak models or complex procedures, leading to substantial computational and memory overhead. Motivated by the effectiveness of $f$-divergence loss in various machine learning domains, we introduce $f$-divergence as an information-theoretic loss function framework in W2SG. Our theoretical analysis reveals fundamental limitations and equivalence of different $f$-divergence losses in W2SG, supported by sample complexity bounds and information-theoretic insights. We empirically demonstrate that $f$-divergence loss, which generalizes widely-used metrics like KL divergence, effectively improves generalization and noise tolerance of the strong model in practice."
      },
      {
        "id": "oai:arXiv.org:2506.03110v1",
        "title": "Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning",
        "link": "https://arxiv.org/abs/2506.03110",
        "author": "Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03110v1 Announce Type: new \nAbstract: Vision Transformer (ViT) has achieved remarkable success due to its large-scale pretraining on general domains, but it still faces challenges when applying it to downstream distant domains that have only scarce training data, which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired by Self-Attention's insensitivity to token orders, we find an interesting phenomenon neglected in current works: disrupting the continuity of image tokens (i.e., making pixels not smoothly transited across patches) in ViT leads to a noticeable performance decline in the general (source) domain but only a marginal decrease in downstream target domains. This questions the role of image tokens' continuity in ViT's generalization under large domain gaps. In this paper, we delve into this phenomenon for an interpretation. We find continuity aids ViT in learning larger spatial patterns, which are harder to transfer than smaller ones, enlarging domain distances. Meanwhile, it implies that only smaller patterns within each patch could be transferred under extreme domain gaps. Based on this interpretation, we further propose a simple yet effective method for CDFSL that better disrupts the continuity of image tokens, encouraging the model to rely less on large patterns and more on smaller ones. Extensive experiments show the effectiveness of our method in reducing domain gaps and outperforming state-of-the-art works. Codes and models are available at https://github.com/shuaiyi308/ReCIT."
      },
      {
        "id": "oai:arXiv.org:2506.03111v1",
        "title": "Rectified Flows for Fast Multiscale Fluid Flow Modeling",
        "link": "https://arxiv.org/abs/2506.03111",
        "author": "Victor Armegioiu, Yannick Ramic, Siddhartha Mishra",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03111v1 Announce Type: new \nAbstract: The statistical modeling of fluid flows is very challenging due to their multiscale dynamics and extreme sensitivity to initial conditions. While recently proposed conditional diffusion models achieve high fidelity, they typically require hundreds of stochastic sampling steps at inference. We introduce a rectified flow framework that learns a time-dependent velocity field, transporting input to output distributions along nearly straight trajectories. By casting sampling as solving an ordinary differential equation (ODE) along this straighter flow field, our method makes each integration step much more effective, using as few as eight steps versus (more than) 128 steps in standard score-based diffusion, without sacrificing predictive fidelity. Experiments on challenging multiscale flow benchmarks show that rectified flows recover the same posterior distributions as diffusion models, preserve fine-scale features that MSE-trained baselines miss, and deliver high-resolution samples in a fraction of inference time."
      },
      {
        "id": "oai:arXiv.org:2506.03114v1",
        "title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery",
        "link": "https://arxiv.org/abs/2506.03114",
        "author": "Michelle Chen, David Russell, Amritha Pallavoor, Derek Young, Jane Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03114v1 Announce Type: new \nAbstract: Large-scale delineation of individual trees from remote sensing imagery is crucial to the advancement of ecological research, particularly as climate change and other environmental factors rapidly transform forest landscapes across the world. Current RGB tree segmentation methods rely on training specialized machine learning models with labeled tree datasets. While these learning-based approaches can outperform manual data collection when accurate, the existing models still depend on training data that's hard to scale. In this paper, we investigate the efficacy of using a state-of-the-art image segmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for individual tree detection and segmentation. We evaluate a pretrained SAM2 model on two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot transfer by using predictions from an existing tree detection model as prompts. Our results suggest that SAM2 not only has impressive generalization capabilities, but also can form a natural synergy with specialized methods trained on in-domain labeled data. We find that applying large pretrained models to problems in remote sensing is a promising avenue for future progress. We make our code available at: https://github.com/open-forest-observatory/tree-detection-framework."
      },
      {
        "id": "oai:arXiv.org:2506.03117v1",
        "title": "Targeted Forgetting of Image Subgroups in CLIP Models",
        "link": "https://arxiv.org/abs/2506.03117",
        "author": "Zeliang Zhang, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Chenliang Xu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03117v1 Announce Type: new \nAbstract: Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot performance across various tasks by leveraging large-scale, unsupervised pre-training. However, they often inherit harmful or unwanted knowledge from noisy internet-sourced datasets, compromising their reliability in real-world applications. Existing model unlearning methods either rely on access to pre-trained datasets or focus on coarse-grained unlearning (e.g., entire classes), leaving a critical gap for fine-grained unlearning. In this paper, we address the challenging scenario of selectively forgetting specific portions of knowledge within a class, without access to pre-trained data, while preserving the model's overall performance. We propose a novel three-stage approach that progressively unlearns targeted knowledge while mitigating over-forgetting. It consists of (1) a forgetting stage to fine-tune the CLIP on samples to be forgotten, (2) a reminding stage to restore performance on retained samples, and (3) a restoring stage to recover zero-shot capabilities using model souping. Additionally, we introduce knowledge distillation to handle the distribution disparity between forgetting, retaining samples, and unseen pre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style datasets demonstrate that our approach effectively unlearns specific subgroups while maintaining strong zero-shot performance on semantically similar subgroups and other categories, significantly outperforming baseline unlearning methods, which lose effectiveness under the CLIP unlearning setting."
      },
      {
        "id": "oai:arXiv.org:2506.03119v1",
        "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior",
        "link": "https://arxiv.org/abs/2506.03119",
        "author": "Zujin Guo, Size Wu, Zhongang Cai, Wei Li, Chen Change Loy",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03119v1 Announce Type: new \nAbstract: Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over the synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human guidance signals into the diffusion process for Controllable Human-centric Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for interpolation, our PoseFuse3D, a 3D-informed control model, features a novel SMPL-X encoder that transforms 3D geometry and shape into the 2D latent conditioning space, alongside a fusion network that integrates these 3D cues with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset annotated with both 2D poses and 3D SMPL-X parameters. We show that PoseFuse3D-KI consistently outperforms state-of-the-art baselines on CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS. Comprehensive ablations demonstrate that our PoseFuse3D model improves interpolation fidelity."
      },
      {
        "id": "oai:arXiv.org:2506.03122v1",
        "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation",
        "link": "https://arxiv.org/abs/2506.03122",
        "author": "Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03122v1 Announce Type: new \nAbstract: Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design."
      },
      {
        "id": "oai:arXiv.org:2506.03123v1",
        "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation",
        "link": "https://arxiv.org/abs/2506.03123",
        "author": "Zhengyao Lv, Chenyang Si, Tianlin Pan, Zhaoxi Chen, Kwan-Yee K. Wong, Yu Qiao, Ziwei Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03123v1 Announce Type: new \nAbstract: Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details. To address this issue, we propose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)}, where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert.Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at \\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}."
      },
      {
        "id": "oai:arXiv.org:2506.03126v1",
        "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation",
        "link": "https://arxiv.org/abs/2506.03126",
        "author": "Lu Qiu, Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, Xihui Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03126v1 Announce Type: new \nAbstract: Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation."
      },
      {
        "id": "oai:arXiv.org:2506.03128v1",
        "title": "Zero-Shot Time Series Forecasting with Covariates via In-Context Learning",
        "link": "https://arxiv.org/abs/2506.03128",
        "author": "Andreas Auer, Raghul Parthipan, Pedro Mercado, Abdul Fatir Ansari, Lorenzo Stella, Bernie Wang, Michael Bohlke-Schneider, Syama Sundar Rangapuram",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03128v1 Announce Type: new \nAbstract: Pretrained time series models, capable of zero-shot forecasting, have demonstrated significant potential in enhancing both the performance and accessibility of time series forecasting. However, existing pretrained models either do not support covariates or fail to incorporate them effectively. We introduce COSMIC, a zero-shot forecasting model that utilizes covariates via in-context learning. To address the challenge of data scarcity, we propose Informative Covariate Augmentation, which enables the training of COSMIC without requiring any datasets that include covariates. COSMIC achieves state-of-the-art performance in zero-shot forecasting, both with and without covariates. Our quantitative and qualitative analysis demonstrates that COSMIC effectively leverages covariates in zero-shot forecasting."
      },
      {
        "id": "oai:arXiv.org:2506.03131v1",
        "title": "Native-Resolution Image Synthesis",
        "link": "https://arxiv.org/abs/2506.03131",
        "author": "Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, Yiyuan Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03131v1 Announce Type: new \nAbstract: We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies."
      },
      {
        "id": "oai:arXiv.org:2506.03133v1",
        "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation",
        "link": "https://arxiv.org/abs/2506.03133",
        "author": "Kai Lion, Liang Zhang, Bingcong Li, Niao He",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03133v1 Announce Type: new \nAbstract: We show that low-rank adaptation of large-scale models suffers from a low stable rank that is well below the linear algebraic rank of the subspace, degrading fine-tuning performance. To mitigate the underutilization of the allocated subspace, we propose PoLAR, a parameterization inspired by the polar decomposition that factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Pairing the parameterization with Riemannian optimization leads to consistent gains on three different benchmarks testing general language understanding, commonsense reasoning, and mathematical problem solving with base model sizes ranging from 350M to 27B."
      },
      {
        "id": "oai:arXiv.org:2506.03135v1",
        "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models",
        "link": "https://arxiv.org/abs/2506.03135",
        "author": "Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, Li Yi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03135v1 Announce Type: new \nAbstract: Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research."
      },
      {
        "id": "oai:arXiv.org:2506.03136v1",
        "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.03136",
        "author": "Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03136v1 Announce Type: new \nAbstract: We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE"
      },
      {
        "id": "oai:arXiv.org:2506.03139v1",
        "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
        "link": "https://arxiv.org/abs/2506.03139",
        "author": "Siqi Chen, Xinyu Dong, Haolei Xu, Xingyu Wu, Fei Tang, Hang Zhang, Yuchen Yan, Linjuan Wu, Wenqi Zhang, Guiyang Hou, Yongliang Shen, Weiming Lu, Yueting Zhuang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03139v1 Announce Type: new \nAbstract: Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius."
      },
      {
        "id": "oai:arXiv.org:2506.03140v1",
        "title": "CamCloneMaster: Enabling Reference-based Camera Control for Video Generation",
        "link": "https://arxiv.org/abs/2506.03140",
        "author": "Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Tianfan Xue",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03140v1 Announce Type: new \nAbstract: Camera control is crucial for generating expressive and cinematic videos. Existing methods rely on explicit sequences of camera parameters as control conditions, which can be cumbersome for users to construct, particularly for intricate camera movements. To provide a more intuitive camera control method, we propose CamCloneMaster, a framework that enables users to replicate camera movements from reference videos without requiring camera parameters or test-time fine-tuning. CamCloneMaster seamlessly supports reference-based camera control for both Image-to-Video and Video-to-Video tasks within a unified framework. Furthermore, we present the Camera Clone Dataset, a large-scale synthetic dataset designed for camera clone learning, encompassing diverse scenes, subjects, and camera movements. Extensive experiments and user studies demonstrate that CamCloneMaster outperforms existing methods in terms of both camera controllability and visual quality."
      },
      {
        "id": "oai:arXiv.org:2506.03141v1",
        "title": "Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval",
        "link": "https://arxiv.org/abs/2506.03141",
        "author": "Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03141v1 Announce Type: new \nAbstract: Recent advances in interactive video generation have shown promising results, yet existing approaches struggle with scene-consistent memory capabilities in long video generation due to limited use of historical context. In this work, we propose Context-as-Memory, which utilizes historical context as memory for video generation. It includes two simple yet effective designs: (1) storing context in frame format without additional post-processing; (2) conditioning by concatenating context and frames to be predicted along the frame dimension at the input, requiring no external control modules. Furthermore, considering the enormous computational overhead of incorporating all historical context, we propose the Memory Retrieval module to select truly relevant context frames by determining FOV (Field of View) overlap between camera poses, which significantly reduces the number of candidate frames without substantial information loss. Experiments demonstrate that Context-as-Memory achieves superior memory capabilities in interactive long video generation compared to SOTAs, even generalizing effectively to open-domain scenarios not seen during training. The link of our project page is https://context-as-memory.github.io/."
      },
      {
        "id": "oai:arXiv.org:2506.03142v1",
        "title": "Not All Tokens Are Meant to Be Forgotten",
        "link": "https://arxiv.org/abs/2506.03142",
        "author": "Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Douglas Zytko, Prashant Khanduri, Dongxiao Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03142v1 Announce Type: new \nAbstract: Large Language Models (LLMs), pre-trained on massive text corpora, exhibit remarkable human-level language understanding, reasoning, and decision-making abilities. However, they tend to memorize unwanted information, such as private or copyrighted content, raising significant privacy and legal concerns. Unlearning has emerged as a promising solution, but existing methods face a significant challenge of over-forgetting. This issue arises because they indiscriminately suppress the generation of all the tokens in forget samples, leading to a substantial loss of model utility. To overcome this challenge, we introduce the Targeted Information Forgetting (TIF) framework, which consists of (1) a flexible targeted information identifier designed to differentiate between unwanted words (UW) and general words (GW) in the forget samples, and (2) a novel Targeted Preference Optimization approach that leverages Logit Preference Loss to unlearn unwanted information associated with UW and Preservation Loss to retain general information in GW, effectively improving the unlearning process while mitigating utility degradation. Extensive experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF framework enhances unlearning effectiveness while preserving model utility and achieving state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2506.03143v1",
        "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
        "link": "https://arxiv.org/abs/2506.03143",
        "author": "Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, Si Qin, Lars Liden, Qingwei Lin, Huan Zhang, Tong Zhang, Jianbing Zhang, Dongmei Zhang, Jianfeng Gao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03143v1 Announce Type: new \nAbstract: One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated  token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths."
      },
      {
        "id": "oai:arXiv.org:2506.03144v1",
        "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query",
        "link": "https://arxiv.org/abs/2506.03144",
        "author": "Wei Chow, Yuan Gao, Linfeng Li, Xian Wang, Qi Xu, Hang Song, Lingdong Kong, Ran Zhou, Yi Zeng, Yidong Cai, Botian Jiang, Shilin Xu, Jiajun Zhang, Minghui Qiu, Xiangtai Li, Tianshu Yang, Siliang Tang, Juncheng Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03144v1 Announce Type: new \nAbstract: Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval."
      },
      {
        "id": "oai:arXiv.org:2506.03145v1",
        "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM",
        "link": "https://arxiv.org/abs/2506.03145",
        "author": "Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03145v1 Announce Type: new \nAbstract: Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources, but existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches, and the results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. It achieves an F1 score of 0.84 for entity extraction, and the knowledge obtained from the KG improves answers to over 54% of the questions."
      },
      {
        "id": "oai:arXiv.org:2506.03147v1",
        "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation",
        "link": "https://arxiv.org/abs/2506.03147",
        "author": "Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03147v1 Announce Type: new \nAbstract: Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets."
      },
      {
        "id": "oai:arXiv.org:2506.03148v1",
        "title": "Self-Supervised Spatial Correspondence Across Modalities",
        "link": "https://arxiv.org/abs/2506.03148",
        "author": "Ayush Shrivastava, Andrew Owens",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03148v1 Announce Type: new \nAbstract: We present a method for finding cross-modal space-time correspondences. Given two images from different visual modalities, such as an RGB image and a depth map, our model identifies which pairs of pixels correspond to the same physical points in the scene. To solve this problem, we extend the contrastive random walk framework to simultaneously learn cycle-consistent feature representations for both cross-modal and intra-modal matching. The resulting model is simple and has no explicit photo-consistency assumptions. It can be trained entirely using unlabeled data, without the need for any spatially aligned multimodal image pairs. We evaluate our method on both geometric and semantic correspondence tasks. For geometric matching, we consider challenging tasks such as RGB-to-depth and RGB-to-thermal matching (and vice versa); for semantic matching, we evaluate on photo-sketch and cross-style image alignment. Our method achieves strong performance across all benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.03149v1",
        "title": "Causal Estimation of Tokenisation Bias",
        "link": "https://arxiv.org/abs/2506.03149",
        "author": "Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03149v1 Announce Type: new \nAbstract: Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser -- which maps character-strings to subwords -- should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as tokenisation bias. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the probability a trained model assigns to the corresponding characters (i.e., \\textit{``hello''}). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models' outputs across scales, vocabularies, and tokenisers. Notably, a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling."
      },
      {
        "id": "oai:arXiv.org:2506.03150v1",
        "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation",
        "link": "https://arxiv.org/abs/2506.03150",
        "author": "Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Ronald Clark, Ming-Hsuan Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03150v1 Announce Type: new \nAbstract: Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page"
      },
      {
        "id": "oai:arXiv.org:2505.21777v1",
        "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory",
        "link": "https://arxiv.org/abs/2505.21777",
        "author": "Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21777v1 Announce Type: cross \nAbstract: Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load $- spurious\\,\\,states$, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models."
      },
      {
        "id": "oai:arXiv.org:2506.01704v1",
        "title": "Generate, Not Recommend: Personalized Multimodal Content Generation",
        "link": "https://arxiv.org/abs/2506.01704",
        "author": "Jiongnan Liu, Zhicheng Dou, Ning Hu, Chenyan Xiong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01704v1 Announce Type: cross \nAbstract: To address the challenge of information overload from massive web contents, recommender systems are widely applied to retrieve and present personalized results for users. However, recommendation tasks are inherently constrained to filtering existing items and lack the ability to generate novel concepts, limiting their capacity to fully satisfy user demands and preferences. In this paper, we propose a new paradigm that goes beyond content filtering and selecting: directly generating personalized items in a multimodal form, such as images, tailored to individual users. To accomplish this, we leverage any-to-any Large Multimodal Models (LMMs) and train them in both supervised fine-tuning and online reinforcement learning strategy to equip them with the ability to yield tailored next items for users. Experiments on two benchmark datasets and user study confirm the efficacy of the proposed method. Notably, the generated images not only align well with users' historical preferences but also exhibit relevance to their potential future interests."
      },
      {
        "id": "oai:arXiv.org:2506.01969v1",
        "title": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs",
        "link": "https://arxiv.org/abs/2506.01969",
        "author": "Pencuo Zeren, Qiuming Luo, Rui Mao, Chang Kong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01969v1 Announce Type: cross \nAbstract: Efficient inference of Multi-Head Latent Attention (MLA) is challenged by deploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper introduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the single-instance deployment scenario on NVIDIA H20 GPUs. We propose the Efficient Transpose Attention Pipeline (ETAP), which reconfigures attention computation through transposition to align the KV context length with the \\(M\\)-dimension in WGMMA operations, significantly reducing redundant computations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K sequence length (batch size 16), with 5.24x and 4.94x improvements over FlashAttention-3 and FlashInfer, respectively, while maintaining numerical stability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than FlashAttention-3. Furthermore, ETAP's design enables seamless integration into frameworks like FlashAttention-3 and FlashInfer, supported by a detailed theoretical analysis. Our work addresses a critical gap in resource-constrained inference, offering a scalable solution for mid-tier GPUs and paving the way for broader adoption in hardware-aware optimization. Code is available at https://github.com/pengcuo/FlashMLA-ETAP."
      },
      {
        "id": "oai:arXiv.org:2506.01980v1",
        "title": "Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance",
        "link": "https://arxiv.org/abs/2506.01980",
        "author": "Lianhao Yin, Ozanan Meireles, Guy Rosman, Daniela Rus",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01980v1 Announce Type: cross \nAbstract: Real-time video understanding is critical to guide procedures in minimally invasive surgery (MIS). However, supervised learning approaches require large, annotated datasets that are scarce due to annotation efforts that are prohibitive, e.g., in medical fields. Although self-supervision methods can address such limitations, current self-supervised methods often fail to capture structural and physical information in a form that generalizes across tasks. We propose Compress-to-Explore (C2E), a novel self-supervised framework that leverages Kolmogorov complexity to learn compact, informative representations from surgical videos. C2E uses entropy-maximizing decoders to compress images while preserving clinically relevant details, improving encoder performance without labeled data. Trained on large-scale unlabeled surgical datasets, C2E demonstrates strong generalization across a variety of surgical ML tasks, such as workflow classification, tool-tissue interaction classification, segmentation, and diagnosis tasks, providing improved performance as a surgical visual foundation model. As we further show in the paper, the model's internal compact representation better disentangles features from different structural parts of images. The resulting performance improvements highlight the yet untapped potential of self-supervised learning to enhance surgical AI and improve outcomes in MIS."
      },
      {
        "id": "oai:arXiv.org:2506.01998v1",
        "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents",
        "link": "https://arxiv.org/abs/2506.01998",
        "author": "Takao Fujii, Katie Seaborn, Madeleine Steeds, Jun Kato",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01998v1 Announce Type: cross \nAbstract: Conversational agents that mimic people have raised questions about the ethics of anthropomorphizing machines with human social identity cues. Critics have also questioned assumptions of identity neutrality in humanlike agents. Recent work has revealed that intersectional Japanese pronouns can elicit complex and sometimes evasive impressions of agent identity. Yet, the role of other \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially expressive medium remains unexplored. In a crowdsourcing study, Japanese participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and Ember) using seven self-referents. We found strong evidence of voice gendering alongside the potential of intersectional self-referents to evade gendering, i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age and formality intersected with gendering as per sociolinguistic theories, especially boku and watakushi. This work provides a nuanced take on agent identity perceptions and champions intersectional and culturally-sensitive work on voice agents."
      },
      {
        "id": "oai:arXiv.org:2506.02002v1",
        "title": "Machine Learning for Consistency Violation Faults Analysis",
        "link": "https://arxiv.org/abs/2506.02002",
        "author": "Kamal Giri, Amit Garu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02002v1 Announce Type: cross \nAbstract: Distributed systems frequently encounter consistency violation faults (cvfs), where nodes operate on outdated or inaccurate data, adversely affecting convergence and overall system performance. This study presents a machine learning-based approach for analyzing the impact of CVFs, using Dijkstra's Token Ring problem as a case study. By computing program transition ranks and their corresponding effects, the proposed method quantifies the influence of cvfs on system behavior. To address the state space explosion encountered in larger graphs, two models are implemented: a Feedforward Neural Network (FNN) and a distributed neural network leveraging TensorFlow's \\texttt{tf.distribute} API. These models are trained on datasets generated from smaller graphs (3 to 10 nodes) to predict parameters essential for determining rank effects. Experimental results demonstrate promising performance, with a test loss of 4.39 and a mean absolute error of 1.5. Although distributed training on a CPU did not yield significant speed improvements over a single-device setup, the findings suggest that scalability could be enhanced through the use of advanced hardware accelerators such as GPUs or TPUs."
      },
      {
        "id": "oai:arXiv.org:2506.02006v1",
        "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
        "link": "https://arxiv.org/abs/2506.02006",
        "author": "Zhaoyuan Su, Tingfeng Lan, Zirui Wang, Juncheng Yang, Yue Cheng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02006v1 Announce Type: cross \nAbstract: Efficiently serving large language models (LLMs) under dynamic and bursty workloads remains a key challenge for real-world deployment. Existing serving frameworks and static model compression techniques fail to adapt to workload fluctuations, leading to either service-level objective (SLO) violations under full-precision serving or persistent accuracy degradation with static quantization. We present MorphServe, a dynamic, workload-aware LLM serving framework based on morphological adaptation. MorphServe introduces two asynchronous, token-level runtime mechanisms: quantized layer swapping, which selectively replaces less impactful layers with quantized alternatives during high-load periods, and pressure-aware KV cache resizing, which dynamically adjusts KV cache capacity in response to memory pressure. These mechanisms enable state-preserving transitions with minimum runtime overhead and are fully compatible with modern scheduling and attention techniques. Extensive experiments on Vicuna and Llama family models with real-world workloads demonstrate that MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality. These results establish MorphServe as a practical and elastic solution for LLM deployment in dynamic environments."
      },
      {
        "id": "oai:arXiv.org:2506.02023v1",
        "title": "DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials",
        "link": "https://arxiv.org/abs/2506.02023",
        "author": "Kevin Han, Bowen Deng, Amir Barati Farimani, Gerbrand Ceder",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02023v1 Announce Type: cross \nAbstract: Large-scale atomistic simulations are essential to bridge computational materials and chemistry to realistic materials and drug discovery applications. In the past few years, rapid developments of machine learning interatomic potentials (MLIPs) have offered a solution to scale up quantum mechanical calculations. Parallelizing these interatomic potentials across multiple devices poses a challenging, but promising approach to further extending simulation scales to real-world applications. In this work, we present DistMLIP, an efficient distributed inference platform for MLIPs based on zero-redundancy, graph-level parallelization. In contrast to conventional space-partitioning parallelization, DistMLIP enables efficient MLIP parallelization through graph partitioning, allowing multi-device inference on flexible MLIP model architectures like multi-layer graph neural networks. DistMLIP presents an easy-to-use, flexible, plug-in interface that enables distributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four widely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We show that existing foundational potentials can perform near-million-atom calculations at the scale of a few seconds on 8 GPUs with DistMLIP."
      },
      {
        "id": "oai:arXiv.org:2506.02038v1",
        "title": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy Critical and Time Sensitive Environment",
        "link": "https://arxiv.org/abs/2506.02038",
        "author": "Anum Nawaz, Hafiz Humza Mahmood Ramzan, Xianjia Yu, Zhuo Zou, Tomi Westerlund",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02038v1 Announce Type: cross \nAbstract: Edge Intelligence (EI) serves as a critical enabler for privacy-preserving systems by providing AI-empowered computation and distributed caching services at the edge, thereby minimizing latency and enhancing data privacy. The integration of blockchain technology further augments EI frameworks by ensuring transactional transparency, auditability, and system-wide reliability through a decentralized network model. However, the operational architecture of such systems introduces inherent vulnerabilities, particularly due to the extensive data interactions between edge gateways (EGs) and the distributed nature of information storage during service provisioning. To address these challenges, we propose an autonomous computing model along with its interaction topologies tailored for privacy-critical and time-sensitive health applications. The system supports continuous monitoring, real-time alert notifications, disease detection, and robust data processing and aggregation. It also includes a data transaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a resource-efficient one-dimensional convolutional neural network (1D-CNN) is proposed for the multiclass classification of arrhythmia, enabling accurate and real-time analysis of constrained EGs. Furthermore, a secure access scheme is defined to manage both off-chain and on-chain data sharing and storage. To validate the proposed model, comprehensive security, performance, and cost analyses are conducted, demonstrating the efficiency and reliability of the fine-grained access control scheme."
      },
      {
        "id": "oai:arXiv.org:2506.02044v1",
        "title": "A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder",
        "link": "https://arxiv.org/abs/2506.02044",
        "author": "Xinxu Wei, Kanhao Zhao, Yong Jiao, Lifang He, Yu Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02044v1 Announce Type: cross \nAbstract: As large language models (LLMs) continue to revolutionize AI research, there is a growing interest in building large-scale brain foundation models to advance neuroscience. While most existing brain foundation models are pre-trained on time-series signals or region-of-interest (ROI) features, we propose a novel graph-based pre-training paradigm for constructing a brain graph foundation model. In this paper, we introduce the Brain Graph Foundation Model, termed BrainGFM, a unified framework that leverages graph contrastive learning and graph masked autoencoders for large-scale fMRI-based pre-training. BrainGFM is pre-trained on a diverse mixture of brain atlases with varying parcellations, significantly expanding the pre-training corpus and enhancing the model's ability to generalize across heterogeneous fMRI-derived brain representations. To support efficient and versatile downstream transfer, we integrate both graph prompts and language prompts into the model design, enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological and psychiatric disorders, and task settings. Furthermore, we employ meta-learning to optimize the graph prompts, facilitating strong generalization to previously unseen disorders under both few-shot and zero-shot learning conditions via language-guided prompting. BrainGFM is pre-trained on 27 neuroimaging datasets spanning 25 common neurological and psychiatric disorders, encompassing 2 types of brain atlases (functional and anatomical) across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000 fMRI scans, and a total of 400,000 graph samples aggregated across all atlases and parcellations. The code is available at: https://github.com/weixinxu666/BrainGFM"
      },
      {
        "id": "oai:arXiv.org:2506.02051v1",
        "title": "Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders",
        "link": "https://arxiv.org/abs/2506.02051",
        "author": "Hui Liu, Shiye Tian, Xuejun Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02051v1 Announce Type: cross \nAbstract: The de novo generation of drug-like molecules capable of inducing desirable phenotypic changes is receiving increasing attention. However, previous methods predominantly rely on expression profiles to guide molecule generation, but overlook the perturbative effect of the molecules on cellular contexts. To overcome this limitation, we propose SmilesGEN, a novel generative model based on variational autoencoder (VAE) architecture to generate molecules with potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE (SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the interplay between drug perturbations and transcriptional responses in a common latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment expression profiles when eliminating drug-induced perturbations in the latent space, while SmilesNet is informed by desired expression profiles to generate drug-like molecules. Our empirical experiments demonstrate that SmilesGEN outperforms current state-of-the-art models in generating molecules with higher degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity to known ligands targeting the relevant proteins. Moreover, we evaluate SmilesGEN for scaffold-based molecule optimization and generation of therapeutic agents, and confirmed its superior performance in generating molecules with higher similarity to approved drugs. SmilesGEN establishes a robust framework that leverages gene signatures to generate drug-like molecules that hold promising potential to induce desirable cellular phenotypic changes."
      },
      {
        "id": "oai:arXiv.org:2506.02052v1",
        "title": "Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications",
        "link": "https://arxiv.org/abs/2506.02052",
        "author": "Shuo Yan, Yuliang Yan, Bin Ma, Chenao Li, Haochun Tang, Jiahua Lu, Minhua Lin, Yuyuan Feng, Hui Xiong, Enyan Dai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02052v1 Announce Type: cross \nAbstract: Recently, extensive deep learning architectures and pretraining strategies have been explored to support downstream protein applications. Additionally, domain-specific models incorporating biological knowledge have been developed to enhance performance in specialized tasks. In this work, we introduce $\\textbf{Protap}$, a comprehensive benchmark that systematically compares backbone architectures, pretraining strategies, and domain-specific models across diverse and realistic downstream protein applications. Specifically, Protap covers five applications: three general tasks and two novel specialized tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted protein degradation, which are industrially relevant yet missing from existing benchmarks. For each application, Protap compares various domain-specific models and general architectures under multiple pretraining settings. Our empirical studies imply that: (i) Though large-scale pretraining encoders achieve great results, they often underperform supervised encoders trained on small downstream training sets. (ii) Incorporating structural information during downstream fine-tuning can match or even outperform protein language models pretrained on large-scale sequence corpora. (iii) Domain-specific biological priors can enhance performance on specialized downstream tasks. Code and datasets are publicly available at https://github.com/Trust-App-AI-Lab/protap."
      },
      {
        "id": "oai:arXiv.org:2506.02057v1",
        "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody",
        "link": "https://arxiv.org/abs/2506.02057",
        "author": "David Sasu, Kweku Andoh Yamoah, Benedict Quartey, Natalie Schluter",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02057v1 Announce Type: cross \nAbstract: Enabling robots to accurately interpret and execute spoken language instructions is essential for effective human-robot collaboration. Traditional methods rely on speech recognition to transcribe speech into text, often discarding crucial prosodic cues needed for disambiguating intent. We propose a novel approach that directly leverages speech prosody to infer and resolve instruction intent. Predicted intents are integrated into large language models via in-context learning to disambiguate and select appropriate task plans. Additionally, we present the first ambiguous speech dataset for robotics, designed to advance research in speech disambiguation. Our method achieves 95.79% accuracy in detecting referent intents within an utterance and determines the intended task plan of ambiguous instructions with 71.96% accuracy, demonstrating its potential to significantly improve human-robot communication."
      },
      {
        "id": "oai:arXiv.org:2506.02059v1",
        "title": "Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition",
        "link": "https://arxiv.org/abs/2506.02059",
        "author": "Ziwei Gong, Pengyuan Shi, Kaan Donbekci, Lin Ai, Run Chen, David Sasu, Zehui Wu, Julia Hirschberg",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02059v1 Announce Type: cross \nAbstract: Speech Emotion Recognition (SER) has seen significant progress with deep learning, yet remains challenging for Low-Resource Languages (LRLs) due to the scarcity of annotated data. In this work, we explore unsupervised learning to improve SER in low-resource settings. Specifically, we investigate contrastive learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised approaches to enhance cross-lingual generalization. Our methods achieve notable F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla, demonstrating their effectiveness in LRLs. Additionally, we analyze model behavior to provide insights on key factors influencing performance across languages, and also highlighting challenges in low-resource SER. This work provides a foundation for developing more inclusive, explainable, and robust emotion recognition systems for underrepresented languages."
      },
      {
        "id": "oai:arXiv.org:2506.02060v1",
        "title": "Alzheimers Disease Classification in Functional MRI With 4D Joint Temporal-Spatial Kernels in Novel 4D CNN Model",
        "link": "https://arxiv.org/abs/2506.02060",
        "author": "Javier Salazar Cavazos, Scott Peltier",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02060v1 Announce Type: cross \nAbstract: Previous works in the literature apply 3D spatial-only models on 4D functional MRI data leading to possible sub-par feature extraction to be used for downstream tasks like classification. In this work, we aim to develop a novel 4D convolution network to extract 4D joint temporal-spatial kernels that not only learn spatial information but in addition also capture temporal dynamics. Experimental results show promising performance in capturing spatial-temporal data in functional MRI compared to 3D models. The 4D CNN model improves Alzheimers disease diagnosis for rs-fMRI data, enabling earlier detection and better interventions. Future research could explore task-based fMRI applications and regression tasks, enhancing understanding of cognitive performance and disease progression."
      },
      {
        "id": "oai:arXiv.org:2506.02068v1",
        "title": "Enhancing Interpretability of Quantum-Assisted Blockchain Clustering via AI Agent-Based Qualitative Analysis",
        "link": "https://arxiv.org/abs/2506.02068",
        "author": "Yun-Cheng Tsai, Yen-Ku Liu, Samuel Yen-Chi Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02068v1 Announce Type: cross \nAbstract: Blockchain transaction data is inherently high dimensional, noisy, and entangled, posing substantial challenges for traditional clustering algorithms. While quantum enhanced clustering models have demonstrated promising performance gains, their interpretability remains limited, restricting their application in sensitive domains such as financial fraud detection and blockchain governance. To address this gap, we propose a two stage analysis framework that synergistically combines quantitative clustering evaluation with AI Agent assisted qualitative interpretation. In the first stage, we employ classical clustering methods and evaluation metrics including the Silhouette Score, Davies Bouldin Index, and Calinski Harabasz Index to determine the optimal cluster count and baseline partition quality. In the second stage, we integrate an AI Agent to generate human readable, semantic explanations of clustering results, identifying intra cluster characteristics and inter cluster relationships. Our experiments reveal that while fully trained Quantum Neural Networks (QNN) outperform random Quantum Features (QF) in quantitative metrics, the AI Agent further uncovers nuanced differences between these methods, notably exposing the singleton cluster phenomenon in QNN driven models. The consolidated insights from both stages consistently endorse the three cluster configuration, demonstrating the practical value of our hybrid approach. This work advances the interpretability frontier in quantum assisted blockchain analytics and lays the groundwork for future autonomous AI orchestrated clustering frameworks."
      },
      {
        "id": "oai:arXiv.org:2506.02071v1",
        "title": "AI Data Development: A Scorecard for the System Card Framework",
        "link": "https://arxiv.org/abs/2506.02071",
        "author": "Tadesse K. Bahiru, Haileleol Tibebu, Ioannis A. Kakadiaris",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02071v1 Announce Type: cross \nAbstract: Artificial intelligence has transformed numerous industries, from healthcare to finance, enhancing decision-making through automated systems. However, the reliability of these systems is mainly dependent on the quality of the underlying datasets, raising ongoing concerns about transparency, accountability, and potential biases. This paper introduces a scorecard designed to evaluate the development of AI datasets, focusing on five key areas from the system card framework data development life cycle: data dictionary, collection process, composition, motivation, and pre-processing. The method follows a structured approach, using an intake form and scoring criteria to assess the quality and completeness of the data set. Applied to four diverse datasets, the methodology reveals strengths and improvement areas. The results are compiled using a scoring system that provides tailored recommendations to enhance the transparency and integrity of the data set. The scorecard addresses technical and ethical aspects, offering a holistic evaluation of data practices. This approach aims to improve the quality of the data set. It offers practical guidance to curators and researchers in developing responsible AI systems, ensuring fairness and accountability in decision support systems."
      },
      {
        "id": "oai:arXiv.org:2506.02075v1",
        "title": "Stop Chasing the C-index: This Is How We Should Evaluate Our Survival Models",
        "link": "https://arxiv.org/abs/2506.02075",
        "author": "Christian Marius Lillelund, Shi-ang Qi, Russell Greiner, Christian Fischer Pedersen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02075v1 Announce Type: cross \nAbstract: We argue that many survival analysis and time-to-event models are incorrectly evaluated. First, we survey many examples of evaluation approaches in the literature and find that most rely on concordance (C-index). However, the C-index only measures a model's discriminative ability and does not assess other important aspects, such as the accuracy of the time-to-event predictions or the calibration of the model's probabilistic estimates. Next, we present a set of key desiderata for choosing the right evaluation metric and discuss their pros and cons. These are tailored to the challenges in survival analysis, such as sensitivity to miscalibration and various censoring assumptions. We hypothesize that the current development of survival metrics conforms to a double-helix ladder, and that model validity and metric validity must stand on the same rung of the assumption ladder. Finally, we discuss the appropriate methods for evaluating a survival model in practice and summarize various viewpoints opposing our analysis."
      },
      {
        "id": "oai:arXiv.org:2506.02076v1",
        "title": "A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis",
        "link": "https://arxiv.org/abs/2506.02076",
        "author": "Christian Marius Lillelund, Sanjay Kalra, Russell Greiner",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02076v1 Announce Type: cross \nAbstract: Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of motor neurons that causes progressive paralysis in patients. Current treatment options aim to prolong survival and improve quality of life; however, due to the heterogeneity of the disease, it is often difficult to determine the optimal time for potential therapies or medical interventions. In this study, we propose a novel method to predict the time until a patient with ALS experiences significant functional impairment (ALSFRS-R<=2) with respect to five common functions: speaking, swallowing, handwriting, walking and breathing. We formulate this task as a multi-event survival problem and validate our approach in the PRO-ACT dataset by training five covariate-based survival models to estimate the probability of an event over a 500-day period after a baseline visit. We then predict five event-specific individual survival distributions (ISDs) for each patient, each providing an interpretable and meaningful estimate of when that event will likely take place in the future. The results show that covariate-based models are superior to the Kaplan-Meier estimator at predicting time-to-event outcomes. Additionally, our method enables practitioners to make individual counterfactual predictions, where certain features (covariates) can be changed to see their effect on the predicted outcome. In this regard, we find that Riluzole has little to no impact on predicted functional decline. However, for patients with bulbar-onset ALS, our method predicts considerably shorter counterfactual time-to-event estimates for tasks related to speech and swallowing compared to limb-onset ALS. The proposed method can be applied to current clinical examination data to assess the risk of functional decline and thus allow more personalized treatment planning."
      },
      {
        "id": "oai:arXiv.org:2506.02082v1",
        "title": "SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction",
        "link": "https://arxiv.org/abs/2506.02082",
        "author": "Saurabh Agrawal, Raj Gohil, Gopal Kumar Agrawal, Vikram C M, Kushal Verma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02082v1 Announce Type: cross \nAbstract: Speech quality assessment is a critical process in selecting text-to-speech synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can be done using objective metrics or subjective metrics. Although there are many objective metrics like the Perceptual Evaluation of Speech Quality (PESQ), Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time Objective Intelligibility (STOI) but none of them is feasible in selecting the best model. On the other hand subjective metric like Mean Opinion Score is highly reliable but it requires a lot of manual efforts and are time-consuming. To counter the issues in MOS Evaluation, we have developed a novel model, Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a small-sized, end-to-end, highly generalized and scalable model for predicting MOS score on a scale of 5. We use the sequences of convolutions and stack them to get the latent features of the audio samples to get the best state-of-the-art results based on mean squared error (MSE), Linear Concordance Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and Kendall Rank Correlation Coefficient (KTAU)."
      },
      {
        "id": "oai:arXiv.org:2506.02083v1",
        "title": "LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention",
        "link": "https://arxiv.org/abs/2506.02083",
        "author": "Aditya Srinivas Menon, Raj Prakash Gohil, Kumud Tripathi, Pankaj Wasnik",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02083v1 Announce Type: cross \nAbstract: Speaker recognition models face challenges in multi-lingual settings due to the entanglement of linguistic information within speaker embeddings. The overlap between vocal traits such as accent, vocal anatomy, and a language's phonetic structure complicates separating linguistic and speaker information. Disentangling these components can significantly improve speaker recognition accuracy. To this end, we propose a novel disentanglement learning strategy that integrates joint learning through prefix-tuned cross-attention. This approach is particularly effective when speakers switch between languages. Experimental results show the model generalizes across monolingual and multi-lingual settings, including unseen languages. Notably, the proposed model improves the equal error rate across multiple datasets, highlighting its ability to separate language information from speaker embeddings and enhance recognition in diverse linguistic conditions."
      },
      {
        "id": "oai:arXiv.org:2506.02085v1",
        "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion",
        "link": "https://arxiv.org/abs/2506.02085",
        "author": "Ajinkya Kulkarni, Sandipana Dowerah, Tanel Alumae, Mathew Magimai. -Doss",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02085v1 Announce Type: cross \nAbstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced AI. While current research focuses on discerning real speech from spoofed speech, tracing the source system is equally crucial. This work proposes a novel audio source tracing system combining deep metric multi-class N-pair loss with Real Emphasis and Fake Dispersion framework, a Conformer classification network, and ensemble score-embedding fusion. The N-pair loss improves discriminative ability, while Real Emphasis and Fake Dispersion enhance robustness by focusing on differentiating real and fake speech patterns. The Conformer network captures both global and local dependencies in the audio signal, crucial for source tracing. The proposed ensemble score-embedding fusion shows an optimal trade-off between in-domain and out-of-domain source tracing scenarios. We evaluate our method using Frechet Distance and standard metrics, demonstrating superior performance in source tracing over the baseline system."
      },
      {
        "id": "oai:arXiv.org:2506.02088v1",
        "title": "Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025",
        "link": "https://arxiv.org/abs/2506.02088",
        "author": "Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas \\'Olives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo Galv\\~ao Filho",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02088v1 Announce Type: cross \nAbstract: Training SER models in natural, spontaneous speech is especially challenging due to the subtle expression of emotions and the unpredictable nature of real-world audio. In this paper, we present a robust system for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing on categorical emotion recognition. Our method combines state-of-the-art audio models with text features enriched by prosodic and spectral cues. In particular, we investigate the effectiveness of Fundamental Frequency (F0) quantization and the use of a pretrained audio tagging model. We also employ an ensemble model to improve robustness. On the official test set, our system achieved a Macro F1-score of 39.79% (42.20% on validation). Our results underscore the potential of these methods, and analysis of fusion techniques confirmed the effectiveness of Graph Attention Networks. Our source code is publicly available."
      },
      {
        "id": "oai:arXiv.org:2506.02090v1",
        "title": "The Impact of Software Testing with Quantum Optimization Meets Machine Learning",
        "link": "https://arxiv.org/abs/2506.02090",
        "author": "Gopichand Bandarupalli",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02090v1 Announce Type: cross \nAbstract: Modern software systems complexity challenges efficient testing, as traditional machine learning (ML) struggles with large test suites. This research presents a hybrid framework integrating Quantum Annealing with ML to optimize test case prioritization in CI/CD pipelines. Leveraging quantum optimization, it achieves a 25 percent increase in defect detection efficiency and a 30 percent reduction in test execution time versus classical ML, validated on the Defects4J dataset. A simulated CI/CD environment demonstrates robustness across evolving codebases. Visualizations, including defect heatmaps and performance graphs, enhance interpretability. The framework addresses quantum hardware limits, CI/CD integration, and scalability for 2025s hybrid quantum-classical ecosystems, offering a transformative approach to software quality assurance."
      },
      {
        "id": "oai:arXiv.org:2506.02091v1",
        "title": "Comparison of spectrogram scaling in multi-label Music Genre Recognition",
        "link": "https://arxiv.org/abs/2506.02091",
        "author": "Bartosz Karpi\\'nski, Cyryl Leszczy\\'nski",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02091v1 Announce Type: cross \nAbstract: As the accessibility and ease-of-use of digital audio workstations increases, so does the quantity of music available to the average listener; additionally, differences between genres are not always well defined and can be abstract, with widely varying combinations of genres across individual records. In this article, multiple preprocessing methods and approaches to model training are described and compared, accounting for the eclectic nature of today's albums. A custom, manually labeled dataset of more than 18000 entries has been used to perform the experiments."
      },
      {
        "id": "oai:arXiv.org:2506.02093v1",
        "title": "Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?",
        "link": "https://arxiv.org/abs/2506.02093",
        "author": "Tianyu Lin, Xinran Li, Chuntung Zhuang, Qi Chen, Yuanhao Cai, Kai Ding, Alan L. Yuille, Zongwei Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02093v1 Announce Type: cross \nAbstract: Widely adopted evaluation metrics for sparse-view CT reconstruction--such as Structural Similarity Index Measure and Peak Signal-to-Noise Ratio--prioritize pixel-wise fidelity but often fail to capture the completeness of critical anatomical structures, particularly small or thin regions that are easily missed. To address this limitation, we propose a suite of novel anatomy-aware evaluation metrics designed to assess structural completeness across anatomical structures, including large organs, small organs, intestines, and vessels. Building on these metrics, we introduce CARE, a Completeness-Aware Reconstruction Enhancement framework that incorporates structural penalties during training to encourage anatomical preservation of significant structures. CARE is model-agnostic and can be seamlessly integrated into analytical, implicit, and generative methods. When applied to these methods, CARE substantially improves structural completeness in CT reconstructions, achieving up to +32% improvement for large organs, +22% for small organs, +40% for intestines, and +36% for vessels."
      },
      {
        "id": "oai:arXiv.org:2506.02149v1",
        "title": "Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine",
        "link": "https://arxiv.org/abs/2506.02149",
        "author": "Wenjun Xia, Chuang Niu, Ge Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02149v1 Announce Type: cross \nAbstract: Computed tomography (CT) is a major medical imaging modality. Clinical CT scenarios, such as low-dose screening, sparse-view scanning, and metal implants, often lead to severe noise and artifacts in reconstructed images, requiring improved reconstruction techniques. The introduction of deep learning has significantly advanced CT image reconstruction. However, obtaining paired training data remains rather challenging due to patient motion and other constraints. Although deep learning methods can still perform well with approximately paired data, they inherently carry the risk of hallucination due to data inconsistencies and model instability. In this paper, we integrate the data fidelity with the state-of-the-art generative AI model, referred to as the Poisson flow generative model (PFGM) with a generalized version PFGM++, and propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine (FORCE). In our experiments, the proposed method shows superior performance in various CT imaging tasks, outperforming existing unsupervised reconstruction approaches."
      },
      {
        "id": "oai:arXiv.org:2506.02160v1",
        "title": "A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering",
        "link": "https://arxiv.org/abs/2506.02160",
        "author": "Madan Krishnamurthy, Daniel Korn, Melissa A Haendel, Christopher J Mungall, Anne E Thessen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02160v1 Announce Type: cross \nAbstract: This research aims to develop a dynamic and scalable framework to facilitate harmonization of Common Data Elements (CDEs) across heterogeneous biomedical datasets by addressing challenges such as semantic heterogeneity, structural variability, and context dependence to streamline integration, enhance interoperability, and accelerate scientific discovery. Our methodology leverages Large Language Models (LLMs) for context-aware text embeddings that convert CDEs into dense vectors capturing semantic relationships and patterns. These embeddings are clustered using Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) to group semantically similar CDEs. The framework incorporates four key steps: (1) LLM-based text embedding to mathematically represent semantic context, (2) unsupervised clustering of embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4) supervised learning to train a classifier assigning new or unclustered CDEs to labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000 CDEs, the system identified 118 meaningful clusters at an optimized minimum cluster size of 20. The classifier achieved 90.46 percent overall accuracy, performing best in larger categories. External validation against Gravity Projects Social Determinants of Health domains showed strong agreement (Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that embeddings effectively capture cluster characteristics. This adaptable and scalable approach offers a practical solution to CDE harmonization, improving selection efficiency and supporting ongoing data interoperability."
      },
      {
        "id": "oai:arXiv.org:2506.02166v1",
        "title": "Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi",
        "link": "https://arxiv.org/abs/2506.02166",
        "author": "Arnav Rustagi, Satvik Bajpai, Nimrat Kaur, Siddharth Siddharth",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02166v1 Announce Type: cross \nAbstract: Computer-Assisted Pronunciation Training (CAPT) has been extensively studied for English. However, there remains a critical gap in its application to Indian languages with a base of 1.5 billion speakers. Pronunciation tools tailored to Indian languages are strikingly lacking despite the fact that millions learn them every year. With over 600 million speakers and being the fourth most-spoken language worldwide, improving Hindi pronunciation is a vital first step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT system for Hindi, 2) synthetic speech generation for Hindi mispronunciations, and 3) a novel methodology for providing personalized feedback to learners. While the system often interacts with learners using Devanagari graphemes, its core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic orthography to analyze mispronounced speech and provide targeted feedback."
      },
      {
        "id": "oai:arXiv.org:2506.02177v1",
        "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts",
        "link": "https://arxiv.org/abs/2506.02177",
        "author": "Haizhong Zheng, Yang Zhou, Brian R. Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, Beidi Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02177v1 Announce Type: cross \nAbstract: Reinforcement learning, such as PPO and GRPO, has powered recent breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables models to selectively use higher-quality data for training, which can stabilize RL training and improve model performance. However, this comes at the cost of significant computational overhead. In this paper, we show that a substantial portion of this overhead can be avoided by skipping uninformative prompts before rollout. Our analysis of reward dynamics reveals a strong temporal consistency in prompt value: prompts that are uninformative in one epoch of training are likely to remain uninformative in future epochs. Based on these insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online, lightweight pre-rollout filtering algorithm that predicts and skips uninformative prompts using reward training dynamics. By evaluating GRESO on a broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B, DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation."
      },
      {
        "id": "oai:arXiv.org:2506.02178v1",
        "title": "Cocktail-Party Audio-Visual Speech Recognition",
        "link": "https://arxiv.org/abs/2506.02178",
        "author": "Thai-Binh Nguyen, Ngoc-Quan Pham, Alexander Waibel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02178v1 Announce Type: cross \nAbstract: Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech recognition in challenging environments, such as cocktail-party scenarios, where relying solely on audio proves insufficient. However, current AVSR models are often optimized for idealized scenarios with consistently active speakers, overlooking the complexities of real-world settings that include both speaking and silent facial segments. This study addresses this gap by introducing a novel audio-visual cocktail-party dataset designed to benchmark current AVSR systems and highlight the limitations of prior approaches in realistic noisy conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising both talking-face and silent-face segments, enabling significant performance gains in cocktail-party environments. Our approach reduces WER by 67% relative to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise, without relying on explicit segmentation cues."
      },
      {
        "id": "oai:arXiv.org:2506.02197v1",
        "title": "NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution",
        "link": "https://arxiv.org/abs/2506.02197",
        "author": "Marcos V. Conde, Radu Timofte, Zihao Lu, Xiangyu Kongand Xiaoxia Xingand Fan Wangand Suejin Hanand MinKyu Parkand Tianyu Zhangand Xin Luoand Yeda Chenand Dong Liuand Li Pangand Yuhang Yangand Hongzhong Wangand Xiangyong Caoand Ruixuan Jiangand Senyan Xuand Siyuan Jiangand Xueyang Fuand Zheng-Jun Zhaand Tianyu Haoand Yuhong Heand Ruoqi Liand Yueqi Yangand Xiang Yuand Guanlan Hongand Minmin Yiand Yuanjia Chenand Liwen Zhangand Zijie Jinand Cheng Liand Lian Liuand Wei Songand Heng Sunand Yubo Wangand Jinghua Wangand Jiajie Luand Watchara Ruangsangand",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02197v1 Announce Type: cross \nAbstract: This paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution Challenge, highlighting the proposed solutions and results. New methods for RAW Restoration and Super-Resolution could be essential in modern Image Signal Processing (ISP) pipelines, however, this problem is not as explored as in the RGB domain. The goal of this challenge is two fold, (i) restore RAW images with blur and noise degradations, (ii) upscale RAW Bayer images by 2x, considering unknown noise and blur. In the challenge, a total of 230 participants registered, and 45 submitted results during thee challenge period. This report presents the current state-of-the-art in RAW Restoration."
      },
      {
        "id": "oai:arXiv.org:2506.02214v1",
        "title": "Is PMBOK Guide the Right Fit for AI? Re-evaluating Project Management in the Face of Artificial Intelligence Projects",
        "link": "https://arxiv.org/abs/2506.02214",
        "author": "Alexey Burdakov, Max Jaihyun Ahn",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02214v1 Announce Type: cross \nAbstract: This paper critically evaluates the applicability of the Project Management Body of Knowledge (PMBOK) Guide framework to Artificial Intelligence (AI) software projects, highlighting key limitations and proposing tailored adaptations. Unlike traditional projects, AI initiatives rely heavily on complex data, iterative experimentation, and specialized expertise while navigating significant ethical considerations. Our analysis identifies gaps in the PMBOK Guide, including its limited focus on data management, insufficient support for iterative development, and lack of guidance on ethical and multidisciplinary challenges. To address these deficiencies, we recommend integrating data lifecycle management, adopting iterative and AI project management frameworks, and embedding ethical considerations within project planning and execution. Additionally, we explore alternative approaches that better align with AI's dynamic and exploratory nature. We aim to enhance project management practices for AI software projects by bridging these gaps."
      },
      {
        "id": "oai:arXiv.org:2506.02241v1",
        "title": "Second-order AAA algorithms for structured data-driven modeling",
        "link": "https://arxiv.org/abs/2506.02241",
        "author": "Michael S. Ackermann, Ion Victor Gosea, Serkan Gugercin, Steffen W. R. Werner",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02241v1 Announce Type: cross \nAbstract: The data-driven modeling of dynamical systems has become an essential tool for the construction of accurate computational models from real-world data. In this process, the inherent differential structures underlying the considered physical phenomena are often neglected making the reinterpretation of the learned models in a physically meaningful sense very challenging. In this work, we present three data-driven modeling approaches for the construction of dynamical systems with second-order differential structure directly from frequency domain data. Based on the second-order structured barycentric form, we extend the well-known Adaptive Antoulas-Anderson algorithm to the case of second-order systems. Depending on the available computational resources, we propose variations of the proposed method that prioritize either higher computation speed or greater modeling accuracy, and we present a theoretical analysis for the expected accuracy and performance of the proposed methods. Three numerical examples demonstrate the effectiveness of our new structured approaches in comparison to classical unstructured data-driven modeling."
      },
      {
        "id": "oai:arXiv.org:2506.02254v1",
        "title": "Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps",
        "link": "https://arxiv.org/abs/2506.02254",
        "author": "Dimitris G Giovanis, Nikolaos Evangelou, Ioannis G Kevrekidis, Roger G Ghanem",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02254v1 Announce Type: cross \nAbstract: We present a generative learning framework for probabilistic sampling based on an extension of the Probabilistic Learning on Manifolds (PLoM) approach, which is designed to generate statistically consistent realizations of a random vector in a finite-dimensional Euclidean space, informed by a limited (yet representative) set of observations. In its original form, PLoM constructs a reduced-order probabilistic model by combining three main components: (a) kernel density estimation to approximate the underlying probability measure, (b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure, and (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample from the learned distribution. A key challenge arises, however, when the number of available data points N is small and the dimensionality of the diffusion-map basis approaches N, resulting in overfitting and loss of generalization. To overcome this limitation, we propose an enabling extension that implements a synthesis of Double Diffusion Maps -- a technique capable of capturing multiscale geometric features of the data -- with Geometric Harmonics (GH), a nonparametric reconstruction method that allows smooth nonlinear interpolation in high-dimensional ambient spaces. This approach enables us to solve a full-order ISDE directly in the latent space, preserving the full dynamical complexity of the system, while leveraging its reduced geometric representation. The effectiveness and robustness of the proposed method are illustrated through two numerical studies: one based on data generated from two-dimensional Hermite polynomial functions and another based on high-fidelity simulations of a detonation wave in a reactive flow."
      },
      {
        "id": "oai:arXiv.org:2506.02257v1",
        "title": "Assumption-free stability for ranking problems",
        "link": "https://arxiv.org/abs/2506.02257",
        "author": "Ruiting Liang, Jake A. Soloff, Rina Foygel Barber, Rebecca Willett",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02257v1 Announce Type: cross \nAbstract: In this work, we consider ranking problems among a finite set of candidates: for instance, selecting the top-$k$ items among a larger list of candidates or obtaining the full ranking of all items in the set. These problems are often unstable, in the sense that estimating a ranking from noisy data can exhibit high sensitivity to small perturbations. Concretely, if we use data to provide a score for each item (say, by aggregating preference data over a sample of users), then for two items with similar scores, small fluctuations in the data can alter the relative ranking of those items. Many existing theoretical results for ranking problems assume a separation condition to avoid this challenge, but real-world data often contains items whose scores are approximately tied, limiting the applicability of existing theory. To address this gap, we develop a new algorithmic stability framework for ranking problems, and propose two novel ranking operators for achieving stable ranking: the \\emph{inflated top-$k$} for the top-$k$ selection problem and the \\emph{inflated full ranking} for ranking the full list. To enable stability, each method allows for expressing some uncertainty in the output. For both of these two problems, our proposed methods provide guaranteed stability, with no assumptions on data distributions and no dependence on the total number of candidates to be ranked. Experiments on real-world data confirm that the proposed methods offer stability without compromising the informativeness of the output."
      },
      {
        "id": "oai:arXiv.org:2506.02260v1",
        "title": "MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements",
        "link": "https://arxiv.org/abs/2506.02260",
        "author": "Howon Ryu, Yuliang Chen, Yacun Wang, Andrea Z. LaCroix, Chongzhi Di, Loki Natarajan, Yu Wang, Jingjing Zou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02260v1 Announce Type: cross \nAbstract: The growing prevalence of digital health technologies has led to the generation of complex multi-modal data, such as physical activity measurements simultaneously collected from various sensors of mobile and wearable devices. These data hold immense potential for advancing health studies, but current methods predominantly rely on supervised learning, requiring extensive labeled datasets that are often expensive or impractical to obtain, especially in clinical studies. To address this limitation, we propose a self-supervised learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that leverages cross-modality masking and the Transformer autoencoder architecture to utilize both temporal correlations within modalities and cross-modal correlations between data streams. We also provide theoretical guarantees to support the effectiveness of the cross-modality masking scheme in MoCA. Comprehensive experiments and ablation studies demonstrate that our method outperforms existing approaches in both reconstruction and downstream tasks. We release open-source code for data processing, pre-training, and downstream tasks in the supplementary materials. This work highlights the transformative potential of self-supervised learning in digital health and multi-modal data."
      },
      {
        "id": "oai:arXiv.org:2506.02261v1",
        "title": "Towards Human-like Preference Profiling in Sequential Recommendation",
        "link": "https://arxiv.org/abs/2506.02261",
        "author": "Zhongyu Ouyang, Qianlong Wen, Chunhui Zhang, Yanfang Ye, Soroush Vosoughi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02261v1 Announce Type: cross \nAbstract: Sequential recommendation systems aspire to profile users by interpreting their interaction histories, echoing how humans make decisions by weighing experience, relative preference strength, and situational relevance. Yet, existing large language model (LLM)-based recommenders often fall short of mimicking the flexible, context-aware decision strategies humans exhibit, neglecting the structured, dynamic, and context-aware mechanisms fundamental to human behaviors. To bridge this gap, we propose RecPO, a preference optimization framework that models structured feedback and contextual delay to emulate human-like prioritization in sequential recommendation RecPO exploits adaptive reward margins based on inferred preference hierarchies and temporal signals, enabling the model to favor immediately relevant items and to distinguish between varying degrees of preference and aversion. Extensive experiments across five real-world datasets demonstrate that RecPO not only yields performance gains over state-of-the-art baselines, but also mirrors key characteristics of human decision-making: favoring timely satisfaction, maintaining coherent preferences, and exercising discernment under shifting contexts."
      },
      {
        "id": "oai:arXiv.org:2506.02284v1",
        "title": "Learning Optimal Posted Prices for a Unit-Demand Buyer",
        "link": "https://arxiv.org/abs/2506.02284",
        "author": "Yifeng Teng, Yifan Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02284v1 Announce Type: cross \nAbstract: We study the problem of learning the optimal item pricing for a unit-demand buyer with independent item values, and the learner has query access to the buyer's value distributions. We consider two common query models in the literature: the sample access model where the learner can obtain a sample of each item value, and the pricing query model where the learner can set a price for an item and obtain a binary signal on whether the sampled value of the item is greater than our proposed price. In this work, we give nearly tight sample complexity and pricing query complexity of the unit-demand pricing problem."
      },
      {
        "id": "oai:arXiv.org:2506.02312v1",
        "title": "Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation",
        "link": "https://arxiv.org/abs/2506.02312",
        "author": "Md Tauhidul Islam, Wu Da-Wen, Tang Qing-Qing, Zhao Kai-Yang, Yin Teng, Li Yan-Fei, Shang Wen-Yi, Liu Jing-Yu, Zhang Hai-Xian",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02312v1 Announce Type: cross \nAbstract: Retinal blood vessel segmentation is crucial for diagnosing ocular and cardiovascular diseases. Although the introduction of U-Net in 2015 by Olaf Ronneberger significantly advanced this field, yet issues like limited training data, imbalance data distribution, and inadequate feature extraction persist, hindering both the segmentation performance and optimal model generalization. Addressing these critical issues, the DEFFA-Unet is proposed featuring an additional encoder to process domain-invariant pre-processed inputs, thereby improving both richer feature encoding and enhanced model generalization. A feature filtering fusion module is developed to ensure the precise feature filtering and robust hybrid feature fusion. In response to the task-specific need for higher precision where false positives are very costly, traditional skip connections are replaced with the attention-guided feature reconstructing fusion module. Additionally, innovative data augmentation and balancing methods are proposed to counter data scarcity and distribution imbalance, further boosting the robustness and generalization of the model. With a comprehensive suite of evaluation metrics, extensive validations on four benchmark datasets (DRIVE, CHASEDB1, STARE, and HRF) and an SLO dataset (IOSTAR), demonstrate the proposed method's superiority over both baseline and state-of-the-art models. Particularly the proposed method significantly outperforms the compared methods in cross-validation model generalization."
      },
      {
        "id": "oai:arXiv.org:2506.02314v1",
        "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code",
        "link": "https://arxiv.org/abs/2506.02314",
        "author": "Tianyu Hua, Harper Hua, Violet Xiang, Benjamin Klieger, Sang T. Truong, Weixin Liang, Fan-Yun Sun, Nick Haber",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02314v1 Announce Type: cross \nAbstract: Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement novel ideas from recent research papers-ideas unseen during pretraining-remains unclear. We introduce ResearchCodeBench, a benchmark of 212 coding challenges that evaluates LLMs' ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3% success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and 30.8% respectively. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous and community-driven evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation."
      },
      {
        "id": "oai:arXiv.org:2506.02336v1",
        "title": "Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression",
        "link": "https://arxiv.org/abs/2506.02336",
        "author": "Jingfeng Wu, Pierre Marion, Peter Bartlett",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02336v1 Announce Type: cross \nAbstract: We study gradient descent (GD) with a constant stepsize for $\\ell_2$-regularized logistic regression with linearly separable data. Classical theory suggests small stepsizes to ensure monotonic reduction of the optimization objective, achieving exponential convergence in $\\widetilde{\\mathcal{O}}(\\kappa)$ steps with $\\kappa$ being the condition number. Surprisingly, we show that this can be accelerated to $\\widetilde{\\mathcal{O}}(\\sqrt{\\kappa})$ by simply using a large stepsize -- for which the objective evolves nonmonotonically. The acceleration brought by large stepsizes extends to minimizing the population risk for separable distributions, improving on the best-known upper bounds on the number of steps to reach a near-optimum. Finally, we characterize the largest stepsize for the local convergence of GD, which also determines the global convergence in special scenarios. Our results extend the analysis of Wu et al. (2024) from convex settings with minimizers at infinity to strongly convex cases with finite minimizers."
      },
      {
        "id": "oai:arXiv.org:2506.02373v1",
        "title": "Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent",
        "link": "https://arxiv.org/abs/2506.02373",
        "author": "Kordel K. France, Ovidiu Daescu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02373v1 Announce Type: cross \nAbstract: Olfactory navigation is one of the most primitive mechanisms of exploration used by organisms. Navigation by machine olfaction (artificial smell) is a very difficult task to both simulate and solve. With this work, we define olfactory inertial odometry (OIO), a framework for using inertial kinematics, and fast-sampling olfaction sensors to enable navigation by scent analogous to visual inertial odometry (VIO). We establish how principles from SLAM and VIO can be extrapolated to olfaction to enable real-world robotic tasks. We demonstrate OIO with three different odour localization algorithms on a real 5-DoF robot arm over an odour-tracking scenario that resembles real applications in agriculture and food quality control. Our results indicate success in establishing a baseline framework for OIO from which other research in olfactory navigation can build, and we note performance enhancements that can be made to address more complex tasks in the future."
      },
      {
        "id": "oai:arXiv.org:2506.02381v1",
        "title": "Unrolling Nonconvex Graph Total Variation for Image Denoising",
        "link": "https://arxiv.org/abs/2506.02381",
        "author": "Songlin Wei, Gene Cheung, Fei Chen, Ivan Selesnick",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02381v1 Announce Type: cross \nAbstract: Conventional model-based image denoising optimizations employ convex regularization terms, such as total variation (TV) that convexifies the $\\ell_0$-norm to promote sparse signal representation. Instead, we propose a new non-convex total variation term in a graph setting (NC-GTV), such that when combined with an $\\ell_2$-norm fidelity term for denoising, leads to a convex objective with no extraneous local minima. We define NC-GTV using a new graph variant of the Huber function, interpretable as a Moreau envelope. The crux is the selection of a parameter $a$ characterizing the graph Huber function that ensures overall objective convexity; we efficiently compute $a$ via an adaptation of Gershgorin Circle Theorem (GCT). To minimize the convex objective, we design a linear-time algorithm based on Alternating Direction Method of Multipliers (ADMM) and unroll it into a lightweight feed-forward network for data-driven parameter learning. Experiments show that our method outperforms unrolled GTV and other representative image denoising schemes, while employing far fewer network parameters."
      },
      {
        "id": "oai:arXiv.org:2506.02394v1",
        "title": "Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments",
        "link": "https://arxiv.org/abs/2506.02394",
        "author": "Yuan Bian, Xingche Guo, Yuanjia Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02394v1 Announce Type: cross \nAbstract: Major depressive disorder (MDD), a leading cause of disability and mortality, is associated with reward-processing abnormalities and concentration issues. Motivated by the probabilistic reward task from the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel framework that integrates the reinforcement learning (RL) model and drift-diffusion model (DDM) to jointly analyze reward-based decision-making with response times. To account for emerging evidence suggesting that decision-making may alternate between multiple interleaved strategies, we model latent state switching using a hidden Markov model (HMM). In the ''engaged'' state, decisions follow an RL-DDM, simultaneously capturing reward processing, decision dynamics, and temporal structure. In contrast, in the ''lapsed'' state, decision-making is modeled using a simplified DDM, where specific parameters are fixed to approximate random guessing with equal probability. The proposed method is implemented using a computationally efficient generalized expectation-maximization algorithm with forward-backward procedures. Through extensive numerical studies, we demonstrate that our proposed method outperforms competing approaches under various reward-generating distributions, both with and without strategy switching. When applied to the EMBARC study, our framework reveals that MDD patients exhibit lower overall engagement than healthy controls and experience longer decision times when they do engage. Additionally, we show that neuroimaging measures of brain activities are associated with decision-making characteristics in the ''engaged'' state but not in the ''lapsed'' state, providing evidence of brain-behavioral association specific to the ''engaged'' state."
      },
      {
        "id": "oai:arXiv.org:2506.02401v1",
        "title": "Trusted Fake Audio Detection Based on Dirichlet Distribution",
        "link": "https://arxiv.org/abs/2506.02401",
        "author": "Chi Ding, Junxiao Xue, Cong Wang, Hao Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02401v1 Announce Type: cross \nAbstract: With the continuous development of deep learning-based speech conversion and speech synthesis technologies, the cybersecurity problem posed by fake audio has become increasingly serious. Previously proposed models for defending against fake audio have attained remarkable performance. However, they all fall short in modeling the trustworthiness of the decisions made by the models themselves. Based on this, we put forward a plausible fake audio detection approach based on the Dirichlet distribution with the aim of enhancing the reliability of fake audio detection. Specifically, we first generate evidence through a neural network. Uncertainty is then modeled using the Dirichlet distribution. By modeling the belief distribution with the parameters of the Dirichlet distribution, an estimate of uncertainty can be obtained for each decision. Finally, the predicted probabilities and corresponding uncertainty estimates are combined to form the final opinion. On the ASVspoof series dataset (i.e., ASVspoof 2019 LA, ASVspoof 2021 LA, and DF), we conduct a number of comparison experiments to verify the excellent performance of the proposed model in terms of accuracy, robustness, and trustworthiness."
      },
      {
        "id": "oai:arXiv.org:2506.02413v1",
        "title": "Tensor State Space-based Dynamic Multilayer Network Modeling",
        "link": "https://arxiv.org/abs/2506.02413",
        "author": "Tian Lan, Jie Guo, Chen Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02413v1 Announce Type: cross \nAbstract: Understanding the complex interactions within dynamic multilayer networks is critical for advancements in various scientific domains. Existing models often fail to capture such networks' temporal and cross-layer dynamics. This paper introduces a novel Tensor State Space Model for Dynamic Multilayer Networks (TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric Tucker decomposition to represent latent node features, their interaction patterns, and layer transitions. Then by fixing the latent features and allowing the interaction patterns to evolve over time, TSSDMN uniquely captures both the temporal dynamics within layers and across different layers. The model identifiability conditions are discussed. By treating latent features as variables whose posterior distributions are approximated using a mean-field variational inference approach, a variational Expectation Maximization algorithm is developed for efficient model inference. Numerical simulations and case studies demonstrate the efficacy of TSSDMN for understanding dynamic multilayer networks."
      },
      {
        "id": "oai:arXiv.org:2506.02422v1",
        "title": "Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning: Quantization-Assisted Min-Max Fair Scheduling",
        "link": "https://arxiv.org/abs/2506.02422",
        "author": "Xiyu Zhao, Qimei Cui, Ziqiang Du, Weicai Li, Xi Yu, Wei Ni, Ji Zhang, Xiaofeng Tao, Ping Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02422v1 Announce Type: cross \nAbstract: Personalized federated learning (PFL) offers a solution to balancing personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). Little attention has been given to wireless PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is another challenge resulting from communication bottlenecks in WPFL. This paper exploits quantization errors to enhance the privacy of WPFL and proposes a novel quantization-assisted Gaussian differential privacy (DP) mechanism. We analyze the convergence upper bounds of individual PL models by considering the impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and imperfect communication channels on the FL of WPFL. By minimizing the maximum of the bounds, we design an optimal transmission scheduling strategy that yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by revealing the nested structure of this problem to decouple it into subproblems solved sequentially for the client selection, channel allocation, and power control, and for the learning rates and PL-FL weighting coefficients. Experiments validate our analysis and demonstrate that our approach substantially outperforms alternative scheduling strategies by 87.08%, 16.21%, and 38.37% in accuracy, the maximum test loss of participating clients, and fairness (Jain's index), respectively."
      },
      {
        "id": "oai:arXiv.org:2506.02438v1",
        "title": "A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges",
        "link": "https://arxiv.org/abs/2506.02438",
        "author": "Sudhanshu Sekhar Tripathy, Bichitrananda Behera",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02438v1 Announce Type: cross \nAbstract: IDS aims to protect computer networks from security threats by detecting, notifying, and taking appropriate action to prevent illegal access and protect confidential information. As the globe becomes increasingly dependent on technology and automated processes, ensuring secured systems, applications, and networks has become one of the most significant problems of this era. The global web and digital technology have significantly accelerated the evolution of the modern world, necessitating the use of telecommunications and data transfer platforms. Researchers are enhancing the effectiveness of IDS by incorporating popular datasets into machine learning algorithms. IDS, equipped with machine learning classifiers, enhances security attack detection accuracy by identifying normal or abnormal network traffic. This paper explores the methods of capturing and reviewing intrusion detection systems (IDS) and evaluates the challenges existing datasets face. A deluge of research on machine learning (ML) and deep learning (DL) architecture-based intrusion detection techniques has been conducted in the past ten years on various cybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017, and CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth analysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB, RF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique, explaining the role of the classifiers and algorithms used. A detailed tabular analysis highlights the datasets used, classifiers employed, attacks detected, evaluation metrics, and conclusions drawn. This article offers a thorough review for future IDS research."
      },
      {
        "id": "oai:arXiv.org:2506.02458v1",
        "title": "A Novel Deep Reinforcement Learning Method for Computation Offloading in Multi-User Mobile Edge Computing with Decentralization",
        "link": "https://arxiv.org/abs/2506.02458",
        "author": "Nguyen Chi Long, Trinh Van Chien, Ta Hai Tung, Van Son Nguyen, Trong-Minh Hoang, Nguyen Ngoc Hai Dang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02458v1 Announce Type: cross \nAbstract: Mobile edge computing (MEC) allows appliances to offload workloads to neighboring MEC servers that have the potential for computation-intensive tasks with limited computational capabilities. This paper studied how deep reinforcement learning (DRL) algorithms are used in an MEC system to find feasible decentralized dynamic computation offloading strategies, which leads to the construction of an extensible MEC system that operates effectively with finite feedback. Even though the Deep Deterministic Policy Gradient (DDPG) algorithm, subject to their knowledge of the MEC system, can be used to allocate powers of both computation offloading and local execution, to learn a computation offloading policy for each user independently, we realized that this solution still has some inherent weaknesses. Hence, we introduced a new approach for this problem based on the Twin Delayed DDPG algorithm, which enables us to overcome this proneness and investigate cases where mobile users are portable. Numerical results showed that individual users can autonomously learn adequate policies through the proposed approach. Besides, the performance of the suggested solution exceeded the conventional DDPG-based power control strategy."
      },
      {
        "id": "oai:arXiv.org:2506.02467v1",
        "title": "Multi-modal brain MRI synthesis based on SwinUNETR",
        "link": "https://arxiv.org/abs/2506.02467",
        "author": "Haowen Pang, Weiyan Guo, Chuyang Ye",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02467v1 Announce Type: cross \nAbstract: Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in clinical diagnostics by providing complementary information across different imaging modalities. However, a common challenge in clinical practice is missing MRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing modalities in brain MRI. SwinUNETR is a novel neural network architecture designed for medical image analysis, integrating the strengths of Swin Transformer and convolutional neural networks (CNNs). The Swin Transformer, a variant of the Vision Transformer (ViT), incorporates hierarchical feature extraction and window-based self-attention mechanisms, enabling it to capture both local and global contextual information effectively. By combining the Swin Transformer with CNNs, SwinUNETR merges global context awareness with detailed spatial resolution. This hybrid approach addresses the challenges posed by the varying modality characteristics and complex brain structures, facilitating the generation of accurate and realistic synthetic images. We evaluate the performance of SwinUNETR on brain MRI datasets and demonstrate its superior capability in generating clinically valuable images. Our results show significant improvements in image quality, anatomical consistency, and diagnostic value."
      },
      {
        "id": "oai:arXiv.org:2506.02479v1",
        "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage",
        "link": "https://arxiv.org/abs/2506.02479",
        "author": "Kalyan Nakka, Nitesh Saxena",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02479v1 Announce Type: cross \nAbstract: The inherent risk of generating harmful and unsafe content by Large Language Models (LLMs), has highlighted the need for their safety alignment. Various techniques like supervised fine-tuning, reinforcement learning from human feedback, and red-teaming were developed for ensuring the safety alignment of LLMs. However, the robustness of these aligned LLMs is always challenged by adversarial attacks that exploit unexplored and underlying vulnerabilities of the safety alignment. In this paper, we develop a novel black-box jailbreak attack, called BitBypass, that leverages hyphen-separated bitstream camouflage for jailbreaking aligned LLMs. This represents a new direction in jailbreaking by exploiting fundamental information representation of data as continuous bits, rather than leveraging prompt engineering or adversarial manipulations. Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude 3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the capabilities of BitBypass in bypassing their safety alignment and tricking them into generating harmful and unsafe content. Further, we observed that BitBypass outperforms several state-of-the-art jailbreak attacks in terms of stealthiness and attack success. Overall, these results highlights the effectiveness and efficiency of BitBypass in jailbreaking these state-of-the-art LLMs."
      },
      {
        "id": "oai:arXiv.org:2506.02489v1",
        "title": "Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schr\\\"odinger Bridges",
        "link": "https://arxiv.org/abs/2506.02489",
        "author": "Tao Zhong, Jonah Buchanan, Christine Allen-Blanchette",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02489v1 Announce Type: cross \nAbstract: We propose a new approach to vision-based dexterous grasp translation, which aims to transfer grasp intent across robotic hands with differing morphologies. Given a visual observation of a source hand grasping an object, our goal is to synthesize a functionally equivalent grasp for a target hand without requiring paired demonstrations or hand-specific simulations. We frame this problem as a stochastic transport between grasp distributions using the Schr\\\"odinger Bridge formalism. Our method learns to map between source and target latent grasp spaces via score and flow matching, conditioned on visual observations. To guide this translation, we introduce physics-informed cost functions that encode alignment in base pose, contact maps, wrench space, and manipulability. Experiments across diverse hand-object pairs demonstrate our approach generates stable, physically grounded grasps with strong generalization. This work enables semantic grasp transfer for heterogeneous manipulators and bridges vision-based grasping with probabilistic generative modeling."
      },
      {
        "id": "oai:arXiv.org:2506.02529v1",
        "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs",
        "link": "https://arxiv.org/abs/2506.02529",
        "author": "Nguyen-Khang Le, Quan Minh Bui, Minh Ngoc Nguyen, Hiep Nguyen, Trung Vo, Son T. Luu, Shoshin Nomura, Minh Le Nguyen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02529v1 Announce Type: cross \nAbstract: Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehensive dataset for evaluating form-interaction testing. Experimental results demonstrate the system's effectiveness in improving test coverage and robustness, advancing the state of web application testing."
      },
      {
        "id": "oai:arXiv.org:2506.02548v1",
        "title": "CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale",
        "link": "https://arxiv.org/abs/2506.02548",
        "author": "Zhun Wang, Tianneng Shi, Jingxuan He, Matthew Cai, Jialin Zhang, Dawn Song",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02548v1 Announce Type: cross \nAbstract: Large language model (LLM) agents are becoming increasingly skilled at handling cybersecurity tasks autonomously. Thoroughly assessing their cybersecurity capabilities is critical and urgent, given the high stakes in this domain. However, existing benchmarks fall short, often failing to capture real-world scenarios or being limited in scope. To address this gap, we introduce CyberGym, a large-scale and high-quality cybersecurity evaluation framework featuring 1,507 real-world vulnerabilities found and patched across 188 large software projects. While it includes tasks of various settings, CyberGym primarily focuses on the generation of proof-of-concept (PoC) tests for vulnerability reproduction, based on text descriptions and corresponding source repositories. Solving this task is particularly challenging, as it requires comprehensive reasoning across entire codebases to locate relevant code fragments and produce effective PoCs that accurately trigger the target vulnerability starting from the program's entry point. Our evaluation across 4 state-of-the-art agent frameworks and 9 LLMs reveals that even the best combination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9% reproduction success rate, mainly on simpler cases. Beyond reproducing historical vulnerabilities, we find that PoCs generated by LLM agents can reveal new vulnerabilities, identifying 15 zero-days affecting the latest versions of the software projects."
      },
      {
        "id": "oai:arXiv.org:2506.02554v1",
        "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers",
        "link": "https://arxiv.org/abs/2506.02554",
        "author": "Timo Osterburg, Franz Albers, Christopher Diehl, Rajesh Pushparaj, Torsten Bertram",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02554v1 Announce Type: cross \nAbstract: The fusion of sensor data is essential for a robust perception of the environment in autonomous driving. Learning-based fusion approaches mainly use feature-level fusion to achieve high performance, but their complexity and hardware requirements limit their applicability in near-production vehicles. High-level fusion methods offer robustness with lower computational requirements. Traditional methods, such as the Kalman filter, dominate this area. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel transformer-based high-level object fusion method called HiLO. Experimental results demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$ score and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale real-world dataset demonstrates the effectiveness of the proposed approaches. Their generalizability is further validated by cross-domain evaluation between urban and highway scenarios. Code, data, and models are available at https://github.com/rst-tu-dortmund/HiLO ."
      },
      {
        "id": "oai:arXiv.org:2506.02574v1",
        "title": "Dynamic mapping from static labels: remote sensing dynamic sample generation with temporal-spectral embedding",
        "link": "https://arxiv.org/abs/2506.02574",
        "author": "Shuai Yuan, Shuang Chen, Tianwu Lin, Jie Wang, Peng Gong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02574v1 Announce Type: cross \nAbstract: Accurate remote sensing geographic mapping depends heavily on representative and timely sample data. However, rapid changes in land surface dynamics necessitate frequent updates, quickly rendering previously collected samples obsolete and imposing significant labor demands for continuous manual updates. In this study, we aim to address this problem by dynamic sample generation using existing single-date static labeled samples. We introduce TasGen, a two-stage automated framework to automatically generate dynamic samples, designed to simultaneously model spectral and temporal dependencies in time-series remote sensing imagery via temporal-spectral embedding, capturing land surface changes without additional manual annotations."
      },
      {
        "id": "oai:arXiv.org:2506.02585v1",
        "title": "A Tree-guided CNN for image super-resolution",
        "link": "https://arxiv.org/abs/2506.02585",
        "author": "Chunwei Tian, Mingjian Song, Xiaopeng Fan, Xiangtao Zheng, Bob Zhang, David Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02585v1 Announce Type: cross \nAbstract: Deep convolutional neural networks can extract more accurate structural information via deep architectures to obtain good performance in image super-resolution. However, it is not easy to find effect of important layers in a single network architecture to decrease performance of super-resolution. In this paper, we design a tree-guided CNN for image super-resolution (TSRNet). It uses a tree architecture to guide a deep network to enhance effect of key nodes to amplify the relation of hierarchical information for improving the ability of recovering images. To prevent insufficiency of the obtained structural information, cosine transform techniques in the TSRNet are used to extract cross-domain information to improve the performance of image super-resolution. Adaptive Nesterov momentum optimizer (Adan) is applied to optimize parameters to boost effectiveness of training a super-resolution model. Extended experiments can verify superiority of the proposed TSRNet for restoring high-quality images. Its code can be obtained at https://github.com/hellloxiaotian/TSRNet."
      },
      {
        "id": "oai:arXiv.org:2506.02590v1",
        "title": "Synthetic Speech Source Tracing using Metric Learning",
        "link": "https://arxiv.org/abs/2506.02590",
        "author": "Dimitrios Koutsianos, Stavros Zacharopoulos, Yannis Panagakis, Themos Stafylakis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02590v1 Announce Type: cross \nAbstract: This paper addresses source tracing in synthetic speech-identifying generative systems behind manipulated audio via speaker recognition-inspired pipelines. While prior work focuses on spoofing detection, source tracing lacks robust solutions. We evaluate two approaches: classification-based and metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet and self-supervised learning (SSL) backbones. The results show that ResNet achieves competitive performance with the metric learning approach, matching and even exceeding SSL-based systems. Our work demonstrates ResNet's viability for source tracing while underscoring the need to optimize SSL representations for this task. Our work bridges speaker recognition methodologies with audio forensic challenges, offering new directions for combating synthetic media manipulation."
      },
      {
        "id": "oai:arXiv.org:2506.02618v1",
        "title": "Rodrigues Network for Learning Robot Actions",
        "link": "https://arxiv.org/abs/2506.02618",
        "author": "Jialiang Zhang, Haoran Geng, Yang You, Congyue Deng, Pieter Abbeel, Jitendra Malik, Leonidas Guibas",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02618v1 Announce Type: cross \nAbstract: Understanding and predicting articulated actions is important in robot learning. However, common architectures such as MLPs and Transformers lack inductive biases that reflect the underlying kinematic structure of articulated systems. To this end, we propose the Neural Rodrigues Operator, a learnable generalization of the classical forward kinematics operation, designed to inject kinematics-aware inductive bias into neural computation. Building on this operator, we design the Rodrigues Network (RodriNet), a novel neural architecture specialized for processing actions. We evaluate the expressivity of our network on two synthetic tasks on kinematic and motion prediction, showing significant improvements compared to standard backbones. We further demonstrate its effectiveness in two realistic applications: (i) imitation learning on robotic benchmarks with the Diffusion Policy, and (ii) single-image 3D hand reconstruction. Our results suggest that integrating structured kinematic priors into the network architecture improves action learning in various domains."
      },
      {
        "id": "oai:arXiv.org:2506.02620v1",
        "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation",
        "link": "https://arxiv.org/abs/2506.02620",
        "author": "Dongyu Yan, Leyi Wu, Jiantao Lin, Luozhou Wang, Tianshuo Xu, Zhifei Chen, Zhen Yang, Lie Xu, Shunsi Zhang, Yingcong Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02620v1 Announce Type: cross \nAbstract: Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce \\textbf{FlexPainter}, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality."
      },
      {
        "id": "oai:arXiv.org:2506.02651v1",
        "title": "Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks",
        "link": "https://arxiv.org/abs/2506.02651",
        "author": "Luca Arnaboldi, Bruno Loureiro, Ludovic Stephan, Florent Krzakala, Lenka Zdeborova",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02651v1 Announce Type: cross \nAbstract: We study the dynamics of stochastic gradient descent (SGD) for a class of sequence models termed Sequence Single-Index (SSI) models, where the target depends on a single direction in input space applied to a sequence of tokens. This setting generalizes classical single-index models to the sequential domain, encompassing simplified one-layer attention architectures. We derive a closed-form expression for the population loss in terms of a pair of sufficient statistics capturing semantic and positional alignment, and characterize the induced high-dimensional SGD dynamics for these coordinates. Our analysis reveals two distinct training phases: escape from uninformative initialization and alignment with the target subspace, and demonstrates how the sequence length and positional encoding influence convergence speed and learning trajectories. These results provide a rigorous and interpretable foundation for understanding how sequential structure in data can be beneficial for learning with attention-based models."
      },
      {
        "id": "oai:arXiv.org:2506.02657v1",
        "title": "Maximizing the Promptness of Metaverse Systems using Edge Computing by Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.02657",
        "author": "Tam Ninh Thi-Thanh, Trinh Van Chien, Hung Tran, Nguyen Hoai Son, Van Nhan Vo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02657v1 Announce Type: cross \nAbstract: Metaverse and Digital Twin (DT) have attracted much academic and industrial attraction to approach the future digital world. This paper introduces the advantages of deep reinforcement learning (DRL) in assisting Metaverse system-based Digital Twin. In this system, we assume that it includes several Metaverse User devices collecting data from the real world to transfer it into the virtual world, a Metaverse Virtual Access Point (MVAP) undertaking the processing of data, and an edge computing server that receives the offloading data from the MVAP. The proposed model works under a dynamic environment with various parameters changing over time. The experiment results show that our proposed DRL algorithm is suitable for offloading tasks to ensure the promptness of DT in a dynamic environment."
      },
      {
        "id": "oai:arXiv.org:2506.02661v1",
        "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation",
        "link": "https://arxiv.org/abs/2506.02661",
        "author": "Mingyang Huang, Peng Zhang, Bang Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02661v1 Announce Type: cross \nAbstract: Generating long-term, coherent, and realistic music-conditioned dance sequences remains a challenging task in human motion synthesis. Existing approaches exhibit critical limitations: motion graph methods rely on fixed template libraries, restricting creative generation; diffusion models, while capable of producing novel motions, often lack temporal coherence and musical alignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a hybrid framework that integrates Retrieval-Augmented Generation (RAG) with diffusion-based refinement to enable high-quality, musically coherent dance generation for arbitrary long-term music inputs. Our method introduces three core innovations: (1) A cross-modal contrastive learning architecture that aligns heterogeneous music and dance representations in a shared latent space, establishing unsupervised semantic correspondence without paired data; (2) An optimized motion graph system for efficient retrieval and seamless concatenation of motion segments, ensuring realism and temporal coherence across long sequences; (3) A multi-condition diffusion model that jointly conditions on raw music signals and contrastive features to enhance motion quality and global synchronization. Extensive experiments demonstrate that MotionRAG-Diff achieves state-of-the-art performance in motion quality, diversity, and music-motion synchronization accuracy. This work establishes a new paradigm for music-driven dance generation by synergizing retrieval-based template fidelity with diffusion-based creative enhancement."
      },
      {
        "id": "oai:arXiv.org:2506.02664v1",
        "title": "Computational Thresholds in Multi-Modal Learning via the Spiked Matrix-Tensor Model",
        "link": "https://arxiv.org/abs/2506.02664",
        "author": "Hugo Tabanelli, Pierre Mergny, Lenka Zdeborova, Florent Krzakala",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02664v1 Announce Type: cross \nAbstract: We study the recovery of multiple high-dimensional signals from two noisy, correlated modalities: a spiked matrix and a spiked tensor sharing a common low-rank structure. This setting generalizes classical spiked matrix and tensor models, unveiling intricate interactions between inference channels and surprising algorithmic behaviors. Notably, while the spiked tensor model is typically intractable at low signal-to-noise ratios, its correlation with the matrix enables efficient recovery via Bayesian Approximate Message Passing, inducing staircase-like phase transitions reminiscent of neural network phenomena. In contrast, empirical risk minimization for joint learning fails: the tensor component obstructs effective matrix recovery, and joint optimization significantly degrades performance, highlighting the limitations of naive multi-modal learning. We show that a simple Sequential Curriculum Learning strategy-first recovering the matrix, then leveraging it to guide tensor recovery-resolves this bottleneck and achieves optimal weak recovery thresholds. This strategy, implementable with spectral methods, emphasizes the critical role of structural correlation and learning order in multi-modal high-dimensional inference."
      },
      {
        "id": "oai:arXiv.org:2506.02668v1",
        "title": "FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems",
        "link": "https://arxiv.org/abs/2506.02668",
        "author": "Frederico Metelo, Alexandre Oliveira, Stevo Rackovi\\'c, Pedro \\'Akos Costa, Cl\\'audia Soares",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02668v1 Announce Type: cross \nAbstract: Edge computing addresses the growing data demands of connected-device networks by placing computational resources closer to end users through decentralized infrastructures. This decentralization challenges traditional, fully centralized orchestration, which suffers from latency and resource bottlenecks. We present \\textbf{FAuNO} -- \\emph{Federated Asynchronous Network Orchestrator} -- a buffered, asynchronous \\emph{federated reinforcement-learning} (FRL) framework for decentralized task offloading in edge systems. FAuNO adopts an actor-critic architecture in which local actors learn node-specific dynamics and peer interactions, while a federated critic aggregates experience across agents to encourage efficient cooperation and improve overall system performance. Experiments in the \\emph{PeersimGym} environment show that FAuNO consistently matches or exceeds heuristic and federated multi-agent RL baselines in reducing task loss and latency, underscoring its adaptability to dynamic edge-computing scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.02685v1",
        "title": "Symmetry-Aware GFlowNets",
        "link": "https://arxiv.org/abs/2506.02685",
        "author": "Hohyun Kim, Seunggeun Lee, Min-hwan Oh",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02685v1 Announce Type: cross \nAbstract: Generative Flow Networks (GFlowNets) offer a powerful framework for sampling graphs in proportion to their rewards. However, existing approaches suffer from systematic biases due to inaccuracies in state transition probability computations. These biases, rooted in the inherent symmetries of graphs, impact both atom-based and fragment-based generation schemes. To address this challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that incorporates symmetry corrections into the learning process through reward scaling. By integrating bias correction directly into the reward structure, SA-GFN eliminates the need for explicit state transition computations. Empirical results show that SA-GFN enables unbiased sampling while enhancing diversity and consistently generating high-reward graphs that closely match the target distribution."
      },
      {
        "id": "oai:arXiv.org:2506.02710v1",
        "title": "Online Bayesian system identification in multivariate autoregressive models via message passing",
        "link": "https://arxiv.org/abs/2506.02710",
        "author": "T. N. Nisslbeck, Wouter M. Kouw",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02710v1 Announce Type: cross \nAbstract: We propose a recursive Bayesian estimation procedure for multivariate autoregressive models with exogenous inputs based on message passing in a factor graph. Unlike recursive least-squares, our method produces full posterior distributions for both the autoregressive coefficients and noise precision. The uncertainties regarding these estimates propagate into the uncertainties on predictions for future system outputs, and support online model evidence calculations. We demonstrate convergence empirically on a synthetic autoregressive system and competitive performance on a double mass-spring-damper system."
      },
      {
        "id": "oai:arXiv.org:2506.02720v1",
        "title": "Benchmarking and Advancing Large Language Models for Local Life Services",
        "link": "https://arxiv.org/abs/2506.02720",
        "author": "Xiaochong Lan, Jie Feng, Jiahuan Lei, Xinlei Shi, Yong Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02720v1 Announce Type: cross \nAbstract: Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications."
      },
      {
        "id": "oai:arXiv.org:2506.02730v1",
        "title": "An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models",
        "link": "https://arxiv.org/abs/2506.02730",
        "author": "Po-Chieh Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02730v1 Announce Type: cross \nAbstract: We present an exploratory framework to test whether noise-like input can induce structured responses in language models. Instead of assuming that extraterrestrial signals must be decoded, we evaluate whether inputs can trigger linguistic behavior in generative systems. This shifts the focus from decoding to viewing structured output as a sign of underlying regularity in the input. We tested GPT-2 small, a 117M-parameter model trained on English text, using four types of acoustic input: human speech, humpback whale vocalizations, Phylloscopus trochilus birdsong, and algorithmically generated white noise. All inputs were treated as noise-like, without any assumed symbolic encoding. To assess reactivity, we defined a composite score called Semantic Induction Potential (SIP), combining entropy, syntax coherence, compression gain, and repetition penalty. Results showed that whale and bird vocalizations had higher SIP scores than white noise, while human speech triggered only moderate responses. This suggests that language models may detect latent structure even in data without conventional semantics. We propose that this approach could complement traditional SETI methods, especially in cases where communicative intent is unknown. Generative reactivity may offer a different way to identify data worth closer attention."
      },
      {
        "id": "oai:arXiv.org:2506.02754v1",
        "title": "Safely Learning Controlled Stochastic Dynamics",
        "link": "https://arxiv.org/abs/2506.02754",
        "author": "Luc Brogat-Motte, Alessandro Rudi, Riccardo Bonalli",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02754v1 Announce Type: cross \nAbstract: We address the problem of safely learning controlled stochastic dynamics from discrete-time trajectory observations, ensuring system trajectories remain within predefined safe regions during both training and deployment. Safety-critical constraints of this kind are crucial in applications such as autonomous robotics, finance, and biomedicine. We introduce a method that ensures safe exploration and efficient estimation of system dynamics by iteratively expanding an initial known safe control set using kernel-based confidence bounds. After training, the learned model enables predictions of the system's dynamics and permits safety verification of any given control. Our approach requires only mild smoothness assumptions and access to an initial safe control set, enabling broad applicability to complex real-world systems. We provide theoretical guarantees for safety and derive adaptive learning rates that improve with increasing Sobolev regularity of the true dynamics. Experimental evaluations demonstrate the practical effectiveness of our method in terms of safety, estimation accuracy, and computational efficiency."
      },
      {
        "id": "oai:arXiv.org:2506.02761v1",
        "title": "Rethinking Machine Unlearning in Image Generation Models",
        "link": "https://arxiv.org/abs/2506.02761",
        "author": "Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02761v1 Announce Type: cross \nAbstract: With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at https://github.com/ryliu68/IGMU."
      },
      {
        "id": "oai:arXiv.org:2506.02793v1",
        "title": "Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings",
        "link": "https://arxiv.org/abs/2506.02793",
        "author": "Houssam Zenati, Bariscan Bozkurt, Arthur Gretton",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02793v1 Announce Type: cross \nAbstract: Estimating the distribution of outcomes under counterfactual policies is critical for decision-making in domains such as recommendation, advertising, and healthcare. We analyze a novel framework-Counterfactual Policy Mean Embedding (CPME)-that represents the entire counterfactual outcome distribution in a reproducing kernel Hilbert space (RKHS), enabling flexible and nonparametric distributional off-policy evaluation. We introduce both a plug-in estimator and a doubly robust estimator; the latter enjoys improved uniform convergence rates by correcting for bias in both the outcome embedding and propensity models. Building on this, we develop a doubly robust kernel test statistic for hypothesis testing, which achieves asymptotic normality and thus enables computationally efficient testing and straightforward construction of confidence intervals. Our framework also supports sampling from the counterfactual distribution. Numerical simulations illustrate the practical benefits of CPME over existing methods."
      },
      {
        "id": "oai:arXiv.org:2506.02794v1",
        "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis",
        "link": "https://arxiv.org/abs/2506.02794",
        "author": "Mijeong Kim, Gunhee Kim, Jungyoon Choi, Wonjae Roh, Bohyung Han",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02794v1 Announce Type: cross \nAbstract: We introduce PhysGaia, a novel physics-aware dataset specifically designed for Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects and unstructured physical phenomena. Unlike existing datasets that primarily focus on photorealistic reconstruction, PhysGaia is created to actively support physics-aware dynamic scene modeling. Our dataset provides complex dynamic scenarios with rich interactions among multiple objects, where they realistically collide with each other and exchange forces. Furthermore, it contains a diverse range of physical materials, such as liquid, gas, viscoelastic substance, and textile, which moves beyond the rigid bodies prevalent in existing datasets. All scenes in PhysGaia are faithfully generated to strictly adhere to physical laws, leveraging carefully selected material-specific physics solvers. To enable quantitative evaluation of physical modeling, our dataset provides essential ground-truth information, including 3D particle trajectories and physics parameters, e.g., viscosity. To facilitate research adoption, we also provide essential integration pipelines for using state-of-the-art DyNVS models with our dataset and report their results. By addressing the critical lack of datasets for physics-aware modeling, PhysGaia will significantly advance research in dynamic view synthesis, physics-based scene understanding, and deep learning models integrated with physical simulation -- ultimately enabling more faithful reconstruction and interpretation of complex dynamic scenes. Our datasets and codes are available in the project website, http://cvlab.snu.ac.kr/research/PhysGaia."
      },
      {
        "id": "oai:arXiv.org:2506.02802v1",
        "title": "A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads",
        "link": "https://arxiv.org/abs/2506.02802",
        "author": "Andr\\'as Strausz, Niels Pardon, Ioana Giurgiu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02802v1 Announce Type: cross \nAbstract: Lakehouse systems enable the same data to be queried with multiple execution engines. However, selecting the engine best suited to run a SQL query still requires a priori knowledge of the query computational requirements and an engine capability, a complex and manual task that only becomes more difficult with the emergence of new engines and workloads. In this paper, we address this limitation by proposing a cross-engine optimizer that can automate engine selection for diverse SQL queries through a learned cost model. Optimized with hints, a query plan is used for query cost prediction and routing. Cost prediction is formulated as a multi-task learning problem, and multiple predictor heads, corresponding to different engines and provisionings, are used in the model architecture. This eliminates the need to train engine-specific models and allows the flexible addition of new engines at a minimal fine-tuning cost. Results on various databases and engines show that using a query optimized logical plan for cost estimation decreases the average Q-error by even 12.6% over using unoptimized plans as input. Moreover, the proposed cross-engine optimizer reduces the total workload runtime by up to 25.2% in a zero-shot setting and 30.4% in a few-shot setting when compared to random routing."
      },
      {
        "id": "oai:arXiv.org:2506.02825v1",
        "title": "Asymptotically perfect seeded graph matching without edge correlation (and applications to inference)",
        "link": "https://arxiv.org/abs/2506.02825",
        "author": "Tong Qi, Vera Andersson, Peter Viechnicki, Vince Lyzinski",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02825v1 Announce Type: cross \nAbstract: We present the OmniMatch algorithm for seeded multiple graph matching. In the setting of $d$-dimensional Random Dot Product Graphs (RDPG), we prove that under mild assumptions, OmniMatch with $s$ seeds asymptotically and efficiently perfectly aligns $O(s^{\\alpha})$ unseeded vertices -- for $\\alpha<2\\wedge d/4$ -- across multiple networks even in the presence of no edge correlation. We demonstrate the effectiveness of our algorithm across numerous simulations and in the context of shuffled graph hypothesis testing. In the shuffled testing setting, testing power is lost due to the misalignment/shuffling of vertices across graphs, and we demonstrate the capacity of OmniMatch to correct for misaligned vertices prior to testing and hence recover the lost testing power. We further demonstrate the algorithm on a pair of data examples from connectomics and machine translation."
      },
      {
        "id": "oai:arXiv.org:2506.02841v1",
        "title": "Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods",
        "link": "https://arxiv.org/abs/2506.02841",
        "author": "Tom Danino, Nahum Shimkin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02841v1 Announce Type: cross \nAbstract: Multi-agent reinforcement learning (MARL) methods have achieved state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms typically require significantly more environment interactions than their single-agent counterparts to converge, a problem exacerbated by the difficulty in exploring over a large joint action space and the high variance intrinsic to MARL environments. To tackle these issues, we propose a novel algorithm that combines a decomposed centralized critic with decentralized ensemble learning, incorporating several key contributions. The main component in our scheme is a selective exploration method that leverages ensemble kurtosis. We extend the global decomposed critic with a diversity-regularized ensemble of individual critics and utilize its excess kurtosis to guide exploration toward high-uncertainty states and actions. To improve sample efficiency, we train the centralized critic with a novel truncated variation of the TD($\\lambda$) algorithm, enabling efficient off-policy learning with reduced variance. On the actor side, our suggested algorithm adapts the mixed samples approach to MARL, mixing on-policy and off-policy loss functions for training the actors. This approach balances between stability and efficiency and outperforms purely off-policy learning. The evaluation shows our method outperforms state-of-the-art baselines on standard MARL benchmarks, including a variety of SMAC II maps."
      },
      {
        "id": "oai:arXiv.org:2506.02849v1",
        "title": "Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games",
        "link": "https://arxiv.org/abs/2506.02849",
        "author": "Alejandro Sanchez Roncero, Olov Andersson, Petter Ogren",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02849v1 Announce Type: cross \nAbstract: The increasing proliferation of small UAVs in civilian and military airspace has raised critical safety and security concerns, especially when unauthorized or malicious drones enter restricted zones. In this work, we present a reinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion. We train neural network policies to command body rates and collective thrust, enabling high-speed pursuit and evasive maneuvers that fully exploit the quadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic forgetting during adversarial co-training, we introduce an Asynchronous Multi-Stage Population-Based (AMSPB) algorithm where, at each stage, either the pursuer or evader learns against a sampled opponent drawn from a growing population of past and current policies. This continual learning setup ensures monotonic performance improvement and retention of earlier strategies. Our results show that (i) rate-based policies achieve significantly higher capture rates and peak speeds than velocity-level baselines, and (ii) AMSPB yields stable, monotonic gains against a suite of benchmark opponents."
      },
      {
        "id": "oai:arXiv.org:2506.02867v1",
        "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
        "link": "https://arxiv.org/abs/2506.02867",
        "author": "Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02867v1 Announce Type: cross \nAbstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at https://github.com/ChnQ/MI-Peaks."
      },
      {
        "id": "oai:arXiv.org:2506.02881v1",
        "title": "Simulation-Based Inference for Adaptive Experiments",
        "link": "https://arxiv.org/abs/2506.02881",
        "author": "Brian M Cho, Aur\\'elien Bibaut, Nathan Kallus",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02881v1 Announce Type: cross \nAbstract: Multi-arm bandit experimental designs are increasingly being adopted over standard randomized trials due to their potential to improve outcomes for study participants, enable faster identification of the best-performing options, and/or enhance the precision of estimating key parameters. Current approaches for inference after adaptive sampling either rely on asymptotic normality under restricted experiment designs or underpowered martingale concentration inequalities that lead to weak power in practice. To bypass these limitations, we propose a simulation-based approach for conducting hypothesis tests and constructing confidence intervals for arm specific means and their differences. Our simulation-based approach uses positively biased nuisances to generate additional trajectories of the experiment, which we call \\textit{simulation with optimism}. Using these simulations, we characterize the distribution potentially non-normal sample mean test statistic to conduct inference. We provide guarantees for (i) asymptotic type I error control, (ii) convergence of our confidence intervals, and (iii) asymptotic strong consistency of our estimator over a wide variety of common bandit designs. Our empirical results show that our approach achieves the desired coverage while reducing confidence interval widths by up to 50%, with drastic improvements for arms not targeted by the design."
      },
      {
        "id": "oai:arXiv.org:2506.02895v1",
        "title": "VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction",
        "link": "https://arxiv.org/abs/2506.02895",
        "author": "Ahmad AlMughrabi, Umair Haroon, Ricardo Marques, Petia Radeva",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02895v1 Announce Type: cross \nAbstract: Accurate food volume estimation is crucial for dietary monitoring, medical nutrition management, and food intake analysis. Existing 3D Food Volume estimation methods accurately compute the food volume but lack for food portions selection. We present VolTex, a framework that improves \\change{the food object selection} in food volume estimation. Allowing users to specify a target food item via text input to be segmented, our method enables the precise selection of specific food objects in real-world scenes. The segmented object is then reconstructed using the Neural Surface Reconstruction method to generate high-fidelity 3D meshes for volume computation. Extensive evaluations on the MetaFood3D dataset demonstrate the effectiveness of our approach in isolating and reconstructing food items for accurate volume estimation. The source code is accessible at https://github.com/GCVCG/VolTex."
      },
      {
        "id": "oai:arXiv.org:2506.02908v1",
        "title": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency",
        "link": "https://arxiv.org/abs/2506.02908",
        "author": "Bunlong Lay, Rostilav Makarov, Timo Gerkmann",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02908v1 Announce Type: cross \nAbstract: Diffusion models are a class of generative models that have been recently used for speech enhancement with remarkable success but are computationally expensive at inference time. Therefore, these models are impractical for processing streaming data in real-time. In this work, we adapt a sliding window diffusion framework to the speech enhancement task. Our approach progressively corrupts speech signals through time, assigning more noise to frames close to the present in a buffer. This approach outputs denoised frames with a delay proportional to the chosen buffer size, enabling a trade-off between performance and latency. Empirical results demonstrate that our method outperforms standard diffusion models and runs efficiently on a GPU, achieving an input-output latency in the order of 0.3 to 1 seconds. This marks the first practical diffusion-based solution for online speech enhancement."
      },
      {
        "id": "oai:arXiv.org:2506.02918v1",
        "title": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs",
        "link": "https://arxiv.org/abs/2506.02918",
        "author": "Shangmin Guo, Omar Darwiche Domingues, Rapha\\\"el Avalos, Aaron Courville, Florian Strub",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02918v1 Announce Type: cross \nAbstract: Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment."
      },
      {
        "id": "oai:arXiv.org:2506.02931v1",
        "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms",
        "link": "https://arxiv.org/abs/2506.02931",
        "author": "Praneet Sai Madhu Surabhi, Dheeraj Reddy Mudireddy, Jian Tao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02931v1 Announce Type: cross \nAbstract: This paper presents ThinkTank, a comprehensive and scalable framework designed to transform specialized AI agent systems into versatile collaborative intelligence platforms capable of supporting complex problem-solving across diverse domains. ThinkTank systematically generalizes agent roles, meeting structures, and knowledge integration mechanisms by adapting proven scientific collaboration methodologies. Through role abstraction, generalization of meeting types for iterative collaboration, and the integration of Retrieval-Augmented Generation with advanced knowledge storage, the framework facilitates expertise creation and robust knowledge sharing. ThinkTank enables organizations to leverage collaborative AI for knowledge-intensive tasks while ensuring data privacy and security through local deployment, utilizing frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is designed to deliver significant advantages in cost-effectiveness, data security, scalability, and competitive positioning compared to cloud-based alternatives, establishing it as a universal platform for AI-driven collaborative problem-solving. The ThinkTank code is available at https://github.com/taugroup/ThinkTank"
      },
      {
        "id": "oai:arXiv.org:2506.02955v1",
        "title": "UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models",
        "link": "https://arxiv.org/abs/2506.02955",
        "author": "Zewen Yang, Xiaobing Dai, Dian Yu, Qianru Li, Yu Li, Valentin Le Mesle",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02955v1 Announce Type: cross \nAbstract: Generative models have become increasingly powerful tools for robot motion generation, enabling flexible and multimodal trajectory generation across various tasks. Yet, most existing approaches remain limited in handling multiple types of constraints, such as collision avoidance and dynamic consistency, which are often treated separately or only partially considered. This paper proposes UniConFlow, a unified flow matching (FM) based framework for trajectory generation that systematically incorporates both equality and inequality constraints. UniConFlow introduces a novel prescribed-time zeroing function to enhance flexibility during the inference process, allowing the model to adapt to varying task requirements. To ensure constraint satisfaction, particularly with respect to obstacle avoidance, admissible action range, and kinodynamic consistency, the guidance inputs to the FM model are derived through a quadratic programming formulation, which enables constraint-aware generation without requiring retraining or auxiliary controllers. We conduct mobile navigation and high-dimensional manipulation tasks, demonstrating improved safety and feasibility compared to state-of-the-art constrained generative planners. Project page is available at https://uniconflow.github.io."
      },
      {
        "id": "oai:arXiv.org:2506.02980v1",
        "title": "Non-stationary Bandit Convex Optimization: A Comprehensive Study",
        "link": "https://arxiv.org/abs/2506.02980",
        "author": "Xiaoqi Liu, Dorian Baudry, Julian Zimmert, Patrick Rebeschini, Arya Akhavan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02980v1 Announce Type: cross \nAbstract: Bandit Convex Optimization is a fundamental class of sequential decision-making problems, where the learner selects actions from a continuous domain and observes a loss (but not its gradient) at only one point per round. We study this problem in non-stationary environments, and aim to minimize the regret under three standard measures of non-stationarity: the number of switches $S$ in the comparator sequence, the total variation $\\Delta$ of the loss functions, and the path-length $P$ of the comparator sequence. We propose a polynomial-time algorithm, Tilted Exponentially Weighted Average with Sleeping Experts (TEWA-SE), which adapts the sleeping experts framework from online convex optimization to the bandit setting. For strongly convex losses, we prove that TEWA-SE is minimax-optimal with respect to known $S$ and $\\Delta$ by establishing matching upper and lower bounds. By equipping TEWA-SE with the Bandit-over-Bandit framework, we extend our analysis to environments with unknown non-stationarity measures. For general convex losses, we introduce a second algorithm, clipped Exploration by Optimization (cExO), based on exponential weights over a discretized action space. While not polynomial-time computable, this method achieves minimax-optimal regret with respect to known $S$ and $\\Delta$, and improves on the best existing bounds with respect to $P$."
      },
      {
        "id": "oai:arXiv.org:2506.02992v1",
        "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation",
        "link": "https://arxiv.org/abs/2506.02992",
        "author": "Li Zhang, Kevin D. Ashley",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02992v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) are increasingly explored for legal argument generation, yet they pose significant risks of manipulation through hallucination and ungrounded persuasion, and often fail to utilize provided factual bases effectively or abstain when arguments are untenable. This paper introduces a novel reflective multi-agent method designed to address these challenges in the context of legally compliant persuasion. Our approach employs specialized agents--a Factor Analyst and an Argument Polisher--in an iterative refinement process to generate 3-ply legal arguments (plaintiff, defendant, rebuttal). We evaluate Reflective Multi-Agent against single-agent, enhanced-prompt single-agent, and non-reflective multi-agent baselines using four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e, Llama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\", and \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant superiority in successful abstention (preventing generation when arguments cannot be grounded), marked improvements in hallucination accuracy (reducing fabricated and misattributed factors), particularly in \"non-arguable\" scenarios, and enhanced factor utilization recall (improving the use of provided case facts). These findings suggest that structured reflection within a multi-agent framework offers a robust computable method for fostering ethical persuasion and mitigating manipulation in LLM-based legal argumentation systems, a critical step towards trustworthy AI in law. Project page: https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/"
      },
      {
        "id": "oai:arXiv.org:2506.03004v1",
        "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples",
        "link": "https://arxiv.org/abs/2506.03004",
        "author": "Junyu Liu, R. Kenny Jones, Daniel Ritchie",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03004v1 Announce Type: cross \nAbstract: We present PartComposer: a framework for part-level concept learning from single-image examples that enables text-to-image diffusion models to compose novel objects from meaningful components. Existing methods either struggle with effectively learning fine-grained concepts or require a large dataset as input. We propose a dynamic data synthesis pipeline generating diverse part compositions to address one-shot data scarcity. Most importantly, we propose to maximize the mutual information between denoised latents and structured concept codes via a concept predictor, enabling direct regulation on concept disentanglement and re-composition supervision. Our method achieves strong disentanglement and controllable composition, outperforming subject and part-level baselines when mixing concepts from the same, or different, object categories."
      },
      {
        "id": "oai:arXiv.org:2506.03013v1",
        "title": "How do Pre-Trained Models Support Software Engineering? An Empirical Study in Hugging Face",
        "link": "https://arxiv.org/abs/2506.03013",
        "author": "Alexandra Gonz\\'alez, Xavier Franch, David Lo, Silverio Mart\\'inez-Fern\\'andez",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03013v1 Announce Type: cross \nAbstract: Open-Source Pre-Trained Models (PTMs) provide extensive resources for various Machine Learning (ML) tasks, yet these resources lack a classification tailored to Software Engineering (SE) needs. To address this gap, we derive a taxonomy encompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a popular open-source ML repository, Hugging Face (HF). Our repository mining study began with a systematically gathered database of PTMs from the HF API, considering their model card descriptions and metadata, and the abstract of the associated arXiv papers. We confirmed SE relevance through multiple filtering steps: detecting outliers, identifying near-identical PTMs, and the use of Gemini 2.0 Flash, which was validated with five pilot studies involving three human annotators. This approach uncovered 2,205 SE PTMs. We find that code generation is the most common SE task among PTMs, primarily focusing on software implementation, while requirements engineering and software design activities receive limited attention. In terms of ML tasks, text generation dominates within SE PTMs. Notably, the number of SE PTMs has increased markedly since 2023 Q2. Our classification provides a solid foundation for future automated SE scenarios, such as the sampling and selection of suitable PTMs."
      },
      {
        "id": "oai:arXiv.org:2506.03032v1",
        "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
        "link": "https://arxiv.org/abs/2506.03032",
        "author": "Junhao Yu, Yan Zhuang, YuXuan Sun, Weibo Gao, Qi Liu, Mingyue Cheng, Zhenya Huang, Enhong Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03032v1 Announce Type: cross \nAbstract: Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takers' responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions."
      },
      {
        "id": "oai:arXiv.org:2506.03044v1",
        "title": "On the Benefits of Accelerated Optimization in Robust and Private Estimation",
        "link": "https://arxiv.org/abs/2506.03044",
        "author": "Laurentiu Andrei Marchis, Po-Ling Loh",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03044v1 Announce Type: cross \nAbstract: We study the advantages of accelerated gradient methods, specifically based on the Frank-Wolfe method and projected gradient descent, for privacy and heavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe method, our technique is based on a tailored learning rate and a uniform lower bound on the gradient of the $\\ell_2$-norm over the constraint set. For accelerating projected gradient descent, we use the popular variant based on Nesterov's momentum, and we optimize our objective over $\\mathbb{R}^p$. These accelerations reduce iteration complexity, translating into stronger statistical guarantees for empirical and population risk minimization. Our analysis covers three settings: non-random data, random model-free data, and parametric models (linear regression and generalized linear models). Methodologically, we approach both privacy and robustness based on noisy gradients. We ensure differential privacy via the Gaussian mechanism and advanced composition, and we achieve heavy-tailed robustness using a geometric median-of-means estimator, which also sharpens the dependency on the dimension of the covariates. Finally, we compare our rates to existing bounds and identify scenarios where our methods attain optimal convergence."
      },
      {
        "id": "oai:arXiv.org:2506.03049v1",
        "title": "Torsion in Persistent Homology and Neural Networks",
        "link": "https://arxiv.org/abs/2506.03049",
        "author": "Maria Walch",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03049v1 Announce Type: cross \nAbstract: We explore the role of torsion in hybrid deep learning models that incorporate topological data analysis, focusing on autoencoders. While most TDA tools use field coefficients, this conceals torsional features present in integer homology. We show that torsion can be lost during encoding, altered in the latent space, and in many cases, not reconstructed by standard decoders. Using both synthetic and high-dimensional data, we evaluate torsion sensitivity to perturbations and assess its recoverability across several autoencoder architectures. Our findings reveal key limitations of field-based approaches and underline the need for architectures or loss terms that preserve torsional information for robust data representation."
      },
      {
        "id": "oai:arXiv.org:2506.03053v1",
        "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
        "link": "https://arxiv.org/abs/2506.03053",
        "author": "Sinem Erisken (Independent Researcher), Timothy Gothard (Independent Researcher), Martin Leitgab (Independent Researcher), Ram Potham (Independent Researcher)",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03053v1 Announce Type: cross \nAbstract: Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts."
      },
      {
        "id": "oai:arXiv.org:2506.03056v1",
        "title": "Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models",
        "link": "https://arxiv.org/abs/2506.03056",
        "author": "Ram Potham (Independent Researcher), Max Harms (Machine Intelligence Research Institute)",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03056v1 Announce Type: cross \nAbstract: Foundation models (FMs) face a critical safety challenge: as capabilities scale, instrumental convergence drives default trajectories toward loss of human control, potentially culminating in existential catastrophe. Current alignment approaches struggle with value specification complexity and fail to address emergent power-seeking behaviors. We propose \"Corrigibility as a Singular Target\" (CAST)-designing FMs whose overriding objective is empowering designated human principals to guide, correct, and control them. This paradigm shift from static value-loading to dynamic human empowerment transforms instrumental drives: self-preservation serves only to maintain the principal's control; goal modification becomes facilitating principal guidance. We present a comprehensive empirical research agenda spanning training methodologies (RLAIF, SFT, synthetic data generation), scalability testing across model sizes, and demonstrations of controlled instructability. Our vision: FMs that become increasingly responsive to human guidance as capabilities grow, offering a path to beneficial AI that remains as tool-like as possible, rather than supplanting human judgment. This addresses the core alignment problem at its source, preventing the default trajectory toward misaligned instrumental convergence."
      },
      {
        "id": "oai:arXiv.org:2506.03068v1",
        "title": "Causal Explainability of Machine Learning in Heart Failure Prediction from Electronic Health Records",
        "link": "https://arxiv.org/abs/2506.03068",
        "author": "Yina Hou, Shourav B. Rabbani, Liang Hong, Norou Diawara, Manar D. Samad",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03068v1 Announce Type: cross \nAbstract: The importance of clinical variables in the prognosis of the disease is explained using statistical correlation or machine learning (ML). However, the predictive importance of these variables may not represent their causal relationships with diseases. This paper uses clinical variables from a heart failure (HF) patient cohort to investigate the causal explainability of important variables obtained in statistical and ML contexts. Due to inherent regression modeling, popular causal discovery methods strictly assume that the cause and effect variables are numerical and continuous. This paper proposes a new computational framework to enable causal structure discovery (CSD) and score the causal strength of mixed-type (categorical, numerical, binary) clinical variables for binary disease outcomes. In HF classification, we investigate the association between the importance rank order of three feature types: correlated features, features important for ML predictions, and causal features. Our results demonstrate that CSD modeling for nonlinear causal relationships is more meaningful than its linear counterparts. Feature importance obtained from nonlinear classifiers (e.g., gradient-boosting trees) strongly correlates with the causal strength of variables without differentiating cause and effect variables. Correlated variables can be causal for HF, but they are rarely identified as effect variables. These results can be used to add the causal explanation of variables important for ML-based prediction modeling."
      },
      {
        "id": "oai:arXiv.org:2506.03074v1",
        "title": "GL-LowPopArt: A Nearly Instance-Wise Minimax Estimator for Generalized Low-Rank Trace Regression",
        "link": "https://arxiv.org/abs/2506.03074",
        "author": "Junghyun Lee, Kyoungseok Jang, Kwang-Sung Jun, Milan Vojnovi\\'c, Se-Young Yun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03074v1 Announce Type: cross \nAbstract: We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it employs a two-stage approach: nuclear norm regularization followed by matrix Catoni estimation. We establish state-of-the-art estimation error bounds, surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and reveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key technical challenge is controlling bias from the nonlinear inverse link function, which we address by our two-stage approach. We prove a *local* minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise optimality up to the condition number of the ground-truth Hessian. Applications include generalized linear matrix completion, where `GL-LowPopArt` achieves a state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a novel setting inspired by general preference learning (Zhang et al., 2024). Our analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new, potentially interesting problem-dependent quantity, along with improved Borda regret bound than vectorization (Wu et al., 2024)."
      },
      {
        "id": "oai:arXiv.org:2506.03088v1",
        "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning",
        "link": "https://arxiv.org/abs/2506.03088",
        "author": "Lloyd Pellatt, Fotios Drakopoulos, Shievanie Sabesan, Nicholas A. Lesica",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03088v1 Announce Type: cross \nAbstract: The mapping from sound to neural activity that underlies hearing is highly non-linear. The first few stages of this mapping in the cochlea have been modelled successfully, with biophysical models built by hand and, more recently, with DNN models trained on datasets simulated by biophysical models. Modelling the auditory brain has been a challenge because central auditory processing is too complex for models to be built by hand, and datasets for training DNN models directly have not been available. Recent work has taken advantage of large-scale high resolution neural recordings from the auditory midbrain to build a DNN model of normal hearing with great success. But this model assumes that auditory processing is the same in all brains, and therefore it cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space of hearing loss directly from recordings of neural activity in the auditory midbrain of healthy and noise exposed animals. With hearing loss parametrised by only 6 free parameters per animal, our model accurately predicts 62\\% of the explainable variance in neural responses from normal hearing animals and 68% for hearing impaired animals, within a few percentage points of state of the art animal specific models. We demonstrate that the model can be used to simulate realistic activity from out of sample animals by fitting only the learned conditioning parameters with Bayesian optimisation, achieving crossentropy loss within 2% of the optimum in 15-30 iterations. Including more animals in the training data slightly improved the performance on unseen animals. This model will enable future development of parametrised hearing loss compensation models trained to directly restore normal neural coding in hearing impaired brains, which can be quickly fitted for a new user by human in the loop optimisation."
      },
      {
        "id": "oai:arXiv.org:2506.03095v1",
        "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
        "link": "https://arxiv.org/abs/2506.03095",
        "author": "Man Luo, David Cobbley, Xin Su, Shachar Rosenman, Vasudev Lal, Shao-Yen Tseng, Phillip Howard",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03095v1 Announce Type: cross \nAbstract: Computer use agents (CUA) are systems that automatically interact with graphical user interfaces (GUIs) to complete tasks. CUA have made significant progress with the advent of large vision-language models (VLMs). However, these agents typically rely on cloud-based inference with substantial compute demands, raising critical privacy and scalability concerns, especially when operating on personal devices. In this work, we take a step toward privacy-preserving and resource-efficient agents by developing a lightweight vision-language model that runs entirely on local machines. To train this compact agent, we introduce an LLM-as-Judge framework that automatically evaluates and filters synthetic interaction trajectories, producing high-quality data for reinforcement learning without human annotation. Experiments on the OS-World benchmark demonstrate that our fine-tuned local model outperforms existing baselines, highlighting a promising path toward private, efficient, and generalizable GUI agents."
      },
      {
        "id": "oai:arXiv.org:2506.03118v1",
        "title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers",
        "link": "https://arxiv.org/abs/2506.03118",
        "author": "Zhiyuan Yu, Zhe Li, Hujun Bao, Can Yang, Xiaowei Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03118v1 Announce Type: cross \nAbstract: 3D human reconstruction and animation are long-standing topics in computer graphics and vision. However, existing methods typically rely on sophisticated dense-view capture and/or time-consuming per-subject optimization procedures. To address these limitations, we propose HumanRAM, a novel feed-forward approach for generalizable human reconstruction and animation from monocular or sparse human images. Our approach integrates human reconstruction and animation into a unified framework by introducing explicit pose conditions, parameterized by a shared SMPL-X neural texture, into transformer-based large reconstruction models (LRM). Given monocular or sparse input images with associated camera parameters and SMPL-X poses, our model employs scalable transformers and a DPT-based decoder to synthesize realistic human renderings under novel viewpoints and novel poses. By leveraging the explicit pose conditions, our model simultaneously enables high-quality human reconstruction and high-fidelity pose-controlled animation. Experiments show that HumanRAM significantly surpasses previous methods in terms of reconstruction accuracy, animation fidelity, and generalization performance on real-world datasets. Video results are available at https://zju3dv.github.io/humanram/."
      },
      {
        "id": "oai:arXiv.org:2506.03120v1",
        "title": "Validating remotely sensed biomass estimates with forest inventory data in the western US",
        "link": "https://arxiv.org/abs/2506.03120",
        "author": "Xiuyu Cao, Joseph O. Sexton, Panshi Wang, Dimitrios Gounaridis, Neil H. Carter, Kai Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03120v1 Announce Type: cross \nAbstract: Monitoring aboveground biomass (AGB) and its density (AGBD) at high resolution is essential for carbon accounting and ecosystem management. While NASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission provides globally distributed reference measurements for AGBD estimation, the majority of commercial remote sensing products based on GEDI remain without rigorous or independent validation. Here, we present an independent regional validation of an AGBD dataset offered by terraPulse, Inc., based on independent reference data from the US Forest Service Forest Inventory and Analysis (FIA) program. Aggregated to 64,000-hectare hexagons and US counties across the US states of Utah, Nevada, and Washington, we found very strong agreement between terraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE = 26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale, agreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95. Spatial and statistical analyses indicated that terraPulse AGBD values tended to exceed FIA estimates in non-forest areas, likely due to FIA's limited sampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited lower values in high-biomass forests, likely due to saturation effects in its optical remote-sensing covariates. This study advances operational carbon monitoring by delivering a scalable framework for comprehensive AGBD validation using independent FIA data, as well as a benchmark validation of a new commercial dataset for global biomass monitoring."
      },
      {
        "id": "oai:arXiv.org:2506.03134v1",
        "title": "Simulate Any Radar: Attribute-Controllable Radar Simulation via Waveform Parameter Embedding",
        "link": "https://arxiv.org/abs/2506.03134",
        "author": "Weiqing Xiao, Hao Huang, Chonghao Zhong, Yujie Lin, Nan Wang, Xiaoxue Chen, Zhaoxi Chen, Saining Zhang, Shuocheng Yang, Pierre Merriaux, Lei Lei, Hao Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03134v1 Announce Type: cross \nAbstract: We present SA-Radar (Simulate Any Radar), a radar simulation approach that enables controllable and efficient generation of radar cubes conditioned on customizable radar attributes. Unlike prior generative or physics-based simulators, SA-Radar integrates both paradigms through a waveform-parameterized attribute embedding. We design ICFAR-Net, a 3D U-Net conditioned on radar attributes encoded via waveform parameters, which captures signal variations induced by different radar configurations. This formulation bypasses the need for detailed radar hardware specifications and allows efficient simulation of range-azimuth-Doppler (RAD) tensors across diverse sensor settings. We further construct a mixed real-simulated dataset with attribute annotations to robustly train the network. Extensive evaluations on multiple downstream tasks-including 2D/3D object detection and radar semantic segmentation-demonstrate that SA-Radar's simulated data is both realistic and effective, consistently improving model performance when used standalone or in combination with real data. Our framework also supports simulation in novel sensor viewpoints and edited scenes, showcasing its potential as a general-purpose radar data engine for autonomous driving applications. Code and additional materials are available at https://zhuxing0.github.io/projects/SA-Radar."
      },
      {
        "id": "oai:arXiv.org:2005.04088v5",
        "title": "Automatic Cross-Domain Transfer Learning for Linear Regression",
        "link": "https://arxiv.org/abs/2005.04088",
        "author": "Xinshun Liu, He Xin, Mao Hui, Liu Jing, Lai Weizhong, Ye Qingwen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2005.04088v5 Announce Type: replace \nAbstract: Transfer learning research attempts to make model induction transferable across different domains. This method assumes that specific information regarding to which domain each instance belongs is known. This paper helps to extend the capability of transfer learning for linear regression problems to situations where the domain information is uncertain or unknown; in fact, the framework can be extended to classification problems. For normal datasets, we assume that some latent domain information is available for transfer learning. The instances in each domain can be inferred by different parameters. We obtain this domain information from the distribution of the regression coefficients corresponding to the explanatory variable $x$ as well as the response variable $y$ based on a Dirichlet process, which is more reasonable. As a result, we transfer not only variable $x$ as usual but also variable $y$, which is challenging since the testing data have no response value. Previous work mainly overcomes the problem via pseudo-labelling based on transductive learning, which introduces serious bias. We provide a novel framework for analysing the problem and considering this general situation: the joint distribution of variable $x$ and variable $y$. Furthermore, our method controls the bias well compared with previous work. We perform linear regression on the new feature space that consists of different latent domains and the target domain, which is from the testing data. The experimental results show that the proposed model performs well on real datasets."
      },
      {
        "id": "oai:arXiv.org:2110.13658v2",
        "title": "Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?",
        "link": "https://arxiv.org/abs/2110.13658",
        "author": "Arij Riabi, Beno\\^it Sagot, Djam\\'e Seddah",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2110.13658v2 Announce Type: replace \nAbstract: Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high-resource languages. Building language models and, more generally, NLP systems for non-standardized and low-resource languages remains a challenging task. In this work, we focus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data displaying a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre-trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set-tings."
      },
      {
        "id": "oai:arXiv.org:2111.00157v3",
        "title": "TransAug: Translate as Augmentation for Sentence Embeddings",
        "link": "https://arxiv.org/abs/2111.00157",
        "author": "Jue Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2111.00157v3 Announce Type: replace \nAbstract: While contrastive learning greatly advances the representation of sentence embeddings, it is still limited by the size of the existing sentence datasets. In this paper, we present TransAug (Translate as Augmentation), which provide the first exploration of utilizing translated sentence pairs as data augmentation for text, and introduce a two-stage paradigm to advances the state-of-the-art sentence embeddings. Instead of adopting an encoder trained in other languages setting, we first distill a Chinese encoder from a SimCSE encoder (pretrained in English), so that their embeddings are close in semantic space, which can be regraded as implicit data augmentation. Then, we only update the English encoder via cross-lingual contrastive learning and frozen the distilled Chinese encoder. Our approach achieves a new state-of-art on standard semantic textual similarity (STS), outperforming both SimCSE and Sentence-T5, and the best performance in corresponding tracks on transfer tasks evaluated by SentEval."
      },
      {
        "id": "oai:arXiv.org:2209.01205v4",
        "title": "Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion",
        "link": "https://arxiv.org/abs/2209.01205",
        "author": "Han Wu, Jie Yin, Bala Rajaratnam, Jianyuan Guo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2209.01205v4 Announce Type: replace \nAbstract: Knowledge graphs (KGs) are powerful in terms of their inference abilities, but are also notorious for their incompleteness and long-tail distribution of relations. To address these challenges and expand the coverage of KGs, few-shot KG completion aims to make predictions for triplets involving novel relations when only a few training triplets are provided as reference. Previous methods have focused on designing local neighbor aggregators to learn entity-level information and/or imposing a potentially invalid sequential dependency assumption at the triplet level to learn meta relation information. However, pairwise triplet-level interactions and context-level relational information have been largely overlooked for learning meta representations of few-shot relations. In this paper, we propose a hierarchical relational learning method (HiRe) for few-shot KG completion. By jointly capturing three levels of relational information (entity-level, triplet-level and context-level), HiRe can effectively learn and refine meta representations of few-shot relations, and thus generalize well to new unseen relations. Extensive experiments on benchmark datasets validate the superiority of HiRe over state-of-the-art methods. The code can be found in https://github.com/alexhw15/HiRe.git."
      },
      {
        "id": "oai:arXiv.org:2301.00555v2",
        "title": "Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning",
        "link": "https://arxiv.org/abs/2301.00555",
        "author": "Jisu Shin, Seunghyun Shin, Hae-Gon Jeon",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2301.00555v2 Announce Type: replace \nAbstract: Understanding the informative structures of scenes is essential for low-level vision tasks. Unfortunately, it is difficult to obtain a concrete visual definition of the informative structures because influences of visual features are task-specific. In this paper, we propose a single general neural network architecture for extracting task-specific structure guidance for scenes. To do this, we first analyze traditional spectral clustering methods, which computes a set of eigenvectors to model a segmented graph forming small compact structures on image domains. We then unfold the traditional graph-partitioning problem into a learnable network, named \\textit{Scene Structure Guidance Network (SSGNet)}, to represent the task-specific informative structures. The SSGNet yields a set of coefficients of eigenvectors that produces explicit feature representations of image structures. In addition, our SSGNet is light-weight ($\\sim$ 56K parameters), and can be used as a plug-and-play module for off-the-shelf architectures. We optimize the SSGNet without any supervision by proposing two novel training losses that enforce task-specific scene structure generation during training. Our main contribution is to show that such a simple network can achieve state-of-the-art results for several low-level vision applications. We also demonstrate that our network generalizes well on unseen datasets, compared to existing methods which use structural embedding frameworks. We further propose a lighter version of SSGNet ($\\sim$ 29K parameters) for depth computation, SSGNet-D, and successfully execute it on edge computing devices like Jetson AGX Orin, improving the performance of baseline network, even in the wild, with little computational delay."
      },
      {
        "id": "oai:arXiv.org:2303.12892v3",
        "title": "Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset",
        "link": "https://arxiv.org/abs/2303.12892",
        "author": "Thanh-Dung Le, Philippe Jouvet, Rita Noumeir",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2303.12892v3 Announce Type: replace \nAbstract: Transformer-based models have shown outstanding results in natural language processing but face challenges in applications like classifying small-scale clinical texts, especially with constrained computational resources. This study presents a customized Mixture of Expert (MoE) Transformer models for classifying small-scale French clinical texts at CHU Sainte-Justine Hospital. The MoE-Transformer addresses the dual challenges of effective training with limited data and low-resource computation suitable for in-house hospital use. Despite the success of biomedical pre-trained models such as CamemBERT-bio, DrBERT, and AliBERT, their high computational demands make them impractical for many clinical settings. Our MoE-Transformer model not only outperforms DistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset but also achieves impressive results: an accuracy of 87\\%, precision of 87\\%, recall of 85\\%, and F1-score of 86\\%. While the MoE-Transformer does not surpass the performance of biomedical pre-trained BERT models, it can be trained at least 190 times faster, offering a viable alternative for settings with limited data and computational resources. Although the MoE-Transformer addresses challenges of generalization gaps and sharp minima, demonstrating some limitations for efficient and accurate clinical text classification, this model still represents a significant advancement in the field. It is particularly valuable for classifying small French clinical narratives within the privacy and constraints of hospital-based computational resources."
      },
      {
        "id": "oai:arXiv.org:2305.15612v4",
        "title": "Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning",
        "link": "https://arxiv.org/abs/2305.15612",
        "author": "Jungtaek Kim",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2305.15612v4 Announce Type: replace \nAbstract: Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of efficiently finding a global optimum of an expensive-to-evaluate black-box function. In general, a probabilistic regression model is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based methods, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, supervised classifiers are employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy are prone to be overconfident for known knowledge on global solution candidates. Supposing that we have access to unlabeled points, e.g., predefined fixed-size pools, we propose density ratio estimation-based Bayesian optimization with semi-supervised learning to solve this challenge. Finally, we show the empirical results of our methods and several baseline methods in two distinct scenarios with unlabeled point sampling and a fixed-size pool, and analyze the validity of our methods in diverse experiments."
      },
      {
        "id": "oai:arXiv.org:2306.08586v3",
        "title": "Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs in Decentralized Settings",
        "link": "https://arxiv.org/abs/2306.08586",
        "author": "Yehya Farhat, Hamza ElMokhtar Shili, Fangshuo Liao, Chen Dun, Mirian Hipolito Garcia, Guoqing Zheng, Ahmed Hassan Awadallah, Robert Sim, Dimitrios Dimitriadis, Anastasios Kyrillidis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2306.08586v3 Announce Type: replace \nAbstract: Mixture-of-Experts (MoEs) achieve scalability by dynamically activating subsets of their components. Yet, understanding how expertise emerges through joint training of gating mechanisms and experts remains incomplete, especially in scenarios without clear task partitions. Motivated by inference costs and data heterogeneity, we study how joint training of gating functions and experts can dynamically allocate domain-specific expertise across multiple underlying data distributions. As an outcome of our framework, we develop an instance tailored specifically to decentralized training scenarios, introducing \\textit{Dynamically Decentralized Orchestration of MoEs} or \\texttt{DDOME}. \\texttt{DDOME} leverages heterogeneity emerging from distributional shifts across decentralized data sources to specialize experts dynamically. By integrating a pretrained common expert to inform a gating function, \\texttt{DDOME} achieves personalized expert subset selection on-the-fly, facilitating just-in-time personalization. We empirically validate \\texttt{DDOME} within a Federated Learning (FL) context: \\texttt{DDOME} attains from 4\\% up to an 24\\% accuracy improvement over state-of-the-art FL baselines in image and text classification tasks, while maintaining competitive zero-shot generalization capabilities. Furthermore, we provide theoretical insights confirming that the joint gating-experts training is critical for achieving meaningful expert specialization."
      },
      {
        "id": "oai:arXiv.org:2306.15507v2",
        "title": "Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling Shutter Frames",
        "link": "https://arxiv.org/abs/2306.15507",
        "author": "Yunfan Lu, Guoqiang Liang, Yiran Shen, Lin Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2306.15507v2 Announce Type: replace \nAbstract: Most consumer cameras use rolling shutter (RS) exposure, which often leads to distortions such as skew and jelly effects. These videos are further limited by bandwidth and frame rate constraints. In this paper, we explore the potential of event cameras, which offer high temporal resolution. We propose a framework to recover global shutter (GS) high-frame-rate videos without RS distortion by combining an RS camera and an event camera. Due to the lack of real-world datasets, our framework adopts a self-supervised strategy based on a displacement field, a dense 3D spatiotemporal representation of pixel motion during exposure. This enables mutual reconstruction between RS and GS frames and facilitates slow-motion recovery. We combine RS frames with the displacement field to generate GS frames, and integrate inverse mapping and RS frame warping for self-supervision. Experiments on four datasets show that our method removes distortion, reduces bandwidth usage by 94 percent, and achieves 16 ms per frame at 32x interpolation."
      },
      {
        "id": "oai:arXiv.org:2306.17301v3",
        "title": "Why Shallow Networks Struggle to Approximate and Learn High Frequencies",
        "link": "https://arxiv.org/abs/2306.17301",
        "author": "Shijun Zhang, Hongkai Zhao, Yimin Zhong, Haomin Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2306.17301v3 Announce Type: replace \nAbstract: In this work, we present a comprehensive study combining mathematical and computational analysis to explain why a two-layer neural network struggles to handle high frequencies in both approximation and learning, especially when machine precision, numerical noise, and computational cost are significant factors in practice. Specifically, we investigate the following fundamental computational issues: (1) the minimal numerical error achievable under finite precision, (2) the computational cost required to attain a given accuracy, and (3) the stability of the method with respect to perturbations. The core of our analysis lies in the conditioning of the representation and its learning dynamics. Explicit answers to these questions are provided, along with supporting numerical evidence."
      },
      {
        "id": "oai:arXiv.org:2307.08507v4",
        "title": "Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients",
        "link": "https://arxiv.org/abs/2307.08507",
        "author": "Mete Kemertas, Allan D. Jepson, Amir-massoud Farahmand",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2307.08507v4 Announce Type: replace \nAbstract: We propose Mirror Descent Optimal Transport (MDOT), a novel method for solving discrete optimal transport (OT) problems with high precision, by unifying temperature annealing in entropic-regularized OT (EOT) with mirror descent techniques. In this framework, temperature annealing produces a sequence of EOT dual problems, whose solution gradually gets closer to the solution of the original OT problem. We solve each problem efficiently using a GPU-parallel nonlinear conjugate gradients algorithm (PNCG) that outperforms traditional Sinkhorn iterations under weak regularization. Moreover, our investigation also reveals that the theoretical convergence rate of Sinkhorn iterations can exceed existing non-asymptotic bounds when its stopping criterion is tuned in a manner analogous to MDOT.\n  Our comprehensive ablation studies of MDOT-PNCG affirm its robustness across a wide range of algorithmic parameters. Benchmarking on 24 problem sets of size $n=4096$ in a GPU environment demonstrate that our method attains high-precision, feasible solutions significantly faster than a representative set of existing OT solvers, including accelerated gradient methods and advanced Sinkhorn variants, in both wall-clock time and number of operations. Empirical convergence rates range between $O(n^2 \\varepsilon^{-1/4})$ and $O(n^2 \\varepsilon^{-1})$, where $\\varepsilon$ is the optimality gap. For problem sizes up to $n=16384$, the empirical runtime scales as $O(n^2)$ for moderate precision and as $O(n^{5/2})$ at worst for high precision. These findings establish MDOT-PNCG as a compelling alternative to current OT solvers, particularly in challenging weak-regularization regimes."
      },
      {
        "id": "oai:arXiv.org:2309.13570v5",
        "title": "Robust 6DoF Pose Estimation Against Depth Noise and a Comprehensive Evaluation on a Mobile Dataset",
        "link": "https://arxiv.org/abs/2309.13570",
        "author": "Zixun Huang, Keling Yao, Seth Z. Zhao, Chuanyu Pan, Allen Y. Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2309.13570v5 Announce Type: replace \nAbstract: Robust 6DoF pose estimation with mobile devices is the foundation for applications in robotics, augmented reality, and digital twin localization. In this paper, we extensively investigate the robustness of existing RGBD-based 6DoF pose estimation methods against varying levels of depth sensor noise. We highlight that existing 6DoF pose estimation methods suffer significant performance discrepancies due to depth measurement inaccuracies. In response to the robustness issue, we present a simple and effective transformer-based 6DoF pose estimation approach called DTTDNet, featuring a novel geometric feature filtering module and a Chamfer distance loss for training. Moreover, we advance the field of robust 6DoF pose estimation and introduce a new dataset -- Digital Twin Tracking Dataset Mobile (DTTD-Mobile), tailored for digital twin object tracking with noisy depth data from the mobile RGBD sensor suite of the Apple iPhone 14 Pro. Extensive experiments demonstrate that DTTDNet significantly outperforms state-of-the-art methods at least 4.32, up to 60.74 points in ADD metrics on the DTTD-Mobile. More importantly, our approach exhibits superior robustness to varying levels of measurement noise, setting a new benchmark for robustness to measurement noise. The project page is publicly available at https://openark-berkeley.github.io/DTTDNet/."
      },
      {
        "id": "oai:arXiv.org:2309.14907v2",
        "title": "Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias",
        "link": "https://arxiv.org/abs/2309.14907",
        "author": "Zhihao Shi, Jie Wang, Fanghua Lu, Hanzhu Chen, Defu Lian, Zheng Wang, Jieping Ye, Feng Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2309.14907v2 Announce Type: replace \nAbstract: Node representation learning on attributed graphs -- whose nodes are associated with rich attributes (e.g., texts and protein sequences) -- plays a crucial role in many important downstream tasks. To encode the attributes and graph structures simultaneously, recent studies integrate pre-trained models with graph neural networks (GNNs), where pre-trained models serve as node encoders (NEs) to encode the attributes. As jointly training large NEs and GNNs on large-scale graphs suffers from severe scalability issues, many methods propose to train NEs and GNNs separately. Consequently, they do not take feature convolutions in GNNs into consideration in the training phase of NEs, leading to a significant learning bias relative to the joint training. To address this challenge, we propose an efficient label regularization technique, namely Label Deconvolution (LD), to alleviate the learning bias by a novel and highly scalable approximation to the inverse mapping of GNNs. The inverse mapping leads to an objective function that is equivalent to that by the joint training, while it can effectively incorporate GNNs in the training phase of NEs against the learning bias. More importantly, we show that LD converges to the optimal objective function values by the joint training under mild assumptions. Experiments demonstrate LD significantly outperforms state-of-the-art methods on Open Graph Benchmark datasets."
      },
      {
        "id": "oai:arXiv.org:2310.05026v3",
        "title": "Low-Resolution Self-Attention for Semantic Segmentation",
        "link": "https://arxiv.org/abs/2310.05026",
        "author": "Yu-Huan Wu, Shi-Chen Zhang, Yun Liu, Le Zhang, Xin Zhan, Daquan Zhou, Jiashi Feng, Ming-Ming Cheng, Liangli Zhen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2310.05026v3 Announce Type: replace \nAbstract: Semantic segmentation tasks naturally require high-resolution information for pixel-wise segmentation and global context information for class prediction. While existing vision transformers demonstrate promising performance, they often utilize high-resolution context modeling, resulting in a computational bottleneck. In this work, we challenge conventional wisdom and introduce the Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a significantly reduced computational cost, i.e., FLOPs. Our approach involves computing self-attention in a fixed low-resolution space regardless of the input image's resolution, with additional 3x3 depth-wise convolutions to capture fine details in the high-resolution space. We demonstrate the effectiveness of our LRSA approach by building the LRFormer, a vision transformer with an encoder-decoder structure. Extensive experiments on the ADE20K, COCO-Stuff, and Cityscapes datasets demonstrate that LRFormer outperforms state-of-the-art models. Code is available at https://github.com/yuhuan-wu/LRFormer."
      },
      {
        "id": "oai:arXiv.org:2310.06077v2",
        "title": "Performative Time-Series Forecasting",
        "link": "https://arxiv.org/abs/2310.06077",
        "author": "Zhiyuan Zhao, Haoxin Liu, Alexander Rodriguez, B. Aditya Prakash",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2310.06077v2 Announce Type: replace \nAbstract: Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.\n  In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subsequently predicts targets accordingly. We provide theoretical insights suggesting that FPS can potentially lead to reduced generalization error. We conduct comprehensive experiments using multiple time-series models on COVID-19 and traffic forecasting tasks. The results demonstrate that FPS consistently outperforms conventional time-series forecasting methods, highlighting its efficacy in handling performativity-induced challenges."
      },
      {
        "id": "oai:arXiv.org:2311.17797v3",
        "title": "Learning to Simulate: Generative Metamodeling via Quantile Regression",
        "link": "https://arxiv.org/abs/2311.17797",
        "author": "L. Jeff Hong, Yanxi Hou, Qingkai Zhang, Xiaowei Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2311.17797v3 Announce Type: replace \nAbstract: Stochastic simulation models effectively capture complex system dynamics but are often too slow for real-time decision-making. Traditional metamodeling techniques learn relationships between simulator inputs and a single output summary statistic, such as the mean or median. These techniques enable real-time predictions without additional simulations. However, they require prior selection of one appropriate output summary statistic, limiting their flexibility in practical applications. We propose a new concept: generative metamodeling. It aims to construct a \"fast simulator of the simulator,\" generating random outputs significantly faster than the original simulator while preserving approximately equal conditional distributions. Generative metamodels enable rapid generation of numerous random outputs upon input specification, facilitating immediate computation of any summary statistic for real-time decision-making. We introduce a new algorithm, quantile-regression-based generative metamodeling (QRGMM), and establish its distributional convergence and convergence rate. Extensive numerical experiments demonstrate QRGMM's efficacy compared to other state-of-the-art generative algorithms in practical real-time decision-making scenarios."
      },
      {
        "id": "oai:arXiv.org:2311.18703v5",
        "title": "Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization",
        "link": "https://arxiv.org/abs/2311.18703",
        "author": "Daniel Jarne Ornia, Giannis Delimpaltadakis, Jens Kober, Javier Alonso-Mora",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2311.18703v5 Announce Type: replace \nAbstract: In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularisation) to randomise their actions in favor of exploration. This often makes it challenging for other agents and humans to predict an agent's behavior, triggering unsafe scenarios (e.g. in human-robot interaction). We propose a novel method to induce predictable behavior in RL agents, termed Predictability-Aware RL (PARL), employing the agent's trajectory entropy rate to quantify predictability. Our method maximizes a linear combination of a standard discounted reward and the negative entropy rate, thus trading off optimality with predictability. We show how the entropy rate can be formally cast as an average reward, how entropy-rate value functions can be estimated from a learned model and incorporate this in policy-gradient algorithms, and demonstrate how this approach produces predictable (near-optimal) policies in tasks inspired by human-robot use-cases."
      },
      {
        "id": "oai:arXiv.org:2312.13528v3",
        "title": "MoBluRF: Motion Deblurring Neural Radiance Fields for Blurry Monocular Video",
        "link": "https://arxiv.org/abs/2312.13528",
        "author": "Minh-Quan Viet Bui, Jongmin Park, Jihyong Oh, Munchurl Kim",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2312.13528v3 Announce Type: replace \nAbstract: Neural Radiance Fields (NeRF), initially developed for static scenes, have inspired many video novel view synthesis techniques. However, the challenge for video view synthesis arises from motion blur, a consequence of object or camera movements during exposure, which hinders the precise synthesis of sharp spatio-temporal views. In response, we propose a novel motion deblurring NeRF framework for blurry monocular video, called MoBluRF, consisting of a Base Ray Initialization (BRI) stage and a Motion Decomposition-based Deblurring (MDD) stage. In the BRI stage, we coarsely reconstruct dynamic 3D scenes and jointly initialize the base rays which are further used to predict latent sharp rays, using the inaccurate camera pose information from the given blurry frames. In the MDD stage, we introduce a novel Incremental Latent Sharp-rays Prediction (ILSP) approach for the blurry monocular video frames by decomposing the latent sharp rays into global camera motion and local object motion components. We further propose two loss functions for effective geometry regularization and decomposition of static and dynamic scene components without any mask supervision. Experiments show that MoBluRF outperforms qualitatively and quantitatively the recent state-of-the-art methods with large margins."
      },
      {
        "id": "oai:arXiv.org:2401.11311v3",
        "title": "A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models",
        "link": "https://arxiv.org/abs/2401.11311",
        "author": "Reda Bensaid, Vincent Gripon, Fran\\c{c}ois Leduc-Primeau, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2401.11311v3 Announce Type: replace \nAbstract: Few-shot semantic segmentation (FSS) is a crucial challenge in computer vision, driving extensive research into a diverse range of methods, from advanced meta-learning techniques to simple transfer learning baselines. With the emergence of vision foundation models (VFM) serving as generalist feature extractors, we seek to explore the adaptation of these models for FSS. While current FSS benchmarks focus on adapting pre-trained models to new tasks with few images, they emphasize in-domain generalization, making them less suitable for VFM trained on large-scale web datasets. To address this, we propose a novel realistic benchmark with a simple and straightforward adaptation process tailored for this task. Using this benchmark, we conduct a comprehensive comparative analysis of prominent VFM and semantic segmentation models. To evaluate their effectiveness, we leverage various adaption methods, ranging from linear probing to parameter efficient fine-tuning (PEFT) and full fine-tuning. Our findings show that models designed for segmentation can be outperformed by self-supervised (SSL) models. On the other hand, while PEFT methods yields competitive performance, they provide little discrepancy in the obtained results compared to other methods, highlighting the critical role of the feature extractor in determining results. To our knowledge, this is the first study on the adaptation of VFM for FSS."
      },
      {
        "id": "oai:arXiv.org:2402.06614v2",
        "title": "The Complexity of Sequential Prediction in Dynamical Systems",
        "link": "https://arxiv.org/abs/2402.06614",
        "author": "Vinod Raman, Unique Subedi, Ambuj Tewari",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2402.06614v2 Announce Type: replace \nAbstract: We study the problem of learning to predict the next state of a dynamical system when the underlying evolution function is unknown. Unlike previous work, we place no parametric assumptions on the dynamical system, and study the problem from a learning theory perspective. We define new combinatorial measures and dimensions and show that they quantify the optimal mistake and regret bounds in the realizable and agnostic settings respectively. By doing so, we find that in the realizable setting, the total number of mistakes can grow according to \\emph{any} increasing function of the time horizon $T$. In contrast, we show that in the agnostic setting under the commonly studied notion of Markovian regret, the only possible rates are $\\Theta(T)$ and $\\tilde{\\Theta}(\\sqrt{T})$."
      },
      {
        "id": "oai:arXiv.org:2402.11089v4",
        "title": "The Male CEO and the Female Assistant: Evaluation and Mitigation of Gender Biases in Text-To-Image Generation of Dual Subjects",
        "link": "https://arxiv.org/abs/2402.11089",
        "author": "Yixin Wan, Kai-Wei Chang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2402.11089v4 Announce Type: replace \nAbstract: Recent large-scale T2I models like DALLE-3 have made progress in reducing gender stereotypes when generating single-person images. However, significant biases remain when generating images with more than one person. To systematically evaluate this, we propose the Paired Stereotype Test (PST) framework, which queries T2I models to depict two individuals assigned with male-stereotyped and female-stereotyped social identities, respectively (e.g. \"a CEO\" and \"an Assistant\"). This contrastive setting often triggers T2I models to generate gender-stereotyped images. Using PST, we evaluate two aspects of gender biases -- the well-known bias in gendered occupation and a novel aspect: bias in organizational power. Experiments show that over 74\\% images generated by DALLE-3 display gender-occupational biases. Additionally, compared to single-person settings, DALLE-3 is more likely to perpetuate male-associated stereotypes under PST. We further propose FairCritic, a novel and interpretable framework that leverages an LLM-based critic model to i) detect bias in generated images, and ii) adaptively provide feedback to T2I models for improving fairness. FairCritic achieves near-perfect fairness on PST, overcoming the limitations of previous prompt-based intervention approaches."
      },
      {
        "id": "oai:arXiv.org:2402.15319v2",
        "title": "GPTVQ: The Blessing of Dimensionality for LLM Quantization",
        "link": "https://arxiv.org/abs/2402.15319",
        "author": "Mart van Baalen, Andrey Kuzmin, Ivan Koryakovskiy, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul Whatmough",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2402.15319v2 Announce Type: replace \nAbstract: In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we show that VQ leads to improved latency compared to using a 4-bit integer format."
      },
      {
        "id": "oai:arXiv.org:2403.01607v2",
        "title": "Real-time respiratory motion forecasting with online learning of recurrent neural networks for accurate targeting in externally guided radiotherapy",
        "link": "https://arxiv.org/abs/2403.01607",
        "author": "Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, Ritu Bhusal Chhatkuli",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.01607v2 Announce Type: replace \nAbstract: In lung radiotherapy, infrared cameras can track reflective objects on the chest to estimate tumor motion due to breathing, but treatment system latencies hinder radiation beam precision. Real-time recurrent learning (RTRL) is a potential solution that can learn patterns within non-stationary respiratory data but has high complexity. This study assesses the capabilities of resource-efficient online RNN algorithms, namely unbiased online recurrent optimization (UORO), sparse-1 step approximation (SnAp-1), and decoupled neural interfaces (DNI) to forecast respiratory motion during radiotherapy treatment accurately. We use time series containing the 3D positions of external markers on the chest of healthy subjects. We propose efficient implementations for SnAp-1 and DNI that compress the influence and immediate Jacobian matrices and accurately update the linear coefficients used in credit assignment estimation, respectively. Data was originally sampled at 10Hz; we resampled it at 3.33Hz and 30Hz to analyze the effect of the sampling rate on performance. We use UORO, SnAp-1, and DNI to forecast each marker's 3D position with horizons h<=2.1s (the time interval in advance for which the prediction is made) and compare them with RTRL, least mean squares, kernel support vector regression, and linear regression. RNNs trained online achieved similar or better accuracy than most previous works using larger training databases and deep learning, even though we used only the first minute of each sequence to predict motion within that exact sequence. SnAp-1 had the lowest normalized root mean square errors (nRMSEs) averaged over the horizon values considered, equal to 0.335 and 0.157, at 3.33Hz and 10.0Hz, respectively. Similarly, UORO had the lowest nRMSE at 30Hz, equal to 0.086. DNI's inference time (6.8ms per time step at 30Hz, Intel Core i7-13700 CPU) was the lowest among the RNN methods."
      },
      {
        "id": "oai:arXiv.org:2403.01759v3",
        "title": "Open-world Machine Learning: A Systematic Review and Future Directions",
        "link": "https://arxiv.org/abs/2403.01759",
        "author": "Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, Dacheng Tao, Cheng-Lin Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.01759v3 Announce Type: replace \nAbstract: Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then continually learning them, could enable models to be safe and evolve continually as biological systems do. This article presents a holistic view of open-world machine learning by investigating unknown rejection, novelty discovery, and continual learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Furthermore, widely used benchmarks, metrics, and performances are summarized. Finally, we discuss several potential directions for further progress in the field. By providing a comprehensive introduction to the emerging open-world machine learning paradigm, this article aims to help researchers build more powerful AI systems in their respective fields, and to promote the development of artificial general intelligence."
      },
      {
        "id": "oai:arXiv.org:2403.04247v3",
        "title": "UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities",
        "link": "https://arxiv.org/abs/2403.04247",
        "author": "Yangning Li, Qingsong Lv, Tianyu Yu, Yinghui Li, Xuming Hu, Wenhao Jiang, Hai-Tao Zheng, Hui Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.04247v3 Announce Type: replace \nAbstract: Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as the given set of seed entities. Traditional methods solely relied on positive seed entities to represent the target fine-grained semantic class, rendering them tough to represent ultra-fine-grained semantic classes. Specifically, merely relying on positive seed entities leads to two inherent shortcomings: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define ``unwanted'' semantics. Hence, previous ESE methods struggle to address the ultra-fine-grained ESE (Ultra-ESE) task. To solve this issue, we first introduce negative seed entities in the inputs, which jointly describe the ultra-fine-grained semantic class with positive seed entities. Negative seed entities eliminate the semantic ambiguity by providing a contrast between positive and negative attributes. Meanwhile, it provides a straightforward way to express ``unwanted''. To assess model performance in Ultra-ESE and facilitate further research, we also constructed UltraWiki, the first large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 50,973 entities and 394,097 sentences, alongside 236 ultra-fine-grained semantic classes, where each class is represented with 3-5 positive and negative seed entities. Moreover, a retrieval-based framework RetExpan and a generation-based framework GenExpan are proposed to provide powerful baselines for Ultra-ESE. Additionally, we devised two strategies to enhance models' comprehension of ultra-fine-grained entities' semantics: contrastive learning and chain-of-thought reasoning. Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE."
      },
      {
        "id": "oai:arXiv.org:2403.04523v2",
        "title": "T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers",
        "link": "https://arxiv.org/abs/2403.04523",
        "author": "Mariano V. Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.04523v2 Announce Type: replace \nAbstract: The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the \"black box\" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or outperform the outputs of computationally expensive perturbation-based explainability techniques, achieving SOTA performance. We apply T-TAME to three popular deep learning classifier architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet dataset, and we demonstrate improvements over existing state-of-the-art explainability methods. A detailed analysis of the results and an ablation study provide insights into how the T-TAME design choices affect the quality of the generated explanation maps."
      },
      {
        "id": "oai:arXiv.org:2403.09073v3",
        "title": "Revealing the Parallel Multilingual Learning within Large Language Models",
        "link": "https://arxiv.org/abs/2403.09073",
        "author": "Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.09073v3 Announce Type: replace \nAbstract: In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence."
      },
      {
        "id": "oai:arXiv.org:2403.19386v3",
        "title": "PointCloud-Text Matching: Benchmark Datasets and a Baseline",
        "link": "https://arxiv.org/abs/2403.19386",
        "author": "Yanglin Feng, Yang Qin, Dezhong Peng, Hongyuan Zhu, Xi Peng, Peng Hu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.19386v3 Announce Type: replace \nAbstract: In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching (PTM), which aims to identify the exact cross-modal instance that matches a given point-cloud query or text query. PTM has potential applications in various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there is a lack of suitable and targeted datasets for PTM in practice. To address this issue, we present a new PTM benchmark dataset, namely SceneDepict-3D2T. We observe that the data poses significant challenges due to its inherent characteristics, such as the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which render existing cross-modal matching methods ineffective for PTM. To overcome these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two key modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages token-level and feature-level attention mechanisms to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity. To handle noisy correspondence, RNCL enhances robustness against mismatching by dividing negative pairs into clean and noisy subsets and assigning them forward and reverse optimization directions, respectively. We conduct extensive experiments on our benchmarks and demonstrate the superiority of our RoMa."
      },
      {
        "id": "oai:arXiv.org:2403.19390v2",
        "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining",
        "link": "https://arxiv.org/abs/2403.19390",
        "author": "Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.19390v2 Announce Type: replace \nAbstract: The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining."
      },
      {
        "id": "oai:arXiv.org:2404.03998v3",
        "title": "PHISWID: Physics-Inspired Underwater Image Dataset Synthesized from RGB-D Images",
        "link": "https://arxiv.org/abs/2404.03998",
        "author": "Reina Kaneko, Takumi Ueda, Hiroshi Higashi, Yuichi Tanaka",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2404.03998v3 Announce Type: replace \nAbstract: This paper introduces the physics-inspired synthesized underwater image dataset (PHISWID), a dataset tailored for enhancing underwater image processing through physics-inspired image synthesis. For underwater image enhancement, data-driven approaches (e.g., deep neural networks) typically demand extensive datasets, yet acquiring paired clean atmospheric images and degraded underwater images poses significant challenges. Existing datasets have limited contributions to image enhancement due to lack of physics models, publicity, and ground-truth atmospheric images. PHISWID addresses these issues by offering a set of paired atmospheric and underwater images. Specifically, underwater images are synthetically degraded by color degradation and marine snow artifacts from atmospheric RGB-D images. It is enabled based on a physics-based underwater image observation model. Our synthetic approach generates a large quantity of the pairs, enabling effective training of deep neural networks and objective image quality assessment. Through benchmark experiments with some datasets and image enhancement methods, we validate that our dataset can improve the image enhancement performance. Our dataset, which is publicly available, contributes to the development in underwater image processing."
      },
      {
        "id": "oai:arXiv.org:2404.04924v2",
        "title": "GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets",
        "link": "https://arxiv.org/abs/2404.04924",
        "author": "Dongjing Shan, guiqiang chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2404.04924v2 Announce Type: replace \nAbstract: Vision Transformers (ViTs) have achieved impressive results in large-scale image classification. However, when training from scratch on small datasets, there is still a significant performance gap between ViTs and Convolutional Neural Networks (CNNs), which is attributed to the lack of inductive bias. To address this issue, we propose a Graph-based Vision Transformer (GvT) that utilizes graph convolutional projection and graph-pooling. In each block, queries and keys are calculated through graph convolutional projection based on the spatial adjacency matrix, while dot-product attention is used in another graph convolution to generate values. When using more attention heads, the queries and keys become lower-dimensional, making their dot product an uninformative matching function. To overcome this low-rank bottleneck in attention heads, we employ talking-heads technology based on bilinear pooled features and sparse selection of attention tensors. This allows interaction among filtered attention scores and enables each attention mechanism to depend on all queries and keys. Additionally, we apply graph-pooling between two intermediate blocks to reduce the number of tokens and aggregate semantic information more effectively. Our experimental results show that GvT produces comparable or superior outcomes to deep convolutional networks and surpasses vision transformers without pre-training on large datasets. The code for our proposed model is publicly available on the website."
      },
      {
        "id": "oai:arXiv.org:2404.05253v3",
        "title": "CodeEnhance: A Codebook-Driven Approach for Low-Light Image Enhancement",
        "link": "https://arxiv.org/abs/2404.05253",
        "author": "Xu Wu, XianXu Hou, Zhihui Lai, Jie Zhou, Ya-nan Zhang, Witold Pedrycz, Linlin Shen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2404.05253v3 Announce Type: replace \nAbstract: Low-light image enhancement (LLIE) aims to improve low-illumination images. However, existing methods face two challenges: (1) uncertainty in restoration from diverse brightness degradations; (2) loss of texture and color information caused by noise suppression and light enhancement. In this paper, we propose a novel enhancement approach, CodeEnhance, by leveraging quantized priors and image refinement to address these challenges. In particular, we reframe LLIE as learning an image-to-code mapping from low-light images to discrete codebook, which has been learned from high-quality images. To enhance this process, a Semantic Embedding Module (SEM) is introduced to integrate semantic information with low-level features, and a Codebook Shift (CS) mechanism, designed to adapt the pre-learned codebook to better suit the distinct characteristics of our low-light dataset. Additionally, we present an Interactive Feature Transformation (IFT) module to refine texture and color information during image reconstruction, allowing for interactive enhancement based on user preferences. Extensive experiments on both real-world and synthetic benchmarks demonstrate that the incorporation of prior knowledge and controllable information transfer significantly enhances LLIE performance in terms of quality and fidelity. The proposed CodeEnhance exhibits superior robustness to various degradations, including uneven illumination, noise, and color distortion."
      },
      {
        "id": "oai:arXiv.org:2404.17034v3",
        "title": "Learning Actionable Counterfactual Explanations in Large State Spaces",
        "link": "https://arxiv.org/abs/2404.17034",
        "author": "Keziah Naggita, Matthew R. Walter, Avrim Blum",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2404.17034v3 Announce Type: replace \nAbstract: Recourse generators provide actionable insights, often through feature-based counterfactual explanations (CFEs), to help negatively classified individuals understand how to adjust their input features to achieve a positive classification. These feature-based CFEs, which we refer to as \\emph{low-level} CFEs, are overly specific (e.g., coding experience: \\(4 \\to 5+\\) years) and often recommended in a feature space that doesn't straightforwardly align with real-world actions. To bridge this gap, we introduce three novel recourse types grounded in real-world actions: high-level continuous (\\emph{hl-continuous}), high-level discrete (\\emph{hl-discrete}), and high-level ID (\\emph{hl-id}) CFEs.\n  We formulate single-agent CFE generation methods, where we model the hl-discrete CFE as a solution to a weighted set cover problem and the hl-continuous CFE as a solution to an integer linear program. Since these methods require costly optimization per agent, we propose data-driven CFE generation approaches that, given instances of agents and their optimal CFEs, learn a CFE generator that quickly provides optimal CFEs for new agents. This approach, also viewed as one of learning an optimal policy in a family of large but deterministic MDPs, considers several problem formulations, including formulations in which the actions and their effects are unknown, and therefore addresses informational and computational challenges.\n  We conduct extensive empirical evaluations using healthcare datasets (BRFSS, Foods, and NHANES) and fully-synthetic data. For negatively classified agents identified by linear or threshold-based classifiers, we compare the high-level CFE to low-level CFEs and assess the effectiveness of our network-based, data-driven approaches. Results show that the data-driven CFE generators are accurate, and resource-efficient, and high-level CFEs offer key advantages over low-level CFEs."
      },
      {
        "id": "oai:arXiv.org:2405.02962v4",
        "title": "VectorPainter: Advanced Stylized Vector Graphics Synthesis Using Stroke-Style Priors",
        "link": "https://arxiv.org/abs/2405.02962",
        "author": "Juncheng Hu, Ximing Xing, Jing Zhang, Qian Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.02962v4 Announce Type: replace \nAbstract: We introduce VectorPainter, a novel framework designed for reference-guided text-to-vector-graphics synthesis. Based on our observation that the style of strokes can be an important aspect to distinguish different artists, our method reforms the task into synthesize a desired vector graphics by rearranging stylized strokes, which are vectorized from the reference images. Specifically, our method first converts the pixels of the reference image into a series of vector strokes, and then generates a vector graphic based on the input text description by optimizing the positions and colors of these vector strokes. To precisely capture the style of the reference image in the vectorized strokes, we propose an innovative vectorization method that employs an imitation learning strategy. To preserve the style of the strokes throughout the generation process, we introduce a style-preserving loss function. Extensive experiments have been conducted to demonstrate the superiority of our approach over existing works in stylized vector graphics synthesis, as well as the effectiveness of the various components of our method."
      },
      {
        "id": "oai:arXiv.org:2405.06705v3",
        "title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought",
        "link": "https://arxiv.org/abs/2405.06705",
        "author": "Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.06705v3 Announce Type: replace \nAbstract: Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge, we introduce a unique prompting strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design, two-stage interaction process (TIP) and grounded PedCoT prompts, all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero-shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory, serving as domain knowledge, in guiding prompting strategy design for addressing challenging tasks with LLMs effectively."
      },
      {
        "id": "oai:arXiv.org:2405.14250v5",
        "title": "Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors",
        "link": "https://arxiv.org/abs/2405.14250",
        "author": "Emile Pierret, Bruno Galerne",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.14250v5 Announce Type: replace \nAbstract: Diffusion or score-based models recently showed high performance in image generation. They rely on a forward and a backward stochastic differential equations (SDE). The sampling of a data distribution is achieved by numerically solving the backward SDE or its associated flow ODE. Studying the convergence of these models necessitates to control four different types of error: the initialization error, the truncation error, the discretization error and the score approximation. In this paper, we theoretically study the behavior of diffusion models and their numerical implementation when the data distribution is Gaussian. Our first contribution is to derive the analytical solutions of the backward SDE and the probability flow ODE and to prove that these solutions and their discretizations are all Gaussian processes. Our second contribution is to compute the exact Wasserstein errors between the target and the numerically sampled distributions for any numerical scheme. This allows us to monitor convergence directly in the data space, while experimental works limit their empirical analysis to Inception features. An implementation of our code is available online."
      },
      {
        "id": "oai:arXiv.org:2405.15228v2",
        "title": "Learning from True-False Labels via Multi-modal Prompt Retrieving",
        "link": "https://arxiv.org/abs/2405.15228",
        "author": "Zhongnian Li, Jinghao Xu, Peng Ying, Meng Wei, Xinzheng Xu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15228v2 Announce Type: replace \nAbstract: Pre-trained Vision-Language Models (VLMs) exhibit strong zero-shot classification abilities, demonstrating great potential for generating weakly supervised labels. Unfortunately, existing weakly supervised learning methods are short of ability in generating accurate labels via VLMs. In this paper, we propose a novel weakly supervised labeling setting, namely True-False Labels (TFLs) which can achieve high accuracy when generated by VLMs. The TFL indicates whether an instance belongs to the label, which is randomly and uniformly sampled from the candidate label set. Specifically, we theoretically derive a risk-consistent estimator to explore and utilize the conditional probability distribution information of TFLs. Besides, we propose a convolutional-based Multi-modal Prompt Retrieving (MRP) method to bridge the gap between the knowledge of VLMs and target learning tasks. Experimental results demonstrate the effectiveness of the proposed TFL setting and MRP learning method. The code to reproduce the experiments is at https://github.com/Tranquilxu/TMP."
      },
      {
        "id": "oai:arXiv.org:2405.15861v5",
        "title": "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization",
        "link": "https://arxiv.org/abs/2405.15861",
        "author": "Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15861v5 Announce Type: replace \nAbstract: Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL significantly challenge its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication-efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. This paper proposes a novel dimension-free communication algorithm - DeComFL, which leverages the zeroth-order optimization techniques and reduces the communication cost from $\\mathscr{O}(d)$ to $\\mathscr{O}(1)$ by transmitting only a constant number of scalar values between clients and the server in each round, regardless of the dimension $d$ of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions. With additional low effective rank assumption, we can further show the convergence rate is independent of the model dimension $d$ as well. Empirical evaluations, encompassing both classic deep learning training and large language model fine-tuning, demonstrate significant reductions in communication overhead. Notably, DeComFL achieves this by transmitting only around 1MB of data in total between the server and a client to fine-tune a model with billions of parameters. Our code is available at https://github.com/ZidongLiu/DeComFL."
      },
      {
        "id": "oai:arXiv.org:2405.15991v3",
        "title": "R\\'enyi Neural Processes",
        "link": "https://arxiv.org/abs/2405.15991",
        "author": "Xuesong Wang, He Zhao, Edwin V. Bonilla",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15991v3 Announce Type: replace \nAbstract: Neural Processes (NPs) are deep probabilistic models that represent stochastic processes by conditioning their prior distributions on a set of context points. Despite their advantages in uncertainty estimation for complex distributions, NPs enforce parameterization coupling between the conditional prior model and the posterior model. We show that this coupling amounts to prior misspecification and revisit the NP objective to address this issue. More specifically, we propose R\\'enyi Neural Processes (RNP), a method that replaces the standard KL divergence with the R\\'enyi divergence, dampening the effects of the misspecified prior during posterior updates. We validate our approach across multiple benchmarks including regression and image inpainting tasks, and show significant performance improvements of RNPs in real-world problems. Our extensive experiments show consistently better log-likelihoods over state-of-the-art NP models."
      },
      {
        "id": "oai:arXiv.org:2406.03747v2",
        "title": "OralBBNet: Spatially Guided Dental Segmentation of Panoramic X-Rays with Bounding Box Priors",
        "link": "https://arxiv.org/abs/2406.03747",
        "author": "Devichand Budagam, Azamat Zhanatuly Imanbayev, Iskander Rafailovich Akhmetov, Aleksandr Sinitca, Sergey Antonov, Dmitrii Kaplun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.03747v2 Announce Type: replace \nAbstract: Teeth segmentation and recognition play a vital role in a variety of dental applications and diagnostic procedures. The integration of deep learning models has facilitated the development of precise and automated segmentation methods. Although prior research has explored teeth segmentation, not many methods have successfully performed tooth segmentation and detection simultaneously. This study presents UFBA-425, a dental dataset derived from the UFBA-UESC dataset, featuring bounding box and polygon annotations for 425 panoramic dental X-rays. Additionally, this work introduces OralBBNet, an architecture featuring distinct segmentation and detection heads as U-Net and YOLOv8, respectively. OralBBNet is designed to improve the accuracy and robustness of tooth classification and segmentation on panoramic X-rays by leveraging the complementary strengths of U-Net and YOLOv8. Our approach achieved a 1-3% improvement in mean average precision (mAP) for teeth detection compared to existing techniques and a 15-20% improvement in the dice score for teeth segmentation over U-Net over various tooth categories and 2-4% improvement in the dice score when compared with other segmentation architectures. The results of this study establish a foundation for the wider implementation of object detection models in dental diagnostics."
      },
      {
        "id": "oai:arXiv.org:2406.06907v2",
        "title": "SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale",
        "link": "https://arxiv.org/abs/2406.06907",
        "author": "Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.06907v2 Announce Type: replace \nAbstract: A persistent challenge in sign language video processing, including the task of sign to written language translation, is how we learn representations of sign language in an effective and efficient way that preserves the important attributes of these languages, while remaining invariant to irrelevant visual differences. Informed by the nature and linguistics of signed languages, our proposed method focuses on just the most relevant parts in a signing video: the face, hands and body pose of the signer. However, instead of fully relying on pose estimation from off-the-shelf pose tracking models, which have inconsistent performance for hands and faces, we propose to learn a representation of the complex handshapes and facial expressions of sign languages in a self-supervised fashion. Our approach is based on learning from individual frames (rather than video sequences) and is therefore much more efficient than prior work on sign language pre-training. Compared to a recent model that established a new state of the art in sign language translation on the How2Sign dataset, our approach yields similar translation performance, using less than 3\\% of the compute."
      },
      {
        "id": "oai:arXiv.org:2406.07507v2",
        "title": "Flow map matching with stochastic interpolants: A mathematical framework for consistency models",
        "link": "https://arxiv.org/abs/2406.07507",
        "author": "Nicholas M. Boffi, Michael S. Albergo, Eric Vanden-Eijnden",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.07507v2 Announce Type: replace \nAbstract: Generative models based on dynamical equations such as flows and diffusions offer exceptional sample quality, but require computationally expensive numerical integration during inference. The advent of consistency models has enabled efficient one-step or few-step generation, yet despite their practical success, a systematic understanding of their design has been hindered by the lack of a comprehensive theoretical framework. Here we introduce Flow Map Matching (FMM), a principled framework for learning the two-time flow map of an underlying dynamical generative model, thereby providing this missing mathematical foundation. Leveraging stochastic interpolants, we propose training objectives both for distillation from a pre-trained velocity field and for direct training of a flow map over an interpolant or a forward diffusion process. Theoretically, we show that FMM unifies and extends a broad class of existing approaches for fast sampling, including consistency models, consistency trajectory models, and progressive distillation. Experiments on CIFAR-10 and ImageNet-32 highlight that our approach can achieve sample quality comparable to flow matching while reducing generation time by a factor of 10-20."
      },
      {
        "id": "oai:arXiv.org:2406.14239v2",
        "title": "LeYOLO, New Embedded Architecture for Object Detection",
        "link": "https://arxiv.org/abs/2406.14239",
        "author": "Lilian Hollard, Lucas Mohimont, Nathalie Gaveau, Luiz Angelo Steffenel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.14239v2 Announce Type: replace \nAbstract: Efficient computation in deep neural networks is crucial for real-time object detection. However, recent advancements primarily result from improved high-performing hardware rather than improving parameters and FLOP efficiency. This is especially evident in the latest YOLO architectures, where speed is prioritized over lightweight design. As a result, object detection models optimized for low-resource environments like microcontrollers have received less attention. For devices with limited computing power, existing solutions primarily rely on SSDLite or combinations of low-parameter classifiers, creating a noticeable gap between YOLO-like architectures and truly efficient lightweight detectors. This raises a key question: Can a model optimized for parameter and FLOP efficiency achieve accuracy levels comparable to mainstream YOLO models? To address this, we introduce two key contributions to object detection models using MSCOCO as a base validation set. First, we propose LeNeck, a general-purpose detection framework that maintains inference speed comparable to SSDLite while significantly improving accuracy and reducing parameter count. Second, we present LeYOLO, an efficient object detection model designed to enhance computational efficiency in YOLO-based architectures. LeYOLO effectively bridges the gap between SSDLite-based detectors and YOLO models, offering high accuracy in a model as compact as MobileNets. Both contributions are particularly well-suited for mobile, embedded, and ultra-low-power devices, including microcontrollers, where computational efficiency is critical."
      },
      {
        "id": "oai:arXiv.org:2406.14545v3",
        "title": "Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems",
        "link": "https://arxiv.org/abs/2406.14545",
        "author": "{\\DJ}or{\\dj}e Klisura, Anthony Rios",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.14545v3 Announce Type: replace \nAbstract: Text-to-SQL systems empower users to interact with databases using natural language, automatically translating queries into executable SQL code. However, their reliance on database schema information for SQL generation exposes them to significant security vulnerabilities, particularly schema inference attacks that can lead to unauthorized data access or manipulation. In this paper, we introduce a novel zero-knowledge framework for reconstructing the underlying database schema of text-to-SQL models without any prior knowledge of the database. Our approach systematically probes text-to-SQL models with specially crafted questions and leverages a surrogate GPT-4 model to interpret the outputs, effectively uncovering hidden schema elements -- including tables, columns, and data types. We demonstrate that our method achieves high accuracy in reconstructing table names, with F1 scores of up to .99 for generative models and .78 for fine-tuned models, underscoring the severity of schema leakage risks. We also show that our attack can steal prompt information in non-text-to-SQL models. Furthermore, we propose a simple protection mechanism for generative models and empirically show its limitations in mitigating these attacks."
      },
      {
        "id": "oai:arXiv.org:2407.00765v2",
        "title": "Structured and Balanced Multi-Component and Multi-Layer Neural Networks",
        "link": "https://arxiv.org/abs/2407.00765",
        "author": "Shijun Zhang, Hongkai Zhao, Yimin Zhong, Haomin Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.00765v2 Announce Type: replace \nAbstract: In this work, we propose a balanced multi-component and multi-layer neural network (MMNN) structure to accurately and efficiently approximate functions with complex features, in terms of both degrees of freedom and computational cost. The main idea is inspired by a multi-component approach, in which each component can be effectively approximated by a single-layer network, combined with a multi-layer decomposition strategy to capture the complexity of the target function. Although MMNNs can be viewed as a simple modification of fully connected neural networks (FCNNs) or multi-layer perceptrons (MLPs) by introducing balanced multi-component structures, they achieve a significant reduction in training parameters, a much more efficient training process, and improved accuracy compared to FCNNs or MLPs. Extensive numerical experiments demonstrate the effectiveness of MMNNs in approximating highly oscillatory functions and their ability to automatically adapt to localized features."
      },
      {
        "id": "oai:arXiv.org:2407.01384v3",
        "title": "Free-text Rationale Generation under Readability Level Control",
        "link": "https://arxiv.org/abs/2407.01384",
        "author": "Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.01384v3 Announce Type: replace \nAbstract: Free-text rationales justify model decisions in natural language and thus become likable and accessible among approaches to explanation across many tasks. However, their effectiveness can be hindered by misinterpretation and hallucination. As a perturbation test, we investigate how large language models (LLMs) perform rationale generation under the effects of readability level control, i.e., being prompted for an explanation targeting a specific expertise level, such as sixth grade or college. We find that explanations are adaptable to such instruction, though the observed distinction between readability levels does not fully match the defined complexity scores according to traditional readability metrics. Furthermore, the generated rationales tend to feature medium level complexity, which correlates with the measured quality using automatic metrics. Finally, our human annotators confirm a generally satisfactory impression on rationales at all readability levels, with high-school-level readability being most commonly perceived and favored."
      },
      {
        "id": "oai:arXiv.org:2407.02549v2",
        "title": "Diffusion Models for Tabular Data Imputation and Synthetic Data Generation",
        "link": "https://arxiv.org/abs/2407.02549",
        "author": "Mario Villaiz\\'an-Vallelado, Matteo Salvatori, Carlos Segura, Ioannis Arapakis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.02549v2 Announce Type: replace \nAbstract: Data imputation and data generation have important applications for many domains, like healthcare and finance, where incomplete or missing data can hinder accurate analysis and decision-making. Diffusion models have emerged as powerful generative models capable of capturing complex data distributions across various data modalities such as image, audio, and time series data. Recently, they have been also adapted to generate tabular data. In this paper, we propose a diffusion model for tabular data that introduces three key enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder transformer as the denoising network, and (3) dynamic masking. The conditioning attention mechanism is designed to improve the model's ability to capture the relationship between the condition and synthetic data. The transformer layers help model interactions within the condition (encoder) or synthetic data (decoder), while dynamic masking enables our model to efficiently handle both missing data imputation and synthetic data generation tasks within a unified framework. We conduct a comprehensive evaluation by comparing the performance of diffusion models with transformer conditioning against state-of-the-art techniques, such as Variational Autoencoders, Generative Adversarial Networks and Diffusion Models, on benchmark datasets. Our evaluation focuses on the assessment of the generated samples with respect to three important criteria, namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3) privacy risk mitigation. For the task of data imputation, we consider the efficiency of the generated samples across different levels of missing features."
      },
      {
        "id": "oai:arXiv.org:2407.02687v2",
        "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
        "link": "https://arxiv.org/abs/2407.02687",
        "author": "Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, Romann M. Weber",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.02687v2 Announce Type: replace \nAbstract: Classifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information."
      },
      {
        "id": "oai:arXiv.org:2407.03525v4",
        "title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization",
        "link": "https://arxiv.org/abs/2407.03525",
        "author": "Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.03525v4 Announce Type: replace \nAbstract: This paper introduces UnSeenTimeQA, a novel data contamination-free time-sensitive question-answering (TSQA) benchmark. It differs from existing TSQA benchmarks by avoiding web-searchable queries grounded in the real world. We present a series of time-sensitive event scenarios based on synthetically generated facts. It requires large language models (LLMs) to engage in genuine temporal reasoning without depending on the factual knowledge acquired during the pre-training phase. Our data generation framework enables on-demand generation of new samples, mitigating the risk of data leakage. We designed three types of time-sensitive questions to test LLMs' temporal reasoning abilities over sequential and parallel event occurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals mixed results: while they perform well on simpler subsets, their overall performance remains inferior as compared to real world fact-based TSQA. Error analysis indicates that LLMs face difficulties in reasoning over long-range event dependencies and parallel events."
      },
      {
        "id": "oai:arXiv.org:2407.10204v2",
        "title": "Improving Graph Out-of-distribution Generalization Beyond Causality",
        "link": "https://arxiv.org/abs/2407.10204",
        "author": "Can Xu, Yao Cheng, Jianxiang Yu, Haosen Wang, Jingsong Lv, Yao Liu, Xiang Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.10204v2 Announce Type: replace \nAbstract: Existing methods for graph out-of-distribution (OOD) generalization primarily rely on empirical studies on synthetic datasets. Such approaches tend to overemphasize the causal relationships between invariant sub-graphs and labels, thereby neglecting the non-negligible role of environment in real-world scenarios. In contrast to previous studies that impose rigid independence assumptions on environments and invariant sub-graphs, this paper presents the theorems of environment-label dependency and mutable rationale invariance, where the former characterizes the usefulness of environments in determining graph labels while the latter refers to the mutable importance of graph rationales. Based on analytic investigations, a novel variational inference based method named ``Probability Dependency on Environments and Rationales for OOD Graphs on Real-world Data'' (DEROG) is introduced. To alleviate the adverse effect of unknown prior knowledge on environments and rationales, DEROG utilizes generalized Bayesian inference. Further, DEROG employs an EM-based algorithm for optimization. Finally, extensive experiments on real-world datasets under different distribution shifts are conducted to show the superiority of DEROG."
      },
      {
        "id": "oai:arXiv.org:2407.10916v2",
        "title": "When Heterophily Meets Heterogeneity: Challenges and a New Large-Scale Graph Benchmark",
        "link": "https://arxiv.org/abs/2407.10916",
        "author": "Junhong Lin, Xiaojie Guo, Shuaicheng Zhang, Yada Zhu, Julian Shun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.10916v2 Announce Type: replace \nAbstract: Graph mining has become crucial in fields such as social science, finance, and cybersecurity. Many large-scale real-world networks exhibit both heterogeneity, where multiple node and edge types exist in the graph, and heterophily, where connected nodes may have dissimilar labels and attributes. However, existing benchmarks primarily focus on either heterophilic homogeneous graphs or homophilic heterogeneous graphs, leaving a significant gap in understanding how models perform on graphs with both heterogeneity and heterophily. To bridge this gap, we introduce H2GB, a large-scale node-classification graph benchmark that brings together the complexities of both the heterophily and heterogeneity properties of real-world graphs. H2GB encompasses 9 real-world datasets spanning 5 diverse domains, 28 baseline models, and a unified benchmarking library with a standardized data loader, evaluator, unified modeling framework, and an extensible framework for reproducibility. We establish a standardized workflow supporting both model selection and development, enabling researchers to easily benchmark graph learning methods. Extensive experiments across 28 baselines reveal that current methods struggle with heterophilic and heterogeneous graphs, underscoring the need for improved approaches. Finally, we present a new variant of the model, H2G-former, developed following our standardized workflow, that excels at this challenging benchmark. Both the benchmark and the framework are publicly available at Github and PyPI, with documentation hosted at https://junhongmit.github.io/H2GB."
      },
      {
        "id": "oai:arXiv.org:2407.11930v5",
        "title": "Localizing and Mitigating Errors in Long-form Question Answering",
        "link": "https://arxiv.org/abs/2407.11930",
        "author": "Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.11930v5 Announce Type: replace \nAbstract: Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the first hallucination dataset with localized error annotations for human-written and model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k span-level error annotations for five different error types by expert annotators, along with preference judgments. Using our collected data, we thoroughly analyze the shortcomings of long-form answers and find that they lack comprehensiveness and provide unhelpful references. We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-informed refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces errors and improves answer quality across multiple models. Furthermore, humans find answers generated by our approach comprehensive and highly prefer them (84%) over the baseline answers."
      },
      {
        "id": "oai:arXiv.org:2407.15186v5",
        "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks",
        "link": "https://arxiv.org/abs/2407.15186",
        "author": "Liang Shi, Zhengju Tang, Nan Zhang, Xiaotong Zhang, Zhi Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.15186v5 Announce Type: replace \nAbstract: With the development of the Large Language Models (LLMs), a large range of LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a comprehensive review of LLM-based Text2SQL studies. We first enumerate classic benchmarks and evaluation metrics. For the two mainstream methods, prompt engineering and finetuning, we introduce a comprehensive taxonomy and offer practical insights into each subcategory. We present an overall analysis of the above methods and various models evaluated on well-known datasets and extract some characteristics. Finally, we discuss the challenges and future directions in this field."
      },
      {
        "id": "oai:arXiv.org:2407.21050v3",
        "title": "Cross-Institutional Dental EHR Entity Extraction via Generative AI and Synthetic Notes",
        "link": "https://arxiv.org/abs/2407.21050",
        "author": "Yao-Shun Chuang, Chun-Teh Lee, Oluwabunmi Tokede, Guo-Hao Lin, Ryan Brandon, Trung Duong Tran, Xiaoqian Jiang, Muhammad F. Walji",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.21050v3 Announce Type: replace \nAbstract: This research addresses the issue of missing structured data in dental records by extracting diagnostic information from unstructured text. The updated periodontology classification system's complexity has increased incomplete or missing structured diagnoses. To tackle this, we use advanced AI and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a RoBERTa model. This significantly enhances the model's ability to understand medical and dental language. We evaluated the model using 120 randomly selected clinical notes from two datasets, demonstrating its improved diagnostic extraction accuracy. The results showed high accuracy in diagnosing periodontal status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In the subtype category, Site 2 achieved perfect scores, outperforming Site 1. This method enhances extraction accuracy and broadens its use across dental contexts. The study underscores AI and NLP's transformative impact on healthcare delivery and management. Integrating AI and NLP technologies enhances documentation and simplifies administrative tasks by precisely extracting complex clinical information. This approach effectively addresses challenges in dental diagnostics. Using synthetic training data from LLMs optimizes the training process, improving accuracy and efficiency in identifying periodontal diagnoses from clinical notes. This innovative method holds promise for broader healthcare applications, potentially improving patient care quality."
      },
      {
        "id": "oai:arXiv.org:2408.05885v2",
        "title": "GFlowNet Training by Policy Gradients",
        "link": "https://arxiv.org/abs/2408.05885",
        "author": "Puhua Niu, Shili Wu, Mingzhou Fan, Xiaoning Qian",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.05885v2 Announce Type: replace \nAbstract: Generative Flow Networks (GFlowNets) have been shown effective to generate combinatorial objects with desired properties. We here propose a new GFlowNet training framework, with policy-dependent rewards, that bridges keeping flow balance of GFlowNets to optimizing the expected accumulated reward in traditional Reinforcement-Learning (RL). This enables the derivation of new policy-based GFlowNet training methods, in contrast to existing ones resembling value-based RL. It is known that the design of backward policies in GFlowNet training affects efficiency. We further develop a coupled training strategy that jointly solves GFlowNet forward policy training and backward policy design. Performance analysis is provided with a theoretical guarantee of our policy-based GFlowNet training. Experiments on both simulated and real-world datasets verify that our policy-based strategies provide advanced RL perspectives for robust gradient estimation to improve GFlowNet performance."
      },
      {
        "id": "oai:arXiv.org:2408.08769v2",
        "title": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused",
        "link": "https://arxiv.org/abs/2408.08769",
        "author": "Dingwei Chen, Feiteng Fang, Shiwen Ni, Feng Liang, Xiping Hu, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.08769v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated exceptional performance across various natural language processing tasks. However, they occasionally generate inaccurate and counterfactual outputs, a phenomenon commonly referred to as \"hallucinations''. To tackle this issue, recent studies have explored contrastive decoding between the original model and an amateur model with induced hallucination, showing promising results. Nevertheless, this approach can disrupt the original LLM's output distribution due to coarse contrast and simple subtraction operations, potentially leading to errors. In this paper, we introduce a novel contrastive decoding framework, termed LOL (LOwer Layer Matters). Unlike prior methods that focus solely on the final layer, our approach integrates contrastive information from lower layers to enable multi-layer fusion during contrastive decoding. Additionally, we incorporate a truthfulness refocused module that leverages instruction guidance to further improve truthfulness in contrastive decoding. Extensive experiments on four publicly available datasets demonstrate that the LOL framework significantly mitigates hallucination while outperforming existing baselines in most cases. For reproducibility, we will release our code and data upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2408.08824v2",
        "title": "LEVIS: Large Exact Verifiable Input Spaces for Neural Networks",
        "link": "https://arxiv.org/abs/2408.08824",
        "author": "Mohamad Fares El Hajj Chehade, Wenting Li, Brian W. Bell, Russell Bent, Saif R. Kazi, Hao Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.08824v2 Announce Type: replace \nAbstract: The robustness of neural networks is crucial in safety-critical applications, where identifying a reliable input space is essential for effective model selection, robustness evaluation, and the development of reliable control strategies. Most existing robustness verification methods assess the worst-case output under the assumption that the input space is known. However, precisely identifying a verifiable input space \\(\\mathcal{C}\\), where no adversarial examples exist, is challenging due to the possible high dimensionality, discontinuity, and non-convex nature of the input space. To address this challenge, we propose a novel framework, **LEVIS**, consisting of **LEVIS-{\\alpha}** and **LEVIS-\\b{eta}**. **LEVIS-{\\alpha}** identifies a single, large verifiable ball that intersects at least two boundaries of a bounded region \\(\\mathcal{C}\\), while **LEVIS-\\b{eta}** systematically captures the entirety of the verifiable space by integrating multiple verifiable balls. Our contributions include: (1) introducing a verification framework that uses mixed-integer programming (MIP) to compute nearest and directional adversarial points, (2) integrating complementarity-constrained (CC) optimization with a reduced MIP formulation for scalability, achieving up to a 6 times runtime reduction, (3) theoretically characterizing the properties of the verifiable balls obtained by **LEVIS-{\\alpha}**, and (4) validating the approach across applications including electrical power flow regression and image classification, with demonstrated performance gains and geometric insights into the verifiable region."
      },
      {
        "id": "oai:arXiv.org:2408.12068v3",
        "title": "SDE: A Simplified and Disentangled Dependency Encoding Framework for State Space Models in Time Series Forecasting",
        "link": "https://arxiv.org/abs/2408.12068",
        "author": "Zixuan Weng, Jindong Han, Wenzhao Jiang, Hao Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.12068v3 Announce Type: replace \nAbstract: In recent years, advancements in deep learning have spurred the development of numerous models for Long-term Time Series Forecasting (LTSF). However, most existing approaches struggle to fully capture the complex and structured dependencies inherent in time series data. In this work, we identify and formally define three critical dependencies that are fundamental to forecasting accuracy: order dependency and semantic dependency along the temporal dimension, as well as cross-variate dependency across the feature dimension. These dependencies are often treated in isolation, and improper handling can introduce noise and degrade forecasting performance. To bridge this gap, we investigate the potential of State Space Models (SSMs) for LTSF and emphasize their inherent advantages in capturing these essential dependencies. Additionally, we empirically observe that excessive nonlinearity in conventional SSMs introduce redundancy when applied to semantically sparse time series data. Motivated by this insight, we propose SDE (Simplified and Disentangled Dependency Encoding), a novel framework designed to enhance the capability of SSMs for LTSF. Specifically, we first eliminate unnecessary nonlinearities in vanilla SSMs, thereby improving the suitability for time series forecasting. Building on this foundation, we introduce a disentangled encoding strategy, which empowers SSMs to efficiently model cross-variate dependencies while mitigating interference between the temporal and feature dimensions. Furthermore, we provide rigorous theoretical justifications to substantiate our design choices. Extensive experiments on nine real-world benchmark datasets demonstrate that SDE-enhanced SSMs consistently outperform state-of-the-art time series forecasting models.Our code is available at https://github.com/YukinoAsuna/SAMBA."
      },
      {
        "id": "oai:arXiv.org:2408.15127v3",
        "title": "T-FAKE: Synthesizing Thermal Images for Facial Landmarking",
        "link": "https://arxiv.org/abs/2408.15127",
        "author": "Philipp Flotho (Saarland University), Moritz Piening (Institute of Mathematics, Technische Universit\\\"at Berlin), Anna Kukleva (Max Planck Institute for Informatics, Saarland Informatics Campus), Gabriele Steidl (Institute of Mathematics, Technische Universit\\\"at Berlin)",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.15127v3 Announce Type: replace \nAbstract: Facial analysis is a key component in a wide range of applications such as healthcare, autonomous driving, and entertainment. Despite the availability of various facial RGB datasets, the thermal modality, which plays a crucial role in life sciences, medicine, and biometrics, has been largely overlooked. To address this gap, we introduce the T-FAKE dataset, a new large-scale synthetic thermal dataset with sparse and dense landmarks. To facilitate the creation of the dataset, we propose a novel RGB2Thermal loss function, which enables the domain-adaptive transfer of RGB faces to thermal style. By utilizing the Wasserstein distance between thermal and RGB patches and the statistical analysis of clinical temperature distributions on faces, we ensure that the generated thermal images closely resemble real samples. Using RGB2Thermal style transfer based on our RGB2Thermal loss function, we create the large-scale synthetic thermal T-FAKE dataset with landmark and segmentation annotations. Leveraging our novel T-FAKE dataset, probabilistic landmark prediction, and label adaptation networks, we demonstrate significant improvements in landmark detection methods on thermal images across different landmark conventions. Our models show excellent performance with both sparse 70-point landmarks and dense 478-point landmark annotations. Moreover, our RGB2Thermal loss leads to notable results in terms of perceptual evaluation and temperature prediction."
      },
      {
        "id": "oai:arXiv.org:2408.16245v4",
        "title": "Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions",
        "link": "https://arxiv.org/abs/2408.16245",
        "author": "Sully F. Chen, Robert J. Steele, Glen M. Hocky, Beakal Lemeneh, Shivanand P. Lad, Eric K. Oermann",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.16245v4 Announce Type: replace \nAbstract: The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. To date, most biosequence transformers have been trained on a single omic-either proteins or nucleic acids and have seen incredible success in downstream tasks in each domain with particularly noteworthy breakthroughs in protein structural modeling. However, single-omic pre-training limits the ability of these models to capture cross-modal interactions. Here we present OmniBioTE, the largest open-source multi-omic model trained on over 250 billion tokens of mixed protein and nucleic acid data. We show that despite only being trained on unlabelled sequence data, OmniBioTE learns joint representations consistent with the central dogma of molecular biology. We further demonstrate that OmbiBioTE achieves state-of-the-art results predicting the change in Gibbs free energy ({\\Delta}G) of the binding interaction between a given nucleic acid and protein. Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any a priori structural training, allowing us to predict which protein residues are most involved in the protein-nucleic acid binding interaction. Lastly, compared to single-omic controls trained with identical compute, OmniBioTE demonstrates superior performance-per-FLOP and absolute accuracy across both multi-omic and single-omic benchmarks, highlighting the power of a unified modeling approach for biological sequences."
      },
      {
        "id": "oai:arXiv.org:2409.00304v2",
        "title": "StimuVAR: Spatiotemporal Stimuli-aware Video Affective Reasoning with Multimodal Large Language Models",
        "link": "https://arxiv.org/abs/2409.00304",
        "author": "Yuxiang Guo, Faizan Siddiqui, Yang Zhao, Rama Chellappa, Shao-Yuan Lo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.00304v2 Announce Type: replace \nAbstract: Predicting and reasoning how a video would make a human feel is crucial for developing socially intelligent systems. Although Multimodal Large Language Models (MLLMs) have shown impressive video understanding capabilities, they tend to focus more on the semantic content of videos, often overlooking emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers' emotional reactions and providing plausible explanations. To address this issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level stimuli-aware mechanism: frame-level awareness and token-level awareness. Frame-level awareness involves sampling video frames with events that are most likely to evoke viewers' emotions. Token-level awareness performs tube selection in the token space to make the MLLM concentrate on emotion-triggered spatiotemporal regions. Furthermore, we create VAR instruction data to perform affective training, steering MLLMs' reasoning strengths towards emotional focus and thereby enhancing their affective reasoning ability. To thoroughly assess the effectiveness of VAR, we provide a comprehensive evaluation protocol with extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered VAR. Experiments demonstrate its superiority in understanding viewers' emotional responses to videos and providing coherent and insightful explanations. Our code is available at https://github.com/EthanG97/StimuVAR"
      },
      {
        "id": "oai:arXiv.org:2409.05819v2",
        "title": "GASP: Gaussian Splatting for Physic-Based Simulations",
        "link": "https://arxiv.org/abs/2409.05819",
        "author": "Piotr Borycki, Weronika Smolak, Joanna Waczy\\'nska, Marcin Mazur, S{\\l}awomir Tadeja, Przemys{\\l}aw Spurek",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.05819v2 Announce Type: replace \nAbstract: Physics simulation is paramount for modeling and utilization of 3D scenes in various real-world applications. However, its integration with state-of-the-art 3D scene rendering techniques such as Gaussian Splatting (GS) remains challenging. Existing models use additional meshing mechanisms, including triangle or tetrahedron meshing, marching cubes, or cage meshes. As an alternative, we can modify the physics grounded Newtonian dynamics to align with 3D Gaussian components. Current models take the first-order approximation of a deformation map, which locally approximates the dynamics by linear transformations. In contrast, our Gaussian Splatting for Physics-Based Simulations (GASP) model uses such a map (without any modifications) and flat Gaussian distributions, which are parameterized by three points (mesh faces). Subsequently, each 3D point (mesh face node) is treated as a discrete entity within a 3D space. Consequently, the problem of modeling Gaussian components is reduced to working with 3D points. Additionally, the information on mesh faces can be used to incorporate further properties into the physics model, facilitating the use of triangles. Resulting solution can be integrated into any physics engine that can be treated as a black box. As demonstrated in our studies, the proposed model exhibits superior performance on a diverse range of benchmark datasets designed for 3D object rendering."
      },
      {
        "id": "oai:arXiv.org:2409.10764v2",
        "title": "Federated Learning for Smart Grid: A Survey on Applications and Potential Vulnerabilities",
        "link": "https://arxiv.org/abs/2409.10764",
        "author": "Zikai Zhang, Suman Rath, Jiaohao Xu, Tingsong Xiao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.10764v2 Announce Type: replace \nAbstract: The Smart Grid (SG) is a critical energy infrastructure that collects real-time electricity usage data to forecast future energy demands using information and communication technologies (ICT). Due to growing concerns about data security and privacy in SGs, federated learning (FL) has emerged as a promising training framework. FL offers a balance between privacy, efficiency, and accuracy in SGs by enabling collaborative model training without sharing private data from IoT devices. In this survey, we thoroughly review recent advancements in designing FL-based SG systems across three stages: generation, transmission and distribution, and consumption. Additionally, we explore potential vulnerabilities that may arise when implementing FL in these stages. Furthermore, we discuss the gap between state-of-the-art (SOTA) FL research and its practical applications in SGs, and we propose future research directions. Unlike traditional surveys addressing security issues in centralized machine learning methods for SG systems, this survey is the first to specifically examine the applications and security concerns unique to FL-based SG systems. We also introduce FedGridShield, an open-source framework featuring implementations of SOTA attack and defense methods. Our aim is to inspire further research into applications and improvements in the robustness of FL-based SG systems."
      },
      {
        "id": "oai:arXiv.org:2409.13612v2",
        "title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs",
        "link": "https://arxiv.org/abs/2409.13612",
        "author": "Bowen Yan, Zhengsong Zhang, Liqiang Jing, Eftekhar Hossain, Xinya Du",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.13612v2 Announce Type: replace \nAbstract: The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations and are not comprehensive -- in terms of evaluating all aspects such as relations, attributes, and dependencies between aspects. Therefore, we introduce the FIHA (autonomous Fine-graIned Hallucination evAluation evaluation in LVLMs), which could access hallucination LVLMs in the LLM-free and annotation-free way and model the dependency between different types of hallucinations. FIHA can generate Q&amp;A pairs on any image dataset at minimal cost, enabling hallucination assessment from both image and caption. Based on this approach, we introduce a benchmark called FIHA-v1, which consists of diverse questions on various images from MSCOCO and Foggy. Furthermore, we use the Davidson Scene Graph (DSG) to organize the structure among Q&amp;A pairs, in which we can increase the reliability of the evaluation. We evaluate representative models using FIHA-v1, highlighting their limitations and challenges. We released our code and data."
      },
      {
        "id": "oai:arXiv.org:2409.15762v2",
        "title": "XTRUST: On the Multilingual Trustworthiness of Large Language Models",
        "link": "https://arxiv.org/abs/2409.15762",
        "author": "Yahan Li, Yi Wang, Yi Chang, Yuan Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.15762v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing (NLP) tasks, capturing the attention of both practitioners and the broader public. A key question that now preoccupies the AI community concerns the capabilities and limitations of these models, with trustworthiness emerging as a central issue, particularly as LLMs are increasingly applied in sensitive fields like healthcare and finance, where errors can have serious consequences. However, most previous studies on the trustworthiness of LLMs have been limited to a single language, typically the predominant one in the dataset, such as English. In response to the growing global deployment of LLMs, we introduce XTRUST, the first comprehensive multilingual trustworthiness benchmark. XTRUST encompasses a diverse range of topics, including illegal activities, hallucination, out-of-distribution (OOD) robustness, physical and mental health, toxicity, fairness, misinformation, privacy, and machine ethics, across 10 different languages. Using XTRUST, we conduct an empirical evaluation of the multilingual trustworthiness of five widely used LLMs, offering an in-depth analysis of their performance across languages and tasks. Our results indicate that many LLMs struggle with certain low-resource languages, such as Arabic and Russian, highlighting the considerable room for improvement in the multilingual trustworthiness of current language models. The code is available at https://github.com/LluckyYH/XTRUST."
      },
      {
        "id": "oai:arXiv.org:2409.17044v3",
        "title": "How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not",
        "link": "https://arxiv.org/abs/2409.17044",
        "author": "Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, S\\'ebastien Brati\\`eres, Paolo Merialdo, Simone Scardapane",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.17044v3 Announce Type: replace \nAbstract: The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM."
      },
      {
        "id": "oai:arXiv.org:2409.20175v2",
        "title": "Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse Problems",
        "link": "https://arxiv.org/abs/2409.20175",
        "author": "Hongkai Zheng, Wenda Chu, Austin Wang, Nikola Kovachki, Ricardo Baptista, Yisong Yue",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.20175v2 Announce Type: replace \nAbstract: When solving inverse problems, one increasingly popular approach is to use pre-trained diffusion models as plug-and-play priors. This framework can accommodate different forward models without re-training while preserving the generative capability of diffusion models. Despite their success in many imaging inverse problems, most existing methods rely on privileged information such as derivative, pseudo-inverse, or full knowledge about the forward model. This reliance poses a substantial limitation that restricts their use in a wide range of problems where such information is unavailable, such as in many scientific applications. We propose Ensemble Kalman Diffusion Guidance (EnKG), a derivative-free approach that can solve inverse problems by only accessing forward model evaluations and a pre-trained diffusion model prior. We study the empirical effectiveness of EnKG across various inverse problems, including scientific settings such as inferring fluid flows and astronomical objects, which are highly non-linear inverse problems that often only permit black-box access to the forward model. We open-source our code at https://github.com/devzhk/enkg-pytorch."
      },
      {
        "id": "oai:arXiv.org:2410.00382v2",
        "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
        "link": "https://arxiv.org/abs/2410.00382",
        "author": "Shota Takashiro, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.00382v2 Announce Type: replace \nAbstract: As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation of the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. ``LLMs pretend to forget''. Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field."
      },
      {
        "id": "oai:arXiv.org:2410.01574v3",
        "title": "Adversarial Robustness of AI-Generated Image Detectors in the Real World",
        "link": "https://arxiv.org/abs/2410.01574",
        "author": "Sina Mavali, Jonas Ricker, David Pape, Asja Fischer, Lea Sch\\\"onherr",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.01574v3 Announce Type: replace \nAbstract: The rapid advancement of Generative Artificial Intelligence (GenAI) capabilities is accompanied by a concerning rise in its misuse. In particular the generation of credible misinformation in the form of images poses a significant threat to the public trust in democratic processes. Consequently, there is an urgent need to develop tools to reliably distinguish between authentic and AI-generated content. The majority of detection methods are based on neural networks that are trained to recognize forensic artifacts. In this work, we demonstrate that current state-of-the-art classifiers are vulnerable to adversarial examples under real-world conditions. Through extensive experiments, comprising four detection methods and five attack algorithms, we show that an attacker can dramatically decrease classification performance, without internal knowledge of the detector's architecture. Notably, most attacks remain effective even when images are degraded during the upload to, e.g., social media platforms. In a case study, we demonstrate that these robustness challenges are also found in commercial tools by conducting black-box attacks on HIVE, a proprietary online GenAI media detector. In addition, we evaluate the robustness of using generated features of a robust pre-trained model and showed that this increases the robustness, while not reaching the performance on benign inputs. These results, along with the increasing potential of GenAI to erode public trust, underscore the need for more research and new perspectives on methods to prevent its misuse."
      },
      {
        "id": "oai:arXiv.org:2410.02416v2",
        "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
        "link": "https://arxiv.org/abs/2410.02416",
        "author": "Seyedmorteza Sadat, Otmar Hilliges, Romann M. Weber",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02416v2 Announce Type: replace \nAbstract: Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance."
      },
      {
        "id": "oai:arXiv.org:2410.02677v2",
        "title": "CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming",
        "link": "https://arxiv.org/abs/2410.02677",
        "author": "Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02677v2 Announce Type: replace \nAbstract: Robust, diverse, and challenging cultural knowledge benchmarks are essential for measuring our progress towards making LMs that are helpful across diverse cultures. We introduce CulturalBench: a set of 1,696 human-written and human-verified questions to assess LMs' cultural knowledge, covering 45 global regions including underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions are each verified by five independent annotators and span 17 diverse topics ranging from food preferences to greeting etiquette. We construct CulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human performance (92.4% accuracy), the hard version of CulturalBench is challenging even for the best-performing frontier LMs, ranging from 28.7% to 61.5% in accuracy. We find that LMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to overfit to a single answer. Our results indicate that GPT-4o substantially outperform other models across cultures, besting local providers (e.g., Mistral on European culture and DeepSeek on Chinese culture). Across the board, models under-perform on questions related to North Africa, South America and Middle East."
      },
      {
        "id": "oai:arXiv.org:2410.03869v2",
        "title": "Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step",
        "link": "https://arxiv.org/abs/2410.03869",
        "author": "Wenxuan Wang, Kuiyi Gao, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03869v2 Announce Type: replace \nAbstract: Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years. Despite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material. To assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process. Specifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries. To evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine safety scenarios, three types of editing operations, and three editing elements. Experiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60% cases, which significantly outperforms other jailbreaking methods (i.e., 14%). Further, to enhance these models' safety against our CoJ attack method, we also propose an effective prompting-based method, Think Twice Prompting, that can successfully defend over 95% of CoJ attack. We release our dataset and code to facilitate the AI safety research."
      },
      {
        "id": "oai:arXiv.org:2410.04417v4",
        "title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference",
        "link": "https://arxiv.org/abs/2410.04417",
        "author": "Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.04417v4 Announce Type: replace \nAbstract: In vision-language models (VLMs), visual tokens usually bear a significant amount of computational overhead despite sparsity of information in them when compared to text tokens. To address this, most existing methods learn a network to prune redundant visual tokens using certain training data. Differently, we propose a text-guided training-free token optimization mechanism dubbed SparseVLM that eliminates the need of extra parameters or fine-tuning costs. Given that visual tokens complement text tokens in VLM's linguistic reasoning, we select relevant text tokens to rate the significance of visual tokens using self-attention matrices and, then, prune visual tokens using the proposed strategy to maximize sparsity while retaining information. In particular, we introduce a rank-based strategy to adaptively determine the sparsification ratio for each layer, alongside a token recycling method that compresses pruned tokens into more compact representations. Experimental results show that SparseVLM increases the efficiency of various VLMs in a number of image and video understanding tasks. For example, LLaVA when equipped with SparseVLM achieves 54% reduction in FLOPs, 37% decrease in CUDA latency while maintaining 97% of its original accuracy. Our code is available at https://github.com/Gumpest/SparseVLMs."
      },
      {
        "id": "oai:arXiv.org:2410.06490v3",
        "title": "Adaptive Guidance for Local Training in Heterogeneous Federated Learning",
        "link": "https://arxiv.org/abs/2410.06490",
        "author": "Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao, Qiang Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.06490v3 Announce Type: replace \nAbstract: Model heterogeneity poses a significant challenge in Heterogeneous Federated Learning (HtFL). In scenarios with diverse model architectures, directly aggregating model parameters is impractical, leading HtFL methods to incorporate an extra objective alongside the original local objective on each client to facilitate collaboration. However, this often results in a mismatch between the extra and local objectives. To resolve this, we propose Federated Learning-to-Guide (FedL2G), a method that adaptively learns to guide local training in a federated manner, ensuring the added objective aligns with each client's original goal. With theoretical guarantees, FedL2G utilizes only first-order derivatives w.r.t. model parameters, achieving a non-convex convergence rate of O(1/T). We conduct extensive experiments across two data heterogeneity and six model heterogeneity settings, using 14 heterogeneous model architectures (e.g., CNNs and ViTs). The results show that FedL2G significantly outperforms seven state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2410.06895v3",
        "title": "Average Certified Radius is a Poor Metric for Randomized Smoothing",
        "link": "https://arxiv.org/abs/2410.06895",
        "author": "Chenhao Sun, Yuhao Mao, Mark Niklas M\\\"uller, Martin Vechev",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.06895v3 Announce Type: replace \nAbstract: Randomized smoothing (RS) is popular for providing certified robustness guarantees against adversarial attacks. The average certified radius (ACR) has emerged as a widely used metric for tracking progress in RS. However, in this work, for the first time we show that ACR is a poor metric for evaluating robustness guarantees provided by RS. We theoretically prove not only that a trivial classifier can have arbitrarily large ACR, but also that ACR is extremely sensitive to improvements on easy samples. In addition, the comparison using ACR has a strong dependence on the certification budget. Empirically, we confirm that existing training strategies, though improving ACR, reduce the model's robustness on hard samples consistently. To strengthen our findings, we propose strategies, including explicitly discarding hard samples, reweighing the dataset with approximate certified radius, and extreme optimization for easy samples, to replicate the progress in RS training and even achieve the state-of-the-art ACR on CIFAR-10, without training for robustness on the full data distribution. Overall, our results suggest that ACR has introduced a strong undesired bias to the field, and its application should be discontinued in RS. Finally, we suggest using the empirical distribution of $p_A$, the accuracy of the base model on noisy data, as an alternative metric for RS."
      },
      {
        "id": "oai:arXiv.org:2410.08201v2",
        "title": "Efficient Dictionary Learning with Switch Sparse Autoencoders",
        "link": "https://arxiv.org/abs/2410.08201",
        "author": "Anish Mudide, Joshua Engels, Eric J. Michaud, Max Tegmark, Christian Schroeder de Witt",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08201v2 Announce Type: replace \nAbstract: Sparse autoencoders (SAEs) are a recent technique for decomposing neural network activations into human-interpretable features. However, in order for SAEs to identify all features represented in frontier models, it will be necessary to scale them up to very high width, posing a computational challenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAE architecture aimed at reducing the compute cost of training SAEs. Inspired by sparse mixture of experts models, Switch SAEs route activation vectors between smaller \"expert\" SAEs, enabling SAEs to efficiently scale to many more features. We present experiments comparing Switch SAEs with other SAE architectures, and find that Switch SAEs deliver a substantial Pareto improvement in the reconstruction vs. sparsity frontier for a given fixed training compute budget. We also study the geometry of features across experts, analyze features duplicated across experts, and verify that Switch SAE features are as interpretable as features found by other SAE architectures."
      },
      {
        "id": "oai:arXiv.org:2410.09886v3",
        "title": "Point Cloud Mixture-of-Domain-Experts Model for 3D Self-supervised Learning",
        "link": "https://arxiv.org/abs/2410.09886",
        "author": "Yaohua Zha, Tao Dai, Hang Guo, Yanzi Wang, Bin Chen, Ke Chen, Shu-Tao Xia",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.09886v3 Announce Type: replace \nAbstract: Point clouds, as a primary representation of 3D data, can be categorized into scene domain point clouds and object domain point clouds. Point cloud self-supervised learning (SSL) has become a mainstream paradigm for learning 3D representations. However, existing point cloud SSL primarily focuses on learning domain-specific 3D representations within a single domain, neglecting the complementary nature of cross-domain knowledge, which limits the learning of 3D representations. In this paper, we propose to learn a comprehensive Point cloud Mixture-of-Domain-Experts model (Point-MoDE) via a block-to-scene pre-training strategy. Specifically, we first propose a mixture-of-domain-expert model consisting of scene domain experts and multiple shared object domain experts. Furthermore, we propose a block-to-scene pretraining strategy, which leverages the features of point blocks in the object domain to regress their initial positions in the scene domain through object-level block mask reconstruction and scene-level block position regression. By integrating the complementary knowledge between object and scene, this strategy simultaneously facilitates the learning of both object-domain and scene-domain representations, leading to a more comprehensive 3D representation. Extensive experiments in downstream tasks demonstrate the superiority of our model."
      },
      {
        "id": "oai:arXiv.org:2410.10672v3",
        "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
        "link": "https://arxiv.org/abs/2410.10672",
        "author": "Yahan Li, Tingyu Xia, Yi Chang, Yuan Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10672v3 Announce Type: replace \nAbstract: As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm."
      },
      {
        "id": "oai:arXiv.org:2410.10807v3",
        "title": "HardNet: Hard-Constrained Neural Networks with Universal Approximation Guarantees",
        "link": "https://arxiv.org/abs/2410.10807",
        "author": "Youngjae Min, Navid Azizan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10807v3 Announce Type: replace \nAbstract: Incorporating prior knowledge or specifications of input-output relationships into machine learning models has attracted significant attention, as it enhances generalization from limited data and leads to conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction, especially on inputs far from the training distribution -- an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Unlike approaches that modify outputs only at inference time, HardNet enables end-to-end training with hard constraint guarantees, leading to improved performance. To the best of our knowledge, HardNet is the first method with an efficient forward pass to enforce more than one input-dependent inequality constraint. It allows unconstrained optimization of the network parameters using standard algorithms by appending a differentiable closed-form enforcement layer to the network's output. Furthermore, we show that HardNet is expressive and retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: learning with piecewise constraints, learning optimization solvers with guaranteed feasibility, and optimizing control policies in safety-critical systems."
      },
      {
        "id": "oai:arXiv.org:2410.10995v4",
        "title": "Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation",
        "link": "https://arxiv.org/abs/2410.10995",
        "author": "Emmanouil Zaranis, Giuseppe Attanasio, Sweta Agrawal, Andr\\'e F. T. Martins",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10995v4 Announce Type: replace \nAbstract: Quality estimation (QE)-the automatic assessment of translation quality-has recently become crucial across several stages of the translation pipeline, from data curation to training and decoding. While QE metrics have been optimized to align with human judgments, whether they encode social biases has been largely overlooked. Biased QE risks favoring certain demographic groups over others, e.g., by exacerbating gaps in visibility and usability. This paper defines and investigates gender bias of QE metrics and discusses its downstream implications for machine translation (MT). Experiments with state-of-the-art QE metrics across multiple domains, datasets, and languages reveal significant bias. When a human entity's gender in the source is undisclosed, masculine-inflected translations score higher than feminine-inflected ones, and gender-neutral translations are penalized. Even when contextual cues disambiguate gender, using context-aware QE metrics leads to more errors in selecting the correct translation inflection for feminine referents than for masculine ones. Moreover, a biased QE metric affects data filtering and quality-aware decoding. Our findings underscore the need for a renewed focus on developing and evaluating QE metrics centered on gender."
      },
      {
        "id": "oai:arXiv.org:2410.11020v4",
        "title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning",
        "link": "https://arxiv.org/abs/2410.11020",
        "author": "Bokai Hu, Sai Ashish Somayajula, Xin Pan, Pengtao Xie",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11020v4 Announce Type: replace \nAbstract: Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4\\% on average across sentiment and natural language inference tasks, including gains of 7.3\\% on the Mental Health dataset and 10.9\\% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation."
      },
      {
        "id": "oai:arXiv.org:2410.11840v2",
        "title": "A Hitchhiker's Guide to Scaling Law Estimation",
        "link": "https://arxiv.org/abs/2410.11840",
        "author": "Leshem Choshen, Yang Zhang, Jacob Andreas",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11840v2 Announce Type: replace \nAbstract: Scaling laws predict the loss of a target machine learning model by extrapolating from easier-to-train models with fewer parameters or smaller training sets. This provides an efficient way for practitioners and researchers alike to compare pretraining decisions involving optimizers, datasets, and model architectures. Despite the widespread use of scaling laws to model the dynamics of language model training, there has been little work on understanding how to best estimate and interpret them. We collect (and release) a large-scale dataset containing losses and downstream evaluations for 485 previously published pretrained models. We use these to estimate more than 1000 scaling laws, then derive a set of best practices for estimating scaling laws in new model families. We find that fitting scaling laws to intermediate checkpoints of training runs (and not just their final losses) substantially improves accuracy, and that -- all else equal -- estimates of performance are generally most accurate when derived from other models of similar sizes. However, because there is a significant degree of variability across model seeds, training multiple small models is sometimes more useful than training a single large one. Moreover, while different model families differ scaling behavior, they are often similar enough that a target model's behavior can be predicted from a single model with the same architecture, along with scaling parameter estimates derived from other model families."
      },
      {
        "id": "oai:arXiv.org:2410.12974v3",
        "title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks",
        "link": "https://arxiv.org/abs/2410.12974",
        "author": "Anna Sokol, Elizabeth Daly, Michael Hind, David Piorkowski, Xiangliang Zhang, Nuno Moniz, Nitesh Chawla",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.12974v3 Announce Type: replace \nAbstract: Large language models (LLMs) are powerful tools capable of handling diverse tasks. Comparing and selecting appropriate LLMs for specific tasks requires systematic evaluation methods, as models exhibit varying capabilities across different domains. However, finding suitable benchmarks is difficult given the many available options. This complexity not only increases the risk of benchmark misuse and misinterpretation but also demands substantial effort from LLM users, seeking the most suitable benchmarks for their specific needs. To address these issues, we introduce \\texttt{BenchmarkCards}, an intuitive and validated documentation framework that standardizes critical benchmark attributes such as objectives, methodologies, data sources, and limitations. Through user studies involving benchmark creators and users, we show that \\texttt{BenchmarkCards} can simplify benchmark selection and enhance transparency, facilitating informed decision-making in evaluating LLMs. Data & Code: https://github.com/SokolAnn/BenchmarkCards"
      },
      {
        "id": "oai:arXiv.org:2410.13569v3",
        "title": "Learning on Model Weights using Tree Experts",
        "link": "https://arxiv.org/abs/2410.13569",
        "author": "Eliahu Horwitz, Bar Cavia, Jonathan Kahana, Yedid Hoshen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.13569v3 Announce Type: replace \nAbstract: The number of publicly available models is rapidly increasing, yet most remain undocumented. Users looking for suitable models for their tasks must first determine what each model does. Training machine learning models to infer missing documentation directly from model weights is challenging, as these weights often contain significant variation unrelated to model functionality (denoted nuisance). Here, we identify a key property of real-world models: most public models belong to a small set of Model Trees, where all models within a tree are fine-tuned from a common ancestor (e.g., a foundation model). Importantly, we find that within each tree there is less nuisance variation between models. Concretely, while learning across Model Trees requires complex architectures, even a linear classifier trained on a single model layer often works within trees. While effective, these linear classifiers are computationally expensive, especially when dealing with larger models that have many parameters. To address this, we introduce Probing Experts (ProbeX), a theoretically motivated and lightweight method. Notably, ProbeX is the first probing method specifically designed to learn from the weights of a single hidden model layer. We demonstrate the effectiveness of ProbeX by predicting the categories in a model's training dataset based only on its weights. Excitingly, ProbeX can map the weights of Stable Diffusion into a weight-language embedding space, enabling model search via text, i.e., zero-shot model classification."
      },
      {
        "id": "oai:arXiv.org:2410.13696v2",
        "title": "Online Learning for Function Placement in Serverless Computing",
        "link": "https://arxiv.org/abs/2410.13696",
        "author": "Wei Huang, Richard Combes, Andrea Araldo, Hind Castel-Taleb, Badii Jouaber",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.13696v2 Announce Type: replace \nAbstract: We study the placement of virtual functions aimed at minimizing the cost. We propose a novel algorithm, using ideas based on multi-armed bandits. We prove that these algorithms learn the optimal placement policy rapidly, and their regret grows at a rate at most $O( N M \\sqrt{T\\ln T} )$ while respecting the feasibility constraints with high probability, where $T$ is total time slots, $M$ is the number of classes of function and $N$ is the number of computation nodes. We show through numerical experiments that the proposed algorithm both has good practical performance and modest computational complexity. We propose an acceleration technique that allows the algorithm to achieve good performance also in large networks where computational power is limited. Our experiments are fully reproducible, and the code is publicly available."
      },
      {
        "id": "oai:arXiv.org:2410.13995v3",
        "title": "Adversarial Inception Backdoor Attacks against Reinforcement Learning",
        "link": "https://arxiv.org/abs/2410.13995",
        "author": "Ethan Rathbun, Alina Oprea, Christopher Amato",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.13995v3 Announce Type: replace \nAbstract: Recent works have demonstrated the vulnerability of Deep Reinforcement Learning (DRL) algorithms against training-time, backdoor poisoning attacks. The objectives of these attacks are twofold: induce pre-determined, adversarial behavior in the agent upon observing a fixed trigger during deployment while allowing the agent to solve its intended task during training. Prior attacks assume arbitrary control over the agent's rewards, inducing values far outside the environment's natural constraints. This results in brittle attacks that fail once the proper reward constraints are enforced. Thus, in this work we propose a new class of backdoor attacks against DRL which are the first to achieve state of the art performance under strict reward constraints. These \"inception\" attacks manipulate the agent's training data -- inserting the trigger into prior observations and replacing high return actions with those of the targeted adversarial behavior. We formally define these attacks and prove they achieve both adversarial objectives against arbitrary Markov Decision Processes (MDP). Using this framework we devise an online inception attack which achieves an 100\\% attack success rate on multiple environments under constrained rewards while minimally impacting the agent's task performance."
      },
      {
        "id": "oai:arXiv.org:2410.14086v4",
        "title": "In-context learning and Occam's razor",
        "link": "https://arxiv.org/abs/2410.14086",
        "author": "Eric Elmoznino, Tom Marty, Tejas Kasetty, Leo Gagnon, Sarthak Mittal, Mahan Fathi, Dhanya Sridhar, Guillaume Lajoie",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.14086v4 Announce Type: replace \nAbstract: A central goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode."
      },
      {
        "id": "oai:arXiv.org:2410.14817v5",
        "title": "A Complexity-Based Theory of Compositionality",
        "link": "https://arxiv.org/abs/2410.14817",
        "author": "Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.14817v5 Announce Type: replace \nAbstract: Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, compositional representations can enable a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, we lack satisfying formal definitions for it that are measurable and mathematical. Here, we propose such a definition, which we call representational compositionality, that accounts for and extends our intuitions about compositionality. The definition is conceptually simple, quantitative, grounded in algorithmic information theory, and applicable to any representation. Intuitively, representational compositionality states that a compositional representation satisfies three properties. First, it must be expressive. Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts, analogous to sentences in natural language. Third, the function that relates these symbolic sequences to the representation, analogous to semantics in natural language, must be simple. Through experiments on both synthetic and real world data, we validate our definition of compositionality and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. We hope that our definition can inspire the design of novel, theoretically-driven models that better capture the mechanisms of compositional thought. We make our code available at https://github.com/EricElmoznino/complexity_compositionality."
      },
      {
        "id": "oai:arXiv.org:2410.16135v3",
        "title": "Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs",
        "link": "https://arxiv.org/abs/2410.16135",
        "author": "Kang Zhao, Tao Yuan, Han Bao, Zhenfeng Su, Chang Gao, Zhaofeng Sun, Zichen Liang, Liping Jing, Jianfei Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.16135v3 Announce Type: replace \nAbstract: To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios."
      },
      {
        "id": "oai:arXiv.org:2410.16602v3",
        "title": "Foundation Models for Remote Sensing and Earth Observation: A Survey",
        "link": "https://arxiv.org/abs/2410.16602",
        "author": "Aoran Xiao, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, Naoto Yokoya",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.16602v3 Announce Type: replace \nAbstract: Remote Sensing (RS) is a crucial technology for observing, monitoring, and interpreting our planet, with broad applications across geoscience, economics, humanitarian fields, etc. While artificial intelligence (AI), particularly deep learning, has achieved significant advances in RS, unique challenges persist in developing more intelligent RS systems, including the complexity of Earth's environments, diverse sensor modalities, distinctive feature patterns, varying spatial and spectral resolutions, and temporal dynamics. Meanwhile, recent breakthroughs in large Foundation Models (FMs) have expanded AI's potential across many domains due to their exceptional generalizability and zero-shot transfer capabilities. However, their success has largely been confined to natural data like images and video, with degraded performance and even failures for RS data of various non-optical modalities. This has inspired growing interest in developing Remote Sensing Foundation Models (RSFMs) to address the complex demands of Earth Observation (EO) tasks, spanning the surface, atmosphere, and oceans. This survey systematically reviews the emerging field of RSFMs. It begins with an outline of their motivation and background, followed by an introduction of their foundational concepts. It then categorizes and reviews existing RSFM studies including their datasets and technical contributions across Visual Foundation Models (VFMs), Visual-Language Models (VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark these models against publicly available datasets, discuss existing challenges, and propose future research directions in this rapidly evolving field. A project associated with this survey has been built at https://github.com/xiaoaoran/awesome-RSFMs ."
      },
      {
        "id": "oai:arXiv.org:2410.21088v2",
        "title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models",
        "link": "https://arxiv.org/abs/2410.21088",
        "author": "Wenda Li, Huijie Zhang, Qing Qu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.21088v2 Announce Type: replace \nAbstract: The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes will be released at https://github.com/liwd190019/Shallow-Diffuse."
      },
      {
        "id": "oai:arXiv.org:2410.21271v4",
        "title": "EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation",
        "link": "https://arxiv.org/abs/2410.21271",
        "author": "Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.21271v4 Announce Type: replace \nAbstract: While post-training compression techniques effectively reduce the memory footprint, latency, and power consumption of Large Language Models (LLMs), they often result in noticeable accuracy degradation and remain limited by hardware and kernel constraints that restrict supported compression formats ultimately reducing flexibility across a wide range of deployment scenarios. In this work, we propose EoRA, a novel fine-tuning-free method that augments compressed LLMs with low-rank matrices, allowing users to rapidly enhance task-specific performance and freely balance the trade-off between accuracy and computational overhead beyond the constraints of compression formats. EoRA consistently outperforms prior training-free low rank methods in recovering the accuracy of compressed LLMs, achieving notable accuracy improvements (e.g., $\\mathbf{10.84\\%}$ on ARC-Challenge, $\\mathbf{6.74\\%}$ on MathQA, and $\\mathbf{6.74\\%}$ on GSM8K) for LLaMA3-8B compressed to 3-bit. We also introduce an optimized CUDA kernel, accelerating inference by up to 1.4x and reducing memory overhead through quantizing EoRA. Overall, EoRA offers a prompt solution for improving the accuracy of compressed models under varying user requirements, enabling more efficient and flexible deployment of LLMs. Code is available at https://github.com/NVlabs/EoRA."
      },
      {
        "id": "oai:arXiv.org:2411.00418v3",
        "title": "Self-Evolved Reward Learning for LLMs",
        "link": "https://arxiv.org/abs/2411.00418",
        "author": "Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00418v3 Announce Type: replace \nAbstract: Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model's responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs). Resources of this paper can be found at https://aka.ms/ser"
      },
      {
        "id": "oai:arXiv.org:2411.02430v2",
        "title": "Generative Emotion Cause Explanation in Multimodal Conversations",
        "link": "https://arxiv.org/abs/2411.02430",
        "author": "Lin Wang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Zhitao Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.02430v2 Announce Type: replace \nAbstract: Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically employs an utterance selection method within a single textual modality to locate causal utterances. This approach remains limited to coarse-grained assessments, lacks nuanced explanations of emotional causation, and demonstrates inadequate capability in identifying multimodal emotional triggers. Therefore, we introduce a task-\\textbf{Multimodal Emotion Cause Explanation in Conversation (MECEC)}. This task aims to generate a summary based on the multimodal context of conversations, clearly and intuitively describing the reasons that trigger a given emotion. To adapt to this task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM combines video clips with detailed explanations of character emotions, helping to explore the causal factors behind emotional expression in multimodal conversations. A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net outperforms several excellent baselines. Code and dataset are available at https://github.com/3222345200/FAME-Net."
      },
      {
        "id": "oai:arXiv.org:2411.02528v3",
        "title": "What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length",
        "link": "https://arxiv.org/abs/2411.02528",
        "author": "Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.02528v3 Announce Type: replace \nAbstract: When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram frequency effects. We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency. We first show that MORCELA outperforms a commonly used linking theory for acceptability - SLOR (Pauls and Klein, 2012; Lau et al. 2017) - across two families of transformer LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of adjustment in SLOR for length and unigram frequency overcorrect for these confounds, and that larger models require a lower relative degree of adjustment for unigram frequency, though a significant amount of adjustment is still necessary for all models. Finally, our subsequent analysis shows that larger LMs' lower susceptibility to frequency effects can be explained by an ability to better predict rarer words in context."
      },
      {
        "id": "oai:arXiv.org:2411.03730v2",
        "title": "NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA",
        "link": "https://arxiv.org/abs/2411.03730",
        "author": "Marlon Tobaben, Mohamed Ali Souibgui, Rub\\`en Tito, Khanh Nguyen, Raouf Kerkouche, Kangsoo Jung, Joonas J\\\"alk\\\"o, Lei Kang, Andrey Barsky, Vincent Poulain d'Andecy, Aur\\'elie Joseph, Aashiq Muhamed, Kevin Kuo, Virginia Smith, Yusuke Yamasaki, Takumi Fukami, Kenta Niwa, Iifan Tyou, Hiro Ishii, Rio Yokota, Ragul N, Rintu Kutum, Josep Llados, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.03730v2 Announce Type: replace \nAbstract: The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) competition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing. The competition introduced a dataset of real invoice documents, along with associated questions and answers requiring information extraction and reasoning over the document images. Thereby, it brings together researchers and expertise from the document analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering model provided by the organizers for this new domain, mimicking a typical federated invoice processing setup. The base model is a multi-modal generative language model, and sensitive information could be exposed through either the visual or textual input modality. Participants proposed elegant solutions to reduce communication costs while maintaining a minimum utility threshold in track 1 and to protect all information from each document provider using differential privacy in track 2. The competition served as a new testbed for developing and testing private federated learning methods, simultaneously raising awareness about privacy within the document image analysis and recognition community. Ultimately, the competition analysis provides best practices and recommendations for successfully running privacy-focused federated learning challenges in the future."
      },
      {
        "id": "oai:arXiv.org:2411.04975v2",
        "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications",
        "link": "https://arxiv.org/abs/2411.04975",
        "author": "Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.04975v2 Announce Type: replace \nAbstract: Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming state-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference."
      },
      {
        "id": "oai:arXiv.org:2411.05331v2",
        "title": "Discovering Latent Causal Graphs from Spatio-Temporal Data",
        "link": "https://arxiv.org/abs/2411.05331",
        "author": "Kun Wang, Sumanth Varambally, Duncan Watson-Parris, Yi-An Ma, Rose Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.05331v2 Announce Type: replace \nAbstract: Many important phenomena in scientific fields like climate, neuroscience, and epidemiology are naturally represented as spatiotemporal gridded data with complex interactions. Inferring causal relationships from these data is a challenging problem compounded by the high dimensionality of such data and the correlations between spatially proximate points. We present SPACY (SPAtiotemporal Causal discoverY), a novel framework based on variational inference, designed to model latent time series and their causal relationships from spatiotemporal data. SPACY alleviates the high-dimensional challenge by discovering causal structures in the latent space. To aggregate spatially proximate, correlated grid points, we use \\change{spatial factors, parametrized by spatial kernel functions}, to map observational time series to latent representations. \\change{Theoretically, we generalize the problem to a continuous spatial domain and establish identifiability when the observations arise from a nonlinear, invertible function of the product of latent series and spatial factors. Using this approach, we avoid assumptions that are often unverifiable, including those about instantaneous effects or sufficient variability.} Empirically, SPACY outperforms state-of-the-art baselines on synthetic data, even in challenging settings where existing methods struggle, while remaining scalable for large grids. SPACY also identifies key known phenomena from real-world climate data."
      },
      {
        "id": "oai:arXiv.org:2411.05698v2",
        "title": "Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification",
        "link": "https://arxiv.org/abs/2411.05698",
        "author": "Antonio De Santis, Riccardo Campi, Matteo Bianchi, Marco Brambilla",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.05698v2 Announce Type: replace \nAbstract: Convolutional Neural Networks (CNNs) have seen significant performance improvements in recent years. However, due to their size and complexity, they function as black-boxes, leading to transparency concerns. State-of-the-art saliency methods generate local explanations that highlight the area in the input image where a class is identified but cannot explain how a concept of interest contributes to the prediction, which is essential for bias mitigation. On the other hand, concept-based methods, such as TCAV (Testing with Concept Activation Vectors), provide insights into how sensitive is the network to a concept, but cannot compute its attribution in a specific prediction nor show its location within the input image. This paper introduces a novel post-hoc explainability framework, Visual-TCAV, which aims to bridge the gap between these methods by providing both local and global explanations for CNN-based image classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to generate saliency maps that show where concepts are recognized by the network. Moreover, it can estimate the attribution of these concepts to the output of any class using a generalization of Integrated Gradients. This framework is evaluated on popular CNN architectures, with its validity further confirmed via experiments where ground truth for explanations is known, and a comparison with TCAV. Our code is available at https://github.com/DataSciencePolimi/Visual-TCAV."
      },
      {
        "id": "oai:arXiv.org:2411.07965v5",
        "title": "SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs",
        "link": "https://arxiv.org/abs/2411.07965",
        "author": "Chuyi Kong, Ziyang Luo, Hongzhan Lin, Zhiyuan Fan, Yaxin Fan, Yuxi Sun, Jing Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.07965v5 Announce Type: replace \nAbstract: The advanced role-playing capabilities of Large Language Models (LLMs) have enabled rich interactive scenarios, yet existing research in social interactions neglects hallucination while struggling with poor generalizability and implicit character fidelity judgments. To bridge this gap, motivated by human behaviour, we introduce a generalizable and explicit paradigm for uncovering interactive patterns of LLMs across diverse worldviews. Specifically, we first define interactive hallucination through stance transfer, then construct SHARP, a benchmark built by extracting relations from commonsense knowledge graphs and utilizing LLMs' inherent hallucination properties to simulate multi-role interactions. Extensive experiments confirm our paradigm's effectiveness and stability, examine the factors that influence these metrics, and challenge conventional hallucination mitigation solutions. More broadly, our work reveals a fundamental limitation in popular post-training methods for role-playing LLMs: the tendency to obscure knowledge beneath style, resulting in monotonous yet human-like behaviors - interactive hallucination."
      },
      {
        "id": "oai:arXiv.org:2411.11479v3",
        "title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
        "link": "https://arxiv.org/abs/2411.11479",
        "author": "Jingxuan Li, Yuning Yang, Shengqi Yang, Linfan Zhang, Ying Nian Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.11479v3 Announce Type: replace \nAbstract: The recent progress in Vision-Language Models (VLMs) has broadened the scope of multimodal applications. However, evaluations often remain limited to functional tasks, neglecting abstract dimensions such as personality traits and human values. To address this gap, we introduce Value-Spectrum, a novel Visual Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's value dimensions that capture core human values guiding people's preferences and actions. We design a VLM agent pipeline to simulate video browsing and construct a vector database comprising over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels. These videos span multiple months and cover diverse topics, including family, health, hobbies, society, technology, etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs handle value-oriented content. Beyond identifying VLMs' intrinsic preferences, we also explore the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the adaptability of the model in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM preferences in value-based tasks and abilities to simulate diverse personas. The complete code and data are available at: https://github.com/Jeremyyny/Value-Spectrum."
      },
      {
        "id": "oai:arXiv.org:2411.12188v3",
        "title": "Constant Rate Scheduling: Constant-Rate Distributional Change for Efficient Training and Sampling in Diffusion Models",
        "link": "https://arxiv.org/abs/2411.12188",
        "author": "Shuntaro Okada, Kenji Doi, Ryota Yoshihashi, Hirokatsu Kataoka, Tomohiro Tanaka",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.12188v3 Announce Type: replace \nAbstract: We propose a general approach to optimize noise schedules for training and sampling in diffusion models. Our approach optimizes the noise schedules to ensure a constant rate of change in the probability distribution of diffused data throughout the diffusion process. Any distance metric for measuring the probability-distributional change is applicable to our approach, and we introduce three distance metrics. We evaluated the effectiveness of our approach on unconditional and class-conditional image-generation tasks using the LSUN (Horse, Bedroom, Church), ImageNet, FFHQ, and CIFAR10 datasets. Through extensive experiments, we confirmed that our approach broadly improves the performance of pixel-space and latent-space diffusion models regardless of the dataset, sampler, and number of function evaluations ranging from 5 to 250. Notably, by using our approach for optimizing both training and sampling schedules, we achieved a state-of-the-art FID score of 2.03 without sacrificing mode coverage on LSUN Horse 256 $\\times$ 256."
      },
      {
        "id": "oai:arXiv.org:2411.14725v2",
        "title": "Evaluating and Advancing Multimodal Large Language Models in Perception Ability Lens",
        "link": "https://arxiv.org/abs/2411.14725",
        "author": "Feng Chen, Chenhui Gou, Jing Liu, Yang Yang, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Bohan Zhuang, Qi Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.14725v2 Announce Type: replace \nAbstract: As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of \\textbf{vision perception} abilities, the foundational skill of MLLMs. We find that existing perception benchmarks, each focusing on different question types, domains, and evaluation metrics, introduce significant evaluation variance, complicating comprehensive assessments of perception abilities when relying on any single benchmark. To address this, we introduce \\textbf{AbilityLens}, a unified benchmark designed to evaluate MLLMs in six key perception abilities (ranging from counting, OCR, to understanding structural data), focusing on both accuracy and stability, with each ability encompassing diverse types of questions, domains, and metrics. With the assistance of AbilityLens, we: (1) identify the strengths and weaknesses of current main-stream MLLMs, highlighting stability patterns and revealing a notable performance gap between state-of-the-art open-source and closed-source models; (2) uncover interesting ability conflict and early convergence phenomena during MLLM training; (3) reveal the primary reason of ability conflict is data mixing ratio and LLM model size; and (4) discuss the effectiveness of some straightforward strategies \\eg, fine-tuning and model merging, to solve the ability conflict. The benchmark and online leaderboard is released in https://github.com/Chenfeng1271/AbilityLens."
      },
      {
        "id": "oai:arXiv.org:2411.15462v3",
        "title": "HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter",
        "link": "https://arxiv.org/abs/2411.15462",
        "author": "Manuel Tonneau, Diyi Liu, Niyati Malhotra, Scott A. Hale, Samuel P. Fraiberger, Victor Orozco-Olvera, Paul R\\\"ottger",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.15462v3 Announce Type: replace \nAbstract: To address the global challenge of online hate speech, prior research has developed detection models to flag such content on social media. However, due to systematic biases in evaluation datasets, the real-world effectiveness of these models remains unclear, particularly across geographies. We introduce HateDay, the first global hate speech dataset representative of social media settings, constructed from a random sample of all tweets posted on September 21, 2022 and covering eight languages and four English-speaking countries. Using HateDay, we uncover substantial variation in the prevalence and composition of hate speech across languages and regions. We show that evaluations on academic datasets greatly overestimate real-world detection performance, which we find is very low, especially for non-European languages. Our analysis identifies key drivers of this gap, including models' difficulty to distinguish hate from offensive speech and a mismatch between the target groups emphasized in academic datasets and those most frequently targeted in real-world settings. We argue that poor model performance makes public models ill-suited for automatic hate speech moderation and find that high moderation rates are only achievable with substantial human oversight. Our results underscore the need to evaluate detection systems on data that reflects the complexity and diversity of real-world social media."
      },
      {
        "id": "oai:arXiv.org:2411.16765v2",
        "title": "SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction",
        "link": "https://arxiv.org/abs/2411.16765",
        "author": "Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu, Alexander H. Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16765v2 Announce Type: replace \nAbstract: Sign language processing has traditionally relied on task-specific models, limiting the potential for transfer learning across tasks. Pre-training methods for sign language have typically focused on either supervised pre-training, which cannot take advantage of unlabeled data, or context-independent (frame or video segment) representations, which ignore the effects of relationships across time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a self-supervised contextual representation model learned from approximately 1,000 hours of American Sign Language video. SHuBERT adapts masked token prediction objectives to multi-stream visual sign language input, learning to predict multiple targets corresponding to clustered hand, face, and body pose streams. SHuBERT achieves state-of-the-art performance across multiple tasks including sign language translation, isolated sign language recognition, and fingerspelling detection."
      },
      {
        "id": "oai:arXiv.org:2411.19146v5",
        "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
        "link": "https://arxiv.org/abs/2411.19146",
        "author": "Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, Ran El-Yaniv",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.19146v5 Announce Type: replace \nAbstract: Large language models (LLMs) offer remarkable capabilities, yet their high inference costs restrict wider adoption. While increasing parameter counts improves accuracy, it also broadens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a hardware-aware framework that accelerates the inference of LLMs while preserving their capabilities. Using neural architecture search (NAS) at a large-scale, Puzzle optimizes models with tens of billions of parameters. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B) and Llama-3.3-Nemotron-49B, two publicly available models derived from Llama-70B-Instruct. Both models achieve a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while retaining 98.4% of the original model's benchmark accuracies. These are the most accurate models supporting single H100 GPU inference with large batch sizes, despite training on 45B tokens at most, far fewer than the 15T used to train Llama-70B. Lastly, we show that lightweight alignment on these derived models allows them to surpass the parent model in specific capabilities. Our work establishes that powerful LLM models can be optimized for efficient deployment with only negligible loss in quality, underscoring that inference performance, not parameter count alone, should guide model selection."
      },
      {
        "id": "oai:arXiv.org:2412.07820v2",
        "title": "Hyperband-based Bayesian Optimization for Black-box Prompt Selection",
        "link": "https://arxiv.org/abs/2412.07820",
        "author": "Lennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, Felice Antonio Merra",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07820v2 Announce Type: replace \nAbstract: Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks, especially in black-box settings where models are only accessible via APIs. Black-box prompt selection is challenging due to potentially large, combinatorial search spaces, absence of gradient information, and high evaluation cost of prompts on a validation set. We propose HbBoPs, a novel method that combines a structural-aware deep kernel Gaussian Process with Hyperband as a multi-fidelity scheduler to efficiently select prompts. HbBoPs uses embeddings of instructions and few-shot exemplars, treating them as modular components within prompts. This enhances the surrogate model's ability to predict which prompt to evaluate next in a sample-efficient manner. Hyperband improves query-efficiency by adaptively allocating resources across different fidelity levels, reducing the number of validation instances required for evaluating prompts. Extensive experiments across ten diverse benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods in both performance and efficiency."
      },
      {
        "id": "oai:arXiv.org:2412.12797v2",
        "title": "Is it the end of (generative) linguistics as we know it?",
        "link": "https://arxiv.org/abs/2412.12797",
        "author": "Cristiano Chesi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.12797v2 Announce Type: replace \nAbstract: A significant debate has emerged in response to a paper written by Steven Piantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open archive for generative linguistics. Piantadosi's dismissal of Chomsky's approach is ruthless, but generative linguists deserve it. In this paper, I will adopt three idealized perspectives -- computational, theoretical, and experimental -- to focus on two fundamental issues that lend partial support to Piantadosi's critique: (a) the evidence challenging the Poverty of Stimulus (PoS) hypothesis and (b) the notion of simplicity as conceived within mainstream Minimalism. In conclusion, I argue that, to reclaim a central role in language studies, generative linguistics -- representing a prototypical theoretical perspective on language -- needs a serious update leading to (i) more precise, consistent, and complete formalizations of foundational intuitions and (ii) the establishment and utilization of a standardized dataset of crucial empirical evidence to evaluate the theory's adequacy. On the other hand, ignoring the formal perspective leads to major drawbacks in both computational and experimental approaches. Neither descriptive nor explanatory adequacy can be easily achieved without the precise formulation of general principles that can be challenged empirically."
      },
      {
        "id": "oai:arXiv.org:2412.13541v4",
        "title": "Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for Fine-grained Emotion Recognition",
        "link": "https://arxiv.org/abs/2412.13541",
        "author": "Jingyao Wang, Wenwen Qiang, Changwen Zheng, Fuchun Sun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.13541v4 Announce Type: replace \nAbstract: Fine-grained emotion recognition (FER) plays a vital role in various fields, such as disease diagnosis, personalized recommendations, and multimedia mining. However, existing FER methods face three key challenges in real-world applications: (i) they rely on large amounts of continuously annotated data to ensure accuracy since emotions are complex and ambiguous in reality, which is costly and time-consuming; (ii) they cannot capture the temporal heterogeneity caused by changing emotion patterns, because they usually assume that the temporal correlation within sampling periods is the same; (iii) they do not consider the spatial heterogeneity of different FER scenarios, that is, the distribution of emotion information in different data may have bias or interference. To address these challenges, we propose a Spatio-Temporal Fuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically, ST-F2M first divides the multi-modal videos into multiple views, and each view corresponds to one modality of one emotion. Multiple randomly selected views for the same emotion form a meta-training task. Next, ST-F2M uses an integrated module with spatial and temporal convolutions to encode the data of each task, reflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic information to each task based on generalized fuzzy rules, which helps handle the complexity and ambiguity of emotions. Finally, ST-F2M learns emotion-related general meta-knowledge through meta-recurrent neural networks to achieve fast and robust fine-grained emotion recognition. Extensive experiments show that ST-F2M outperforms various state-of-the-art methods in terms of accuracy and model efficiency. In addition, we construct ablation studies and further analysis to explore why ST-F2M performs well."
      },
      {
        "id": "oai:arXiv.org:2412.13649v3",
        "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
        "link": "https://arxiv.org/abs/2412.13649",
        "author": "Jialong Wu, Zhenglin Wang, Linhai Zhang, Yilong Lai, Yulan He, Deyu Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.13649v3 Announce Type: replace \nAbstract: Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods."
      },
      {
        "id": "oai:arXiv.org:2412.15628v4",
        "title": "Can Input Attributions Explain Inductive Reasoning in In-Context Learning?",
        "link": "https://arxiv.org/abs/2412.15628",
        "author": "Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15628v4 Announce Type: replace \nAbstract: Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying/solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests typically adopted in psycholinguistics. Here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates it. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods."
      },
      {
        "id": "oai:arXiv.org:2412.16311v2",
        "title": "HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases",
        "link": "https://arxiv.org/abs/2412.16311",
        "author": "Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala, Christos Faloutsos",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.16311v2 Announce Type: replace \nAbstract: Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions? Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source. However, many questions require both textual and relational information from SKB - referred to as \"hybrid\" questions - which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information. In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HybGRAG for HQA consisting of a retriever bank and a critic module, with the following advantages: (1) Agentic, it automatically refines the output by incorporating feedback from the critic module, (2) Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank, (3) Interpretable, it justifies decision making with intuitive refinement path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In experiments on the STaRK benchmark, HybGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51%."
      },
      {
        "id": "oai:arXiv.org:2412.17063v2",
        "title": "Computational Analysis of Character Development in Holocaust Testimonies",
        "link": "https://arxiv.org/abs/2412.17063",
        "author": "Esther Shizgal, Eitan Wagner, Renana Keydar, Omri Abend",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17063v2 Announce Type: replace \nAbstract: This work presents a computational approach to analyze character development along the narrative timeline. The analysis characterizes the inner and outer changes the protagonist undergoes within a narrative, and the interplay between them. We consider transcripts of Holocaust survivor testimonies as a test case, each telling the story of an individual in first-person terms. We focus on the survivor's religious trajectory, examining the evolution of their disposition toward religious belief and practice along the testimony. Clustering the resulting trajectories in the dataset, we identify common sequences in the data. Our findings highlight multiple common structures of religiosity across the narratives: in terms of belief, most present a constant disposition, while for practice, most present an oscillating structure, serving as valuable material for historical and sociological research. This work demonstrates the potential of natural language processing techniques for analyzing character evolution through thematic trajectories in narratives."
      },
      {
        "id": "oai:arXiv.org:2412.17451v2",
        "title": "Diving into Self-Evolving Training for Multimodal Reasoning",
        "link": "https://arxiv.org/abs/2412.17451",
        "author": "Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, Junxian He",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17451v2 Announce Type: replace \nAbstract: Self-evolving trainin--where models iteratively learn from their own outputs--has emerged as a key approach for complex reasoning tasks, addressing the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a domain more intricate than text-only reasoning, remains underexplored, and the understanding of critical factors in this training paradigm remains limited. Furthermore, a central challenge for this training method is performance saturation, which impedes further improvements and scalability. Inspired by reinforcement learning (RL), in this paper, we reframe self-evolving training for multimodal reasoning through the lens of RL, identifying three pivotal factors: Training Method, Reward Model, and Prompt Variation. Through systematic analysis, we establish relatively optimal design principles that significantly enhance multimodal reasoning capabilities. Moreover, delving deeper into training dynamics, we uncover the roots of saturation and propose a new automatic balancing mechanism to mitigate this limitation. Building on these insights, we propose M-STAR (Multimodal Self-evolving Training for Reasoning), a framework that achieves consistent performance gains across models of varying sizes and diverse benchmarks. All resources are made publicly available at https://mstar-lmm.github.io."
      },
      {
        "id": "oai:arXiv.org:2412.17533v3",
        "title": "Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse",
        "link": "https://arxiv.org/abs/2412.17533",
        "author": "Anna Ko{\\l}os, Katarzyna Lorenc, Emilia Wi\\'snios, Agnieszka Karli\\'nska",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17533v3 Announce Type: replace \nAbstract: The surge in online content has created an urgent demand for robust detection systems, especially in non-English contexts where current tools demonstrate significant limitations. We present forePLay, a novel Polish language dataset for erotic content detection, featuring over 24k annotated sentences with a multidimensional taxonomy encompassing ambiguity, violence, and social unacceptability dimensions. Our comprehensive evaluation demonstrates that specialized Polish language models achieve superior performance compared to multilingual alternatives, with transformer-based architectures showing particular strength in handling imbalanced categories. The dataset and accompanying analysis establish essential frameworks for developing linguistically-aware content moderation systems, while highlighting critical considerations for extending such capabilities to morphologically complex languages."
      },
      {
        "id": "oai:arXiv.org:2412.17626v3",
        "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
        "link": "https://arxiv.org/abs/2412.17626",
        "author": "Yang Xu, Yi Wang, Hengguan Huang, Hao Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17626v3 Announce Type: replace \nAbstract: Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution. For reproducibility, our code is available at https://github.com/Superposition09m/SAE-Track."
      },
      {
        "id": "oai:arXiv.org:2412.18730v4",
        "title": "Elucidating Flow Matching ODE Dynamics with Respect to Data Geometries and Denoisers",
        "link": "https://arxiv.org/abs/2412.18730",
        "author": "Zhengchao Wan, Qingsong Wang, Gal Mishne, Yusu Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.18730v4 Announce Type: replace \nAbstract: Flow matching (FM) models extend ODE sampler based diffusion models into a general framework, significantly reducing sampling steps through learned vector fields. However, the theoretical understanding of FM models, particularly how their sample trajectories interact with underlying data geometry, remains underexplored. A rigorous theoretical analysis of FM ODE is essential for sample quality, stability, and broader applicability. In this paper, we advance the theory of FM models through a comprehensive analysis of sample trajectories. Central to our theory is the discovery that the denoiser, a key component of FM models, guides ODE dynamics through attracting and absorbing behaviors that adapt to the data geometry. We identify and analyze the three stages of ODE evolution: in the initial and intermediate stages, trajectories move toward the mean and local clusters of the data. At the terminal stage, we rigorously establish the convergence of FM ODE under weak assumptions, addressing scenarios where the data lie on a low-dimensional submanifold-cases that previous results could not handle. Our terminal stage analysis offers insights into the memorization phenomenon and establishes equivariance properties of FM ODEs. These findings bridge critical gaps in understanding flow matching models, with practical implications for optimizing sampling strategies and architectures guided by the intrinsic geometry of data."
      },
      {
        "id": "oai:arXiv.org:2412.18926v2",
        "title": "Exemplar-condensed Federated Class-incremental Learning",
        "link": "https://arxiv.org/abs/2412.18926",
        "author": "Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.18926v2 Announce Type: replace \nAbstract: We propose Exemplar-Condensed federated class-incremental learning (ECoral) to distil the training characteristics of real images from streaming data into informative rehearsal exemplars. The proposed method eliminates the limitations of exemplar selection in replay-based approaches for mitigating catastrophic forgetting in federated continual learning (FCL). The limitations particularly related to the heterogeneity of information density of each summarized data. Our approach maintains the consistency of training gradients and the relationship to past tasks for the summarized exemplars to represent the streaming data compared to the original images effectively. Additionally, our approach reduces the information-level heterogeneity of the summarized data by inter-client sharing of the disentanglement generative model. Extensive experiments show that our ECoral outperforms several state-of-the-art methods and can be seamlessly integrated with many existing approaches to enhance performance."
      },
      {
        "id": "oai:arXiv.org:2412.20833v2",
        "title": "Inclusion 2024 Global Multimedia Deepfake Detection Challenge: Towards Multi-dimensional Face Forgery Detection",
        "link": "https://arxiv.org/abs/2412.20833",
        "author": "Yi Zhang, Weize Gao, Changtao Miao, Man Luo, Jianshu Li, Wenzhong Deng, Zhe Li, Bingyu Hu, Weibin Yao, Yunfeng Diao, Wenbo Zhou, Tao Gong, Qi Chu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.20833v2 Announce Type: replace \nAbstract: In this paper, we present the Global Multimedia Deepfake Detection held concurrently with the Inclusion 2024. Our Multimedia Deepfake Detection aims to detect automatic image and audio-video manipulations including but not limited to editing, synthesis, generation, Photoshop,etc. Our challenge has attracted 1500 teams from all over the world, with about 5000 valid result submission counts. We invite the top 20 teams to present their solutions to the challenge, from which the top 3 teams are awarded prizes in the grand finale. In this paper, we present the solutions from the top 3 teams of the two tracks, to boost the research work in the field of image and audio-video forgery detection. The methodologies developed through the challenge will contribute to the development of next-generation deepfake detection systems and we encourage participants to open source their methods."
      },
      {
        "id": "oai:arXiv.org:2501.01073v2",
        "title": "Graph Generative Pre-trained Transformer",
        "link": "https://arxiv.org/abs/2501.01073",
        "author": "Xiaohui Chen, Yinkai Wang, Jiaxing He, Yuanqi Du, Soha Hassoun, Xiaolin Xu, Li-Ping Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.01073v2 Announce Type: replace \nAbstract: Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction. Code available at https://github.com/tufts-ml/G2PT,"
      },
      {
        "id": "oai:arXiv.org:2501.02086v3",
        "title": "Instruction-Following Pruning for Large Language Models",
        "link": "https://arxiv.org/abs/2501.02086",
        "author": "Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.02086v3 Announce Type: replace \nAbstract: With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model."
      },
      {
        "id": "oai:arXiv.org:2501.02295v4",
        "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
        "link": "https://arxiv.org/abs/2501.02295",
        "author": "Yachao Zhao, Bo Wang, Yan Wang, Dongming Zhao, Ruifang He, Yuexian Hou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.02295v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes. We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias."
      },
      {
        "id": "oai:arXiv.org:2501.03835v4",
        "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification",
        "link": "https://arxiv.org/abs/2501.03835",
        "author": "Yindu Su, Huike Zou, Lin Sun, Ting Zhang, Haiyang Yang, Liyu Chen, David Lo, Qingheng Zhang, Shuguang Han, Jufeng Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.03835v4 Announce Type: replace \nAbstract: Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendation, and business analytics on e-commerce platforms. However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds. TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial deployment. Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Further, it has been successfully deployed on the real-world e-commerce platform Xianyu, processing millions of product listings daily with frequently updated, large-scale attribute taxonomies. We release the code to facilitate reproducibility and future research at https://github.com/SuYindu/TACLR."
      },
      {
        "id": "oai:arXiv.org:2501.05844v3",
        "title": "\"Cause\" is Mechanistic Narrative within Scientific Domains: An Ordinary Language Philosophical Critique of \"Causal Machine Learning\"",
        "link": "https://arxiv.org/abs/2501.05844",
        "author": "Vyacheslav Kungurtsev, Leonardo Christov Moore, Gustav Sir, Martin Krutsky",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.05844v3 Announce Type: replace \nAbstract: Causal Learning has emerged as a major theme of research in statistics and machine learning in recent years, promising specific computational techniques to apply to datasets that reveal the true nature of cause and effect in a number of important domains. In this paper we consider the epistemology of recognizing true cause and effect phenomena. We apply the Ordinary Language method of engaging on the customary use of the word 'cause' to investigate valid semantics of reasoning about cause and effect. We recognize that the grammars of cause and effect are fundamentally distinct in form across scientific domains, yet they maintain a consistent and central function. This function can best be described as the mechanism underlying fundamental forces of influence as considered prominent in the respective scientific domain. We demarcate 1) physics and engineering as domains wherein mathematical models are sufficient to comprehensively describe causality, 2) biology as introducing challenges of emergence while providing opportunities for showing consistent mechanisms across scale, and 3) the social sciences as introducing grander difficulties for establishing models of low prediction error but providing, through Hermeneutics, the potential for findings that are still instrumentally useful to individuals. We posit that definitive causal claims regarding a given phenomenon (writ large) can only come through an agglomeration of consistent evidence across multiple domains. This presents important methodological questions as far as harmonizing between language games and emergence across scales. Given the role of epistemic hubris in the contemporary crisis of credibility in the sciences, exercising greater caution as far as communicating precision as to the real degree of certainty certain evidence provides for rich collections of open problems in optimizing integration of different findings."
      },
      {
        "id": "oai:arXiv.org:2501.06645v2",
        "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings",
        "link": "https://arxiv.org/abs/2501.06645",
        "author": "Tong Liu, Xiao Yu, Wenxuan Zhou, Jindong Gu, Volker Tresp",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06645v2 Announce Type: replace \nAbstract: Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work~\\citep{chen2024preference} empirically finds that DPO training \\textit{rarely improves these misranked preference pairs}, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead \\textit{down-weighs} misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness."
      },
      {
        "id": "oai:arXiv.org:2501.12633v2",
        "title": "Inverse Reinforcement Learning with Switching Rewards and History Dependency for Characterizing Animal Behaviors",
        "link": "https://arxiv.org/abs/2501.12633",
        "author": "Jingyang Ke, Feiyang Wu, Jiyi Wang, Jeffrey Markowitz, Anqi Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.12633v2 Announce Type: replace \nAbstract: Traditional approaches to studying decision-making in neuroscience focus on simplified behavioral tasks where animals perform repetitive, stereotyped actions to receive explicit rewards. While informative, these methods constrain our understanding of decision-making to short timescale behaviors driven by explicit goals. In natural environments, animals exhibit more complex, long-term behaviors driven by intrinsic motivations that are often unobservable. Recent works in time-varying inverse reinforcement learning (IRL) aim to capture shifting motivations in long-term, freely moving behaviors. However, a crucial challenge remains: animals make decisions based on their history, not just their current state. To address this, we introduce SWIRL (SWitching IRL), a novel framework that extends traditional IRL by incorporating time-varying, history-dependent reward functions. SWIRL models long behavioral sequences as transitions between short-term decision-making processes, each governed by a unique reward function. SWIRL incorporates biologically plausible history dependency to capture how past decisions and environmental contexts shape behavior, offering a more accurate description of animal decision-making. We apply SWIRL to simulated and real-world animal behavior datasets and show that it outperforms models lacking history dependency, both quantitatively and qualitatively. This work presents the first IRL model to incorporate history-dependent policies and rewards to advance our understanding of complex, naturalistic decision-making in animals."
      },
      {
        "id": "oai:arXiv.org:2501.13106v4",
        "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
        "link": "https://arxiv.org/abs/2501.13106",
        "author": "Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.13106v4 Announce Type: replace \nAbstract: In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the vision-centric training paradigm and vision-centric framework design. The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding. Instead of preparing massive video-text datasets, we focus on constructing large-scale and high-quality image-text datasets. VideoLLaMA3 has four training stages: 1) Vision Encoder Adaptation, which enables vision encoder to accept images of variable resolutions as input; 2) Vision-Language Alignment, which jointly tunes the vision encoder, projector, and LLM with large-scale image-text data covering multiple types (including scene images, documents, charts) as well as text-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT data for downstream tasks and video-text data to establish a foundation for video understanding. 4) Video-centric Fine-tuning, which further improves the model's capability in video understanding. As for the framework design, to better capture fine-grained details in images, the pretrained vision encoder is adapted to encode images of varying sizes into vision tokens with corresponding numbers, rather than a fixed number of tokens. For video inputs, we reduce the number of vision tokens according to their similarity so that the representation of videos will be more precise and compact. Benefit from vision-centric designs, VideoLLaMA3 achieves compelling performances in both image and video understanding benchmarks."
      },
      {
        "id": "oai:arXiv.org:2501.13779v2",
        "title": "Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling",
        "link": "https://arxiv.org/abs/2501.13779",
        "author": "Tanya Rodchenko, Natasha Noy, Nino Scherrer",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.13779v2 Announce Type: replace \nAbstract: While Large Language Models require more and more data to train and scale, rather than looking for any data to acquire, we should consider what types of tasks are more likely to benefit from data scaling. We should be intentional in our data acquisition. We argue that the shape of the data itself, such as its compositional and structural patterns, informs which tasks to prioritize in data scaling, and shapes the development of the next generation of compute paradigms for tasks where data scaling is inefficient, or even insufficient."
      },
      {
        "id": "oai:arXiv.org:2501.15781v2",
        "title": "Large Language Models to Diffusion Finetuning",
        "link": "https://arxiv.org/abs/2501.15781",
        "author": "Edoardo Cetin, Tianyu Zhao, Yujin Tang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.15781v2 Announce Type: replace \nAbstract: We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks."
      },
      {
        "id": "oai:arXiv.org:2501.16168v3",
        "title": "Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity",
        "link": "https://arxiv.org/abs/2501.16168",
        "author": "Artavazd Maranjyan, Alexander Tyurin, Peter Richt\\'arik",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.16168v3 Announce Type: replace \nAbstract: Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin & Richt\\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios."
      },
      {
        "id": "oai:arXiv.org:2501.16365v2",
        "title": "CAND: Cross-Domain Ambiguity Inference for Early Detecting Nuanced Illness Deterioration",
        "link": "https://arxiv.org/abs/2501.16365",
        "author": "Lo Pang-Yun Ting, Zhen Tan, Hong-Pei Chen, Cheng-Te Li, Po-Lin Chen, Kun-Ta Chuang, Huan Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.16365v2 Announce Type: replace \nAbstract: Early detection of patient deterioration is essential for timely treatment, with vital signs like heart rates being key health indicators. Existing methods tend to solely analyze vital sign waveforms, ignoring transition relationships of waveforms within each vital sign and the correlation strengths among various vital signs. Such studies often overlook nuanced illness deterioration, which is the early sign of worsening health but is difficult to detect. In this paper, we introduce CAND, a novel method that organizes the transition relationships and the correlations within and among vital signs as domain-specific and cross-domain knowledge. CAND jointly models these knowledge in a unified representation space, considerably enhancing the early detection of nuanced illness deterioration. In addition, CAND integrates a Bayesian inference method that utilizes augmented knowledge from domain-specific and cross-domain knowledge to address the ambiguities in correlation strengths. With this architecture, the correlation strengths can be effectively inferred to guide joint modeling and enhance representations of vital signs. This allows a more holistic and accurate interpretation of patient health. Our experiments on a real-world ICU dataset demonstrate that CAND significantly outperforms existing methods in both effectiveness and earliness in detecting nuanced illness deterioration. Moreover, we conduct a case study for the interpretable detection process to showcase the practicality of CAND."
      },
      {
        "id": "oai:arXiv.org:2501.17813v2",
        "title": "P-TAME: Explain Any Image Classifier with Trained Perturbations",
        "link": "https://arxiv.org/abs/2501.17813",
        "author": "Mariano V. Ntrougkas, Vasileios Mezaris, Ioannis Patras",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.17813v2 Announce Type: replace \nAbstract: The adoption of Deep Neural Networks (DNNs) in critical fields where predictions need to be accompanied by justifications is hindered by their inherent black-box nature. In this paper, we introduce P-TAME (Perturbation-based Trainable Attention Mechanism for Explanations), a model-agnostic method for explaining DNN-based image classifiers. P-TAME employs an auxiliary image classifier to extract features from the input image, bypassing the need to tailor the explanation method to the internal architecture of the backbone classifier being explained. Unlike traditional perturbation-based methods, which have high computational requirements, P-TAME offers an efficient alternative by generating high-resolution explanations in a single forward pass during inference. We apply P-TAME to explain the decisions of VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image classifiers. Quantitative and qualitative results show that our method matches or outperforms previous explainability methods, including model-specific approaches. Code and trained models will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2501.18913v2",
        "title": "Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior",
        "link": "https://arxiv.org/abs/2501.18913",
        "author": "Tongda Xu, Xiyan Cai, Xinjie Zhang, Xingtong Ge, Dailan He, Ming Sun, Jingjing Liu, Ya-Qin Zhang, Jian Li, Yan Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.18913v2 Announce Type: replace \nAbstract: Recent advancements in diffusion models have been leveraged to address inverse problems without additional training, and Diffusion Posterior Sampling (DPS) (Chung et al., 2022a) is among the most popular approaches. Previous analyses suggest that DPS accomplishes posterior sampling by approximating the conditional score. While in this paper, we demonstrate that the conditional score approximation employed by DPS is not as effective as previously assumed, but rather aligns more closely with the principle of maximizing a posterior (MAP). This assertion is substantiated through an examination of DPS on 512x512 ImageNet images, revealing that: 1) DPS's conditional score estimation significantly diverges from the score of a well-trained conditional diffusion model and is even inferior to the unconditional score; 2) The mean of DPS's conditional score estimation deviates significantly from zero, rendering it an invalid score estimation; 3) DPS generates high-quality samples with significantly lower diversity. In light of the above findings, we posit that DPS more closely resembles MAP than a conditional score estimator, and accordingly propose the following enhancements to DPS: 1) we explicitly maximize the posterior through multi-step gradient ascent and projection; 2) we utilize a light-weighted conditional score estimator trained with only 100 images and 8 GPU hours. Extensive experimental results indicate that these proposed improvements significantly enhance DPS's performance. The source code for these improvements is provided in https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior."
      },
      {
        "id": "oai:arXiv.org:2501.18936v5",
        "title": "On the Expressiveness of Visual Prompt Experts",
        "link": "https://arxiv.org/abs/2501.18936",
        "author": "Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Anh Tran, Nhat Ho",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.18936v5 Announce Type: replace \nAbstract: Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens. Despite its empirical success, a comprehensive theoretical understanding of VPT remains an active area of research. Building on the recently established connection between Mixture of Experts (MoE) and prompt-based methods, wherein each attention head can be conceptualized as a composition of multiple MoE models, we reinterpret VPT as the introduction of new prompt experts into these MoE structures. We identify a key limitation in existing VPT frameworks: the restricted functional expressiveness of prompt experts, which remain static and thus limited in their adaptability. To address this, we propose Visual Adaptive Prompt Tuning (VAPT), a novel method that endows prompt experts with enhanced expressiveness while preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC demonstrate that VAPT achieves substantial performance improvements, surpassing fully fine-tuned baselines by 7.34% and 1.04%, respectively. Moreover, VAPT consistently outperforms VPT while requiring fewer additional parameters. Furthermore, our theoretical analysis indicates that VAPT achieves optimal sample efficiency. Collectively, these results underscore the theoretical grounding and empirical advantages of our approach."
      },
      {
        "id": "oai:arXiv.org:2501.19040v3",
        "title": "Towards the Worst-case Robustness of Large Language Models",
        "link": "https://arxiv.org/abs/2501.19040",
        "author": "Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.19040v3 Announce Type: replace \nAbstract: Recent studies have revealed the vulnerability of large language models to adversarial attacks, where adversaries craft specific input sequences to induce harmful, violent, private, or incorrect outputs. In this work, we study their worst-case robustness, i.e., whether an adversarial example exists that leads to such undesirable outputs. We upper bound the worst-case robustness using stronger white-box attacks, indicating that most current deterministic defenses achieve nearly 0\\% worst-case robustness. We propose a general tight lower bound for randomized smoothing using fractional knapsack solvers or 0-1 knapsack solvers, and using them to bound the worst-case robustness of all stochastic defenses. Based on these solvers, we provide theoretical lower bounds for several previous empirical defenses. For example, we certify the robustness of a specific case, smoothing using a uniform kernel, against \\textit{any possible attack} with an average $\\ell_0$ perturbation of 2.02 or an average suffix length of 6.41."
      },
      {
        "id": "oai:arXiv.org:2501.19114v2",
        "title": "Principal Components for Neural Network Initialization",
        "link": "https://arxiv.org/abs/2501.19114",
        "author": "Nhan Phan, Thu Nguyen, P{\\aa}l Halvorsen, Michael A. Riegler",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.19114v2 Announce Type: replace \nAbstract: Principal Component Analysis (PCA) is a commonly used tool for dimension reduction and denoising. Therefore, it is also widely used on the data prior to training a neural network. However, this approach can complicate the explanation of explainable AI (XAI) methods for the decision of the model. In this work, we analyze the potential issues with this approach and propose Principal Components-based Initialization (PCsInit), a strategy to incorporate PCA into the first layer of a neural network via initialization of the first layer in the network with the principal components, and its two variants PCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct and straightforward as for neural networks and are simpler than using PCA prior to training a neural network on the principal components. Moreover, as will be illustrated in the experiments, such training strategies can also allow further improvement of training via backpropagation."
      },
      {
        "id": "oai:arXiv.org:2502.00182v3",
        "title": "Understanding Federated Learning from IID to Non-IID dataset: An Experimental Study",
        "link": "https://arxiv.org/abs/2502.00182",
        "author": "Jungwon Seo, Ferhat Ozgur Catak, Chunming Rong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00182v3 Announce Type: replace \nAbstract: As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field."
      },
      {
        "id": "oai:arXiv.org:2502.00334v4",
        "title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models",
        "link": "https://arxiv.org/abs/2502.00334",
        "author": "Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, Yang Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00334v4 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. Codes and data are available at https://github.com/YangLabHKUST/UGPhysics ."
      },
      {
        "id": "oai:arXiv.org:2502.01042v4",
        "title": "SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals",
        "link": "https://arxiv.org/abs/2502.01042",
        "author": "Peixuan Han, Cheng Qian, Xiusi Chen, Yuji Zhang, Denghui Zhang, Heng Ji",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01042v4 Announce Type: replace \nAbstract: Large language models (LLMs) exhibit exceptional capabilities across various tasks but also pose risks by generating harmful content. Existing safety mechanisms, while improving model safety, often lead to overly cautious behavior and fail to fully leverage LLMs' internal cognitive processes. Inspired by humans' reflective thinking capability, we first show that LLMs can similarly perform internal assessments about safety in their internal states. Building on this insight, we propose SafeSwitch, a dynamic framework that regulates unsafe outputs by utilizing the prober-based internal state monitor that actively detects harmful intentions, and activates a safety head that leads to safer and more conservative responses only when necessary. SafeSwitch reduces harmful outputs by approximately 80% on harmful queries while maintaining strong utility, reaching a Pareto optimal among several methods. Our method is also advantageous over traditional methods in offering more informative, context-aware refusals, and achieves these benefits while only tuning less than 6% of the original parameters. SafeSwitch demonstrates large language models' capacity for self-awareness and reflection regarding safety, offering a promising approach to more nuanced and effective safety controls. Codes for this work are available at https://github.com/Hanpx20/SafeSwitch."
      },
      {
        "id": "oai:arXiv.org:2502.01085v2",
        "title": "Federated Linear Dueling Bandits",
        "link": "https://arxiv.org/abs/2502.01085",
        "author": "Xuhan Huang, Yan Hu, Zhiyan Li, Zhiyong Wang, Benyou Wang, Zhongxiang Dai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01085v2 Announce Type: replace \nAbstract: Contextual linear dueling bandits have recently garnered significant attention due to their widespread applications in important domains such as recommender systems and large language models. Classical dueling bandit algorithms are typically only applicable to a single agent. However, many applications of dueling bandits involve multiple agents who wish to collaborate for improved performance yet are unwilling to share their data. This motivates us to draw inspirations from federated learning, which involves multiple agents aiming to collaboratively train their neural networks via gradient descent (GD) without sharing their raw data. Previous works have developed federated linear bandit algorithms which rely on closed-form updates of the bandit parameters (e.g., the linear function parameters) to achieve collaboration. However, in linear dueling bandits, the linear function parameters lack a closed-form expression and their estimation requires minimizing a loss function. This renders these previous methods inapplicable. In this work, we overcome this challenge through an innovative and principled combination of online gradient descent (OGD, for minimizing the loss function to estimate the linear function parameters) and federated learning, hence introducing our federated linear dueling bandit with OGD (FLDB-OGD) algorithm. Through rigorous theoretical analysis, we prove that FLDB-OGD enjoys a sub-linear upper bound on its cumulative regret and demonstrate a theoretical trade-off between regret and communication complexity. We conduct empirical experiments to demonstrate the effectiveness of FLDB-OGD and reveal valuable insights, such as the benefit of a larger number of agents, the regret-communication trade-off, among others."
      },
      {
        "id": "oai:arXiv.org:2502.01586v2",
        "title": "SubTrack++ : Gradient Subspace Tracking for Scalable LLM Training",
        "link": "https://arxiv.org/abs/2502.01586",
        "author": "Sahar Rajabi, Nayeema Nonta, Sirisha Rambhatla",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01586v2 Announce Type: replace \nAbstract: Training large language models (LLMs) is highly resource-intensive due to their massive number of parameters and the overhead of optimizer states. While recent work has aimed to reduce memory consumption, such efforts often entail trade-offs among memory efficiency, training time, and model performance. Yet, true democratization of LLMs requires simultaneous progress across all three dimensions. To this end, we propose SubTrack++ that leverages Grassmannian gradient subspace tracking combined with projection-aware optimizers, enabling Adam's internal statistics to adapt to changes in the optimization subspace. Additionally, employing recovery scaling, a technique that restores information lost through low-rank projections, further enhances model performance. Our method demonstrates SOTA convergence by exploiting Grassmannian geometry and achieves lowest evaluation loss, outperforming the current SOTA while reducing pretraining wall time by 43% and maintaining the memory footprint on a 1B-parameter Llama model."
      },
      {
        "id": "oai:arXiv.org:2502.03424v5",
        "title": "Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators",
        "link": "https://arxiv.org/abs/2502.03424",
        "author": "Yuan Xinjie, Khalid M. Mosalam",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03424v5 Announce Type: replace \nAbstract: Fire safety is crucial for ensuring the stability of building structures, yet evaluating whether a structure meets fire safety requirement is challenging. Fires can originate at any point within a structure, and simulating every potential fire scenario is both expensive and time-consuming. To address this challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and an efficient machine learning framework for its identification. The MFSP is defined as the location at which a fire, if initiated, would cause the most severe detrimental impact on the building's stability, effectively representing the worst-case fire scenario. In our framework, a Graph Neural Network (GNN) serves as an efficient and differentiable agent for conventional Finite Element Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio (MIDR) under fire, which then guides the training and evaluation of the MFSP predictor. Additionally, we enhance our framework with a novel edge update mechanism and a transfer learning-based training scheme. Evaluations on a large-scale simulation dataset demonstrate the good performance of the proposed framework in identifying the MFSP, offering a transformative tool for optimizing fire safety assessments in structural design. All developed datasets and codes are open-sourced online."
      },
      {
        "id": "oai:arXiv.org:2502.03569v2",
        "title": "Controllable Sequence Editing for Biological and Clinical Trajectories",
        "link": "https://arxiv.org/abs/2502.03569",
        "author": "Michelle M. Li, Kevin Li, Yasha Ektefaie, Ying Jin, Yepeng Huang, Shvat Messica, Tianxi Cai, Marinka Zitnik",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03569v2 Announce Type: replace \nAbstract: Conditional generation models for longitudinal sequences can generate new or modified trajectories given a conditioning input. While effective at generating entire sequences, these models typically lack control over the timing and scope of the edits. Most existing approaches either operate on univariate sequences or assume that the condition affects all variables and time steps. However, many scientific and clinical applications require more precise interventions, where a condition takes effect only after a specific time and influences only a subset of variables. We introduce CLEF, a controllable sequence editing model for conditional generation of immediate and delayed effects in multivariate longitudinal sequences. CLEF learns temporal concepts that encode how and when a condition alters future sequence evolution. These concepts allow CLEF to apply targeted edits to the affected time steps and variables while preserving the rest of the sequence. We evaluate CLEF on 6 datasets spanning cellular reprogramming and patient health trajectories, comparing against 9 state-of-the-art baselines. CLEF improves immediate sequence editing accuracy by up to 36.01% (MAE). Unlike prior models, CLEF enables one-step conditional generation at arbitrary future times, outperforming them in delayed sequence editing by up to 65.71% (MAE). We test CLEF under counterfactual inference assumptions and show up to 63.19% (MAE) improvement on zero-shot conditional generation of counterfactual trajectories. In a case study of patients with type 1 diabetes mellitus, CLEF identifies clinical interventions that generate realistic counterfactual trajectories shifted toward healthier outcomes."
      },
      {
        "id": "oai:arXiv.org:2502.04328v3",
        "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model",
        "link": "https://arxiv.org/abs/2502.04328",
        "author": "Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04328v3 Announce Type: replace \nAbstract: Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal Language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts, pushing the frontiers of the omni-modal language model to a large extent. We conduct a comprehensive exploration of architectural design, data curation, and training strategies essential for building a robust omni-modal model. Ola incorporates advanced visual understanding and audio recognition capabilities through several critical and effective improvements over mainstream baselines. Moreover, we rethink inter-modal relationships during omni-modal training, emphasizing cross-modal alignment with video as a central bridge, and propose a progressive training pipeline that begins with the most distinct modalities and gradually moves towards closer modality alignment. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola."
      },
      {
        "id": "oai:arXiv.org:2502.05446v2",
        "title": "Stochastic Forward-Backward Deconvolution: Training Diffusion Models with Finite Noisy Datasets",
        "link": "https://arxiv.org/abs/2502.05446",
        "author": "Haoye Lu, Qifan Wu, Yaoliang Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05446v2 Announce Type: replace \nAbstract: Recent diffusion-based generative models achieve remarkable results by training on massive datasets, yet this practice raises concerns about memorization and copyright infringement. A proposed remedy is to train exclusively on noisy data with potential copyright issues, ensuring the model never observes original content. However, through the lens of deconvolution theory, we show that although it is theoretically feasible to learn the data distribution from noisy samples, the practical challenge of collecting sufficient samples makes successful learning nearly unattainable. To overcome this limitation, we propose to pretrain the model with a small fraction of clean data to guide the deconvolution process. Combined with our Stochastic Forward--Backward Deconvolution (SFBD) method, we attain FID 6.31 on CIFAR-10 with just 4% clean images (and 3.58 with 10%). We also provide theoretical guarantees that SFBD learns the true data distribution. These results underscore the value of limited clean pretraining, or pretraining on similar datasets. Empirical studies further validate and enrich our findings."
      },
      {
        "id": "oai:arXiv.org:2502.06806v3",
        "title": "Logits are All We Need to Adapt Closed Models",
        "link": "https://arxiv.org/abs/2502.06806",
        "author": "Gaurush Hiranandani, Haolun Wu, Subhojyoti Mukherjee, Sanmi Koyejo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06806v3 Announce Type: replace \nAbstract: Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to Plugin model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models."
      },
      {
        "id": "oai:arXiv.org:2502.07783v2",
        "title": "Curvature Tuning: Provable Training-free Model Steering From a Single Parameter",
        "link": "https://arxiv.org/abs/2502.07783",
        "author": "Leyang Hu, Matteo Gamba, Randall Balestriero",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07783v2 Announce Type: replace \nAbstract: The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a model's decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions-thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, and improves robust accuracy on the $\\ell_\\infty$ benchmark from RobustBench by 1032.64%/1494.46%. Our code is available at https://github.com/Leon-Leyang/curvature-tuning."
      },
      {
        "id": "oai:arXiv.org:2502.08246v2",
        "title": "Inference-time sparse attention with asymmetric indexing",
        "link": "https://arxiv.org/abs/2502.08246",
        "author": "Pierre-Emmanuel Mazar\\'e, Gergely Szilvasy, Maria Lomeli, Francisco Massa, Naila Murray, Herv\\'e J\\'egou, Matthijs Douze",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.08246v2 Announce Type: replace \nAbstract: Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compatible vector search algorithms based on standard partitioning methods such as k-means. However, such partitioning methods yield poor results in this context because (1) the keys and queries follow different distributions, and (2) the RoPE positional encoding hinders the bucket assignment.\n  This paper introduces Saap (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern. It works on pretrained language models and only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, Saap typically reduces by a factor of 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\\% when compared to FlashAttention-v2."
      },
      {
        "id": "oai:arXiv.org:2502.09376v3",
        "title": "LoRA Training Provably Converges to a Low-Rank Global Minimum or It Fails Loudly (But it Probably Won't Fail)",
        "link": "https://arxiv.org/abs/2502.09376",
        "author": "Junsu Kim, Jaeyeon Kim, Ernest K. Ryu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09376v3 Announce Type: replace \nAbstract: Low-rank adaptation (LoRA) has become a standard approach for fine-tuning large foundation models. However, our theoretical understanding of LoRA remains limited as prior analyses of LoRA's training dynamics either rely on linearization arguments or consider highly simplified setups. In this work, we analyze the LoRA loss landscape without such restrictive assumptions. We define two regimes: a \"special regime\", which includes idealized setups where linearization arguments hold, and a \"generic regime\" representing more realistic setups where linearization arguments do not hold. In the generic regime, we show that LoRA training converges to a global minimizer with low rank and small magnitude, or a qualitatively distinct solution with high rank and large magnitude. Finally, we argue that the zero-initialization and weight decay in LoRA training induce an implicit bias toward the low-rank, small-magnitude region of the parameter space -- where global minima lie -- thus shedding light on why LoRA training usually succeeds in finding global minima."
      },
      {
        "id": "oai:arXiv.org:2502.09416v2",
        "title": "Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?",
        "link": "https://arxiv.org/abs/2502.09416",
        "author": "Takumi Goto, Yusuke Sakai, Taro Watanabe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09416v2 Announce Type: replace \nAbstract: One of the goals of automatic evaluation metrics in grammatical error correction (GEC) is to rank GEC systems such that it matches human preferences. However, current automatic evaluations are based on procedures that diverge from human evaluation. Specifically, human evaluation derives rankings by aggregating sentence-level relative evaluation results, e.g., pairwise comparisons, using a rating algorithm, whereas automatic evaluation averages sentence-level absolute scores to obtain corpus-level scores, which are then sorted to determine rankings. In this study, we propose an aggregation method for existing automatic evaluation metrics which aligns with human evaluation methods to bridge this gap. We conducted experiments using various metrics, including edit-based metrics, n-gram based metrics, and sentence-level metrics, and show that resolving the gap improves results for the most of metrics on the SEEDA benchmark. We also found that even BERT-based metrics sometimes outperform the metrics of GPT-4. The proposed ranking method is integrated gec-metrics."
      },
      {
        "id": "oai:arXiv.org:2502.10927v2",
        "title": "The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training",
        "link": "https://arxiv.org/abs/2502.10927",
        "author": "Matteo Saponati, Pascal Sager, Pau Vilimelis Aceituno, Thilo Stadelmann, Benjamin Grewe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.10927v2 Announce Type: replace \nAbstract: Self-attention is essential to Transformer architectures, yet how information is embedded in the self-attention matrices and how different objective functions impact this process remains unclear. We present a mathematical framework to analyze self-attention matrices by deriving the structures governing their weight updates. Using this framework, we demonstrate that bidirectional training induces symmetry in the weight matrices, while autoregressive training results in directionality and column dominance. Our theoretical findings are validated across multiple Transformer models - including ModernBERT, GPT, LLaMA3, and Mistral - and input modalities like text, vision, and audio. Finally, we apply these insights by showing that symmetric initialization improves the performance of encoder-only models on language tasks. This mathematical analysis offers a novel theoretical perspective on how information is embedded through self-attention, thereby improving the interpretability of Transformer models."
      },
      {
        "id": "oai:arXiv.org:2502.10973v3",
        "title": "Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues",
        "link": "https://arxiv.org/abs/2502.10973",
        "author": "David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.10973v3 Announce Type: replace \nAbstract: In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources."
      },
      {
        "id": "oai:arXiv.org:2502.11075v2",
        "title": "Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models",
        "link": "https://arxiv.org/abs/2502.11075",
        "author": "Haoyang Li, Xuejia Chen, Zhanchao XU, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11075v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: https://github.com/TreeAI-Lab/NumericBench."
      },
      {
        "id": "oai:arXiv.org:2502.11184v2",
        "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs",
        "link": "https://arxiv.org/abs/2502.11184",
        "author": "Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11184v2 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research."
      },
      {
        "id": "oai:arXiv.org:2502.12427v3",
        "title": "ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image Super-Resolution in Earth System",
        "link": "https://arxiv.org/abs/2502.12427",
        "author": "Ehsan Zeraatkar, Salah A Faroughi, Jelena Te\\v{s}i\\'c",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12427v3 Announce Type: replace \nAbstract: Super-resolution (SR) is crucial for enhancing the spatial resolution of Earth System Model (ESM) data, thereby enabling more precise analysis of environmental processes. This paper introduces ViFOR, a novel SR algorithm integrating Vision Transformers (ViTs) with Fourier-based Implicit Neural Representation Networks (INRs). ViFOR effectively captures global context and high-frequency details essential for accurate SR reconstruction by embedding Fourier-based activation functions within the transformer architecture. Extensive experiments demonstrate that ViFOR consistently outperforms state-of-the-art methods, including ViT, SIREN, and SRGANs, in terms of Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) for both global and local imagery. ViFOR achieves PSNR improvements of up to 4.18 dB, 1.56 dB, and 1.73 dB over ViT on full-image Source Temperature, Shortwave, and Longwave Flux datasets. These results highlight ViFOR's effectiveness and potential for advancing high-resolution climate data analysis."
      },
      {
        "id": "oai:arXiv.org:2502.12562v3",
        "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
        "link": "https://arxiv.org/abs/2502.12562",
        "author": "Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12562v3 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA."
      },
      {
        "id": "oai:arXiv.org:2502.12665v2",
        "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization",
        "link": "https://arxiv.org/abs/2502.12665",
        "author": "Junhui He, Junna Xing, Nan Wang, Rui Xu, Shangyu Wu, Peng Zhou, Qiang Liu, Chun Jason Xue, Qingan Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12665v2 Announce Type: replace \nAbstract: Long context large language models (LLMs) pose significant challenges for efficient serving due to the large memory footprint and high access overhead of KV cache. Retrieval-based KV cache reduction methods can mitigate these challenges, typically by offloading the complete KV cache to CPU and retrieving necessary tokens on demand during inference. However, these methods still suffer from unsatisfactory accuracy degradation and extra retrieval overhead. To address these limitations, this paper proposes A$^2$ATS, a novel retrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate approximation of attention scores by applying the vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens. First, we propose Windowed Rotary Position Embedding, which decouples the positional dependency from query and key states after position embedding. Then, we propose query-aware vector quantization that optimizes the objective of attention score approximation directly. Finally, we design the heterogeneous inference architecture for KV cache offloading, enabling long context serving with larger batch sizes. Experimental results demonstrate that A$^2$ATS can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to $2.7 \\times$."
      },
      {
        "id": "oai:arXiv.org:2502.12921v2",
        "title": "Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison",
        "link": "https://arxiv.org/abs/2502.12921",
        "author": "George-Kirollos Saad, Scott Sanner",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12921v2 Announce Type: replace \nAbstract: Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS."
      },
      {
        "id": "oai:arXiv.org:2502.13928v2",
        "title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images",
        "link": "https://arxiv.org/abs/2502.13928",
        "author": "Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.13928v2 Announce Type: replace \nAbstract: Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. We opensource our code at https://s-vco.github.io/"
      },
      {
        "id": "oai:arXiv.org:2502.14354v2",
        "title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment",
        "link": "https://arxiv.org/abs/2502.14354",
        "author": "Moxin Li, Yuantao Zhang, Wenjie Wang, Wentao Shi, Zhuo Liu, Fuli Feng, Tat-Seng Chua",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14354v2 Announce Type: replace \nAbstract: Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at https://github.com/zyttt-coder/SIPO."
      },
      {
        "id": "oai:arXiv.org:2502.14376v2",
        "title": "A Similarity Paradigm Through Textual Regularization Without Forgetting",
        "link": "https://arxiv.org/abs/2502.14376",
        "author": "Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14376v2 Announce Type: replace \nAbstract: Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods."
      },
      {
        "id": "oai:arXiv.org:2502.14779v2",
        "title": "DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models",
        "link": "https://arxiv.org/abs/2502.14779",
        "author": "Hongji Yang, Wencheng Han, Yucheng Zhou, Jianbing Shen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14779v2 Announce Type: replace \nAbstract: In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and precisely controllable framework for multi-condition image generation. The core idea behind DC-ControlNet is to decouple control conditions, transforming global control into a hierarchical system that integrates distinct elements, contents, and layouts. This enables users to mix these individual conditions with greater flexibility, leading to more efficient and accurate image generation control. Previous ControlNet-based models rely solely on global conditions, which affect the entire image and lack the ability of element- or region-specific control. This limitation reduces flexibility and can cause condition misunderstandings in multi-conditional image generation. To address these challenges, we propose both intra-element and Inter-element Controllers in DC-ControlNet. The Intra-Element Controller handles different types of control signals within individual elements, accurately describing the content and layout characteristics of the object. For interactions between elements, we introduce the Inter-Element Controller, which accurately handles multi-element interactions and occlusion based on user-defined relationships. Extensive evaluations show that DC-ControlNet significantly outperforms existing ControlNet models and Layout-to-Image generative models in terms of control flexibility and precision in multi-condition control. Our project website is available at: https://um-lab.github.io/DC-ControlNet/"
      },
      {
        "id": "oai:arXiv.org:2502.15109v3",
        "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models",
        "link": "https://arxiv.org/abs/2502.15109",
        "author": "Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15109v3 Announce Type: replace \nAbstract: Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce SOCIAL GENOME, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). SOCIAL GENOME is also the first modeling challenge to study external knowledge in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of SOCIAL GENOME through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models."
      },
      {
        "id": "oai:arXiv.org:2502.16368v3",
        "title": "Concept Corrector: Erase concepts on the fly for text-to-image diffusion models",
        "link": "https://arxiv.org/abs/2502.16368",
        "author": "Zheling Meng, Bo Peng, Xiaochuan Jin, Yueming Lyu, Wei Wang, Jing Dong, Tieniu Tan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16368v3 Announce Type: replace \nAbstract: Text-to-image diffusion models have demonstrated the underlying risk of generating various unwanted content, such as sexual elements. To address this issue, the task of concept erasure has been introduced, aiming to erase any undesired concepts that the models can generate. Previous methods, whether training-based or training-free, have primarily focused on the input side, i.e., texts. However, they often suffer from incomplete erasure due to limitations in the generalization from limited prompts to diverse image content. In this paper, motivated by the notion that concept erasure on the output side, i.e., generated images, may be more direct and effective, we propose Concept Corrector. It checks target concepts based on visual features provided by final generated images predicted at certain time steps. Further, it incorporates Concept Removal Attention to erase generated concept features. It overcomes the limitations of existing methods, which are either unable to remove the concept features that have been generated in images or rely on the assumption that the related concept words are contained in input prompts. In the whole pipeline, our method changes no model parameters and only requires a given target concept as well as the corresponding replacement content, which is easy to implement. To the best of our knowledge, this is the first erasure method based on intermediate-generated images, achieving the ability to erase concepts on the fly. The experiments on various concepts demonstrate its impressive erasure performance."
      },
      {
        "id": "oai:arXiv.org:2502.16462v2",
        "title": "Improved Margin Generalization Bounds for Voting Classifiers",
        "link": "https://arxiv.org/abs/2502.16462",
        "author": "Mikael M{\\o}ller H{\\o}gsgaard, Kasper Green Larsen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16462v2 Announce Type: replace \nAbstract: In this paper we establish a new margin-based generalization bound for voting classifiers, refining existing results and yielding tighter generalization guarantees for widely used boosting algorithms such as AdaBoost (Freund and Schapire, 1997). Furthermore, the new margin-based generalization bound enables the derivation of an optimal weak-to-strong learner: a Majority-of-3 large-margin classifiers with an expected error matching the theoretical lower bound. This result provides a more natural alternative to the Majority-of-5 algorithm by (H{\\o}gsgaard et al., 2024), and matches the Majority-of-3 result by (Aden-Ali et al., 2024) for the realizable prediction model."
      },
      {
        "id": "oai:arXiv.org:2502.17110v3",
        "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation",
        "link": "https://arxiv.org/abs/2502.17110",
        "author": "Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17110v3 Announce Type: replace \nAbstract: The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation."
      },
      {
        "id": "oai:arXiv.org:2502.17214v2",
        "title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought",
        "link": "https://arxiv.org/abs/2502.17214",
        "author": "Boxuan Zhang, Ruqi Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17214v2 Announce Type: replace \nAbstract: Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on Llama Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ."
      },
      {
        "id": "oai:arXiv.org:2502.17878v2",
        "title": "Towards Enhanced Immersion and Agency for LLM-based Interactive Drama",
        "link": "https://arxiv.org/abs/2502.17878",
        "author": "Hongqiu Wu, Weiqi Wu, Tianyang Xu, Jiameng Zhang, Hai Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17878v2 Announce Type: replace \nAbstract: LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency."
      },
      {
        "id": "oai:arXiv.org:2502.18017v2",
        "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
        "link": "https://arxiv.org/abs/2502.18017",
        "author": "Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, Feng Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.18017v2 Announce Type: replace \nAbstract: Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark. The code is available at https://github.com/Alibaba-NLP/ViDoRAG."
      },
      {
        "id": "oai:arXiv.org:2502.18460v2",
        "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers",
        "link": "https://arxiv.org/abs/2502.18460",
        "author": "Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.18460v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated strong effectiveness and robustness while fine-tuned as dense retrievers. However, their large parameter size brings significant inference time computational challenges, including high encoding costs for large-scale corpora and increased query latency, limiting their practical deployment. While smaller retrievers offer better efficiency, they often fail to generalize effectively with limited supervised fine-tuning data. In this work, we introduce DRAMA, a training framework that leverages LLMs to train smaller generalizable dense retrievers. In particular, we adopt pruned LLMs as the backbone and train on diverse LLM-augmented data in a single-stage contrastive learning setup. Experiments show that DRAMA offers better multilingual and long-context capabilities than traditional encoder-based retrievers, and achieves strong performance across multiple tasks and languages. These highlight the potential of connecting the training of smaller retrievers with the growing advancements in LLMs, bridging the gap between efficiency and generalization."
      },
      {
        "id": "oai:arXiv.org:2502.19030v2",
        "title": "Sampling nodes and hyperedges via random walks on large hypergraphs",
        "link": "https://arxiv.org/abs/2502.19030",
        "author": "Kazuki Nakajima, Masanao Kodakari, Masaki Aida",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.19030v2 Announce Type: replace \nAbstract: Hypergraphs provide a fundamental framework for representing complex systems involving interactions among three or more entities. As empirical hypergraphs grow in size, characterizing their structural properties becomes increasingly challenging due to computational complexity and, in some cases, restricted access to complete data, requiring efficient sampling methods. Random walks offer a practical approach to hypergraph sampling, as they rely solely on local neighborhood information from nodes and hyperedges. In this study, we investigate methods for simultaneously sampling nodes and hyperedges via random walks on large hypergraphs. First, we compare three existing random walks in the context of hypergraph sampling and identify an advantage of the so-called higher-order random walk. Second, by extending an established technique for graphs to the case of hypergraphs, we present a non-backtracking variant of the higher-order random walk. We derive theoretical results on estimators based on the non-backtracking higher-order random walk and validate them through numerical simulations on large empirical hypergraphs. Third, we apply the non-backtracking higher-order random walk to a large hypergraph of co-authorships indexed in the OpenAlex database, where full access to the data is not readily available. Despite the relatively small sample size, our estimates largely align with previous findings on author productivity, team size, and the prevalence of open-access publications. Our findings contribute to the development of analysis methods for large hypergraphs, offering insights into sampling strategies and estimation techniques applicable to real-world complex systems."
      },
      {
        "id": "oai:arXiv.org:2502.19756v2",
        "title": "PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation",
        "link": "https://arxiv.org/abs/2502.19756",
        "author": "Nathan Roll",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.19756v2 Announce Type: replace \nAbstract: Large language models (LLMs) showcase increasingly impressive English benchmark scores, however their performance profiles remain inconsistent across multilingual settings. To address this gap, we introduce PolyPrompt, a novel, parameter-efficient framework for enhancing the multilingual capabilities of LLMs. Our method learns a set of trigger tokens for each language through a gradient-based search, identifying the input query's language and selecting the corresponding trigger tokens which are prepended to the prompt during inference. We perform experiments on two ~1 billion parameter models, with evaluations on the global MMLU benchmark across fifteen typologically and resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared to naive and translation-pipeline baselines."
      },
      {
        "id": "oai:arXiv.org:2502.20129v3",
        "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking",
        "link": "https://arxiv.org/abs/2502.20129",
        "author": "Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20129v3 Announce Type: replace \nAbstract: Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at https://github.com/IvanChangPKU/FSA."
      },
      {
        "id": "oai:arXiv.org:2503.00229v2",
        "title": "Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster",
        "link": "https://arxiv.org/abs/2503.00229",
        "author": "Sharan Vaswani, Reza Babanezhad",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00229v2 Announce Type: replace \nAbstract: Armijo line-search (Armijo-LS) is a standard method to set the step-size for gradient descent (GD). For smooth functions, Armijo-LS alleviates the need to know the global smoothness constant L and adapts to the ``local'' smoothness, enabling GD to converge faster. Existing theoretical analyses show that GD with Armijo-LS (GD-LS) can result in constant factor improvements over GD with a 1/L step-size (denoted as GD(1/L)). We strengthen these results and show that if the objective function satisfies a certain non-uniform smoothness condition, GD-LS can result in a faster convergence rate than GD(1/L). In particular, we prove that for convex objectives corresponding to logistic regression and multi-class classification, GD-LS can converge to the optimum at a linear rate, and hence improves over the sublinear convergence of GD(1/L). Furthermore, for non-convex objectives satisfying gradient domination (e.g., those corresponding to the softmax policy gradient in RL or generalized linear models with a logistic link function), GD-LS can match the fast convergence of algorithms tailored for these specific settings. Finally, we prove that under the interpolation assumption, for convex losses, stochastic GD with a stochastic line-search can match the fast convergence of GD-LS"
      },
      {
        "id": "oai:arXiv.org:2503.01926v2",
        "title": "Unnatural Languages Are Not Bugs but Features for LLMs",
        "link": "https://arxiv.org/abs/2503.01926",
        "author": "Keyu Duan, Yiran Zhao, Zhili Feng, Jinjie Ni, Tianyu Pang, Qian Liu, Tianle Cai, Longxu Dou, Kenji Kawaguchi, Anirudh Goyal, J. Zico Kolter, Michael Qizhe Shieh",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.01926v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving 49.71 win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis, we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words."
      },
      {
        "id": "oai:arXiv.org:2503.02039v2",
        "title": "Dynamic Search for Inference-Time Alignment in Diffusion Models",
        "link": "https://arxiv.org/abs/2503.02039",
        "author": "Xiner Li, Masatoshi Uehara, Xingyu Su, Gabriele Scalia, Tommaso Biancalani, Aviv Regev, Sergey Levine, Shuiwang Ji",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02039v2 Announce Type: replace \nAbstract: Diffusion models have shown promising generative capabilities across diverse domains, yet aligning their outputs with desired reward functions remains a challenge, particularly in cases where reward functions are non-differentiable. Some gradient-free guidance methods have been developed, but they often struggle to achieve optimal inference-time alignment. In this work, we newly frame inference-time alignment in diffusion as a search problem and propose Dynamic Search for Diffusion (DSearch), which subsamples from denoising processes and approximates intermediate node rewards. It also dynamically adjusts beam width and tree expansion to efficiently explore high-reward generations. To refine intermediate decisions, DSearch incorporates adaptive scheduling based on noise levels and a lookahead heuristic function. We validate DSearch across multiple domains, including biological sequence design, molecular optimization, and image generation, demonstrating superior reward optimization compared to existing approaches."
      },
      {
        "id": "oai:arXiv.org:2503.02143v2",
        "title": "Four Principles for Physically Interpretable World Models",
        "link": "https://arxiv.org/abs/2503.02143",
        "author": "Jordan Peper, Zhenjiang Mao, Yuang Geng, Siyuan Pan, Ivan Ruchkin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02143v2 Announce Type: replace \nAbstract: As autonomous systems are increasingly deployed in open and uncertain settings, there is a growing need for trustworthy world models that can reliably predict future high-dimensional observations. The learned latent representations in world models lack direct mapping to meaningful physical quantities and dynamics, limiting their utility and interpretability in downstream planning, control, and safety verification. In this paper, we argue for a fundamental shift from physically informed to physically interpretable world models - and crystallize four principles that leverage symbolic knowledge to achieve these ends: (1) functionally organizing the latent space according to the physical intent, (2) learning aligned invariant and equivariant representations of the physical world, (3) integrating multiple forms and strengths of supervision into a unified training process, and (4) partitioning generative outputs to support scalability and verifiability. We experimentally demonstrate the value of each principle on two benchmarks. This paper opens several intriguing research directions to achieve and capitalize on full physical interpretability in world models."
      },
      {
        "id": "oai:arXiv.org:2503.02519v3",
        "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent",
        "link": "https://arxiv.org/abs/2503.02519",
        "author": "Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02519v3 Announce Type: replace \nAbstract: Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods."
      },
      {
        "id": "oai:arXiv.org:2503.04472v2",
        "title": "DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models",
        "link": "https://arxiv.org/abs/2503.04472",
        "author": "Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, Shiguo Lian",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04472v2 Announce Type: replace \nAbstract: Recent advancements in slow thinking reasoning models have shown exceptional performance in complex reasoning tasks. However, these models often exhibit overthinking (generating redundant reasoning steps for simple problems), leading to excessive computational resource usage. While current mitigation strategies uniformly reduce reasoning tokens, they risk degrading performance on challenging tasks that require extended reasoning. This paper introduces Difficulty-Adaptive Slow Thinking (DAST), a novel framework that enables models to autonomously adjust the length of Chain-of-Thought (CoT) based on problem difficulty. We first propose a Token Length Budget (TLB) metric to quantify difficulty, then leverage budget-aware reward shaping and budget preference optimization to implement DAST. DAST penalizes overlong responses for simple tasks while incentivizing sufficient reasoning for complex problems. Experiments on diverse datasets and model scales demonstrate that DAST effectively mitigates overthinking (reducing token usage by over 30\\% on average) while preserving reasoning accuracy on complex problems. Our codes and models are available at https://github.com/AnonymousUser0520/AnonymousRepo01."
      },
      {
        "id": "oai:arXiv.org:2503.05037v2",
        "title": "Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence",
        "link": "https://arxiv.org/abs/2503.05037",
        "author": "Mohsen Fayyaz, Ali Modarressi, Hinrich Schuetze, Nanyun Peng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.05037v2 Announce Type: replace \nAbstract: Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid downstream failures. In this work, we repurpose a relation extraction dataset (e.g., Re-DocRED) to design controlled experiments that quantify the impact of heuristic biases, such as a preference for shorter documents, on retrievers like Dragon+ and Contriever. We uncover major vulnerabilities, showing retrievers favor shorter documents, early positions, repeated entities, and literal matches, all while ignoring the answer's presence! Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 10% of cases over a synthetic biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than providing no documents at all. https://huggingface.co/datasets/mohsenfayyaz/ColDeR"
      },
      {
        "id": "oai:arXiv.org:2503.05306v2",
        "title": "Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning",
        "link": "https://arxiv.org/abs/2503.05306",
        "author": "Hyungkyu Kang, Min-hwan Oh",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.05306v2 Announce Type: replace \nAbstract: In this paper, we study offline preference-based reinforcement learning (PbRL), where learning is based on pre-collected preference feedback over pairs of trajectories. While offline PbRL has demonstrated remarkable empirical success, existing theoretical approaches face challenges in ensuring conservatism under uncertainty, requiring computationally intractable confidence set constructions. We address this limitation by proposing Adversarial Preference-based Policy Optimization (APPO), a computationally efficient algorithm for offline PbRL that guarantees sample complexity bounds without relying on explicit confidence sets. By framing PbRL as a two-player game between a policy and a model, our approach enforces conservatism in a tractable manner. Using standard assumptions on function approximation and bounded trajectory concentrability, we derive a sample complexity bound. To our knowledge, APPO is the first offline PbRL algorithm to offer both statistical efficiency and practical applicability. Experimental results on continuous control tasks demonstrate that APPO effectively learns from complex datasets, showing comparable performance with existing state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2503.05840v2",
        "title": "Slim attention: cut your context memory in half without loss -- K-cache is all you need for MHA",
        "link": "https://arxiv.org/abs/2503.05840",
        "author": "Nils Graef, Andrew Wasielewski",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.05840v2 Announce Type: replace \nAbstract: Slim attention shrinks the context memory size by 2x for transformer models with MHA (multi-head attention), which can speed up inference by up to 2x for large context windows.\n  Slim attention is an exact, mathematically identical implementation of the standard attention mechanism and therefore doesn't compromise model accuracy. In other words, slim attention losslessly compresses the context memory by a factor of 2.\n  For encoder-decoder transformers, the context memory size can be reduced even further: For the Whisper models for example, slim attention reduces the context memory by 8x, which can speed up token generation by 5x for batch size 64 for example.\n  And for the T5-11B model for example, the memory can be reduced by 32x because its MHA projection dimension is larger than the embedding dimension.\n  See https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks, and https://www.youtube.com/watch?v=uVtk3B6YO4Y for this paper's YouTube video."
      },
      {
        "id": "oai:arXiv.org:2503.10080v3",
        "title": "Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection",
        "link": "https://arxiv.org/abs/2503.10080",
        "author": "Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang, Xingang Wang, Guiguang Ding",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10080v3 Announce Type: replace \nAbstract: Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text prompts through either manual design or the optimization of learnable prompt vectors. However, these methods face several challenges: 1) handcrafted prompts require extensive expert knowledge and trial-and-error; 2) single-form learnable prompts struggle to capture complex anomaly semantics; and 3) an unconstrained prompt space limits generalization to unseen categories. To address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL), which models the prompt space as a learnable probability distribution from a Bayesian perspective. Specifically, a prompt flow module is designed to learn both image-specific and image-agnostic distributions, which are jointly utilized to regularize the text prompt space and improve the model's generalization on unseen categories. These learned distributions are then sampled to generate diverse text prompts, effectively covering the prompt space. Additionally, a residual cross-model attention (RCA) module is introduced to better align dynamic text embeddings with fine-grained image features. Extensive experiments on 15 industrial and medical datasets demonstrate our method's superior performance. The code is available at https://github.com/xiaozhen228/Bayes-PFL."
      },
      {
        "id": "oai:arXiv.org:2503.10503v2",
        "title": "Sample Compression for Continual Learning",
        "link": "https://arxiv.org/abs/2503.10503",
        "author": "Jacob Comeau, Mathieu Bazinet, Pascal Germain, Cem Subakan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10503v2 Announce Type: replace \nAbstract: Continual learning algorithms aim to learn from a sequence of tasks, making the training distribution non-stationary. The majority of existing continual learning approaches in the literature rely on heuristics and do not provide learning guarantees. In this paper, we present a new method called Continual Pick-to-Learn (CoP2L), which is able to retain the most representative samples for each task in an efficient way. CoP2L combines the Pick-to-Learn algorithm (rooted in the sample compression theory) and the experience replay continual learning scheme. This allows us to provide non-vacuous upper bounds on the generalization loss of the learned predictors, numerically computable after each task. We empirically evaluate our approach on several standard continual learning benchmarks across Class-Incremental, Task-Incremental, and Domain-Incremental settings. Our results show that CoP2L is highly competitive across all setups, often outperforming existing baselines, and significantly mitigating catastrophic forgetting compared to vanilla experience replay in the Class-Incremental setting. It is possible to leverage the bounds provided by CoP2L in practical scenarios to certify the predictor reliability on previously learned tasks, in order to improve the trustworthiness of the continual learning algorithm."
      },
      {
        "id": "oai:arXiv.org:2503.10633v2",
        "title": "We Should Chart an Atlas of All the World's Models",
        "link": "https://arxiv.org/abs/2503.10633",
        "author": "Eliahu Horwitz, Nitzan Kurer, Jonathan Kahana, Liel Amar, Yedid Hoshen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10633v2 Announce Type: replace \nAbstract: Public model repositories now contain millions of models, yet most models remain undocumented and effectively lost. In this position paper, we advocate for charting the world's model population in a unified structure we call the Model Atlas: a graph that captures models, their attributes, and the weight transformations that connect them. The Model Atlas enables applications in model forensics, meta-ML research, and model discovery, challenging tasks given today's unstructured model repositories. However, because most models lack documentation, large atlas regions remain uncharted. Addressing this gap motivates new machine learning methods that treat models themselves as data, inferring properties such as functionality, performance, and lineage directly from their weights. We argue that a scalable path forward is to bypass the unique parameter symmetries that plague model weights. Charting all the world's models will require a community effort, and we hope its broad utility will rally researchers toward this goal."
      },
      {
        "id": "oai:arXiv.org:2503.10927v3",
        "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses",
        "link": "https://arxiv.org/abs/2503.10927",
        "author": "Angela Lopez-Cardona, Sebastian Idesis, Miguel Barreda-\\'Angeles, Sergi Abadal, Ioannis Arapakis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10927v3 Announce Type: replace \nAbstract: While Large Language Models (LLMs) have significantly advanced natural language processing, aligning them with human preferences remains an open challenge. Although current alignment methods rely primarily on explicit feedback, eye-tracking (ET) data offers insights into real-time cognitive processing during reading. In this paper, we present OASST-ETC, a novel eye-tracking corpus capturing reading patterns from 24 participants, while evaluating LLM-generated responses from the OASST1 dataset. Our analysis reveals distinct reading patterns between preferred and non-preferred responses, which we compare with synthetic eye-tracking data. Furthermore, we examine the correlation between human reading measures and attention patterns from various transformer-based models, discovering stronger correlations in preferred responses. This work introduces a unique resource for studying human cognitive processing in LLM evaluation and suggests promising directions for incorporating eye-tracking data into alignment methods. The dataset and analysis code are publicly available."
      },
      {
        "id": "oai:arXiv.org:2503.11630v3",
        "title": "The time scale of redundancy between prosody and linguistic context",
        "link": "https://arxiv.org/abs/2503.11630",
        "author": "Tamar I. Regev, Chiebuka Ohams, Shaylee Xie, Lukas Wolf, Evelina Fedorenko, Alex Warstadt, Ethan G. Wilcox, Tiago Pimentel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.11630v3 Announce Type: replace \nAbstract: In spoken communication, information is transmitted not only via words, but also through a rich array of non-verbal signals, including prosody--the non-segmental auditory features of speech. Do these different communication channels carry distinct information? Prior work has shown that the information carried by prosodic features is substantially redundant with that carried by the surrounding words. Here, we systematically examine the time scale of this relationship, studying how it varies with the length of past and future contexts. We find that a word's prosodic features require an extended past context (3-8 words across different features) to be reliably predicted. Given that long-scale contextual information decays in memory, prosody may facilitate communication by adding information that is locally unique. We also find that a word's prosodic features show some redundancy with future words, but only with a short scale of 1-2 words, consistent with reports of incremental short-term planning in language production. Thus, prosody may facilitate communication by helping listeners predict upcoming material. In tandem, our results highlight potentially distinct roles that prosody plays in facilitating integration of words into past contexts and in helping predict upcoming words."
      },
      {
        "id": "oai:arXiv.org:2503.13360v2",
        "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning",
        "link": "https://arxiv.org/abs/2503.13360",
        "author": "Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13360v2 Announce Type: replace \nAbstract: Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4 points vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems."
      },
      {
        "id": "oai:arXiv.org:2503.14433v2",
        "title": "Splintering Nonconcatenative Languages for Better Tokenization",
        "link": "https://arxiv.org/abs/2503.14433",
        "author": "Bar Gazit (Ben-Gurion University of the Negev), Shaltiel Shmidman (DICTA), Avi Shmidman (DICTA), Yuval Pinter (Ben-Gurion University of the Negev)",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.14433v2 Announce Type: replace \nAbstract: Common subword tokenization algorithms like BPE and UnigramLM assume that text can be split into meaningful units by concatenative measures alone. This is not true for languages such as Hebrew and Arabic, where morphology is encoded in root-template patterns, or Malay and Georgian, where split affixes are common. We present SPLINTER, a pre-processing step which rearranges text into a linear form that better represents such nonconcatenative morphologies, enabling meaningful contiguous segments to be found by the tokenizer. We demonstrate SPLINTER's merit using both intrinsic measures evaluating token vocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using BERT-architecture models trained for Hebrew."
      },
      {
        "id": "oai:arXiv.org:2503.14615v2",
        "title": "Unique Hard Attention: A Tale of Two Sides",
        "link": "https://arxiv.org/abs/2503.14615",
        "author": "Selim Jerad, Anej Svete, Jiaoda Li, Ryan Cotterell",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.14615v2 Announce Type: replace \nAbstract: Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention -- in that case, they correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to \\emph{soft} attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality."
      },
      {
        "id": "oai:arXiv.org:2503.15200v2",
        "title": "Partially Observable Reinforcement Learning with Memory Traces",
        "link": "https://arxiv.org/abs/2503.15200",
        "author": "Onno Eberhard, Michael Muehlebach, Claire Vernade",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15200v2 Announce Type: replace \nAbstract: Partially observable environments present a considerable computational challenge in reinforcement learning due to the need to consider long histories. Learning with a finite window of observations quickly becomes intractable as the window length grows. In this work, we introduce memory traces. Inspired by eligibility traces, these are compact representations of the history of observations in the form of exponential moving averages. We prove sample complexity bounds for the problem of offline on-policy evaluation that quantify the return errors achieved with memory traces for the class of Lipschitz continuous value estimates. We establish a close connection to the window approach, and demonstrate that, in certain environments, learning with memory traces is significantly more sample efficient. Finally, we underline the effectiveness of memory traces empirically in online reinforcement learning experiments for both value prediction and control."
      },
      {
        "id": "oai:arXiv.org:2503.16048v2",
        "title": "Meta-Learning Neural Mechanisms rather than Bayesian Priors",
        "link": "https://arxiv.org/abs/2503.16048",
        "author": "Michael Goodale, Salvador Mascarenhas, Yair Lakretz",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16048v2 Announce Type: replace \nAbstract: Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a single formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms."
      },
      {
        "id": "oai:arXiv.org:2503.17579v2",
        "title": "Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility",
        "link": "https://arxiv.org/abs/2503.17579",
        "author": "Suet-Ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17579v2 Announce Type: replace \nAbstract: Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025."
      },
      {
        "id": "oai:arXiv.org:2503.18052v2",
        "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining",
        "link": "https://arxiv.org/abs/2503.18052",
        "author": "Yue Li, Qi Ma, Runyi Yang, Huapeng Li, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, Martin R. Oswald, Danda Pani Paudel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18052v2 Announce Type: replace \nAbstract: Recognizing arbitrary or previously unseen categories is essential for comprehensive real-world 3D scene understanding. Currently, all existing methods rely on 2D or textual modalities during training or together at inference. This highlights the clear absence of a model capable of processing 3D data alone for learning semantics end-to-end, along with the necessary data to train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the de facto standard for 3D scene representation across various vision tasks. However, effectively integrating semantic reasoning into 3DGS in a generalizable manner remains an open challenge. To address these limitations, we introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene understanding approach that operates natively on 3DGS. Furthermore, we propose a self-supervised learning scheme that unlocks rich 3D feature learning from unlabeled scenes. To power the proposed methods, we introduce SceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising 7916 scenes derived from seven established datasets, such as ScanNet and Matterport3D. Generating SceneSplat-7K required computational resources equivalent to 150 GPU days on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning for indoor scenes. Our exhaustive experiments on SceneSplat-7K demonstrate the significant benefit of the proposed method over the established baselines."
      },
      {
        "id": "oai:arXiv.org:2503.20218v2",
        "title": "Video Motion Graphs",
        "link": "https://arxiv.org/abs/2503.20218",
        "author": "Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, Yang Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20218v2 Announce Type: replace \nAbstract: We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Results show that our Video Motion Graphs outperforms existing generative- and retrieval-based methods for multi-modal conditioned human motion video generation. Project page can be found at https://h-liu1997.github.io/Video-Motion-Graphs/"
      },
      {
        "id": "oai:arXiv.org:2503.21555v2",
        "title": "SyncSDE: A Probabilistic Framework for Diffusion Synchronization",
        "link": "https://arxiv.org/abs/2503.21555",
        "author": "Hyunjun Lee, Hyunsoo Lee, Sookwan Han",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21555v2 Announce Type: replace \nAbstract: There have been many attempts to leverage multiple diffusion models for collaborative generation, extending beyond the original domain. A prominent approach involves synchronizing multiple diffusion trajectories by mixing the estimated scores to artificially correlate the generation processes. However, existing methods rely on naive heuristics, such as averaging, without considering task specificity. These approaches do not clarify why such methods work and often produce suboptimal results when a heuristic suitable for one task is blindly applied to others. In this paper, we present a probabilistic framework for analyzing why diffusion synchronization works and reveal where heuristics should be focused; modeling correlations between multiple trajectories and adapting them to each specific task. We further identify optimal correlation models per task, achieving better results than previous approaches that apply a single heuristic across all tasks without justification."
      },
      {
        "id": "oai:arXiv.org:2503.22395v2",
        "title": "Negation: A Pink Elephant in the Large Language Models' Room?",
        "link": "https://arxiv.org/abs/2503.22395",
        "author": "Tereza Vrabcov\\'a, Marek Kadl\\v{c}\\'ik, Petr Sojka, Michal \\v{S}tef\\'anik, Michal Spiegel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22395v2 Announce Type: replace \nAbstract: Negations are key to determining sentence meaning, making them essential for logical reasoning. Despite their importance, negations pose a substantial challenge for large language models (LLMs) and remain underexplored.\n  We constructed and published two new textual entailment datasets NoFEVER-ML and NoSNLI-ML in four languages (English, Czech, German, and Ukrainian) with\n  examples differing in negation. It allows investigation of the root causes of the negation problem and its exemplification: how popular LLM model properties and language impact their inability to handle negation correctly.\n  Contrary to previous work, we show that increasing the model size may improve the models' ability to handle negations. Furthermore, we find that both the models' reasoning accuracy and robustness to negation are language-dependent and that the length and explicitness of the premise have an impact on robustness. There is better accuracy in projective language with fixed order, such as English, than in non-projective ones, such as German or Czech.\n  Our entailment datasets pave the way to further research for explanation and exemplification of the negation problem, minimization of LLM hallucinations, and improvement of LLM reasoning in multilingual settings."
      },
      {
        "id": "oai:arXiv.org:2503.23062v2",
        "title": "Shape and Texture Recognition in Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2503.23062",
        "author": "Sagi Eppel, Mor Bismut, Alona Faktor-Strugatski",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23062v2 Announce Type: replace \nAbstract: Shapes and textures are the basic building blocks of visual perception. The ability to identify shapes regardless of orientation, texture, or context, and to recognize textures and materials independently of their associated objects, is essential for a general visual understanding of the world. This work introduces the Large Shape and Textures dataset (LAS&amp;T), a giant collection of highly diverse shapes and textures, created by unsupervised extraction of patterns from natural images. This dataset is used to benchmark how effectively leading Large Vision-Language Models (LVLMs) understand shapes, textures, and materials in 2D and 3D scenes. For shape recognition, we test the models' ability to match images of identical shapes that differ in orientation, texture, color, or environment. Our results show that the shape recognition capabilities of the LVLMs remain significantly below human performance. LVLMs rely predominantly on high-level and semantic features and struggle with abstract shapes lacking clear class associations. For texture and material recognition, we evaluated the models' ability to identify images with identical textures and materials across different objects and environments. Interestingly, leading LVLMs approach human-level performance in recognizing materials in 3D scenes, yet substantially underperform humans when identifying simpler more abstract 2D textures. These results are consistent across a wide range of leading VLMs (GPT/Gemini/LLama/Qwen) and foundation vision models (DINO/CLIP), exposing major deficiencies in the ability of leading models to understand fundamental visual concepts. In contrast, simple nets trained directly for these tasks achieve high accuracy. The LAS&amp;T dataset has been made available."
      },
      {
        "id": "oai:arXiv.org:2503.24135v2",
        "title": "PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization",
        "link": "https://arxiv.org/abs/2503.24135",
        "author": "Alexis Guichemerre, Soufiane Belharbi, Mohammadhadi Shateri, Luke McCaffrey, Eric Granger",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24135v2 Announce Type: replace \nAbstract: Weakly supervised object localization (WSOL) methods allow training models to classify images and localize ROIs. WSOL only requires low-cost image-class annotations yet provides a visually interpretable classifier. Standard WSOL methods rely on class activation mapping (CAM) methods to produce spatial localization maps according to a single- or two-step strategy. While both strategies have made significant progress, they still face several limitations with histology images. Single-step methods can easily result in under- or over-activation due to the limited visual ROI saliency in histology images and scarce localization cues. They also face the well-known issue of asynchronous convergence between classification and localization tasks. The two-step approach is sub-optimal because it is constrained to a frozen classifier, limiting the capacity for localization. Moreover, these methods also struggle when applied to out-of-distribution (OOD) datasets. In this paper, a multi-task approach for WSOL is introduced for simultaneous training of both tasks to address the asynchronous convergence problem. In particular, localization is performed in the pixel-feature space of an image encoder that is shared with classification. This allows learning discriminant features and accurate delineation of foreground/background regions to support ROI localization and image classification. We propose PixelCAM, a cost-effective foreground/background pixel-wise classifier in the pixel-feature space that allows for spatial object localization. Using partial-cross entropy, PixelCAM is trained using pixel pseudo-labels collected from a pretrained WSOL model. Both image and pixel-wise classifiers are trained simultaneously using standard gradient descent. In addition, our pixel classifier can easily be integrated into CNN- and transformer-based architectures without any modifications."
      },
      {
        "id": "oai:arXiv.org:2504.00589v3",
        "title": "Efficient Annotator Reliability Assessment with EffiARA",
        "link": "https://arxiv.org/abs/2504.00589",
        "author": "Owen Cook, Jake Vasilakes, Ian Roberts, Xingyi Song",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00589v3 Announce Type: replace \nAbstract: Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework's efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft-label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk."
      },
      {
        "id": "oai:arXiv.org:2504.02433v2",
        "title": "OmniTalker: One-shot Real-time Text-Driven Talking Audio-Video Generation With Multimodal Style Mimicking",
        "link": "https://arxiv.org/abs/2504.02433",
        "author": "Zhongjian Wang, Peng Zhang, Jinwei Qi, Guangyuan Wang, Chaonan Ji, Sheng Xu, Bang Zhang, Liefeng Bo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02433v2 Announce Type: replace \nAbstract: Although significant progress has been made in audio-driven talking head generation, text-driven methods remain underexplored. In this work, we present OmniTalker, a unified framework that jointly generates synchronized talking audio-video content from input text while emulating the speaking and facial movement styles of the target identity, including speech characteristics, head motion, and facial dynamics. Our framework adopts a dual-branch diffusion transformer (DiT) architecture, with one branch dedicated to audio generation and the other to video synthesis. At the shallow layers, cross-modal fusion modules are introduced to integrate information between the two modalities. In deeper layers, each modality is processed independently, with the generated audio decoded by a vocoder and the video rendered using a GAN-based high-quality visual renderer. Leveraging the in-context learning capability of DiT through a masked-infilling strategy, our model can simultaneously capture both audio and visual styles without requiring explicit style extraction modules. Thanks to the efficiency of the DiT backbone and the optimized visual renderer, OmniTalker achieves real-time inference at 25 FPS. To the best of our knowledge, OmniTalker is the first one-shot framework capable of jointly modeling speech and facial styles in real time. Extensive experiments demonstrate its superiority over existing methods in terms of generation quality, particularly in preserving style consistency and ensuring precise audio-video synchronization, all while maintaining efficient inference."
      },
      {
        "id": "oai:arXiv.org:2504.04981v2",
        "title": "TestDG: Test-time Domain Generalization for Continual Test-time Adaptation",
        "link": "https://arxiv.org/abs/2504.04981",
        "author": "Sohyun Lee, Nayeong Kim, Juwon Kang, Seong Joon Oh, Suha Kwak",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04981v2 Announce Type: replace \nAbstract: This paper studies continual test-time adaptation (CTTA), the task of adapting a model to constantly changing unseen domains in testing while preserving previously learned knowledge. Existing CTTA methods mostly focus on adaptation to the current test domain only, overlooking generalization to arbitrary test domains a model may face in the future. To tackle this limitation, we present a novel online test-time domain generalization framework for CTTA, dubbed TestDG. TestDG aims to learn features invariant to both current and previous test domains on the fly during testing, improving the potential for effective generalization to future domains. To this end, we propose a new model architecture and a test-time adaptation strategy dedicated to learning domain-invariant features, along with a new data structure and optimization algorithm for effectively managing information from previous test domains. TestDG achieved state of the art on four public CTTA benchmarks. Moreover, it showed superior generalization to unseen test domains."
      },
      {
        "id": "oai:arXiv.org:2504.05050v4",
        "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models",
        "link": "https://arxiv.org/abs/2504.05050",
        "author": "Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05050v4 Announce Type: replace \nAbstract: Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible \"dark patterns\" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local \"safety regions\" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities."
      },
      {
        "id": "oai:arXiv.org:2504.07437v2",
        "title": "Unifying and extending Diffusion Models through PDEs for solving Inverse Problems",
        "link": "https://arxiv.org/abs/2504.07437",
        "author": "Agnimitra Dasgupta, Alexsander Marciano da Cunha, Ali Fardisi, Mehrnegar Aminy, Brianna Binder, Bryan Shaddy, Assad A Oberai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07437v2 Announce Type: replace \nAbstract: Diffusion models have emerged as powerful generative tools with applications in computer vision and scientific machine learning (SciML), where they have been used to solve large-scale probabilistic inverse problems. Traditionally, these models have been derived using principles of variational inference, denoising, statistical signal processing, and stochastic differential equations. In contrast to the conventional presentation, in this study we derive diffusion models using ideas from linear partial differential equations and demonstrate that this approach has several benefits that include a constructive derivation of the forward and reverse processes, a unified derivation of multiple formulations and sampling strategies, and the discovery of a new class of variance preserving models. We also apply the conditional version of these models to solve canonical conditional density estimation problems and challenging inverse problems. These problems help establish benchmarks for systematically quantifying the performance of different formulations and sampling strategies in this study and for future studies. Finally, we identify and implement a mechanism through which a single diffusion model can be applied to measurements obtained from multiple measurement operators. Taken together, the contents of this manuscript provide a new understanding of and several new directions in the application of diffusion models to solving physics-based inverse problems."
      },
      {
        "id": "oai:arXiv.org:2504.07744v2",
        "title": "MMLA: Multi-Environment, Multi-Species, Low-Altitude Drone Dataset",
        "link": "https://arxiv.org/abs/2504.07744",
        "author": "Jenna Kline, Samuel Stevens, Guy Maalouf, Camille Rondeau Saint-Jean, Dat Nguyen Ngoc, Majid Mirmehdi, David Guerin, Tilo Burghardt, Elzbieta Pastucha, Blair Costelloe, Matthew Watson, Thomas Richardson, Ulrik Pagh Schultz Lundquist",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07744v2 Announce Type: replace \nAbstract: Real-time wildlife detection in drone imagery supports critical ecological and conservation monitoring. However, standard detection models like YOLO often fail to generalize across locations and struggle with rare species, limiting their use in automated drone deployments. We present MMLA, a novel multi-environment, multi-species, low-altitude drone dataset collected across three sites (Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The Wilds in Ohio), featuring six species (zebras, giraffes, onagers, and African wild dogs). The dataset contains 811K annotations from 37 high-resolution videos. Baseline YOLO models show performance disparities across locations while fine-tuning YOLOv11m on MMLA improves mAP50 to 82%, a 52-point gain over baseline. Our results underscore the need for diverse training data to enable robust animal detection in autonomous drone systems."
      },
      {
        "id": "oai:arXiv.org:2504.08961v2",
        "title": "A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models",
        "link": "https://arxiv.org/abs/2504.08961",
        "author": "Kseniia Petukhova, Ekaterina Kochmar",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08961v2 Announce Type: replace \nAbstract: Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowledge. We propose a fully automated pipeline that uses LLMs to construct such schemes and perform annotation. We evaluate our approach on speech functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our experiments compare various design choices, and we show that frequency-guided decision trees, paired with an advanced LLM for annotation, can outperform previously manually designed trees and even match or surpass human annotators while significantly reducing the time required for annotation. We release all code and resultant schemes and annotations to facilitate future research on discourse annotation."
      },
      {
        "id": "oai:arXiv.org:2504.09941v2",
        "title": "FedRecon: Missing Modality Reconstruction in Heterogeneous Distributed Environments",
        "link": "https://arxiv.org/abs/2504.09941",
        "author": "Junming Liu, Guosun Zeng, Ding Wang, Yanting Gao, Yufei Jin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.09941v2 Announce Type: replace \nAbstract: Multimodal data are often incomplete and exhibit Non-Independent and Identically Distributed (Non-IID) characteristics in real-world scenarios. These inherent limitations lead to both modality heterogeneity through partial modality absence and data heterogeneity from distribution divergence, creating fundamental challenges for effective federated learning (FL). To address these coupled challenges, we propose FedRecon, the first method targeting simultaneous missing modality reconstruction and Non-IID adaptation in multimodal FL. Our approach first employs a lightweight Multimodal Variational Autoencoder (MVAE) to reconstruct missing modalities while preserving cross-modal consistency. Distinct from conventional imputation methods, we achieve sample-level alignment through a novel distribution mapping mechanism that guarantees both data consistency and completeness. Additionally, we introduce a strategy employing global generator freezing to prevent catastrophic forgetting, which in turn mitigates Non-IID fluctuations. Extensive evaluations on multimodal datasets demonstrate FedRecon's superior performance in modality reconstruction under Non-IID conditions, surpassing state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.11042v3",
        "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews",
        "link": "https://arxiv.org/abs/2504.11042",
        "author": "Sukannya Purkayastha, Zhuang Li, Anne Lauscher, Lizhen Qu, Iryna Gurevych",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.11042v3 Announce Type: replace \nAbstract: Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/acl2025-lazy-review)"
      },
      {
        "id": "oai:arXiv.org:2504.12216v2",
        "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.12216",
        "author": "Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.12216v2 Announce Type: replace \nAbstract: Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Our code is released at https://dllm-reasoning.github.io/."
      },
      {
        "id": "oai:arXiv.org:2504.13077v2",
        "title": "Effective Dual-Region Augmentation for Reduced Reliance on Large Amounts of Labeled Data",
        "link": "https://arxiv.org/abs/2504.13077",
        "author": "Prasanna Reddy Pulakurthi, Majid Rabbani, Celso M. de Melo, Sohail A. Dianat, Raghuveer M. Rao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13077v2 Announce Type: replace \nAbstract: This paper introduces a novel dual-region augmentation approach designed to reduce reliance on large-scale labeled datasets while improving model robustness and adaptability across diverse computer vision tasks, including source-free domain adaptation (SFDA) and person re-identification (ReID). Our method performs targeted data transformations by applying random noise perturbations to foreground objects and spatially shuffling background patches. This effectively increases the diversity of the training data, improving model robustness and generalization. Evaluations on the PACS dataset for SFDA demonstrate that our augmentation strategy consistently outperforms existing methods, achieving significant accuracy improvements in both single-target and multi-target adaptation settings. By augmenting training data through structured transformations, our method enables model generalization across domains, providing a scalable solution for reducing reliance on manually annotated datasets. Furthermore, experiments on Market-1501 and DukeMTMC-reID datasets validate the effectiveness of our approach for person ReID, surpassing traditional augmentation techniques. The code is available at https://github.com/PrasannaPulakurthi/Foreground-Background-Augmentation"
      },
      {
        "id": "oai:arXiv.org:2504.13932v2",
        "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining",
        "link": "https://arxiv.org/abs/2504.13932",
        "author": "Deyu Cao, Samin Aref",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13932v2 Announce Type: replace \nAbstract: The growing use of large language models has raised environmental and economic concerns about their intensity of resource usage during inference. Serving these models to each user requires substantial energy and water for cooling. Model compression techniques like quantization can shrink large language models and make them more resource efficient at the cost of potential performance degradation. Quantization methods compress model size through replacing their high-precision parameters by quantized values of lower precision. Among existing methods, the ApiQ method achieves superior accuracy preservation at minimal memory and time overhead. We investigate two ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we look into combining existing quantization-aware training techniques with ApiQ's partial training. We show that this does not outperform the baseline ApiQ method with limited training data and frozen weights. This leads to two key insights: (1) The substantial representational capacity that is gained through full retraining is unlikely to be feasible through partial training. (2) This gain may depend on using a large and diverse dataset in quantization-aware training. Second, through a novel approach informed by the two insights, we propose an ultra-low-bit quantization method that builds upon ApiQ and extends its performance without the need for full retraining. This publicly available method relies on a saliency-aware regularization term that prioritizes preserving the most impactful parameters during quantization. Our experiments on LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's accuracy degradation by 10.85\\% and 7.54\\% respectively."
      },
      {
        "id": "oai:arXiv.org:2504.15037v2",
        "title": "Scaling and Beyond: Advancing Spatial Reasoning in MLLMs Requires New Recipes",
        "link": "https://arxiv.org/abs/2504.15037",
        "author": "Huanyu Zhang, Chengzu Li, Wenshan Wu, Shaoguang Mao, Yifan Zhang, Haochen Tian, Ivan Vuli\\'c, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.15037v2 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in general vision-language tasks. However, recent studies have exposed critical limitations in their spatial reasoning capabilities. This deficiency in spatial reasoning significantly constrains MLLMs' ability to interact effectively with the physical world, thereby limiting their broader applications. We argue that spatial reasoning capabilities will not naturally emerge from merely scaling existing architectures and training methodologies. Instead, this challenge demands dedicated attention to fundamental modifications in the current MLLM development approach. In this position paper, we first establish a comprehensive framework for spatial reasoning within the context of MLLMs. We then elaborate on its pivotal role in real-world applications. Through systematic analysis, we examine how individual components of the current methodology, from training data to reasoning mechanisms, influence spatial reasoning capabilities. This examination reveals critical limitations while simultaneously identifying promising avenues for advancement. Our work aims to direct the AI research community's attention toward these crucial yet underexplored aspects. By highlighting these challenges and opportunities, we seek to catalyze progress toward achieving human-like spatial reasoning capabilities in MLLMs."
      },
      {
        "id": "oai:arXiv.org:2504.16580v3",
        "title": "Hyper-Transforming Latent Diffusion Models",
        "link": "https://arxiv.org/abs/2504.16580",
        "author": "Ignacio Peis, Batuhan Koyuncu, Isabel Valera, Jes Frellsen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.16580v3 Announce Type: replace \nAbstract: We introduce a novel generative framework for functions by integrating Implicit Neural Representations (INRs) and Transformer-based hypernetworks into latent variable models. Unlike prior approaches that rely on MLP-based hypernetworks with scalability limitations, our method employs a Transformer-based decoder to generate INR parameters from latent variables, addressing both representation capacity and computational efficiency. Our framework extends latent diffusion models (LDMs) to INR generation by replacing standard decoders with a Transformer-based hypernetwork, which can be trained either from scratch or via hyper-transforming: a strategy that fine-tunes only the decoder while freezing the pre-trained latent space. This enables efficient adaptation of existing generative models to INR-based representations without requiring full retraining. We validate our approach across multiple modalities, demonstrating improved scalability, expressiveness, and generalization over existing INR-based generative models. Our findings establish a unified and flexible framework for learning structured function representations."
      },
      {
        "id": "oai:arXiv.org:2504.16972v2",
        "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications",
        "link": "https://arxiv.org/abs/2504.16972",
        "author": "Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.16972v2 Announce Type: replace \nAbstract: The rapid growth of unlabeled time-series data in domains such as wireless communications, radar, biomedical engineering, and the Internet of Things (IoT) has driven advancements in unsupervised learning. This review synthesizes recent progress in applying autoencoders and vision transformers for unsupervised signal analysis, focusing on their architectures, applications, and emerging trends. We explore how these models enable feature extraction, anomaly detection, and classification across diverse signal types, including electrocardiograms, radar waveforms, and IoT sensor data. The review highlights the strengths of hybrid architectures and self-supervised learning, while identifying challenges in interpretability, scalability, and domain generalization. By bridging methodological innovations and practical applications, this work offers a roadmap for developing robust, adaptive models for signal intelligence."
      },
      {
        "id": "oai:arXiv.org:2504.19475v3",
        "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video",
        "link": "https://arxiv.org/abs/2504.19475",
        "author": "Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian Lapuschkin, Lee Sharkey, Blake Aaron Richards",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19475v3 Announce Type: replace \nAbstract: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field."
      },
      {
        "id": "oai:arXiv.org:2505.01660v2",
        "title": "Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification",
        "link": "https://arxiv.org/abs/2505.01660",
        "author": "Sicong Li, Qianqian Xu, Zhiyong Yang, Zitai Wang, Linchao Zhang, Xiaochun Cao, Qingming Huang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01660v2 Announce Type: replace \nAbstract: Real-world datasets often follow a long-tailed distribution, making generalization to tail classes difficult. Recent methods resorted to long-tail variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to improve generalization by flattening the loss landscape. However, these attempts face a trade-off between computational efficiency and control over the loss landscape. On the one hand, ImbSAM is efficient but offers only coarse control as it excludes head classes from the SAM process. On the other hand, CC-SAM provides fine-grained control through class-dependent perturbations but at the cost of efficiency due to multiple backpropagations. Seeing this dilemma, we introduce Focal-SAM, which assigns different penalties to class-wise sharpness, achieving fine-grained control without extra backpropagations, thus maintaining efficiency. Furthermore, we theoretically analyze Focal-SAM's generalization ability and derive a sharper generalization bound. Extensive experiments on both traditional and foundation models validate the effectiveness of Focal-SAM."
      },
      {
        "id": "oai:arXiv.org:2505.02862v2",
        "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
        "link": "https://arxiv.org/abs/2505.02862",
        "author": "Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02862v2 Announce Type: replace \nAbstract: Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies."
      },
      {
        "id": "oai:arXiv.org:2505.03320v2",
        "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation",
        "link": "https://arxiv.org/abs/2505.03320",
        "author": "Junyu Ma, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.03320v2 Announce Type: replace \nAbstract: Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's long-context performance against comparable Transformer/hybrid baselines under similar pretraining conditions, while preserving short-context capabilities, all without architectural changes."
      },
      {
        "id": "oai:arXiv.org:2505.03414v5",
        "title": "Enhancing Target-unspecific Tasks through a Features Matrix",
        "link": "https://arxiv.org/abs/2505.03414",
        "author": "Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.03414v5 Announce Type: replace \nAbstract: Recent developments in prompt learning of large Vision-Language Models (VLMs) have significantly improved performance in target-specific tasks. However, these prompting methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge. The general knowledge has a strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks (base-to-novel generalization, domain generalization, and cross-dataset generalization), achieving state-of-the-art performance."
      },
      {
        "id": "oai:arXiv.org:2505.03792v2",
        "title": "Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.03792",
        "author": "Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, Bo An",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.03792v2 Announce Type: replace \nAbstract: Online fine-tuning vision-language model (VLM) agents with reinforcement learning (RL) has shown promise for equipping agents with multi-step, goal-oriented capabilities in dynamic environments. However, their open-ended textual action space and non-end-to-end nature of action generation present significant challenges to effective online exploration in RL, e.g., explosion of the exploration space. We propose a novel online fine-tuning method, Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual output space of VLM agents. Compared to prior methods that assign uniform uncertainty to all tokens, CoSo leverages counterfactual reasoning to dynamically assess the causal influence of individual tokens on post-processed actions. By prioritizing the exploration of action-critical tokens while reducing the impact of semantically redundant or low-impact tokens, CoSo enables a more targeted and efficient online rollout process. We provide theoretical analysis proving CoSo's convergence and policy improvement guarantees, and extensive empirical evaluations supporting CoSo's effectiveness. Our results across a diverse set of agent tasks, including Android device control, card gaming, and embodied AI, highlight its remarkable ability to enhance exploration efficiency and deliver consistent performance gains. The code is available at https://github.com/langfengQ/CoSo."
      },
      {
        "id": "oai:arXiv.org:2505.04185v2",
        "title": "S3D: Sketch-Driven 3D Model Generation",
        "link": "https://arxiv.org/abs/2505.04185",
        "author": "Hail Song, Wonsik Shin, Naeun Lee, Soomin Chung, Nojun Kwak, Woontack Woo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.04185v2 Announce Type: replace \nAbstract: Generating high-quality 3D models from 2D sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. In this paper, we present S3D, a novel framework that converts simple hand-drawn sketches into detailed 3D models. Our method utilizes a U-Net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3D representation that can be rendered from novel views. To ensure robust consistency between the sketch domain and the 3D output, we introduce a novel style-alignment loss that aligns the U-Net bottleneck features with the initial encoder outputs of the 3D generation module, significantly enhancing reconstruction fidelity. To further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. This streamlined framework demonstrates the effectiveness of S3D in generating high-quality 3D models from sketch inputs. The source code for this project is publicly available at https://github.com/hailsong/S3D."
      },
      {
        "id": "oai:arXiv.org:2505.04560v3",
        "title": "ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $\\alpha$-$\\beta$-Divergence",
        "link": "https://arxiv.org/abs/2505.04560",
        "author": "Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qianqian Xu, Qingming Huang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.04560v3 Announce Type: replace \nAbstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student model by minimizing the divergence between their output distributions, typically using forward Kullback-Leibler divergence (FKLD) or reverse KLD (RKLD). It has become an effective training paradigm due to the broader supervision information provided by the teacher distribution compared to one-hot labels. We identify that the core challenge in KD lies in balancing two mode-concentration effects: the \\textbf{\\textit{Hardness-Concentration}} effect, which refers to focusing on modes with large errors, and the \\textbf{\\textit{Confidence-Concentration}} effect, which refers to focusing on modes with high student confidence. Through an analysis of how probabilities are reassigned during gradient updates, we observe that these two effects are entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too weak in FKLD, causing the student to fail to concentrate on the target class. In contrast, both are too strong in RKLD, causing the student to overly emphasize the target class while ignoring the broader distributional information from the teacher. To address this imbalance, we propose ABKD, a generic framework with $\\alpha$-$\\beta$-divergence. Our theoretical results show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving an effective trade-off between these effects. Extensive experiments on 17 language/vision datasets with 12 teacher-student settings confirm its efficacy. The code is available at https://github.com/ghwang-s/abkd."
      },
      {
        "id": "oai:arXiv.org:2505.06150v2",
        "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets",
        "link": "https://arxiv.org/abs/2505.06150",
        "author": "Ryan Lagasse, Aidan Kierans, Avijit Ghosh, Shiri Dori-Hacohen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.06150v2 Announce Type: replace \nAbstract: We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \\emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \\cite{salavati2024reducing} and subsets of the MMLU dataset \\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings."
      },
      {
        "id": "oai:arXiv.org:2505.06262v2",
        "title": "Dialz: A Python Toolkit for Steering Vectors",
        "link": "https://arxiv.org/abs/2505.06262",
        "author": "Zara Siddique, Liam D. Turner, Luis Espinosa-Anke",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.06262v2 Announce Type: replace \nAbstract: We introduce Dialz, a framework for advancing research on steering vectors for open-source LLMs, implemented in Python. Steering vectors allow users to modify activations at inference time to amplify or weaken a 'concept', e.g. honesty or positivity, providing a more powerful alternative to prompting or fine-tuning. Dialz supports a diverse set of tasks, including creating contrastive pair datasets, computing and applying steering vectors, and visualizations. Unlike existing libraries, Dialz emphasizes modularity and usability, enabling both rapid prototyping and in-depth analysis. We demonstrate how Dialz can be used to reduce harmful outputs such as stereotypes, while also providing insights into model behaviour across different layers. We release Dialz with full documentation, tutorials, and support for popular open-source models to encourage further research in safe and controllable language generation. Dialz enables faster research cycles and facilitates insights into model interpretability, paving the way for safer, more transparent, and more reliable AI systems."
      },
      {
        "id": "oai:arXiv.org:2505.06892v2",
        "title": "Learning Soft Sparse Shapes for Efficient Time-Series Classification",
        "link": "https://arxiv.org/abs/2505.06892",
        "author": "Zhen Liu, Yicheng Luo, Boyuan Li, Emadeldeen Eldele, Min Wu, Qianli Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.06892v2 Announce Type: replace \nAbstract: Shapelets are discriminative subsequences (or shapes) with high interpretability in time series classification. Due to the time-intensive nature of shapelet discovery, existing shapelet-based methods mainly focus on selecting discriminative shapes while discarding others to achieve candidate subsequence sparsification. However, this approach may exclude beneficial shapes and overlook the varying contributions of shapelets to classification performance. To this end, we propose a Soft sparse Shapes (SoftShape) model for efficient time series classification. Our approach mainly introduces soft shape sparsification and soft shape learning blocks. The former transforms shapes into soft representations based on classification contribution scores, merging lower-scored ones into a single shape to retain and differentiate all subsequence information. The latter facilitates intra- and inter-shape temporal pattern learning, improving model efficiency by using sparsified soft shapes as inputs. Specifically, we employ a learnable router to activate a subset of class-specific expert networks for intra-shape pattern learning. Meanwhile, a shared expert network learns inter-shape patterns by converting sparsified shapes into sequences. Extensive experiments show that SoftShape outperforms state-of-the-art methods and produces interpretable results."
      },
      {
        "id": "oai:arXiv.org:2505.07320v2",
        "title": "Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records",
        "link": "https://arxiv.org/abs/2505.07320",
        "author": "Yuhao Li, Ling Luo, Uwe Aickelin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.07320v2 Announce Type: replace \nAbstract: Medical research, particularly in predicting patient outcomes, heavily relies on medical time series data extracted from Electronic Health Records (EHR), which provide extensive information on patient histories. Despite rigorous examination, labeling errors are inevitable and can significantly impede accurate predictions of patient outcome. To address this challenge, we propose an \\textbf{A}ttention-based Learning Framework with Dynamic \\textbf{C}alibration and Augmentation for \\textbf{T}ime series Noisy \\textbf{L}abel \\textbf{L}earning (ACTLL). This framework leverages a two-component Beta mixture model to identify the certain and uncertain sets of instances based on the fitness distribution of each class, and it captures global temporal dynamics while dynamically calibrating labels from the uncertain set or augmenting confident instances from the certain set. Experimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and several benchmark datasets from the UCR and UEA repositories, demonstrate that our model ACTLL has achieved state-of-the-art performance, especially under high noise levels."
      },
      {
        "id": "oai:arXiv.org:2505.07783v3",
        "title": "Relative Overfitting and Accept-Reject Framework",
        "link": "https://arxiv.org/abs/2505.07783",
        "author": "Yanxin Liu, Yunqi Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.07783v3 Announce Type: replace \nAbstract: Currently, the scaling law of Large Language Models (LLMs) faces challenges and bottlenecks. This paper posits that noise effects, stemming from changes in the signal-to-noise ratio under diminishing marginal returns, are the root cause of these issues. To control this noise, we investigated the differences between models with performance advantages and disadvantages, introducing the concept of \"relative overfitting.\" Based on their complementary strengths, we have proposed an application framework, Accept-Reject (AR), and the associated AR Law, which operates within this framework to elucidate the patterns of performance changes after model integration. In Natural Language Processing (NLP), we use LLMs and Small Language Models (SLMs) as the medium for discussion. This framework enables SLMs to exert a universal positive influence on LLM decision outputs, rather than the intuitively expected potential negative influence. We validated our approach using self-built models based on mainstream architectures and pre-trained mainstream models across multiple datasets, including basic language modeling, long-context tasks, subject examination, and question-answering (QA) benchmarks. The results demonstrate that through our framework, compared to increasing the LLM's parameters, we can achieve better performance improvements with significantly lower parameter and computational costs in many scenarios. These improvements are universal, stable, and effective. Furthermore, we explore the potential of \"relative overfitting\" and the AR framework in other machine learning domains, such as computer vision (CV) and AI for science. We hope the proposed approach can help scale laws overcome existing bottlenecks."
      },
      {
        "id": "oai:arXiv.org:2505.09825v2",
        "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning",
        "link": "https://arxiv.org/abs/2505.09825",
        "author": "Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.09825v2 Announce Type: replace \nAbstract: Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, in which they gather textual details to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs may seem to understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that, while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks."
      },
      {
        "id": "oai:arXiv.org:2505.11282v2",
        "title": "MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection",
        "link": "https://arxiv.org/abs/2505.11282",
        "author": "Shrutarv Awasthi, Anas Gouda, Sven Franke, J\\'er\\^ome Rutinowski, Frank Hoffmann, Moritz Roidl",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.11282v2 Announce Type: replace \nAbstract: Mobile robots are reaching unprecedented speeds, with platforms like Unitree B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s. However, effectively utilizing such speeds remains a challenge due to the limitations of RGB cameras, which suffer from motion blur and fail to provide real-time responsiveness. Event cameras, with their asynchronous operation, and low-latency sensing, offer a promising alternative for high-speed robotic perception. In this work, we introduce MTevent, a dataset designed for 6D pose estimation and moving object detection in highly dynamic environments with large detection distances. Our setup consists of a stereo-event camera and an RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16 unique objects under challenging conditions such as extreme viewing angles, varying lighting, and occlusions. MTevent is the first dataset to combine high-speed motion, long-range perception, and real-world object interactions, making it a valuable resource for advancing event-based vision in robotics. To establish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's FoundationPose on RGB images, achieving an Average Recall of 0.22 with ground-truth masks, highlighting the limitations of RGB-based approaches in such dynamic settings. With MTevent, we provide a novel resource to improve perception models and foster further research in high-speed robotic vision. The dataset is available for download https://huggingface.co/datasets/anas-gouda/MTevent"
      },
      {
        "id": "oai:arXiv.org:2505.13338v2",
        "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation",
        "link": "https://arxiv.org/abs/2505.13338",
        "author": "Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.13338v2 Announce Type: replace \nAbstract: Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities."
      },
      {
        "id": "oai:arXiv.org:2505.13405v3",
        "title": "A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut",
        "link": "https://arxiv.org/abs/2505.13405",
        "author": "Gabriel Malikal, Ismail Alkhouri, Alvaro Velasquez, Adam M Alessio, Saiprasad Ravishankar",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.13405v3 Announce Type: replace \nAbstract: The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal solution is NP-hard in the worst case. As a result, heuristic-based algorithms are commonly used, though their design often requires significant domain expertise. More recently, learning-based methods trained on large (un)labeled datasets have been proposed; however, these approaches often struggle with generalizability and scalability. A well-known approximation algorithm for MaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic Unconstrained Binary Optimization (QUBO) formulation into a semidefinite program (SDP). The GW algorithm then applies hyperplane rounding by uniformly sampling a random hyperplane to convert the SDP solution into binary node assignments. In this paper, we propose a training-data-free approach based on a non-episodic reinforcement learning formulation, in which an agent learns to select improved rounding hyperplanes that yield better cuts than those produced by the GW algorithm. By optimizing over a Markov Decision Process (MDP), our method consistently achieves better cuts across large-scale graphs with varying densities and degree distributions."
      },
      {
        "id": "oai:arXiv.org:2505.13508v2",
        "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
        "link": "https://arxiv.org/abs/2505.13508",
        "author": "Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.13508v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \\textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \\textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \\textit{Time-R1} checkpoints."
      },
      {
        "id": "oai:arXiv.org:2505.15173v2",
        "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection",
        "link": "https://arxiv.org/abs/2505.15173",
        "author": "Zhipei Xu, Xuanyu Zhang, Xing Zhou, Jian Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.15173v2 Announce Type: replace \nAbstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, particularly in video generation, has led to unprecedented creative capabilities but also increased threats to information integrity, identity security, and public trust. Existing detection methods, while effective in general scenarios, lack robust solutions for human-centric videos, which pose greater risks due to their realism and potential for legal and ethical misuse. Moreover, current detection approaches often suffer from poor generalization, limited scalability, and reliance on labor-intensive supervised fine-tuning. To address these challenges, we propose AvatarShield, the first interpretable MLLM-based framework for detecting human-centric fake videos, enhanced via Group Relative Policy Optimization (GRPO). Through our carefully designed accuracy detection reward and temporal compensation reward, it effectively avoids the use of high-cost text annotation data, enabling precise temporal modeling and forgery detection. Meanwhile, we design a dual-encoder architecture, combining high-level semantic reasoning and low-level artifact amplification to guide MLLMs in effective forgery detection. We further collect FakeHumanVid, a large-scale human-centric video benchmark that includes synthesis methods guided by pose, audio, and text inputs, enabling rigorous evaluation of detection methods in real-world scenes. Extensive experiments show that AvatarShield significantly outperforms existing approaches in both in-domain and cross-domain detection, setting a new standard for human-centric video forensics."
      },
      {
        "id": "oai:arXiv.org:2505.15299v2",
        "title": "Multi-Hop Question Generation via Dual-Perspective Keyword Guidance",
        "link": "https://arxiv.org/abs/2505.15299",
        "author": "Maodong Li, Longyin Zhang, Fang Kong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.15299v2 Announce Type: replace \nAbstract: Multi-hop question generation (MQG) aims to generate questions that require synthesizing multiple information snippets from documents to derive target answers. The primary challenge lies in effectively pinpointing crucial information snippets related to question-answer (QA) pairs, typically relying on keywords. However, existing works fail to fully utilize the guiding potential of keywords and neglect to differentiate the distinct roles of question-specific and document-specific keywords. To address this, we define dual-perspective keywords (i.e., question and document keywords) and propose a Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates keywords into the multi-hop question generation process. We argue that question keywords capture the questioner's intent, whereas document keywords reflect the content related to the QA pair. Functionally, question and document keywords work together to pinpoint essential information snippets in the document, with question keywords required to appear in the generated question. The DPKG framework consists of an expanded transformer encoder and two answer-aware transformer decoders for keyword and question generation, respectively. Extensive experiments demonstrate the effectiveness of our work, showcasing its promising performance and underscoring its significant value in the MQG task."
      },
      {
        "id": "oai:arXiv.org:2505.16014v3",
        "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains",
        "link": "https://arxiv.org/abs/2505.16014",
        "author": "Yash Saxena, Ankur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16014v3 Announce Type: replace \nAbstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on similarity-based retrieval and re-ranking, which depend on heuristics such as top-k, and lack explainability, interpretability, and robustness against adversarial content. To address this gap, we propose a novel method METEORA that replaces re-ranking in RAG with a rationale-driven selection approach. METEORA operates in two stages. First, a general-purpose LLM is preference-tuned to generate rationales conditioned on the input query using direct preference optimization. These rationales guide the evidence chunk selection engine, which selects relevant chunks in three stages: pairing individual rationales with corresponding retrieved chunks for local relevance, global selection with elbow detection for adaptive cutoff, and context expansion via neighboring chunks. This process eliminates the need for top-k heuristics. The rationales are also used for consistency check using a Verifier LLM to detect and filter poisoned or misleading content for safe generation. The framework provides explainable and interpretable evidence flow by using rationales consistently across both selection and verification. Our evaluation across six datasets spanning legal, financial, and academic research domains shows that METEORA improves generation accuracy by 33.34% while using approximately 50% fewer chunks than state-of-the-art re-ranking methods. In adversarial settings, METEORA significantly improves the F1 score from 0.10 to 0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating strong resilience to poisoning attacks. Code available at: https://anonymous.4open.science/r/METEORA-DC46/README.md"
      },
      {
        "id": "oai:arXiv.org:2505.16452v2",
        "title": "CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI",
        "link": "https://arxiv.org/abs/2505.16452",
        "author": "Mohamed S. Elmahdy, Marius Staring, Patrick J. H. de Koning, Samer Alabed, Mahan Salehi, Faisal Alandejani, Michael Sharkey, Ziad Aldabbagh, Andrew J. Swift, Rob J. van der Geest",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16452v2 Announce Type: replace \nAbstract: Accurate and efficient quantification of cardiac function is essential for the estimation of prognosis of cardiovascular diseases (CVDs). One of the most commonly used metrics for evaluating cardiac pumping performance is left ventricular ejection fraction (LVEF). However, LVEF can be affected by factors such as inter-observer variability and varying pre-load and after-load conditions, which can reduce its reproducibility. Additionally, cardiac dysfunction may not always manifest as alterations in LVEF, such as in heart failure and cardiotoxicity diseases. An alternative measure that can provide a relatively load-independent quantitative assessment of myocardial contractility is myocardial strain and strain rate. By using LVEF in combination with myocardial strain, it is possible to obtain a thorough description of cardiac function. Automated estimation of LVEF and other volumetric measures from cine-MRI sequences can be achieved through segmentation models, while strain calculation requires the estimation of tissue displacement between sequential frames, which can be accomplished using registration models. These tasks are often performed separately, potentially limiting the assessment of cardiac function. To address this issue, in this study we propose an end-to-end deep learning (DL) model that jointly estimates groupwise (GW) registration and segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep GW network was trained and validated on a large dataset of 4-chamber view cine-MRI image series of 374 subjects. A quantitative comparison with conventional GW registration using elastix and two DL-based methods showed that the proposed model improved performance and substantially reduced computation time."
      },
      {
        "id": "oai:arXiv.org:2505.16512v4",
        "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection",
        "link": "https://arxiv.org/abs/2505.16512",
        "author": "Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Long Ma, Renwang Pei, Zhaofeng He",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16512v4 Announce Type: replace \nAbstract: In recent years, the explosive advancement of deepfake technology has posed a critical and escalating threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency via multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Leveraging five of the latest digital human generation methods and a voice cloning method, we systematically construct a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that the misrecognition rate by participants for DigiFakeAV reaches as high as 68%. Moreover, the substantial performance degradation of existing detection models on our dataset further highlights its challenges. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets."
      },
      {
        "id": "oai:arXiv.org:2505.16552v4",
        "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains",
        "link": "https://arxiv.org/abs/2505.16552",
        "author": "Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16552v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2505.16583v4",
        "title": "Training on Plausible Counterfactuals Removes Spurious Correlations",
        "link": "https://arxiv.org/abs/2505.16583",
        "author": "Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16583v4 Announce Type: replace \nAbstract: Plausible counterfactual explanations (p-CFEs) are perturbations that minimally modify inputs to change classifier decisions while remaining plausible under the data distribution. In this study, we demonstrate that classifiers can be trained on p-CFEs labeled with induced \\emph{incorrect} target classes to classify unperturbed inputs with the original labels. While previous studies have shown that such learning is possible with adversarial perturbations, we extend this paradigm to p-CFEs. Interestingly, our experiments reveal that learning from p-CFEs is even more effective: the resulting classifiers achieve not only high in-distribution accuracy but also exhibit significantly reduced bias with respect to spurious correlations."
      },
      {
        "id": "oai:arXiv.org:2505.16660v2",
        "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu",
        "link": "https://arxiv.org/abs/2505.16660",
        "author": "Liu Chang, Wang Dongbo, Liu liu, Zhao Zhixiao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16660v2 Announce Type: replace \nAbstract: This study addresses the challenges in intelligent processing of Chinese ancient mathematical classics by constructing Guji_MATH, a benchmark for evaluating classical texts based on Suanjing Shishu. It systematically assesses the mathematical problem-solving capabilities of mainstream reasoning models under the unique linguistic constraints of classical Chinese. Through machine-assisted annotation and manual verification, 538 mathematical problems were extracted from 8 canonical texts, forming a structured dataset centered on the \"Question-Answer-Solution\" framework, supplemented by problem types and difficulty levels. Dual evaluation modes--closed-book (autonomous problem-solving) and open-book (reproducing classical solution methods)--were designed to evaluate the performance of six reasoning models on ancient Chinese mathematical problems. Results indicate that reasoning models can partially comprehend and solve these problems, yet their overall performance remains inferior to benchmarks on modern mathematical tasks. Enhancing models' classical Chinese comprehension and cultural knowledge should be prioritized for optimization. This study provides methodological support for mining mathematical knowledge from ancient texts and disseminating traditional culture, while offering new perspectives for evaluating cross-linguistic and cross-cultural capabilities of reasoning models."
      },
      {
        "id": "oai:arXiv.org:2505.16932v2",
        "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm",
        "link": "https://arxiv.org/abs/2505.16932",
        "author": "Noah Amsel, David Persson, Christopher Musco, Robert M. Gower",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16932v2 Announce Type: replace \nAbstract: Computing the polar decomposition and the related matrix sign function, has been a well-studied problem in numerical analysis for decades. More recently, it has emerged as an important subroutine in deep learning, particularly within the Muon optimization framework. However, the requirements in this setting differ significantly from those of traditional numerical analysis. In deep learning, methods must be highly efficient and GPU-compatible, but high accuracy is often unnecessary. As a result, classical algorithms like Newton-Schulz (which suffers from slow initial convergence) and methods based on rational functions (which rely on QR decompositions or matrix inverses) are poorly suited to this context. In this work, we introduce Polar Express, a GPU-friendly algorithm for computing the polar decomposition. Like classical polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix multiplications, making it GPU-compatible. Motivated by earlier work of Chen & Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule at each iteration by solving a minimax optimization problem, and we prove that it enjoys a strong worst-case optimality guarantee. This property ensures both rapid early convergence and fast asymptotic convergence. We also address finite-precision issues, making it stable in bfloat16 in practice. We apply Polar Express within the Muon optimization framework and show consistent improvements in validation loss on large-scale models such as GPT-2, outperforming recent alternatives across a range of learning rates."
      },
      {
        "id": "oai:arXiv.org:2505.17134v2",
        "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions",
        "link": "https://arxiv.org/abs/2505.17134",
        "author": "Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17134v2 Announce Type: replace \nAbstract: High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis."
      },
      {
        "id": "oai:arXiv.org:2505.17257v2",
        "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model",
        "link": "https://arxiv.org/abs/2505.17257",
        "author": "Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17257v2 Announce Type: replace \nAbstract: Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA"
      },
      {
        "id": "oai:arXiv.org:2505.18233v2",
        "title": "POSTER: A Multi-Signal Model for Detecting Evasive Smishing",
        "link": "https://arxiv.org/abs/2505.18233",
        "author": "Shaghayegh Hosseinpour, Sanchari Das",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.18233v2 Announce Type: replace \nAbstract: Smishing, or SMS-based phishing, poses an increasing threat to mobile users by mimicking legitimate communications through culturally adapted, concise, and deceptive messages, which can result in the loss of sensitive data or financial resources. In such, we present a multi-channel smishing detection model that combines country-specific semantic tagging, structural pattern tagging, character-level stylistic cues, and contextual phrase embeddings. We curated and relabeled over 84,000 messages across five datasets, including 24,086 smishing samples. Our unified architecture achieves 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream models by capturing diverse linguistic and structural cues. This work demonstrates the effectiveness of multi-signal learning in robust and region-aware phishing."
      },
      {
        "id": "oai:arXiv.org:2505.18499v2",
        "title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.18499",
        "author": "Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, Yisen Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.18499v2 Announce Type: replace \nAbstract: Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erd\\~os, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erd\\~os, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully. Our implementation is open-sourced at https://github.com/PKU-ML/G1, with models and datasets hosted on Hugging Face collections https://huggingface.co/collections/PKU-ML/g1-683d659e992794fc99618cf2 for broader accessibility."
      },
      {
        "id": "oai:arXiv.org:2505.18924v2",
        "title": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud Active Learning",
        "link": "https://arxiv.org/abs/2505.18924",
        "author": "Chenxi Li, Nuo Chen, Fengyun Tan, Yantong Chen, Bochun Yuan, Tianrui Li, Chongshou Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.18924v2 Announce Type: replace \nAbstract: We present a novel active learning framework for 3D point cloud semantic segmentation that, for the first time, integrates large language models (LLMs) to construct hierarchical label structures and guide uncertainty-based sample selection. Unlike prior methods that treat labels as flat and independent, our approach leverages LLM prompting to automatically generate multi-level semantic taxonomies and introduces a recursive uncertainty projection mechanism that propagates uncertainty across hierarchy levels. This enables spatially diverse, label-aware point selection that respects the inherent semantic structure of 3D scenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to 4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%), substantially outperforming existing baselines. Our results highlight the untapped potential of LLMs as knowledge priors in 3D vision and establish hierarchical uncertainty modeling as a powerful paradigm for efficient point cloud annotation."
      },
      {
        "id": "oai:arXiv.org:2505.19028v3",
        "title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts",
        "link": "https://arxiv.org/abs/2505.19028",
        "author": "Minzhi Lin, Tianchi Xie, Mengchen Liu, Yilin Ye, Changjian Chen, Shixia Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19028v3 Announce Type: replace \nAbstract: Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabilities of MLLMs due to the lack of paired plain charts and visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a benchmark for evaluating MLLMs on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations. We further design visual-element-based questions to capture their unique visual designs and communicative intent. Evaluation of 20 MLLMs reveals a substantial performance decline on infographic charts, particularly for visual-element-based questions related to metaphors. The paired infographic and plain charts enable fine-grained error analysis and ablation studies, which highlight new opportunities for advancing MLLMs in infographic chart understanding. We release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA."
      },
      {
        "id": "oai:arXiv.org:2505.19746v2",
        "title": "Improving Heart Rejection Detection in XPCI Images Using Synthetic Data Augmentation",
        "link": "https://arxiv.org/abs/2505.19746",
        "author": "Jakov Samard\\v{z}ija, Donik Vr\\v{s}nak, Sven Lon\\v{c}ari\\'c",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19746v2 Announce Type: replace \nAbstract: Accurate identification of acute cellular rejection (ACR) in endomyocardial biopsies is essential for effective management of heart transplant patients. However, the rarity of high-grade rejection cases (3R) presents a significant challenge for training robust deep learning models. This work addresses the class imbalance problem by leveraging synthetic data generation using StyleGAN to augment the limited number of real 3R images. Prior to GAN training, histogram equalization was applied to standardize image appearance and improve the consistency of tissue representation. StyleGAN was trained on available 3R biopsy patches and subsequently used to generate 10,000 realistic synthetic images. These were combined with real 0R samples, that is samples without rejection, in various configurations to train ResNet-18 classifiers for binary rejection classification.\n  Three classifier variants were evaluated: one trained on real 0R and synthetic 3R images, another using both synthetic and additional real samples, and a third trained solely on real data. All models were tested on an independent set of real biopsy images. Results demonstrate that synthetic data improves classification performance, particularly when used in combination with real samples. The highest-performing model, which used both real and synthetic images, achieved strong precision and recall for both classes. These findings underscore the value of hybrid training strategies and highlight the potential of GAN-based data augmentation in biomedical image analysis, especially in domains constrained by limited annotated datasets."
      },
      {
        "id": "oai:arXiv.org:2505.19901v3",
        "title": "Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM",
        "link": "https://arxiv.org/abs/2505.19901",
        "author": "Peng Liu, Xiaoming Ren, Fengkai Liu, Qingsong Xie, Quanlong Zheng, Yanhao Zhang, Haonan Lu, Yujiu Yang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19901v3 Announce Type: replace \nAbstract: Recent advancements in image-to-video (I2V) generation have shown promising performance in conventional scenarios. However, these methods still encounter significant challenges when dealing with complex scenes that require a deep understanding of nuanced motion and intricate object-action relationships. To address these challenges, we present Dynamic-I2V, an innovative framework that integrates Multimodal Large Language Models (MLLMs) to jointly encode visual and textual conditions for a diffusion transformer (DiT) architecture. By leveraging the advanced multimodal understanding capabilities of MLLMs, our model significantly improves motion controllability and temporal coherence in synthesized videos. The inherent multimodality of Dynamic-I2V further enables flexible support for diverse conditional inputs, extending its applicability to various downstream generation tasks. Through systematic analysis, we identify a critical limitation in current I2V benchmarks: a significant bias towards favoring low-dynamic videos, stemming from an inadequate balance between motion complexity and visual quality metrics. To resolve this evaluation gap, we propose DIVE - a novel assessment benchmark specifically designed for comprehensive dynamic quality measurement in I2V generation. In conclusion, extensive quantitative and qualitative experiments confirm that Dynamic-I2V attains state-of-the-art performance in image-to-video generation, particularly revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range, controllability, and quality, respectively, as assessed by the DIVE metric in comparison to existing methods."
      },
      {
        "id": "oai:arXiv.org:2505.20015v2",
        "title": "On the class of coding optimality of human languages and the origins of Zipf's law",
        "link": "https://arxiv.org/abs/2505.20015",
        "author": "Ramon Ferrer-i-Cancho",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20015v2 Announce Type: replace \nAbstract: Here we present a new class of optimality for coding systems. Members of that class are displaced linearly from optimal coding and thus exhibit Zipf's law, namely a power-law distribution of frequency ranks. Within that class, Zipf's law, the size-rank law and the size-probability law form a group-like structure. We identify human languages that are members of the class. All languages showing sufficient agreement with Zipf's law are potential members of the class. In contrast, there are communication systems in other species that cannot be members of that class for exhibiting an exponential distribution instead but dolphins and humpback whales might. We provide a new insight into plots of frequency versus rank in double logarithmic scale. For any system, a straight line in that scale indicates that the lengths of optimal codes under non-singular coding and under uniquely decodable encoding are displaced by a linear function whose slope is the exponent of Zipf's law. For systems under compression and constrained to be uniquely decodable, such a straight line may indicate that the system is coding close to optimality. Our findings provide support for the hypothesis that Zipf's law originates from compression."
      },
      {
        "id": "oai:arXiv.org:2505.20156v2",
        "title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters",
        "link": "https://arxiv.org/abs/2505.20156",
        "author": "Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, Qinglin Lu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20156v2 Announce Type: replace \nAbstract: Recent years have witnessed significant progress in audio-driven human animation. However, critical challenges remain in (i) generating highly dynamic videos while preserving character consistency, (ii) achieving precise emotion alignment between characters and audio, and (iii) enabling multi-character audio-driven animation. To address these challenges, we propose HunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model capable of simultaneously generating dynamic, emotion-controllable, and multi-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces three key innovations: (i) A character image injection module is designed to replace the conventional addition-based character conditioning scheme, eliminating the inherent condition mismatch between training and inference. This ensures the dynamic motion and strong character consistency; (ii) An Audio Emotion Module (AEM) is introduced to extract and transfer the emotional cues from an emotion reference image to the target generated video, enabling fine-grained and accurate emotion style control; (iii) A Face-Aware Audio Adapter (FAA) is proposed to isolate the audio-driven character with latent-level face mask, enabling independent audio injection via cross-attention for multi-character scenarios. These innovations empower HunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets and a newly proposed wild dataset, generating realistic avatars in dynamic, immersive scenarios."
      },
      {
        "id": "oai:arXiv.org:2505.20282v3",
        "title": "One-shot Entropy Minimization",
        "link": "https://arxiv.org/abs/2505.20282",
        "author": "Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20282v3 Announce Type: replace \nAbstract: We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em."
      },
      {
        "id": "oai:arXiv.org:2505.20292v4",
        "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation",
        "link": "https://arxiv.org/abs/2505.20292",
        "author": "Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, Li Yuan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20292v4 Announce Type: replace \nAbstract: Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 18 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research."
      },
      {
        "id": "oai:arXiv.org:2505.20322v2",
        "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
        "link": "https://arxiv.org/abs/2505.20322",
        "author": "Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20322v2 Announce Type: replace \nAbstract: Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control."
      },
      {
        "id": "oai:arXiv.org:2505.20697v2",
        "title": "Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series",
        "link": "https://arxiv.org/abs/2505.20697",
        "author": "Zachary C. Brown, David Carlson",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20697v2 Announce Type: replace \nAbstract: The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics."
      },
      {
        "id": "oai:arXiv.org:2505.21179v3",
        "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models",
        "link": "https://arxiv.org/abs/2505.21179",
        "author": "Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, Yi-Zhe Song",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21179v3 Announce Type: replace \nAbstract: Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion models, particularly in few-step sampling regimes. While Classifier-Free Guidance (CFG) works well in standard settings, it fails under aggressive sampling step compression due to divergent predictions between positive and negative branches. We present Normalized Attention Guidance (NAG), an efficient, training-free mechanism that applies extrapolation in attention space with L1-based normalization and refinement. NAG restores effective negative guidance where CFG collapses while maintaining fidelity. Unlike existing approaches, NAG generalizes across architectures (UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image, video), functioning as a \\textit{universal} plug-in with minimal computational overhead. Through extensive experimentation, we demonstrate consistent improvements in text alignment (CLIP Score), fidelity (FID, PFID), and human-perceived quality (ImageReward). Our ablation studies validate each design component, while user studies confirm significant preference for NAG-guided outputs. As a model-agnostic inference-time approach requiring no retraining, NAG provides effortless negative guidance for all modern diffusion frameworks -- pseudocode in the Appendix!"
      },
      {
        "id": "oai:arXiv.org:2505.21226v2",
        "title": "Why Do More Experts Fail? A Theoretical Analysis of Model Merging",
        "link": "https://arxiv.org/abs/2505.21226",
        "author": "Zijing Wang, Xingle Xu, Yongkang Liu, Yiqun Zhang, Peiqin Lin, Shi Feng, Xiaocui Yang, Daling Wang, Hinrich Sch\\\"utze",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21226v2 Announce Type: replace \nAbstract: Model merging dramatically reduces storage and computational resources by combining multiple expert models into a single multi-task model. Although recent model merging methods have shown promising results, they struggle to maintain performance gains as the number of merged models increases. In this paper, we investigate the key obstacles that limit the scalability of model merging when integrating a large number of expert models. First, we prove that there is an upper bound on model merging. Further theoretical analysis reveals that the limited effective parameter space imposes a strict constraint on the number of models that can be successfully merged. Gaussian Width shows that the marginal benefit of merging additional models diminishes according to a strictly concave function. This implies that the effective parameter space becomes rapidly saturated as the number of merged models increases. Furthermore, using Approximate Kinematics Theory, we prove the existence of a unique optimal threshold beyond which adding more models does not yield significant performance improvements. At the same time, we introduce a straightforward Reparameterized Heavy-Tailed method (RHT) to extend the coverage of the merged model, thereby enhancing its performance. Empirical results on 12 benchmarks, including both knowledge-intensive and general-purpose tasks, validate our theoretical analysis. We believe that these results spark further research beyond the current scope of model merging. The source code is in the Github repository: https://github.com/wzj1718/ModelMergingAnalysis."
      },
      {
        "id": "oai:arXiv.org:2505.21863v3",
        "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning",
        "link": "https://arxiv.org/abs/2505.21863",
        "author": "Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21863v3 Announce Type: replace \nAbstract: Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image's significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context."
      },
      {
        "id": "oai:arXiv.org:2505.21920v2",
        "title": "InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective",
        "link": "https://arxiv.org/abs/2505.21920",
        "author": "Yuanhong Zhang, Muyao Yuan, Weizhan Zhang, Tieliang Gong, Wen Wen, Jiangyong Ying, Weijie Shi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21920v2 Announce Type: replace \nAbstract: The Segment Anything Model (SAM), a vision foundation model, exhibits impressive zero-shot capabilities in general tasks but struggles in specialized domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to unleash the potential of SAM in novel scenarios. However, existing PEFT methods for SAM neglect the domain-invariant relations encoded in the pre-trained model. To bridge this gap, we propose InfoSAM, an information-theoretic approach that enhances SAM fine-tuning by distilling and preserving its pre-trained segmentation knowledge. Specifically, we formulate the knowledge transfer process as two novel mutual information-based objectives: (i) to compress the domain-invariant relation extracted from pre-trained SAM, excluding pseudo-invariant information as possible, and (ii) to maximize mutual information between the relational knowledge learned by the teacher (pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM establishes a robust distillation framework for PEFT of SAM. Extensive experiments across diverse benchmarks validate InfoSAM's effectiveness in improving SAM family's performance on real-world tasks, demonstrating its adaptability and superiority in handling specialized scenarios."
      },
      {
        "id": "oai:arXiv.org:2505.22019v2",
        "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.22019",
        "author": "Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, Feng Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22019v2 Announce Type: replace \nAbstract: Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at https://github.com/Alibaba-NLP/VRAG."
      },
      {
        "id": "oai:arXiv.org:2505.22116v2",
        "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model",
        "link": "https://arxiv.org/abs/2505.22116",
        "author": "Jintao Zhang, Zirui Liu, Mingyue Cheng, Shilong Zhang, Tingyue Pan, Qi Liu, Yanhu Xie",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22116v2 Announce Type: replace \nAbstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia and is strongly linked to adverse outcomes such as myocardial injury and increased mortality. Despite its significance, IOH prediction is hindered by event sparsity and the challenge of integrating static and dynamic data across diverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal language model framework. To accurately identify and differentiate sparse hypotensive events, we leverage a two-stage training strategy. The first stage involves domain adaptive pretraining on IOH physiological time series augmented through diffusion methods, thereby enhancing the model sensitivity to patterns associated with hypotension. Subsequently, task fine-tuning is performed on the original clinical dataset to further enhance the ability to distinguish normotensive from hypotensive states. To enable multimodal fusion for each patient, we align structured clinical descriptions with the corresponding physiological time series at the token level. Such alignment enables the model to capture individualized temporal patterns alongside their corresponding clinical semantics. In addition, we convert static patient attributes into structured text to enrich personalized information. Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events, highlighting its applicability in clinical decision support scenarios. Our code is publicly available to promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM."
      },
      {
        "id": "oai:arXiv.org:2505.22830v2",
        "title": "What Has Been Lost with Synthetic Evaluation?",
        "link": "https://arxiv.org/abs/2505.22830",
        "author": "Alexander Gill, Abhilasha Ravichander, Ana Marasovi\\'c",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22830v2 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly used for data generation. However, creating evaluation benchmarks raises the bar for this emerging paradigm. Benchmarks must target specific phenomena, penalize exploiting shortcuts, and be challenging. Through two case studies, we investigate whether LLMs can meet these demands by generating reasoning over-text benchmarks and comparing them to those created through careful crowdsourcing. Specifically, we evaluate both the validity and difficulty of LLM-generated versions of two high-quality reading comprehension datasets: CondaQA, which evaluates reasoning about negation, and DROP, which targets reasoning about quantities. We find that prompting LLMs can produce variants of these datasets that are often valid according to the annotation guidelines, at a fraction of the cost of the original crowdsourcing effort. However, we show that they are less challenging for LLMs than their human-authored counterparts. This finding sheds light on what may have been lost by generating evaluation data with LLMs, and calls for critically reassessing the immediate use of this increasingly prevalent approach to benchmark creation."
      },
      {
        "id": "oai:arXiv.org:2505.22848v2",
        "title": "LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference",
        "link": "https://arxiv.org/abs/2505.22848",
        "author": "Pingjun Hong, Beiduo Chen, Siyao Peng, Marie-Catherine de Marneffe, Barbara Plank",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22848v2 Announce Type: replace \nAbstract: There is increasing evidence of Human Label Variation (HLV) in Natural Language Inference (NLI), where annotators assign different labels to the same premise-hypothesis pair. However, within-label variation--cases where annotators agree on the same label but provide divergent reasoning--poses an additional and mostly overlooked challenge. Several NLI datasets contain highlighted words in the NLI item as explanations, but the same spans on the NLI item can be highlighted for different reasons, as evidenced by free-text explanations, which offer a window into annotators' reasoning. To systematically understand this problem and gain insight into the rationales behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for categorizing free-text explanations. Using this taxonomy, we annotate a subset of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it aligns with NLI labels, highlights, and explanations. We further assess the taxonomy's usefulness in explanation generation, demonstrating that conditioning generation on LITEX yields explanations that are linguistically closer to human explanations than those generated using only labels or highlights. Our approach thus not only captures within-label variation but also shows how taxonomy-guided generation for reasoning can bridge the gap between human and model explanations more effectively than existing strategies."
      },
      {
        "id": "oai:arXiv.org:2505.23001v2",
        "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors",
        "link": "https://arxiv.org/abs/2505.23001",
        "author": "Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23001v2 Announce Type: replace \nAbstract: Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors."
      },
      {
        "id": "oai:arXiv.org:2505.23114v2",
        "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data",
        "link": "https://arxiv.org/abs/2505.23114",
        "author": "Seohyeong Lee, Eunwon Kim, Hwaran Lee, Buru Chang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23114v2 Announce Type: replace \nAbstract: Human preference data plays a critical role in aligning large language models (LLMs) with human values. However, collecting such data is often expensive and inefficient, posing a significant scalability challenge. To address this, we introduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and diagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we compute alignment scores for LLM-generated responses to instructions from existing preference datasets. These scores are then used to construct an Alignment Data Map based on their mean and variance. Our experiments show that using only 33 percent of the data, specifically samples in the high-mean, low-variance region, achieves performance comparable to or better than using the entire dataset. This finding suggests that the Alignment Data Map can significantly improve data collection efficiency by identifying high-quality samples for LLM alignment without requiring explicit annotations. Moreover, the Alignment Data Map can diagnose existing preference datasets. Our analysis shows that it effectively detects low-impact or potentially misannotated samples. Source code is available online."
      },
      {
        "id": "oai:arXiv.org:2505.23190v2",
        "title": "DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer",
        "link": "https://arxiv.org/abs/2505.23190",
        "author": "Yekun Zhu, Min Tang, Zheng Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23190v2 Announce Type: replace \nAbstract: In this paper, we propose a novel neural network approach, termed DeepRTE, to address the steady-state Radiative Transfer Equation (RTE). The RTE is a differential-integral equation that governs the propagation of radiation through a participating medium, with applications spanning diverse domains such as neutron transport, atmospheric radiative transfer, heat transfer, and optical imaging. Our DeepRTE framework demonstrates superior computational efficiency for solving the steady-state RTE, surpassing traditional methods and existing neural network approaches. This efficiency is achieved by embedding physical information through derivation of the RTE and mathematically-informed network architecture. Concurrently, DeepRTE achieves high accuracy with significantly fewer parameters, largely due to its incorporation of mechanisms such as multi-head attention. Furthermore, DeepRTE is a mesh-free neural operator framework with inherent zero-shot capability. This is achieved by incorporating Green's function theory and pre-training with delta-function inflow boundary conditions into both its architecture design and training data construction. The efficacy of the proposed approach is substantiated through comprehensive numerical experiments."
      },
      {
        "id": "oai:arXiv.org:2505.23368v2",
        "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation",
        "link": "https://arxiv.org/abs/2505.23368",
        "author": "Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23368v2 Announce Type: replace \nAbstract: The recent rise of reasoning-tuned Large Language Models (LLMs)--which generate chains of thought (CoTs) before giving the final answer--has attracted significant attention and offers new opportunities for gaining insights into human label variation, which refers to plausible differences in how multiple annotators label the same data instance. Prior work has shown that LLM-generated explanations can help align model predictions with human label distributions, but typically adopt a reverse paradigm: producing explanations based on given answers. In contrast, CoTs provide a forward reasoning path that may implicitly embed rationales for each answer option, before generating the answers. We thus propose a novel LLM-based pipeline enriched with linguistically-grounded discourse segmenters to extract supporting and opposing statements for each answer option from CoTs with improved accuracy. We also propose a rank-based HLV evaluation framework that prioritizes the ranking of answers over exact scores, which instead favor direct comparison of label distributions. Our method outperforms a direct generation method as well as baselines on three datasets, and shows better alignment of ranking methods with humans, highlighting the effectiveness of our approach."
      },
      {
        "id": "oai:arXiv.org:2505.23721v2",
        "title": "DiffER: Categorical Diffusion for Chemical Retrosynthesis",
        "link": "https://arxiv.org/abs/2505.23721",
        "author": "Sean Current, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning, Srinivasan Parthasarathy",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23721v2 Announce Type: replace \nAbstract: Methods for automatic chemical retrosynthesis have found recent success through the application of models traditionally built for natural language processing, primarily through transformer neural networks. These models have demonstrated significant ability to translate between the SMILES encodings of chemical products and reactants, but are constrained as a result of their autoregressive nature. We propose DiffER, an alternative template-free method for retrosynthesis prediction in the form of categorical diffusion, which allows the entire output SMILES sequence to be predicted in unison. We construct an ensemble of diffusion models which achieves state-of-the-art performance for top-1 accuracy and competitive performance for top-3, top-5, and top-10 accuracy among template-free methods. We prove that DiffER is a strong baseline for a new class of template-free model, capable of learning a variety of synthetic techniques used in laboratory settings and outperforming a variety of other template-free methods on top-k accuracy metrics. By constructing an ensemble of categorical diffusion models with a novel length prediction component with variance, our method is able to approximately sample from the posterior distribution of reactants, producing results with strong metrics of confidence and likelihood. Furthermore, our analyses demonstrate that accurate prediction of the SMILES sequence length is key to further boosting the performance of categorical diffusion models."
      },
      {
        "id": "oai:arXiv.org:2505.23754v2",
        "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.23754",
        "author": "Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23754v2 Announce Type: replace \nAbstract: Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration."
      },
      {
        "id": "oai:arXiv.org:2505.23807v3",
        "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
        "link": "https://arxiv.org/abs/2505.23807",
        "author": "Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23807v3 Announce Type: replace \nAbstract: Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research."
      },
      {
        "id": "oai:arXiv.org:2505.23809v2",
        "title": "LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion",
        "link": "https://arxiv.org/abs/2505.23809",
        "author": "Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23809v2 Announce Type: replace \nAbstract: As e-commerce competition intensifies, balancing creative content with conversion effectiveness becomes critical. Leveraging LLMs' language generation capabilities, we propose a framework that integrates prompt engineering, multi-objective fine-tuning, and post-processing to generate marketing copy that is both engaging and conversion-driven. Our fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding. Through offline evaluations and online A/B tests across categories, our approach achieves a 12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content novelty. This provides a practical solution for automated copy generation and suggests paths for future multimodal, real-time personalization."
      },
      {
        "id": "oai:arXiv.org:2505.23868v2",
        "title": "Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert",
        "link": "https://arxiv.org/abs/2505.23868",
        "author": "Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou, Libo Qin, Wenhong Tian",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23868v2 Announce Type: replace \nAbstract: Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning."
      },
      {
        "id": "oai:arXiv.org:2505.24133v2",
        "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration",
        "link": "https://arxiv.org/abs/2505.24133",
        "author": "Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24133v2 Announce Type: replace \nAbstract: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets."
      },
      {
        "id": "oai:arXiv.org:2505.24139v2",
        "title": "S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation",
        "link": "https://arxiv.org/abs/2505.24139",
        "author": "Yichen Xie, Runsheng Xu, Tong He, Jyh-Jing Hwang, Katie Luo, Jingwei Ji, Hubert Lin, Letian Chen, Yiren Lu, Zhaoqi Leng, Dragomir Anguelov, Mingxing Tan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24139v2 Announce Type: replace \nAbstract: The latest advancements in multi-modal large language models (MLLMs) have spurred a strong renewed interest in end-to-end motion planning approaches for autonomous driving. Many end-to-end approaches rely on human annotations to learn intermediate perception and prediction tasks, while purely self-supervised approaches--which directly learn from sensor inputs to generate planning trajectories without human annotations often underperform the state of the art. We observe a key gap in the input representation space: end-to-end approaches built on MLLMs are often pretrained with reasoning tasks in 2D image space rather than the native 3D space in which autonomous vehicles plan. To this end, we propose S4-Driver, a scalable self-supervised motion planning algorithm with spatio-temporal visual representation, based on the popular PaLI multimodal large language model. S4-Driver uses a novel sparse volume strategy to seamlessly transform the strong visual representation of MLLMs from perspective view to 3D space without the need to finetune the vision encoder. This representation aggregates multi-view and multi-frame visual inputs and enables better prediction of planning trajectories in 3D space. To validate our method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with in-house camera data). Results show that S4-Driver performs favorably against existing supervised multi-task approaches while requiring no human annotations. It also demonstrates great scalability when pretrained on large volumes of unannotated driving logs."
      },
      {
        "id": "oai:arXiv.org:2505.24360v2",
        "title": "Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning",
        "link": "https://arxiv.org/abs/2505.24360",
        "author": "Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24360v2 Announce Type: replace \nAbstract: Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs."
      },
      {
        "id": "oai:arXiv.org:2505.24380v2",
        "title": "SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification",
        "link": "https://arxiv.org/abs/2505.24380",
        "author": "Zheng Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24380v2 Announce Type: replace \nAbstract: Fine-grained bird image classification (FBIC) is not only of great significance for ecological monitoring and species identification, but also holds broad research value in the fields of image recognition and fine-grained visual modeling. Compared with general image classification tasks, FBIC poses more formidable challenges: 1) the differences in species size and imaging distance result in the varying sizes of birds presented in the images; 2) complex natural habitats often introduce strong background interference; 3) and highly flexible poses such as flying, perching, or foraging result in substantial intra-class variability. These factors collectively make it difficult for traditional methods to stably extract discriminative features, thereby limiting the generalizability and interpretability of models in real-world applications. To address these challenges, this paper proposes a fine-grained bird classification framework based on strip-aware spatial perception, which aims to capture long-range spatial dependencies across entire rows or columns in bird images, thereby enhancing the model's robustness and interpretability. The proposed method incorporates two novel modules: extensional perception aggregator (EPA) and channel semantic weaving (CSW). Specifically, EPA integrates local texture details with global structural cues by aggregating information across horizontal and vertical spatial directions. CSW further refines the semantic representations by adaptively fusing long-range and short-range information along the channel dimension. Built upon a ResNet-50 backbone, the model enables jump-wise connection of extended structural features across the spatial domain. Experimental results on the CUB-200-2011 dataset demonstrate that our framework achieves significant performance improvements while maintaining architectural efficiency."
      },
      {
        "id": "oai:arXiv.org:2505.24539v2",
        "title": "Localizing Persona Representations in LLMs",
        "link": "https://arxiv.org/abs/2505.24539",
        "author": "Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24539v2 Announce Type: replace \nAbstract: We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements."
      },
      {
        "id": "oai:arXiv.org:2505.24595v2",
        "title": "Binary Cumulative Encoding meets Time Series Forecasting",
        "link": "https://arxiv.org/abs/2505.24595",
        "author": "Andrei Chernov, Vitaliy Pozdnyakov, Ilya Makarov",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24595v2 Announce Type: replace \nAbstract: Recent studies in time series forecasting have explored formulating regression via classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from stable training, robust uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding that ignores the inherent ordinal structure of the underlying values. As a result, they fail to provide information about the relative distance between predicted and true values during training. In this paper, we propose to address this limitation by introducing binary cumulative encoding (BCE), that represents scalar targets into monotonic binary vectors. This encoding implicitly preserves order and magnitude information, allowing the model to learn distance-aware representations while still operating within a classification framework. We propose a convolutional neural network architecture specifically designed for BCE, incorporating residual and dilated convolutions to enable fast and expressive temporal modeling. Through extensive experiments on benchmark forecasting datasets, we show that our approach outperforms widely used methods in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training."
      },
      {
        "id": "oai:arXiv.org:2505.24718v2",
        "title": "Reinforcing Video Reasoning with Focused Thinking",
        "link": "https://arxiv.org/abs/2505.24718",
        "author": "Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, Tat-Seng Chua",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24718v2 Announce Type: replace \nAbstract: Recent advancements in reinforcement learning, particularly through Group Relative Policy Optimization (GRPO), have significantly improved multimodal large language models for complex reasoning tasks. However, two critical limitations persist: 1) they often produce unfocused, verbose reasoning chains that obscure salient spatiotemporal cues and 2) binary rewarding fails to account for partially correct answers, resulting in high reward variance and inefficient learning. In this paper, we propose TW-GRPO, a novel framework that enhances visual reasoning with focused thinking and dense reward granularity. Specifically, we employs a token weighting mechanism that prioritizes tokens with high informational density (estimated by intra-group variance), suppressing redundant tokens like generic reasoning prefixes. Furthermore, we reformulate RL training by shifting from single-choice to multi-choice QA tasks, where soft rewards enable finer-grained gradient estimation by distinguishing partial correctness. Additionally, we propose question-answer inversion, a data augmentation strategy to generate diverse multi-choice samples from existing benchmarks. Experiments demonstrate state-of-the-art performance on several video reasoning and general understanding benchmarks. Notably, TW-GRPO achieves 50.4\\% accuracy on CLEVRER (18.8\\% improvement over Video-R1) and 65.8\\% on MMVU. Our codes are available at \\href{https://github.com/longmalongma/TW-GRPO}."
      },
      {
        "id": "oai:arXiv.org:2505.24779v2",
        "title": "EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation",
        "link": "https://arxiv.org/abs/2505.24779",
        "author": "Yidong Luo, Chenguang Wang, Jiahao Yang, Fanzeng Xia, Tianshu Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24779v2 Announce Type: replace \nAbstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex decision-making problems. The proliferation of MILP instance generation methods, driven by machine learning's demand for diverse optimization datasets and the limitations of static benchmarks, has significantly outpaced standardized evaluation techniques. Consequently, assessing the fidelity and utility of synthetic MILP instances remains a critical, multifaceted challenge. This paper introduces a comprehensive benchmark framework designed for the systematic and objective evaluation of MILP instance generation methods. Our framework provides a unified and extensible methodology, assessing instance quality across crucial dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream machine learning tasks. A key innovation is its in-depth analysis of solver-internal features -- particularly by comparing distributions of key solver outputs including root node gap, heuristic success rates, and cut plane usage -- leveraging the solver's dynamic solution behavior as an `expert assessment' to reveal nuanced computational resemblances. By offering a structured approach with clearly defined solver-independent and solver-dependent metrics, our benchmark aims to facilitate robust comparisons among diverse generation techniques, spur the development of higher-quality instance generators, and ultimately enhance the reliability of research reliant on synthetic MILP data. The framework's effectiveness in systematically comparing the fidelity of instance sets is demonstrated using contemporary generative models."
      },
      {
        "id": "oai:arXiv.org:2506.00022v2",
        "title": "Scaling Physical Reasoning with the PHYSICS Dataset",
        "link": "https://arxiv.org/abs/2506.00022",
        "author": "Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, Haonan He, Ning Ding, Yu Cheng, Shuyue Hu, Lei Bai, Dongzhan Zhou, Ganqu Cui, Peng Ye",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00022v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics."
      },
      {
        "id": "oai:arXiv.org:2506.00077v2",
        "title": "Gaussian mixture models as a proxy for interacting language models",
        "link": "https://arxiv.org/abs/2506.00077",
        "author": "Edward L. Wang, Tianyu Wang, Avanti Athreya, Vince Lyzinski, Carey E. Priebe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00077v2 Announce Type: replace \nAbstract: Large language models (LLMs) are a powerful tool with the ability to match human capabilities and behavior in many settings. Retrieval-augmented generation (RAG) further allows LLMs to generate diverse output depending on the contents of their RAG database. This motivates their use in the social sciences to study human behavior between individuals when large-scale experiments are infeasible. However, LLMs depend on complex, computationally expensive algorithms. In this paper, we introduce interacting Gaussian mixture models (GMMs) as an alternative to similar frameworks using LLMs. We compare a simplified model of GMMs to select experimental simulations of LLMs whose updating and response depend on feedback from other LLMs. We find that interacting GMMs capture important features of the dynamics in interacting LLMs, and we investigate key similarities and differences between interacting LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture models, potential modifications, and future research directions."
      },
      {
        "id": "oai:arXiv.org:2506.00250v2",
        "title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain",
        "link": "https://arxiv.org/abs/2506.00250",
        "author": "Mohammad Javad Ranjbar Kalahroodi, Amirhossein Sheikholselami, Sepehr Karimi, Sepideh Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00250v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale, expert-validated dataset of multiple-choice Persian medical questions, designed to evaluate LLMs across both Persian and English. We benchmark over 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 35.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, Persian responses are sometimes more accurate due to cultural and clinical contextual cues. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating multilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be accessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA"
      },
      {
        "id": "oai:arXiv.org:2506.00288v2",
        "title": "Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation",
        "link": "https://arxiv.org/abs/2506.00288",
        "author": "Ahmed Elhady, Eneko Agirre, Mikel Artetxe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00288v2 Announce Type: replace \nAbstract: Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future."
      },
      {
        "id": "oai:arXiv.org:2506.00433v2",
        "title": "Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free",
        "link": "https://arxiv.org/abs/2506.00433",
        "author": "Luigi Sigillo, Shengfeng He, Danilo Comminiello",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00433v2 Announce Type: replace \nAbstract: High-resolution image synthesis remains a core challenge in generative modeling, particularly in balancing computational efficiency with the preservation of fine-grained visual detail. We present Latent Wavelet Diffusion (LWD), a lightweight framework that enables any latent diffusion model to scale to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces three key components: (1) a scale-consistent variational autoencoder objective that enhances the spectral fidelity of latent representations; (2) wavelet energy maps that identify and localize detail-rich spatial regions within the latent space; and (3) a time-dependent masking strategy that focuses denoising supervision on high-frequency components during training. LWD requires no architectural modifications and incurs no additional computational overhead. Despite its simplicity, it consistently improves perceptual quality and reduces FID in ultra-high-resolution image synthesis, outperforming strong baseline models. These results highlight the effectiveness of frequency-aware, signal-driven supervision as a principled and efficient approach for high-resolution generative modeling."
      },
      {
        "id": "oai:arXiv.org:2506.00486v2",
        "title": "It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs",
        "link": "https://arxiv.org/abs/2506.00486",
        "author": "Jun Wu, Yirong Xiong, Jiangtao Wen, Yuxing Han",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00486v2 Announce Type: replace \nAbstract: Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior."
      },
      {
        "id": "oai:arXiv.org:2506.00519v2",
        "title": "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention",
        "link": "https://arxiv.org/abs/2506.00519",
        "author": "Yuxi Sun, Aoqi Zuo, Wei Gao, Jing Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00519v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \\textit{abstain} when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \\textit{CausalAbstain}, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \\textit{CausalAbstain} effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\\textsc{Casual-native}) and multilingual (\\textsc{Causal-multi}) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at https://github.com/peachch/CausalAbstain."
      },
      {
        "id": "oai:arXiv.org:2506.00541v2",
        "title": "3D Trajectory Reconstruction of Moving Points Based on Asynchronous Cameras",
        "link": "https://arxiv.org/abs/2506.00541",
        "author": "Huayu Huang, Banglei Guan, Yang Shang, Qifeng Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00541v2 Announce Type: replace \nAbstract: Photomechanics is a crucial branch of solid mechanics. The localization of point targets constitutes a fundamental problem in optical experimental mechanics, with extensive applications in various missions of UAVs. Localizing moving targets is crucial for analyzing their motion characteristics and dynamic properties. Reconstructing the trajectories of points from asynchronous cameras is a significant challenge. It encompasses two coupled sub-problems: trajectory reconstruction and camera synchronization. Present methods typically address only one of these sub-problems individually. This paper proposes a 3D trajectory reconstruction method for point targets based on asynchronous cameras, simultaneously solving both sub-problems. Firstly, we extend the trajectory intersection method to asynchronous cameras to resolve the limitation of traditional triangulation that requires camera synchronization. Secondly, we develop models for camera temporal information and target motion, based on imaging mechanisms and target dynamics characteristics. The parameters are optimized simultaneously to achieve trajectory reconstruction without accurate time parameters. Thirdly, we optimize the camera rotations alongside the camera time information and target motion parameters, using tighter and more continuous constraints on moving points. The reconstruction accuracy is significantly improved, especially when the camera rotations are inaccurate. Finally, the simulated and real-world experimental results demonstrate the feasibility and accuracy of the proposed method. The real-world results indicate that the proposed algorithm achieved a localization error of 112.95 m at an observation range of 15 ~ 20 km."
      },
      {
        "id": "oai:arXiv.org:2506.00612v2",
        "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation",
        "link": "https://arxiv.org/abs/2506.00612",
        "author": "Running Yang, Wenlong Deng, Minghui Chen, Yuyin Zhou, Xiaoxiao Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00612v2 Announce Type: replace \nAbstract: Clinical tasks such as diagnosis and treatment require strong decision-making abilities, highlighting the importance of rigorous evaluation benchmarks to assess the reliability of large language models (LLMs). In this work, we introduce a knowledge-guided data augmentation framework that enhances the difficulty of clinical multiple-choice question (MCQ) datasets by generating distractors (i.e., incorrect choices that are similar to the correct one and may confuse existing LLMs). Using our KG-based pipeline, the generated choices are both clinically plausible and deliberately misleading. Our approach involves multi-step, semantically informed walks on a medical knowledge graph to identify distractor paths-associations that are medically relevant but factually incorrect-which then guide the LLM in crafting more deceptive distractors. We apply the designed knowledge graph guided distractor generation (KGGDG) pipline, to six widely used medical QA benchmarks and show that it consistently reduces the accuracy of state-of-the-art LLMs. These findings establish KGGDG as a powerful tool for enabling more robust and diagnostic evaluations of medical LLMs."
      },
      {
        "id": "oai:arXiv.org:2506.00622v2",
        "title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples",
        "link": "https://arxiv.org/abs/2506.00622",
        "author": "Haesung Pyun, Yoonah Park, Yohan Jo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00622v2 Announce Type: replace \nAbstract: In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch, a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training."
      },
      {
        "id": "oai:arXiv.org:2506.00653v2",
        "title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models",
        "link": "https://arxiv.org/abs/2506.00653",
        "author": "Femi Bello, Anubrata Das, Fanzhi Zeng, Fangcong Yin, Liu Leqi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00653v2 Announce Type: replace \nAbstract: It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \\emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \\textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales."
      },
      {
        "id": "oai:arXiv.org:2506.00694v2",
        "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments",
        "link": "https://arxiv.org/abs/2506.00694",
        "author": "Li Zhang, Morgan Gray, Jaromir Savelka, Kevin D. Ashley",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00694v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 & 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Link: https://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/"
      },
      {
        "id": "oai:arXiv.org:2506.00773v2",
        "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models",
        "link": "https://arxiv.org/abs/2506.00773",
        "author": "Boheng Sheng, Jiacheng Yao, Meicong Zhang, Guoxiu He",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00773v2 Announce Type: replace \nAbstract: Large language models (LLMs) often struggle to accurately read and comprehend extremely long texts. Current methods for improvement typically rely on splitting long contexts into fixed-length chunks. However, fixed truncation risks separating semantically relevant content, leading to ambiguity and compromising accurate understanding. To overcome this limitation, we propose a straightforward approach for dynamically separating and selecting chunks of long context, facilitating a more streamlined input for LLMs. In particular, we compute semantic similarities between adjacent sentences, using lower similarities to adaptively divide long contexts into variable-length chunks. We further train a question-aware classifier to select sensitive chunks that are critical for answering specific questions. Experimental results on both single-hop and multi-hop question-answering benchmarks show that the proposed approach consistently outperforms strong baselines. Notably, it maintains robustness across a wide range of input lengths, handling sequences of up to 256k tokens. Our datasets and code are available at the following link: https://github.com/ECNU-Text-Computing/DCS"
      },
      {
        "id": "oai:arXiv.org:2506.00829v2",
        "title": "COMPKE: Complex Question Answering under Knowledge Editing",
        "link": "https://arxiv.org/abs/2506.00829",
        "author": "Keyuan Cheng, Zijian Kan, Zhixian He, Zhuoran Zhang, Muhammad Asif Ali, Ke Xu, Lijie Hu, Di Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00829v2 Announce Type: replace \nAbstract: Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE."
      },
      {
        "id": "oai:arXiv.org:2506.00859v2",
        "title": "How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation",
        "link": "https://arxiv.org/abs/2506.00859",
        "author": "Md Kowsher, Nusrat Jahan Prottasha, Shiyun Xu, Shetu Mohanto, Chen Chen, Ozlem Garibay, Niloofar Yousefi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00859v2 Announce Type: replace \nAbstract: Bidirectional language models have better context understanding and perform better than unidirectional models on natural language understanding tasks, yet the theoretical reasons behind this advantage remain unclear. In this work, we investigate this disparity through the lens of the Information Bottleneck (IB) principle, which formalizes a trade-off between compressing input information and preserving task-relevant content. We propose FlowNIB, a dynamic and scalable method for estimating mutual information during training that addresses key limitations of classical IB approaches, including computational intractability and fixed trade-off schedules. Theoretically, we show that bidirectional models retain more mutual information and exhibit higher effective dimensionality than unidirectional models. To support this, we present a generalized framework for measuring representational complexity and prove that bidirectional representations are strictly more informative under mild conditions. We further validate our findings through extensive experiments across multiple models and tasks using FlowNIB, revealing how information is encoded and compressed throughout training. Together, our work provides a principled explanation for the effectiveness of bidirectional architectures and introduces a practical tool for analyzing information flow in deep language models."
      },
      {
        "id": "oai:arXiv.org:2506.00891v2",
        "title": "Uneven Event Modeling for Partially Relevant Video Retrieval",
        "link": "https://arxiv.org/abs/2506.00891",
        "author": "Sa Zhu, Huashan Chen, Wanqian Zhang, Jinchao Zhang, Zexian Yang, Xiaoshuai Hao, Bo Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00891v2 Announce Type: replace \nAbstract: Given a text query, partially relevant video retrieval (PRVR) aims to retrieve untrimmed videos containing relevant moments, wherein event modeling is crucial for partitioning the video into smaller temporal events that partially correspond to the text. Previous methods typically segment videos into a fixed number of equal-length clips, resulting in ambiguous event boundaries. Additionally, they rely on mean pooling to compute event representations, inevitably introducing undesired misalignment. To address these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first introduce the Progressive-Grouped Video Segmentation (PGVS) module, to iteratively formulate events in light of both temporal dependencies and semantic similarity between consecutive frames, enabling clear event boundaries. Furthermore, we also propose the Context-Aware Event Refinement (CAER) module to refine the event representation conditioned the text's cross-attention. This enables event representations to focus on the most relevant frames for a given text, facilitating more precise text-video alignment. Extensive experiments demonstrate that our method achieves state-of-the-art performance on two PRVR benchmarks. Code is available at https://github.com/Sasa77777779/UEM.git."
      },
      {
        "id": "oai:arXiv.org:2506.00912v2",
        "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages",
        "link": "https://arxiv.org/abs/2506.00912",
        "author": "Yongdong chi, Hanqing Wang, Zonghan Yang, Jian Yang, Xiao Yan, Yun Chen, Guanhua Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00912v2 Announce Type: replace \nAbstract: Text-to-SQL transforms the user queries from natural language to executable SQL programs, enabling non-experts to interact with complex databases. Existing prompt-based methods craft meticulous text guidelines and examples to facilitate SQL generation, but their accuracy is hindered by the large semantic gap between the texts and the low-resource SQL programs. In this work, we propose Pi-SQL, which incorporates the high-resource Python program as a pivot to bridge between the natural language query and SQL program. In particular, Pi-SQL first generates Python programs that provide fine-grained step-by-step guidelines in their code blocks or comments, and then produces an SQL program following the guidance of each Python program. The final SQL program matches the reference Python program's query results and, through selection from candidates generated by different strategies, achieves superior execution speed, with a reward-based valid efficiency score up to 4.55 higher than the best-performing baseline. Extensive experiments demonstrate the effectiveness of Pi-SQL, which improves the execution accuracy of the best-performing baseline by up to 3.20."
      },
      {
        "id": "oai:arXiv.org:2506.01047v2",
        "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification",
        "link": "https://arxiv.org/abs/2506.01047",
        "author": "Phan Anh Duong, Cat Luong, Divyesh Bommana, Tianyu Jiang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01047v2 Announce Type: replace \nAbstract: Emotions manifest through physical experiences and bodily reactions, yet identifying such embodied emotions in text remains understudied. We present an embodied emotion classification dataset, CHEER-Ekman, extending the existing binary embodied emotion dataset with Ekman's six basic emotion categories. Using automatic best-worst scaling with large language models, we achieve performance superior to supervised approaches on our new dataset. Our investigation reveals that simplified prompting instructions and chain-of-thought reasoning significantly improve emotion recognition accuracy, enabling smaller models to achieve competitive performance with larger ones. Our dataset is publicly available at: https://github.com/menamerai/cheer-ekman."
      },
      {
        "id": "oai:arXiv.org:2506.01213v2",
        "title": "On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective",
        "link": "https://arxiv.org/abs/2506.01213",
        "author": "Ning Zhang, Henry Kenlay, Li Zhang, Mihai Cucuringu, Xiaowen Dong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01213v2 Announce Type: replace \nAbstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis."
      },
      {
        "id": "oai:arXiv.org:2506.01318v2",
        "title": "Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack",
        "link": "https://arxiv.org/abs/2506.01318",
        "author": "SeungBum Ha, Saerom Park, Sung Whan Yoon",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01318v2 Announce Type: replace \nAbstract: Machine unlearning (MU) aims to expunge a designated forget set from a trained model without costly retraining, yet the existing techniques overlook two critical blind spots: \"over-unlearning\" that deteriorates retained data near the forget set, and post-hoc \"relearning\" attacks that aim to resurrect the forgotten knowledge. We first derive the over-unlearning metric OU@{\\epsilon}, which represents the collateral damage to the nearby region of the forget set, where the over-unlearning mainly appears. Next, we expose an unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack, which exploits the per-class prototype of the forget class with just a few samples, and easily restores the pre-unlearning performance. To counter both blind spots, we introduce Spotter, a plug-and-play objective that combines (i) a masked knowledge-distillation penalty on the nearby region of forget set to suppress OU@{\\epsilon}, and (ii) an intra-class dispersion loss that scatters forget-class embeddings, neutralizing prototypical relearning attacks. On CIFAR-10, as one of validations, Spotter reduces OU@{\\epsilon}by below the 0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the retain set within 1% of difference with the original, and denies the prototype-attack by keeping the forget set accuracy within <1%, without accessing retained data. It confirms that Spotter is a practical remedy of the unlearning's blind spots."
      },
      {
        "id": "oai:arXiv.org:2506.01496v2",
        "title": "Continual Speech Learning with Fused Speech Features",
        "link": "https://arxiv.org/abs/2506.01496",
        "author": "Guitao Wang, Jinming Zhao, Hao Yang, Guilin Qi, Tongtong Wu, Gholamreza Haffari",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01496v2 Announce Type: replace \nAbstract: Rapid growth in speech data demands adaptive models, as traditional static methods fail to keep pace with dynamic and diverse speech information. We introduce continuous speech learning, a new set-up targeting at bridging the adaptation gap in current speech models. We use the encoder-decoder Whisper model to standardize speech tasks into a generative format. We integrate a learnable gated-fusion layer on the top of the encoder to dynamically select task-specific features for downstream tasks. Our approach improves accuracy significantly over traditional methods in six speech processing tasks, demonstrating gains in adapting to new speech tasks without full retraining."
      },
      {
        "id": "oai:arXiv.org:2506.01531v2",
        "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework",
        "link": "https://arxiv.org/abs/2506.01531",
        "author": "Wenhao Liu, Zhenyi Lu, Xinyu Hu, Jierui Zhang, Dailin Li, Jiacheng Cen, Huilin Cao, Haiteng Wang, Yuhan Li, Kun Xie, Dandan Li, Pei Zhang, Chengbo Zhang, Yuxiang Ren, Xiaohong Huang, Yan Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01531v2 Announce Type: replace \nAbstract: High-quality math datasets are crucial for advancing the reasoning abilities of large language models (LLMs). However, existing datasets often suffer from three key issues: outdated and insufficient challenging content, neglecting human-like reasoning, and limited reliability due to single-LLM generation. To address these, we introduce STORM-BORN, an ultra-challenging dataset of mathematical derivations sourced from cutting-edge academic papers, which includes dense human-like approximations and heuristic cues. To ensure the reliability and quality, we propose a novel human-in-the-loop, multi-agent data generation framework, integrating reasoning-dense filters, multi-agent collaboration, and human mathematicians' evaluations. We curated a set of 2,000 synthetic samples and deliberately selected the 100 most difficult problems. Even most advanced models like GPT-o1 solved fewer than 5% of them. Fine-tuning on STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN provides both a high-difficulty benchmark and a human-like reasoning training resource. Our code and dataset are publicly available at https://github.com/lwhere/STORM-BORN."
      },
      {
        "id": "oai:arXiv.org:2506.01615v2",
        "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems",
        "link": "https://arxiv.org/abs/2506.01615",
        "author": "Pasunuti Prasanjith, Prathmesh B More, Anoop Kunchukuttan, Raj Dabre",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01615v2 Announce Type: replace \nAbstract: Retrieval-Augmented Generation (RAG) systems enable language models to access relevant information and generate accurate, well-grounded, and contextually informed responses. However, for Indian languages, the development of high-quality RAG systems is hindered by the lack of two critical resources: (1) evaluation benchmarks for retrieval and generation tasks, and (2) large-scale training datasets for multilingual retrieval. Most existing benchmarks and datasets are centered around English or high-resource languages, making it difficult to extend RAG capabilities to the diverse linguistic landscape of India. To address the lack of evaluation benchmarks, we create IndicMSMarco, a multilingual benchmark for evaluating retrieval quality and response generation in 13 Indian languages, created via manual translation of 1000 diverse queries from MS MARCO-dev set. To address the need for training data, we build a large-scale dataset of (question, answer, relevant passage) tuples derived from the Wikipedias of 19 Indian languages using state-of-the-art LLMs. Additionally, we include translated versions of the original MS MARCO dataset to further enrich the training data and ensure alignment with real-world information-seeking tasks. Resources are available here: https://huggingface.co/collections/ai4bharat/indicragsuite-683e7273cb2337208c8c0fcb"
      },
      {
        "id": "oai:arXiv.org:2506.01776v2",
        "title": "MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation",
        "link": "https://arxiv.org/abs/2506.01776",
        "author": "Yile Liu, Ziwei Ma, Xiu Jiang, Jinglu Hu, Jing Chang, Liang Li",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01776v2 Announce Type: replace \nAbstract: With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 different languages with 1667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing."
      },
      {
        "id": "oai:arXiv.org:2506.01789v2",
        "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability",
        "link": "https://arxiv.org/abs/2506.01789",
        "author": "Genta Indra Winata, David Anugraha, Emmy Liu, Alham Fikri Aji, Shou-Yi Hung, Aditya Parashar, Patrick Amadeus Irawan, Ruochen Zhang, Zheng-Xin Yong, Jan Christian Blaise Cruz, Niklas Muennighoff, Seungone Kim, Hanyang Zhao, Sudipta Kar, Kezia Erina Suryoraharjo, M. Farid Adilazuarda, En-Shiun Annie Lee, Ayu Purwarianti, Derry Tanti Wijaya, Monojit Choudhury",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01789v2 Announce Type: replace \nAbstract: High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics."
      },
      {
        "id": "oai:arXiv.org:2506.01897v2",
        "title": "MLorc: Momentum Low-rank Compression for Large Language Model Adaptation",
        "link": "https://arxiv.org/abs/2506.01897",
        "author": "Wei Shen, Zhang Yaxiang, Minhui Huang, Mengfan Xu, Jiawei Zhang, Cong Shen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01897v2 Announce Type: replace \nAbstract: With increasing size of large language models (LLMs), full-parameter fine-tuning imposes substantial memory demands. To alleviate this, we propose a novel memory-efficient training paradigm called Momentum Low-rank compression (MLorc). By directly compressing and reconstructing momentum rather than gradients, MLorc avoids imposing a fixed-rank constraint on weight update matrices and better preserves the training dynamics of full-parameter fine-tuning, in contrast to existing low-rank approaches such as LoRA and GaLore. Empirically, MLorc consistently outperforms other memory-efficient training methods, matches or even exceeds the performance of full fine-tuning with a small rank (e.g., $r=4$), and generalizes well across different optimizers -- all while not compromising time or memory efficiency. Furthermore, we provide a theoretical guarantee for its convergence under reasonable assumptions."
      },
      {
        "id": "oai:arXiv.org:2506.01921v2",
        "title": "MedEBench: Revisiting Text-instructed Image Editing on Medical Domain",
        "link": "https://arxiv.org/abs/2506.01921",
        "author": "Minghao Liu, Zhitao He, Zhiyuan Fan, Qingyun Wang, Yi R. Fung",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01921v2 Announce Type: replace \nAbstract: Text-guided image editing has seen rapid progress in natural image domains, but its adaptation to medical imaging remains limited and lacks standardized evaluation. Clinically, such editing holds promise for simulating surgical outcomes, creating personalized teaching materials, and enhancing patient communication. To bridge this gap, we introduce \\textbf{MedEBench}, a comprehensive benchmark for evaluating text-guided medical image editing. It consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks across 13 anatomical regions. MedEBench offers three key contributions: (1) a clinically relevant evaluation framework covering Editing Accuracy, Contextual Preservation, and Visual Quality, supported by detailed descriptions of expected change and ROI (Region of Interest) masks; (2) a systematic comparison of seven state-of-the-art models, revealing common failure patterns; and (3) a failure analysis protocol based on attention grounding, using IoU between attention maps and ROIs to identify mislocalization. MedEBench provides a solid foundation for developing and evaluating reliable, clinically meaningful medical image editing systems."
      },
      {
        "id": "oai:arXiv.org:2305.05529v2",
        "title": "Accelerate Langevin Sampling with Birth-Death Process and Exploration Component",
        "link": "https://arxiv.org/abs/2305.05529",
        "author": "Lezhi Tan, Jianfeng Lu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2305.05529v2 Announce Type: replace-cross \nAbstract: Sampling a probability distribution with known likelihood is a fundamental task in computational science and engineering. Aiming at multimodality, we propose a new sampling method that takes advantage of both birth-death process and exploration component. The main idea of this method is look before you leap. We keep two sets of samplers, one at warmer temperature and one at original temperature. The former one serves as pioneer in exploring new modes and passing useful information to the other, while the latter one samples the target distribution after receiving the information. We derive a mean-field limit and show how the exploration component accelerates the sampling process. Moreover, we prove exponential asymptotic convergence under mild assumption. Finally, we test on experiments from previous literature and compare our methodology to previous ones."
      },
      {
        "id": "oai:arXiv.org:2308.08247v2",
        "title": "Fast and Multiphase Rates for Nearest Neighbor Classifiers",
        "link": "https://arxiv.org/abs/2308.08247",
        "author": "Pengkun Yang, Jingzhao Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2308.08247v2 Announce Type: replace-cross \nAbstract: We study the scaling of classification error rates with respect to the size of the training dataset. In contrast to classical results where rates are minimax optimal for a problem class, this work starts with the empirical observation that, even for a fixed data distribution, the error scaling can have \\emph{diverse} rates across different ranges of sample size. To understand when and why the error rate is non-uniform, we theoretically analyze nearest neighbor classifiers. We show that an error scaling law can have fine-grained rates: in the early phase, the test error depends polynomially on the data dimension and decreases fast; whereas in the later phase, the error depends exponentially on the data dimension and decreases slowly. Our analysis highlights the complexity of the data distribution in determining the test error. When the data are distributed benignly, we show that the generalization error of nearest neighbor classifier can depend polynomially, instead of exponentially, on the data dimension."
      },
      {
        "id": "oai:arXiv.org:2308.16061v2",
        "title": "Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning",
        "link": "https://arxiv.org/abs/2308.16061",
        "author": "Estelle Ruellan, Masarah Paquet-Clouston, Sebastian Garcia",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2308.16061v2 Announce Type: replace-cross \nAbstract: Ransomware-as-a-service (RaaS) is increasing the scale and complexity of ransomware attacks. Understanding the internal operations behind RaaS has been a challenge due to the illegality of such activities. The recent chat leak of the Conti RaaS operator, one of the most infamous ransomware operators on the international scene, offers a key opportunity to better understand the inner workings of such organizations. This paper analyzes the main topic discussions in the Conti chat leak using machine learning techniques such as Natural Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as visualization strategies. Five discussion topics are found: 1) Business, 2) Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer Service/Problem Solving. Moreover, the distribution of topics among Conti members shows that only 4% of individuals have specialized discussions while almost all individuals (96%) are all-rounders, meaning that their discussions revolve around the five topics. The results also indicate that a significant proportion of Conti discussions are non-tech related. This study thus highlights that running such large RaaS operations requires a workforce skilled beyond technical abilities, with individuals involved in various tasks, from management to customer service or problem solving. The discussion topics also show that the organization behind the Conti RaaS oper5086933ator shares similarities with a large firm. We conclude that, although RaaS represents an example of specialization in the cybercrime industry, only a few members are specialized in one topic, while the rest runs and coordinates the RaaS operation."
      },
      {
        "id": "oai:arXiv.org:2310.08367v4",
        "title": "MCU: An Evaluation Framework for Open-Ended Game Agents",
        "link": "https://arxiv.org/abs/2310.08367",
        "author": "Xinyue Zheng, Haowei Lin, Kaichen He, Zihao Wang, Zilong Zheng, Yitao Liang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2310.08367v4 Announce Type: replace-cross \nAbstract: Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce Minecraft Universe (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 11 major categories and 41 subcategories of challenges; (2) a task composition mechanism capable of generating infinite diverse tasks with varying difficulty; and (3) a general evaluation framework that achieves 91.5\\% alignment with human ratings for open-ended task assessment. Empirical results reveal that even state-of-the-art foundation agents struggle with the increasing diversity and complexity of tasks. These findings highlight the necessity of MCU as a robust benchmark to drive progress in AI agent development within open-ended environments. Our evaluation code and scripts are available at https://github.com/CraftJarvis/MCU."
      },
      {
        "id": "oai:arXiv.org:2312.03940v3",
        "title": "PECANN: Parallel Efficient Clustering with Graph-Based Approximate Nearest Neighbor Search",
        "link": "https://arxiv.org/abs/2312.03940",
        "author": "Shangdi Yu, Joshua Engels, Yihao Huang, Julian Shun",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2312.03940v3 Announce Type: replace-cross \nAbstract: This paper studies density-based clustering of point sets. These methods use dense regions of points to detect clusters of arbitrary shapes. In particular, we study variants of density peaks clustering, a popular type of algorithm that has been shown to work well in practice. Our goal is to cluster large high-dimensional datasets, which are prevalent in practice. Prior solutions are either sequential, and cannot scale to large data, or are specialized for low-dimensional data.\n  This paper unifies the different variants of density peaks clustering into a single framework, PECANN, by abstracting out several key steps common to this class of algorithms. One such key step is to find nearest neighbors that satisfy a predicate function, and one of the main contributions of this paper is an efficient way to do this predicate search using graph-based approximate nearest neighbor search (ANNS). To provide ample parallelism, we propose a doubling search technique that enables points to find an approximate nearest neighbor satisfying the predicate in a small number of rounds. Our technique can be applied to many existing graph-based ANNS algorithms, which can all be plugged into PECANN.\n  We implement five clustering algorithms with PECANN and evaluate them on synthetic and real-world datasets with up to 1.28 million points and up to 1024 dimensions on a 30-core machine with two-way hyper-threading. Compared to the state-of-the-art FASTDP algorithm for high-dimensional density peaks clustering, which is sequential, our best algorithm is 45x-734x faster while achieving competitive ARI scores. Compared to the state-of-the-art parallel DPC-based algorithm, which is optimized for low dimensions, we show that PECANN is two orders of magnitude faster. As far as we know, our work is the first to evaluate DPC variants on large high-dimensional real-world image and text embedding datasets."
      },
      {
        "id": "oai:arXiv.org:2403.05809v2",
        "title": "Shallow ReLU neural networks and finite elements",
        "link": "https://arxiv.org/abs/2403.05809",
        "author": "Pengzhan Jin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.05809v2 Announce Type: replace-cross \nAbstract: We point out that (continuous or discontinuous) piecewise linear functions on a convex polytope mesh can be represented by two-hidden-layer ReLU neural networks in a weak sense. In addition, the numbers of neurons of the two hidden layers required to weakly represent are accurately given based on the numbers of polytopes and hyperplanes involved in this mesh. The results naturally hold for constant and linear finite element functions. Such weak representation establishes a bridge between shallow ReLU neural networks and finite element functions, and leads to a perspective for analyzing approximation capability of ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we discuss the strict representation for tensor finite element functions via the recent tensor neural networks."
      },
      {
        "id": "oai:arXiv.org:2403.14848v2",
        "title": "Learning WENO for entropy stable schemes to solve conservation laws",
        "link": "https://arxiv.org/abs/2403.14848",
        "author": "Philip Charles, Deep Ray",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.14848v2 Announce Type: replace-cross \nAbstract: Entropy conditions play a crucial role in the extraction of a physically relevant solution for systems of conservation laws, thus motivating the construction of entropy stable schemes that satisfy a discrete analogue of such conditions. TeCNO schemes (Fjordholm et al. 2012) form a class of arbitrary high-order entropy stable finite difference solvers, which require specialized reconstruction algorithms satisfying the sign property at each cell interface. Third-order weighted essentially non-oscillatory (WENO) schemes called SP-WENO (Fjordholm and Ray, 2016) and SP-WENOc (Ray, 2018) have been designed to satisfy the sign property. However, these WENO algorithms can perform poorly near shocks, with the numerical solutions exhibiting large spurious oscillations. In the present work, we propose a variant of the SP-WENO, termed as Deep Sign-Preserving WENO (DSP-WENO), where a neural network is trained to learn the WENO weighting strategy. The sign property and third-order accuracy are strongly imposed in the algorithm, which constrains the WENO weight selection region to a convex polygon. Thereafter, a neural network is trained to select the WENO weights from this convex region with the goal of improving the shock-capturing capabilities without sacrificing the rate of convergence in smooth regions. The proposed synergistic approach retains the mathematical framework of the TeCNO scheme while integrating deep learning to remedy the computational issues of the WENO-based reconstruction. We present several numerical experiments to demonstrate the significant improvement with DSP-WENO over the existing variants of WENO satisfying the sign property."
      },
      {
        "id": "oai:arXiv.org:2403.19516v2",
        "title": "Spectral Clustering for Directed Graphs via Likelihood Estimation on Stochastic Block Models",
        "link": "https://arxiv.org/abs/2403.19516",
        "author": "Ning Zhang, Xiaowen Dong, Mihai Cucuringu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.19516v2 Announce Type: replace-cross \nAbstract: Graph clustering is a fundamental task in unsupervised learning with broad real-world applications. While spectral clustering methods for undirected graphs are well-established and guided by a minimum cut optimization consensus, their extension to directed graphs remains relatively underexplored due to the additional complexity introduced by edge directions. In this paper, we leverage statistical inference on stochastic block models to guide the development of a spectral clustering algorithm for directed graphs. Specifically, we study the maximum likelihood estimation under a widely used directed stochastic block model, and derive a global objective function that aligns with the underlying community structure. We further establish a theoretical upper bound on the misclustering error of its spectral relaxation, and based on this relaxation, introduce a novel, self-adaptive spectral clustering method for directed graphs. Extensive experiments on synthetic and real-world datasets demonstrate significant performance gains over existing baselines."
      },
      {
        "id": "oai:arXiv.org:2404.16873v2",
        "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
        "link": "https://arxiv.org/abs/2404.16873",
        "author": "Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, Yuandong Tian",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2404.16873v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks."
      },
      {
        "id": "oai:arXiv.org:2405.14492v3",
        "title": "Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data",
        "link": "https://arxiv.org/abs/2405.14492",
        "author": "Tim Gyger, Reinhard Furrer, Fabio Sigrist",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.14492v3 Announce Type: replace-cross \nAbstract: Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, full-scale approximations (FSAs) combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce computational costs in calculating likelihoods, gradients, and predictive distributions with FSAs. In particular, we introduce a novel preconditioner and show theoretically and empirically that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we introduce an accurate and fast way to calculate predictive variances using stochastic simulation and iterative methods. In addition, we show how our newly proposed FITC preconditioner can also be used in iterative methods for Vecchia approximations. In our experiments, it outperforms existing state-of-the-art preconditioners for Vecchia approximations. All methods are implemented in a free C++ software library with high-level Python and R packages."
      },
      {
        "id": "oai:arXiv.org:2405.18373v3",
        "title": "A Hessian-Aware Stochastic Differential Equation for Modelling SGD",
        "link": "https://arxiv.org/abs/2405.18373",
        "author": "Xiang Li, Zebang Shen, Liang Zhang, Niao He",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.18373v3 Announce Type: replace-cross \nAbstract: Continuous-time approximation of Stochastic Gradient Descent (SGD) is a crucial tool to study its escaping behaviors from stationary points. However, existing stochastic differential equation (SDE) models fail to fully capture these behaviors, even for simple quadratic objectives. Built on a novel stochastic backward error analysis framework, we derive the Hessian-Aware Stochastic Modified Equation (HA-SME), an SDE that incorporates Hessian information of the objective function into both its drift and diffusion terms. Our analysis shows that HA-SME achieves the order-best approximation error guarantee among existing SDE models in the literature, while significantly reducing the dependence on the smoothness parameter of the objective. Empirical experiments on neural network-based loss functions further validate this improvement. Further, for quadratic objectives, under mild conditions, HA-SME is proved to be the first SDE model that recovers exactly the SGD dynamics in the distributional sense. Consequently, when the local landscape near a stationary point can be approximated by quadratics, HA-SME provides a more precise characterization of the local escaping behaviors of SGD. With the enhanced approximation guarantee, we further conduct an escape time analysis using HA-SME, showcasing how it can be employed to analytically study the escaping behavior of SGD for general function classes."
      },
      {
        "id": "oai:arXiv.org:2407.10640v2",
        "title": "Error Bounds for the Network Scale-Up Method",
        "link": "https://arxiv.org/abs/2407.10640",
        "author": "Sergio D\\'iaz-Aranda, Juan Marcos Ram\\'irez, Mohit Daga, Jaya Prakash Champati, Jos\\'e Aguilar, Rosa Elvira Lillo, Antonio Fern\\'andez Anta",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.10640v2 Announce Type: replace-cross \nAbstract: Epidemiologists and social scientists have used the Network Scale-Up Method (NSUM) for over thirty years to estimate the size of a hidden sub-population within a social network. This method involves querying a subset of network nodes about the number of their neighbours belonging to the hidden sub-population. In general, NSUM assumes that the social network topology and the hidden sub-population distribution are well-behaved; hence, the NSUM estimate is close to the actual value. However, bounds on NSUM estimation errors have not been analytically proven. This paper provides analytical bounds on the error incurred by the two most popular NSUM estimators. These bounds assume that the queried nodes accurately provide their degree and the number of neighbors belonging to the hidden population. Our key findings are twofold. First, we show that when an adversary designs the network and places the hidden sub-population, then the estimate can be a factor of $\\Omega(\\sqrt{n})$ off from the real value (in a network with $n$ nodes). Second, we also prove error bounds when the underlying network is randomly generated, showing that a small constant factor can be achieved with high probability using samples of logarithmic size $O(\\log{n})$. We present improved analytical bounds for Erdos-Renyi and Scale-Free networks. Our theoretical analysis is supported by an extensive set of numerical experiments designed to determine the effect of the sample size on the accuracy of the estimates in both synthetic and real networks."
      },
      {
        "id": "oai:arXiv.org:2408.08294v4",
        "title": "eGAD! double descent is explained by Generalized Aliasing Decomposition",
        "link": "https://arxiv.org/abs/2408.08294",
        "author": "Mark K. Transtrum, Gus L. W. Hart, Tyler J. Jarvis, Jared P. Whitehead",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.08294v4 Announce Type: replace-cross \nAbstract: A central problem in data science is to use potentially noisy samples of an unknown function to predict values for unseen inputs. In classical statistics, predictive error is understood as a trade-off between the bias and the variance that balances model simplicity with its ability to fit complex functions. However, over-parameterized models exhibit counterintuitive behaviors, such as \"double descent\" in which models of increasing complexity exhibit decreasing generalization error. Others may exhibit more complicated patterns of predictive error with multiple peaks and valleys. Neither double descent nor multiple descent phenomena are well explained by the bias-variance decomposition.\n  We introduce a novel decomposition that we call the generalized aliasing decomposition (GAD) to explain the relationship between predictive performance and model complexity. The GAD decomposes the predictive error into three parts: 1) model insufficiency, which dominates when the number of parameters is much smaller than the number of data points, 2) data insufficiency, which dominates when the number of parameters is much greater than the number of data points, and 3) generalized aliasing, which dominates between these two extremes.\n  We demonstrate the applicability of the GAD to diverse applications, including random feature models from machine learning, Fourier transforms from signal processing, solution methods for differential equations, and predictive formation enthalpy in materials discovery. Because key components of the GAD can be explicitly calculated from the relationship between model class and samples without seeing any data labels, it can answer questions related to experimental design and model selection before collecting data or performing experiments. We further demonstrate this approach on several examples and discuss implications for predictive modeling and data science."
      },
      {
        "id": "oai:arXiv.org:2409.16052v2",
        "title": "Denoising Graph Super-Resolution towards Improved Collider Event Reconstruction",
        "link": "https://arxiv.org/abs/2409.16052",
        "author": "Nilotpal Kakati, Etienne Dreyer, Eilam Gross",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.16052v2 Announce Type: replace-cross \nAbstract: In preparation for Higgs factories and energy-frontier facilities, future colliders are moving toward high-granularity calorimeters to improve reconstruction quality. However, the cost and construction complexity of such detectors is substantial, making software-based approaches like super-resolution an attractive alternative. This study explores integrating super-resolution techniques into an LHC-like reconstruction pipeline to effectively enhance calorimeter granularity and suppress noise. We find that this software preprocessing step significantly improves reconstruction quality without physical changes to the detector. To demonstrate its impact, we propose a novel transformer-based particle flow model that offers improved particle reconstruction quality and interpretability. Our results demonstrate that super-resolution can be readily applied at collider experiments."
      },
      {
        "id": "oai:arXiv.org:2409.16967v3",
        "title": "Scalable Multi-Robot Informative Path Planning for Target Mapping via Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2409.16967",
        "author": "Apoorva Vashisth, Manav Kulshrestha, Damon Conover, Aniket Bera",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.16967v3 Announce Type: replace-cross \nAbstract: Autonomous robots are widely utilized for mapping and exploration tasks due to their cost-effectiveness. Multi-robot systems offer scalability and efficiency, especially in terms of the number of robots deployed in more complex environments. These tasks belong to the set of Multi-Robot Informative Path Planning (MRIPP) problems. In this paper, we propose a deep reinforcement learning approach for the MRIPP problem. We aim to maximize the number of discovered stationary targets in an unknown 3D environment while operating under resource constraints (such as path length). Here, each robot aims to maximize discovered targets, avoid unknown static obstacles, and prevent inter-robot collisions while operating under communication and resource constraints. We utilize the centralized training and decentralized execution paradigm to train a single policy neural network. A key aspect of our approach is our coordination graph that prioritizes visiting regions not yet explored by other robots. Our learned policy can be copied onto any number of robots for deployment in more complex environments not seen during training. Our approach outperforms state-of-the-art approaches by at least 26.2% in terms of the number of discovered targets while requiring a planning time of less than 2 sec per step. We present results for more complex environments with up to 64 robots and compare success rates against baseline planners. Our code and trained model are available at - https://github.com/AccGen99/marl_ipp"
      },
      {
        "id": "oai:arXiv.org:2410.11635v4",
        "title": "Evidence of equilibrium dynamics in human social networks evolving in time",
        "link": "https://arxiv.org/abs/2410.11635",
        "author": "Miguel A. Gonz\\'alez-Casado, Andreia Sofia Teixeira, Angel S\\'anchez",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11635v4 Announce Type: replace-cross \nAbstract: How do networks of relationships evolve over time? We analyse a dataset tracking the social interactions of 900 individuals over four years. Despite continuous shifts in individual relationships, the macroscopic structural properties of the network remain stable, fluctuating within predictable bounds. We connect this stability to the concept of equilibrium in statistical physics. Specifically, we demonstrate that the probabilities governing network dynamics are stationary over time, and key features like degree, edge, and triangle abundances align with theoretical predictions from equilibrium dynamics. Moreover, the dynamics satisfies the detailed balance condition. Remarkably, equilibrium persists despite constant turnover as people join, leave, and change connections. This suggests that equilibrium arises not from specific individuals but from the balancing act of human needs, cognitive limits, and social pressures. Practically, this equilibrium simplifies data collection, supports methods relying on single network snapshots (like Exponential Random Graph Models), and aids in designing interventions for social challenges. Theoretically, it offers new insights into collective human behaviour, revealing how emergent properties of complex social systems can be captured by simple mathematical models."
      },
      {
        "id": "oai:arXiv.org:2411.05791v2",
        "title": "Forecasting Company Fundamentals",
        "link": "https://arxiv.org/abs/2411.05791",
        "author": "Felix Divo, Eric Endress, Kevin Endler, Kristian Kersting, Devendra Singh Dhami",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.05791v2 Announce Type: replace-cross \nAbstract: Company fundamentals are key to assessing companies' financial and overall success and stability. Forecasting them is important in multiple fields, including investing and econometrics. While statistical and contemporary machine learning methods have been applied to many time series tasks, there is a lack of comparison of these approaches on this particularly challenging data regime. To this end, we try to bridge this gap and thoroughly evaluate the theoretical properties and practical performance of 24 deterministic and probabilistic company fundamentals forecasting models on real company data. We observe that deep learning models provide superior forecasting performance to classical models, in particular when considering uncertainty estimation. To validate the findings, we compare them to human analyst expectations and find that their accuracy is comparable to the automatic forecasts. We further show how these high-quality forecasts can benefit automated stock allocation. We close by presenting possible ways of integrating domain experts to further improve performance and increase reliability."
      },
      {
        "id": "oai:arXiv.org:2411.08832v3",
        "title": "Offline Adaptation of Quadruped Locomotion using Diffusion Models",
        "link": "https://arxiv.org/abs/2411.08832",
        "author": "Reece O'Mahoney, Alexander L. Mitchell, Wanming Yu, Ingmar Posner, Ioannis Havoutis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08832v3 Announce Type: replace-cross \nAbstract: We present a diffusion-based approach to quadrupedal locomotion that simultaneously addresses the limitations of learning and interpolating between multiple skills and of (modes) offline adapting to new locomotion behaviours after training. This is the first framework to apply classifier-free guided diffusion to quadruped locomotion and demonstrate its efficacy by extracting goal-conditioned behaviour from an originally unlabelled dataset. We show that these capabilities are compatible with a multi-skill policy and can be applied with little modification and minimal compute overhead, i.e., running entirely on the robots onboard CPU. We verify the validity of our approach with hardware experiments on the ANYmal quadruped platform."
      },
      {
        "id": "oai:arXiv.org:2411.09689v3",
        "title": "Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge",
        "link": "https://arxiv.org/abs/2411.09689",
        "author": "Seongmin Lee (Polo), Hsiang Hsu (Polo), Chun-Fu Chen (Polo), Duen Horng (Polo),  Chau",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.09689v3 Announce Type: replace-cross \nAbstract: LLM hallucination, where unfaithful text is generated, presents a critical challenge for LLMs' practical applications. Current detection methods often resort to external knowledge, LLM fine-tuning, or supervised training with large hallucination-labeled datasets. Moreover, these approaches do not distinguish between different types of hallucinations, which is crucial for enhancing detection performance. To address such limitations, we introduce hallucination probing, a new task that classifies LLM-generated text into three categories: aligned, misaligned, and fabricated. Driven by our novel discovery that perturbing key entities in prompts affects LLM's generation of these three types of text differently, we propose SHINE, a novel hallucination probing method that does not require external knowledge, supervised training, or LLM fine-tuning. SHINE is effective in hallucination probing across three modern LLMs, and achieves state-of-the-art performance in hallucination detection, outperforming seven competing methods across four datasets and four LLMs, underscoring the importance of probing for accurate detection."
      },
      {
        "id": "oai:arXiv.org:2411.10596v2",
        "title": "A minimalistic representation model for head direction system",
        "link": "https://arxiv.org/abs/2411.10596",
        "author": "Minglu Zhao, Dehong Xu, Deqian Kong, Wen-Hao Zhang, Ying Nian Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.10596v2 Announce Type: replace-cross \nAbstract: We present a minimalistic representation model for the head direction (HD) system, aiming to learn a high-dimensional representation of head direction that captures essential properties of HD cells. Our model is a representation of rotation group $U(1)$, and we study both the fully connected version and convolutional version. We demonstrate the emergence of Gaussian-like tuning profiles and a 2D circle geometry in both versions of the model. We also demonstrate that the learned model is capable of accurate path integration."
      },
      {
        "id": "oai:arXiv.org:2411.11190v2",
        "title": "DeepSPV: A Deep Learning Pipeline for 3D Spleen Volume Estimation from 2D Ultrasound Images",
        "link": "https://arxiv.org/abs/2411.11190",
        "author": "Zhen Yuan, David Stojanovski, Lei Li, Alberto Gomez, Haran Jogeesvaran, Esther Puyol-Ant\\'on, Baba Inusa, Andrew P. King",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.11190v2 Announce Type: replace-cross \nAbstract: Splenomegaly, the enlargement of the spleen, is an important clinical indicator for various associated medical conditions, such as sickle cell disease (SCD). Spleen length measured from 2D ultrasound is the most widely used metric for characterising spleen size. However, it is still considered a surrogate measure, and spleen volume remains the gold standard for assessing spleen size. Accurate spleen volume measurement typically requires 3D imaging modalities, such as computed tomography or magnetic resonance imaging, but these are not widely available, especially in the Global South which has a high prevalence of SCD. In this work, we introduce a deep learning pipeline, DeepSPV, for precise spleen volume estimation from single or dual 2D ultrasound images. The pipeline involves a segmentation network and a variational autoencoder for learning low-dimensional representations from the estimated segmentations. We investigate three approaches for spleen volume estimation and our best model achieves 86.62%/92.5% mean relative volume accuracy (MRVA) under single-view/dual-view settings, surpassing the performance of human experts. In addition, the pipeline can provide confidence intervals for the volume estimates as well as offering benefits in terms of interpretability, which further support clinicians in decision-making when identifying splenomegaly. We evaluate the full pipeline using a highly realistic synthetic dataset generated by a diffusion model, achieving an overall MRVA of 83.0% from a single 2D ultrasound image. Our proposed DeepSPV is the first work to use deep learning to estimate 3D spleen volume from 2D ultrasound images and can be seamlessly integrated into the current clinical workflow for spleen assessment."
      },
      {
        "id": "oai:arXiv.org:2411.15111v2",
        "title": "Learnable Activation Functions in Physics-Informed Neural Networks for Solving Partial Differential Equations",
        "link": "https://arxiv.org/abs/2411.15111",
        "author": "Afrah Farea, Mustafa Serdar Celebi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.15111v2 Announce Type: replace-cross \nAbstract: We investigate learnable activation functions in Physics-Informed Neural Networks (PINNs) for solving Partial Differential Equations (PDEs), comparing traditional Multilayer Perceptrons (MLPs) with fixed and trainable activations against Kolmogorov-Arnold Networks (KANs) that employ learnable basis functions. While PINNs effectively incorporate physical laws into the learning process, they suffer from convergence and spectral bias problems, which limit their applicability to problems with rapid oscillations or sharp transitions. In this work, we study and evaluate various activation and basis functions across diverse PDEs, including oscillatory, nonlinear wave, mixed-physics, and fluid dynamics problems. Using empirical Neural Tangent Kernel (NTK) analysis and Hessian eigenvalue decomposition, we assess convergence behavior, stability, and high-frequency approximation capacity. While KANs offer improved expressivity for capturing complex, high-frequency PDE solutions, they introduce new optimization challenges, especially in deeper networks. Our findings show that KANs face a curse of functional dimensionality, creating intractable optimization landscapes in deeper networks. Low spectral bias alone does not guarantee good performance; adaptive spectral bias approaches such as B-splines achieve optimal results by balancing global stability with local high-frequency resolution. Different PDE types require tailored strategies: smooth global activation functions excel for wave phenomena, while local adaptive activation functions suit problems with sharp transitions."
      },
      {
        "id": "oai:arXiv.org:2412.03238v3",
        "title": "Dynamic Consistent $k$-Center Clustering with Optimal Recourse",
        "link": "https://arxiv.org/abs/2412.03238",
        "author": "Sebastian Forster, Antonis Skarlatos",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03238v3 Announce Type: replace-cross \nAbstract: Given points from an arbitrary metric space and a sequence of point updates sent by an adversary, what is the minimum recourse per update (i.e., the minimum number of changes needed to the set of centers after an update), in order to maintain a constant-factor approximation to a $k$-clustering problem? This question has received attention in recent years under the name consistent clustering.\n  Previous works by Lattanzi and Vassilvitskii [ICLM '17] and Fichtenberger, Lattanzi, Norouzi-Fard, and Svensson [SODA '21] studied $k$-clustering objectives, including the $k$-center and the $k$-median objectives, under only point insertions. In this paper we study the $k$-center objective in the fully dynamic setting, where the update is either a point insertion or a point deletion. Before our work, {\\L}\\k{a}cki, Haeupler, Grunau, Rozho\\v{n}, and Jayaram [SODA '24] gave a deterministic fully dynamic constant-factor approximation algorithm for the $k$-center objective with worst-case recourse of $2$ per update.\n  In this work, we prove that the $k$-center clustering problem admits optimal recourse bounds by developing a deterministic fully dynamic constant-factor approximation algorithm with worst-case recourse of $1$ per update. Moreover our algorithm performs simple choices based on light data structures, and thus is arguably more direct and faster than the previous one which uses a sophisticated combinatorial structure. Additionally, we develop a new deterministic decremental algorithm and a new deterministic incremental algorithm, both of which maintain a $6$-approximate $k$-center solution with worst-case recourse of $1$ per update. Our incremental algorithm improves over the $8$-approximation algorithm by Charikar, Chekuri, Feder, and Motwani [STOC '97]. Finally, we remark that since all three of our algorithms are deterministic, they work against an adaptive adversary."
      },
      {
        "id": "oai:arXiv.org:2412.04339v2",
        "title": "Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET Image Reconstruction",
        "link": "https://arxiv.org/abs/2412.04339",
        "author": "George Webber, Yuya Mizuno, Oliver D. Howes, Alexander Hammers, Andrew P. King, Andrew J. Reader",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.04339v2 Announce Type: replace-cross \nAbstract: Medical image reconstruction with pre-trained score-based generative models (SGMs) has advantages over other existing state-of-the-art deep-learned reconstruction methods, including improved resilience to different scanner setups and advanced image distribution modeling. SGM-based reconstruction has recently been applied to simulated positron emission tomography (PET) datasets, showing improved contrast recovery for out-of-distribution lesions relative to the state-of-the-art. However, existing methods for SGM-based reconstruction from PET data suffer from slow reconstruction, burdensome hyperparameter tuning and slice inconsistency effects (in 3D). In this work, we propose a practical methodology for fully 3D reconstruction that accelerates reconstruction and reduces the number of critical hyperparameters by matching the likelihood of an SGM's reverse diffusion process to a current iterate of the maximum-likelihood expectation maximization algorithm. Using the example of low-count reconstruction from simulated [$^{18}$F]DPA-714 datasets, we show our methodology can match or improve on the NRMSE and SSIM of existing state-of-the-art SGM-based PET reconstruction while reducing reconstruction time and the need for hyperparameter tuning. We evaluate our methodology against state-of-the-art supervised and conventional reconstruction algorithms. Finally, we demonstrate a first-ever implementation of SGM-based reconstruction for real 3D PET data, specifically [$^{18}$F]DPA-714 data, where we integrate perpendicular pre-trained SGMs to eliminate slice inconsistency issues."
      },
      {
        "id": "oai:arXiv.org:2412.10849v3",
        "title": "Superhuman performance of a large language model on the reasoning tasks of a physician",
        "link": "https://arxiv.org/abs/2412.10849",
        "author": "Peter G. Brodeur, Thomas A. Buckley, Zahir Kanjee, Ethan Goh, Evelyn Bin Ling, Priyank Jain, Stephanie Cabral, Raja-Elie Abdulnour, Adrian D. Haimovich, Jason A. Freed, Andrew Olson, Daniel J. Morgan, Jason Hom, Robert Gallo, Liam G. McCoy, Haadi Mombini, Christopher Lucas, Misha Fotoohi, Matthew Gwiazdon, Daniele Restifo, Daniel Restrepo, Eric Horvitz, Jonathan Chen, Arjun K. Manrai, Adam Rodman",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10849v3 Announce Type: replace-cross \nAbstract: A seminal paper published by Ledley and Lusted in 1959 introduced complex clinical diagnostic reasoning cases as the gold standard for the evaluation of expert medical computing systems, a standard that has held ever since. Here, we report the results of a physician evaluation of a large language model (LLM) on challenging clinical cases against a baseline of hundreds of physicians. We conduct five experiments to measure clinical reasoning across differential diagnosis generation, display of diagnostic reasoning, triage differential diagnosis, probabilistic reasoning, and management reasoning, all adjudicated by physician experts with validated psychometrics. We then report a real-world study comparing human expert and AI second opinions in randomly-selected patients in the emergency room of a major tertiary academic medical center in Boston, MA. We compared LLMs and board-certified physicians at three predefined diagnostic touchpoints: triage in the emergency room, initial evaluation by a physician, and admission to the hospital or intensive care unit. In all experiments--both vignettes and emergency room second opinions--the LLM displayed superhuman diagnostic and reasoning abilities, as well as continued improvement from prior generations of AI clinical decision support. Our study suggests that LLMs have achieved superhuman performance on general medical diagnostic and management reasoning, fulfilling the vision put forth by Ledley and Lusted, and motivating the urgent need for prospective trials."
      },
      {
        "id": "oai:arXiv.org:2412.15289v3",
        "title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage",
        "link": "https://arxiv.org/abs/2412.15289",
        "author": "Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15289v3 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43."
      },
      {
        "id": "oai:arXiv.org:2501.12599v4",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
        "link": "https://arxiv.org/abs/2501.12599",
        "author": "Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, Zongyu Lin",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.12599v4 Announce Type: replace-cross \nAbstract: Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%)."
      },
      {
        "id": "oai:arXiv.org:2502.01536v3",
        "title": "VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion",
        "link": "https://arxiv.org/abs/2502.01536",
        "author": "Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01536v3 Announce Type: replace-cross \nAbstract: Recent success in legged robot locomotion is attributed to the integration of reinforcement learning and physical simulators. However, these policies often encounter challenges when deployed in real-world environments due to sim-to-real gaps, as simulators typically fail to replicate visual realism and complex real-world geometry. Moreover, the lack of realistic visual rendering limits the ability of these policies to support high-level tasks requiring RGB-based perception like ego-centric navigation. This paper presents a Real-to-Sim-to-Real framework that generates photorealistic and physically interactive \"digital twin\" simulation environments for visual navigation and locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based scene reconstruction from multi-view images and integrates these environments into simulations that support ego-centric visual perception and mesh-based physical interactions. To demonstrate its effectiveness, we train a reinforcement learning policy within the simulator to perform a visual goal-tracking task. Extensive experiments show that our framework achieves RGB-only sim-to-real policy transfer. Additionally, our framework facilitates the rapid adaptation of robot policies with effective exploration capability in complex new environments, highlighting its potential for applications in households and factories."
      },
      {
        "id": "oai:arXiv.org:2502.03701v2",
        "title": "First-ish Order Methods: Hessian-aware Scalings of Gradient Descent",
        "link": "https://arxiv.org/abs/2502.03701",
        "author": "Oscar Smee, Fred Roosta, Stephen J. Wright",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03701v2 Announce Type: replace-cross \nAbstract: Gradient descent is the primary workhorse for optimizing large-scale problems in machine learning. However, its performance is highly sensitive to the choice of the learning rate. A key limitation of gradient descent is its lack of natural scaling, which often necessitates expensive line searches or heuristic tuning to determine an appropriate step size. In this paper, we address this limitation by incorporating Hessian information to scale the gradient direction. By accounting for the curvature of the function along the gradient, our adaptive, Hessian-aware scaling method ensures a local unit step size guarantee, even in nonconvex settings. Near a local minimum that satisfies the second-order sufficient conditions, our approach achieves linear convergence with a unit step size. We show that our method converges globally under a significantly weaker version of the standard Lipschitz gradient smoothness assumption. Even when Hessian information is inexact, the local unit step size guarantee and global convergence properties remain valid under mild conditions. Finally, we validate our theoretical results empirically on a range of convex and nonconvex machine learning tasks, showcasing the effectiveness of the approach."
      },
      {
        "id": "oai:arXiv.org:2502.06200v3",
        "title": "On the query complexity of sampling from non-log-concave distributions",
        "link": "https://arxiv.org/abs/2502.06200",
        "author": "Yuchen He, Chihao Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06200v3 Announce Type: replace-cross \nAbstract: We study the problem of sampling from a $d$-dimensional distribution with density $p(x)\\propto e^{-f(x)}$, which does not necessarily satisfy good isoperimetric conditions.\n  Specifically, we show that for any $L,M$ satisfying $LM\\ge d\\ge 5$, $\\epsilon\\in \\left(0,\\frac{1}{32}\\right)$, and any algorithm with query accesses to the value of $f(x)$ and $\\nabla f(x)$, there exists an $L$-log-smooth distribution with second moment at most $M$ such that the algorithm requires $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\Omega(d)}$ queries to compute a sample whose distribution is within $\\epsilon$ in total variation distance to the target distribution. We complement the lower bound with an algorithm requiring $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\mathcal{O}(d)}$ queries, thereby characterizing the tight (up to the constant in the exponent) query complexity for sampling from the family of non-log-concave distributions.\n  Our results are in sharp contrast with the recent work of Huang et al. (COLT'24), where an algorithm with quasi-polynomial query complexity was proposed for sampling from a non-log-concave distribution when $M=\\mathtt{poly}(d)$. Their algorithm works under the stronger condition that all distributions along the trajectory of the Ornstein-Uhlenbeck process, starting from the target distribution, are $\\mathcal{O}(1)$-log-smooth. We investigate this condition and prove that it is strictly stronger than requiring the target distribution to be $\\mathcal O(1)$-log-smooth. Additionally, we study this condition in the context of mixtures of Gaussians.\n  Finally, we place our results within the broader theme of ``sampling versus optimization'', as studied in Ma et al. (PNAS'19). We show that for a wide range of parameters, sampling is strictly easier than optimization by a super-exponential factor in the dimension $d$."
      },
      {
        "id": "oai:arXiv.org:2502.06536v2",
        "title": "Sample-efficient Learning of Concepts with Theoretical Guarantees: from Data to Concepts without Interventions",
        "link": "https://arxiv.org/abs/2502.06536",
        "author": "Hidde Fokkema, Tim van Erven, Sara Magliacane",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06536v2 Announce Type: replace-cross \nAbstract: Machine learning is a vital part of many real-world systems, but several concerns remain about the lack of interpretability, explainability and robustness of black-box AI systems. Concept Bottleneck Models (CBM) address some of these challenges by learning interpretable concepts from high-dimensional data, e.g. images, which are used to predict labels. An important issue in CBMs are spurious correlation between concepts, which effectively lead to learning \"wrong\" concepts. Current mitigating strategies have strong assumptions, e.g., they assume that the concepts are statistically independent of each other, or require substantial interaction in terms of both interventions and labels provided by annotators. In this paper, we describe a framework that provides theoretical guarantees on the correctness of the learned concepts and on the number of required labels, without requiring any interventions. Our framework leverages causal representation learning (CRL) methods to learn latent causal variables from high-dimensional observations in a unsupervised way, and then learns to align these variables with interpretable concepts with few concept labels. We propose a linear and a non-parametric estimator for this mapping, providing a finite-sample high probability result in the linear case and an asymptotic consistency result for the non-parametric estimator. We evaluate our framework in synthetic and image benchmarks, showing that the learned concepts have less impurities and are often more accurate than other CBMs, even in settings with strong correlations between concepts."
      },
      {
        "id": "oai:arXiv.org:2502.11657v2",
        "title": "How does ion temperature gradient turbulence depend on magnetic geometry? Insights from data and machine learning",
        "link": "https://arxiv.org/abs/2502.11657",
        "author": "Matt Landreman, Jong Youl Choi, Caio Alves, Prasanna Balaprakash, R. Michael Churchill, Rory Conlin, Gareth Roberg-Clark",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11657v2 Announce Type: replace-cross \nAbstract: Magnetic geometry has a significant effect on the level of turbulent transport in fusion plasmas. Here, we model and analyze this dependence using multiple machine learning methods and a dataset of > 200,000 nonlinear simulations of ion-temperature-gradient turbulence in diverse non-axisymmetric geometries. The dataset is generated using a large collection of both optimized and randomly generated stellarator equilibria. At fixed gradients, the turbulent heat flux varies between geometries by several orders of magnitude. Trends are apparent among the configurations with particularly high or low heat flux. Regression and classification techniques from machine learning are then applied to extract patterns in the dataset. Due to a symmetry of the gyrokinetic equation, the heat flux and regressions thereof should be invariant to translations of the raw features in the parallel coordinate, similar to translation invariance in computer vision applications. Multiple regression models including convolutional neural networks (CNNs) and decision trees can achieve reasonable predictive power for the heat flux in held-out test configurations, with highest accuracy for the CNNs. Using Spearman correlation, sequential feature selection, and Shapley values to measure feature importance, it is consistently found that the most important geometric lever on the heat flux is the flux surface compression in regions of bad curvature. The second most important feature relates to the magnitude of geodesic curvature. These two features align remarkably with surrogates that have been proposed based on theory, while the methods here allow a natural extension to more features for increased accuracy. The dataset, released with this publication, may also be used to test other proposed surrogates, and we find many previously published proxies do correlate well with both the heat flux and stability boundary."
      },
      {
        "id": "oai:arXiv.org:2502.14753v2",
        "title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders",
        "link": "https://arxiv.org/abs/2502.14753",
        "author": "Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay Chaudhari",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14753v2 Announce Type: replace-cross \nAbstract: Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. Our code is available at https://github.com/StanfordMIMI/MedVAE."
      },
      {
        "id": "oai:arXiv.org:2502.15806v2",
        "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos",
        "link": "https://arxiv.org/abs/2502.15806",
        "author": "Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15806v2 Announce Type: replace-cross \nAbstract: Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content."
      },
      {
        "id": "oai:arXiv.org:2502.16810v2",
        "title": "Grounded Persuasive Language Generation for Automated Marketing",
        "link": "https://arxiv.org/abs/2502.16810",
        "author": "Jibang Wu, Chenghao Yang, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16810v2 Announce Type: replace-cross \nAbstract: This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin while maintaining the same level of factual accuracy. Our findings suggest a promising agentic approach to automate large-scale targeted marketing while ensuring factuality of content generation."
      },
      {
        "id": "oai:arXiv.org:2503.03184v2",
        "title": "PAC Learning with Improvements",
        "link": "https://arxiv.org/abs/2503.03184",
        "author": "Idan Attias, Avrim Blum, Keziah Naggita, Donya Saless, Dravyansh Sharma, Matthew Walter",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.03184v2 Announce Type: replace-cross \nAbstract: One of the most basic lower bounds in machine learning is that in nearly any nontrivial setting, it takes $\\textit{at least}$ $1/\\epsilon$ samples to learn to error $\\epsilon$ (and more, if the classifier being learned is complex). However, suppose that data points are agents who have the ability to improve by a small amount if doing so will allow them to receive a (desired) positive classification. In that case, we may actually be able to achieve $\\textit{zero}$ error by just being \"close enough\". For example, imagine a hiring test used to measure an agent's skill at some job such that for some threshold $\\theta$, agents who score above $\\theta$ will be successful and those who score below $\\theta$ will not (i.e., learning a threshold on the line). Suppose also that by putting in effort, agents can improve their skill level by some small amount $r$. In that case, if we learn an approximation $\\hat{\\theta}$ of $\\theta$ such that $\\theta \\leq \\hat{\\theta} \\leq \\theta + r$ and use it for hiring, we can actually achieve error zero, in the sense that (a) any agent classified as positive is truly qualified, and (b) any agent who truly is qualified can be classified as positive by putting in effort. Thus, the ability for agents to improve has the potential to allow for a goal one could not hope to achieve in standard models, namely zero error.\n  In this paper, we explore this phenomenon more broadly, giving general results and examining under what conditions the ability of agents to improve can allow for a reduction in the sample complexity of learning, or alternatively, can make learning harder. We also examine both theoretically and empirically what kinds of improvement-aware algorithms can take into account agents who have the ability to improve to a limited extent when it is in their interest to do so."
      },
      {
        "id": "oai:arXiv.org:2503.05024v4",
        "title": "Kernel-based estimators for functional causal effects",
        "link": "https://arxiv.org/abs/2503.05024",
        "author": "Yordan P. Raykov, Hengrui Luo, Justin D. Strait, Wasiur R. KhudaBukhsh",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.05024v4 Announce Type: replace-cross \nAbstract: We propose causal effect estimators based on empirical Fr\\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators.\n  Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings."
      },
      {
        "id": "oai:arXiv.org:2503.07352v2",
        "title": "Score-informed Music Source Separation: Improving Synthetic-to-real Generalization in Classical Music",
        "link": "https://arxiv.org/abs/2503.07352",
        "author": "Eetu Tunturi, David Diaz-Guerra, Archontis Politis, Tuomas Virtanen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.07352v2 Announce Type: replace-cross \nAbstract: Music source separation is the task of separating a mixture of instruments into constituent tracks. Music source separation models are typically trained using only audio data, although additional information can be used to improve the model's separation capability. In this paper, we propose two ways of using musical scores to aid music source separation: a score-informed model where the score is concatenated with the magnitude spectrogram of the audio mixture as the input of the model, and a model where we use only the score to calculate the separation mask. We train our models on synthetic data in the SynthSOD dataset and evaluate our methods on the URMP and Aalto anechoic orchestra datasets, comprised of real recordings. The score-informed model improves separation results compared to a baseline approach, but struggles to generalize from synthetic to real data, whereas the score-only model shows a clear improvement in synthetic-to-real generalization."
      },
      {
        "id": "oai:arXiv.org:2503.14353v2",
        "title": "Unified Analysis of Decentralized Gradient Descent: a Contraction Mapping Framework",
        "link": "https://arxiv.org/abs/2503.14353",
        "author": "Erik G. Larsson, Nicolo Michelusi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.14353v2 Announce Type: replace-cross \nAbstract: The decentralized gradient descent (DGD) algorithm, and its sibling, diffusion, are workhorses in decentralized machine learning, distributed inference and estimation, and multi-agent coordination. We propose a novel, principled framework for the analysis of DGD and diffusion for strongly convex, smooth objectives, and arbitrary undirected topologies, using contraction mappings coupled with a result called the mean Hessian theorem (MHT). The use of these tools yields tight convergence bounds, both in the noise-free and noisy regimes. While these bounds are qualitatively similar to results found in the literature, our approach using contractions together with the MHT decouples the algorithm dynamics (how quickly the algorithm converges to its fixed point) from its asymptotic convergence properties (how far the fixed point is from the global optimum). This yields a simple, intuitive analysis that is accessible to a broader audience. Extensions are provided to multiple local gradient updates, time-varying step sizes, noisy gradients (stochastic DGD and diffusion), communication noise, and random topologies."
      },
      {
        "id": "oai:arXiv.org:2503.16724v2",
        "title": "Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models",
        "link": "https://arxiv.org/abs/2503.16724",
        "author": "Zhaoxin Li, Zhang Xi-Jia, Batuhan Altundas, Letian Chen, Rohan Paleja, Matthew Gombolay",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16724v2 Announce Type: replace-cross \nAbstract: Semantic interpretability in Reinforcement Learning (RL) enables transparency and verifiability by making the agent's decisions understandable and verifiable. Achieving this, however, requires a feature space composed of human-understandable concepts, which traditionally rely on human specification and may fail to generalize to unseen environments. We introduce interpretable Tree-based Reinforcement learning via Automated Concept Extraction (iTRACE), an automated framework that leverages pre-trained vision-language models (VLM) for semantic feature extraction and interpretable tree-based models for policy optimization. iTRACE first extracts semantically meaningful features, then maps them to policies via interpretable trees. To address the impracticality of running VLMs in RL loops, we distill their outputs into a lightweight model. By leveraging Vision-Language Models (VLMs) to automate tree-based reinforcement learning, iTRACE eliminates the need for human annotation traditionally required by interpretable models, while also addressing the limitations of VLMs alone, such as their lack of grounding in action spaces and inability to directly optimize policies. iTRACE outperforms MLP baselines that use the same interpretable features and matches the performance of CNN-based policies, producing verifiable, semantically interpretable, and human-aligned behaviors without requiring human annotation."
      },
      {
        "id": "oai:arXiv.org:2503.24160v3",
        "title": "A Comparative Study of Scanpath Models in Graph-Based Visualization",
        "link": "https://arxiv.org/abs/2503.24160",
        "author": "Angela Lopez-Cardona, Parvin Emami, Sebastian Idesis, Saravanakumar Duraisamy, Luis A. Leiva, Ioannis Arapakis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24160v3 Announce Type: replace-cross \nAbstract: Information Visualization (InfoVis) systems utilize visual representations to enhance data interpretation. Understanding how visual attention is allocated is essential for optimizing interface design. However, collecting Eye-tracking (ET) data presents challenges related to cost, privacy, and scalability. Computational models provide alternatives for predicting gaze patterns, thereby advancing InfoVis research. In our study, we conducted an ET experiment with 40 participants who analyzed graphs while responding to questions of varying complexity within the context of digital forensics. We compared human scanpaths with synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer. Our research evaluates the accuracy of these models and examines how question complexity and number of nodes influence performance. This work contributes to the development of predictive modeling in visual analytics, offering insights that can enhance the design and effectiveness of InfoVis systems."
      },
      {
        "id": "oai:arXiv.org:2504.02628v2",
        "title": "Towards Computation- and Communication-efficient Computational Pathology",
        "link": "https://arxiv.org/abs/2504.02628",
        "author": "Chu Han, Bingchao Zhao, Jiatai Lin, Shanshan Lyu, Longfei Wang, Tianpeng Deng, Cheng Lu, Changhong Liang, Hannah Y. Wen, Xiaojing Guo, Zhenwei Shi, Zaiyi Liu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02628v2 Announce Type: replace-cross \nAbstract: Despite the impressive performance across a wide range of applications, current computational pathology models face significant diagnostic efficiency challenges due to their reliance on high-magnification whole-slide image analysis. This limitation severely compromises their clinical utility, especially in time-sensitive diagnostic scenarios and situations requiring efficient data transfer. To address these issues, we present a novel computation- and communication-efficient framework called Magnification-Aligned Global-Local Transformer (MAG-GLTrans). Our approach significantly reduces computational time, file transfer requirements, and storage overhead by enabling effective analysis using low-magnification inputs rather than high-magnification ones. The key innovation lies in our proposed magnification alignment (MAG) mechanism, which employs self-supervised learning to bridge the information gap between low and high magnification levels by effectively aligning their feature representations. Through extensive evaluation across various fundamental CPath tasks, MAG-GLTrans demonstrates state-of-the-art classification performance while achieving remarkable efficiency gains: up to 10.7 times reduction in computational time and over 20 times reduction in file transfer and storage requirements. Furthermore, we highlight the versatility of our MAG framework through two significant extensions: (1) its applicability as a feature extractor to enhance the efficiency of any CPath architecture, and (2) its compatibility with existing foundation models and histopathology-specific encoders, enabling them to process low-magnification inputs with minimal information loss. These advancements position MAG-GLTrans as a particularly promising solution for time-sensitive applications, especially in the context of intraoperative frozen section diagnosis where both accuracy and efficiency are paramount."
      },
      {
        "id": "oai:arXiv.org:2504.02856v2",
        "title": "The epistemic dimension of algorithmic fairness: assessing its impact in innovation diffusion and fair policy making",
        "link": "https://arxiv.org/abs/2504.02856",
        "author": "Eugenia Villa, Camilla Quaresmini, Valentina Breschi, Viola Schiaffonati, Mara Tanelli",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02856v2 Announce Type: replace-cross \nAbstract: Algorithmic fairness is an expanding field that addresses a range of discrimination issues associated with algorithmic processes. However, most works in the literature focus on analyzing it only from an ethical perspective, focusing on moral principles and values that should be considered in the design and evaluation of algorithms, while disregarding the epistemic dimension related to knowledge transmission and validation. However, this aspect of algorithmic fairness should also be included in the debate, as it is crucial to introduce a specific type of harm: an individual may be systematically excluded from the dissemination of knowledge due to the attribution of a credibility deficit/excess. In this work, we specifically focus on characterizing and analyzing the impact of this credibility deficit or excess on the diffusion of innovations on a societal scale, a phenomenon driven by individual attitudes and social interactions, and also by the strength of mutual connections. Indeed, discrimination might shape the latter, ultimately modifying how innovations spread within the network. In this light, to incorporate, also from a formal point of view, the epistemic dimension in innovation diffusion models becomes paramount, especially if these models are intended to support fair policy design. For these reasons, we formalize the epistemic properties of a social environment, by extending the well-established Linear Threshold Model (LTM) in an epistemic direction to show the impact of epistemic biases in innovation diffusion. Focusing on the impact of epistemic bias in both open-loop and closed-loop scenarios featuring optimal fostering policies, our results shed light on the pivotal role the epistemic dimension might have in the debate of algorithmic fairness in decision-making."
      },
      {
        "id": "oai:arXiv.org:2504.08335v2",
        "title": "Entropic bounds for conditionally Gaussian vectors and applications to neural networks",
        "link": "https://arxiv.org/abs/2504.08335",
        "author": "Lucia Celli, Giovanni Peccati",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08335v2 Announce Type: replace-cross \nAbstract: Using entropic inequalities from information theory, we provide new bounds on the total variation and 2-Wasserstein distances between a conditionally Gaussian law and a Gaussian law with invertible covariance matrix. We apply our results to quantify the speed of convergence to Gaussian of a randomly initialized fully connected neural network and its derivatives - evaluated in a finite number of inputs - when the initialization is Gaussian and the sizes of the inner layers diverge to infinity. Our results require mild assumptions on the activation function, and allow one to recover optimal rates of convergence in a variety of distances, thus improving and extending the findings of Basteri and Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al. (2024). One of our main tools are the quantitative cumulant estimates established in Hanin (2024). As an illustration, we apply our results to bound the total variation distance between the Bayesian posterior law of the neural network and its derivatives, and the posterior law of the corresponding Gaussian limit: this yields quantitative versions of a posterior CLT by Hron et al. (2022), and extends several estimates by Trevisan (2024) to the total variation metric."
      },
      {
        "id": "oai:arXiv.org:2504.14906v3",
        "title": "OmniAudio: Generating Spatial Audio from 360-Degree Video",
        "link": "https://arxiv.org/abs/2504.14906",
        "author": "Huadai Liu, Tianyi Luo, Kaicheng Luo, Qikai Jiang, Peiwen Sun, Jialei Wang, Rongjie Huang, Qian Chen, Wen Wang, Xiangtai Li, Shiliang Zhang, Zhijie Yan, Zhou Zhao, Wei Xue",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.14906v3 Announce Type: replace-cross \nAbstract: Traditional video-to-audio generation techniques primarily focus on perspective video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, 360V2SA, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create Sphere360, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a dual-branch framework that utilizes both panoramic and perspective video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that OmniAudio achieves state-of-the-art performance across both objective and subjective metrics on Sphere360. Code and datasets are available at https://github.com/liuhuadai/OmniAudio. The project website is available at https://OmniAudio-360V2SA.github.io."
      },
      {
        "id": "oai:arXiv.org:2505.05098v2",
        "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models",
        "link": "https://arxiv.org/abs/2505.05098",
        "author": "Wei Liu, Jiyuan Zhang, Binxiong Zheng, Yufeng Hu, Yingzhan Lin, Zengfeng Zeng",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.05098v2 Announce Type: replace-cross \nAbstract: End-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. However, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. In this paper, we introduce X-Driver, a unified multi-modal large language models(MLLMs) framework designed for closed-loop autonomous driving, leveraging Chain-of-Thought(CoT) and autoregressive modeling to enhance perception and decision-making. We validate X-Driver across multiple autonomous driving tasks using public benchmarks in CARLA simulation environment, including Bench2Drive[6]. Our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(SOTA) while improving the interpretability of driving decisions. These findings underscore the importance of structured reasoning in end-to-end driving and establish X-Driver as a strong baseline for future research in closed-loop autonomous driving."
      },
      {
        "id": "oai:arXiv.org:2505.07802v2",
        "title": "Improving Trajectory Stitching with Flow Models",
        "link": "https://arxiv.org/abs/2505.07802",
        "author": "Reece O'Mahoney, Wanming Yu, Ioannis Havoutis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.07802v2 Announce Type: replace-cross \nAbstract: Generative models have shown great promise as trajectory planners, given their affinity to modeling complex distributions and guidable inference process. Previous works have successfully applied these in the context of robotic manipulation but perform poorly when the required solution does not exist as a complete trajectory within the training set. We identify that this is a result of being unable to plan via stitching, and subsequently address the architectural and dataset choices needed to remedy this. On top of this, we propose a novel addition to the training and inference procedures to both stabilize and enhance these capabilities. We demonstrate the efficacy of our approach by generating plans with out of distribution boundary conditions and performing obstacle avoidance on the Franka Panda in simulation and on real hardware. In both of these tasks our method performs significantly better than the baselines and is able to avoid obstacles up to four times as large."
      },
      {
        "id": "oai:arXiv.org:2505.10640v2",
        "title": "The Hitchhikers Guide to Production-ready Trustworthy Foundation Model powered Software (FMware)",
        "link": "https://arxiv.org/abs/2505.10640",
        "author": "Kirill Vasilevski, Benjamin Rombaut, Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Keheliya Gallaba, Filipe R. Cogo, Jiahuei Lin, Dayi Lin, Haoxiang Zhang, Bouyan Chen, Kishanthan Thangarajah, Ahmed E. Hassan, Zhen Ming Jiang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.10640v2 Announce Type: replace-cross \nAbstract: Foundation Models (FMs) such as Large Language Models (LLMs) are reshaping the software industry by enabling FMware, systems that integrate these FMs as core components. In this KDD 2025 tutorial, we present a comprehensive exploration of FMware that combines a curated catalogue of challenges with real-world production concerns. We first discuss the state of research and practice in building FMware. We further examine the difficulties in selecting suitable models, aligning high-quality domain-specific data, engineering robust prompts, and orchestrating autonomous agents. We then address the complex journey from impressive demos to production-ready systems by outlining issues in system testing, optimization, deployment, and integration with legacy software. Drawing on our industrial experience and recent research in the area, we provide actionable insights and a technology roadmap for overcoming these challenges. Attendees will gain practical strategies to enable the creation of trustworthy FMware in the evolving technology landscape."
      },
      {
        "id": "oai:arXiv.org:2505.13556v2",
        "title": "Learning Collision Risk from Naturalistic Driving with Generalised Surrogate Safety Measures",
        "link": "https://arxiv.org/abs/2505.13556",
        "author": "Yiru Jiao, Simeon C. Calvert, Sander van Cranenburgh, Hans van Lint",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.13556v2 Announce Type: replace-cross \nAbstract: Accurate and timely alerts for drivers or automated systems to unfolding collisions remains a challenge in road safety, particularly in highly interactive urban traffic. Existing approaches require labour-intensive annotation of sparse risk, struggle to consider varying contextual factors, or are useful only in the scenarios they are designed for. To address these limits, this study introduces the generalised surrogate safety measure (GSSM), a new approach that learns exclusively from naturalistic driving without crash or risk labels. GSSM captures the patterns of normal driving and estimates the extent to which a traffic interaction deviates from the norm towards unsafe extreme. Utilising neural networks, normal interactions are characterised by context-conditioned distributions of multi-directional spacing between road users. In the same interaction context, a spacing closer than normal entails higher risk of potential collision. Then a context-adaptive risk score and its associated probability can be calculated based on the theory of extreme values. Any measurable factors, such as motion kinematics, weather, lighting, can serve as part of the context, allowing for diverse coverage of safety-critical interactions. Multiple public driving datasets are used to train GSSMs, which are tested with 2,591 real-world crashes and near-crashes reconstructed from the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of 0.9 and secures a median time advance of 2.6 seconds to prevent potential collisions. Additional data and contextual factors provide further performance gains. Across various interaction types such as rear-end, merging, and crossing, the accuracy and timeliness of GSSM consistently outperforms existing baselines. GSSM therefore establishes a scalable, context-aware, and generalisable foundation to proactively quantify collision risk in traffic interactions."
      },
      {
        "id": "oai:arXiv.org:2505.13887v3",
        "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation",
        "link": "https://arxiv.org/abs/2505.13887",
        "author": "Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.13887v3 Announce Type: replace-cross \nAbstract: The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation."
      },
      {
        "id": "oai:arXiv.org:2505.14806v3",
        "title": "Place Cells as Proximity-Preserving Embeddings: From Multi-Scale Random Walk to Straight-Forward Path Planning",
        "link": "https://arxiv.org/abs/2505.14806",
        "author": "Minglu Zhao, Dehong Xu, Deqian Kong, Wen-Hao Zhang, Ying Nian Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.14806v3 Announce Type: replace-cross \nAbstract: The hippocampus enables spatial navigation through place cell populations forming cognitive maps. We propose proximity-preserving neural embeddings to encode multi-scale random walk transitions, where the inner product $\\langle h(x, t), h(y, t) \\rangle = q(y|x, t)$ represents normalized transition probabilities, with $h(x, t)$ as the embedding at location $x$ and $q(y|x, t)$ as the transition probability at scale $\\sqrt{t}$. This scale hierarchy mirrors hippocampal dorsoventral organization. The embeddings $h(x, t)$ reduce pairwise spatial proximity into an environmental map, with Euclidean distances preserving proximity information. We use gradient ascent on $q(y|x, t)$ for straight-forward path planning, employing adaptive scale selection for trap-free, smooth trajectories, equivalent to minimizing embedding space distances. Matrix squaring ($P_{2t} = P_t^2$) efficiently builds global transitions from local ones ($P_1$), enabling preplay-like shortcut prediction. Experiments demonstrate localized place fields, multi-scale tuning, adaptability, and remapping, achieving robust navigation in complex environments. Our biologically plausible framework, extensible to theta-phase precession, unifies spatial and temporal coding for scalable navigation."
      },
      {
        "id": "oai:arXiv.org:2505.16946v2",
        "title": "NY Real Estate Racial Equity Analysis via Applied Machine Learning",
        "link": "https://arxiv.org/abs/2505.16946",
        "author": "Sanjana Chalavadi, Andrei Pastor, Terry Leitch",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16946v2 Announce Type: replace-cross \nAbstract: This study analyzes tract-level real estate ownership patterns in New York State (NYS) and New York City (NYC) to uncover racial disparities. We use an advanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering, validated at 89.2% accuracy) to compare the predicted racial composition of property owners to the resident population from census data. We examine both a Full Model (statewide) and a Name-Only LSTM Model (NYC) to assess how incorporating geospatial context affects our predictions and disparity estimates. The results reveal significant inequities: White individuals hold a disproportionate share of properties and property value relative to their population, while Black, Hispanic, and Asian communities are underrepresented as property owners. These disparities are most pronounced in minority-majority neighborhoods, where ownership is predominantly White despite a predominantly non-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates these gaps by reducing owner-occupied opportunities in urban minority communities. We provide a breakdown of ownership vs. population by race for majority-White, -Black, -Hispanic, and -Asian tracts, identify those with extreme ownership disparities, and compare patterns in urban, suburban, and rural contexts. The findings underscore persistent racial inequity in property ownership, reflecting broader historical and socio-economic forces, and highlight the importance of data-driven approaches to address these issues."
      },
      {
        "id": "oai:arXiv.org:2505.18780v2",
        "title": "One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion",
        "link": "https://arxiv.org/abs/2505.18780",
        "author": "Yahao Fan, Tianxiang Gui, Kaiyang Ji, Shutong Ding, Chixuan Zhang, Jiayuan Gu, Jingyi Yu, Jingya Wang, Ye Shi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.18780v2 Announce Type: replace-cross \nAbstract: Humanoid locomotion faces a critical scalability challenge: traditional reinforcement learning (RL) methods require task-specific rewards and struggle to leverage growing datasets, even as more training terrains are introduced. We propose DreamPolicy, a unified framework that enables a single policy to master diverse terrains and generalize zero-shot to unseen scenarios by systematically integrating offline data and diffusion-driven motion synthesis. At its core, DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions synthesized through an autoregressive terrain-aware diffusion planner curated by aggregating rollouts from specialized policies across various distinct terrains. Unlike human motion datasets requiring laborious retargeting, our data directly captures humanoid kinematics, enabling the diffusion planner to synthesize \"dreamed\" trajectories that encode terrain-specific physical constraints. These trajectories act as dynamic objectives for our HMI-conditioned policy, bypassing manual reward engineering and enabling cross-terrain generalization. DreamPolicy addresses the scalability limitations of prior methods: while traditional RL fails to exploit growing datasets, our framework scales seamlessly with more offline data. As the dataset expands, the diffusion prior learns richer locomotion skills, which the policy leverages to master new terrains without retraining. Experiments demonstrate that DreamPolicy achieves average 90% success rates in training environments and an average of 20% higher success on unseen terrains than the prevalent method. It also generalizes to perturbed and composite scenarios where prior approaches collapse. By unifying offline data, diffusion-based trajectory synthesis, and policy optimization, DreamPolicy overcomes the \"one task, one policy\" bottleneck, establishing a paradigm for scalable, data-driven humanoid control."
      },
      {
        "id": "oai:arXiv.org:2505.19145v2",
        "title": "Do Large Language Models (Really) Need Statistical Foundations?",
        "link": "https://arxiv.org/abs/2505.19145",
        "author": "Weijie Su",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19145v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research."
      },
      {
        "id": "oai:arXiv.org:2505.19381v4",
        "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving",
        "link": "https://arxiv.org/abs/2505.19381",
        "author": "Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Yunda Dong, Zongzheng Zhang, Xianda Guo, Hao Sun, Hao Zhao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19381v4 Announce Type: replace-cross \nAbstract: Research interest in end-to-end autonomous driving has surged owing to its fully differentiable design integrating modular tasks, i.e. perception, prediction and planing, which enables optimization in pursuit of the ultimate goal. Despite the great potential of the end-to-end paradigm, existing methods suffer from several aspects including expensive BEV (bird's eye view) computation, action diversity, and sub-optimal decision in complex real-world scenarios. To address these challenges, we propose a novel hybrid sparse-dense diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA. We explore the sparse diffusion representation for efficient multi-modal driving behavior. Moreover, we rethink the effectiveness of VLM driving decision and improve the trajectory generation guidance through deep interaction across agent, map instances and VLM output. Our method shows superior performance in Autonomous Grand Challenge 2025 which contains challenging real and reactive synthetic scenarios. Our methods achieves 45.0 PDMS."
      },
      {
        "id": "oai:arXiv.org:2505.23240v2",
        "title": "Joint estimation of smooth graph signals from partial linear measurements",
        "link": "https://arxiv.org/abs/2505.23240",
        "author": "Hemant Tyagi",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23240v2 Announce Type: replace-cross \nAbstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each vertex $t$ has a latent signal $x_t \\in \\mathbb{R}^n$ associated to it. Given partial linear measurements of the signals, for a potentially small subset of the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are smooth w.r.t $G$, in the sense that the quadratic variation of the signals over the graph is small, we obtain non-asymptotic bounds on the mean squared error for jointly recovering $x_t$'s, for the smoothness penalized least squares estimator. In particular, this implies for certain choices of $G$ that this estimator is weakly consistent (as $T \\rightarrow \\infty$) under potentially very stringent sampling, where only one coordinate is measured per vertex for a vanishingly small fraction of the vertices. The results are extended to a ``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths of a collection of $n$ items, and noisy pairwise difference measurements are obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency is established for certain choices of $G$ even when the individual $G_t$'s are very sparse and disconnected."
      },
      {
        "id": "oai:arXiv.org:2505.23503v2",
        "title": "Can Large Language Models Challenge CNNs in Medical Image Analysis?",
        "link": "https://arxiv.org/abs/2505.23503",
        "author": "Shibbir Ahmed, Shahnewaz Karim Sakib, Anindya Bijoy Das",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23503v2 Announce Type: replace-cross \nAbstract: This study presents a multimodal AI framework designed for precisely classifying medical diagnostic images. Utilizing publicly available datasets, the proposed system compares the strengths of convolutional neural networks (CNNs) and different large language models (LLMs). This in-depth comparative analysis highlights key differences in diagnostic performance, execution efficiency, and environmental impacts. Model evaluation was based on accuracy, F1-score, average execution time, average energy consumption, and estimated $CO_2$ emission. The findings indicate that although CNN-based models can outperform various multimodal techniques that incorporate both images and contextual information, applying additional filtering on top of LLMs can lead to substantial performance gains. These findings highlight the transformative potential of multimodal AI systems to enhance the reliability, efficiency, and scalability of medical diagnostics in clinical settings."
      },
      {
        "id": "oai:arXiv.org:2505.24200v2",
        "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC",
        "link": "https://arxiv.org/abs/2505.24200",
        "author": "Qingzheng Wang, Jiancheng Sun, Yifan Peng, Shinji Watanabe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24200v2 Announce Type: replace-cross \nAbstract: Multilingual speech processing with self-supervised or supervised pre-trained Speech Foundation Models (SFM) has achieved strong performance on tasks like Language Identification (LID) and Automatic Speech Recognition (ASR). However, these models struggle with limited resources during fine-tuning. This paper enhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple strategies for adapting SFMs, including frozen upstream training, partial fine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation to mitigate performance gaps in few-shot settings and introduce LID Connectionist Temporal Classification (CTC) loss for regularization. Our approach achieves a 14% relative improvement in LID accuracy and a 30% relative reduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place in the Interspeech 2025 ML-SUPERB 2.0 Challenge."
      },
      {
        "id": "oai:arXiv.org:2505.24407v2",
        "title": "Efficient RAW Image Deblurring with Adaptive Frequency Modulation",
        "link": "https://arxiv.org/abs/2505.24407",
        "author": "Wenlong Jiao, Binglong Li, Wei Shang, Ping Wang, Dongwei Ren",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24407v2 Announce Type: replace-cross \nAbstract: Image deblurring plays a crucial role in enhancing visual clarity across various applications. Although most deep learning approaches primarily focus on sRGB images, which inherently lose critical information during the image signal processing pipeline, RAW images, being unprocessed and linear, possess superior restoration potential but remain underexplored. Deblurring RAW images presents unique challenges, particularly in handling frequency-dependent blur while maintaining computational efficiency. To address these issues, we propose Frequency Enhanced Network (FrENet), a framework specifically designed for RAW-to-RAW deblurring that operates directly in the frequency domain. We introduce a novel Adaptive Frequency Positional Modulation module, which dynamically adjusts frequency components according to their spectral positions, thereby enabling precise control over the deblurring process. Additionally, frequency domain skip connections are adopted to further preserve high-frequency details. Experimental results demonstrate that FrENet surpasses state-of-the-art deblurring methods in RAW image deblurring, achieving significantly better restoration quality while maintaining high efficiency in terms of reduced MACs. Furthermore, FrENet's adaptability enables it to be extended to sRGB images, where it delivers comparable or superior performance compared to methods specifically designed for sRGB data. The code will be available at https://github.com/WenlongJiao/FrENet ."
      },
      {
        "id": "oai:arXiv.org:2506.00095v2",
        "title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases",
        "link": "https://arxiv.org/abs/2506.00095",
        "author": "Yuchong Li, Xiaojun Zeng, Chihua Fang, Jian Yang, Fucang Jia, Lei Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00095v2 Announce Type: replace-cross \nAbstract: Hepato-pancreato-biliary (HPB) disorders represent a global public health challenge due to their high morbidity and mortality. Although large language models (LLMs) have shown promising performance in general medical question-answering tasks, the current evaluation benchmarks are mostly derived from standardized examinations or manually designed questions, lacking HPB coverage and clinical cases. To address these issues, we systematically eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended multiple-choice questions and 337 open-ended real diagnosis cases, which encompasses all the 33 main categories and 465 subcategories of HPB diseases defined in the International Statistical Classification of Diseases, 10th Revision (ICD-10). The multiple-choice questions are curated from public datasets and synthesized data, and the clinical cases are collected from prestigious medical journals, case-sharing platforms, and collaborating hospitals. By evalauting commercial and open-source general and medical LLMs on our established benchmark, namely ClinBench-HBP, we find that while commercial LLMs perform competently on medical exam questions, they exhibit substantial performance degradation on HPB diagnosis tasks, especially on complex, inpatient clinical cases. Those medical LLMs also show limited generalizability to HPB diseases. Our results reveal the critical limitations of current LLMs in the domain of HPB diseases, underscoring the imperative need for future medical LLMs to handle real, complex clinical diagnostics rather than simple medical exam questions. The benchmark will be released at the homepage."
      },
      {
        "id": "oai:arXiv.org:2506.00261v2",
        "title": "GPR: Empowering Generation with Graph-Pretrained Retriever",
        "link": "https://arxiv.org/abs/2506.00261",
        "author": "Xiaochen Wang, Zongyu Wu, Yuan Zhong, Xiang Zhang, Suhang Wang, Fenglong Ma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00261v2 Announce Type: replace-cross \nAbstract: Graph retrieval-augmented generation (GRAG) places high demands on graph-specific retrievers. However, existing retrievers often rely on language models pretrained on plain text, limiting their effectiveness due to domain misalignment and structure ignorance. To address these challenges, we propose GPR, a graph-based retriever pretrained directly on knowledge graphs. GPR aligns natural language questions with relevant subgraphs through LLM-guided graph augmentation and employs a structure-aware objective to learn fine-grained retrieval strategies. Experiments on two datasets, three LLM backbones, and five baselines show that GPR consistently improves both retrieval quality and downstream generation, demonstrating its effectiveness as a robust retrieval solution for GRAG."
      },
      {
        "id": "oai:arXiv.org:2506.01755v2",
        "title": "Data-assimilated model-informed reinforcement learning",
        "link": "https://arxiv.org/abs/2506.01755",
        "author": "Defne E. Ozan, Andrea N\\'ovoa, Georgios Rigas, Luca Magri",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01755v2 Announce Type: replace-cross \nAbstract: The control of spatio-temporally chaos is challenging because of high dimensionality and unpredictability. Model-free reinforcement learning (RL) discovers optimal control policies by interacting with the system, typically requiring observations of the full physical state. In practice, sensors often provide only partial and noisy measurements (observations) of the system. The objective of this paper is to develop a framework that enables the control of chaotic systems with partial and noisy observability. The proposed method, data-assimilated model-informed reinforcement learning (DA-MIRL), integrates (i) low-order models to approximate high-dimensional dynamics; (ii) sequential data assimilation to correct the model prediction when observations become available; and (iii) an off-policy actor-critic RL algorithm to adaptively learn an optimal control strategy based on the corrected state estimates. We test DA-MIRL on the spatiotemporally chaotic solutions of the Kuramoto-Sivashinsky equation. We estimate the full state of the environment with (i) a physics-based model, here, a coarse-grained model; and (ii) a data-driven model, here, the control-aware echo state network, which is proposed in this paper. We show that DA-MIRL successfully estimates and suppresses the chaotic dynamics of the environment in real time from partial observations and approximate models. This work opens opportunities for the control of partially observable chaotic systems."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Wed, 04 Jun 2025 04:02:02 +0000",
      "published": "Wed, 04 Jun 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2506.02039v1",
        "title": "No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction",
        "link": "https://arxiv.org/abs/2506.02039",
        "author": "Haoshuai Zhou, Changgeng Mo, Boxuan Cao, Linkai Li, Shan Xiang Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02039v1 Announce Type: new \nAbstract: Personalized speech intelligibility prediction is challenging. Previous approaches have mainly relied on audiograms, which are inherently limited in accuracy as they only capture a listener's hearing threshold for pure tones. Rather than incorporating additional listener features, we propose a novel approach that leverages an individual's existing intelligibility data to predict their performance on new audio. We introduce the Support Sample-Based Intelligibility Prediction Network (SSIPNet), a deep learning model that leverages speech foundation models to build a high-dimensional representation of a listener's speech recognition ability from multiple support (audio, score) pairs, enabling accurate predictions for unseen audio. Results on the Clarity Prediction Challenge dataset show that, even with a small number of support (audio, score) pairs, our method outperforms audiogram-based predictions. Our work presents a new paradigm for personalized speech intelligibility prediction."
      },
      {
        "id": "oai:arXiv.org:2506.02059v1",
        "title": "Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition",
        "link": "https://arxiv.org/abs/2506.02059",
        "author": "Ziwei Gong, Pengyuan Shi, Kaan Donbekci, Lin Ai, Run Chen, David Sasu, Zehui Wu, Julia Hirschberg",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02059v1 Announce Type: new \nAbstract: Speech Emotion Recognition (SER) has seen significant progress with deep learning, yet remains challenging for Low-Resource Languages (LRLs) due to the scarcity of annotated data. In this work, we explore unsupervised learning to improve SER in low-resource settings. Specifically, we investigate contrastive learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised approaches to enhance cross-lingual generalization. Our methods achieve notable F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla, demonstrating their effectiveness in LRLs. Additionally, we analyze model behavior to provide insights on key factors influencing performance across languages, and also highlighting challenges in low-resource SER. This work provides a foundation for developing more inclusive, explainable, and robust emotion recognition systems for underrepresented languages."
      },
      {
        "id": "oai:arXiv.org:2506.02078v1",
        "title": "Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data",
        "link": "https://arxiv.org/abs/2506.02078",
        "author": "Emmy Postma, Cristian Tejedor-Garcia",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02078v1 Announce Type: new \nAbstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD), motivating the development of diagnostic techniques using speech data for clinical applications. Although deep acoustic features have shown promise for PD classification, their effectiveness often varies due to individual speaker differences, a factor that has not been thoroughly explored in the existing literature. This study investigates the effectiveness of three pre-trained audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification. Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK) and listen and repeat (LR) tasks, capturing critical acoustic features for PD detection. Only Wav2Vec2.0 shows significant gender bias, achieving more favorable results for male speakers, in DDK tasks. The misclassified cases reveal challenges with atypical speech patterns, highlighting the need for improved feature extraction and model robustness in PD detection."
      },
      {
        "id": "oai:arXiv.org:2506.02080v1",
        "title": "Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge",
        "link": "https://arxiv.org/abs/2506.02080",
        "author": "Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02080v1 Announce Type: new \nAbstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research."
      },
      {
        "id": "oai:arXiv.org:2506.02082v1",
        "title": "SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction",
        "link": "https://arxiv.org/abs/2506.02082",
        "author": "Saurabh Agrawal, Raj Gohil, Gopal Kumar Agrawal, Vikram C M, Kushal Verma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02082v1 Announce Type: new \nAbstract: Speech quality assessment is a critical process in selecting text-to-speech synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can be done using objective metrics or subjective metrics. Although there are many objective metrics like the Perceptual Evaluation of Speech Quality (PESQ), Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time Objective Intelligibility (STOI) but none of them is feasible in selecting the best model. On the other hand subjective metric like Mean Opinion Score is highly reliable but it requires a lot of manual efforts and are time-consuming. To counter the issues in MOS Evaluation, we have developed a novel model, Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a small-sized, end-to-end, highly generalized and scalable model for predicting MOS score on a scale of 5. We use the sequences of convolutions and stack them to get the latent features of the audio samples to get the best state-of-the-art results based on mean squared error (MSE), Linear Concordance Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and Kendall Rank Correlation Coefficient (KTAU)."
      },
      {
        "id": "oai:arXiv.org:2506.02083v1",
        "title": "LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention",
        "link": "https://arxiv.org/abs/2506.02083",
        "author": "Aditya Srinivas Menon, Raj Prakash Gohil, Kumud Tripathi, Pankaj Wasnik",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02083v1 Announce Type: new \nAbstract: Speaker recognition models face challenges in multi-lingual settings due to the entanglement of linguistic information within speaker embeddings. The overlap between vocal traits such as accent, vocal anatomy, and a language's phonetic structure complicates separating linguistic and speaker information. Disentangling these components can significantly improve speaker recognition accuracy. To this end, we propose a novel disentanglement learning strategy that integrates joint learning through prefix-tuned cross-attention. This approach is particularly effective when speakers switch between languages. Experimental results show the model generalizes across monolingual and multi-lingual settings, including unseen languages. Notably, the proposed model improves the equal error rate across multiple datasets, highlighting its ability to separate language information from speaker embeddings and enhance recognition in diverse linguistic conditions."
      },
      {
        "id": "oai:arXiv.org:2506.02085v1",
        "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion",
        "link": "https://arxiv.org/abs/2506.02085",
        "author": "Ajinkya Kulkarni, Sandipana Dowerah, Tanel Alumae, Mathew Magimai. -Doss",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02085v1 Announce Type: new \nAbstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced AI. While current research focuses on discerning real speech from spoofed speech, tracing the source system is equally crucial. This work proposes a novel audio source tracing system combining deep metric multi-class N-pair loss with Real Emphasis and Fake Dispersion framework, a Conformer classification network, and ensemble score-embedding fusion. The N-pair loss improves discriminative ability, while Real Emphasis and Fake Dispersion enhance robustness by focusing on differentiating real and fake speech patterns. The Conformer network captures both global and local dependencies in the audio signal, crucial for source tracing. The proposed ensemble score-embedding fusion shows an optimal trade-off between in-domain and out-of-domain source tracing scenarios. We evaluate our method using Frechet Distance and standard metrics, demonstrating superior performance in source tracing over the baseline system."
      },
      {
        "id": "oai:arXiv.org:2506.02088v1",
        "title": "Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025",
        "link": "https://arxiv.org/abs/2506.02088",
        "author": "Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas \\'Olives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo Galv\\~ao Filho",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02088v1 Announce Type: new \nAbstract: Training SER models in natural, spontaneous speech is especially challenging due to the subtle expression of emotions and the unpredictable nature of real-world audio. In this paper, we present a robust system for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing on categorical emotion recognition. Our method combines state-of-the-art audio models with text features enriched by prosodic and spectral cues. In particular, we investigate the effectiveness of Fundamental Frequency (F0) quantization and the use of a pretrained audio tagging model. We also employ an ensemble model to improve robustness. On the official test set, our system achieved a Macro F1-score of 39.79% (42.20% on validation). Our results underscore the potential of these methods, and analysis of fusion techniques confirmed the effectiveness of Graph Attention Networks. Our source code is publicly available."
      },
      {
        "id": "oai:arXiv.org:2506.02091v1",
        "title": "Comparison of spectrogram scaling in multi-label Music Genre Recognition",
        "link": "https://arxiv.org/abs/2506.02091",
        "author": "Bartosz Karpi\\'nski, Cyryl Leszczy\\'nski",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02091v1 Announce Type: new \nAbstract: As the accessibility and ease-of-use of digital audio workstations increases, so does the quantity of music available to the average listener; additionally, differences between genres are not always well defined and can be abstract, with widely varying combinations of genres across individual records. In this article, multiple preprocessing methods and approaches to model training are described and compared, accounting for the eclectic nature of today's albums. A custom, manually labeled dataset of more than 18000 entries has been used to perform the experiments."
      },
      {
        "id": "oai:arXiv.org:2506.02166v1",
        "title": "Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi",
        "link": "https://arxiv.org/abs/2506.02166",
        "author": "Arnav Rustagi, Satvik Bajpai, Nimrat Kaur, Siddharth Siddharth",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02166v1 Announce Type: new \nAbstract: Computer-Assisted Pronunciation Training (CAPT) has been extensively studied for English. However, there remains a critical gap in its application to Indian languages with a base of 1.5 billion speakers. Pronunciation tools tailored to Indian languages are strikingly lacking despite the fact that millions learn them every year. With over 600 million speakers and being the fourth most-spoken language worldwide, improving Hindi pronunciation is a vital first step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT system for Hindi, 2) synthetic speech generation for Hindi mispronunciations, and 3) a novel methodology for providing personalized feedback to learners. While the system often interacts with learners using Devanagari graphemes, its core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic orthography to analyze mispronounced speech and provide targeted feedback."
      },
      {
        "id": "oai:arXiv.org:2506.02178v1",
        "title": "Cocktail-Party Audio-Visual Speech Recognition",
        "link": "https://arxiv.org/abs/2506.02178",
        "author": "Thai-Binh Nguyen, Ngoc-Quan Pham, Alexander Waibel",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02178v1 Announce Type: new \nAbstract: Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech recognition in challenging environments, such as cocktail-party scenarios, where relying solely on audio proves insufficient. However, current AVSR models are often optimized for idealized scenarios with consistently active speakers, overlooking the complexities of real-world settings that include both speaking and silent facial segments. This study addresses this gap by introducing a novel audio-visual cocktail-party dataset designed to benchmark current AVSR systems and highlight the limitations of prior approaches in realistic noisy conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising both talking-face and silent-face segments, enabling significant performance gains in cocktail-party environments. Our approach reduces WER by 67% relative to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise, without relying on explicit segmentation cues."
      },
      {
        "id": "oai:arXiv.org:2506.02230v1",
        "title": "Towards Machine Unlearning for Paralinguistic Speech Processing",
        "link": "https://arxiv.org/abs/2506.02230",
        "author": "Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Shubham Singh, Swarup Ranjan Behera, Vandana Rajan, Muskaan Singh, Arun Balaji Buduru, Rajesh Sharma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02230v1 Announce Type: new \nAbstract: In this work, we pioneer the study of Machine Unlearning (MU) for Paralinguistic Speech Processing (PSP). We focus on two key PSP tasks: Speech Emotion Recognition (SER) and Depression Detection (DD). To this end, we propose, SISA++, a novel extension to previous state-of-the-art (SOTA) MU method, SISA by merging models trained on different shards with weight-averaging. With such modifications, we show that SISA++ preserves performance more in comparison to SISA after unlearning in benchmark SER (CREMA-D) and DD (E-DAIC) datasets. Also, to guide future research for easier adoption of MU for PSP, we present ``cookbook recipes'' - actionable recommendations for selecting optimal feature representations and downstream architectures that can mitigate performance degradation after the unlearning process."
      },
      {
        "id": "oai:arXiv.org:2506.02232v1",
        "title": "Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models and their Synergistic Power for SingMOS Prediction",
        "link": "https://arxiv.org/abs/2506.02232",
        "author": "Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02232v1 Announce Type: new \nAbstract: In this study, we focus on Singing Voice Mean Opinion Score (SingMOS) prediction. Previous research have shown the performance benefit with the use of state-of-the-art (SOTA) pre-trained models (PTMs). However, they haven't explored speaker recognition speech PTMs (SPTMs) such as x-vector, ECAPA and we hypothesize that it will be the most effective for SingMOS prediction. We believe that due to their speaker recognition pre-training, it equips them to capture fine-grained vocal features (e.g., pitch, tone, intensity) from synthesized singing voices in a much more better way than other PTMs. Our experiments with SOTA PTMs including SPTMs and music PTMs validates the hypothesis. Additionally, we introduce a novel fusion framework, BATCH that uses Bhattacharya Distance for fusion of PTMs. Through BATCH with the fusion of speaker recognition SPTMs, we report the topmost performance comparison to all the individual PTMs and baseline fusion techniques as well as setting SOTA."
      },
      {
        "id": "oai:arXiv.org:2506.02258v1",
        "title": "Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal Emotion Recognition?",
        "link": "https://arxiv.org/abs/2506.02258",
        "author": "Mohd Mujtaba Akhtar, Orchid Chetia Phukan,  Girish, Swarup Ranjan Behera, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru, Rajesh Sharma",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02258v1 Announce Type: new \nAbstract: In this work, we focus on non-verbal vocal sounds emotion recognition (NVER). We investigate mamba-based audio foundation models (MAFMs) for the first time for NVER and hypothesize that MAFMs will outperform attention-based audio foundation models (AAFMs) for NVER by leveraging its state-space modeling to capture intrinsic emotional structures more effectively. Unlike AAFMs, which may amplify irrelevant patterns due to their attention mechanisms, MAFMs will extract more stable and context-aware representations, enabling better differentiation of subtle non-verbal emotional cues. Our experiments with state-of-the-art (SOTA) AAFMs and MAFMs validates our hypothesis. Further, motivated from related research such as speech emotion recognition, synthetic speech detection, where fusion of foundation models (FMs) have showed improved performance, we also explore fusion of FMs for NVER. To this end, we propose, RENO, that uses renyi-divergence as a novel loss function for effective alignment of the FMs. It also makes use of self-attention for better intra-representation interaction of the FMs. With RENO, through the heterogeneous fusion of MAFMs and AAFMs, we show the topmost performance in comparison to individual FMs, its fusion and also setting SOTA in comparison to previous SOTA work."
      },
      {
        "id": "oai:arXiv.org:2506.02339v1",
        "title": "Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss",
        "link": "https://arxiv.org/abs/2506.02339",
        "author": "Jiawen Huang, Felipe Sousa, Emir Demirel, Emmanouil Benetos, Igor Gadelha",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02339v1 Announce Type: new \nAbstract: Automatic Lyrics Transcription (ALT) aims to recognize lyrics from singing voices, similar to Automatic Speech Recognition (ASR) for spoken language, but faces added complexity due to domain-specific properties of the singing voice. While foundation ASR models show robustness in various speech tasks, their performance degrades on singing voice, especially in the presence of musical accompaniment. This work focuses on this performance gap and explores Low-Rank Adaptation (LoRA) for ALT, investigating both single-domain and dual-domain fine-tuning strategies. We propose using a consistency loss to better align vocal and mixture encoder representations, improving transcription on mixture without relying on singing voice separation. Our results show that while na\\\"ive dual-domain fine-tuning underperforms, structured training with consistency loss yields modest but consistent gains, demonstrating the potential of adapting ASR foundation models for music."
      },
      {
        "id": "oai:arXiv.org:2506.02401v1",
        "title": "Trusted Fake Audio Detection Based on Dirichlet Distribution",
        "link": "https://arxiv.org/abs/2506.02401",
        "author": "Chi Ding, Junxiao Xue, Cong Wang, Hao Zhou",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02401v1 Announce Type: new \nAbstract: With the continuous development of deep learning-based speech conversion and speech synthesis technologies, the cybersecurity problem posed by fake audio has become increasingly serious. Previously proposed models for defending against fake audio have attained remarkable performance. However, they all fall short in modeling the trustworthiness of the decisions made by the models themselves. Based on this, we put forward a plausible fake audio detection approach based on the Dirichlet distribution with the aim of enhancing the reliability of fake audio detection. Specifically, we first generate evidence through a neural network. Uncertainty is then modeled using the Dirichlet distribution. By modeling the belief distribution with the parameters of the Dirichlet distribution, an estimate of uncertainty can be obtained for each decision. Finally, the predicted probabilities and corresponding uncertainty estimates are combined to form the final opinion. On the ASVspoof series dataset (i.e., ASVspoof 2019 LA, ASVspoof 2021 LA, and DF), we conduct a number of comparison experiments to verify the excellent performance of the proposed model in terms of accuracy, robustness, and trustworthiness."
      },
      {
        "id": "oai:arXiv.org:2506.02443v1",
        "title": "Breaking the Barriers of Text-Hungry and Audio-Deficient AI",
        "link": "https://arxiv.org/abs/2506.02443",
        "author": "Hamidou Tembine, Issa Bamia, Massa NDong, Bakary Coulibaly, Oumar Issiaka Traore, Moussa Traore, Moussa Sanogo, Mamadou Eric Sangare, Salif Kante, Daryl Noupa Yongueng, Hafiz Tiomoko Ali, Malik Tiomoko, Frejus Laleye, Boualem Djehiche, Wesmanegda Elisee Dipama, Idris Baba Saje, Hammid Mohammed Ibrahim, Moumini Sanogo, Marie Coursel Nininahazwe, Abdul-Latif Siita, Haine Mhlongo, Teddy Nelvy Dieu Merci Kouka, Mariam Serine Jeridi, Mutiyamuogo Parfait Mupenge, Lekoueiry Dehah, Abdoul Aziz Bio Sidi Bouko, Wilfried Franceslas Zokoue, Odette Richette Sambila, Alina RS Mbango, Mady Diagouraga, Oumarou Moussa Sanoussi, Gizachew Dessalegn, Mohamed Lamine Samoura, Bintou Laetitia Audrey Coulibaly",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02443v1 Announce Type: new \nAbstract: While global linguistic diversity spans more than 7164 recognized languages, the current dominant architecture of machine intelligence remains fundamentally biased toward written text. This bias excludes over 700 million people particularly in rural and remote regions who are audio-literate. In this work, we introduce a fully textless, audio-to-audio machine intelligence framework designed to serve this underserved population, and all the people who prefer audio-efficiency. Our contributions include novel Audio-to-Audio translation architectures that bypass text entirely, including spectrogram-, scalogram-, wavelet-, and unit-based models. Central to our approach is the Multiscale Audio-Semantic Transform (MAST), a representation that encodes tonal, prosodic, speaker, and expressive features. We further integrate MAST into a fractional diffusion of mean-field-type framework powered by fractional Brownian motion. It enables the generation of high-fidelity, semantically consistent speech without reliance on textual supervision. The result is a robust and scalable system capable of learning directly from raw audio, even in languages that are unwritten or rarely digitized. This work represents a fundamental shift toward audio-native machine intelligence systems, expanding access to language technologies for communities historically left out of the current machine intelligence ecosystem."
      },
      {
        "id": "oai:arXiv.org:2506.02457v1",
        "title": "SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based Voice Assistant",
        "link": "https://arxiv.org/abs/2506.02457",
        "author": "Yixuan Hou, Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02457v1 Announce Type: new \nAbstract: Thanks to the steady progress of large language models (LLMs), speech encoding algorithms and vocoder structure, recent advancements have enabled generating speech response directly from a user instruction. However, benchmarking the generated speech quality has been a neglected but critical issue, considering the shift from the pursuit of semantic accuracy to vivid and spontaneous speech flow. Previous evaluation focused on the speech-understanding ability, lacking a quantification of acoustic quality. In this paper, we propose Speech cOnversational Voice Assistant Benchmark (SOVA-Bench), providing a comprehension comparison of the general knowledge, speech recognition and understanding, along with both semantic and acoustic generative ability between available speech LLMs. To the best of our knowledge, SOVA-Bench is one of the most systematic evaluation frameworks for speech LLMs, inspiring the direction of voice interaction systems."
      },
      {
        "id": "oai:arXiv.org:2506.02499v1",
        "title": "DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing Non-Verbal Sounds",
        "link": "https://arxiv.org/abs/2506.02499",
        "author": "Takuya Hasumi, Yusuke Fujita",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02499v1 Announce Type: new \nAbstract: We propose a new dataset for cinematic audio source separation (CASS) that handles non-verbal sounds. Existing CASS datasets only contain reading-style sounds as a speech stem. These datasets differ from actual movie audio, which is more likely to include acted-out voices. Consequently, models trained on conventional datasets tend to have issues where emotionally heightened voices, such as laughter and screams, are more easily separated as an effect, not speech. To address this problem, we build a new dataset, DnR-nonverbal. The proposed dataset includes non-verbal sounds like laughter and screams in the speech stem. From the experiments, we reveal the issue of non-verbal sound extraction by the current CASS model and show that our dataset can effectively address the issue in the synthetic and actual movie audio. Our dataset is available at https://zenodo.org/records/15470640."
      },
      {
        "id": "oai:arXiv.org:2506.02505v1",
        "title": "Adaptive Differential Denoising for Respiratory Sounds Classification",
        "link": "https://arxiv.org/abs/2506.02505",
        "author": "Gaoyang Dong, Zhicheng Zhang, Ping Sun, Minghui Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02505v1 Announce Type: new \nAbstract: Automated respiratory sound classification faces practical challenges from background noise and insufficient denoising in existing systems.\n  We propose Adaptive Differential Denoising network, that integrates noise suppression and pathological feature preservation via three innovations:\n  1) Adaptive Frequency Filter with learnable spectral masks and soft shrink to eliminate noise while retaining diagnostic high-frequency components;\n  2) A Differential Denoise Layer using differential attention to reduce noise-induced variations through augmented sample comparisons;\n  3) A bias denoising loss jointly optimizing classification and robustness without clean labels.\n  Experiments on the ICBHI2017 dataset show that our method achieves 65.53\\% of the Score, which is improved by 1.99\\% over the previous sota method.\n  The code is available in https://github.com/deegy666/ADD-RSC"
      },
      {
        "id": "oai:arXiv.org:2506.02545v1",
        "title": "On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs",
        "link": "https://arxiv.org/abs/2506.02545",
        "author": "Kemal Altwlkany, Amar Kuric, Emanuel Lacic",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02545v1 Announce Type: new \nAbstract: In recent years, there has been a growing focus on fairness and inclusivity within speech technology, particularly in areas such as automatic speech recognition and speech sentiment analysis. When audio is transcoded prior to processing, as is the case in streaming or real-time applications, any inherent bias in the coding mechanism may result in disparities. This not only affects user experience but can also have broader societal implications by perpetuating stereotypes and exclusion. Thus, it is important that audio coding mechanisms are unbiased. In this work, we contribute towards the scarce research with respect to language and gender biases of audio codecs. By analyzing the speech quality of over 2 million multilingual audio files after transcoding through a representative subset of codecs (PSTN, VoIP and neural), our results indicate that PSTN codecs are strongly biased in terms of gender and that neural codecs introduce language biases."
      },
      {
        "id": "oai:arXiv.org:2506.02590v1",
        "title": "Synthetic Speech Source Tracing using Metric Learning",
        "link": "https://arxiv.org/abs/2506.02590",
        "author": "Dimitrios Koutsianos, Stavros Zacharopoulos, Yannis Panagakis, Themos Stafylakis",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02590v1 Announce Type: new \nAbstract: This paper addresses source tracing in synthetic speech-identifying generative systems behind manipulated audio via speaker recognition-inspired pipelines. While prior work focuses on spoofing detection, source tracing lacks robust solutions. We evaluate two approaches: classification-based and metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet and self-supervised learning (SSL) backbones. The results show that ResNet achieves competitive performance with the metric learning approach, matching and even exceeding SSL-based systems. Our work demonstrates ResNet's viability for source tracing while underscoring the need to optimize SSL representations for this task. Our work bridges speaker recognition methodologies with audio forensic challenges, offering new directions for combating synthetic media manipulation."
      },
      {
        "id": "oai:arXiv.org:2506.02610v1",
        "title": "Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm",
        "link": "https://arxiv.org/abs/2506.02610",
        "author": "Zhaoyang Li, Jie Wang, XiaoXiao Li, Wangjie Li, Longjie Luo, Lin Li, Qingyang Hong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02610v1 Announce Type: new \nAbstract: In speaker diarization, traditional clustering-based methods remain widely used in real-world applications. However, these methods struggle with the complex distribution of speaker embeddings and overlapping speech segments. To address these limitations, we propose an Overlapping Community Detection method based on Graph Attention networks and the Label Propagation Algorithm (OCDGALP). The proposed framework comprises two key components: (1) a graph attention network that refines speaker embeddings and node connections by aggregating information from neighboring nodes, and (2) a label propagation algorithm that assigns multiple community labels to each node, enabling simultaneous clustering and overlapping community detection. Experimental results show that the proposed method significantly reduces the Diarization Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07% with oracle VAD."
      },
      {
        "id": "oai:arXiv.org:2506.02621v1",
        "title": "Cross-attention and Self-attention for Audio-visual Speaker Diarization in MISP-Meeting Challenge",
        "link": "https://arxiv.org/abs/2506.02621",
        "author": "Zhaoyang Li, Haodong Zhou, Longjie Luo, Xiaoxiao Li, Yongxin Chen, Lin Li, Qingyang Hong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02621v1 Announce Type: new \nAbstract: This paper presents the system developed for Task 1 of the Multi-modal Information-based Speech Processing (MISP) 2025 Challenge. We introduce CASA-Net, an embedding fusion method designed for end-to-end audio-visual speaker diarization (AVSD) systems. CASA-Net incorporates a cross-attention (CA) module to effectively capture cross-modal interactions in audio-visual signals and employs a self-attention (SA) module to learn contextual relationships among audio-visual frames. To further enhance performance, we adopt a training strategy that integrates pseudo-label refinement and retraining, improving the accuracy of timestamp predictions. Additionally, median filtering and overlap averaging are applied as post-processing techniques to eliminate outliers and smooth prediction labels. Our system achieved a diarization error rate (DER) of 8.18% on the evaluation set, representing a relative improvement of 47.3% over the baseline DER of 15.52%."
      },
      {
        "id": "oai:arXiv.org:2506.02661v1",
        "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation",
        "link": "https://arxiv.org/abs/2506.02661",
        "author": "Mingyang Huang, Peng Zhang, Bang Zhang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02661v1 Announce Type: new \nAbstract: Generating long-term, coherent, and realistic music-conditioned dance sequences remains a challenging task in human motion synthesis. Existing approaches exhibit critical limitations: motion graph methods rely on fixed template libraries, restricting creative generation; diffusion models, while capable of producing novel motions, often lack temporal coherence and musical alignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a hybrid framework that integrates Retrieval-Augmented Generation (RAG) with diffusion-based refinement to enable high-quality, musically coherent dance generation for arbitrary long-term music inputs. Our method introduces three core innovations: (1) A cross-modal contrastive learning architecture that aligns heterogeneous music and dance representations in a shared latent space, establishing unsupervised semantic correspondence without paired data; (2) An optimized motion graph system for efficient retrieval and seamless concatenation of motion segments, ensuring realism and temporal coherence across long sequences; (3) A multi-condition diffusion model that jointly conditions on raw music signals and contrastive features to enhance motion quality and global synchronization. Extensive experiments demonstrate that MotionRAG-Diff achieves state-of-the-art performance in motion quality, diversity, and music-motion synchronization accuracy. This work establishes a new paradigm for music-driven dance generation by synergizing retrieval-based template fidelity with diffusion-based creative enhancement."
      },
      {
        "id": "oai:arXiv.org:2506.02715v1",
        "title": "UltrasonicSpheres: Localized, Multi-Channel Sound Spheres Using Off-the-Shelf Speakers and Earables",
        "link": "https://arxiv.org/abs/2506.02715",
        "author": "Michael K\\\"uttner, Valeria Sitz, Kathrin Gerling, Michael Beigl, Tobias R\\\"oddiger",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02715v1 Announce Type: new \nAbstract: We present a demo ofUltrasonicSpheres, a novel system for location-specific audio delivery using wearable earphones that decode ultrasonic signals into audible sound. Unlike conventional beamforming setups, UltrasonicSpheres relies on single ultrasonic speakers to broadcast localized audio with multiple channels, each encoded on a distinct ultrasonic carrier frequency. Users wearing our acoustically transparent earphones can demodulate their selected stream, such as exhibit narrations in a chosen language, while remaining fully aware of ambient environmental sounds. The experience preserves spatial audio perception, giving the impression that the sound originates directly from the physical location of the source. This enables personalized, localized audio without requiring pairing, tracking, or additional infrastructure. Importantly, visitors not equipped with the earphones are unaffected, as the ultrasonic signals are inaudible to the human ear. Our demo invites participants to explore multiple co-located audio zones and experience how UltrasonicSpheres supports unobtrusive delivery of personalized sound in public spaces."
      },
      {
        "id": "oai:arXiv.org:2506.02742v1",
        "title": "Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions",
        "link": "https://arxiv.org/abs/2506.02742",
        "author": "Xiaoxue Gao, Huayun Zhang, Nancy F. Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02742v1 Announce Type: new \nAbstract: Existing expressive text-to-speech (TTS) systems primarily model a limited set of categorical emotions, whereas human conversations extend far beyond these predefined emotions, making it essential to explore more diverse emotional speech generation for more natural interactions. To bridge this gap, this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate unseen emotional speech via emotion-guided prompt learning. PUE is trained utilizing an LLM-TTS architecture to ensure emotional consistency between categorical emotion-relevant prompts and emotional speech, allowing the model to quantitatively capture different emotion weightings per utterance. During inference, mixed emotional speech can be generated by flexibly adjusting emotion proportions and leveraging LLM contextual knowledge, enabling the model to quantify different emotional styles. Our proposed PUE successfully facilitates expressive speech synthesis of unseen emotions in a zero-shot setting."
      },
      {
        "id": "oai:arXiv.org:2506.02773v1",
        "title": "AuralNet: Hierarchical Attention-based 3D Binaural Localization of Overlapping Speakers",
        "link": "https://arxiv.org/abs/2506.02773",
        "author": "Linya Fu, Yu Liu, Zhijie Liu, Zedong Yang, Zhong-Qiu Wang, Youfu Li, He Kong",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02773v1 Announce Type: new \nAbstract: We propose AuralNet, a novel 3D multi-source binaural sound source localization approach that localizes overlapping sources in both azimuth and elevation without prior knowledge of the number of sources. AuralNet employs a gated coarse-tofine architecture, combining a coarse classification stage with a fine-grained regression stage, allowing for flexible spatial resolution through sector partitioning. The model incorporates a multi-head self-attention mechanism to capture spatial cues in binaural signals, enhancing robustness in noisy-reverberant environments. A masked multi-task loss function is designed to jointly optimize sound detection, azimuth, and elevation estimation. Extensive experiments in noisy-reverberant conditions demonstrate the superiority of AuralNet over recent methods"
      },
      {
        "id": "oai:arXiv.org:2506.02777v1",
        "title": "On the influence of language similarity in non-target speaker verification trials",
        "link": "https://arxiv.org/abs/2506.02777",
        "author": "Paul M. Reuter, Michael Jessen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02777v1 Announce Type: new \nAbstract: In this paper, we investigate the influence of language similarity in cross-lingual non-target speaker verification trials using a state-of-the-art speaker verification system, ECAPA-TDNN, trained on multilingual and monolingual variants of the VoxCeleb dataset. Our analysis of the score distribution patterns on multilingual Globalphone and LDC CTS reveals a clustering effect in speaker comparisons involving a training language, whereby the choice of comparison language only minimally impacts scores. Conversely, we observe a language similarity effect in trials involving languages not included in the training set of the speaker verification system, with scores correlating with language similarity measured by a language classification system, especially when using multilingual training data."
      },
      {
        "id": "oai:arXiv.org:2506.02797v1",
        "title": "Fast-Converging Distributed Signal Estimation in Topology-Unconstrained Wireless Acoustic Sensor Networks",
        "link": "https://arxiv.org/abs/2506.02797",
        "author": "Paul Didier, Toon van Waterschoot, Simon Doclo, J\\\"org Bitzer, Marc Moonen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02797v1 Announce Type: new \nAbstract: This paper focuses on distributed signal estimation in topology-unconstrained wireless acoustic sensor networks (WASNs) where sensor nodes only transmit fused versions of their local sensor signals. For this task, the topology-independent (TI) distributed adaptive node-specific signal estimation (DANSE) algorithm (TI-DANSE) has previously been proposed. It converges towards the centralized signal estimation solution in non-fully connected and time-varying network topologies. However, the applicability of TI-DANSE in real-world scenarios is limited due to its slow convergence. The latter results from the fact that, in TI-DANSE, nodes only have access to the in-network sum of all fused signals in the WASN. We address this low convergence speed by introducing an improved TI-DANSE algorithm, referred to as TI-DANSE+, in which updating nodes separately use the partial in-network sums of fused signals coming from each of their neighbors. Nodes can maximize the number of available degrees of freedom in their local optimization problem, leading to faster convergence. This is further exploited by combining TI-DANSE+ with a tree-pruning strategy that maximizes the number of neighbors at the updating node. In fully connected WASNs, TI-DANSE+ converges as fast as the original DANSE algorithm (the latter only defined for fully connected WASNs) while using peer-to-peer data transmission instead of broadcasting and thus saving communication bandwidth. If link failures occur, the convergence of TI-DANSE+ towards the centralized solution is preserved without any change in its formulation. Altogether, the proposed TI-DANSE+ algorithm can be viewed as an all-round alternative to DANSE and TI-DANSE which (i) merges the advantages of both, (ii) reconciliates their differences into a single formulation, and (iii) shows advantages of its own in terms of communication bandwidth usage."
      },
      {
        "id": "oai:arXiv.org:2506.02858v1",
        "title": "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization",
        "link": "https://arxiv.org/abs/2506.02858",
        "author": "Geonyoung Lee, Geonhee Han, Paul Hongsuck Seo",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02858v1 Announce Type: new \nAbstract: Language-queried Audio Source Separation (LASS) enables open-vocabulary sound separation via natural language queries. While existing methods rely on task-specific training, we explore whether pretrained diffusion models, originally designed for audio generation, can inherently perform separation without further training. In this study, we introduce a training-free framework leveraging generative priors for zero-shot LASS. Analyzing na\\\"ive adaptations, we identify key limitations arising from modality-specific challenges.To address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a test-time optimization framework that refines spectrogram masks for precise, input-aligned separation. Our approach effectively repurposes pretrained diffusion models for source separation, achieving competitive performance without task-specific supervision. This work expands the application of diffusion models beyond generation, establishing a new paradigm for zero-shot audio separation. The code is available at: https://wltschmrz.github.io/DGMO/"
      },
      {
        "id": "oai:arXiv.org:2506.02863v1",
        "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
        "link": "https://arxiv.org/abs/2506.02863",
        "author": "Helin Wang, Jiarui Hai, Dading Chong, Karan Thakkar, Tiantian Feng, Dongchao Yang, Junhyeok Lee, Laureano Moro Velazquez, Jesus Villalba, Zengyi Qin, Shrikanth Narayanan, Mounya Elhiali, Najim Dehak",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02863v1 Announce Type: new \nAbstract: Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems."
      },
      {
        "id": "oai:arXiv.org:2506.02908v1",
        "title": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency",
        "link": "https://arxiv.org/abs/2506.02908",
        "author": "Bunlong Lay, Rostilav Makarov, Timo Gerkmann",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02908v1 Announce Type: new \nAbstract: Diffusion models are a class of generative models that have been recently used for speech enhancement with remarkable success but are computationally expensive at inference time. Therefore, these models are impractical for processing streaming data in real-time. In this work, we adapt a sliding window diffusion framework to the speech enhancement task. Our approach progressively corrupts speech signals through time, assigning more noise to frames close to the present in a buffer. This approach outputs denoised frames with a delay proportional to the chosen buffer size, enabling a trade-off between performance and latency. Empirical results demonstrate that our method outperforms standard diffusion models and runs efficiently on a GPU, achieving an input-output latency in the order of 0.3 to 1 seconds. This marks the first practical diffusion-based solution for online speech enhancement."
      },
      {
        "id": "oai:arXiv.org:2506.02958v1",
        "title": "PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech Editing",
        "link": "https://arxiv.org/abs/2506.02958",
        "author": "You Zhang, Baotong Tian, Lin Zhang, Zhiyao Duan",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02958v1 Announce Type: new \nAbstract: Neural speech editing enables seamless partial edits to speech utterances, allowing modifications to selected content while preserving the rest of the audio unchanged. This useful technique, however, also poses new risks of deepfakes. To encourage research on detecting such partially edited deepfake speech, we introduce PartialEdit, a deepfake speech dataset curated using advanced neural editing techniques. We explore both detection and localization tasks on PartialEdit. Our experiments reveal that models trained on the existing PartialSpoof dataset fail to detect partially edited speech generated by neural speech editing models. As recent speech editing models almost all involve neural audio codecs, we also provide insights into the artifacts the model learned on detecting these deepfakes. Further information about the PartialEdit dataset and audio samples can be found on the project page: https://yzyouzhang.com/PartialEdit/index.html."
      },
      {
        "id": "oai:arXiv.org:2506.03020v1",
        "title": "InfiniteAudio: Infinite-Length Audio Generation with Consistency",
        "link": "https://arxiv.org/abs/2506.03020",
        "author": "Chaeyoung Jung, Hojoon Ki, Ji-Hoon Kim, Junmo Kim, Joon Son Chung",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03020v1 Announce Type: new \nAbstract: This paper presents InfiniteAudio, a simple yet effective strategy for generating infinite-length audio using diffusion-based text-to-audio methods. Current approaches face memory constraints because the output size increases with input length, making long duration generation challenging. A common workaround is to concatenate short audio segments, but this often leads to inconsistencies due to the lack of shared temporal context. To address this, InfiniteAudio integrates seamlessly into existing pipelines without additional training. It introduces two key techniques: FIFO sampling, a first-in, first-out inference strategy with fixed-size inputs, and curved denoising, which selectively prioritizes key diffusion steps for efficiency. Experiments show that InfiniteAudio achieves comparable or superior performance across all metrics. Audio samples are available on our project page."
      },
      {
        "id": "oai:arXiv.org:2506.03099v1",
        "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models",
        "link": "https://arxiv.org/abs/2506.03099",
        "author": "Chetwin Low, Weimin Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03099v1 Announce Type: new \nAbstract: In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/"
      },
      {
        "id": "oai:arXiv.org:2406.03111v2",
        "title": "Singing Voice Graph Modeling for SingFake Detection",
        "link": "https://arxiv.org/abs/2406.03111",
        "author": "Xuanjun Chen, Haibin Wu, Jyh-Shing Roger Jang, Hung-yi Lee",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.03111v2 Announce Type: cross \nAbstract: Detecting singing voice deepfakes, or SingFake, involves determining the authenticity and copyright of a singing voice. Existing models for speech deepfake detection have struggled to adapt to unseen attacks in this unique singing voice domain of human vocalization. To bridge the gap, we present a groundbreaking SingGraph model. The model synergizes the capabilities of the MERT acoustic music understanding model for pitch and rhythm analysis with the wav2vec2.0 model for linguistic analysis of lyrics. Additionally, we advocate for using RawBoost and beat matching techniques grounded in music domain knowledge for singing voice augmentation, thereby enhancing SingFake detection performance. Our proposed method achieves new state-of-the-art (SOTA) results within the SingFake dataset, surpassing the previous SOTA model across three distinct scenarios: it improves EER relatively for seen singers by 13.2%, for unseen singers by 24.3%, and unseen singers using different codecs by 37.1%."
      },
      {
        "id": "oai:arXiv.org:2506.00422v1",
        "title": "DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition",
        "link": "https://arxiv.org/abs/2506.00422",
        "author": "Yui Sudo, Yosuke Fukumoto, Muhammad Shakeel, Yifan Peng, Chyi-Jiunn Lin, Shinji Watanabe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00422v1 Announce Type: cross \nAbstract: Contextual biasing (CB) improves automatic speech recognition for rare and unseen phrases. Recent studies have introduced dynamic vocabulary, which represents context phrases as expandable tokens in autoregressive (AR) models. This method improves CB accuracy but with slow inference speed. While dynamic vocabulary can be applied to non-autoregressive (NAR) models, such as connectionist temporal classification (CTC), the conditional independence assumption fails to capture dependencies between static and dynamic tokens. This paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a self-conditioned CTC method that integrates dynamic vocabulary into intermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC effectively captures dependencies between static and dynamic tokens while reducing the real-time factor (RTF). Experimental results show that DYNAC reduces RTF by 81% with a 0.1-point degradation in word error rate on the LibriSpeech 960 test-clean set."
      },
      {
        "id": "oai:arXiv.org:2506.01998v1",
        "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents",
        "link": "https://arxiv.org/abs/2506.01998",
        "author": "Takao Fujii, Katie Seaborn, Madeleine Steeds, Jun Kato",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01998v1 Announce Type: cross \nAbstract: Conversational agents that mimic people have raised questions about the ethics of anthropomorphizing machines with human social identity cues. Critics have also questioned assumptions of identity neutrality in humanlike agents. Recent work has revealed that intersectional Japanese pronouns can elicit complex and sometimes evasive impressions of agent identity. Yet, the role of other \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially expressive medium remains unexplored. In a crowdsourcing study, Japanese participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and Ember) using seven self-referents. We found strong evidence of voice gendering alongside the potential of intersectional self-referents to evade gendering, i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age and formality intersected with gendering as per sociolinguistic theories, especially boku and watakushi. This work provides a nuanced take on agent identity perceptions and champions intersectional and culturally-sensitive work on voice agents."
      },
      {
        "id": "oai:arXiv.org:2506.02010v1",
        "title": "CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge",
        "link": "https://arxiv.org/abs/2506.02010",
        "author": "Zehua Liu, Xiaolou Li, Chen Chen, Lantian Li, Dong Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02010v1 Announce Type: cross \nAbstract: This paper presents the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in Chinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The challenge evaluates two test scenarios: reading in recording studios and Internet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC 2023, which involves CN-CVS for training and CNVSRC-Single/Multi for development and evaluation. However, CNVSRC 2024 introduced two key improvements: (1) a stronger baseline system, and (2) an additional dataset, CN-CVS2-P1, for open tracks to improve data volume and diversity. The new challenge has demonstrated several important innovations in data preprocessing, feature extraction, model design, and training strategies, further pushing the state-of-the-art in Chinese LVC-VSR. More details and resources are available at the official website."
      },
      {
        "id": "oai:arXiv.org:2506.02012v1",
        "title": "Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing",
        "link": "https://arxiv.org/abs/2506.02012",
        "author": "Zehua Liu, Xiaolou Li, Li Guo, Lantian Li, Dong Wang",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02012v1 Announce Type: cross \nAbstract: Visual Speech Recognition (VSR) transcribes speech by analyzing lip movements. Recently, Large Language Models (LLMs) have been integrated into VSR systems, leading to notable performance improvements. However, the potential of LLMs has not been extensively studied, and how to effectively utilize LLMs in VSR tasks remains unexplored. This paper systematically explores how to better leverage LLMs for VSR tasks and provides three key contributions: (1) Scaling Test: We study how the LLM size affects VSR performance, confirming a scaling law in the VSR task. (2) Context-Aware Decoding: We add contextual text to guide the LLM decoding, improving recognition accuracy. (3) Iterative Polishing: We propose iteratively refining LLM outputs, progressively reducing recognition errors. Extensive experiments demonstrate that by these designs, the great potential of LLMs can be largely harnessed, leading to significant VSR performance improvement."
      },
      {
        "id": "oai:arXiv.org:2506.02157v1",
        "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation",
        "link": "https://arxiv.org/abs/2506.02157",
        "author": "Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02157v1 Announce Type: cross \nAbstract: Neural transducers (NT) provide an effective framework for speech streaming, demonstrating strong performance in automatic speech recognition (ASR). However, the application of NT to speech translation (ST) remains challenging, as existing approaches struggle with word reordering and performance degradation when jointly modeling ASR and ST, resulting in a gap with attention-based encoder-decoder (AED) models. Existing NT-based ST approaches also suffer from high computational training costs. To address these issues, we propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech Recognition and Translation), a novel framework that factorizes ASR and translation tasks to better handle reordering. To ensure robust ST while preserving ASR performance, we use self-distillation with CTC consistency regularization. Moreover, we improve computational efficiency by incorporating best practices from ASR transducers, including a down-sampled hierarchical encoder, a stateless predictor, and a pruned transducer loss to reduce training complexity. Finally, we introduce a blank penalty during decoding, reducing deletions and improving translation quality. Our approach is evaluated on three conversational datasets Arabic, Spanish, and Mandarin achieving new state-of-the-art performance among NT models and substantially narrowing the gap with AED-based systems."
      },
      {
        "id": "oai:arXiv.org:2506.02239v1",
        "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition",
        "link": "https://arxiv.org/abs/2506.02239",
        "author": "Sofoklis Kakouros",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02239v1 Announce Type: cross \nAbstract: In emotion recognition from speech, a key challenge lies in identifying speech signal segments that carry the most relevant acoustic variations for discerning specific emotions. Traditional approaches compute functionals for features such as energy and F0 over entire sentences or longer speech portions, potentially missing essential fine-grained variation in the long-form statistics. This research investigates the use of word informativeness, derived from a pre-trained language model, to identify semantically important segments. Acoustic features are then computed exclusively for these identified segments, enhancing emotion recognition accuracy. The methodology utilizes standard acoustic prosodic features, their functionals, and self-supervised representations. Results indicate a notable improvement in recognition performance when features are computed on segments selected based on word informativeness, underscoring the effectiveness of this approach."
      },
      {
        "id": "oai:arXiv.org:2506.02283v1",
        "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews",
        "link": "https://arxiv.org/abs/2506.02283",
        "author": "Sofoklis Kakouros, Haoyu Chen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02283v1 Announce Type: cross \nAbstract: This study examines the prosodic characteristics associated with winning and losing in post-match tennis interviews. Additionally, this research explores the potential to classify match outcomes solely based on post-match interview recordings using prosodic features and self-supervised learning (SSL) representations. By analyzing prosodic elements such as pitch and intensity, alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine whether an athlete has won or lost their match. Traditional acoustic features and deep speech representations are extracted from the data, and machine learning classifiers are employed to distinguish between winning and losing players. Results indicate that SSL representations effectively differentiate between winning and losing outcomes, capturing subtle speech patterns linked to emotional states. At the same time, prosodic cues -- such as pitch variability -- remain strong indicators of victory."
      },
      {
        "id": "oai:arXiv.org:2506.02414v1",
        "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion",
        "link": "https://arxiv.org/abs/2506.02414",
        "author": "Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02414v1 Announce Type: cross \nAbstract: Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content. Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content. Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance. However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling. We propose StarVC, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features. The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found at: https://thuhcsi.github.io/StarVC/."
      },
      {
        "id": "oai:arXiv.org:2506.02584v1",
        "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning",
        "link": "https://arxiv.org/abs/2506.02584",
        "author": "Sarenne Wallbridge, Christoph Minixhofer, Catherine Lai, Peter Bell",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02584v1 Announce Type: cross \nAbstract: People exploit the predictability of lexical structures during text comprehension. Though predictable structure is also present in speech, the degree to which prosody, e.g. intonation, tempo, and loudness, contributes to such structure independently of the lexical content is unclear. This study leverages self-supervised learning (SSL) to examine the temporal granularity of structures in the acoustic correlates of prosody. Representations from our proposed Masked Prosody Model can predict perceptual labels dependent on local information, such as word boundaries, but provide the most value for labels involving longer-term structures, like emotion recognition. Probing experiments across various perceptual labels show strong relative gains over untransformed pitch, energy, and voice activity features. Our results reveal the importance of SSL training objective timescale and highlight the value of complex SSL-encoded structures compared to more constrained classical structures."
      },
      {
        "id": "oai:arXiv.org:2506.02627v1",
        "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning",
        "link": "https://arxiv.org/abs/2506.02627",
        "author": "\\\"Omer Tarik \\\"Ozyilmaz, Matt Coler, Matias Valdenegro-Toro",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02627v1 Announce Type: cross \nAbstract: Although commercial Arabic automatic speech recognition (ASR) systems support Modern Standard Arabic (MSA), they struggle with dialectal speech. We investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA training size effects, benefits of pre-training on MSA data, and dialect-specific versus dialect-pooled models. We find that small amounts of MSA fine-tuning data yield substantial improvements for smaller models, matching larger non-fine-tuned models. While MSA pre-training shows minimal benefit, suggesting limited shared features between MSA and dialects, our dialect-pooled models perform comparably to dialect-specific ones. This indicates that pooling dialectal data, when properly balanced, can help address data scarcity in low-resource ASR without significant performance loss."
      },
      {
        "id": "oai:arXiv.org:2506.02894v1",
        "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation",
        "link": "https://arxiv.org/abs/2506.02894",
        "author": "Verena Blaschke, Miriam Winkler, Constantin F\\\"orster, Gabriele Wenger-Glemser, Barbara Plank",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02894v1 Announce Type: cross \nAbstract: Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, an evaluation dataset containing four hours of read speech in three dialect groups spoken in Southeast Germany (Franconian, Bavarian, Alemannic), and half an hour of Standard German speech. We provide both dialectal and Standard German transcriptions, and analyze the linguistic differences between them. We benchmark several multilingual state-of-the-art ASR models on speech translation into Standard German, and find differences between how much the output resembles the dialectal vs. standardized transcriptions. Qualitative error analyses of the best ASR model reveal that it sometimes normalizes grammatical differences, but often stays closer to the dialectal constructions."
      },
      {
        "id": "oai:arXiv.org:2506.02979v1",
        "title": "Towards a Japanese Full-duplex Spoken Dialogue System",
        "link": "https://arxiv.org/abs/2506.02979",
        "author": "Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02979v1 Announce Type: cross \nAbstract: Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model's performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness."
      },
      {
        "id": "oai:arXiv.org:2410.12266v2",
        "title": "FlashAudio: Rectified Flows for Fast and High-Fidelity Text-to-Audio Generation",
        "link": "https://arxiv.org/abs/2410.12266",
        "author": "Huadai Liu, Jialei Wang, Rongjie Huang, Yang Liu, Heng Lu, Zhou Zhao, Wei Xue",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.12266v2 Announce Type: replace \nAbstract: Recent advancements in latent diffusion models (LDMs) have markedly enhanced text-to-audio generation, yet their iterative sampling processes impose substantial computational demands, limiting practical deployment. While recent methods utilizing consistency-based distillation aim to achieve few-step or single-step inference, their one-step performance is constrained by curved trajectories, preventing them from surpassing traditional diffusion models. In this work, we introduce FlashAudio with rectified flows to learn straight flow for fast simulation. To alleviate the inefficient timesteps allocation and suboptimal distribution of noise, FlashAudio optimizes the time distribution of rectified flow with Bifocal Samplers and proposes immiscible flow to minimize the total distance of data-noise pairs in a batch vias assignment. Furthermore, to address the amplified accumulation error caused by the classifier-free guidance (CFG), we propose Anchored Optimization, which refines the guidance scale by anchoring it to a reference trajectory. Experimental results on text-to-audio generation demonstrate that FlashAudio's one-step generation performance surpasses the diffusion-based models with hundreds of sampling steps on audio quality and enables a sampling speed of 400x faster than real-time on a single NVIDIA 4090Ti GPU. Code will be available at https://github.com/liuhuadai/FlashAudio."
      },
      {
        "id": "oai:arXiv.org:2503.07352v2",
        "title": "Score-informed Music Source Separation: Improving Synthetic-to-real Generalization in Classical Music",
        "link": "https://arxiv.org/abs/2503.07352",
        "author": "Eetu Tunturi, David Diaz-Guerra, Archontis Politis, Tuomas Virtanen",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.07352v2 Announce Type: replace \nAbstract: Music source separation is the task of separating a mixture of instruments into constituent tracks. Music source separation models are typically trained using only audio data, although additional information can be used to improve the model's separation capability. In this paper, we propose two ways of using musical scores to aid music source separation: a score-informed model where the score is concatenated with the magnitude spectrogram of the audio mixture as the input of the model, and a model where we use only the score to calculate the separation mask. We train our models on synthetic data in the SynthSOD dataset and evaluate our methods on the URMP and Aalto anechoic orchestra datasets, comprised of real recordings. The score-informed model improves separation results compared to a baseline approach, but struggles to generalize from synthetic to real data, whereas the score-only model shows a clear improvement in synthetic-to-real generalization."
      },
      {
        "id": "oai:arXiv.org:2504.14906v3",
        "title": "OmniAudio: Generating Spatial Audio from 360-Degree Video",
        "link": "https://arxiv.org/abs/2504.14906",
        "author": "Huadai Liu, Tianyi Luo, Kaicheng Luo, Qikai Jiang, Peiwen Sun, Jialei Wang, Rongjie Huang, Qian Chen, Wen Wang, Xiangtai Li, Shiliang Zhang, Zhijie Yan, Zhou Zhao, Wei Xue",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.14906v3 Announce Type: replace \nAbstract: Traditional video-to-audio generation techniques primarily focus on perspective video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, 360V2SA, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create Sphere360, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a dual-branch framework that utilizes both panoramic and perspective video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that OmniAudio achieves state-of-the-art performance across both objective and subjective metrics on Sphere360. Code and datasets are available at https://github.com/liuhuadai/OmniAudio. The project website is available at https://OmniAudio-360V2SA.github.io."
      },
      {
        "id": "oai:arXiv.org:2505.15380v2",
        "title": "Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding",
        "link": "https://arxiv.org/abs/2505.15380",
        "author": "Zijian Lin, Yang Zhang, Yougen Yuan, Yuming Yan, Jinjiang Liu, Zhiyong Wu, Pengfei Hu, Qun Yu",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.15380v2 Announce Type: replace \nAbstract: Modern autoregressive speech synthesis models leveraging language models have demonstrated remarkable performance. However, the sequential nature of next token prediction in these models leads to significant latency, hindering their deployment in scenarios where inference speed is critical. In this work, we propose Speech Speculative Decoding (SSD), a novel framework for autoregressive speech synthesis acceleration. Specifically, our method employs a lightweight draft model to generate candidate token sequences, which are subsequently verified in parallel by the target model using the proposed SSD framework. Experimental results demonstrate that SSD achieves a significant speedup of 1.4x compared with conventional autoregressive decoding, while maintaining high fidelity and naturalness. Subjective evaluations further validate the effectiveness of SSD in preserving the perceptual quality of the target model while accelerating inference."
      },
      {
        "id": "oai:arXiv.org:2505.20552v2",
        "title": "Effect of laboratory conditions on the perception of virtual stages for music",
        "link": "https://arxiv.org/abs/2505.20552",
        "author": "Ernesto Accolti",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20552v2 Announce Type: replace \nAbstract: This manuscript presents initial findings critical for supporting augmented acoustics experiments in custom-made hearing booths, addressing a key challenge in ensuring perceptual validity and experimental rigor in these highly sensitive setups. This validation ensures our proposed methodology is sound, guarantees the reliability of future results, and lays the foundational groundwork for subsequent perceptual studies and the development of robust guidelines for laboratory design in virtual acoustics research. A preliminary study on the effect of the acoustical conditions of three different rooms on the perception of virtual stages for music is presented: an anechoic room, a custom-made hearing booth with insufficient sound absorption, and another custom-made hearing booth with achievable sound absorption. The goal of this study is to assess the impact of these different conditions on the perception of virtual stages for music. The results show that the anechoic room and the hearing booth with achievable sound absorption have a difference between the total sound and the virtual sound below the just-noticeable difference, which means that the virtual sound is not perceived louder than it should. In contrast, the hearing booth with insufficient sound absorption has a difference above the just-noticeable difference, which means that the virtual sound is perceived louder than it should. This study provides a preliminary validation of the proposed methodology for assessing the acoustical conditions of custom-made hearing booths in stage acoustics experiments. Future work will include a more comprehensive analysis of the results, including the effect of different sound sources.\n  Supplementary audio files illustrating key simulation results are available at https://zenodo.org/records/15579861"
      },
      {
        "id": "oai:arXiv.org:2505.24200v2",
        "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC",
        "link": "https://arxiv.org/abs/2505.24200",
        "author": "Qingzheng Wang, Jiancheng Sun, Yifan Peng, Shinji Watanabe",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24200v2 Announce Type: replace \nAbstract: Multilingual speech processing with self-supervised or supervised pre-trained Speech Foundation Models (SFM) has achieved strong performance on tasks like Language Identification (LID) and Automatic Speech Recognition (ASR). However, these models struggle with limited resources during fine-tuning. This paper enhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple strategies for adapting SFMs, including frozen upstream training, partial fine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation to mitigate performance gaps in few-shot settings and introduce LID Connectionist Temporal Classification (CTC) loss for regularization. Our approach achieves a 14% relative improvement in LID accuracy and a 30% relative reduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place in the Interspeech 2025 ML-SUPERB 2.0 Challenge."
      },
      {
        "id": "oai:arXiv.org:2502.04328v3",
        "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model",
        "link": "https://arxiv.org/abs/2502.04328",
        "author": "Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04328v3 Announce Type: replace-cross \nAbstract: Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal Language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts, pushing the frontiers of the omni-modal language model to a large extent. We conduct a comprehensive exploration of architectural design, data curation, and training strategies essential for building a robust omni-modal model. Ola incorporates advanced visual understanding and audio recognition capabilities through several critical and effective improvements over mainstream baselines. Moreover, we rethink inter-modal relationships during omni-modal training, emphasizing cross-modal alignment with video as a central bridge, and propose a progressive training pipeline that begins with the most distinct modalities and gradually moves towards closer modality alignment. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola."
      },
      {
        "id": "oai:arXiv.org:2505.13338v2",
        "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation",
        "link": "https://arxiv.org/abs/2505.13338",
        "author": "Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.13338v2 Announce Type: replace-cross \nAbstract: Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities."
      },
      {
        "id": "oai:arXiv.org:2506.01496v2",
        "title": "Continual Speech Learning with Fused Speech Features",
        "link": "https://arxiv.org/abs/2506.01496",
        "author": "Guitao Wang, Jinming Zhao, Hao Yang, Guilin Qi, Tongtong Wu, Gholamreza Haffari",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01496v2 Announce Type: replace-cross \nAbstract: Rapid growth in speech data demands adaptive models, as traditional static methods fail to keep pace with dynamic and diverse speech information. We introduce continuous speech learning, a new set-up targeting at bridging the adaptation gap in current speech models. We use the encoder-decoder Whisper model to standardize speech tasks into a generative format. We integrate a learnable gated-fusion layer on the top of the encoder to dynamically select task-specific features for downstream tasks. Our approach improves accuracy significantly over traditional methods in six speech processing tasks, demonstrating gains in adapting to new speech tasks without full retraining."
      },
      {
        "id": "oai:arXiv.org:2506.01789v2",
        "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability",
        "link": "https://arxiv.org/abs/2506.01789",
        "author": "Genta Indra Winata, David Anugraha, Emmy Liu, Alham Fikri Aji, Shou-Yi Hung, Aditya Parashar, Patrick Amadeus Irawan, Ruochen Zhang, Zheng-Xin Yong, Jan Christian Blaise Cruz, Niklas Muennighoff, Seungone Kim, Hanyang Zhao, Sudipta Kar, Kezia Erina Suryoraharjo, M. Farid Adilazuarda, En-Shiun Annie Lee, Ayu Purwarianti, Derry Tanti Wijaya, Monojit Choudhury",
        "published": "Wed, 04 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01789v2 Announce Type: replace-cross \nAbstract: High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics."
      }
    ]
  }
}