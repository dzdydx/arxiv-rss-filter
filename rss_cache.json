{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Wed, 11 Jun 2025 04:15:07 +0000",
      "published": "Wed, 11 Jun 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2506.08018v1",
        "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache",
        "link": "https://arxiv.org/abs/2506.08018",
        "author": "Fei Li, Song Liu, Weiguo Wu, Shiqiang Nie, Jinyu Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08018v1 Announce Type: new \nAbstract: The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput."
      },
      {
        "id": "oai:arXiv.org:2506.08019v1",
        "title": "Gridding Forced Displacement using Semi-Supervised Learning",
        "link": "https://arxiv.org/abs/2506.08019",
        "author": "Andrew Wells, Geraldine Henningsen, Brice Bolane Tchinde Kengne",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08019v1 Announce Type: new \nAbstract: We present a semi-supervised approach that disaggregates refugee statistics from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan African countries. By integrating UNHCR's ProGres registration data with satellite-derived building footprints from Google Open Buildings and location coordinates from OpenStreetMap Populated Places, our label spreading algorithm creates spatially explicit refugee statistics at high granularity.This methodology achieves 92.9% average accuracy in placing over 10 million refugee observations into appropriate grid cells, enabling the identification of localized displacement patterns previously obscured in broader regional and national statistics. The resulting high-resolution dataset provides a foundation for a deeper understanding of displacement drivers."
      },
      {
        "id": "oai:arXiv.org:2506.08020v1",
        "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation",
        "link": "https://arxiv.org/abs/2506.08020",
        "author": "Zi-Ying Chen, Chuan-Xian Ren, Hong Yan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08020v1 Announce Type: new \nAbstract: Partial domain adaptation (PDA) problem requires aligning cross-domain samples while distinguishing the outlier classes for accurate knowledge transfer. The widely used weighting framework tries to address the outlier classes by introducing the reweighed source domain with a similar label distribution to the target domain. However, the empirical modeling of weights can only characterize the sample-wise relations, which leads to insufficient exploration of cluster structures, and the weights could be sensitive to the inaccurate prediction and cause confusion on the outlier classes. To tackle these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model to simultaneously characterize the sample-wise and class-wise relations in a unified transport framework. Specifically, a cooperation mechanism between sample-level and class-level transport is introduced, where the sample-level transport provides essential structure information for the class-level knowledge transfer, while the class-level transport supplies discriminative information for the outlier identification. The bi-level transport plan provides guidance for the alignment process. By incorporating the label-aware transport cost, the local transport structure is ensured and a fast computation formulation is derived to improve the efficiency. Extensive experiments on benchmark datasets validate the competitiveness of BUOT."
      },
      {
        "id": "oai:arXiv.org:2506.08021v1",
        "title": "FlowBERT: Prompt-tuned BERT for variable flow field prediction",
        "link": "https://arxiv.org/abs/2506.08021",
        "author": "Weihao Zou, Weibing Feng, Pin Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08021v1 Announce Type: new \nAbstract: This study proposes a universal flow field prediction framework based on knowledge transfer\n  from large language model (LLM), addressing the high computational costs of traditional\n  computational fluid dynamics (CFD) methods and the limited cross-condition transfer capability\n  of existing deep learning models. The framework innovatively integrates Proper Orthogonal\n  Decomposition (POD) dimensionality reduction with fine-tuning strategies for pretrained LLM,\n  where POD facilitates compressed representation of flow field features while the fine-tuned model\n  learns to encode system dynamics in state space. To enhance the model's adaptability to flow field\n  data, we specifically designed fluid dynamics-oriented text templates that improve predictive\n  performance through enriched contextual semantic information. Experimental results demonstrate\n  that our framework outperforms conventional Transformer models in few-shot learning scenarios while\n  exhibiting exceptional generalization across various inflow conditions and airfoil geometries.\n  Ablation studies reveal the contributions of key components in the FlowBERT architecture. Compared\n  to traditional Navier-Stokes equation solvers requiring hours of computation, our approach reduces\n  prediction time to seconds while maintaining over 90% accuracy. The developed knowledge transfer\n  paradigm establishes a new direction for rapid fluid dynamics prediction, with potential\n  applications extending to aerodynamic optimization, flow control, and other engineering domains."
      },
      {
        "id": "oai:arXiv.org:2506.08022v1",
        "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining",
        "link": "https://arxiv.org/abs/2506.08022",
        "author": "Chenxi Liu, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Tianyi Zhou, Heng Huang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08022v1 Announce Type: new \nAbstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations."
      },
      {
        "id": "oai:arXiv.org:2506.08027v1",
        "title": "Recipes for Pre-training LLMs with MXFP8",
        "link": "https://arxiv.org/abs/2506.08027",
        "author": "Asit Mishra, Dusan Stosic, Simon Layton",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08027v1 Announce Type: new \nAbstract: Precision scaling - using fewer bits to represent model parameters and related tensors during pre-training - has emerged as a compelling technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling this precision scaling aspect. These formats combine narrow floating-point data types with per-block scaling factors, offering a fine-grained approach to quantizing tensors.\n  Although MX-formats offer the promise of improved numeric stability compared to other reduced-precision representations, in practice they must be used carefully in order to successfully converge an LLM on a multi-trillion token dataset. In this paper, we show that the rounding mode suggested in OCP specification can lead to divergence when pre-training an LLM. We show an improved rounding mode, which uses round-to-infinity to compute scaling factors, enables successful pre-training in MXFP8 for an 8B model on 15T tokens."
      },
      {
        "id": "oai:arXiv.org:2506.08048v1",
        "title": "Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts",
        "link": "https://arxiv.org/abs/2506.08048",
        "author": "Zheng Han, Jun Zhou, Jialun Pei, Jing Qin, Yingfang Fan, Qi Dou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08048v1 Announce Type: new \nAbstract: In augmented reality (AR)-guided surgical navigation, preoperative organ models are superimposed onto the patient's intraoperative anatomy to visualize critical structures such as vessels and tumors. Accurate deformation modeling is essential to maintain the reliability of AR overlays by ensuring alignment between preoperative models and the dynamically changing anatomy. Although the finite element method (FEM) offers physically plausible modeling, its high computational cost limits intraoperative applicability. Moreover, existing algorithms often fail to handle large anatomical changes, such as those induced by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical correspondences and compromised AR guidance. To address these challenges, we propose a data-driven biomechanics algorithm that preserves FEM-level accuracy while improving computational efficiency. In addition, we introduce a novel human-in-the-loop mechanism into the deformation modeling process. This enables surgeons to interactively provide prompts to correct anatomical misalignments, thereby incorporating clinical expertise and allowing the model to adapt dynamically to complex surgical scenarios. Experiments on a publicly available dataset demonstrate that our algorithm achieves a mean target registration error of 3.42 mm. Incorporating surgeon prompts through the interactive framework further reduces the error to 2.78 mm, surpassing state-of-the-art methods in volumetric accuracy. These results highlight the ability of our framework to deliver efficient and accurate deformation modeling while enhancing surgeon-algorithm collaboration, paving the way for safer and more reliable computer-assisted surgeries."
      },
      {
        "id": "oai:arXiv.org:2506.08051v1",
        "title": "ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity",
        "link": "https://arxiv.org/abs/2506.08051",
        "author": "Mahmuda Sultana Mimi, Md Monzurul Islam, Anannya Ghosh Tusti, Shriyank Somvanshi, Subasish Das",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08051v1 Announce Type: new \nAbstract: Understanding the spatial and temporal dynamics of automated vehicle (AV) crash severity is critical for advancing urban mobility safety and infrastructure planning. In this work, we introduce ST-GraphNet, a spatio-temporal graph neural network framework designed to model and predict AV crash severity by using both fine-grained and region-aggregated spatial graphs. Using a balanced dataset of 2,352 real-world AV-related crash reports from Texas (2024), including geospatial coordinates, crash timestamps, SAE automation levels, and narrative descriptions, we construct two complementary graph representations: (1) a fine-grained graph with individual crash events as nodes, where edges are defined via spatio-temporal proximity; and (2) a coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical Spatial Indexing (H3)-based spatial cells, connected through hexagonal adjacency. Each node in the graph is enriched with multimodal data, including semantic, spatial, and temporal attributes, including textual embeddings from crash narratives using a pretrained Sentence-BERT model. We evaluate various graph neural network (GNN) architectures, such as Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN (DSTGCN), to classify crash severity and predict high-risk regions. Our proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3 graph, achieves a test accuracy of 97.74\\%, substantially outperforming the best fine-grained model (64.7\\% test accuracy). These findings highlight the effectiveness of spatial aggregation, dynamic message passing, and multi-modal feature integration in capturing the complex spatio-temporal patterns underlying AV crash severity."
      },
      {
        "id": "oai:arXiv.org:2506.08052v1",
        "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving",
        "link": "https://arxiv.org/abs/2506.08052",
        "author": "Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08052v1 Announce Type: new \nAbstract: Although end-to-end autonomous driving has made remarkable progress, its performance degrades significantly in rare and long-tail scenarios. Recent approaches attempt to address this challenge by leveraging the rich world knowledge of Vision-Language Models (VLMs), but these methods suffer from several limitations: (1) a significant domain gap between the pre-training data of VLMs and real-world driving data, (2) a dimensionality mismatch between the discrete language space and the continuous action space, and (3) imitation learning tends to capture the average behavior present in the dataset, which may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an autonomous driving system that integrates VLMs with diffusion planner, which adopts a three-stage paradigm for training. In the first stage, we use a large-scale driving question-answering datasets to train the VLMs, mitigating the domain discrepancy between generic content and real-world driving scenarios. In the second stage, we employ a diffusion-based planner to perform imitation learning, mapping representations from the latent language space to continuous driving actions. Finally, we fine-tune the diffusion planner using reinforcement learning with NAVSIM non-reactive simulator, enabling the model to generate safer, more human-like driving trajectories. We evaluate our approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6 and setting a new state-of-the-art that surpasses the previous vision-only SOTA by 5.6 PDMS."
      },
      {
        "id": "oai:arXiv.org:2506.08054v1",
        "title": "STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation",
        "link": "https://arxiv.org/abs/2506.08054",
        "author": "Yiming Wang, Hao Peng, Senzhang Wang, Haohua Du, Chunyang Liu, Jia Wu, Guanlin Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08054v1 Announce Type: new \nAbstract: Traffic data imputation is fundamentally important to support various applications in intelligent transportation systems such as traffic flow prediction. However, existing time-to-space sequential methods often fail to effectively extract features in block-wise missing data scenarios. Meanwhile, the static graph structure for spatial feature propagation significantly constrains the models flexibility in handling the distribution shift issue for the nonstationary traffic data. To address these issues, this paper proposes a SpatioTemporal Attention Mixture of experts network named STAMImputer for traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE) framework to capture latent spatio-temporal features and their influence weights, effectively imputing block missing. A novel Low-rank guided Sampling Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local and global correlations across road networks. The sampled attention vectors are utilized to generate dynamic graphs that capture real-time spatial correlations. Extensive experiments are conducted on four traffic datasets for evaluation. The result shows STAMImputer achieves significantly performance improvement compared with existing SOTA approaches. Our codes are available at https://github.com/RingBDStack/STAMImupter."
      },
      {
        "id": "oai:arXiv.org:2506.08060v1",
        "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques",
        "link": "https://arxiv.org/abs/2506.08060",
        "author": "Asankhaya Sharma",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08060v1 Announce Type: new \nAbstract: Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length $l$, datasets of size $\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log \\frac{m}{\\delta} \\right)$ or, with bounded context, $\\mathrm{O}\\left( \\frac{l \\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ suffice to approximate fine-tuned behavior across $m$ contexts within error $\\varepsilon$, where $V$ is the vocabulary size and $\\delta$ is the failure probability. For linear classification, datasets of size $\\mathrm{O}\\left( \\frac{d}{\\varepsilon} \\right)$ or, with fixed context, $\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ are sufficient, where $d$ is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.08062v1",
        "title": "FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08062",
        "author": "Woosung Kim, Jinho Lee, Jongmin Lee, Byung-Jun Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08062v1 Announce Type: new \nAbstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in the presence of conflicting objectives, where linear scalarization is commonly used to reduce vector-valued returns into scalar signals. While effective for certain preferences, this approach cannot capture fairness-oriented goals such as Nash social welfare or max-min fairness, which require nonlinear and non-additive trade-offs. Although several online algorithms have been proposed for specific fairness objectives, a unified approach for optimizing nonlinear welfare criteria in the offline setting-where learning must proceed from a fixed dataset-remains unexplored. In this work, we present FairDICE, the first offline MORL framework that directly optimizes nonlinear welfare objective. FairDICE leverages distribution correction estimation to jointly account for welfare maximization and distributional regularization, enabling stable and sample-efficient learning without requiring explicit preference weights or exhaustive weight search. Across multiple offline benchmarks, FairDICE demonstrates strong fairness-aware performance compared to existing baselines."
      },
      {
        "id": "oai:arXiv.org:2506.08063v1",
        "title": "Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift",
        "link": "https://arxiv.org/abs/2506.08063",
        "author": "Songqiao Hu, Zeyi Liu, Xiao He",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08063v1 Announce Type: new \nAbstract: The change in data distribution over time, also known as concept drift, poses a significant challenge to the reliability of online learning methods. Existing methods typically require model retraining or drift detection, both of which demand high computational costs and are often unsuitable for real-time applications. To address these limitations, a lightweight, fast and efficient random vector functional-link network termed Lite-RVFL is proposed, capable of adapting to concept drift without drift detection and retraining. Lite-RVFL introduces a novel objective function that assigns weights exponentially increasing to new samples, thereby emphasizing recent data and enabling timely adaptation. Theoretical analysis confirms the feasibility of this objective function for drift adaptation, and an efficient incremental update rule is derived. Experimental results on a real-world safety assessment task validate the efficiency, effectiveness in adapting to drift, and potential to capture temporal patterns of Lite-RVFL. The source code is available at https://github.com/songqiaohu/Lite-RVFL."
      },
      {
        "id": "oai:arXiv.org:2506.08070v1",
        "title": "Info-Coevolution: An Efficient Framework for Data Model Coevolution",
        "link": "https://arxiv.org/abs/2506.08070",
        "author": "Ziheng Qin, Hailun Xu, Wei Chee Yew, Qi Jia, Yang Luo, Kanchan Sarkar, Danhui Guan, Kai Wang, Yang You",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08070v1 Announce Type: new \nAbstract: Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32\\% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50\\% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/."
      },
      {
        "id": "oai:arXiv.org:2506.08071v1",
        "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems",
        "link": "https://arxiv.org/abs/2506.08071",
        "author": "Aniket Rege, Zinnia Nie, Mahesh Ramesh, Unmesh Raskar, Zhuoran Yu, Aditya Kusupati, Yong Jae Lee, Ramya Korlakai Vinayak",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08071v1 Announce Type: new \nAbstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is heavily Amero and Euro-centric, underrepresenting the cultures of the Global South. To analyze these biases, we introduce CuRe, a novel and scalable benchmarking and scoring suite for cultural representativeness that leverages the marginal utility of attribute specification to T2I systems as a proxy for human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy built from the crowdsourced Wikimedia knowledge graph, with 300 cultural artifacts across 32 cultural subcategories grouped into six broad cultural axes (food, art, fashion, architecture, celebrations, and people). Our dataset's categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing their response to increasing the informativeness of text conditioning, enabling fine-grained cultural comparisons. We empirically observe much stronger correlations of our class of scorers to human judgments of perceptual similarity, image-text alignment, and cultural diversity across image encoders (SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2, Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0, and DALL-E 3. The code and dataset is open-sourced and available at https://aniketrege.github.io/cure/."
      },
      {
        "id": "oai:arXiv.org:2506.08113v1",
        "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting",
        "link": "https://arxiv.org/abs/2506.08113",
        "author": "Timoth\\'ee Hornek Amir Sartipi, Igor Tchappi, Gilbert Fridgen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08113v1 Announce Type: new \nAbstract: Accurate electricity price forecasting (EPF) is crucial for effective decision-making in power trading on the spot market. While recent advances in generative artificial intelligence (GenAI) and pre-trained large language models (LLMs) have inspired the development of numerous time series foundation models (TSFMs) for time series forecasting, their effectiveness in EPF remains uncertain. To address this gap, we benchmark several state-of-the-art pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and TimeGPT--against established statistical and machine learning (ML) methods for EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany, France, the Netherlands, Austria, and Belgium, we generate daily forecasts with a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the TSFMs, performing on par with traditional models. However, the biseasonal MSTL model, which captures daily and weekly seasonality, stands out for its consistent performance across countries and evaluation metrics, with no TSFM statistically outperforming it."
      },
      {
        "id": "oai:arXiv.org:2506.08120v1",
        "title": "Conservative Bias in Large Language Models: Measuring Relation Predictions",
        "link": "https://arxiv.org/abs/2506.08120",
        "author": "Toyin Aguda, Erik Wilson, Allan Anzagira, Simerjot Kaur, Charese Smiley",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08120v1 Announce Type: new \nAbstract: Large language models (LLMs) exhibit pronounced conservative bias in relation extraction tasks, frequently defaulting to No_Relation label when an appropriate option is unavailable. While this behavior helps prevent incorrect relation assignments, our analysis reveals that it also leads to significant information loss when reasoning is not explicitly included in the output. We systematically evaluate this trade-off across multiple prompts, datasets, and relation types, introducing the concept of Hobson's choice to capture scenarios where models opt for safe but uninformative labels over hallucinated ones. Our findings suggest that conservative bias occurs twice as often as hallucination. To quantify this effect, we use SBERT and LLM prompts to capture the semantic similarity between conservative bias behaviors in constrained prompts and labels generated from semi-constrained and open-ended prompts."
      },
      {
        "id": "oai:arXiv.org:2506.08123v1",
        "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA",
        "link": "https://arxiv.org/abs/2506.08123",
        "author": "Jacob Dineen (Arizona State University), Aswin RRV (Arizona State University), Qin Liu (University of California Davis), Zhikun Xu (Arizona State University), Xiao Ye (Arizona State University), Ming Shen (Arizona State University), Zhaonan Li (Arizona State University), Shijie Lu (Arizona State University), Chitta Baral (Arizona State University), Muhao Chen (University of California Davis), Ben Zhou (Arizona State University)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08123v1 Announce Type: new \nAbstract: Alignment of large language models with explicit principles (such as helpfulness, honesty, and harmlessness) is crucial for ensuring safe and reliable AI systems. However, standard reward-based alignment methods typically collapse diverse feedback into a single scalar reward, entangling multiple objectives into one opaque training signal, which hinders interpretability. In this work, we introduce QA-LIGN, an automatic symbolic reward decomposition approach that preserves the structure of each constitutional principle within the reward mechanism. Instead of training a black-box reward model that outputs a monolithic score, QA-LIGN formulates principle-specific evaluation questions and derives separate reward components for each principle, making it a drop-in reward model replacement. Experiments aligning an uncensored large language model with a set of constitutional principles demonstrate that QA-LIGN offers greater transparency and adaptability in the alignment process. At the same time, our approach achieves performance on par with or better than a DPO baseline. Overall, these results represent a step toward more interpretable and controllable alignment of language models, achieved without sacrificing end-task performance."
      },
      {
        "id": "oai:arXiv.org:2506.08125v1",
        "title": "Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08125",
        "author": "Hanbing Liu, Lang Cao, Yuanyi Ren, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, Dongmei Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08125v1 Announce Type: new \nAbstract: Large language models have demonstrated impressive reasoning capabilities, yet they often suffer from inefficiencies due to unnecessarily verbose or redundant outputs. While many works have explored reinforcement learning (RL) to enhance reasoning abilities, most primarily focus on improving accuracy, with limited attention to reasoning efficiency. Some existing approaches introduce direct length-based rewards to encourage brevity, but this often leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL framework that advances length-based reward design to boost efficient reasoning. Bingo incorporates two key mechanisms: a significance-aware length reward, which gradually guides the model to reduce only insignificant tokens, and a dynamic length reward, which initially encourages elaborate reasoning for hard questions but decays over time to improve overall efficiency. Experiments across multiple reasoning benchmarks show that Bingo improves both accuracy and efficiency. It outperforms the vanilla reward and several other length-based reward baselines in RL, achieving a favorable trade-off between accuracy and efficiency. These results underscore the potential of training LLMs explicitly for efficient reasoning."
      },
      {
        "id": "oai:arXiv.org:2506.08136v1",
        "title": "EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments",
        "link": "https://arxiv.org/abs/2506.08136",
        "author": "Zefang Liu, Yinzhu Quan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08136v1 Announce Type: new \nAbstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on complex, multimodal economic tasks in realistic web environments. The benchmark comprises 360 curated tasks from 82 authoritative websites spanning domains such as macroeconomics, labor, finance, trade, and public policy. Each task challenges agents to navigate live websites, interpret structured and visual content, interact with real interfaces, and extract precise, time-sensitive data through multi-step workflows. We construct the benchmark by prompting multiple large language models (LLMs) to generate candidate tasks, followed by rigorous human curation to ensure clarity, feasibility, and source reliability. Unlike prior work, EconWebArena emphasizes fidelity to authoritative data sources and the need for grounded web-based economic reasoning. We evaluate a diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure cases, and conduct ablation studies to assess the impact of visual grounding, plan-based reasoning, and interaction design. Our results reveal substantial performance gaps and highlight persistent challenges in grounding, navigation, and multimodal understanding, positioning EconWebArena as a rigorous testbed for economic web intelligence."
      },
      {
        "id": "oai:arXiv.org:2506.08137v1",
        "title": "IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation",
        "link": "https://arxiv.org/abs/2506.08137",
        "author": "Oishee Bintey Hoque, Abhijin Adiga, Aniruddha Adiga, Siddharth Chaudhary, Madhav V. Marathe, S. S. Ravi, Kirti Rajagopalan, Amanda Wilson, Samarth Swarup",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08137v1 Announce Type: new \nAbstract: Accurate canal network mapping is essential for water management, including irrigation planning and infrastructure maintenance. State-of-the-art semantic segmentation models for infrastructure mapping, such as roads, rely on large, well-annotated remote sensing datasets. However, incomplete or inadequate ground truth can hinder these learning approaches. Many infrastructure networks have graph-level properties such as reachability to a source (like canals) or connectivity (roads) that can be leveraged to improve these existing ground truth. This paper develops a novel iterative framework IGraSS, combining a semantic segmentation module-incorporating RGB and additional modalities (NDWI, DEM)-with a graph-based ground-truth refinement module. The segmentation module processes satellite imagery patches, while the refinement module operates on the entire data viewing the infrastructure network as a graph. Experiments show that IGraSS reduces unreachable canal segments from around 18% to 3%, and training with refined ground truth significantly improves canal identification. IGraSS serves as a robust framework for both refining noisy ground truth and mapping canal networks from remote sensing imagery. We also demonstrate the effectiveness and generalizability of IGraSS using road networks as an example, applying a different graph-theoretic constraint to complete road networks."
      },
      {
        "id": "oai:arXiv.org:2506.08139v1",
        "title": "Nearness of Neighbors Attention for Regression in Supervised Finetuning",
        "link": "https://arxiv.org/abs/2506.08139",
        "author": "Aviad Susman, Mayte Su\\'arez-Fari\\~nas, Joseph T Colonel",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08139v1 Announce Type: new \nAbstract: It is common in supervised machine learning to combine the feature extraction capabilities of neural networks with the predictive power of traditional algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This procedure involves performing supervised fine-tuning (SFT) on a domain-appropriate feature extractor, followed by training a traditional predictor on the resulting SFT embeddings. When used in this manner, traditional predictors often deliver increased performance over the SFT model itself, despite the fine-tuned feature extractor yielding embeddings specifically optimized for prediction by the neural network's final dense layer. This suggests that directly incorporating traditional algorithms into SFT as prediction layers may further improve performance. However, many traditional algorithms have not been implemented as neural network layers due to their non-differentiable nature and their unique optimization requirements. As a step towards solving this problem, we introduce the Nearness of Neighbors Attention (NONA) regression layer. NONA uses the mechanics of neural network attention and a novel learned attention-masking scheme to yield a differentiable proxy of the k-NN regression algorithm. Results on multiple unstructured datasets show improved performance over both dense layer prediction and k-NN on SFT embeddings for regression."
      },
      {
        "id": "oai:arXiv.org:2506.08140v1",
        "title": "AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists",
        "link": "https://arxiv.org/abs/2506.08140",
        "author": "Yifei Li, Hanane Nour Moussa, Ziru Chen, Shijie Chen, Botao Yu, Mingyi Xue, Benjamin Burns, Tzu-Yao Chiu, Vishal Dey, Zitong Lu, Chen Wei, Qianheng Zhang, Tianyu Zhang, Song Gao, Xuhui Huang, Xia Ning, Nesreen K. Ahmed, Ali Payani, Huan Sun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08140v1 Announce Type: new \nAbstract: Despite long-standing efforts in accelerating scientific discovery with AI, building AI co-scientists remains challenging due to limited high-quality data for training and evaluation. To tackle this data scarcity issue, we present AutoSDT, an automatic pipeline that collects high-quality coding tasks in real-world data-driven discovery workflows. AutoSDT leverages the coding capabilities and parametric knowledge of LLMs to search for diverse sources, select ecologically valid tasks, and synthesize accurate task instructions and code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404 coding tasks for data-driven discovery that covers four scientific disciplines and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the only automatically collected and the largest open dataset for data-driven scientific discovery. Expert feedback on a subset of 256 tasks shows the effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid, and 92.2% of the synthesized programs are functionally correct. Trained on AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show substantial improvement on two challenging data-driven discovery benchmarks, ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches the same level of performance as GPT-4o on ScienceAgentBench with a success rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it lifts the hypothesis matching score to 8.1, bringing a 17.4% relative improvement and closing the gap between open-weight models and GPT-4o."
      },
      {
        "id": "oai:arXiv.org:2506.08143v1",
        "title": "Accelerating Spectral Clustering under Fairness Constraints",
        "link": "https://arxiv.org/abs/2506.08143",
        "author": "Francesco Tonin, Alex Lambert, Johan A. K. Suykens, Volkan Cevher",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08143v1 Announce Type: new \nAbstract: Fairness of decision-making algorithms is an increasingly important issue. In this paper, we focus on spectral clustering with group fairness constraints, where every demographic group is represented in each cluster proportionally as in the general population. We present a new efficient method for fair spectral clustering (Fair SC) by casting the Fair SC problem within the difference of convex functions (DC) framework. To this end, we introduce a novel variable augmentation strategy and employ an alternating direction method of multipliers type of algorithm adapted to DC problems. We show that each associated subproblem can be solved efficiently, resulting in higher computational efficiency compared to prior work, which required a computationally expensive eigendecomposition. Numerical experiments demonstrate the effectiveness of our approach on both synthetic and real-world benchmarks, showing significant speedups in computation time over prior art, especially as the problem size grows. This work thus represents a considerable step forward towards the adoption of fair clustering in real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.08146v1",
        "title": "Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields",
        "link": "https://arxiv.org/abs/2506.08146",
        "author": "Vahidullah Ta\\c{c}, Amirhossein Amiri-Hezaveh, Manuel K. Rausch, Grace N. Bechtel, Francisco Sahli Costabal, Adrian Buganza Tepole",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08146v1 Announce Type: new \nAbstract: We propose a new framework for identifying mechanical properties of heterogeneous materials without a closed-form constitutive equation. Given a full-field measurement of the displacement field, for instance as obtained from digital image correlation (DIC), a continuous approximation of the strain field is obtained by training a neural network that incorporates Fourier features to effectively capture sharp gradients in the data. A physics-based data-driven method built upon ordinary neural differential equations (NODEs) is employed to discover constitutive equations. The NODE framework can represent arbitrary materials while satisfying constraints in the theory of constitutive equations by default. To account for heterogeneity, a hyper-network is defined, where the input is the material coordinate system, and the output is the NODE-based constitutive equation. The parameters of the hyper-network are optimized by minimizing a multi-objective loss function that includes penalty terms for violations of the strong form of the equilibrium equations of elasticity and the associated Neumann boundary conditions. We showcase the framework with several numerical examples, including heterogeneity arising from variations in material parameters, spatial transitions from isotropy to anisotropy, material identification in the presence of noise, and, ultimately, application to experimental data. As the numerical results suggest, the proposed approach is robust and general in identifying the mechanical properties of heterogeneous materials with very few assumptions, making it a suitable alternative to classical inverse methods."
      },
      {
        "id": "oai:arXiv.org:2506.08147v1",
        "title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models",
        "link": "https://arxiv.org/abs/2506.08147",
        "author": "Muhammad Usman, Muhammad Ahmad, M. Shahiki Tash, Irina Gelbukh, Rolando Quintero Tellez, Grigori Sidorov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08147v1 Announce Type: new \nAbstract: Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide."
      },
      {
        "id": "oai:arXiv.org:2506.08158v1",
        "title": "ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding",
        "link": "https://arxiv.org/abs/2506.08158",
        "author": "Lijing Zhu, Qizhen Lan, Qing Tian, Wenbo Sun, Li Yang, Lu Xia, Yixin Xie, Xi Xiao, Tiehang Duan, Cui Tao, Shuteng Niu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08158v1 Announce Type: new \nAbstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge while preserving past information. However, existing methods struggle with efficiency and scalability due to two key limitations: (1) suboptimal knowledge preservation between snapshots caused by manually designed node/relation importance scores that ignore graph dependencies relevant to the downstream task, and (2) computationally expensive graph traversal for node/relation importance calculation, leading to slow training and high memory overhead. To address these limitations, we introduce ETT-CKGE (Efficient, Task-driven, Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE method that leverages efficient task-driven tokens for efficient and effective knowledge transfer between snapshots. Our method introduces a set of learnable tokens that directly capture task-relevant signals, eliminating the need for explicit node scoring or traversal. These tokens serve as consistent and reusable guidance across snapshots, enabling efficient token-masked embedding alignment between snapshots. Importantly, knowledge transfer is achieved through simple matrix operations, significantly reducing training time and memory usage. Extensive experiments across six benchmark datasets demonstrate that ETT-CKGE consistently achieves superior or competitive predictive performance, while substantially improving training efficiency and scalability compared to state-of-the-art CKGE methods. The code is available at: https://github.com/lijingzhu1/ETT-CKGE/tree/main"
      },
      {
        "id": "oai:arXiv.org:2506.08163v1",
        "title": "Spectral Domain Neural Reconstruction for Passband FMCW Radars",
        "link": "https://arxiv.org/abs/2506.08163",
        "author": "Harshvardhan Takawale, Nirupam Roy",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08163v1 Announce Type: new \nAbstract: We present SpINRv2, a neural framework for high-fidelity volumetric reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar. Extending our prior work (SpINR), this version introduces enhancements that allow accurate learning under high start frequencies-where phase aliasing and sub-bin ambiguity become prominent. Our core contribution is a fully differentiable frequency-domain forward model that captures the complex radar response using closed-form synthesis, paired with an implicit neural representation (INR) for continuous volumetric scene modeling. Unlike time-domain baselines, SpINRv2 directly supervises the complex frequency spectrum, preserving spectral fidelity while drastically reducing computational overhead. Additionally, we introduce sparsity and smoothness regularization to disambiguate sub-bin ambiguities that arise at fine range resolutions. Experimental results show that SpINRv2 significantly outperforms both classical and learning-based baselines, especially under high-frequency regimes, establishing a new benchmark for neural radar-based 3D imaging."
      },
      {
        "id": "oai:arXiv.org:2506.08164v1",
        "title": "BLUR: A Bi-Level Optimization Approach for LLM Unlearning",
        "link": "https://arxiv.org/abs/2506.08164",
        "author": "Hadi Reisizadeh, Jinghan Jia, Zhiqi Bu, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Sijia Liu, Mingyi Hong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08164v1 Announce Type: new \nAbstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities acquired during training has proven vital for ensuring compliance with data regulations and promoting ethical practices in generative AI. Although there are growing interests in developing various unlearning algorithms, it remains unclear how to best formulate the unlearning problem. The most popular formulation uses a weighted sum of forget and retain loss, but it often leads to performance degradation due to the inherent trade-off between forget and retain losses. In this work, we argue that it is important to model the hierarchical structure of the unlearning problem, where the forget problem (which \\textit{unlearns} certain knowledge and/or capabilities) takes priority over the retain problem (which preserves model utility). This hierarchical structure naturally leads to a bi-level optimization formulation where the lower-level objective focuses on minimizing the forget loss, while the upper-level objective aims to maintain the model's utility. Based on this new formulation, we propose a novel algorithm, termed Bi-Level UnleaRning (\\texttt{BLUR}), which not only possesses strong theoretical guarantees but more importantly, delivers superior performance. In particular, our extensive experiments demonstrate that \\texttt{BLUR} consistently outperforms all the state-of-the-art algorithms across various unlearning tasks, models, and metrics. Codes are available at https://github.com/OptimAI-Lab/BLURLLMUnlearning."
      },
      {
        "id": "oai:arXiv.org:2506.08167v1",
        "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data",
        "link": "https://arxiv.org/abs/2506.08167",
        "author": "Sunny Gupta, Nikita Jangid, Amit Sethi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08167v1 Announce Type: new \nAbstract: Federated Learning (FL) often suffers from severe performance degradation when faced with non-IID data, largely due to local classifier bias. Traditional remedies such as global model regularization or layer freezing either incur high computational costs or struggle to adapt to feature shifts. In this work, we propose UniVarFL, a novel FL framework that emulates IID-like training dynamics directly at the client level, eliminating the need for global model dependency. UniVarFL leverages two complementary regularization strategies during local training: Classifier Variance Regularization, which aligns class-wise probability distributions with those expected under IID conditions, effectively mitigating local classifier bias; and Hyperspherical Uniformity Regularization, which encourages a uniform distribution of feature representations across the hypersphere, thereby enhancing the model's ability to generalize under diverse data distributions. Extensive experiments on multiple benchmark datasets demonstrate that UniVarFL outperforms existing methods in accuracy, highlighting its potential as a highly scalable and efficient solution for real-world FL deployments, especially in resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL"
      },
      {
        "id": "oai:arXiv.org:2506.08169v1",
        "title": "Federated Learning on Stochastic Neural Networks",
        "link": "https://arxiv.org/abs/2506.08169",
        "author": "Jingqiao Tang (Department of Mathematics at Florida State University, Tallahassee, Florida, USA), Ryan Bausback (Department of Mathematics at Florida State University, Tallahassee, Florida, USA), Feng Bao (Department of Mathematics at Florida State University, Tallahassee, Florida, USA), Richard Archibald (Division of Computer Science and Mathematics, Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08169v1 Announce Type: new \nAbstract: Federated learning is a machine learning paradigm that leverages edge computing on client devices to optimize models while maintaining user privacy by ensuring that local data remains on the device. However, since all data is collected by clients, federated learning is susceptible to latent noise in local datasets. Factors such as limited measurement capabilities or human errors may introduce inaccuracies in client data. To address this challenge, we propose the use of a stochastic neural network as the local model within the federated learning framework. Stochastic neural networks not only facilitate the estimation of the true underlying states of the data but also enable the quantification of latent noise. We refer to our federated learning approach, which incorporates stochastic neural networks as local models, as Federated stochastic neural networks. We will present numerical experiments demonstrating the performance and effectiveness of our method, particularly in handling non-independent and identically distributed data."
      },
      {
        "id": "oai:arXiv.org:2506.08172v1",
        "title": "Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction",
        "link": "https://arxiv.org/abs/2506.08172",
        "author": "Gerardo Aleman Manzanarez, Nora de la Cruz Arana, Jorge Garcia Flores, Yobany Garcia Medina, Raul Monroy, Nathalie Pernelle",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08172v1 Announce Type: new \nAbstract: Automated story writing has been a subject of study for over 60 years. Large language models can generate narratively consistent and linguistically coherent short fiction texts. Despite these advancements, rigorous assessment of such outputs for literary merit - especially concerning aesthetic qualities - has received scant attention. In this paper, we address the challenge of evaluating AI-generated microfictions and argue that this task requires consideration of literary criteria across various aspects of the text, such as thematic coherence, textual clarity, interpretive depth, and aesthetic quality. To facilitate this, we present GrAImes: an evaluation protocol grounded in literary theory, specifically drawing from a literary perspective, to offer an objective framework for assessing AI-generated microfiction. Furthermore, we report the results of our validation of the evaluation protocol, as answered by both literature experts and literary enthusiasts. This protocol will serve as a foundation for evaluating automatically generated microfictions and assessing their literary value."
      },
      {
        "id": "oai:arXiv.org:2506.08174v1",
        "title": "LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding",
        "link": "https://arxiv.org/abs/2506.08174",
        "author": "Li Weigang, Pedro Carvalho Brom",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08174v1 Announce Type: new \nAbstract: The rapid growth of English technical terms challenges traditional expert-driven standardization, especially in fast-evolving fields like AI and quantum computing. Manual methods struggle to ensure multilingual consistency. We propose \\textbf{LLM-BT}, a back-translation framework powered by large language models (LLMs) to automate terminology verification and standardization via cross-lingual semantic alignment. Our contributions are: \\textbf{(1) Term-Level Consistency Validation:} Using English $\\rightarrow$ intermediate language $\\rightarrow$ English back-translation, LLM-BT achieves high term consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies showing over 90\\% exact or semantic matches. \\textbf{(2) Multi-Path Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize'' pipeline integrates serial (e.g., EN $\\rightarrow$ ZHcn $\\rightarrow$ ZHtw $\\rightarrow$ EN) and parallel (e.g., EN $\\rightarrow$ Chinese/Portuguese $\\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\\%). \\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as dynamic semantic embedding, revealing latent meaning trajectories. Unlike static embeddings, LLM-BT provides transparent path-based embeddings shaped by model evolution. LLM-BT transforms back-translation into an active engine for multilingual terminology standardization, enabling human--AI collaboration: machines ensure semantic fidelity, humans guide cultural interpretation. This infrastructure supports terminology governance across scientific and technological fields worldwide."
      },
      {
        "id": "oai:arXiv.org:2506.08176v1",
        "title": "FedGA-Tree: Federated Decision Tree using Genetic Algorithm",
        "link": "https://arxiv.org/abs/2506.08176",
        "author": "Anh V Nguyen, Diego Klabjan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08176v1 Announce Type: new \nAbstract: In recent years, with rising concerns for data privacy, Federated Learning has gained prominence, as it enables collaborative training without the aggregation of raw data from participating clients. However, much of the current focus has been on parametric gradient-based models, while nonparametric counterparts such as decision tree are relatively understudied. Existing methods for adapting decision trees to Federated Learning generally combine a greedy tree-building algorithm with differential privacy to produce a global model for all clients. These methods are limited to classification trees and categorical data due to the constraints of differential privacy. In this paper, we explore an alternative approach that utilizes Genetic Algorithm to facilitate the construction of personalized decision trees and accommodate categorical and numerical data, thus allowing for both classification and regression trees. Comprehensive experiments demonstrate that our method surpasses decision trees trained solely on local data and a benchmark algorithm."
      },
      {
        "id": "oai:arXiv.org:2506.08184v1",
        "title": "Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length",
        "link": "https://arxiv.org/abs/2506.08184",
        "author": "Chupei Wang (University of Virginia), Jiaqiu Vince Sun (New York University)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08184v1 Announce Type: new \nAbstract: Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval."
      },
      {
        "id": "oai:arXiv.org:2506.08185v1",
        "title": "Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework",
        "link": "https://arxiv.org/abs/2506.08185",
        "author": "Huixin Zhan, Jason H. Moore",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08185v1 Announce Type: new \nAbstract: Surgeons exhibit distinct operating styles due to differences in training, experience, and motor behavior - yet current AI systems often ignore this personalization signal. We propose a novel approach to model fine-grained, surgeon-specific fingerprinting in robotic surgery using a discrete diffusion framework integrated with a vision-language-action (VLA) pipeline. Our method formulates gesture prediction as a structured sequence denoising task, conditioned on multimodal inputs including endoscopic video, surgical intent language, and a privacy-aware embedding of surgeon identity and skill. Personalized surgeon fingerprinting is encoded through natural language prompts using third-party language models, allowing the model to retain individual behavioral style without exposing explicit identity. We evaluate our method on the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture sequences while learning meaningful motion fingerprints unique to each surgeon. To quantify the privacy implications of personalization, we perform membership inference attacks and find that more expressive embeddings improve task performance but simultaneously increase susceptibility to identity leakage. These findings demonstrate that while personalized embeddings improve performance, they also increase vulnerability to identity leakage, revealing the importance of balancing personalization with privacy risk in surgical modeling. Code is available at: https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting."
      },
      {
        "id": "oai:arXiv.org:2506.08189v1",
        "title": "Open World Scene Graph Generation using Vision Language Models",
        "link": "https://arxiv.org/abs/2506.08189",
        "author": "Amartya Dutta, Kazi Sajeed Mehrab, Medha Sawhney, Abhilash Neog, Mridul Khurana, Sepideh Fatemi, Aanish Pradhan, M. Maruf, Ismini Lourentzou, Arka Daw, Anuj Karpatne",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08189v1 Announce Type: new \nAbstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and distill their salient pairwise relationships. Most methods depend on dataset-specific supervision to learn the variety of interactions, restricting their usefulness in open-world settings, involving novel objects and/or relations. Even methods that leverage large Vision Language Models (VLMs) typically require benchmark-specific fine-tuning. We introduce Open-World SGG, a training-free, efficient, model-agnostic framework that taps directly into the pretrained knowledge of VLMs to produce scene graphs with zero additional learning. Casting SGG as a zero-shot structured-reasoning problem, our method combines multimodal prompting, embedding alignment, and a lightweight pair-refinement strategy, enabling inference over unseen object vocabularies and relation sets. To assess this setting, we formalize an Open-World evaluation protocol that measures performance when no SGG-specific data have been observed either in terms of objects and relations. Experiments on Visual Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate the capacity of pretrained VLMs to perform relational understanding without task-level training."
      },
      {
        "id": "oai:arXiv.org:2506.08191v1",
        "title": "Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes",
        "link": "https://arxiv.org/abs/2506.08191",
        "author": "Antoni Nowinowski, Krzysztof Krawiec",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08191v1 Announce Type: new \nAbstract: This study builds on the architecture of the Disentangler of Visual Priors (DVP), a type of autoencoder that learns to interpret scenes by decomposing the perceived objects into independent visual aspects of shape, size, orientation, and color appearance. These aspects are expressed as latent parameters which control a differentiable renderer that performs image reconstruction, so that the model can be trained end-to-end with gradient using reconstruction loss. In this study, we extend the original DVP so that it can handle multiple objects in a scene. We also exploit the interpretability of its latent by using the decoder to sample additional training examples and devising alternative training modes that rely on loss functions defined not only in the image space, but also in the latent space. This significantly facilitates training, which is otherwise challenging due to the presence of extensive plateaus in the image-space reconstruction loss. To examine the performance of this approach, we propose a new benchmark featuring multiple 2D objects, which subsumes the previously proposed Multi-dSprites dataset while being more parameterizable. We compare the DVP extended in these ways with two baselines (MONet and LIVE) and demonstrate its superiority in terms of reconstruction quality and capacity to decompose overlapping objects. We also analyze the gradients induced by the considered loss functions, explain how they impact the efficacy of training, and discuss the limitations of differentiable rendering in autoencoders and the ways in which they can be addressed."
      },
      {
        "id": "oai:arXiv.org:2506.08194v1",
        "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra",
        "link": "https://arxiv.org/abs/2506.08194",
        "author": "Mateusz Michalkiewicz, Anekha Sokhal, Tadeusz Michalkiewicz, Piotr Pawlikowski, Mahsa Baktashmotlagh, Varun Jampani, Guha Balakrishnan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08194v1 Announce Type: new \nAbstract: Monocular 3D reconstruction methods and vision-language models (VLMs) demonstrate impressive results on standard benchmarks, yet their true understanding of geometric properties remains unclear. We introduce GIQ , a comprehensive benchmark specifically designed to evaluate the geometric reasoning capabilities of vision and vision-language foundation models. GIQ comprises synthetic and real-world images of 224 diverse polyhedra - including Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and compound shapes - covering varying levels of complexity and symmetry. Through systematic experiments involving monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification tasks, we reveal significant shortcomings in current models. State-of-the-art reconstruction algorithms trained on extensive 3D datasets struggle to reconstruct even basic geometric forms accurately. While foundation models effectively detect specific 3D symmetry elements via linear probing, they falter significantly in tasks requiring detailed geometric differentiation, such as mental rotation. Moreover, advanced vision-language assistants exhibit remarkably low accuracy on complex polyhedra, systematically misinterpreting basic properties like face geometry, convexity, and compound structures. GIQ is publicly available, providing a structured platform to highlight and address critical gaps in geometric intelligence, facilitating future progress in robust, geometry-aware representation learning."
      },
      {
        "id": "oai:arXiv.org:2506.08201v1",
        "title": "Correlated Noise Mechanisms for Differentially Private Learning",
        "link": "https://arxiv.org/abs/2506.08201",
        "author": "Krishna Pillutla, Jalaj Upadhyay, Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, Arun Ganesh, Monika Henzinger, Jonathan Katz, Ryan McKenna, H. Brendan McMahan, Keith Rush, Thomas Steinke, Abhradeep Thakurta",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08201v1 Announce Type: new \nAbstract: This monograph explores the design and analysis of correlated noise mechanisms for differential privacy (DP), focusing on their application to private training of AI and machine learning models via the core primitive of estimation of weighted prefix sums. While typical DP mechanisms inject independent noise into each step of a stochastic gradient (SGD) learning algorithm in order to protect the privacy of the training data, a growing body of recent research demonstrates that introducing (anti-)correlations in the noise can significantly improve privacy-utility trade-offs by carefully canceling out some of the noise added on earlier steps in subsequent steps. Such correlated noise mechanisms, known variously as matrix mechanisms, factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when applied to learning algorithms, have also been influential in practice, with industrial deployment at a global scale."
      },
      {
        "id": "oai:arXiv.org:2506.08205v1",
        "title": "A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts",
        "link": "https://arxiv.org/abs/2506.08205",
        "author": "Shadab Anwar Shaikh, Kranthi Balusu, Ayoub Soulami",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08205v1 Announce Type: new \nAbstract: Residual stresses, which remain within a component after processing, can deteriorate performance. Accurately determining their full-field distributions is essential for optimizing the structural integrity and longevity. However, the experimental effort required for full-field characterization is impractical. Given these challenges, this work proposes a machine learning (ML) based Residual Stress Generator (RSG) to infer full-field stresses from limited measurements. An extensive dataset was initially constructed by performing numerous process simulations with a diverse parameter set. A ML model based on U-Net architecture was then trained to learn the underlying structure through systematic hyperparameter tuning. Then, the model's ability to generate simulated stresses was evaluated, and it was ultimately tested on actual characterization data to validate its effectiveness. The model's prediction of simulated stresses shows that it achieved excellent predictive accuracy and exhibited a significant degree of generalization, indicating that it successfully learnt the latent structure of residual stress distribution. The RSG's performance in predicting experimentally characterized data highlights the feasibility of the proposed approach in providing a comprehensive understanding of residual stress distributions from limited measurements, thereby significantly reducing experimental efforts."
      },
      {
        "id": "oai:arXiv.org:2506.08210v1",
        "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2506.08210",
        "author": "Andrew Z. Wang, Songwei Ge, Tero Karras, Ming-Yu Liu, Yogesh Balaji",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08210v1 Announce Type: new \nAbstract: Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills."
      },
      {
        "id": "oai:arXiv.org:2506.08214v1",
        "title": "Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation",
        "link": "https://arxiv.org/abs/2506.08214",
        "author": "Ioannis Iakovidis, Zahra Kalantari, Amir Hossein Payberah, Fernando Jaramillo, Francisco Pena Escobar",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08214v1 Announce Type: new \nAbstract: In recent years the wide availability of high-resolution radar satellite images along with the advancement of computer vision models have enabled the remote monitoring of the surface area of wetlands. However, these models require large amounts of manually annotated satellite images, which are slow and expensive to produce. To overcome this problem, self-supervised training methods have been deployed to train models without using annotated data. In this paper we use a combination of deep clustering and negative sampling to train a model to segment radar satellite images into areas that separate water from land without the use of any manual annotations. Furthermore, we implement an ensemble version of the model to reduce variance and improve performance. Compared to a single fully-supervised model using the same architecture, our ensemble of self-supervised models achieves a 0.02 improvement in the Intersection Over Union metric over our test dataset."
      },
      {
        "id": "oai:arXiv.org:2506.08216v1",
        "title": "What makes an Ensemble (Un) Interpretable?",
        "link": "https://arxiv.org/abs/2506.08216",
        "author": "Shahaf Bassan, Guy Amir, Meirav Zehavi, Guy Katz",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08216v1 Announce Type: new \nAbstract: Ensemble models are widely recognized in the ML community for their limited interpretability. For instance, while a single decision tree is considered interpretable, ensembles of trees (e.g., boosted trees) are often treated as black-boxes. Despite this folklore recognition, there remains a lack of rigorous mathematical understanding of what particularly makes an ensemble (un)-interpretable, including how fundamental factors like the (1) *number*, (2) *size*, and (3) *type* of base models influence its interpretability. In this work, we seek to bridge this gap by applying concepts from computational complexity theory to study the challenges of generating explanations for various ensemble configurations. Our analysis uncovers nuanced complexity patterns influenced by various factors. For example, we demonstrate that under standard complexity assumptions like P$\\neq$NP, interpreting ensembles remains intractable even when base models are of constant size. Surprisingly, the complexity changes drastically with the number of base models: small ensembles of decision trees are efficiently interpretable, whereas interpreting ensembles with even a constant number of linear models remains intractable. We believe that our findings provide a more robust foundation for understanding the interpretability of ensembles, emphasizing the benefits of examining it through a computational complexity lens."
      },
      {
        "id": "oai:arXiv.org:2506.08220v1",
        "title": "Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence",
        "link": "https://arxiv.org/abs/2506.08220",
        "author": "Octave Mariotti, Zhipeng Du, Yash Bhalgat, Oisin Mac Aodha, Hakan Bilen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08220v1 Announce Type: new \nAbstract: Semantic correspondence (SC) aims to establish semantically meaningful matches across different instances of an object category. We illustrate how recent supervised SC methods remain limited in their ability to generalize beyond sparsely annotated training keypoints, effectively acting as keypoint detectors. To address this, we propose a novel approach for learning dense correspondences by lifting 2D keypoints into a canonical 3D space using monocular depth estimation. Our method constructs a continuous canonical manifold that captures object geometry without requiring explicit 3D supervision or camera annotations. Additionally, we introduce SPair-U, an extension of SPair-71k with novel keypoint annotations, to better assess generalization. Experiments not only demonstrate that our model significantly outperforms supervised baselines on unseen keypoints, highlighting its effectiveness in learning robust correspondences, but that unsupervised baselines outperform supervised counterparts when generalized across different datasets."
      },
      {
        "id": "oai:arXiv.org:2506.08221v1",
        "title": "\"I Wrote, I Paused, I Rewrote\" Teaching LLMs to Read Between the Lines of Student Writing",
        "link": "https://arxiv.org/abs/2506.08221",
        "author": "Samra Zafar, Shaheer Minhas, Syed Ali Hassan Zaidi, Arfa Naeem, Zahra Ali",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08221v1 Announce Type: new \nAbstract: Large language models(LLMs) like Gemini are becoming common tools for supporting student writing. But most of their feedback is based only on the final essay missing important context about how that text was written. In this paper, we explore whether using writing process data, collected through keystroke logging and periodic snapshots, can help LLMs give feedback that better reflects how learners think and revise while writing. We built a digital writing tool that captures both what students type and how their essays evolve over time. Twenty students used this tool to write timed essays, which were then evaluated in two ways: (i) LLM generated feedback using both the final essay and the full writing trace, and (ii) After the task, students completed surveys about how useful and relatable they found the feedback. Early results show that learners preferred the process-aware LLM feedback, finding it more in tune with their own thinking. We also found that certain types of edits, like adding new content or reorganizing paragraphs, aligned closely with higher scores in areas like coherence and elaboration. Our findings suggest that making LLMs more aware of the writing process can lead to feedback that feels more meaningful, personal, and supportive."
      },
      {
        "id": "oai:arXiv.org:2506.08226v1",
        "title": "Mondrian: Transformer Operators via Domain Decomposition",
        "link": "https://arxiv.org/abs/2506.08226",
        "author": "Arthur Feeney, Kuei-Hsiang Huang, Aparna Chandramowlishwaran",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08226v1 Announce Type: new \nAbstract: Operator learning enables data-driven modeling of partial differential equations (PDEs) by learning mappings between function spaces. However, scaling transformer-based operator models to high-resolution, multiscale domains remains a challenge due to the quadratic cost of attention and its coupling to discretization. We introduce \\textbf{Mondrian}, transformer operators that decompose a domain into non-overlapping subdomains and apply attention over sequences of subdomain-restricted functions. Leveraging principles from domain decomposition, Mondrian decouples attention from discretization. Within each subdomain, it replaces standard layers with expressive neural operators, and attention across subdomains is computed via softmax-based inner products over functions. The formulation naturally extends to hierarchical windowed and neighborhood attention, supporting both local and global interactions. Mondrian achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating resolution scaling without retraining. These results highlight the promise of domain-decomposed attention for scalable and general-purpose neural operators."
      },
      {
        "id": "oai:arXiv.org:2506.08227v1",
        "title": "A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks",
        "link": "https://arxiv.org/abs/2506.08227",
        "author": "Vishaal Udandarao, Mehdi Cherti, Shyamgopal Karthik, Jenia Jitsev, Samuel Albanie, Matthias Bethge",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08227v1 Announce Type: new \nAbstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for measuring compositional understanding capabilities of vision-language models (VLMs). We scrutinize design choices in their construction, including data source (e.g. MS-COCO) and curation procedures (e.g. constructing negative images/captions), uncovering several inherent biases across most benchmarks. We find that blind heuristics (e.g. token-length, log-likelihood under a language model) perform on par with CLIP models, indicating that these benchmarks do not effectively measure compositional understanding. We demonstrate that the underlying factor is a distribution asymmetry between positive and negative images/captions, induced by the benchmark construction procedures. To mitigate these issues, we provide a few key recommendations for constructing more robust vision-language compositional understanding benchmarks, that would be less prone to such simple attacks."
      },
      {
        "id": "oai:arXiv.org:2506.08228v1",
        "title": "Scaling Laws of Motion Forecasting and Planning -- A Technical Report",
        "link": "https://arxiv.org/abs/2506.08228",
        "author": "Mustafa Baniodeh, Kratarth Goel, Scott Ettinger, Carlos Fuertes, Ari Seff, Tim Shen, Cole Gulino, Chenjie Yang, Ghassen Jerfel, Dokook Choe, Rui Wang, Vinutha Kallem, Sergio Casas, Rami Al-Rfou, Benjamin Sapp, Dragomir Anguelov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08228v1 Announce Type: new \nAbstract: We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training."
      },
      {
        "id": "oai:arXiv.org:2506.08231v1",
        "title": "Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework",
        "link": "https://arxiv.org/abs/2506.08231",
        "author": "Melissa Estevez, Nisha Singh, Lauren Dyson, Blythe Adamson, Qianyu Yuan, Megan W. Hildner, Erin Fidyk, Olive Mbah, Farhad Khan, Kathi Seidl-Rathkopf, Aaron B. Cohen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08231v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly used to extract clinical data from electronic health records (EHRs), offering significant improvements in scalability and efficiency for real-world data (RWD) curation in oncology. However, the adoption of LLMs introduces new challenges in ensuring the reliability, accuracy, and fairness of extracted data, which are essential for research, regulatory, and clinical applications. Existing quality assurance frameworks for RWD and artificial intelligence do not fully address the unique error modes and complexities associated with LLM-extracted data. In this paper, we propose a comprehensive framework for evaluating the quality of clinical data extracted by LLMs. The framework integrates variable-level performance benchmarking against expert human abstraction, automated verification checks for internal consistency and plausibility, and replication analyses comparing LLM-extracted data to human-abstracted datasets or external standards. This multidimensional approach enables the identification of variables most in need of improvement, systematic detection of latent errors, and confirmation of dataset fitness-for-purpose in real-world research. Additionally, the framework supports bias assessment by stratifying metrics across demographic subgroups. By providing a rigorous and transparent method for assessing LLM-extracted RWD, this framework advances industry standards and supports the trustworthy use of AI-powered evidence generation in oncology research and practice."
      },
      {
        "id": "oai:arXiv.org:2506.08234v1",
        "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions",
        "link": "https://arxiv.org/abs/2506.08234",
        "author": "Yu-Ang Lee, Guan-Ting Yi, Mei-Yi Liu, Jui-Chao Lu, Guan-Bo Yang, Yun-Nung Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08234v1 Announce Type: new \nAbstract: Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey."
      },
      {
        "id": "oai:arXiv.org:2506.08235v1",
        "title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
        "link": "https://arxiv.org/abs/2506.08235",
        "author": "Shashidhar Reddy Javaji, Yupeng Cao, Haohang Li, Yangyang Yu, Nikhil Muralidhar, Zining Zhu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08235v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers."
      },
      {
        "id": "oai:arXiv.org:2506.08240v1",
        "title": "Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations",
        "link": "https://arxiv.org/abs/2506.08240",
        "author": "Dongkyu Cho, Rumi Chunara",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08240v1 Announce Type: new \nAbstract: Data augmentation is a promising tool for enhancing out-of-distribution generalization, where the key is to produce diverse, challenging variations of the source domain via costly targeted augmentations that maximize its generalization effect. Conversely, random augmentation is inexpensive but is deemed suboptimal due to its limited effect. In this paper, we revisit random augmentation and explore methods to address its shortcomings. We show that the stochastic nature of random augmentation can produce a set of colliding augmentations that distorts the learned features, similar to catastrophic forgetting. We propose a simple solution that improves the generalization effect of random augmentation by addressing forgetting, which displays strong generalization performance across various single source domain generalization (sDG) benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08243v1",
        "title": "Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic",
        "link": "https://arxiv.org/abs/2506.08243",
        "author": "Zhenjiang Mao, Artem Bisliouk, Rohith Reddy Nama, Ivan Ruchkin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08243v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown impressive performance in mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting. However, they tend to produce highly confident yet incorrect outputs, which poses significant risks in domains like education, where users may lack the expertise to assess reasoning steps. To address this, we propose a structured framework that models stepwise confidence as a temporal signal and evaluates it using Signal Temporal Logic (STL). In particular, we define formal STL-based constraints to capture desirable temporal properties and compute robustness scores that serve as structured, interpretable confidence estimates. Our approach also introduces a set of uncertainty reshaping strategies to enforce smoothness, monotonicity, and causal consistency across the reasoning trajectory. Experiments show that our approach consistently improves calibration metrics and provides more reliable uncertainty estimates than conventional confidence aggregation and post-hoc calibration."
      },
      {
        "id": "oai:arXiv.org:2506.08244v1",
        "title": "Parameter-free approximate equivariance for tasks with finite group symmetry",
        "link": "https://arxiv.org/abs/2506.08244",
        "author": "Riccardo Ali, Pietro Li\\`o, Jamie Vicary",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08244v1 Announce Type: new \nAbstract: Equivariant neural networks incorporate symmetries through group actions, embedding them as an inductive bias to improve performance on a wide variety of tasks. However, existing equivariant methods can be computationally intensive, with high parameter counts, and are often tied to a specific architecture. We propose a simple zero-parameter approach that imposes approximate equivariance for a finite group in the latent representation, as an additional term in the loss function. We conduct experiments which allow the network to learn a group representation on the latent space, and show in every case it prefers to learn the regular representation. Fixing this action on the latent space, this yields a simple method to impose approximate equivariance as an additional loss penalty. We benchmark our approach on three datasets and compare it against several existing equivariant methods, showing that in many cases it achieves similar or better performance for a fraction of the parameters."
      },
      {
        "id": "oai:arXiv.org:2506.08255v1",
        "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense",
        "link": "https://arxiv.org/abs/2506.08255",
        "author": "Patryk Krukowski, {\\L}ukasz Gorczyca, Piotr Helm, Kamil Ksi\\k{a}\\.zek, Przemys{\\l}aw Spurek",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08255v1 Announce Type: new \nAbstract: Traditional deep neural networks suffer from several limitations, including catastrophic forgetting. When models are adapted to new datasets, they tend to quickly forget previously learned knowledge. Another significant issue is the lack of robustness to even small perturbations in the input data. In practice, we can often easily perform adversarial attacks and change the network's predictions, adding minimal noise to the input. Dedicated architectures and training procedures can solve each of the above problems separately. Unfortunately, currently, no model can simultaneously address both catastrophic forgetting and vulnerability to adversarial attacks. We introduce SHIELD (Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel approach that integrates a hypernetwork-based continual learning approach with interval arithmetic. SHIELD use the hypernetwork to transfer trainable task embedding vectors into the weights of a target model dedicated to specific data. This paradigm allows for the dynamic generation of separate networks for each subtask, while the hypernetwork aggregates and analyzes information across all tasks. The target model takes in the input a data sample with a defined interval range, and by creating a hypercube, produces a prediction for the given range. Therefore, such target models provide strict guarantees against all possible attacks for data samples within the interval range. Our approach enhances security without sacrificing network adaptability, addressing the overlooked challenge of safety in continual learning."
      },
      {
        "id": "oai:arXiv.org:2506.08257v1",
        "title": "Highly Compressed Tokenizer Can Generate Without Training",
        "link": "https://arxiv.org/abs/2506.08257",
        "author": "L. Lao Beyer, T. Li, X. Chen, S. Karaman, K. He",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08257v1 Announce Type: new \nAbstract: Commonly used image tokenizers produce a 2D grid of spatially arranged tokens. In contrast, so-called 1D image tokenizers represent images as highly compressed one-dimensional sequences of as few as 32 discrete tokens. We find that the high degree of compression achieved by a 1D tokenizer with vector quantization enables image editing and generative capabilities through heuristic manipulation of tokens, demonstrating that even very crude manipulations -- such as copying and replacing tokens between latent representations of images -- enable fine-grained image editing by transferring appearance and semantic attributes. Motivated by the expressivity of the 1D tokenizer's latent space, we construct an image generation pipeline leveraging gradient-based test-time optimization of tokens with plug-and-play loss functions such as reconstruction or CLIP similarity. Our approach is demonstrated for inpainting and text-guided image editing use cases, and can generate diverse and realistic samples without requiring training of any generative model."
      },
      {
        "id": "oai:arXiv.org:2506.08260v1",
        "title": "Automatic Generation of Inference Making Questions for Reading Comprehension Assessments",
        "link": "https://arxiv.org/abs/2506.08260",
        "author": "Wanjing Anya Ma, Michael Flor, Zuowei Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08260v1 Announce Type: new \nAbstract: Inference making is an essential but complex skill in reading comprehension (RC). Some inferences require resolving references across sentences, and some rely on using prior knowledge to fill in the detail that is not explicitly written in the text. Diagnostic RC questions can help educators provide more effective and targeted reading instruction and interventions for school-age students. We introduce a taxonomy of inference types for RC and use it to analyze the distribution of items within a diagnostic RC item bank. Next, we present experiments using GPT-4o to generate bridging-inference RC items for given reading passages via few-shot prompting, comparing conditions with and without chain-of-thought prompts. Generated items were evaluated on three aspects: overall item quality, appropriate inference type, and LLM reasoning, achieving high inter-rater agreements above 0.90. Our results show that GPT-4o produced 93.8% good-quality questions suitable for operational use in grade 3-12 contexts; however, only 42.6% of the generated questions accurately matched the targeted inference type. We conclude that combining automatic item generation with human judgment offers a promising path toward scalable, high-quality diagnostic RC assessments."
      },
      {
        "id": "oai:arXiv.org:2506.08266v1",
        "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints",
        "link": "https://arxiv.org/abs/2506.08266",
        "author": "Yaswanth Chittepu, Blossom Metevier, Will Schwarzer, Austin Hoag, Scott Niekum, Philip S. Thomas",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08266v1 Announce Type: new \nAbstract: Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods."
      },
      {
        "id": "oai:arXiv.org:2506.08267v1",
        "title": "Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression",
        "link": "https://arxiv.org/abs/2506.08267",
        "author": "Mansooreh Montazerin, Majd Al Aawar, Antonio Ortega, Ajitesh Srivastava",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08267v1 Announce Type: new \nAbstract: Symbolic regression (SR) aims to discover closed-form mathematical expressions that accurately describe data, offering interpretability and analytical insight beyond standard black-box models. Existing SR methods often rely on population-based search or autoregressive modeling, which struggle with scalability and symbolic consistency. We introduce LIES (Logarithm, Identity, Exponential, Sine), a fixed neural network architecture with interpretable primitive activations that are optimized to model symbolic expressions. We develop a framework to extract compact formulae from LIES networks by training with an appropriate oversampling strategy and a tailored loss function to promote sparsity and to prevent gradient instability. After training, it applies additional pruning strategies to further simplify the learned expressions into compact formulae. Our experiments on SR benchmarks show that the LIES framework consistently produces sparse and accurate symbolic formulae outperforming all baselines. We also demonstrate the importance of each design component through ablation studies."
      },
      {
        "id": "oai:arXiv.org:2506.08270v1",
        "title": "SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space",
        "link": "https://arxiv.org/abs/2506.08270",
        "author": "Zitong Huang, Mansooreh Montazerin, Ajitesh Srivastava",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08270v1 Announce Type: new \nAbstract: Designing neural networks typically relies on manual trial and error or a neural architecture search (NAS) followed by weight training. The former is time-consuming and labor-intensive, while the latter often discretizes architecture search and weight optimization. In this paper, we propose a fundamentally different approach that simultaneously optimizes both the architecture and the weights of a neural network. Our framework first trains a universal multi-scale autoencoder that embeds both architectural and parametric information into a continuous latent space, where functionally similar neural networks are mapped closer together. Given a dataset, we then randomly initialize a point in the embedding space and update it via gradient descent to obtain the optimal neural network, jointly optimizing its structure and weights. The optimization process incorporates sparsity and compactness penalties to promote efficient models. Experiments on synthetic regression tasks demonstrate that our method effectively discovers sparse and compact neural networks with strong performance."
      },
      {
        "id": "oai:arXiv.org:2506.08272v1",
        "title": "Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids",
        "link": "https://arxiv.org/abs/2506.08272",
        "author": "Tarushri N. S.",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08272v1 Announce Type: new \nAbstract: Universal Differential Equations (UDEs), which blend neural networks with physical differential equations, have emerged as a powerful framework for scientific machine learning (SciML), enabling data-efficient, interpretable, and physically consistent modeling. In the context of smart grid systems, modeling node-wise battery dynamics remains a challenge due to the stochasticity of solar input and variability in household load profiles. Traditional approaches often struggle with generalization and fail to capture unmodeled residual dynamics. This work proposes a UDE-based approach to learn node-specific battery evolution by embedding a neural residual into a physically inspired battery ODE. Synthetic yet realistic solar generation and load demand data are used to simulate battery dynamics over time. The neural component learns to model unobserved or stochastic corrections arising from heterogeneity in node demand and environmental conditions. Comprehensive experiments reveal that the trained UDE aligns closely with ground truth battery trajectories, exhibits smooth convergence behavior, and maintains stability in long-term forecasts. These findings affirm the viability of UDE-based SciML approaches for battery modeling in decentralized energy networks and suggest broader implications for real-time control and optimization in renewable-integrated smart grids."
      },
      {
        "id": "oai:arXiv.org:2506.08274v1",
        "title": "The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks",
        "link": "https://arxiv.org/abs/2506.08274",
        "author": "Jo\\~ao Manoel Herrera Pinheiro, Suzana Vilas Boas de Oliveira, Thiago Henrique Segreto Silva, Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Leonardo Andr\\'e Ambrosio, Marcelo Becker",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08274v1 Announce Type: new \nAbstract: This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques."
      },
      {
        "id": "oai:arXiv.org:2506.08279v1",
        "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage",
        "link": "https://arxiv.org/abs/2506.08279",
        "author": "Aditi Sundararaman, Amogh Adishesha, Andrew Jaegle, Dan Bigioi, Hyoung-Kyu Song, Jon Kyl, Justin Mao, Kevin Lan, Mojtaba Komeili, ShahRukh Athar, Sheila Babayan, Stanislau Beliasau, William Buchwalter",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08279v1 Announce Type: new \nAbstract: From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links)."
      },
      {
        "id": "oai:arXiv.org:2506.08292v1",
        "title": "From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium",
        "link": "https://arxiv.org/abs/2506.08292",
        "author": "Xie Yi, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, Bo Han",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08292v1 Announce Type: new \nAbstract: Multi-agent frameworks can substantially boost the reasoning power of large language models (LLMs), but they typically incur heavy computational costs and lack convergence guarantees. To overcome these challenges, we recast multi-LLM coordination as an incomplete-information game and seek a Bayesian Nash equilibrium (BNE), in which each agent optimally responds to its probabilistic beliefs about the strategies of others. We introduce Efficient Coordination via Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that marries distributed reasoning with centralized final output. Under ECON, each LLM independently selects responses that maximize its expected reward, conditioned on its beliefs about co-agents, without requiring costly inter-agent exchanges. We mathematically prove that ECON attains a markedly tighter regret bound than non-equilibrium multi-agent schemes. Empirically, ECON outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks spanning complex reasoning and planning tasks. Further experiments demonstrate ECON's ability to flexibly incorporate additional models, confirming its scalability and paving the way toward larger, more powerful multi-LLM ensembles. The code is publicly available at: https://github.com/tmlr-group/ECON."
      },
      {
        "id": "oai:arXiv.org:2506.08295v1",
        "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?",
        "link": "https://arxiv.org/abs/2506.08295",
        "author": "Zhanke Zhou, Xiao Feng, Zhaocheng Zhu, Jiangchao Yao, Sanmi Koyejo, Bo Han",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08295v1 Announce Type: new \nAbstract: While existing benchmarks probe the reasoning abilities of large language models (LLMs) across diverse domains, they predominantly assess passive reasoning, providing models with all the information needed to reach a solution. By contrast, active reasoning-where an LLM must interact with external systems to acquire missing evidence or data-has received little systematic attention. To address this shortfall, we present AR-Bench, a novel benchmark designed explicitly to evaluate an LLM's active reasoning skills. AR-Bench comprises three task families-detective cases, situation puzzles, and guessing numbers-that together simulate real-world, agentic scenarios and measure performance across commonsense, logical, and symbolic reasoning challenges. Empirical evaluation on AR-Bench demonstrates that contemporary LLMs exhibit pronounced difficulties with active reasoning: they frequently fail to acquire or leverage the information needed to solve tasks. This gap highlights a stark divergence between their passive and active reasoning abilities. Moreover, ablation studies indicate that even advanced strategies, such as tree-based searching or post-training approaches, yield only modest gains and fall short of the levels required for real-world deployment. Collectively, these findings highlight the critical need to advance methodology for active reasoning, e.g., incorporating interactive learning, real-time feedback loops, and environment-aware objectives for training. The benchmark is publicly available at: https://github.com/tmlr-group/AR-Bench."
      },
      {
        "id": "oai:arXiv.org:2506.08297v1",
        "title": "SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging",
        "link": "https://arxiv.org/abs/2506.08297",
        "author": "Nhat Thanh Tran, Fanghui Xue, Shuai Zhang, Jiancheng Lyu, Yunling Zheng, Yingyong Qi, Jack Xin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08297v1 Announce Type: new \nAbstract: Attention is the critical component of a transformer. Yet the quadratic computational complexity of vanilla full attention in the input size and the inability of its linear attention variant to focus have been challenges for computer vision tasks. We provide a mathematical definition of generalized attention and formulate both vanilla softmax attention and linear attention within the general framework. We prove that generalized attention disperses, that is, as the number of keys tends to infinity, the query assigns equal weights to all keys. Motivated by the dispersion property and recent development of Mamba form of attention, we design Scalable and Efficient Mamba like Attention (SEMA) which utilizes token localization to avoid dispersion and maintain focusing, complemented by theoretically consistent arithmetic averaging to capture global aspect of attention. We support our approach on Imagenet-1k where classification results show that SEMA is a scalable and effective alternative beyond linear attention, outperforming recent vision Mamba models on increasingly larger scales of images at similar model parameter sizes."
      },
      {
        "id": "oai:arXiv.org:2506.08298v1",
        "title": "H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs",
        "link": "https://arxiv.org/abs/2506.08298",
        "author": "Trung-Kien Nguyen, Heng Ping, Shixuan Li, Peiyu Zhang, Nikos Kanakaris, Nicholas Kotov, Paul Bogdan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08298v1 Announce Type: new \nAbstract: The growing interests and applications of graph learning in diverse domains have propelled the development of a unified model generalizing well across different graphs and tasks, known as the Graph Foundation Model (GFM). Existing research has leveraged text-attributed graphs (TAGs) to tackle the heterogeneity in node features among graphs. However, they primarily focus on homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple types of nodes/edges reside, underexplored. To enhance the capabilities and applications of GFM, we introduce H$^2$GFM, a novel framework designed to generalize across both HoTAGs and HeTAGs. Our model projects diverse meta-relations among graphs under a unified textual space, and employs a context encoding to capture spatial and higher-order semantic relationships. To achieve robust node representations, we propose a novel context-adaptive graph transformer (CGT), effectively capturing information from both context neighbors and their relationships. Furthermore, we employ a mixture of CGT experts to capture the heterogeneity in structural patterns among graph types. Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as learning scenarios demonstrate the effectiveness of our model."
      },
      {
        "id": "oai:arXiv.org:2506.08299v1",
        "title": "OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal",
        "link": "https://arxiv.org/abs/2506.08299",
        "author": "Kangning Yang, Ling Ouyang, Huiming Sun, Jie Cai, Lan Fu, Jiaming Ding, Chiu Man Ho, Zibo Meng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08299v1 Announce Type: new \nAbstract: Reflection removal technology plays a crucial role in photography and computer vision applications. However, existing techniques are hindered by the lack of high-quality in-the-wild datasets. In this paper, we propose a novel paradigm for collecting reflection datasets from a fresh perspective. Our approach is convenient, cost-effective, and scalable, while ensuring that the collected data pairs are of high quality, perfectly aligned, and represent natural and diverse scenarios. Following this paradigm, we collect a Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which contains 1,000 high-quality transmission-reflection image pairs collected in the wild. Through the analysis of several reflection removal methods and benchmark evaluation experiments on our dataset, we demonstrate its effectiveness in improving robustness in challenging real-world environments. Our dataset is available at https://github.com/caijie0620/OpenRR-1k."
      },
      {
        "id": "oai:arXiv.org:2506.08300v1",
        "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability",
        "link": "https://arxiv.org/abs/2506.08300",
        "author": "Matteo Cargnelutti, Catherine Brobston, John Hess, Jack Cushman, Kristi Mukk, Aristana Scourtas, Kyle Courtney, Greg Leppert, Amanda Watson, Martha Whitehead, Jonathan Zittrain",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08300v1 Announce Type: new \nAbstract: Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use."
      },
      {
        "id": "oai:arXiv.org:2506.08309v1",
        "title": "Learnable Spatial-Temporal Positional Encoding for Link Prediction",
        "link": "https://arxiv.org/abs/2506.08309",
        "author": "Katherine Tieu, Dongqi Fu, Zihao Li, Ross Maciejewski, Jingrui He",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08309v1 Announce Type: new \nAbstract: Accurate predictions rely on the expressiveness power of graph deep learning frameworks like graph neural networks and graph transformers, where a positional encoding mechanism has become much more indispensable in recent state-of-the-art works to record the canonical position information. However, the current positional encoding is limited in three aspects: (1) most positional encoding methods use pre-defined, and fixed functions, which are inadequate to adapt to the complex attributed graphs; (2) a few pioneering works proposed the learnable positional encoding but are still limited to the structural information, not considering the real-world time-evolving topological and feature information; (3) most positional encoding methods are equipped with transformers' attention mechanism to fully leverage their capabilities, where the dense or relational attention is often unaffordable on large-scale structured data. Hence, we aim to develop Learnable Spatial-Temporal Positional Encoding in an effective and efficient manner and propose a simple temporal link prediction model named L-STEP. Briefly, for L-STEP, we (1) prove the proposed positional learning scheme can preserve the graph property from the spatial-temporal spectral viewpoint, (2) verify that MLPs can fully exploit the expressiveness and reach transformers' performance on that encoding, (3) change different initial positional encoding inputs to show robustness, (4) analyze the theoretical complexity and obtain less empirical running time than SOTA, and (5) demonstrate its temporal link prediction out-performance on 13 classic datasets and with 10 algorithms in both transductive and inductive settings using 3 different sampling strategies. Also, \\name\\ obtains the leading performance in the newest large-scale TGB benchmark. Our code is available at https://github.com/kthrn22/L-STEP."
      },
      {
        "id": "oai:arXiv.org:2506.08312v1",
        "title": "Private Evolution Converges",
        "link": "https://arxiv.org/abs/2506.08312",
        "author": "Tom\\'as Gonz\\'alez, Giulia Fanti, Aaditya Ramdas",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08312v1 Announce Type: new \nAbstract: Private Evolution (PE) is a promising training-free method for differentially private (DP) synthetic data generation. While it achieves strong performance in some domains (e.g., images and text), its behavior in others (e.g., tabular data) is less consistent. To date, the only theoretical analysis of the convergence of PE depends on unrealistic assumptions about both the algorithm's behavior and the structure of the sensitive dataset. In this work, we develop a new theoretical framework to explain PE's practical behavior and identify sufficient conditions for its convergence. For $d$-dimensional sensitive datasets with $n$ data points from a bounded domain, we prove that PE produces an $(\\epsilon, \\delta)$-DP synthetic dataset with expected 1-Wasserstein distance of order $\\tilde{O}(d(n\\epsilon)^{-1/d})$ from the original, establishing worst-case convergence of the algorithm as $n \\to \\infty$. Our analysis extends to general Banach spaces as well. We also connect PE to the Private Signed Measure Mechanism, a method for DP synthetic data generation that has thus far not seen much practical adoption. We demonstrate the practical relevance of our theoretical findings in simulations."
      },
      {
        "id": "oai:arXiv.org:2506.08316v1",
        "title": "Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion",
        "link": "https://arxiv.org/abs/2506.08316",
        "author": "Alan N. Amin, Nate Gruver, Andrew Gordon Wilson",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08316v1 Announce Type: new \nAbstract: Discrete diffusion models, like continuous diffusion models, generate high-quality samples by gradually undoing noise applied to datapoints with a Markov process. Gradual generation in theory comes with many conceptual benefits; for example, inductive biases can be incorporated into the noising Markov process, and access to improved sampling algorithms. In practice, however, the consistently best performing discrete diffusion model is, surprisingly, masking diffusion, which does not denoise gradually. Here we explain the superior performance of masking diffusion by noting that it makes use of a fundamental difference between continuous and discrete Markov processes: discrete Markov processes evolve by discontinuous jumps at a fixed rate and, unlike other discrete diffusion models, masking diffusion builds in the known distribution of jump times and only learns where to jump to. We show that we can similarly bake in the known distribution of jump times into any discrete diffusion model. The resulting models - schedule-conditioned discrete diffusion (SCUD) - generalize classical discrete diffusion and masking diffusion. By applying SCUD to models with noising processes that incorporate inductive biases on images, text, and protein data, we build models that outperform masking."
      },
      {
        "id": "oai:arXiv.org:2506.08324v1",
        "title": "Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating",
        "link": "https://arxiv.org/abs/2506.08324",
        "author": "Guandong Li, Mengxia Ye",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08324v1 Announce Type: new \nAbstract: Deep neural networks face several challenges in hyperspectral image classification, including high-dimensional data, sparse distribution of ground objects, and spectral redundancy, which often lead to classification overfitting and limited generalization capability. To more effectively extract and fuse spatial context with fine spectral information in hyperspectral image (HSI) classification, this paper proposes a novel network architecture called STNet. The core advantage of STNet stems from the dual innovative design of its Spatial-Spectral Transformer module: first, the fundamental explicit decoupling of spatial and spectral attention ensures targeted capture of key information in HSI; second, two functionally distinct gating mechanisms perform intelligent regulation at both the fusion level of attention flows (adaptive attention fusion gating) and the internal level of feature transformation (GFFN). This characteristic demonstrates superior feature extraction and fusion capabilities compared to traditional convolutional neural networks, while reducing overfitting risks in small-sample and high-noise scenarios. STNet enhances model representation capability without increasing network depth or width. The proposed method demonstrates superior performance on IN, UP, and KSC datasets, outperforming mainstream hyperspectral image classification approaches."
      },
      {
        "id": "oai:arXiv.org:2506.08326v1",
        "title": "Graph Prompting for Graph Learning Models: Recent Advances and Future Directions",
        "link": "https://arxiv.org/abs/2506.08326",
        "author": "Xingbo Fu, Zehong Wang, Zihan Chen, Jiazheng Li, Yaochen Zhu, Zhenyu Lei, Cong Shen, Yanfang Ye, Chuxu Zhang, Jundong Li",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08326v1 Announce Type: new \nAbstract: Graph learning models have demonstrated great prowess in learning expressive representations from large-scale graph data in a wide variety of real-world scenarios. As a prevalent strategy for training powerful graph learning models, the \"pre-training, adaptation\" scheme first pre-trains graph learning models on unlabeled graph data in a self-supervised manner and then adapts them to specific downstream tasks. During the adaptation phase, graph prompting emerges as a promising approach that learns trainable prompts while keeping the pre-trained graph learning models unchanged. In this paper, we present a systematic review of recent advancements in graph prompting. First, we introduce representative graph pre-training methods that serve as the foundation step of graph prompting. Next, we review mainstream techniques in graph prompting and elaborate on how they design learnable prompts for graph prompting. Furthermore, we summarize the real-world applications of graph prompting from different domains. Finally, we discuss several open challenges in existing studies with promising future directions in this field."
      },
      {
        "id": "oai:arXiv.org:2506.08327v1",
        "title": "Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera",
        "link": "https://arxiv.org/abs/2506.08327",
        "author": "Yuto Kase, Kai Ishibe, Ryoma Yasuda, Yudai Washida, Sakiko Hashimoto",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08327v1 Announce Type: new \nAbstract: In racket sports, such as tennis, locating the ball's position at impact is important in clarifying player and equipment characteristics, thereby aiding in personalized equipment design. High-speed cameras are used to measure the impact location; however, their excessive memory consumption limits prolonged scene capture, and manual digitization for position detection is time-consuming and prone to human error. These limitations make it difficult to effectively capture the entire playing scene, hindering the ability to analyze the player's performance. We propose a method for locating the tennis ball impact on the racket in real time using an event camera. Event cameras efficiently measure brightness changes (called `events') with microsecond accuracy under high-speed motion while using lower memory consumption. These cameras enable users to continuously monitor their performance over extended periods. Our method consists of three identification steps: time range of swing, timing at impact, and contours of ball and racket. Conventional computer vision techniques are utilized along with an original event-based processing to detect the timing at impact (PATS: the amount of polarity asymmetry in time symmetry). The results of the experiments were within the permissible range for measuring tennis players' performance. Moreover, the computation time was sufficiently short for real-time applications."
      },
      {
        "id": "oai:arXiv.org:2506.08337v1",
        "title": "A Simple Analysis of Discretization Error in Diffusion Models",
        "link": "https://arxiv.org/abs/2506.08337",
        "author": "Juhyeok Choi, Chenglin Fan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08337v1 Announce Type: new \nAbstract: Diffusion models, formulated as discretizations of stochastic differential equations (SDEs), achieve state-of-the-art generative performance. However, existing analyses of their discretization error often rely on complex probabilistic tools. In this work, we present a simplified theoretical framework for analyzing the Euler--Maruyama discretization of variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models (DDPMs), where $ T $ denotes the number of denoising steps in the diffusion process. Our approach leverages Gr\\\"onwall's inequality to derive a convergence rate of $ \\mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise in the discretization can be replaced by a discrete random variable (e.g., Rademacher or uniform noise) without sacrificing convergence guarantees-an insight with practical implications for efficient sampling. Experiments validate our theory, showing that (1) the error scales as predicted, (2) discrete noise achieves comparable sample quality to Gaussian noise, and (3) incorrect noise scaling degrades performance. By unifying simplified analysis and discrete noise substitution, our work bridges theoretical rigor with practical efficiency in diffusion-based generative modeling."
      },
      {
        "id": "oai:arXiv.org:2506.08340v1",
        "title": "Dynamical System Optimization",
        "link": "https://arxiv.org/abs/2506.08340",
        "author": "Emo Todorov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08340v1 Announce Type: new \nAbstract: We develop an optimization framework centered around a core idea: once a (parametric) policy is specified, control authority is transferred to the policy, resulting in an autonomous dynamical system. Thus we should be able to optimize policy parameters without further reference to controls or actions, and without directly using the machinery of approximate Dynamic Programming and Reinforcement Learning. Here we derive simpler algorithms at the autonomous system level, and show that they compute the same quantities as policy gradients and Hessians, natural gradients, proximal methods. Analogs to approximate policy iteration and off-policy learning are also available. Since policy parameters and other system parameters are treated uniformly, the same algorithms apply to behavioral cloning, mechanism design, system identification, learning of state estimators. Tuning of generative AI models is not only possible, but is conceptually closer to the present framework than to Reinforcement Learning."
      },
      {
        "id": "oai:arXiv.org:2506.08343v1",
        "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency",
        "link": "https://arxiv.org/abs/2506.08343",
        "author": "Chenlong Wang, Yuanning Feng, Dongping Chen, Zhaoyang Chu, Ranjay Krishna, Tianyi Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08343v1 Announce Type: new \nAbstract: Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning."
      },
      {
        "id": "oai:arXiv.org:2506.08347v1",
        "title": "Differentially Private Relational Learning with Entity-level Privacy Guarantees",
        "link": "https://arxiv.org/abs/2506.08347",
        "author": "Yinan Huang, Haoteng Ying, Eli Chien, Rongzhe Wei, Pan Li",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08347v1 Announce Type: new \nAbstract: Learning with relational and network-structured data is increasingly vital in sensitive domains where protecting the privacy of individual entities is paramount. Differential Privacy (DP) offers a principled approach for quantifying privacy risks, with DP-SGD emerging as a standard mechanism for private model training. However, directly applying DP-SGD to relational learning is challenging due to two key factors: (i) entities often participate in multiple relations, resulting in high and difficult-to-control sensitivity; and (ii) relational learning typically involves multi-stage, potentially coupled (interdependent) sampling procedures that make standard privacy amplification analyses inapplicable. This work presents a principled framework for relational learning with formal entity-level DP guarantees. We provide a rigorous sensitivity analysis and introduce an adaptive gradient clipping scheme that modulates clipping thresholds based on entity occurrence frequency. We also extend the privacy amplification results to a tractable subclass of coupled sampling, where the dependence arises only through sample sizes. These contributions lead to a tailored DP-SGD variant for relational data with provable privacy guarantees. Experiments on fine-tuning text encoders over text-attributed network-structured relational data demonstrate the strong utility-privacy trade-offs of our approach. Our code is available at https://github.com/Graph-COM/Node_DP."
      },
      {
        "id": "oai:arXiv.org:2506.08349v1",
        "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
        "link": "https://arxiv.org/abs/2506.08349",
        "author": "Yuxuan Zhou, Xien Liu, Chenwei Yan, Chen Ning, Xiao Zhang, Boxun Li, Xiangling Fu, Shijin Wang, Guoping Hu, Yu Wang, Ji Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08349v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications."
      },
      {
        "id": "oai:arXiv.org:2506.08351v1",
        "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models",
        "link": "https://arxiv.org/abs/2506.08351",
        "author": "Huixuan Zhang, Junzhe Zhang, Xiaojun Wan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08351v1 Announce Type: new \nAbstract: With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method."
      },
      {
        "id": "oai:arXiv.org:2506.08353v1",
        "title": "An Adaptive Method Stabilizing Activations for Enhanced Generalization",
        "link": "https://arxiv.org/abs/2506.08353",
        "author": "Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08353v1 Announce Type: new \nAbstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning rates according to activation variance. Our method enhances the stability of neuron outputs by incorporating neuron-wise adaptivity during the training process, which subsequently leads to better generalization -- a complementary approach to conventional activation regularization methods. Experimental results demonstrate AdaAct's competitive performance across standard image classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing it with other state-of-the-art methods. Importantly, AdaAct effectively bridges the gap between the convergence speed of Adam and the strong generalization capabilities of SGD, all while maintaining competitive execution times. Code is available at https://github.com/hseung88/adaact."
      },
      {
        "id": "oai:arXiv.org:2506.08354v1",
        "title": "Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning",
        "link": "https://arxiv.org/abs/2506.08354",
        "author": "Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08354v1 Announce Type: new \nAbstract: This position paper argues that the text embedding research community should move beyond surface meaning and embrace implicit semantics as a central modeling goal. Text embedding models have become foundational in modern NLP, powering a wide range of applications and drawing increasing research attention. Yet, much of this progress remains narrowly focused on surface-level semantics. In contrast, linguistic theory emphasizes that meaning is often implicit, shaped by pragmatics, speaker intent, and sociocultural context. Current embedding models are typically trained on data that lacks such depth and evaluated on benchmarks that reward the capture of surface meaning. As a result, they struggle with tasks requiring interpretive reasoning, speaker stance, or social meaning. Our pilot study highlights this gap, showing that even state-of-the-art models perform only marginally better than simplistic baselines on implicit semantics tasks. To address this, we call for a paradigm shift: embedding research should prioritize more diverse and linguistically grounded training data, design benchmarks that evaluate deeper semantic understanding, and explicitly frame implicit meaning as a core modeling objective, better aligning embeddings with real-world language complexity."
      },
      {
        "id": "oai:arXiv.org:2506.08356v1",
        "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding",
        "link": "https://arxiv.org/abs/2506.08356",
        "author": "Shivang Chopra, Lingchao Mao, Gabriela Sanchez-Rodriguez, Andrew J Feola, Jing Li, Zsolt Kira",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08356v1 Announce Type: new \nAbstract: Different medical imaging modalities capture diagnostic information at varying spatial resolutions, from coarse global patterns to fine-grained localized structures. However, most existing vision-language frameworks in the medical domain apply a uniform strategy for local feature extraction, overlooking the modality-specific demands. In this work, we present MedMoE, a modular and extensible vision-language processing framework that dynamically adapts visual representation based on the diagnostic context. MedMoE incorporates a Mixture-of-Experts (MoE) module conditioned on the report type, which routes multi-scale image features through specialized expert branches trained to capture modality-specific visual semantics. These experts operate over feature pyramids derived from a Swin Transformer backbone, enabling spatially adaptive attention to clinically relevant regions. This framework produces localized visual representations aligned with textual descriptions, without requiring modality-specific supervision at inference. Empirical results on diverse medical benchmarks demonstrate that MedMoE improves alignment and retrieval performance across imaging modalities, underscoring the value of modality-specialized visual representations in clinical vision-language systems."
      },
      {
        "id": "oai:arXiv.org:2506.08359v1",
        "title": "DEAL: Disentangling Transformer Head Activations for LLM Steering",
        "link": "https://arxiv.org/abs/2506.08359",
        "author": "Li-Ming Zhan, Bo Liu, Zexin Lu, Chengqiang Xie, Jiannong Cao, Xiao-Ming Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08359v1 Announce Type: new \nAbstract: Inference-time steering aims to alter the response characteristics of large language models (LLMs) without modifying their underlying parameters. A critical step in this process is the identification of internal modules within LLMs that are associated with the target behavior. However, current approaches to module selection often depend on superficial cues or ad-hoc heuristics, which can result in suboptimal or unintended outcomes. In this work, we propose a principled causal-attribution framework for identifying behavior-relevant attention heads in transformers. For each head, we train a vector-quantized autoencoder (VQ-AE) on its attention activations, partitioning the latent space into behavior-relevant and behavior-irrelevant subspaces, each quantized with a shared learnable codebook. We assess the behavioral relevance of each head by quantifying the separability of VQ-AE encodings for behavior-aligned versus behavior-violating responses using a binary classification metric. This yields a behavioral relevance score that reflects each head discriminative capacity with respect to the target behavior, guiding both selection and importance weighting. Experiments on seven LLMs from two model families and five behavioral steering datasets demonstrate that our method enables more accurate inference-time interventions, achieving superior performance on the truthfulness-steering task. Furthermore, the heads selected by our approach exhibit strong zero-shot generalization in cross-domain truthfulness-steering scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.08360v1",
        "title": "NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation",
        "link": "https://arxiv.org/abs/2506.08360",
        "author": "Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08360v1 Announce Type: new \nAbstract: Adaptive gradient methods are computationally efficient and converge quickly, but they often suffer from poor generalization. In contrast, second-order methods enhance convergence and generalization but typically incur high computational and memory costs. In this work, we introduce NysAct, a scalable first-order gradient preconditioning method that strikes a balance between state-of-the-art first-order and second-order optimization methods. NysAct leverages an eigenvalue-shifted Nystrom method to approximate the activation covariance matrix, which is used as a preconditioning matrix, significantly reducing time and memory complexities with minimal impact on test accuracy. Our experiments show that NysAct not only achieves improved test accuracy compared to both first-order and second-order methods but also demands considerably less computational resources than existing second-order methods. Code is available at https://github.com/hseung88/nysact."
      },
      {
        "id": "oai:arXiv.org:2506.08361v1",
        "title": "Image Demoir\\'eing Using Dual Camera Fusion on Mobile Phones",
        "link": "https://arxiv.org/abs/2506.08361",
        "author": "Yanting Mei, Zhilu Zhang, Xiaohe Wu, Wangmeng Zuo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08361v1 Announce Type: new \nAbstract: When shooting electronic screens, moir\\'e patterns usually appear in captured images, which seriously affects the image quality. Existing image demoir\\'eing methods face great challenges in removing large and heavy moir\\'e. To address the issue, we propose to utilize Dual Camera fusion for Image Demoir\\'eing (DCID), \\ie, using the ultra-wide-angle (UW) image to assist the moir\\'e removal of wide-angle (W) image. This is inspired by two motivations: (1) the two lenses are commonly equipped with modern smartphones, (2) the UW image generally can provide normal colors and textures when moir\\'e exists in the W image mainly due to their different focal lengths. In particular, we propose an efficient DCID method, where a lightweight UW image encoder is integrated into an existing demoir\\'eing network and a fast two-stage image alignment manner is present. Moreover, we construct a large-scale real-world dataset with diverse mobile phones and monitors, containing about 9,000 samples. Experiments on the dataset show our method performs better than state-of-the-art methods. Code and dataset are available at https://github.com/Mrduckk/DCID."
      },
      {
        "id": "oai:arXiv.org:2506.08364v1",
        "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs",
        "link": "https://arxiv.org/abs/2506.08364",
        "author": "Jash Rajesh Parekh, Pengcheng Jiang, Jiawei Han",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08364v1 Announce Type: new \nAbstract: Understanding cause and effect relationships remains a formidable challenge for Large Language Models (LLMs), particularly in specialized domains where reasoning requires more than surface-level correlations. Retrieval-Augmented Generation (RAG) improves factual accuracy, but standard RAG pipelines treat evidence as flat context, lacking the structure required to model true causal dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that integrates zero-shot triple extraction and theme-aware graph chaining into the RAG pipeline, enabling structured multi-hop inference. Given a domain specific corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of  triples and uses forward/backward chaining to guide structured answer generation. Experiments on two real-world domains: Bitcoin price fluctuations and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity. Both LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results demonstrate that explicitly modeling causal structure enables LLMs to generate more accurate and interpretable responses, especially in specialized domains where flat retrieval fails."
      },
      {
        "id": "oai:arXiv.org:2506.08365v1",
        "title": "AlphaFold Database Debiasing for Robust Inverse Folding",
        "link": "https://arxiv.org/abs/2506.08365",
        "author": "Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Siyuan Li, Yufei Huang, Stan Z. Li",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08365v1 Announce Type: new \nAbstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled structural coverage at near-experimental accuracy, positioning it as a valuable resource for data-driven protein design. However, its direct use in training deep models that are sensitive to fine-grained atomic geometry, such as inverse folding, exposes a critical limitation. Comparative analysis of structural feature distributions reveals that AFDB structures exhibit distinct statistical regularities, reflecting a systematic geometric bias that deviates from the conformational diversity found in experimentally determined structures from the Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized, PDB structures capture the intrinsic variability and physical realism essential for generalization in downstream tasks. To address this discrepancy, we introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct native-like conformations from intentionally corrupted backbone geometries. By training the model to recover plausible structural states, DeSAE implicitly captures a more robust and natural structural manifold. At inference, applying DeSAE to AFDB structures produces debiased structures that significantly improve inverse folding performance across multiple benchmarks. This work highlights the critical impact of subtle systematic biases in predicted structures and presents a principled framework for debiasing, significantly boosting the performance of structure-based learning tasks like inverse folding."
      },
      {
        "id": "oai:arXiv.org:2506.08371v1",
        "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding",
        "link": "https://arxiv.org/abs/2506.08371",
        "author": "Zikai Xiao, Ziyang Wang, Wen Ma, Yan Zhang, Wei Shen, Yan Wang, Luqi Gong, Zuozhu Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08371v1 Announce Type: new \nAbstract: While Large Language Models (LLMs) support long contexts, they struggle with performance degradation within the context window. Current solutions incur prohibitive training costs, leaving statistical behaviors and cost-effective approaches underexplored. From the decoding perspective, we identify the Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio correlates with long-text performance degradation. Notably, despite the attenuation, gold tokens still occupy high-ranking positions in the decoding space. Motivated by it, we propose the training-free Positional Contrastive Decoding (PCD) that contrasts the logits derived from long-aware attention with those from designed local-aware attention, enabling the model to focus on the gains introduced by large-scale short-to-long training. Through the analysis of long-term decay simulation, we demonstrate that PCD effectively alleviates attention score degradation. Experimental results show that PCD achieves state-of-the-art performance on long-context benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08373v1",
        "title": "Draft-based Approximate Inference for LLMs",
        "link": "https://arxiv.org/abs/2506.08373",
        "author": "Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08373v1 Announce Type: new \nAbstract: Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm."
      },
      {
        "id": "oai:arXiv.org:2506.08375v1",
        "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models",
        "link": "https://arxiv.org/abs/2506.08375",
        "author": "Tao Zou, Xinghua Zhang, Haiyang Yu, Minzheng Wang, Fei Huang, Yongbin Li",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08375v1 Announce Type: new \nAbstract: With the development and widespread application of large language models (LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands higher capabilities to address complex user needs, often requiring precise workflow execution which involves the accurate understanding of multiple tasks. However, existing benchmarks focusing on single-task environments with limited constraints lack the complexity required to fully reflect real-world scenarios. To bridge this gap, we present the Extremely Complex Instruction Following Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that enable comprehensive assessment across diverse task types concurrently, but also integrates a variety of constraints, replicating complex operational environments. Furthermore, we propose the Segment Policy Optimization (SegPO) algorithm to enhance the LLM's ability to accurately fulfill multi-task workflow. Evaluations on EIFBENCH have unveiled considerable performance discrepancies in existing LLMs when challenged with these extremely complex instructions. This finding underscores the necessity for ongoing optimization to navigate the intricate challenges posed by LLM applications."
      },
      {
        "id": "oai:arXiv.org:2506.08379v1",
        "title": "Reinforce LLM Reasoning through Multi-Agent Reflection",
        "link": "https://arxiv.org/abs/2506.08379",
        "author": "Yurun Yuan, Tengyang Xie",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08379v1 Announce Type: new \nAbstract: Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization."
      },
      {
        "id": "oai:arXiv.org:2506.08383v1",
        "title": "Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest",
        "link": "https://arxiv.org/abs/2506.08383",
        "author": "Jiaqi Chen, Rongbin Ye",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08383v1 Announce Type: new \nAbstract: With the rapid expansion of Internet of Things (IoT) networks, detecting malicious traffic in real-time has become a critical cybersecurity challenge. This research addresses the detection challenges by presenting a comprehensive empirical analysis of machine learning techniques for malware detection using the IoT-23 dataset provided by the Stratosphere Laboratory. We address the significant class imbalance within the dataset through three resampling strategies. We implement and compare a few machine learning techniques. Our findings demonstrate that the combination of appropriate imbalance treatment techniques with ensemble methods, particularly gcForest, achieves better detection performance compared to traditional approaches. This work contributes significantly to the development of more intelligent and efficient automated threat detection systems for IoT environments, helping to secure critical infrastructure against sophisticated cyber attacks while optimizing computational resource usage."
      },
      {
        "id": "oai:arXiv.org:2506.08388v1",
        "title": "Reinforcement Learning Teachers of Test Time Scaling",
        "link": "https://arxiv.org/abs/2506.08388",
        "author": "Edoardo Cetin, Tianyu Zhao, Yujin Tang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08388v1 Announce Type: new \nAbstract: Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply \"connect-the-dots\" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework."
      },
      {
        "id": "oai:arXiv.org:2506.08391v1",
        "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding",
        "link": "https://arxiv.org/abs/2506.08391",
        "author": "Woohyeon Park, Woojin Kim, Jaeik Kim, Jaeyoung Do",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08391v1 Announce Type: new \nAbstract: Despite significant advancements in Vision-Language Models (VLMs), the performance of existing VLMs remains hindered by object hallucination, a critical challenge to achieving accurate visual understanding. To address this issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach that enables VLMs to effectively leverage multi-scale visual information with an object-centric manner, closely aligning with human visual perception. SECOND progressively selects and integrates multi-scale visual information, facilitating a more precise interpretation of images. By contrasting these visual information iteratively, SECOND significantly reduces perceptual hallucinations and outperforms a wide range of benchmarks. Our theoretical analysis and experiments highlight the largely unexplored potential of multi-scale application in VLMs, showing that prioritizing and contrasting across scales outperforms existing methods."
      },
      {
        "id": "oai:arXiv.org:2506.08397v1",
        "title": "Spatiotemporal deep learning models for detection of rapid intensification in cyclones",
        "link": "https://arxiv.org/abs/2506.08397",
        "author": "Vamshika Sutar, Amandeep Singh, Rohitash Chandra",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08397v1 Announce Type: new \nAbstract: Cyclone rapid intensification is the rapid increase in cyclone wind intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid intensification is considered an extreme event during a cyclone, and its occurrence is relatively rare, contributing to a class imbalance in the dataset. A diverse array of factors influences the likelihood of a cyclone undergoing rapid intensification, further complicating the task for conventional machine learning models. In this paper, we evaluate deep learning, ensemble learning and data augmentation frameworks to detect cyclone rapid intensification based on wind intensity and spatial coordinates. We note that conventional data augmentation methods cannot be utilised for generating spatiotemporal patterns replicating cyclones that undergo rapid intensification. Therefore, our framework employs deep learning models to generate spatial coordinates and wind intensity that replicate cyclones to address the class imbalance problem of rapid intensification. We also use a deep learning model for the classification module within the data augmentation framework to differentiate between rapid and non-rapid intensification events during a cyclone. Our results show that data augmentation improves the results for rapid intensification detection in cyclones, and spatial coordinates play a critical role as input features to the given models. This paves the way for research in synthetic data generation for spatiotemporal data with extreme events."
      },
      {
        "id": "oai:arXiv.org:2506.08400v1",
        "title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks",
        "link": "https://arxiv.org/abs/2506.08400",
        "author": "Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08400v1 Announce Type: new \nAbstract: Large Language models (LLMs) have demonstrated impressive performance on a wide range of tasks, including in multimodal settings such as speech. However, their evaluation is often limited to English and a few high-resource languages. For low-resource languages, there is no standardized evaluation benchmark. In this paper, we address this gap by introducing mSTEB, a new benchmark to evaluate the performance of LLMs on a wide range of tasks covering language identification, text classification, question answering, and translation tasks on both speech and text modalities. We evaluated the performance of leading LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in performance between high-resource and low-resource languages, especially for languages spoken in Africa and Americas/Oceania. Our findings show that more investment is needed to address their under-representation in LLMs coverage."
      },
      {
        "id": "oai:arXiv.org:2506.08403v1",
        "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration",
        "link": "https://arxiv.org/abs/2506.08403",
        "author": "Weiya Li, Junjie Chen, Bei Li, Boyang Liu, Zichen Wen, Nuanqiao Shan, Xiaoqian Liu, Anping Liu, Huajie Liu, Youyan Wang, Wujiuge Yin, Hu Song, Bing Huang, Zhiyuan Xia, Jialiang Chen, Linfeng Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08403v1 Announce Type: new \nAbstract: Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC."
      },
      {
        "id": "oai:arXiv.org:2506.08409v1",
        "title": "FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion",
        "link": "https://arxiv.org/abs/2506.08409",
        "author": "Fred Xu, Song Jiang, Zijie Huang, Xiao Luo, Shichang Zhang, Adrian Chen, Yizhou Sun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08409v1 Announce Type: new \nAbstract: Taxonomy Expansion, which models complex concepts and their relations, can be formulated as a set representation learning task. The generalization of set, fuzzy set, incorporates uncertainty and measures the information within a semantic concept, making it suitable for concept modeling. Existing works usually model sets as vectors or geometric objects such as boxes, which are not closed under set operations. In this work, we propose a sound and efficient formulation of set representation learning based on its volume approximation as a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE), satisfies all set operations and compactly approximates the underlying fuzzy set, hence preserving information while being efficient to learn, relying on minimum neural architecture. We empirically demonstrate the power of FUSE on the task of taxonomy expansion, where FUSE achieves remarkable improvements up to 23% compared with existing baselines. Our work marks the first attempt to understand and efficiently compute the embeddings of fuzzy sets."
      },
      {
        "id": "oai:arXiv.org:2506.08410v1",
        "title": "Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens",
        "link": "https://arxiv.org/abs/2506.08410",
        "author": "Ziyang Ma, Qingyue Yuan, Zhenglin Wang, Deyu Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08410v1 Announce Type: new \nAbstract: Previous research has primarily focused on the cognitive error detection capabilities of Large Language Models (LLMs), often prompting them to analyze mistakes in reasoning chains. However, few studies have examined the meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors), which are crucial for their reliability. While studies on LLM self-evaluation present some measures, such as perplexity, which can reflect the answer correctness and be viewed as the lens of meta-cognition, they lack step-level analysis and adaptation. This paper studies the evaluation of LLM meta-cognition using the current lenses and how to improve these lenses. Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking the existing lenses. Furthermore, a training-free Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost current meta-cognition lenses. Experimental results on three mathematical reasoning datasets and three LLMs show the reasonableness of AutoMeco by comparing it with Best-of-N verification. Moreover, the meta-cognition ability of LLMs can be better evaluated using MIRA."
      },
      {
        "id": "oai:arXiv.org:2506.08412v1",
        "title": "Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics",
        "link": "https://arxiv.org/abs/2506.08412",
        "author": "Saraa Ali, Aleksandr Khizhik, Stepan Svirin, Artem Ryzhikov, Denis Derkach",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08412v1 Announce Type: new \nAbstract: The application of machine learning (ML) algorithms in the intelligent diagnosis of three-phase engines has the potential to significantly enhance diagnostic performance and accuracy. Traditional methods largely rely on signature analysis, which, despite being a standard practice, can benefit from the integration of advanced ML techniques. In our study, we innovate by combining ML algorithms with a novel unsupervised anomaly generation methodology that takes into account the engine physics model. We propose Signature-Guided Data Augmentation (SGDA), an unsupervised framework that synthesizes physically plausible faults directly in the frequency domain of healthy current signals. Guided by Motor Current Signature Analysis, SGDA creates diverse and realistic anomalies without resorting to computationally intensive simulations. This hybrid approach leverages the strengths of both supervised ML and unsupervised signature analysis, achieving superior diagnostic accuracy and reliability along with wide industrial application. The findings highlight the potential of our approach to contribute significantly to the field of engine diagnostics, offering a robust and efficient solution for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2506.08415v1",
        "title": "Improved Scaling Laws in Linear Regression via Data Reuse",
        "link": "https://arxiv.org/abs/2506.08415",
        "author": "Licong Lin, Jingfeng Wu, Peter L. Bartlett",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08415v1 Announce Type: new \nAbstract: Neural scaling laws suggest that the test error of large language models trained online decreases polynomially as the model size and data size increase. However, such scaling can be unsustainable when running out of new data. In this work, we show that data reuse can improve existing scaling laws in linear regression. Specifically, we derive sharp test error bounds on $M$-dimensional linear models trained by multi-pass stochastic gradient descent (multi-pass SGD) on $N$ data with sketched features. Assuming that the data covariance has a power-law spectrum of degree $a$, and that the true parameter follows a prior with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show that multi-pass SGD achieves a test error of $\\Theta(M^{1-b} + L^{(1-b)/a})$, where $L \\lesssim N^{a/b}$ is the number of iterations. In the same setting, one-pass SGD only attains a test error of $\\Theta(M^{1-b} + N^{(1-b)/a})$ (see e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse (i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are also provided to verify our theoretical findings."
      },
      {
        "id": "oai:arXiv.org:2506.08417v1",
        "title": "Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood",
        "link": "https://arxiv.org/abs/2506.08417",
        "author": "Qingmao Yao, Zhichao Lei, Tianyuan Chen, Ziyue Yuan, Xuefan Chen, Jianxiang Liu, Faguo Wu, Xiao Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08417v1 Announce Type: new \nAbstract: Offline Reinforcement Learning (RL) struggles with distributional shifts, leading to the $Q$-value overestimation for out-of-distribution (OOD) actions. Existing methods address this issue by imposing constraints; however, they often become overly conservative when evaluating OOD regions, which constrains the $Q$-function generalization. This over-constraint issue results in poor $Q$-value estimation and hinders policy improvement. In this paper, we introduce a novel approach to achieve better $Q$-value estimation by enhancing $Q$-function generalization in OOD regions within Convex Hull and its Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by smoothing them with neighboring in-sample $Q$-values. We theoretically show that SBO approximates true $Q$-values for both in-sample and OOD actions within the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG), empirically alleviates the over-constraint issue, achieving near-accurate $Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing state-of-the-art methods in both performance and computational efficiency."
      },
      {
        "id": "oai:arXiv.org:2506.08418v1",
        "title": "RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation",
        "link": "https://arxiv.org/abs/2506.08418",
        "author": "Taiqin Chen, Zikun Zhou, Zheng Fang, Wenzhen Zou, Kanjun Liu, Ke Chen, Yongbing Zhang, Yaowei Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08418v1 Announce Type: new \nAbstract: The radio map represents the spatial distribution of spectrum resources within a region, supporting efficient resource allocation and interference mitigation. However, it is difficult to construct a dense radio map as a limited number of samples can be measured in practical scenarios. While existing works have used deep learning to estimate dense radio maps from sparse samples, they are hard to integrate with the physical characteristics of the radio map. To address this challenge, we cast radio map estimation as the sparse signal recovery problem. A physical propagation model is further incorporated to decompose the problem into multiple factor optimization sub-problems, thereby reducing recovery complexity. Inspired by the existing compressive sensing methods, we propose the Radio Deep Unfolding Network (RadioDUN) to unfold the optimization process, achieving adaptive parameter adjusting and prior fitting in a learnable manner. To account for the radio propagation characteristics, we develop a dynamic reweighting module (DRM) to adaptively model the importance of each factor for the radio map. Inspired by the shadowing factor in the physical propagation model, we integrate obstacle-related factors to express the obstacle-induced signal stochastic decay. The shadowing loss is further designed to constrain the factor prediction and act as a supplementary supervised objective, which enhances the performance of RadioDUN. Extensive experiments have been conducted to demonstrate that the proposed method outperforms the state-of-the-art methods. Our code will be made publicly available upon publication."
      },
      {
        "id": "oai:arXiv.org:2506.08419v1",
        "title": "Online Learning-guided Learning Rate Adaptation via Gradient Alignment",
        "link": "https://arxiv.org/abs/2506.08419",
        "author": "Ruichen Jiang, Ali Kavis, Aryan Mokhtari",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08419v1 Announce Type: new \nAbstract: The performance of an optimizer on large-scale deep learning models depends critically on fine-tuning the learning rate, often requiring an extensive grid search over base learning rates, schedules, and other hyperparameters. In this paper, we propose a principled framework called GALA (Gradient Alignment-based Learning rate Adaptation), which dynamically adjusts the learning rate by tracking the alignment between consecutive gradients and using a local curvature estimate. Guided by the convergence analysis, we formulate the problem of selecting the learning rate as a one-dimensional online learning problem. When paired with an online learning algorithm such as Follow-the-Regularized-Leader, our method produces a flexible, adaptive learning rate schedule that tends to increase when consecutive gradients are aligned and decrease otherwise. We establish a data-adaptive convergence rate for normalized SGD equipped with GALA in the smooth, nonconvex setting. Empirically, common optimizers such as SGD and Adam, when augmented with GALA, demonstrate robust performance across a wide range of initial learning rates and perform competitively without the need for tuning."
      },
      {
        "id": "oai:arXiv.org:2506.08426v1",
        "title": "HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems",
        "link": "https://arxiv.org/abs/2506.08426",
        "author": "Zheng Lin, Zhe Chen, Xianhao Chen, Wei Ni, Yue Gao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08426v1 Announce Type: new \nAbstract: Split federated learning (SFL) has emerged as a promising paradigm to democratize machine learning (ML) on edge devices by enabling layer-wise model partitioning. However, existing SFL approaches suffer significantly from the straggler effect due to the heterogeneous capabilities of edge devices. To address the fundamental challenge, we propose adaptively controlling batch sizes (BSs) and model splitting (MS) for edge devices to overcome resource heterogeneity. We first derive a tight convergence bound of SFL that quantifies the impact of varied BSs and MS on learning performance. Based on the convergence bound, we propose HASFL, a heterogeneity-aware SFL framework capable of adaptively controlling BS and MS to balance communication-computing latency and training convergence in heterogeneous edge networks. Extensive experiments with various datasets validate the effectiveness of HASFL and demonstrate its superiority over state-of-the-art benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08427v1",
        "title": "Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models",
        "link": "https://arxiv.org/abs/2506.08427",
        "author": "Jiaxiang Liu, Boxuan Xing, Chenhao Yuan, Chenxiang Zhang, Di Wu, Xiusheng Huang, Haida Yu, Chuhan Lang, Pengfei Cao, Jun Zhao, Kang Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08427v1 Announce Type: new \nAbstract: As large language models (LLMs) continue to advance, there is a growing urgency to enhance the interpretability of their internal knowledge mechanisms. Consequently, many interpretation methods have emerged, aiming to unravel the knowledge mechanisms of LLMs from various perspectives. However, current interpretation methods differ in input data formats and interpreting outputs. The tools integrating these methods are only capable of supporting tasks with specific inputs, significantly constraining their practical applications. To address these challenges, we present an open-source Knowledge Mechanisms Revealer&amp;Interpreter (Know-MRI) designed to analyze the knowledge mechanisms within LLMs systematically. Specifically, we have developed an extensible core module that can automatically match different input data with interpretation methods and consolidate the interpreting outputs. It enables users to freely choose appropriate interpretation methods based on the inputs, making it easier to comprehensively diagnose the model's internal knowledge mechanisms from multiple perspectives. Our code is available at https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on https://youtu.be/NVWZABJ43Bs."
      },
      {
        "id": "oai:arXiv.org:2506.08429v1",
        "title": "Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring",
        "link": "https://arxiv.org/abs/2506.08429",
        "author": "Mingjie Xu, Andrew Estornell, Hongzheng Yang, Yuzhi Zhao, Zhaowei Zhu, Qi Xuan, Jiaheng Wei",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08429v1 Announce Type: new \nAbstract: The application of visual instruction tuning and other post-training techniques has significantly enhanced the capabilities of Large Language Models (LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with more comprehensive visual language datasets. However, the effectiveness of VLMs is highly dependent on large-scale, high-quality datasets that ensure precise recognition and accurate reasoning. Two key challenges hinder progress: (1) noisy alignments between images and the corresponding text, which leads to misinterpretation, and (2) ambiguous or misleading text, which obscures visual content. To address these challenges, we propose SCALE (Single modality data quality and Cross modality Alignment Evaluation), a novel quality-driven data selection pipeline for VLM instruction tuning datasets. Specifically, SCALE integrates a cross-modality assessment framework that first assigns each data entry to its appropriate vision-language task, generates general and task-specific captions (covering scenes, objects, style, etc.), and evaluates the alignment, clarity, task rarity, text coherence, and image clarity of each entry based on the generated captions. We reveal that: (1) current unimodal quality assessment methods evaluate one modality while overlooking the rest, which can underestimate samples essential for specific tasks and discard the lower-quality instances that help build model robustness; and (2) appropriately generated image captions provide an efficient way to transfer the image-text multimodal task into a unified text modality."
      },
      {
        "id": "oai:arXiv.org:2506.08430v1",
        "title": "CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models",
        "link": "https://arxiv.org/abs/2506.08430",
        "author": "Ziqi. Liu, Ziyang. Zhou, Mingxuan. Hu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08430v1 Announce Type: new \nAbstract: Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability."
      },
      {
        "id": "oai:arXiv.org:2506.08433v1",
        "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption",
        "link": "https://arxiv.org/abs/2506.08433",
        "author": "Hern\\'an Maina, Nicol\\'as Wolovick, Luciana Benotti",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08433v1 Announce Type: new \nAbstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware, and annotated data, often resulting in a positionality rooted in predominant cultures and values (Santy et al., 2023). Domain adaptation has emerged as a promising strategy to better align models with diverse cultural and value contexts (Hershcovich et al., 2022), but its computational cost remains a significant barrier, particularly for research groups lacking access to large-scale infrastructure. In this paper, we evaluate how the use of different numerical precisions and data parallelization strategies impacts both training speed (as a proxy to energy and hardware consumption) and model accuracy, with the goal of facilitating domain adaptation in low-resource environments. Our findings are relevant to any setting where energy efficiency, accessibility, or limited hardware availability are key concerns."
      },
      {
        "id": "oai:arXiv.org:2506.08435v1",
        "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
        "link": "https://arxiv.org/abs/2506.08435",
        "author": "Mingyuan Fan, Fuyi Wang, Cen Chen, Jianying Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08435v1 Announce Type: new \nAbstract: Federated learning (FL) enables collaborative model training among multiple clients without the need to expose raw data. Its ability to safeguard privacy, at the heart of FL, has recently been a hot-button debate topic. To elaborate, several studies have introduced a type of attacks known as gradient leakage attacks (GLAs), which exploit the gradients shared during training to reconstruct clients' raw data. On the flip side, some literature, however, contends no substantial privacy risk in practical FL environments due to the effectiveness of such GLAs being limited to overly relaxed conditions, such as small batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that clients' data can still be effectively reconstructed, even within realistic FL environments. Upon revisiting GLAs, we recognize that their performance failures stem from their inability to handle the gradient matching problem. To alleviate the performance bottlenecks identified above, we develop FedLeak, which introduces two novel techniques, partial gradient matching and gradient regularization. Moreover, to evaluate the performance of FedLeak in real-world FL environments, we formulate a practical evaluation protocol grounded in a thorough review of extensive FL literature and industry practices. Under this protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby underscoring the significant vulnerability in FL systems and the urgent need for more effective defense methods."
      },
      {
        "id": "oai:arXiv.org:2506.08436v1",
        "title": "Olica: Efficient Structured Pruning of Large Language Models without Retraining",
        "link": "https://arxiv.org/abs/2506.08436",
        "author": "Jiujun He, Huazhen Lin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08436v1 Announce Type: new \nAbstract: Most existing structured pruning methods for Large Language Models (LLMs) require substantial computational and data resources for retraining to reestablish the corrupted correlations, making them prohibitively expensive. To address this, we propose a pruning framework for LLMs called Orthogonal decomposition and Linear Calibration (Olica), which eliminates the need for retraining. A key observation is that the multi-head attention (MHA) layer depends on two types of matrix products. By treating these matrix products as unified entities and applying principal component analysis (PCA), we extract the most important information to compress LLMs without sacrificing accuracy or disrupting their original structure. Consequently, retraining becomes unnecessary. A fast decomposition method is devised, reducing the complexity of PCA by a factor of the square of the number of attention heads. Additionally, to mitigate error accumulation problem caused by pruning the feed-forward network (FFN) layer, we introduce a linear calibration method to reconstruct the residual errors of pruned layers using low-rank matrices. By leveraging singular value decomposition (SVD) on the solution of the least-squares problem, these matrices are obtained without requiring retraining. Extensive experiments show that the proposed Olica is efficient in terms of data usage, GPU memory, and running time, while delivering superior performance across multiple benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08438v1",
        "title": "Learning to Lead: Incentivizing Strategic Agents in the Dark",
        "link": "https://arxiv.org/abs/2506.08438",
        "author": "Yuchen Wu, Xinyi Zhong, Zhuoran Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08438v1 Announce Type: new \nAbstract: We study an online learning version of the generalized principal-agent model, where a principal interacts repeatedly with a strategic agent possessing private types, private rewards, and taking unobservable actions. The agent is non-myopic, optimizing a discounted sum of future rewards and may strategically misreport types to manipulate the principal's learning. The principal, observing only her own realized rewards and the agent's reported types, aims to learn an optimal coordination mechanism that minimizes strategic regret. We develop the first provably sample-efficient algorithm for this challenging setting. Our approach features a novel pipeline that combines (i) a delaying mechanism to incentivize approximately myopic agent behavior, (ii) an innovative reward angle estimation framework that uses sector tests and a matching procedure to recover type-dependent reward functions, and (iii) a pessimistic-optimistic LinUCB algorithm that enables the principal to explore efficiently while respecting the agent's incentive constraints. We establish a near optimal $\\tilde{O}(\\sqrt{T}) $ regret bound for learning the principal's optimal policy, where $\\tilde{O}(\\cdot) $ omits logarithmic factors. Our results open up new avenues for designing robust online learning algorithms for a wide range of game-theoretic settings involving private types and strategic agents."
      },
      {
        "id": "oai:arXiv.org:2506.08441v1",
        "title": "Time-Aware World Model for Adaptive Prediction and Control",
        "link": "https://arxiv.org/abs/2506.08441",
        "author": "Anh N. Nhu, Sanghyun Son, Ming Lin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08441v1 Announce Type: new \nAbstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based approach that explicitly incorporates temporal dynamics. By conditioning on the time-step size, {\\Delta}t, and training over a diverse range of {\\Delta}t values -- rather than sampling at a fixed time-step -- TAWM learns both high- and low-frequency task dynamics across diverse control problems. Grounded in the information-theoretic insight that the optimal sampling rate depends on a system's underlying dynamics, this time-aware formulation improves both performance and data efficiency. Empirical evaluations show that TAWM consistently outperforms conventional models across varying observation rates in a variety of control tasks, using the same number of training samples and iterations. Our code can be found online at: github.com/anh-nn01/Time-Aware-World-Model."
      },
      {
        "id": "oai:arXiv.org:2506.08456v1",
        "title": "Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance",
        "link": "https://arxiv.org/abs/2506.08456",
        "author": "June Suk Choi, Kyungmin Lee, Sihyun Yu, Yisol Choi, Jinwoo Shin, Kimin Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08456v1 Announce Type: new \nAbstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in producing high-quality, dynamic videos. To improve the visual controllability, recent works have considered fine-tuning pre-trained T2V models to support image-to-video (I2V) generation. However, such adaptation frequently suppresses motion dynamics of generated outputs, resulting in more static videos compared to their T2V counterparts. In this work, we analyze this phenomenon and identify that it stems from the premature exposure to high-frequency details in the input image, which biases the sampling process toward a shortcut trajectory that overfits to the static appearance of the reference image. To address this, we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model sampling procedure to generate more dynamic videos without compromising per-frame image quality. Specifically, ALG adaptively modulates the frequency content of the conditioning image by applying low-pass filtering at the early stage of denoising. Extensive experiments demonstrate that ALG significantly improves the temporal dynamics of generated videos, while preserving image fidelity and text alignment. Especially, under VBench-I2V test suite, ALG achieves an average improvement of 36% in dynamic degree without a significant drop in video quality or image fidelity."
      },
      {
        "id": "oai:arXiv.org:2506.08460v1",
        "title": "MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08460",
        "author": "Yihong Guo, Yu Yang, Pan Xu, Anqi Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08460v1 Announce Type: new \nAbstract: We study the off-dynamics offline reinforcement learning problem, where the goal is to learn a policy from offline datasets collected from source and target domains with mismatched transition. Existing off-dynamics offline RL methods typically either filter source transitions that resemble those of the target domain or apply reward augmentation to source data, both constrained by the limited transitions available from the target domain. As a result, the learned policy is unable to explore target domain beyond the offline datasets. We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that addresses this limitation by enabling exploration of the target domain via learned dynamics. MOBODY generates new synthetic transitions in the target domain through model rollouts, which are used as data augmentation during offline policy learning. Unlike existing model-based methods that learn dynamics from a single domain, MOBODY tackles the challenge of mismatched dynamics by leveraging both source and target datasets. Directly merging these datasets can bias the learned model toward source dynamics. Instead, MOBODY learns target dynamics by discovering a shared latent representation of states and transitions across domains through representation learning. To stabilize training, MOBODY incorporates a behavior cloning loss that regularizes the policy. Specifically, we introduce a Q-weighted behavior cloning loss that regularizes the policy toward actions with high target-domain Q-values, rather than uniformly imitating all actions in the dataset. These Q-values are learned from an enhanced target dataset composed of offline target data, augmented source data, and rollout data from the learned target dynamics. We evaluate MOBODY on MuJoCo benchmarks and show that it significantly outperforms state-of-the-art baselines, with especially pronounced improvements in challenging scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.08463v1",
        "title": "How to Provably Improve Return Conditioned Supervised Learning?",
        "link": "https://arxiv.org/abs/2506.08463",
        "author": "Zhishuai Liu, Yu Yang, Ruhan Wang, Pan Xu, Dongruo Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08463v1 Announce Type: new \nAbstract: In sequential decision-making problems, Return-Conditioned Supervised Learning (RCSL) has gained increasing recognition for its simplicity and stability in modern decision-making tasks. Unlike traditional offline reinforcement learning (RL) algorithms, RCSL frames policy learning as a supervised learning problem by taking both the state and return as input. This approach eliminates the instability often associated with temporal difference (TD) learning in offline RL. However, RCSL has been criticized for lacking the stitching property, meaning its performance is inherently limited by the quality of the policy used to generate the offline dataset. To address this limitation, we propose a principled and simple framework called Reinforced RCSL. The key innovation of our framework is the introduction of a concept we call the in-distribution optimal return-to-go. This mechanism leverages our policy to identify the best achievable in-dataset future return based on the current state, avoiding the need for complex return augmentation techniques. Our theoretical analysis demonstrates that Reinforced RCSL can consistently outperform the standard RCSL approach. Empirical results further validate our claims, showing significant performance improvements across a range of benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08464v1",
        "title": "MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature",
        "link": "https://arxiv.org/abs/2506.08464",
        "author": "Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08464v1 Announce Type: new \nAbstract: Second-order optimization methods for training neural networks, such as KFAC, exhibit superior convergence by utilizing curvature information of loss landscape. However, it comes at the expense of high computational burden. In this work, we analyze the two components that constitute the layer-wise Fisher information matrix (FIM) used in KFAC: the Kronecker factors related to activations and pre-activation gradients. Based on empirical observations on their eigenspectra, we propose efficient approximations for them, resulting in a computationally efficient optimization method called MAC. To the best of our knowledge, MAC is the first algorithm to apply the Kronecker factorization to the FIM of attention layers used in transformers and explicitly integrate attention scores into the preconditioning. We also study the convergence property of MAC on nonlinear neural networks and provide two conditions under which it converges to global minima. Our extensive evaluations on various network architectures and datasets show that the proposed method outperforms KFAC and other state-of-the-art methods in terms of accuracy, end-to-end training time, and memory usage. Code is available at https://github.com/hseung88/mac."
      },
      {
        "id": "oai:arXiv.org:2506.08470v1",
        "title": "MARMOT: Masked Autoencoder for Modeling Transient Imaging",
        "link": "https://arxiv.org/abs/2506.08470",
        "author": "Siyuan Shen, Ziheng Wang, Xingyue Peng, Suan Xia, Ruiqian Li, Shiying Li, Jingyi Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08470v1 Announce Type: new \nAbstract: Pretrained models have demonstrated impressive success in many modalities such as language and vision. Recent works facilitate the pretraining paradigm in imaging research. Transients are a novel modality, which are captured for an object as photon counts versus arrival times using a precisely time-resolved sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of hidden objects are measured beyond the sensor's direct line of sight. Using NLOS transients, the majority of previous works optimize volume density or surfaces to reconstruct the hidden objects and do not transfer priors learned from datasets. In this work, we present a masked autoencoder for modeling transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a self-supervised model pretrianed on massive and diverse NLOS transient datasets. Using a Transformer-based encoder-decoder, MARMOT learns features from partially masked transients via a scanning pattern mask (SPM), where the unmasked subset is functionally equivalent to arbitrary sampling, and predicts full measurements. Pretrained on TransVerse-a synthesized transient dataset of 500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature transfer or decoder finetuning. Comprehensive experiments are carried out in comparisons with state-of-the-art methods. Quantitative and qualitative results demonstrate the efficiency of our MARMOT."
      },
      {
        "id": "oai:arXiv.org:2506.08473v1",
        "title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin",
        "link": "https://arxiv.org/abs/2506.08473",
        "author": "Shuo Yang, Qihui Zhang, Yuyang Liu, Yue Huang, Xiaojun Jia, Kunpeng Ning, Jiayu Yao, Jigang Wang, Hailiang Dai, Yibing Song, Li Yuan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08473v1 Announce Type: new \nAbstract: Large language models (LLMs) are vulnerable to safety risks during fine-tuning, where small amounts of malicious or harmless data can compromise safeguards. In this paper, building on the concept of alignment direction -- defined by the weight difference between aligned and unaligned models -- we observe that perturbations along this direction preserve model safety. In contrast, perturbations along directions orthogonal to this alignment are strongly linked to harmful direction perturbations, rapidly degrading safety and framing the parameter space as a narrow safety basin. Based on this insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring Safety in Fine-Tuning), which integrates a regularization term into the training objective. This term uses the alignment direction as an anchor to suppress updates in harmful directions, ensuring that fine-tuning is constrained within the narrow safety basin. Extensive experiments on multiple datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by 7.60 percent, improving model performance by 3.44 percent, and maintaining robust performance across various experimental settings. Code is available at https://github.com/PKU-YuanGroup/AsFT"
      },
      {
        "id": "oai:arXiv.org:2506.08475v1",
        "title": "Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems",
        "link": "https://arxiv.org/abs/2506.08475",
        "author": "Xiaolong He, Yeonjong Shin, Anthony Gruber, Sohyeon Jung, Kookjin Lee, Youngsoo Choi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08475v1 Announce Type: new \nAbstract: We propose an efficient thermodynamics-informed latent space dynamics identification (tLaSDI) framework for the reduced-order modeling of parametric nonlinear dynamical systems. This framework integrates autoencoders for dimensionality reduction with newly developed parametric GENERIC formalism-informed neural networks (pGFINNs), which enable efficient learning of parametric latent dynamics while preserving key thermodynamic principles such as free energy conservation and entropy generation across the parameter space. To further enhance model performance, a physics-informed active learning strategy is incorporated, leveraging a greedy, residual-based error indicator to adaptively sample informative training data, outperforming uniform sampling at equivalent computational cost. Numerical experiments on the Burgers' equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed method achieves up to 3,528x speed-up with 1-3% relative errors, and significant reduction in training (50-90%) and inference (57-61%) cost. Moreover, the learned latent space dynamics reveal the underlying thermodynamic behavior of the system, offering valuable insights into the physical-space dynamics."
      },
      {
        "id": "oai:arXiv.org:2506.08477v1",
        "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning",
        "link": "https://arxiv.org/abs/2506.08477",
        "author": "Fengjun Pan, Anh Tuan Luu, Xiaobao Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08477v1 Announce Type: new \nAbstract: Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md."
      },
      {
        "id": "oai:arXiv.org:2506.08479v1",
        "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$",
        "link": "https://arxiv.org/abs/2506.08479",
        "author": "Chihiro Taguchi, Seiji Maekawa, Nikita Bhutani",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08479v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs) both address context limitations of LLMs in open-domain question answering (QA). However, optimal external context to retrieve remains an open problem: fixing the retrieval size risks either wasting tokens or omitting key evidence. Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM prompting and perform well on factoid QA, but struggle with aggregation QA, where the optimal context size is both unknown and variable. We present Adaptive-$k$ retrieval, a simple and effective single-pass method that adaptively selects the number of passages based on the distribution of the similarity scores between the query and the candidate passages. It does not require model fine-tuning, extra LLM inferences or changes to existing retriever-reader pipelines. On both factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x fewer tokens than full-context input, yet still retrieves 70% of relevant passages. It improves accuracy across five LCLMs and two embedding models, highlighting that dynamically adjusting context size leads to more efficient and accurate QA."
      },
      {
        "id": "oai:arXiv.org:2506.08480v1",
        "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models",
        "link": "https://arxiv.org/abs/2506.08480",
        "author": "Huixuan Zhang, Xiaojun Wan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08480v1 Announce Type: new \nAbstract: Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation."
      },
      {
        "id": "oai:arXiv.org:2506.08487v1",
        "title": "Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models",
        "link": "https://arxiv.org/abs/2506.08487",
        "author": "Sumanth Manduru, Carlotta Domeniconi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08487v1 Announce Type: new \nAbstract: The rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an overlooked \"middle tier\" between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2506.08488v1",
        "title": "EtiCor++: Towards Understanding Etiquettical Bias in LLMs",
        "link": "https://arxiv.org/abs/2506.08488",
        "author": "Ashutosh Dwivedi, Siddhant Shivdutt Singh, Ashutosh Modi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08488v1 Announce Type: new \nAbstract: In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions."
      },
      {
        "id": "oai:arXiv.org:2506.08490v1",
        "title": "Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework",
        "link": "https://arxiv.org/abs/2506.08490",
        "author": "Xiao Wei, Xiaobao Wang, Ning Zhuang, Chenyang Wang, Longbiao Wang, Jianwu dang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08490v1 Announce Type: new \nAbstract: Intent detection aims to identify user intents from natural language inputs, where supervised methods rely heavily on labeled in-domain (IND) data and struggle with out-of-domain (OOD) intents, limiting their practical applicability. Generalized Intent Discovery (GID) addresses this by leveraging unlabeled OOD data to discover new intents without additional annotation. However, existing methods focus solely on clustering unsupervised data while neglecting domain adaptation. Therefore, we propose a consistency-driven prototype-prompting framework for GID from the perspective of integrating old and new knowledge, which includes a prototype-prompting framework for transferring old knowledge from external sources, and a hierarchical consistency constraint for learning new knowledge from target domains. We conducted extensive experiments and the results show that our method significantly outperforms all baseline methods, achieving state-of-the-art results, which strongly demonstrates the effectiveness and generalization of our methods. Our source code is publicly available at https://github.com/smileix/cpp."
      },
      {
        "id": "oai:arXiv.org:2506.08493v1",
        "title": "Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization",
        "link": "https://arxiv.org/abs/2506.08493",
        "author": "Qilin Yin, Wei Lu, Xiangyang Luo, Xiaochun Cao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08493v1 Announce Type: new \nAbstract: Most research efforts in the multimedia forensics domain have focused on detecting forgery audio-visual content and reached sound achievements. However, these works only consider deepfake detection as a classification task and ignore the case where partial segments of the video are tampered with. Temporal forgery localization (TFL) of small fake audio-visual clips embedded in real videos is still challenging and more in line with realistic application scenarios. To resolve this issue, we propose a universal context-aware contrastive learning framework (UniCaCLF) for TFL. Our approach leverages supervised contrastive learning to discover and identify forged instants by means of anomaly detection, allowing for the precise localization of temporal forged segments. To this end, we propose a novel context-aware perception layer that utilizes a heterogeneous activation operation and an adaptive context updater to construct a context-aware contrastive objective, which enhances the discriminability of forged instant features by contrasting them with genuine instant features in terms of their distances to the global context. An efficient context-aware contrastive coding is introduced to further push the limit of instant feature distinguishability between genuine and forged instants in a supervised sample-by-sample manner, suppressing the cross-sample influence to improve temporal forgery localization performance. Extensive experimental results over five public datasets demonstrate that our proposed UniCaCLF significantly outperforms the state-of-the-art competing algorithms."
      },
      {
        "id": "oai:arXiv.org:2506.08500v1",
        "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs",
        "link": "https://arxiv.org/abs/2506.08500",
        "author": "Arie Cattan, Alon Jacovi, Ori Ram, Jonathan Herzig, Roee Aharoni, Sasha Goldshtein, Eran Ofek, Idan Szpektor, Avi Caciularu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08500v1 Announce Type: new \nAbstract: Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains."
      },
      {
        "id": "oai:arXiv.org:2506.08504v1",
        "title": "CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations",
        "link": "https://arxiv.org/abs/2506.08504",
        "author": "Divyaksh Shukla, Ritesh Baviskar, Dwijesh Gohil, Aniket Tiwari, Atul Shree, Ashutosh Modi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08504v1 Announce Type: new \nAbstract: Discourse parsing is an important task useful for NLU applications such as summarization, machine comprehension, and emotion recognition. The current discourse parsing datasets based on conversations consists of written English dialogues restricted to a single domain. In this resource paper, we introduce CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations. The corpus (code-mixed in Hindi and English) has both audio and transcribed text and is annotated with nine discourse relations. We experiment with various SoTA baseline models; the poor performance of SoTA models highlights the challenges of multi-domain code-mixed corpus, pointing towards the need for developing better models for such realistic settings."
      },
      {
        "id": "oai:arXiv.org:2506.08505v1",
        "title": "Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations",
        "link": "https://arxiv.org/abs/2506.08505",
        "author": "Shahaf Bassan, Yizhak Yisrael Elboher, Tobias Ladner, Matthias Althoff, Guy Katz",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08505v1 Announce Type: new \nAbstract: Despite significant advancements in post-hoc explainability techniques for neural networks, many current methods rely on heuristics and do not provide formally provable guarantees over the explanations provided. Recent work has shown that it is possible to obtain explanations with formal guarantees by identifying subsets of input features that are sufficient to determine that predictions remain unchanged using neural network verification techniques. Despite the appeal of these explanations, their computation faces significant scalability challenges. In this work, we address this gap by proposing a novel abstraction-refinement technique for efficiently computing provably sufficient explanations of neural network predictions. Our method abstracts the original large neural network by constructing a substantially reduced network, where a sufficient explanation of the reduced network is also provably sufficient for the original network, hence significantly speeding up the verification process. If the explanation is in sufficient on the reduced network, we iteratively refine the network size by gradually increasing it until convergence. Our experiments demonstrate that our approach enhances the efficiency of obtaining provably sufficient explanations for neural network predictions while additionally providing a fine-grained interpretation of the network's predictions across different abstraction levels."
      },
      {
        "id": "oai:arXiv.org:2506.08512v1",
        "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding",
        "link": "https://arxiv.org/abs/2506.08512",
        "author": "Zhiyi Zhu, Xiaoyu Wu, Zihao Liu, Linlin Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08512v1 Announce Type: new \nAbstract: Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding. Existing Transformer-based methods often suffer from redundant attention and suboptimal multi-modal alignment. To address these limitations, we propose MLVTG, a novel framework that integrates two key modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba blocks as a backbone instead of Transformers to model temporal dependencies and extract robust video representations for multi-modal alignment. LLMRefiner leverages the specific frozen layer of a pre-trained Large Language Model (LLM) to implicitly transfer semantic priors, enhancing multi-modal alignment without fine-tuning. This dual alignment strategy, temporal modeling via structured state-space dynamics and semantic purification via textual priors, enables more precise localization. Extensive experiments on QVHighlights, Charades-STA, and TVSum demonstrate that MLVTG achieves state-of-the-art performance and significantly outperforms existing baselines."
      },
      {
        "id": "oai:arXiv.org:2506.08514v1",
        "title": "DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training",
        "link": "https://arxiv.org/abs/2506.08514",
        "author": "Jacob Piland, Chris Sweet, Adam Czakja",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08514v1 Announce Type: new \nAbstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g., GradCAM) have become standard tools for explaining Convolutional Neural Network (CNN) predictions. However, these approaches typically focus on individual logits, while for neural networks using softmax, the class membership probability estimates depend \\textit{only} on the \\textit{differences} between logits, not on their absolute values. This disconnect leaves standard CAMs vulnerable to adversarial manipulation, such as passive fooling, where a model is trained to produce misleading CAMs without affecting decision performance. We introduce \\textbf{Salience-Hoax Activation Maps (SHAMs)}, an \\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM robustness under adversarial conditions. To address the passive fooling vulnerability, we then propose \\textbf{DiffGradCAM}, a novel, lightweight, and contrastive approach to class activation mapping that is both non-suceptible to passive fooling, but also matches the output of standard CAM methods such as GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a new framework for probing and improving the robustness of saliency-based explanations. We validate both contributions across multi-class tasks with few and many classes."
      },
      {
        "id": "oai:arXiv.org:2506.08516v1",
        "title": "NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis",
        "link": "https://arxiv.org/abs/2506.08516",
        "author": "Mouadh Yagoubi, David Danan, Milad Leyli-Abadi, Ahmed Mazari, Jean-Patrick Brunet, Abbas Kabalan, Fabien Casenave, Yuxin Ma, Giovanni Catalani, Jean Fesquet, Jacob Helwig, Xuan Zhang, Haiyang Yu, Xavier Bertrand, Frederic Tost, Michael Baurheim, Joseph Morlier, Shuiwang Ji",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08516v1 Announce Type: new \nAbstract: The integration of machine learning (ML) into the physical sciences is reshaping computational paradigms, offering the potential to accelerate demanding simulations such as computational fluid dynamics (CFD). Yet, persistent challenges in accuracy, generalization, and physical consistency hinder the practical deployment of ML models in scientific domains. To address these limitations and systematically benchmark progress, we organized the ML4CFD competition, centered on surrogate modeling for aerodynamic simulations over two-dimensional airfoils. The competition attracted over 240 teams, who were provided with a curated dataset generated via OpenFOAM and evaluated through a multi-criteria framework encompassing predictive accuracy, physical fidelity, computational efficiency, and out-of-distribution generalization. This retrospective analysis reviews the competition outcomes, highlighting several approaches that outperformed baselines under our global evaluation score. Notably, the top entry exceeded the performance of the original OpenFOAM solver on aggregate metrics, illustrating the promise of ML-based surrogates to outperform traditional solvers under tailored criteria. Drawing from these results, we analyze the key design principles of top submissions, assess the robustness of our evaluation framework, and offer guidance for future scientific ML challenges."
      },
      {
        "id": "oai:arXiv.org:2506.08523v1",
        "title": "Leveraging chaos in the training of artificial neural networks",
        "link": "https://arxiv.org/abs/2506.08523",
        "author": "Pedro Jim\\'enez-Gonz\\'alez, Miguel C. Soriano, Lucas Lacasa",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08523v1 Announce Type: new \nAbstract: Traditional algorithms to optimize artificial neural networks when confronted with a supervised learning task are usually exploitation-type relaxational dynamics such as gradient descent (GD). Here, we explore the dynamics of the neural network trajectory along training for unconventionally large learning rates. We show that for a region of values of the learning rate, the GD optimization shifts away from purely exploitation-like algorithm into a regime of exploration-exploitation balance, as the neural network is still capable of learning but the trajectory shows sensitive dependence on initial conditions -- as characterized by positive network maximum Lyapunov exponent --. Interestingly, the characteristic training time required to reach an acceptable accuracy in the test set reaches a minimum precisely in such learning rate region, further suggesting that one can accelerate the training of artificial neural networks by locating at the onset of chaos. Our results -- initially illustrated for the MNIST classification task -- qualitatively hold for a range of supervised learning tasks, learning architectures and other hyperparameters, and showcase the emergent, constructive role of transient chaotic dynamics in the training of artificial neural networks."
      },
      {
        "id": "oai:arXiv.org:2506.08526v1",
        "title": "Robust Visual Localization via Semantic-Guided Multi-Scale Transformer",
        "link": "https://arxiv.org/abs/2506.08526",
        "author": "Zhongtao Tian, Wenhao Huang, Zhidong Chen, Xiao Wei Sun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08526v1 Announce Type: new \nAbstract: Visual localization remains challenging in dynamic environments where fluctuating lighting, adverse weather, and moving objects disrupt appearance cues. Despite advances in feature representation, current absolute pose regression methods struggle to maintain consistency under varying conditions. To address this challenge, we propose a framework that synergistically combines multi-scale feature learning with semantic scene understanding. Our approach employs a hierarchical Transformer with cross-scale attention to fuse geometric details and contextual cues, preserving spatial precision while adapting to environmental changes. We improve the performance of this architecture with semantic supervision via neural scene representation during training, guiding the network to learn view-invariant features that encode persistent structural information while suppressing complex environmental interference. Experiments on TartanAir demonstrate that our approach outperforms existing pose regression methods in challenging scenarios with dynamic objects, illumination changes, and occlusions. Our findings show that integrating multi-scale processing with semantic guidance offers a promising strategy for robust visual localization in real-world dynamic environments."
      },
      {
        "id": "oai:arXiv.org:2506.08529v1",
        "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\\times$RTX 4090s",
        "link": "https://arxiv.org/abs/2506.08529",
        "author": "Xijun Wang, Xin Li, Bingchen Li, Zhibo Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08529v1 Announce Type: new \nAbstract: Diffusion models have significantly advanced video super-resolution (VSR) by enhancing perceptual quality, largely through elaborately designed temporal modeling to ensure inter-frame consistency. However, existing methods usually suffer from limited temporal coherence and prohibitively high computational costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for long videos. In this work, we propose LiftVSR, an efficient VSR framework that leverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$, achieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To balance long-term consistency and efficiency, we introduce a hybrid temporal modeling mechanism that decomposes temporal learning into two complementary components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal modeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii) Attention Memory Cache (AMC) for long-term temporal modeling across segments ($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token flows across frames within multi-head query and key tokens to warp inter-frame contexts in the value tokens. AMC adaptively aggregates historical segment information via a cache unit, ensuring long-term coherence with minimal overhead. To further stabilize the cache interaction during inference, we introduce an asymmetric sampling strategy that mitigates feature mismatches arising from different diffusion sampling steps. Extensive experiments on several typical VSR benchmarks have demonstrated that LiftVSR achieves impressive performance with significantly lower computational costs."
      },
      {
        "id": "oai:arXiv.org:2506.08533v1",
        "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)",
        "link": "https://arxiv.org/abs/2506.08533",
        "author": "Nihal Acharya Adde, Alexandra Gianzina, Hanno Gottschalk, Andreas Ebert",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08533v1 Announce Type: new \nAbstract: This paper introduces Evolutionary Multi-Objective Network Architecture Search (EMNAS) for the first time to optimize neural network architectures in large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses genetic algorithms to automate network design, tailored to enhance rewards and reduce model size without compromising performance. Additionally, parallelization techniques are employed to accelerate the search, and teacher-student methodologies are implemented to ensure scalable optimization. This research underscores the potential of transfer learning as a robust framework for optimizing performance across iterative learning processes by effectively leveraging knowledge from earlier generations to enhance learning efficiency and stability in subsequent generations. Experimental results demonstrate that tailored EMNAS outperforms manually designed models, achieving higher rewards with fewer parameters. The findings of these strategies contribute positively to EMNAS for RL in autonomous driving, advancing the field toward better-performing networks suitable for real-world scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.08541v1",
        "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching",
        "link": "https://arxiv.org/abs/2506.08541",
        "author": "Qi Yan, Brian Zhang, Yutong Zhang, Daniel Yang, Joshua White, Di Chen, Jiachao Liu, Langechuan Liu, Binnan Zhuang, Shaoshuai Shi, Renjie Liao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08541v1 Announce Type: new \nAbstract: Efficient and accurate motion prediction is crucial for ensuring safety and informed decision-making in autonomous driving, particularly under dynamic real-world conditions that necessitate multi-modal forecasts. We introduce TrajFlow, a novel flow matching-based motion prediction framework that addresses the scalability and efficiency challenges of existing generative trajectory prediction methods. Unlike conventional generative approaches that employ i.i.d. sampling and require multiple inference passes to capture diverse outcomes, TrajFlow predicts multiple plausible future trajectories in a single pass, significantly reducing computational overhead while maintaining coherence across predictions. Moreover, we propose a ranking loss based on the Plackett-Luce distribution to improve uncertainty estimation of predicted trajectories. Additionally, we design a self-conditioning training technique that reuses the model's own predictions to construct noisy inputs during a second forward pass, thereby improving generalization and accelerating inference. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across various key metrics, underscoring its effectiveness for safety-critical autonomous driving applications. The code and other details are available on the project website https://traj-flow.github.io/."
      },
      {
        "id": "oai:arXiv.org:2506.08543v1",
        "title": "Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs",
        "link": "https://arxiv.org/abs/2506.08543",
        "author": "Bowei Tian, Xuntao Lyu, Meng Liu, Hongyi Wang, Ang Li",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08543v1 Announce Type: new \nAbstract: High-level representations have become a central focus in enhancing AI transparency and control, shifting attention from individual neurons or circuits to structured semantic directions that align with human-interpretable concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned directions originate in the input space and are selectively amplified with increasing depth. We then introduce the Spectral Principal Path (SPP) framework, which formalizes how deep networks progressively distill linear representations along a small set of dominant spectral directions. Building on this framework, we further demonstrate the multimodal robustness of these representations in Vision-Language Models (VLMs). By bridging theoretical insights with empirical validation, this work advances a structured theory of representation formation in deep networks, paving the way for improving AI robustness, fairness, and transparency."
      },
      {
        "id": "oai:arXiv.org:2506.08551v1",
        "title": "DeepForm: Reasoning Large Language Model for Communication System Formulation",
        "link": "https://arxiv.org/abs/2506.08551",
        "author": "Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, Fangxin Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08551v1 Announce Type: new \nAbstract: Communication system formulation is critical for advancing 6G and future wireless technologies, yet it remains a complex, expertise-intensive task. While Large Language Models (LLMs) offer potential, existing general-purpose models often lack the specialized domain knowledge, nuanced reasoning capabilities, and access to high-quality, domain-specific training data required for adapting a general LLM into an LLM specially for communication system formulation. To bridge this gap, we introduce DeepForm, the first reasoning LLM specially for automated communication system formulation. We propose the world-first large-scale, open-source dataset meticulously curated for this domain called Communication System Formulation Reasoning Corpus (CSFRC). Our framework employs a two-stage training strategy: first, Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge; second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated reasoning patterns like self-correction and verification. Extensive experiments demonstrate that our model achieves state-of-the-art performance, significantly outperforming larger proprietary LLMs on diverse senerios. We will release related resources to foster further research in this area after the paper is accepted."
      },
      {
        "id": "oai:arXiv.org:2506.08552v1",
        "title": "Efficient Post-Training Refinement of Latent Reasoning in Large Language Models",
        "link": "https://arxiv.org/abs/2506.08552",
        "author": "Xinyuan Wang, Dongjie Wang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Sixun Dong, Kunpeng Liu, Yanjie Fu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08552v1 Announce Type: new \nAbstract: Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\\% accuracy gain on MathQA without additional training."
      },
      {
        "id": "oai:arXiv.org:2506.08553v1",
        "title": "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge",
        "link": "https://arxiv.org/abs/2506.08553",
        "author": "Agnese Taluzzi, Davide Gesualdi, Riccardo Santambrogio, Chiara Plizzari, Francesca Palermo, Simone Mentasti, Matteo Matteucci",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08553v1 Announce Type: new \nAbstract: This report presents SceneNet and KnowledgeNet, our approaches developed for the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with a multi-modal large language model (MLLM) to capture fine-grained object interactions, spatial relationships, and temporally grounded events. In parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge to introduce high-level semantic connections between entities, enabling reasoning beyond directly observable visual evidence. Each method demonstrates distinct strengths across the seven categories of the HD-EPIC benchmark, and their combination within our framework results in an overall accuracy of 44.21% on the challenge, highlighting its effectiveness for complex egocentric VQA tasks."
      },
      {
        "id": "oai:arXiv.org:2506.08555v1",
        "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement",
        "link": "https://arxiv.org/abs/2506.08555",
        "author": "Xinyue Niu, Akira Furui",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08555v1 Announce Type: new \nAbstract: Cross-subject electromyography (EMG) pattern recognition faces significant challenges due to inter-subject variability in muscle anatomy, electrode placement, and signal characteristics. Traditional methods rely on subject-specific calibration data to adapt models to new users, an approach that is both time-consuming and impractical for large-scale, real-world deployment. This paper presents an approach to eliminate calibration requirements through feature disentanglement, enabling effective cross-subject generalization. We propose an end-to-end dual-branch adversarial neural network that simultaneously performs pattern recognition and individual identification by disentangling EMG features into pattern-specific and subject-specific components. The pattern-specific components facilitate robust pattern recognition for new users without model calibration, while the subject-specific components enable downstream applications such as task-invariant biometric identification. Experimental results demonstrate that the proposed model achieves robust performance on data from unseen users, outperforming various baseline methods in cross-subject scenarios. Overall, this study offers a new perspective for cross-subject EMG pattern recognition without model calibration and highlights the proposed model's potential for broader applications, such as task-independent biometric systems."
      },
      {
        "id": "oai:arXiv.org:2506.08562v1",
        "title": "Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection",
        "link": "https://arxiv.org/abs/2506.08562",
        "author": "Duc Thanh Pham, Hong Dang Nguyen, Nhat Minh Nguyen Quoc, Linh Ngo Van, Sang Dinh Viet, Duc Anh Nguyen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08562v1 Announce Type: new \nAbstract: Recently, object detection models have witnessed notable performance improvements, particularly with transformer-based models. However, new objects frequently appear in the real world, requiring detection models to continually learn without suffering from catastrophic forgetting. Although Incremental Object Detection (IOD) has emerged to address this challenge, these existing models are still not practical due to their limited performance and prolonged inference time. In this paper, we introduce a novel framework for IOD, called Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both efficiency and competitive performance by leveraging Neural Collapse for imbalance dataset and Hierarchical relation of classes' labels."
      },
      {
        "id": "oai:arXiv.org:2506.08564v1",
        "title": "Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?",
        "link": "https://arxiv.org/abs/2506.08564",
        "author": "Tuukka T\\\"or\\\"o, Antti Suni, Juraj \\v{S}imko",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08564v1 Announce Type: new \nAbstract: Investigating linguistic relationships on a global scale requires analyzing diverse features such as syntax, phonology and prosody, which evolve at varying rates influenced by internal diversification, language contact, and sociolinguistic factors. Recent advances in machine learning (ML) offer complementary alternatives to traditional historical and typological approaches. Instead of relying on expert labor in analyzing specific linguistic features, these new methods enable the exploration of linguistic variation through embeddings derived directly from speech, opening new avenues for large-scale, data-driven analyses.\n  This study employs embeddings from the fine-tuned XLS-R self-supervised language identification model voxlingua107-xls-r-300m-wav2vec, to analyze relationships between 106 world languages based on speech recordings. Using linear discriminant analysis (LDA), language embeddings are clustered and compared with genealogical, lexical, and geographical distances. The results demonstrate that embedding-based distances align closely with traditional measures, effectively capturing both global and local typological patterns. Challenges in visualizing relationships, particularly with hierarchical clustering and network-based methods, highlight the dynamic nature of language change.\n  The findings show potential for scalable analyses of language variation based on speech embeddings, providing new perspectives on relationships among languages. By addressing methodological considerations such as corpus size and latent space dimensionality, this approach opens avenues for studying low-resource languages and bridging macro- and micro-level linguistic variation. Future work aims to extend these methods to underrepresented languages and integrate sociolinguistic variation for a more comprehensive understanding of linguistic diversity."
      },
      {
        "id": "oai:arXiv.org:2506.08566v1",
        "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations",
        "link": "https://arxiv.org/abs/2506.08566",
        "author": "Yibo Cui, Liang Xie, Yu Zhao, Jiawei Sun, Erwei Yin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08566v1 Announce Type: new \nAbstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances agents' state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks."
      },
      {
        "id": "oai:arXiv.org:2506.08572v1",
        "title": "The Geometries of Truth Are Orthogonal Across Tasks",
        "link": "https://arxiv.org/abs/2506.08572",
        "author": "Waiss Azizian, Michael Kirchhof, Eugene Ndiaye, Louis Bethune, Michal Klein, Pierre Ablin, Marco Cuturi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08572v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated impressive generalization capabilities across various tasks, but their claim to practical relevance is still mired by concerns on their reliability. Recent works have proposed examining the activations produced by an LLM at inference time to assess whether its answer to a question is correct. Some works claim that a \"geometry of truth\" can be learned from examples, in the sense that the activations that generate correct answers can be distinguished from those leading to mistakes with a linear classifier. In this work, we underline a limitation of these approaches: we observe that these \"geometries of truth\" are intrinsically task-dependent and fail to transfer across tasks. More precisely, we show that linear classifiers trained across distinct tasks share little similarity and, when trained with sparsity-enforcing regularizers, have almost disjoint supports. We show that more sophisticated approaches (e.g., using mixtures of probes and tasks) fail to overcome this limitation, likely because activation vectors commonly used to classify answers form clearly separated clusters when examined across tasks."
      },
      {
        "id": "oai:arXiv.org:2506.08574v1",
        "title": "SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models",
        "link": "https://arxiv.org/abs/2506.08574",
        "author": "Alvise Dei Rossi, Matteo Metaldi, Michal Bechny, Irina Filchenko, Julia van der Meer, Markus H. Schmidt, Claudio L. A. Bassetti, Athina Tzovara, Francesca D. Faraci, Luigi Fiorillo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08574v1 Announce Type: new \nAbstract: Despite advances in deep learning for automatic sleep staging, clinical adoption remains limited due to challenges in fair model evaluation, generalization across diverse datasets, model bias, and variability in human annotations. We present SLEEPYLAND, an open-source sleep staging evaluation framework designed to address these barriers. It includes more than 22'0000 hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain (OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders, and hardware setups. We release pre-trained models based on high-performing SoA architectures and evaluate them under standardized conditions across single- and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble combining models across architectures and channel setups via soft voting. SOMNUS achieves robust performance across twenty-four different datasets, with macro-F1 scores between 68.7% and 87.2%, outperforming individual models in 94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including cases where compared models were trained ID while SOMNUS treated the same data as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked to age, gender, AHI, and PLMI, showing that while ensemble improves robustness, no model architecture consistently minimizes bias in performance and clinical markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H, DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA cohorts). Finally, we introduce ensemble disagreement metrics - entropy and inter-model divergence based - predicting regions of scorer disagreement with ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty."
      },
      {
        "id": "oai:arXiv.org:2506.08577v1",
        "title": "Diffusion-based Time Series Forecasting for Sewerage Systems",
        "link": "https://arxiv.org/abs/2506.08577",
        "author": "Nicholas A. Pearson, Francesca Cairoli, Luca Bortolussi, Davide Russo, Francesca Zanello",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08577v1 Announce Type: new \nAbstract: We introduce a novel deep learning approach that harnesses the power of generative artificial intelligence to enhance the accuracy of contextual forecasting in sewerage systems. By developing a diffusion-based model that processes multivariate time series data, our system excels at capturing complex correlations across diverse environmental signals, enabling robust predictions even during extreme weather events. To strengthen the model's reliability, we further calibrate its predictions with a conformal inference technique, tailored for probabilistic time series data, ensuring that the resulting prediction intervals are statistically reliable and cover the true target values with a desired confidence level. Our empirical tests on real sewerage system data confirm the model's exceptional capability to deliver reliable contextual predictions, maintaining accuracy even under severe weather conditions."
      },
      {
        "id": "oai:arXiv.org:2506.08584v1",
        "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling",
        "link": "https://arxiv.org/abs/2506.08584",
        "author": "Yahan Li, Jifan Yao, John Bosco S. Bunyi, Adam C. Frank, Angel Hwang, Ruishan Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08584v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly proposed for use in mental health support, yet their behavior in realistic counseling scenarios remains largely untested. We introduce CounselBench, a large-scale benchmark developed with 100 mental health professionals to evaluate and stress-test LLMs in single-turn counseling. The first component, CounselBench-EVAL, contains 2,000 expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human therapists to real patient questions. Each response is rated along six clinically grounded dimensions, with written rationales and span-level annotations. We find that LLMs often outperform online human therapists in perceived quality, but experts frequently flag their outputs for safety concerns such as unauthorized medical advice. Follow-up experiments show that LLM judges consistently overrate model responses and overlook safety issues identified by human experts. To probe failure modes more directly, we construct CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling questions designed to trigger specific model issues. Evaluation across 2,880 responses from eight LLMs reveals consistent, model-specific failure patterns. Together, CounselBench establishes a clinically grounded framework for benchmarking and improving LLM behavior in high-stakes mental health settings."
      },
      {
        "id": "oai:arXiv.org:2506.08591v1",
        "title": "Diversity-Guided MLP Reduction for Efficient Large Vision Transformers",
        "link": "https://arxiv.org/abs/2506.08591",
        "author": "Chengchao Shen, Hourun Zhu, Gongfan Fang, Jianxin Wang, Xinchao Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08591v1 Announce Type: new \nAbstract: Transformer models achieve excellent scaling property, where the performance is improved with the increment of model capacity. However, large-scale model parameters lead to an unaffordable cost of computing and memory. We analyze popular transformer architectures and find that multilayer perceptron (MLP) modules take up the majority of model parameters.\nTo this end, we focus on the recoverability of the compressed models and propose a Diversity-Guided MLP Reduction (DGMR) method to significantly reduce the parameters of large vision transformers with only negligible performance degradation. Specifically, we conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons of MLP hidden layer, while preserving weight diversity for better performance recover during distillation. Compared to the model trained from scratch, our pruned model only requires 0.06\\% data of LAION-2B (for the training of large vision transformers) without labels (ImageNet-1K) to recover the original performance. Experimental results on several state-of-the-art large vision transformers demonstrate that our method achieves a more than 57.0\\% parameter and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B), our method accomplishes a 71.5\\% parameter and FLOPs reduction without performance degradation. The source code and trained weights are available at https://github.com/visresearch/DGMR."
      },
      {
        "id": "oai:arXiv.org:2506.08592v1",
        "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings",
        "link": "https://arxiv.org/abs/2506.08592",
        "author": "Liyan Xu, Zhenlin Su, Mo Yu, Jiangnan Li, Fandong Meng, Jie Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08592v1 Announce Type: new \nAbstract: This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval."
      },
      {
        "id": "oai:arXiv.org:2506.08593v1",
        "title": "Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models",
        "link": "https://arxiv.org/abs/2506.08593",
        "author": "Shuzhou Yuan, Ercong Nie, Mario Tawfelis, Helmut Schmid, Hinrich Sch\\\"utze, Michael F\\\"arber",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08593v1 Announce Type: new \nAbstract: Hate speech detection is a socially sensitive and inherently subjective task, with judgments often varying based on personal traits. While prior work has examined how socio-demographic factors influence annotation, the impact of personality traits on Large Language Models (LLMs) remains largely unexplored. In this paper, we present the first comprehensive study on the role of persona prompts in hate speech classification, focusing on MBTI-based traits. A human annotation survey confirms that MBTI dimensions significantly affect labeling behavior. Extending this to LLMs, we prompt four open-source models with MBTI personas and evaluate their outputs across three hate speech datasets. Our analysis uncovers substantial persona-driven variation, including inconsistencies with ground truth, inter-persona disagreement, and logit-level biases. These findings highlight the need to carefully define persona prompts in LLM-based annotation workflows, with implications for fairness and alignment with human values."
      },
      {
        "id": "oai:arXiv.org:2506.08596v1",
        "title": "Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems",
        "link": "https://arxiv.org/abs/2506.08596",
        "author": "Guyang Zhang, Waleed Abdulla",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08596v1 Announce Type: new \nAbstract: Transformers have become the architecture of choice for learning long-range dependencies, yet their adoption in hyperspectral imaging (HSI) is still emerging. We reviewed more than 300 papers published up to 2025 and present the first end-to-end survey dedicated to Transformer-based HSI classification. The study categorizes every stage of a typical pipeline-pre-processing, patch or pixel tokenization, positional encoding, spatial-spectral feature extraction, multi-head self-attention variants, skip connections, and loss design-and contrasts alternative design choices with the unique spatial-spectral properties of HSI. We map the field's progress against persistent obstacles: scarce labeled data, extreme spectral dimensionality, computational overhead, and limited model explainability. Finally, we outline a research agenda prioritizing valuable public data sets, lightweight on-edge models, illumination and sensor shifts robustness, and intrinsically interpretable attention mechanisms. Our goal is to guide researchers in selecting, combining, or extending Transformer components that are truly fit for purpose for next-generation HSI applications."
      },
      {
        "id": "oai:arXiv.org:2506.08600v1",
        "title": "CALT: A Library for Computer Algebra with Transformer",
        "link": "https://arxiv.org/abs/2506.08600",
        "author": "Hiroshi Kera, Shun Arakawa, Yuta Sato",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08600v1 Announce Type: new \nAbstract: Recent advances in artificial intelligence have demonstrated the learnability of symbolic computation through end-to-end deep learning. Given a sufficient number of examples of symbolic expressions before and after the target computation, Transformer models - highly effective learners of sequence-to-sequence functions - can be trained to emulate the computation. This development opens up several intriguing challenges and new research directions, which require active contributions from the symbolic computation community. In this work, we introduce Computer Algebra with Transformer (CALT), a user-friendly Python library designed to help non-experts in deep learning train models for symbolic computation tasks."
      },
      {
        "id": "oai:arXiv.org:2506.08604v1",
        "title": "Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation",
        "link": "https://arxiv.org/abs/2506.08604",
        "author": "Giacomo Baldan, Qiang Liu, Alberto Guardone, Nils Thuerey",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08604v1 Announce Type: new \nAbstract: Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $\\sigma_{\\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications."
      },
      {
        "id": "oai:arXiv.org:2506.08607v1",
        "title": "Sample Efficient Demonstration Selection for In-Context Learning",
        "link": "https://arxiv.org/abs/2506.08607",
        "author": "Kiran Purohit, V Venktesh, Sourangshu Bhattacharya, Avishek Anand",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08607v1 Announce Type: new \nAbstract: The in-context learning paradigm with LLMs has been instrumental in advancing a wide range of natural language processing tasks. The selection of few-shot examples (exemplars / demonstration samples) is essential for constructing effective prompts under context-length budget constraints. In this paper, we formulate the exemplar selection task as a top-m best arms identification problem. A key challenge in this setup is the exponentially large number of arms that need to be evaluated to identify the m-best arms. We propose CASE (Challenger Arm Sampling for Exemplar selection), a novel sample-efficient selective exploration strategy that maintains a shortlist of \"challenger\" arms, which are current candidates for the top-m arms. In each iteration, only one of the arms from this shortlist or the current topm set is pulled, thereby reducing sample complexity and, consequently, the number of LLM evaluations. Furthermore, we model the scores of exemplar subsets (arms) using a parameterized linear scoring function, leading to stochastic linear bandits setting. CASE achieves remarkable efficiency gains of up to 7x speedup in runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing performance compared to state-of-the-art exemplar selection methods. We release our code and data at https://github.com/kiranpurohit/CASE"
      },
      {
        "id": "oai:arXiv.org:2506.08611v1",
        "title": "Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation",
        "link": "https://arxiv.org/abs/2506.08611",
        "author": "Shiji Zhao, Chi Chen, Ranjie Duan, Xizhe Wang, Xingxing Wei",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08611v1 Announce Type: new \nAbstract: Adversarial Training (AT) is widely recognized as an effective approach to enhance the adversarial robustness of Deep Neural Networks. As a variant of AT, Adversarial Robustness Distillation (ARD) has shown outstanding performance in enhancing the robustness of small models. However, both AT and ARD face robust fairness issue: these models tend to display strong adversarial robustness against some classes (easy classes) while demonstrating weak adversarial robustness against others (hard classes). This paper explores the underlying factors of this problem and points out the smoothness degree of soft labels for different classes significantly impacts the robust fairness from both empirical observation and theoretical analysis. Based on the above exploration, we propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge Distillation framework to enhance the adversarial robust fairness. Specifically, ABSLD adaptively reduces the student's error risk gap between different classes, which is accomplished by adjusting the class-wise smoothness degree of teacher's soft labels during the training process, and the adjustment is managed by assigning varying temperatures to different classes. Additionally, as a label-based approach, ABSLD is highly adaptable and can be integrated with the sample-based methods. Extensive experiments demonstrate ABSLD outperforms state-of-the-art methods on the comprehensive performance of robustness and fairness."
      },
      {
        "id": "oai:arXiv.org:2506.08612v1",
        "title": "Data-Efficient Challenges in Visual Inductive Priors: A Retrospective",
        "link": "https://arxiv.org/abs/2506.08612",
        "author": "Robert-Jan Bruintjes, Attila Lengyel, Osman Semih Kayhan, Davide Zambrano, Nergis T\\\"omen, Hadi Jamali-Rad, Jan van Gemert",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08612v1 Announce Type: new \nAbstract: Deep Learning requires large amounts of data to train models that work well. In data-deficient settings, performance can be degraded. We investigate which Deep Learning methods benefit training models in a data-deficient setting, by organizing the \"VIPriors: Visual Inductive Priors for Data-Efficient Deep Learning\" workshop series, featuring four editions of data-impaired challenges. These challenges address the problem of training deep learning models for computer vision tasks with limited data. Participants are limited to training models from scratch using a low number of training samples and are not allowed to use any form of transfer learning. We aim to stimulate the development of novel approaches that incorporate prior knowledge to improve the data efficiency of deep learning models. Successful challenge entries make use of large model ensembles that mix Transformers and CNNs, as well as heavy data augmentation. Novel prior knowledge-based methods contribute to success in some entries."
      },
      {
        "id": "oai:arXiv.org:2506.08613v1",
        "title": "SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything",
        "link": "https://arxiv.org/abs/2506.08613",
        "author": "Joost van Dalen, Yuki M. Asano, Marc Russwurm",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08613v1 Announce Type: new \nAbstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel visualization for multispectral images. We develop SAMSelect and show its use for marine scientists visually interpreting floating marine debris in Sentinel-2 imagery. These debris are notoriously difficult to visualize due to their compositional heterogeneity in medium-resolution imagery. Out of these difficulties, a visual interpretation of imagery showing marine debris remains a common practice by domain experts, who select bands and spectral indices on a case-by-case basis informed by common practices and heuristics. SAMSelect selects the band or index combination that achieves the best classification accuracy on a small annotated dataset through the Segment Anything Model. Its central assumption is that the three-channel visualization achieves the most accurate segmentation results also provide good visual information for photo-interpretation.\n  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets from the Plastic Litter Project. This reveals the potential of new previously unused band combinations (e.g., a normalized difference index of B8, B2), which demonstrate improved performance compared to literature-based indices. We describe the algorithm in this paper and provide an open-source code repository that will be helpful for domain scientists doing visual photo interpretation, especially in the marine field."
      },
      {
        "id": "oai:arXiv.org:2506.08618v1",
        "title": "HSG-12M: A Large-Scale Spatial Multigraph Dataset",
        "link": "https://arxiv.org/abs/2506.08618",
        "author": "Xianquan Yan, Hakan Akg\\\"un, Kenji Kawaguchi, N. Duane Loh, Ching Hua Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08618v1 Announce Type: new \nAbstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing physically distinct paths into a single link. We introduce HSG-12M, the first large-scale dataset of $\\textbf{spatial multigraphs}-$graphs embedded in a metric space where multiple geometrically distinct trajectories between two nodes are retained as separate edges. HSG-12M contains 11.6 million static and 5.1 million dynamic $\\textit{Hamiltonian spectral graphs}$ across 1401 characteristic-polynomial classes, derived from 177 TB of spectral potential data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum on the complex plane, producing diverse, physics-grounded topologies that transcend conventional node-coordinate datasets. To enable future extensions, we release $\\texttt{Poly2Graph}$: a high-performance, open-source pipeline that maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with popular GNNs expose new challenges in learning from multi-edge geometry at scale. Beyond its practical utility, we show that spectral graphs serve as universal topological fingerprints of polynomials, vectors, and matrices, forging a new algebra-to-graph link. HSG-12M lays the groundwork for geometry-aware graph learning and new opportunities of data-driven scientific discovery in condensed matter physics and beyond."
      },
      {
        "id": "oai:arXiv.org:2506.08619v1",
        "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering",
        "link": "https://arxiv.org/abs/2506.08619",
        "author": "Gon\\c{c}alo Dias Pais, Valter Piedade, Moitreya Chatterjee, Marcus Greiff, Pedro Miraldo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08619v1 Announce Type: new \nAbstract: Several variants of Neural Radiance Fields (NeRFs) have significantly improved the accuracy of synthesized images and surface reconstruction of 3D scenes/objects. In all of these methods, a key characteristic is that none can train the neural network with every possible input data, specifically, every pixel and potential 3D point along the projection rays due to scalability issues. While vanilla NeRFs uniformly sample both the image pixels and 3D points along the projection rays, some variants focus only on guiding the sampling of the 3D points along the projection rays. In this paper, we leverage the implicit surface representation of the foreground scene and model a probability density function in a 3D image projection space to achieve a more targeted sampling of the rays toward regions of interest, resulting in improved rendering. Additionally, a new surface reconstruction loss is proposed for improved performance. This new loss fully explores the proposed 3D image projection space model and incorporates near-to-surface and empty space components. By integrating our novel sampling strategy and novel loss into current state-of-the-art neural implicit surface renderers, we achieve more accurate and detailed 3D reconstructions and improved image rendering, especially for the regions of interest in any given scene."
      },
      {
        "id": "oai:arXiv.org:2506.08625v1",
        "title": "RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval",
        "link": "https://arxiv.org/abs/2506.08625",
        "author": "Minhae Oh, Jeonghye Kim, Nakyung Lee, Donggeon Seo, Taeuk Kim, Jungwoo Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08625v1 Announce Type: new \nAbstract: Scientific reasoning requires not only long-chain reasoning processes, but also knowledge of domain-specific terminologies and adaptation to updated findings. To deal with these challenges for scientific reasoning, we introduce RAISE, a step-by-step retrieval-augmented framework which retrieves logically relevant documents from in-the-wild corpus. RAISE is divided into three steps: problem decomposition, logical query generation, and logical retrieval. We observe that RAISE consistently outperforms other baselines on scientific reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves documents that are not only similar in terms of the domain knowledge, but also documents logically more relevant."
      },
      {
        "id": "oai:arXiv.org:2506.08629v1",
        "title": "ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network",
        "link": "https://arxiv.org/abs/2506.08629",
        "author": "Feixiang Du, Shengkun Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08629v1 Announce Type: new \nAbstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers have achieved wide applicaiton in semantic segmentation tasks. Although CNNs with Transformer models greatly improve performance, the global context modeling remains inadequate. Recently, Mamba achieved great potential in vision tasks, showing its advantages in modeling long-range dependency. In this paper, we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation, dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based framework to address their complementary weaknesses. Specifically, We design a Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to improve the representations ability of feature, We devise a Multi-Scale Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion Module (FFM) merges diverse level feature, significantly enhancing segmented accuracy. Extensive experiments on two representative datasets demonstrate that the proposed model excels in accuracy and efficiency balance, achieving 70.6% mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M parameters and 8.27G FLOPs on a single RTX 3090 GPU platform."
      },
      {
        "id": "oai:arXiv.org:2506.08632v1",
        "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping",
        "link": "https://arxiv.org/abs/2506.08632",
        "author": "Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Dong Chen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08632v1 Announce Type: new \nAbstract: Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning."
      },
      {
        "id": "oai:arXiv.org:2506.08635v1",
        "title": "SurfR: Surface Reconstruction with Multi-scale Attention",
        "link": "https://arxiv.org/abs/2506.08635",
        "author": "Siddhant Ranade, Gon\\c{c}alo Dias Pais, Ross Tyler Whitaker, Jacinto C. Nascimento, Pedro Miraldo, Srikumar Ramalingam",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08635v1 Announce Type: new \nAbstract: We propose a fast and accurate surface reconstruction algorithm for unorganized point clouds using an implicit representation. Recent learning methods are either single-object representations with small neural models that allow for high surface details but require per-object training or generalized representations that require larger models and generalize to newer shapes but lack details, and inference is slow. We propose a new implicit representation for general 3D shapes that is faster than all the baselines at their optimum resolution, with only a marginal loss in performance compared to the state-of-the-art. We achieve the best accuracy-speed trade-off using three key contributions. Many implicit methods extract features from the point cloud to classify whether a query point is inside or outside the object. First, to speed up the reconstruction, we show that this feature extraction does not need to use the query point at an early stage (lazy query). Second, we use a parallel multi-scale grid representation to develop robust features for different noise levels and input resolutions. Finally, we show that attention across scales can provide improved reconstruction results."
      },
      {
        "id": "oai:arXiv.org:2506.08640v1",
        "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned",
        "link": "https://arxiv.org/abs/2506.08640",
        "author": "Yichong Lu, Yuzhuo Tian, Zijin Jiang, Yikun Zhao, Yuanbo Yang, Hao Ouyang, Haoji Hu, Huimin Yu, Yujun Shen, Yiyi Liao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08640v1 Announce Type: new \nAbstract: Humans intuitively perceive object shape and orientation from a single image, guided by strong priors about canonical poses. However, existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability in downstream tasks. To address this gap, we introduce the task of orientation-aligned 3D object generation: producing 3D objects from single images with consistent orientations across categories. To facilitate this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two representative 3D generative models based on multi-view diffusion and 3D variational autoencoder frameworks to produce aligned objects that generalize well to unseen objects across various categories. Experimental results demonstrate the superiority of our method over post-hoc alignment approaches. Furthermore, we showcase downstream applications enabled by our aligned object generation, including zero-shot object orientation estimation via analysis-by-synthesis and efficient arrow-based object rotation manipulation."
      },
      {
        "id": "oai:arXiv.org:2506.08641v1",
        "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers",
        "link": "https://arxiv.org/abs/2506.08641",
        "author": "Simon Roschmann, Quentin Bouniot, Vasilii Feofanov, Ievgen Redko, Zeynep Akata",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08641v1 Announce Type: new \nAbstract: Time series classification is a fundamental task in healthcare and industry, yet the development of time series foundation models (TSFMs) remains limited by the scarcity of publicly available time series datasets. In this work, we propose Time Vision Transformer (TiViT), a framework that converts time series into images to leverage the representational power of frozen Vision Transformers (ViTs) pretrained on large-scale image datasets. First, we theoretically motivate our approach by analyzing the 2D patching of ViTs for time series, showing that it can increase the number of label-relevant tokens and reduce the sample complexity. Second, we empirically demonstrate that TiViT achieves state-of-the-art performance on standard time series classification benchmarks by utilizing the hidden representations of large OpenCLIP models. We explore the structure of TiViT representations and find that intermediate layers with high intrinsic dimension are the most effective for time series classification. Finally, we assess the alignment between TiViT and TSFM representation spaces and identify a strong complementarity, with further performance gains achieved by combining their features. Our findings reveal yet another direction for reusing vision representations in a non-visual domain."
      },
      {
        "id": "oai:arXiv.org:2506.08643v1",
        "title": "MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models",
        "link": "https://arxiv.org/abs/2506.08643",
        "author": "Son The Nguyen, Theja Tulabandhula",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08643v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly used for both open-ended and structured tasks, yet their inference-time behavior is still largely dictated by heuristic decoding strategies such as greedy search, sampling, or reranking. These methods provide limited control and do not explicitly optimize for task-specific objectives. We introduce MEMETRON, a task-agnostic framework that formulates LLM decoding as a discrete black-box optimization problem. MEMETRON leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the response space, guided by reward models and contextual operations performed by the LLM itself. This approach enables efficient discovery of high-reward responses without requiring model retraining or gradient access. The framework is modular and generalizes across diverse tasks, requiring only a reward function and lightweight prompt templates. We evaluate our framework on the critical human preference alignment task and demonstrate that it significantly outperforms standard decoding and reranking methods, highlighting its potential to improve alignment without model retraining."
      },
      {
        "id": "oai:arXiv.org:2506.08644v1",
        "title": "Semi-gradient DICE for Offline Constrained Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08644",
        "author": "Woosung Kim, JunHo Seo, Jongmin Lee, Byung-Jun Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08644v1 Announce Type: new \nAbstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch between the stationary distribution induced by a policy and the target distribution required for reliable off-policy evaluation (OPE) and policy optimization. DICE-based offline constrained RL particularly benefits from the flexibility of DICE, as it simultaneously maximizes return while estimating costs in offline settings. However, we have observed that recent approaches designed to enhance the offline RL performance of the DICE framework inadvertently undermine its ability to perform OPE, making them unsuitable for constrained RL scenarios. In this paper, we identify the root cause of this limitation: their reliance on a semi-gradient optimization, which solves a fundamentally different optimization problem and results in failures in cost estimation. Building on these insights, we propose a novel method to enable OPE and constrained RL through semi-gradient DICE. Our method ensures accurate cost estimation and achieves state-of-the-art performance on the offline constrained RL benchmark, DSRL."
      },
      {
        "id": "oai:arXiv.org:2506.08645v1",
        "title": "Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach",
        "link": "https://arxiv.org/abs/2506.08645",
        "author": "Youqi Wu, Jingwei Zhang, Farzan Farnia",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08645v1 Announce Type: new \nAbstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved promising results in aligning representations across modalities. However, these embeddings could underperform compared to state-of-the-art single-modality embeddings on modality-specific tasks. On the other hand, single-modality embeddings excel in their domains but lack cross-modal alignment capabilities. In this work, we focus on the problem of unifying cross-modality and single-modality embeddings to achieve the performance of modality-expert embedding within individual modalities while preserving cross-modal alignment. To this end, we propose RP-KrossFuse, a method that leverages a random projection-based Kronecker product to integrate cross-modal embeddings with single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise similarity scores of the fused embeddings and operates efficiently in a specified kernel space and supports scalable implementations via random Fourier features for shift-invariant kernels such as the Gaussian kernel. We demonstrate the effectiveness of RP-KrossFuse through several numerical experiments, combining CLIP embeddings with uni-modal image and text embeddings. Our numerical results indicate that RP-KrossFuse achieves competitive modality-specific performance while retaining cross-modal alignment, bridging the gap between cross-modal and single-modality embeddings."
      },
      {
        "id": "oai:arXiv.org:2506.08646v1",
        "title": "TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning",
        "link": "https://arxiv.org/abs/2506.08646",
        "author": "Mingyu Zheng, Zhifan Feng, Jia Wang, Lanrui Wang, Zheng Lin, Yang Hao, Weiping Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08646v1 Announce Type: new \nAbstract: Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at https://github.com/SpursGoZmy/TableDreamer"
      },
      {
        "id": "oai:arXiv.org:2506.08647v1",
        "title": "Summarization for Generative Relation Extraction in the Microbiome Domain",
        "link": "https://arxiv.org/abs/2506.08647",
        "author": "Oumaima El Khettari, Solen Quiniou, Samuel Chaffron",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08647v1 Announce Type: new \nAbstract: We explore a generative relation extraction (RE) pipeline tailored to the study of interactions in the intestinal microbiome, a complex and low-resource biomedical domain. Our method leverages summarization with large language models (LLMs) to refine context before extracting relations via instruction-tuned generation. Preliminary results on a dedicated corpus show that summarization improves generative RE performance by reducing noise and guiding the model. However, BERT-based RE approaches still outperform generative models. This ongoing work demonstrates the potential of generative methods to support the study of specialized domains in low-resources setting."
      },
      {
        "id": "oai:arXiv.org:2506.08649v1",
        "title": "Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization",
        "link": "https://arxiv.org/abs/2506.08649",
        "author": "Zhiyi Zhu, Xiaoyu Wu, Youwei Lu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08649v1 Announce Type: new \nAbstract: Video memorability refers to the ability of videos to be recalled after viewing, playing a crucial role in creating content that remains memorable. Existing models typically focus on extracting multimodal features to predict video memorability scores but often fail to fully utilize motion cues. The representation of motion features is compromised during the fine-tuning phase of the motion feature extractor due to a lack of labeled data. In this paper, we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal video memorability prediction model designed to enhance the representation of motion features. We tackle the challenge of improving motion feature representation by leveraging text description similarities across videos to establish positive and negative motion sample sets for a given target. This enhancement allows the model to learn similar feature representations for semantically related motion content, resulting in more accurate memorability predictions. Our model achieves state-of-the-art performance on two video memorability prediction datasets. Moreover, the potential applications of video memorability prediction have been underexplored. To address this gap, we present Memorability Weighted Correction for Video Summarization (MWCVS), using video memorability prediction to reduce subjectivity in video summarization labels. Experimental results on two video summarization datasets demonstrate the effectiveness of MWCVS, showcasing the promising applications of video memorability prediction."
      },
      {
        "id": "oai:arXiv.org:2506.08650v1",
        "title": "Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping",
        "link": "https://arxiv.org/abs/2506.08650",
        "author": "Peter Gr\\\"onquist, Stepan Tulyakov, Dengxin Dai",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08650v1 Announce Type: new \nAbstract: Achieving consistent color reproduction across multiple cameras is essential for seamless image fusion and Image Processing Pipeline (ISP) compatibility in modern devices, but it is a challenging task due to variations in sensors and optics. Existing raw-to-raw conversion methods face limitations such as poor adaptability to changing illumination, high computational costs, or impractical requirements such as simultaneous camera operation and overlapping fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight, physically-informed approach that simulates raw images under specified illumination to estimate transformations between devices. The NPM effectively adapts to varying illumination conditions, can be initialized with physical measurements, and supports training with or without paired data. Experiments on public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent state-of-the-art methods, providing robust chromatic consistency across different sensors and optical systems."
      },
      {
        "id": "oai:arXiv.org:2506.08652v1",
        "title": "JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset",
        "link": "https://arxiv.org/abs/2506.08652",
        "author": "Mahesh Godavarti",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08652v1 Announce Type: new \nAbstract: Transformers have demonstrated remarkable success in sequence modeling, yet effectively incorporating positional information remains a challenging and active area of research. In this paper, we introduce JoFormer, a journey-based Transformer architecture grounded in a recently proposed non-commutative algebra for composing transformations across positions. JoFormer represents relative positions through learnable directional transforms that are sequentially composed along the input, thereby extending and generalizing existing approaches based on relative position representations. We derive the JoFormer attention mechanism from first principles and show that it subsumes standard methods such as rotary transformations as special cases. To evaluate its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny Shakespeare character-level language modeling task. Our results demonstrate that\n  JoFormer consistently achieves lower perplexity and faster convergence, highlighting the advantages of its more expressive, journey-based treatment of position. Notably, the per-token JoFormer is still a primitive, conceptual variant with layer-independent angles, yet it already demonstrates strong performance-underscoring its promise as a proof of concept for more expressive architectures. We conclude by discussing how JoFormer offers a principled approach to integrating positional structure into Transformer architectures. The code used in this work is available at https://github.com/mahesh-godavarti/joformer."
      },
      {
        "id": "oai:arXiv.org:2506.08655v1",
        "title": "When Simple Model Just Works: Is Network Traffic Classification in Crisis?",
        "link": "https://arxiv.org/abs/2506.08655",
        "author": "Kamil Jerabek, Jan Luxemburk, Richard Plny, Josef Koumar, Jaroslav Pesek, Karel Hynek",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08655v1 Announce Type: new \nAbstract: Machine learning has been applied to network traffic classification (TC) for over two decades. While early efforts used shallow models, the latter 2010s saw a shift toward complex neural networks, often reporting near-perfect accuracy. However, it was recently revealed that a simple k-NN baseline using packet sequences metadata (sizes, times, and directions) can be on par or even outperform more complex methods. In this paper, we investigate this phenomenon further and evaluate this baseline across 12 datasets and 15 TC tasks, and investigate why it performs so well. Our analysis shows that most datasets contain over 50% redundant samples (identical packet sequences), which frequently appear in both training and test sets due to common splitting practices. This redundancy can lead to overestimated model performance and reduce the theoretical maximum accuracy when identical flows have conflicting labels. Given its distinct characteristics, we further argue that standard machine learning practices adapted from domains like NLP or computer vision may be ill-suited for TC. Finally, we propose new directions for task formulation and evaluation to address these challenges and help realign the field."
      },
      {
        "id": "oai:arXiv.org:2506.08660v1",
        "title": "Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness",
        "link": "https://arxiv.org/abs/2506.08660",
        "author": "Jinkwan Jang, Hyungjin Park, Jinmyeong Choi, Taesup Kim",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08660v1 Announce Type: new \nAbstract: Real-world time series data are inherently multivariate, often exhibiting complex inter-channel dependencies. Each channel is typically sampled at its own period and is prone to missing values due to various practical and operational constraints. These characteristics pose fundamental challenges related to channel dependency, sampling asynchrony, and missingness, all of which must be addressed to enable robust and reliable forecasting in practical settings. However, most existing architectures are built on oversimplified assumptions, such as identical sampling periods across channels and fully observed inputs at test time, which often do not hold in real-world scenarios. To bridge this gap, we propose ChannelTokenFormer, a Transformer-based forecasting model with a flexible architecture designed to explicitly capture cross-channel interactions, accommodate channel-wise asynchronous sampling, and effectively handle missing values. Extensive experiments on three benchmark datasets modified to reflect practical settings, along with one real-world industrial dataset, demonstrate the superior robustness and accuracy of ChannelTokenFormer under challenging real-world conditions."
      },
      {
        "id": "oai:arXiv.org:2506.08662v1",
        "title": "Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization",
        "link": "https://arxiv.org/abs/2506.08662",
        "author": "Florian Borzechowski, Michael Sch\\\"afer, Heiko Schwarz, Jonathan Pfaff, Detlev Marpe, Thomas Wiegand",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08662v1 Announce Type: new \nAbstract: The continuous improvements on image compression with variational autoencoders have lead to learned codecs competitive with conventional approaches in terms of rate-distortion efficiency. Nonetheless, taking the quantization into account during the training process remains a problem, since it produces zero derivatives almost everywhere and needs to be replaced with a differentiable approximation which allows end-to-end optimization. Though there are different methods for approximating the quantization, none of them model the quantization noise correctly and thus, result in suboptimal networks. Hence, we propose an additional finetuning training step: After conventional end-to-end training, parts of the network are retrained on quantized latents obtained at the inference stage. For entropy-constraint quantizers like Trellis-Coded Quantization, the impact of the quantizer is particularly difficult to approximate by rounding or adding noise as the quantized latents are interdependently chosen through a trellis search based on both the entropy model and a distortion measure. We show that retraining on correctly quantized data consistently yields additional coding gain for both uniform scalar and especially for entropy-constraint quantization, without increasing inference complexity. For the Kodak test set, we obtain average savings between 1% and 2%, and for the TecNick test set up to 2.2% in terms of Bj{\\o}ntegaard-Delta bitrate."
      },
      {
        "id": "oai:arXiv.org:2506.08666v1",
        "title": "LLaVA-c: Continual Improved Visual Instruction Tuning",
        "link": "https://arxiv.org/abs/2506.08666",
        "author": "Wenzhuo Liu, Fei Zhu, Haiyang Guo, Longhui Wei, Cheng-Lin Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08666v1 Announce Type: new \nAbstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual understanding through visual instruction tuning on multitask datasets, enabling strong instruction-following and multimodal performance. However, multitask learning faces challenges such as task balancing, requiring careful adjustment of data proportions, and expansion costs, where new tasks risk catastrophic forgetting and need costly retraining. Continual learning provides a promising alternative to acquiring new knowledge incrementally while preserving existing capabilities. However, current methods prioritize task-specific performance, neglecting base model degradation from overfitting to specific instructions, which undermines general capabilities. In this work, we propose a simple but effective method with two modifications on LLaVA-1.5: spectral-aware consolidation for improved task balance and unsupervised inquiry regularization to prevent base model degradation. We evaluate both general and task-specific performance across continual pretraining and fine-tuning. Experiments demonstrate that LLaVA-c consistently enhances standard benchmark performance and preserves general capabilities. For the first time, we show that task-by-task continual learning can achieve results that match or surpass multitask joint learning. The code will be publicly released."
      },
      {
        "id": "oai:arXiv.org:2506.08669v1",
        "title": "Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search",
        "link": "https://arxiv.org/abs/2506.08669",
        "author": "Dongge Han, Menglin Xia, Daniel Madrigal Diaz, Samuel Kessler, Ankur Mallick, Xuchao Zhang, Mirian Del Carmen Hipolito Garcia, Jin Xu, Victor R\\\"uhle, Saravan Rajmohan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08669v1 Announce Type: new \nAbstract: Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMs' limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2506.08672v1",
        "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling",
        "link": "https://arxiv.org/abs/2506.08672",
        "author": "Yang Liu, Jiaqi Li, Zilong Zheng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08672v1 Announce Type: new \nAbstract: Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1% average points on eight ID tasks and $\\Delta$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL."
      },
      {
        "id": "oai:arXiv.org:2506.08673v1",
        "title": "Towards Fair Representation: Clustering and Consensus",
        "link": "https://arxiv.org/abs/2506.08673",
        "author": "Diptarka Chakraborty, Kushagra Chatterjee, Debarati Das, Tien Long Nguyen, Romina Nobahari",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08673v1 Announce Type: new \nAbstract: Consensus clustering, a fundamental task in machine learning and data analysis, aims to aggregate multiple input clusterings of a dataset, potentially based on different non-sensitive attributes, into a single clustering that best represents the collective structure of the data. In this work, we study this fundamental problem through the lens of fair clustering, as introduced by Chierichetti et al. [NeurIPS'17], which incorporates the disparate impact doctrine to ensure proportional representation of each protected group in the dataset within every cluster. Our objective is to find a consensus clustering that is not only representative but also fair with respect to specific protected attributes. To the best of our knowledge, we are the first to address this problem and provide a constant-factor approximation.\n  As part of our investigation, we examine how to minimally modify an existing clustering to enforce fairness -- an essential postprocessing step in many clustering applications that require fair representation. We develop an optimal algorithm for datasets with equal group representation and near-linear time constant factor approximation algorithms for more general scenarios with different proportions of two group sizes. We complement our approximation result by showing that the problem is NP-hard for two unequal-sized groups. Given the fundamental nature of this problem, we believe our results on Closest Fair Clustering could have broader implications for other clustering problems, particularly those for which no prior approximation guarantees exist for their fair variants."
      },
      {
        "id": "oai:arXiv.org:2506.08678v1",
        "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction",
        "link": "https://arxiv.org/abs/2506.08678",
        "author": "Juan Yeo, Soonwoo Cha, Jiwoo Song, Hyunbin Jin, Taesup Kim",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08678v1 Announce Type: new \nAbstract: Vision-language models such as CLIP have recently propelled open-vocabulary dense prediction tasks by enabling recognition of a broad range of visual concepts. However, CLIP still struggles with fine-grained, region-level understanding, hindering its effectiveness on these dense prediction tasks. We identify two pivotal factors required to address this limitation: semantic coherence and fine-grained vision-language alignment. Current adaptation methods often improve fine-grained alignment at the expense of semantic coherence, and often rely on extra modules or supervised fine-tuning. To overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel approach that simultaneously enhances semantic coherence and fine-grained alignment by leveraging own knowledge of a model across all representation levels. Unlike prior methods, ATAS uses only unlabeled images and an internal self-distillation process to refine representations of CLIP vision encoders, preserving local semantic consistency while sharpening local detail recognition. On open-vocabulary object detection and semantic segmentation benchmarks, ATAS achieves substantial performance gains, outperforming baseline CLIP models. These results validate the effectiveness of our approach and underscore the importance of jointly maintaining semantic coherence and fine-grained alignment for advanced open-vocabulary dense prediction."
      },
      {
        "id": "oai:arXiv.org:2506.08681v1",
        "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling",
        "link": "https://arxiv.org/abs/2506.08681",
        "author": "Phuc Minh Nguyen, Ngoc-Hieu Nguyen, Duy H. M. Nguyen, Anji Liu, An Mai, Binh T. Nguyen, Daniel Sonntag, Khoa D. Doan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08681v1 Announce Type: new \nAbstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) with human values. However, these methods are more susceptible to over-optimization, in which the model drifts away from the reference policy, leading to degraded performance as training progresses. This paper proposes a novel importance-sampling approach to mitigate the over-optimization problem of offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective with an importance ratio that accounts for the reference policy distribution. IS-DAAs additionally avoid the high variance issue associated with importance sampling by clipping the importance ratio to a maximum value. Our extensive experiments demonstrate that IS-DAAs can effectively mitigate over-optimization, especially under low regularization strength, and achieve better performance than other methods designed to address this problem. Our implementations are provided publicly at this link."
      },
      {
        "id": "oai:arXiv.org:2506.08686v1",
        "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths",
        "link": "https://arxiv.org/abs/2506.08686",
        "author": "Soham Poddar, Paramita Koley, Janardan Misra, Sanjay Podder, Navveen Balani, Niloy Ganguly, Saptarshi Ghosh",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08686v1 Announce Type: new \nAbstract: A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\\% by reducing the response length while preserving the quality of LLM responses."
      },
      {
        "id": "oai:arXiv.org:2506.08690v1",
        "title": "CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities",
        "link": "https://arxiv.org/abs/2506.08690",
        "author": "Hugo Porta, Emanuele Dalsasso, Jessica L. McCarty, Devis Tuia",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08690v1 Announce Type: new \nAbstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent history, causing damage across ecosystems, destroying communities, and emitting large quantities of CO2. This extreme wildfire season is symptomatic of a climate-change-induced increase in the length and severity of the fire season that affects the boreal ecosystem. Therefore, it is critical to empower wildfire management in boreal communities with better mitigation solutions. Wildfire probability maps represent an important tool for understanding the likelihood of wildfire occurrence and the potential severity of future wildfires. The massive increase in the availability of Earth observation data has enabled the development of deep learning-based wildfire forecasting models, aiming at providing precise wildfire probability maps at different spatial and temporal scales. A main limitation of such methods is their reliance on coarse-resolution environmental drivers and satellite products, leading to wildfire occurrence prediction of reduced resolution, typically around $\\sim 0.1${\\deg}. This paper presents a benchmark dataset: CanadaFireSat, and baseline methods for high-resolution: 100 m wildfire forecasting across Canada, leveraging multi-modal data from high-resolution multi-spectral satellite images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and environmental factors (ERA5 reanalysis data). Our experiments consider two major deep learning architectures. We observe that using multi-modal temporal inputs outperforms single-modal temporal inputs across all metrics, achieving a peak performance of 60.3% in F1 score for the 2023 wildfire season, a season never seen during model training. This demonstrates the potential of multi-modal deep learning models for wildfire forecasting at high-resolution and continental scale."
      },
      {
        "id": "oai:arXiv.org:2506.08691v1",
        "title": "VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism",
        "link": "https://arxiv.org/abs/2506.08691",
        "author": "Congzhi Zhang, Jiawei Peng, Zhenglin Wang, Yilong Lai, Haowen Sun, Heng Chang, Fei Ma, Weijiang Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08691v1 Announce Type: new \nAbstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in multimodal tasks, but their effectiveness in complex visual reasoning is still constrained, especially when employing Chain-of-Thought prompting techniques. In this paper, we propose VReST, a novel training-free approach that enhances Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms. VReST meticulously traverses the reasoning landscape by establishing a search tree, where each node encapsulates a reasoning step, and each path delineates a comprehensive reasoning sequence. Our innovative multimodal Self-Reward mechanism assesses the quality of reasoning steps by integrating the utility of sub-questions, answer correctness, and the relevance of vision-language clues, all without the need for additional models. VReST surpasses current prompting methods and secures state-of-the-art performance across three multimodal mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy of test-time scaling laws in multimodal tasks, offering a promising direction for future research."
      },
      {
        "id": "oai:arXiv.org:2506.08694v1",
        "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning",
        "link": "https://arxiv.org/abs/2506.08694",
        "author": "Mohammadreza Salehi, Shashanka Venkataramanan, Ioana Simion, Efstratios Gavves, Cees G. M. Snoek, Yuki M Asano",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08694v1 Announce Type: new \nAbstract: Dense self-supervised learning has shown great promise for learning pixel- and patch-level representations, but extending it to videos remains challenging due to the complexity of motion dynamics. Existing approaches struggle as they rely on static augmentations that fail under object deformations, occlusions, and camera movement, leading to inconsistent feature learning over time. We propose a motion-guided self-supervised learning framework that clusters dense point tracks to learn spatiotemporally consistent representations. By leveraging an off-the-shelf point tracker, we extract long-range motion trajectories and optimize feature clustering through a momentum-encoder-based optimal transport mechanism. To ensure temporal coherence, we propagate cluster assignments along tracked points, enforcing feature consistency across views despite viewpoint changes. Integrating motion as an implicit supervisory signal, our method learns representations that generalize across frames, improving robustness in dynamic scenes and challenging occlusion scenarios. By initializing from strong image-pretrained models and leveraging video data for training, we improve state-of-the-art by 1% to 6% on six image and video datasets and four evaluation benchmarks. The implementation is publicly available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main"
      },
      {
        "id": "oai:arXiv.org:2506.08698v1",
        "title": "Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data",
        "link": "https://arxiv.org/abs/2506.08698",
        "author": "Boyu Xie, Tangtang Xie",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08698v1 Announce Type: new \nAbstract: With the development of smart grids, High-Dimensional and Incomplete (HDI) Power Load Monitoring (PLM) data challenges the performance of Power Load Forecasting (PLF) models. In this paper, we propose a potential characterization model VAE-LF based on Variational Autoencoder (VAE) for efficiently representing and complementing PLM missing data. VAE-LF learns a low-dimensional latent representation of the data using an Encoder-Decoder structure by splitting the HDI PLM data into vectors and feeding them sequentially into the VAE-LF model, and generates the complementary data. Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark models in both 5% and 10% sparsity test cases, with significantly lower RMSE and MAE, and especially outperforms on low sparsity ratio data. The method provides an efficient data-completion solution for electric load management in smart grids."
      },
      {
        "id": "oai:arXiv.org:2506.08699v1",
        "title": "ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds",
        "link": "https://arxiv.org/abs/2506.08699",
        "author": "Frederik Hagelskjaer",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08699v1 Announce Type: new \nAbstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose estimation network for colorless point clouds. The pose estimation is calculated from center and top points of the object, predicted by the neural network. The network is trained on synthetic data, and tested on a benchmark dataset, where it demonstrates state-of-the-art performance and outperforms all colorless methods. The network is able to run inference in only 250 milliseconds making it usable in many scenarios. Project page with code at arrowpose.github.io"
      },
      {
        "id": "oai:arXiv.org:2506.08700v1",
        "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts",
        "link": "https://arxiv.org/abs/2506.08700",
        "author": "Ruiran Su, Jiasheng Si, Zhijiang Guo, Janet B. Pierrehumbert",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08700v1 Announce Type: new \nAbstract: Scientific fact-checking has mostly focused on text and tables, overlooking scientific charts, which are key for presenting quantitative evidence and statistical reasoning. We introduce ClimateViz, the first large-scale benchmark for scientific fact-checking using expert-curated scientific charts. ClimateViz contains 49,862 claims linked to 2,896 visualizations, each labeled as support, refute, or not enough information. To improve interpretability, each example includes structured knowledge graph explanations covering trends, comparisons, and causal relations. We evaluate state-of-the-art multimodal language models, including both proprietary and open-source systems, in zero-shot and few-shot settings. Results show that current models struggle with chart-based reasoning: even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to 77.8 percent accuracy in label-only settings, far below human performance (89.3 and 92.7 percent). Explanation-augmented outputs improve performance in some models. We released our dataset and code alongside the paper."
      },
      {
        "id": "oai:arXiv.org:2506.08704v1",
        "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering",
        "link": "https://arxiv.org/abs/2506.08704",
        "author": "Xiaohan Zhang, Sitong Wang, Yushen Yan, Yi Yang, Mingda Xu, Qi Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08704v1 Announce Type: new \nAbstract: High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches."
      },
      {
        "id": "oai:arXiv.org:2506.08710v1",
        "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting",
        "link": "https://arxiv.org/abs/2506.08710",
        "author": "Mengjiao Ma, Qi Ma, Yue Li, Jiahuan Cheng, Runyi Yang, Bin Ren, Nikola Popovic, Mingqiang Wei, Nicu Sebe, Luc Van Gool, Theo Gevers, Martin R. Oswald, Danda Pani Paudel",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08710v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets will be made public to accelerate research in generalizable 3DGS scene understanding."
      },
      {
        "id": "oai:arXiv.org:2506.08712v1",
        "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization",
        "link": "https://arxiv.org/abs/2506.08712",
        "author": "Hee Suk Yoon, Eunseop Yoon, Mark A. Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08712v1 Announce Type: new \nAbstract: We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead."
      },
      {
        "id": "oai:arXiv.org:2506.08713v1",
        "title": "Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure",
        "link": "https://arxiv.org/abs/2506.08713",
        "author": "Fariz Ikhwantri, Dusica Marijan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08713v1 Announce Type: new \nAbstract: Ensuring complex systems meet regulations typically requires checking the validity of assurance cases through a claim-argument-evidence framework. Some challenges in this process include the complicated nature of legal and technical texts, the need for model explanations, and limited access to assurance case data. We propose a compliance detection approach based on Natural Language Inference (NLI): EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the claim-argument-evidence structure of an assurance case as a multi-hop inference for explainable and traceable compliance detection. We address the limited number of assurance cases by generating them using large language models (LLMs). We introduce metrics that measure the coverage and structural consistency. We demonstrate the effectiveness of the generated assurance case from GDPR requirements in a multi-hop inference task as a case study. Our results highlight the potential of NLI-based approaches in automating the regulatory compliance process."
      },
      {
        "id": "oai:arXiv.org:2506.08717v1",
        "title": "Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition",
        "link": "https://arxiv.org/abs/2506.08717",
        "author": "Mehedi Hasan Bijoy, Dejan Porjazovski, Tam\\'as Gr\\'osz, Mikko Kurimo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08717v1 Announce Type: new \nAbstract: Speech Emotion Recognition (SER) is crucial for improving human-computer interaction. Despite strides in monolingual SER, extending them to build a multilingual system remains challenging. Our goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models. To address this, we introduce a novel language-aware multi-teacher knowledge distillation method to advance SER in English, Finnish, and French. It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and then distills their knowledge into a single multilingual student model. The student model demonstrates state-of-the-art performance, with a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing fine-tuning and knowledge distillation baselines. Our method excels in improving recall for sad and neutral emotions, although it still faces challenges in recognizing anger and happiness."
      },
      {
        "id": "oai:arXiv.org:2506.08726v1",
        "title": "Improved LLM Agents for Financial Document Question Answering",
        "link": "https://arxiv.org/abs/2506.08726",
        "author": "Nelvin Tan, Zian Seng, Liang Zhang, Yu-Ching Shih, Dong Yang, Amol Salunkhe",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08726v1 Announce Type: new \nAbstract: Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance."
      },
      {
        "id": "oai:arXiv.org:2506.08727v1",
        "title": "Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs",
        "link": "https://arxiv.org/abs/2506.08727",
        "author": "Samarth Sikand, Rohit Mehra, Priyavanshi Pathania, Nikhil Bamby, Vibhu Saujanya Sharma, Vikrant Kaulgud, Sanjay Podder, Adam P. Burden",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08727v1 Announce Type: new \nAbstract: While Generative AI stands to be one of the fastest adopted technologies ever, studies have made evident that the usage of Large Language Models (LLMs) puts significant burden on energy grids and our environment. It may prove a hindrance to the Sustainability goals of any organization. A crucial step in any Sustainability strategy is monitoring or estimating the energy consumption of various components. While there exist multiple tools for monitoring energy consumption, there is a dearth of tools/frameworks for estimating the consumption or carbon emissions. Current drawbacks of both monitoring and estimation tools include high input data points, intrusive nature, high error margin, etc. We posit that leveraging emerging LLM benchmarks and related data points can help overcome aforementioned challenges while balancing accuracy of the emission estimations. To that extent, we discuss the challenges of current approaches and present our evolving framework, R-ICE, which estimates prompt level inference carbon emissions by leveraging existing state-of-the-art(SOTA) benchmark. This direction provides a more practical and non-intrusive way to enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our promising validation results suggest that benchmark-based modelling holds great potential for inference emission estimation and warrants further exploration from the scientific community."
      },
      {
        "id": "oai:arXiv.org:2506.08729v1",
        "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces",
        "link": "https://arxiv.org/abs/2506.08729",
        "author": "Dieuwertje Alblas, Patryk Rygiel, Julian Suk, Kaj O. Kappe, Marieke Hofman, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08729v1 Announce Type: new \nAbstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the abdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current clinical guidelines recommend elective surgical repair when the maximum AAA diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet these criteria are periodically monitored, with surveillance intervals based on the maximum AAA diameter. However, this diameter does not take into account the complex relation between the 3D AAA shape and its growth, making standardized intervals potentially unfit. Personalized AAA growth predictions could improve monitoring strategies. We propose to use an SE(3)-symmetric transformer model to predict AAA growth directly on the vascular model surface enriched with local, multi-physical features. In contrast to other works which have parameterized the AAA shape, this representation preserves the vascular surface's anatomical structure and geometric fidelity. We train our model using a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24 AAA patients at irregularly sampled intervals. After training, our model predicts AAA growth to the next scan moment with a median diameter error of 1.18 mm. We further demonstrate our model's utility to identify whether a patient will become eligible for elective repair within two years (acc = 0.93). Finally, we evaluate our model's generalization on an external validation set consisting of 25 CTAs from 7 AAA patients from a different hospital. Our results show that local directional AAA growth prediction from the vascular surface is feasible and may contribute to personalized surveillance strategies."
      },
      {
        "id": "oai:arXiv.org:2506.08735v1",
        "title": "InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba",
        "link": "https://arxiv.org/abs/2506.08735",
        "author": "Yuhang Wang, Jun Li, Zhijian Wu, Jianhua Xu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08735v1 Announce Type: new \nAbstract: Within the family of convolutional neural networks, InceptionNeXt has shown excellent competitiveness in image classification and a number of downstream tasks. Built on parallel one-dimensional strip convolutions, however, it suffers from limited ability of capturing spatial dependencies along different dimensions and fails to fully explore spatial modeling in local neighborhood. Besides, inherent locality constraints of convolution operations are detrimental to effective global context modeling. To overcome these limitations, we propose a novel backbone architecture termed InceptionMamba in this study. More specifically, the traditional one-dimensional strip convolutions are replaced by orthogonal band convolutions in our InceptionMamba to achieve cohesive spatial modeling. Furthermore, global contextual modeling can be achieved via a bottleneck Mamba module, facilitating enhanced cross-channel information fusion and enlarged receptive field. Extensive evaluations on classification and various downstream tasks demonstrate that the proposed InceptionMamba achieves state-of-the-art performance with superior parameter and computational efficiency. The source code will be available at https://github.com/Wake1021/InceptionMamba."
      },
      {
        "id": "oai:arXiv.org:2506.08737v1",
        "title": "Exploration by Random Reward Perturbation",
        "link": "https://arxiv.org/abs/2506.08737",
        "author": "Haozhe Ma, Guoji Fu, Zhengding Luo, Jiele Wu, Tze-Yun Leong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08737v1 Announce Type: new \nAbstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy for reinforcement learning (RL). Our theoretical analyses demonstrate that adding zero-mean noise to environmental rewards effectively enhances policy diversity during training, thereby expanding the range of exploration. RRP is fully compatible with the action-perturbation-based exploration strategies, such as $\\epsilon$-greedy, stochastic policies, and entropy regularization, providing additive improvements to exploration effects. It is general, lightweight, and can be integrated into existing RL algorithms with minimal implementation effort and negligible computational overhead. RRP establishes a theoretical connection between reward shaping and noise-driven exploration, highlighting their complementary potential. Experiments show that RRP significantly boosts the performance of Proximal Policy Optimization and Soft Actor-Critic, achieving higher sample efficiency and escaping local optima across various tasks, under both sparse and dense reward scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.08738v1",
        "title": "Societal AI Research Has Become Less Interdisciplinary",
        "link": "https://arxiv.org/abs/2506.08738",
        "author": "Dror Kris Markus, Fabrizio Gilardi, Daria Stetsenko",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08738v1 Announce Type: new \nAbstract: As artificial intelligence (AI) systems become deeply embedded in everyday life, calls to align AI development with ethical and societal values have intensified. Interdisciplinary collaboration is often championed as a key pathway for fostering such engagement. Yet it remains unclear whether interdisciplinary research teams are actually leading this shift in practice. This study analyzes over 100,000 AI-related papers published on ArXiv between 2014 and 2024 to examine how ethical values and societal concerns are integrated into technical AI research. We develop a classifier to identify societal content and measure the extent to which research papers express these considerations. We find a striking shift: while interdisciplinary teams remain more likely to produce societally-oriented research, computer science-only teams now account for a growing share of the field's overall societal output. These teams are increasingly integrating societal concerns into their papers and tackling a wide range of domains - from fairness and safety to healthcare and misinformation. These findings challenge common assumptions about the drivers of societal AI and raise important questions. First, what are the implications for emerging understandings of AI safety and governance if most societally-oriented research is being undertaken by exclusively technical teams? Second, for scholars in the social sciences and humanities: in a technical field increasingly responsive to societal demands, what distinctive perspectives can we still offer to help shape the future of AI?"
      },
      {
        "id": "oai:arXiv.org:2506.08740v1",
        "title": "Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports",
        "link": "https://arxiv.org/abs/2506.08740",
        "author": "Sidhika Balachandar, Shuvom Sadhuka, Bonnie Berger, Emma Pierson, Nikhil Garg",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08740v1 Announce Type: new \nAbstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal forecasting, such as predicting infrastructure problems. In this setting, government officials wish to know in which neighborhoods incidents like potholes or rodent issues occur. The true state of incidents (e.g., street conditions) for each neighborhood is observed via government inspection ratings. However, these ratings are only conducted for a sparse set of neighborhoods and incident types. We also observe the state of incidents via crowdsourced reports, which are more densely observed but may be biased due to heterogeneous reporting behavior. First, for such settings, we propose a multiview, multioutput GNN-based model that uses both unbiased rating data and biased reporting data to predict the true latent state of incidents. Second, we investigate a case study of New York City urban incidents and collect, standardize, and make publicly available a dataset of 9,615,863 crowdsourced reports and 1,041,415 government inspection ratings over 3 years and across 139 types of incidents. Finally, we show on both real and semi-synthetic data that our model can better predict the latent state compared to models that use only reporting data or models that use only rating data, especially when rating data is sparse and reports are predictive of ratings. We also quantify demographic biases in crowdsourced reporting, e.g., higher-income neighborhoods report problems at higher rates. Our analysis showcases a widely applicable approach for latent state prediction using heterogeneous, sparse, and biased data."
      },
      {
        "id": "oai:arXiv.org:2506.08746v1",
        "title": "Towards Secure and Private Language Models for Nuclear Power Plants",
        "link": "https://arxiv.org/abs/2506.08746",
        "author": "Muhammad Anwar, Mishca de Costa, Issam Hammad, Daniel Lau",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08746v1 Announce Type: new \nAbstract: This paper introduces a domain-specific Large Language Model for nuclear applications, built from the publicly accessible Essential CANDU textbook. Drawing on a compact Transformer-based architecture, the model is trained on a single GPU to protect the sensitive data inherent in nuclear operations. Despite relying on a relatively small dataset, it shows encouraging signs of capturing specialized nuclear vocabulary, though the generated text sometimes lacks syntactic coherence. By focusing exclusively on nuclear content, this approach demonstrates the feasibility of in-house LLM solutions that align with rigorous cybersecurity and data confidentiality standards. Early successes in text generation underscore the model's utility for specialized tasks, while also revealing the need for richer corpora, more sophisticated preprocessing, and instruction fine-tuning to enhance domain accuracy. Future directions include extending the dataset to cover diverse nuclear subtopics, refining tokenization to reduce noise, and systematically evaluating the model's readiness for real-world applications in nuclear domain."
      },
      {
        "id": "oai:arXiv.org:2506.08750v1",
        "title": "Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data",
        "link": "https://arxiv.org/abs/2506.08750",
        "author": "Muhammad Anwar, Daniel Lau, Mishca de Costa, Issam Hammad",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08750v1 Announce Type: new \nAbstract: The nuclear industry possesses a wealth of valuable information locked away in unstructured text data. This data, however, is not readily usable for advanced Large Language Model (LLM) applications that require clean, structured question-answer pairs for tasks like model training, fine-tuning, and evaluation. This paper explores how synthetic data generation can bridge this gap, enabling the development of robust LLMs for the nuclear domain. We discuss the challenges of data scarcity and privacy concerns inherent in the nuclear industry and how synthetic data provides a solution by transforming existing text data into usable Q&amp;A pairs. This approach leverages LLMs to analyze text, extract key information, generate relevant questions, and evaluate the quality of the resulting synthetic dataset. By unlocking the potential of LLMs in the nuclear industry, synthetic data can pave the way for improved information retrieval, enhanced knowledge sharing, and more informed decision-making in this critical sector."
      },
      {
        "id": "oai:arXiv.org:2506.08753v1",
        "title": "Factors affecting the in-context learning abilities of LLMs for dialogue state tracking",
        "link": "https://arxiv.org/abs/2506.08753",
        "author": "Pradyoth Hegde, Santosh Kesiraju, Jan \\v{S}vec, \\v{S}imon Sedl\\'a\\v{c}ek, Bolaji Yusuf, Old\\v{r}ich Plchot, Deepak K T, Jan \\v{C}ernock\\'y",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08753v1 Announce Type: new \nAbstract: This study explores the application of in-context learning (ICL) to the dialogue state tracking (DST) problem and investigates the factors that influence its effectiveness. We use a sentence embedding based k-nearest neighbour method to retrieve the suitable demonstrations for ICL. The selected demonstrations, along with the test samples, are structured within a template as input to the LLM. We then conduct a systematic study to analyse the impact of factors related to demonstration selection and prompt context on DST performance. This work is conducted using the MultiWoZ2.4 dataset and focuses primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct models. Our findings provide several useful insights on in-context learning abilities of LLMs for dialogue state tracking."
      },
      {
        "id": "oai:arXiv.org:2506.08757v1",
        "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL",
        "link": "https://arxiv.org/abs/2506.08757",
        "author": "Mishca de Costa, Muhammad Anwar, Dave Mercier, Mark Randall, Issam Hammad",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08757v1 Announce Type: new \nAbstract: Retrieving operational data from nuclear power plants requires exceptional accuracy and transparency due to the criticality of the decisions it supports. Traditionally, natural language to SQL (NL-to-SQL) approaches have been explored for querying such data. While NL-to-SQL promises ease of use, it poses significant risks: end-users cannot easily validate generated SQL queries, and legacy nuclear plant databases -- often complex and poorly structured -- complicate query generation due to decades of incremental modifications. These challenges increase the likelihood of inaccuracies and reduce trust in the approach. In this work, we propose an alternative paradigm: leveraging function-calling large language models (LLMs) to address these challenges. Instead of directly generating SQL queries, we define a set of pre-approved, purpose-specific functions representing common use cases. Queries are processed by invoking these functions, which encapsulate validated SQL logic. This hybrid approach mitigates the risks associated with direct NL-to-SQL translations by ensuring that SQL queries are reviewed and optimized by experts before deployment. While this strategy introduces the upfront cost of developing and maintaining the function library, we demonstrate how NL-to-SQL tools can assist in the initial generation of function code, allowing experts to focus on validation rather than creation. Our study includes a performance comparison between direct NL-to-SQL generation and the proposed function-based approach, highlighting improvements in accuracy and maintainability. This work underscores the importance of balancing user accessibility with operational safety and provides a novel, actionable framework for robust data retrieval in critical systems."
      },
      {
        "id": "oai:arXiv.org:2506.08764v1",
        "title": "On the Stability of the Jacobian Matrix in Deep Neural Networks",
        "link": "https://arxiv.org/abs/2506.08764",
        "author": "Benjamin Dadoun, Soufiane Hayou, Hanan Salam, Mohamed El Amine Seddik, Pierre Youssef",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08764v1 Announce Type: new \nAbstract: Deep neural networks are known to suffer from exploding or vanishing gradients as depth increases, a phenomenon closely tied to the spectral behavior of the input-output Jacobian. Prior work has identified critical initialization schemes that ensure Jacobian stability, but these analyses are typically restricted to fully connected networks with i.i.d. weights. In this work, we go significantly beyond these limitations: we establish a general stability theorem for deep neural networks that accommodates sparsity (such as that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g. induced by training). Our results rely on recent advances in random matrix theory, and provide rigorous guarantees for spectral stability in a much broader class of network models. This extends the theoretical foundation for initialization schemes in modern neural networks with structured and dependent randomness."
      },
      {
        "id": "oai:arXiv.org:2506.08768v1",
        "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP",
        "link": "https://arxiv.org/abs/2506.08768",
        "author": "Ahmed Hasanaath, Aisha Alansari, Ahmed Ashraf, Chafik Salmane, Hamzah Luqman, Saad Ezzini",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08768v1 Announce Type: new \nAbstract: Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at https://anonymous.4open.science/r/AraReasoner41299"
      },
      {
        "id": "oai:arXiv.org:2506.08772v1",
        "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation",
        "link": "https://arxiv.org/abs/2506.08772",
        "author": "Jiayi Song, Kaiyu Li, Xiangyong Cao, Deyu Meng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08772v1 Announce Type: new \nAbstract: Semantic segmentation in remote sensing images is crucial for various applications, yet its performance is heavily reliant on large-scale, high-quality pixel-wise annotations, which are notoriously expensive and time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a promising alternative to mitigate this data dependency. However, existing SSS methods often struggle with the inherent distribution mismatch between limited labeled data and abundant unlabeled data, leading to suboptimal generalization. We propose that Vision Foundation Models (VFMs), pre-trained on vast and diverse datasets, possess robust generalization capabilities that can effectively bridge this distribution gap and provide strong semantic priors for SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework that leverages the powerful semantic knowledge embedded in VFMs to guide semi-supervised learning in remote sensing. Specifically, RS-MTDF employs multiple frozen VFMs (\\textit{e.g.}, DINOv2 and CLIP) as expert teachers, utilizing feature-level distillation to align student features with their robust representations. To further enhance discriminative power, the distilled knowledge is seamlessly fused into the student decoder. Extensive experiments on three challenging remote sensing datasets (ISPRS Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves state-of-the-art performance. Notably, our method outperforms existing approaches across various label ratios on LoveDA and secures the highest IoU in the majority of semantic categories. These results underscore the efficacy of multi-teacher VFM guidance in significantly enhancing both generalization and semantic understanding for remote sensing segmentation. Ablation studies further validate the contribution of each proposed module."
      },
      {
        "id": "oai:arXiv.org:2506.08777v1",
        "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2506.08777",
        "author": "Keyi Liu, Weidong Yang, Ben Fei, Ying He",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08777v1 Announce Type: new \nAbstract: Self-supervised learning (SSL) for point cloud pre-training has become a cornerstone for many 3D vision tasks, enabling effective learning from large-scale unannotated data. At the scene level, existing SSL methods often incorporate volume rendering into the pre-training framework, using RGB-D images as reconstruction signals to facilitate cross-modal learning. This strategy promotes alignment between 2D and 3D modalities and enables the model to benefit from rich visual cues in the RGB-D inputs. However, these approaches are limited by their reliance on implicit scene representations and high memory demands. Furthermore, since their reconstruction objectives are applied only in 2D space, they often fail to capture underlying 3D geometric structures. To address these challenges, we propose Gaussian2Scene, a novel scene-level SSL framework that leverages the efficiency and explicit nature of 3D Gaussian Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the computational burden associated with volume rendering but also supports direct 3D scene reconstruction, thereby enhancing the geometric understanding of the backbone network. Our approach follows a progressive two-stage training strategy. In the first stage, a dual-branch masked autoencoder learns both 2D and 3D scene representations. In the second stage, we initialize training with reconstructed point clouds and further supervise learning using the geometric locations of Gaussian primitives and rendered RGB images. This process reinforces both geometric and cross-modal learning. We demonstrate the effectiveness of Gaussian2Scene across several downstream 3D object detection tasks, showing consistent improvements over existing pre-training methods."
      },
      {
        "id": "oai:arXiv.org:2506.08780v1",
        "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models",
        "link": "https://arxiv.org/abs/2506.08780",
        "author": "Isaac Corley, Lakshay Sharma, Ruth Crasto",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08780v1 Announce Type: new \nAbstract: The Landsat program offers over 50 years of globally consistent Earth imagery. However, the lack of benchmarks for this data constrains progress towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and LC100-L. We establish baseline and standardized evaluation methods across both common architectures and Landsat foundation models pretrained on the SSL4EO-L dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract better representations for downstream tasks in comparison to ImageNet, including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and BigEarthNet-L."
      },
      {
        "id": "oai:arXiv.org:2506.08784v1",
        "title": "HomographyAD: Deep Anomaly Detection Using Self Homography Learning",
        "link": "https://arxiv.org/abs/2506.08784",
        "author": "Jongyub Seok, Chanjin Kang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08784v1 Announce Type: new \nAbstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data, which is important for applying automation technologies of the manufacturing facilities. For MVTec dataset that is a representative AD dataset for industrial environment, many recent works have shown remarkable performances. However, the existing anomaly detection works have a limitation of showing good performance for fully-aligned datasets only, unlike real-world industrial environments. To solve this limitation, we propose HomographyAD, a novel deep anomaly detection methodology based on the ImageNet-pretrained network, which is specially designed for actual industrial dataset. Specifically, we first suggest input foreground alignment using the deep homography estimation method. In addition, we fine-tune the model by self homography learning to learn additional shape information from normal samples. Finally, we conduct anomaly detection based on the measure of how far the feature of test sample is from the distribution of the extracted normal features. By applying our proposed method to various existing AD approaches, we show performance enhancement through extensive experiments."
      },
      {
        "id": "oai:arXiv.org:2506.08793v1",
        "title": "A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory",
        "link": "https://arxiv.org/abs/2506.08793",
        "author": "Zhuoran Zheng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08793v1 Announce Type: new \nAbstract: This paper presents a novel partial differential equation (PDE) framework for single-image dehazing. By integrating the atmospheric scattering model with nonlocal regularization and dark channel prior, we propose the improved PDE: \\[ -\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A) \\] where $D(\\nabla u) = (|\\nabla u| + \\epsilon)^{-1}$ is the edge-preserving diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and $\\lambda(t)$ is the adaptive regularization parameter based on transmission map $t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\\Omega)$ using Lax-Milgram theorem, and implement an efficient fixed-point iteration scheme accelerated by PyTorch GPU computation. The experimental results demonstrate that this method is a promising deghazing solution that can be generalized to the deep model paradigm."
      },
      {
        "id": "oai:arXiv.org:2506.08796v1",
        "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling",
        "link": "https://arxiv.org/abs/2506.08796",
        "author": "Zhiyuan Ma, Ruixun Liu, Sixian Liu, Jianjun Li, Bowen Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08796v1 Announce Type: new \nAbstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art among flow-based diffusion models due to its high efficiency advantage in straight path sampling, especially with the amazing images generated by a series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line connection between the noisy and natural data distributions is intuitive, fast, and easy to optimize, it still inevitably leads to: 1) Diversity concerns, which arise since straight-line paths only cover a fairly restricted sampling space. 2) Multi-scale noise modeling concerns, since the straight line flow only needs to optimize the constant velocity field $\\bm v$ between the two distributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present Discretized-RF, a new family of rectified flow (also called momentum flow models since they refer to the previous velocity component and the random velocity component in each diffusion step), which discretizes the straight path into a series of variable velocity field sub-paths (namely ``momentum fields'') to expand the search space, especially when close to the distribution $p_\\text{noise}$. Different from the previous case where noise is directly superimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the sub-path to change its direction in order to improve the diversity and multi-scale noise modeling abilities. Experimental results on several representative datasets demonstrate that learning momentum flow matching by sampling random velocity fields will produce trajectories that are both diverse and efficient, and can consistently generate high-quality and diverse results. Code is available at https://github.com/liuruixun/momentum-fm."
      },
      {
        "id": "oai:arXiv.org:2506.08797v1",
        "title": "HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation",
        "link": "https://arxiv.org/abs/2506.08797",
        "author": "Ziyao Huang, Zixiang Zhou, Juan Cao, Yifeng Ma, Yi Chen, Zejing Rao, Zhiyong Xu, Hongmei Wang, Qin Lin, Yuan Zhou, Qinglin Lu, Fan Tang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08797v1 Announce Type: new \nAbstract: To address key limitations in human-object interaction (HOI) video generation -- specifically the reliance on curated motion data, limited generalization to novel objects/scenarios, and restricted accessibility -- we introduce HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework. HunyuanVideo-HOMA enhances controllability and reduces dependency on precise inputs through sparse, decoupled motion guidance. It encodes appearance and motion signals into the dual input space of a multimodal diffusion transformer (MMDiT), fusing them within a shared context space to synthesize temporally consistent and physically plausible interactions. To optimize training, we integrate a parameter-space HOI adapter initialized from pretrained MMDiT weights, preserving prior knowledge while enabling efficient adaptation, and a facial cross-attention adapter for anatomically accurate audio-driven lip synchronization. Extensive experiments confirm state-of-the-art performance in interaction naturalness and generalization under weak supervision. Finally, HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and interactive object manipulation, supported by a user-friendly demo interface. The project page is at https://anonymous.4open.science/w/homa-page-0FBE/."
      },
      {
        "id": "oai:arXiv.org:2506.08809v1",
        "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference",
        "link": "https://arxiv.org/abs/2506.08809",
        "author": "Jiaze E, Srutarshi Banerjee, Tekin Bicer, Guannan Wang, Yanfu Zhang, Bin Ren",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08809v1 Announce Type: new \nAbstract: High-resolution sinogram inpainting is essential for computed tomography reconstruction, as missing high-frequency projections can lead to visible artifacts and diagnostic errors. Diffusion models are well-suited for this task due to their robustness and detail-preserving capabilities, but their application to high-resolution inputs is limited by excessive memory and computational demands. To address this limitation, we propose HiSin, a novel diffusion based framework for efficient sinogram inpainting via resolution-guided progressive inference. It progressively extracts global structure at low resolution and defers high-resolution inference to small patches, enabling memory-efficient inpainting. It further incorporates frequency-aware patch skipping and structure-adaptive step allocation to reduce redundant computation. Experimental results show that HiSin reduces peak memory usage by up to 31.25% and inference time by up to 18.15%, and maintains inpainting accuracy across datasets, resolutions, and mask conditions."
      },
      {
        "id": "oai:arXiv.org:2506.08817v1",
        "title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought",
        "link": "https://arxiv.org/abs/2506.08817",
        "author": "Shuyi Zhang, Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Pengwei Wang, Zhongyuan Wang, Hongxuan Ma, Shanghang Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08817v1 Announce Type: new \nAbstract: Video content comprehension is essential for various applications, ranging from video analysis to interactive systems. Despite advancements in large-scale vision-language models (VLMs), these models often struggle to capture the nuanced, spatiotemporal details essential for thorough video analysis. To address this gap, we introduce Video-CoT, a groundbreaking dataset designed to enhance spatiotemporal understanding using Chain-of-Thought (CoT) methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal question-answer pairs and 23,000 high-quality CoT-annotated samples, providing a solid foundation for evaluating spatiotemporal understanding in video comprehension. Additionally, we provide a comprehensive benchmark for assessing these tasks, with each task featuring 750 images and tailored evaluation metrics. Our extensive experiments reveal that current VLMs face significant challenges in achieving satisfactory performance, high-lighting the difficulties of effective spatiotemporal understanding. Overall, the Video-CoT dataset and benchmark open new avenues for research in multimedia understanding and support future innovations in intelligent systems requiring advanced video analysis capabilities. By making these resources publicly available, we aim to encourage further exploration in this critical area. Project website:https://video-cot.github.io/ ."
      },
      {
        "id": "oai:arXiv.org:2506.08827v1",
        "title": "The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation",
        "link": "https://arxiv.org/abs/2506.08827",
        "author": "Francisco Vargas, Alejandro Gonz\\'alez Coene, Gaston Escalante, Exequiel Lob\\'on, Manuel Pulido",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08827v1 Announce Type: new \nAbstract: The extraction of information about traffic accidents from legal documents is crucial for quantifying insurance company costs. Extracting entities such as percentages of physical and/or psychological disability and the involved compensation amounts is a challenging process, even for experts, due to the subtle arguments and reasoning in the court decision. A two-step procedure is proposed: first, segmenting the document identifying the most relevant segments, and then extracting the entities. For text segmentation, two methodologies are compared: a classic method based on regular expressions and a second approach that divides the document into blocks of n-tokens, which are then vectorized using multilingual models for semantic searches (text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models (LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to the selected segments for entity extraction. For the LLaMA models, fine-tuning is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a significant number of hallucinations in extractions which are an important contention point for named entity extraction. This work shows that these hallucinations are substantially reduced after finetuning the model. The performance of the methodology based on segment vectorization and subsequent use of LLMs significantly surpasses the classic method which achieves an accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning achieves the highest accuracy 79.4%, surpassing its base version 61.7%. Notably, the base LLaMA-3 8B model already performs comparably to the finetuned LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%."
      },
      {
        "id": "oai:arXiv.org:2506.08835v1",
        "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics",
        "link": "https://arxiv.org/abs/2506.08835",
        "author": "Shravan Nayak, Mehar Bhatia, Xiaofeng Zhang, Verena Rieser, Lisa Anne Hendricks, Sjoerd van Steenkiste, Yash Goyal, Karolina Sta\\'nczak, Aishwarya Agrawal",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08835v1 Announce Type: new \nAbstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual content generation raises concerns about their ability to accurately represent diverse cultural contexts. In this work, we present the first study to systematically quantify the alignment of T2I models and evaluation metrics with respect to both explicit as well as implicit cultural expectations. To this end, we introduce CulturalFrames, a novel benchmark designed for rigorous human evaluation of cultural representation in visual generations. Spanning 10 countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts, 3637 corresponding images generated by 4 state-of-the-art T2I models, and over 10k detailed human annotations. We find that T2I models not only fail to meet the more challenging implicit expectations but also the less challenging explicit expectations. Across models and countries, cultural expectations are missed an average of 44% of the time. Among these failures, explicit expectations are missed at a surprisingly high average rate of 68%, while implicit expectation failures are also significant, averaging 49%. Furthermore, we demonstrate that existing T2I evaluation metrics correlate poorly with human judgments of cultural alignment, irrespective of their internal reasoning. Collectively, our findings expose critical gaps, providing actionable directions for developing more culturally informed T2I models and evaluation methodologies."
      },
      {
        "id": "oai:arXiv.org:2506.08836v1",
        "title": "Advancing STT for Low-Resource Real-World Speech",
        "link": "https://arxiv.org/abs/2506.08836",
        "author": "Flavio D'Intino, Hans-Peter Hutter",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08836v1 Announce Type: new \nAbstract: Swiss German is a low-resource language represented by diverse dialects that differ significantly from Standard German and from each other, lacking a standardized written form. As a result, transcribing Swiss German involves translating into Standard German. Existing datasets have been collected in controlled environments, yielding effective speech-to-text (STT) models, but these models struggle with spontaneous conversational speech.\n  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour annotated speech corpus featuring real-world long-audio recordings from 39 Swiss German radio and TV stations. It captures spontaneous speech across all major Swiss dialects recorded in various realistic environments and overcomes the limitation of prior sentence-level corpora.\n  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, achieving notable enhancements over previous zero-shot performance metrics. Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for developing effective and robust STT systems for Swiss German and other low-resource languages in real-world contexts."
      },
      {
        "id": "oai:arXiv.org:2506.08837v1",
        "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
        "link": "https://arxiv.org/abs/2506.08837",
        "author": "Luca Beurer-Kellner, Beat Buesser Ana-Maria Cre\\c{t}u, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tram\\`er, V\\'aclav Volhejn",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08837v1 Announce Type: new \nAbstract: As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their security has become a critical challenge. Among the most pressing threats are prompt injection attacks, which exploit the agent's resilience on natural language inputs -- an especially dangerous threat when agents are granted tool access or handle sensitive information. In this work, we propose a set of principled design patterns for building AI agents with provable resistance to prompt injection. We systematically analyze these patterns, discuss their trade-offs in terms of utility and security, and illustrate their real-world applicability through a series of case studies."
      },
      {
        "id": "oai:arXiv.org:2506.08844v1",
        "title": "IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)",
        "link": "https://arxiv.org/abs/2506.08844",
        "author": "Siyi Sun, David Antony Selby, Yunchuan Huang, Sebastian Vollmer, Seth Flaxman, Anisoara Calinescu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08844v1 Announce Type: new \nAbstract: Missing data imputation in tabular datasets remains a pivotal challenge in data science and machine learning, particularly within socioeconomic research. However, real-world socioeconomic datasets are typically subject to strict data protection protocols, which often prohibit public sharing, even for synthetic derivatives. This severely limits the reproducibility and accessibility of benchmark studies in such settings. Further, there are very few publicly available synthetic datasets. Thus, there is limited availability of benchmarks for systematic evaluation of imputation methods on socioeconomic datasets, whether real or synthetic. In this study, we utilize the World Bank's publicly available synthetic dataset, Synthetic Data for an Imaginary Country, which closely mimics a real World Bank household survey while being fully public, enabling broad access for methodological research. With this as a starting point, we derived the IMAGIC-500 dataset: we select a subset of 500k individuals across approximately 100k households with 19 socioeconomic features, designed to reflect the hierarchical structure of real-world household surveys. This paper introduces a comprehensive missing data imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR, MNAR) and missingness ratios (10\\%, 20\\%, 30\\%, 40\\%, 50\\%). Our evaluation considers the imputation accuracy for continuous and categorical variables, computational efficiency, and impact on downstream predictive tasks, such as estimating educational attainment at the individual level. The results highlight the strengths and weaknesses of statistical, traditional machine learning, and deep learning imputation techniques, including recent diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate the development of robust imputation algorithms and foster reproducible social science research."
      },
      {
        "id": "oai:arXiv.org:2506.08849v1",
        "title": "Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis",
        "link": "https://arxiv.org/abs/2506.08849",
        "author": "Jingguo Qu, Xinyang Han, Tonghuan Xiao, Jia Ai, Juan Wu, Tong Zhao, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying{\\i}nst",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08849v1 Announce Type: new \nAbstract: Medical ultrasonography is an essential imaging technique for examining superficial organs and tissues, including lymph nodes, breast, and thyroid. It employs high-frequency ultrasound waves to generate detailed images of the internal structures of the human body. However, manually contouring regions of interest in these images is a labor-intensive task that demands expertise and often results in inconsistent interpretations among individuals. Vision-language foundation models, which have excelled in various computer vision applications, present new opportunities for enhancing ultrasound image analysis. Yet, their performance is hindered by the significant differences between natural and medical imaging domains. This research seeks to overcome these challenges by developing domain adaptation methods for vision-language foundation models. In this study, we explore the fine-tuning pipeline for vision-language foundation models by utilizing large language model as text refiner with special-designed adaptation strategies and task-driven heads. Our approach has been extensively evaluated on six ultrasound datasets and two tasks: segmentation and classification. The experimental results show that our method can effectively improve the performance of vision-language foundation models for ultrasound image analysis, and outperform the existing state-of-the-art vision-language and pure foundation models. The source code of this study is available at \\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}."
      },
      {
        "id": "oai:arXiv.org:2506.08850v1",
        "title": "Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing",
        "link": "https://arxiv.org/abs/2506.08850",
        "author": "Amin Avan, Akramul Azim, Qusay Mahmoud",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08850v1 Announce Type: new \nAbstract: Soft real-time applications are becoming increasingly complex, posing significant challenges for scheduling offloaded tasks in edge computing environments while meeting task timing constraints. Moreover, the exponential growth of the search space, presence of multiple objectives and parameters, and highly dynamic nature of edge computing environments further exacerbate the complexity of task scheduling. As a result, schedulers based on heuristic and metaheuristic algorithms frequently encounter difficulties in generating optimal or near-optimal task schedules due to their constrained ability to adapt to the dynamic conditions and complex environmental characteristics of edge computing. Accordingly, reinforcement learning algorithms have been incorporated into schedulers to address the complexity and dynamic conditions inherent in task scheduling in edge computing. However, a significant limitation of reinforcement learning algorithms is the prolonged learning time required to adapt to new environments and to address medium- and large-scale problems. This challenge arises from the extensive global action space and frequent random exploration of irrelevant actions. Therefore, this study proposes Agile Reinforcement learning (aRL), in which the RL-agent performs informed exploration and executes only relevant actions. Consequently, the predictability of the RL-agent is enhanced, leading to rapid adaptation and convergence, which positions aRL as a suitable candidate for scheduling the tasks of soft real-time applications in edge computing. The experiments demonstrate that the combination of informed exploration and action-masking methods enables aRL to achieve a higher hit-ratio and converge faster than the baseline approaches."
      },
      {
        "id": "oai:arXiv.org:2506.08854v1",
        "title": "Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning",
        "link": "https://arxiv.org/abs/2506.08854",
        "author": "Junzhuo Liu, Markus Eckstein, Zhixiang Wang, Friedrich Feuerhake, Dorit Merhof",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08854v1 Announce Type: new \nAbstract: Spatial transcriptomics is a technology that captures gene expression levels at different spatial locations, widely used in tumor microenvironment analysis and molecular profiling of histopathology, providing valuable insights into resolving gene expression and clinical diagnosis of cancer. Due to the high cost of data acquisition, large-scale spatial transcriptomics data remain challenging to obtain. In this study, we develop a contrastive learning-based deep learning method to predict spatially resolved gene expression from whole-slide images. Evaluation across six different disease datasets demonstrates that, compared to existing studies, our method improves Pearson Correlation Coefficient (PCC) in the prediction of highly expressed genes, highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26% respectively. Further analysis indicates that our method preserves gene-gene correlations and applies to datasets with limited samples. Additionally, our method exhibits potential in cancer tissue localization based on biomarker expression."
      },
      {
        "id": "oai:arXiv.org:2506.08862v1",
        "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams",
        "link": "https://arxiv.org/abs/2506.08862",
        "author": "Zike Wu, Qi Yan, Xuanyu Yi, Lele Wang, Renjie Liao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08862v1 Announce Type: new \nAbstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat."
      },
      {
        "id": "oai:arXiv.org:2506.08871v1",
        "title": "Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery",
        "link": "https://arxiv.org/abs/2506.08871",
        "author": "Victor M. Tenorio, Madeline Navarro, Samuel Rey, Santiago Segarra, Antonio G. Marques",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08871v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where connected nodes may have dissimilar labels, as they typically assume homophily and rely on local message passing. To address this, we propose creating alternative graph structures by linking nodes with similar structural attributes (e.g., role-based or global), thereby fostering higher label homophily on these new graphs. We theoretically prove that GNN performance can be improved by utilizing graphs with fewer false positive edges (connections between nodes of different classes) and that considering multiple graph views increases the likelihood of finding such beneficial structures. Building on these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture that processes the original graph alongside the newly created structural graphs, adaptively learning to weigh their contributions. Extensive experiments on various benchmark datasets, particularly those with heterophilic characteristics, demonstrate that our SG-GNN achieves state-of-the-art or highly competitive performance, highlighting the efficacy of exploiting structural information to guide GNNs."
      },
      {
        "id": "oai:arXiv.org:2506.08882v1",
        "title": "Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data",
        "link": "https://arxiv.org/abs/2506.08882",
        "author": "Dimitrios Amaxilatis, Themistoklis Sarantakos, Ioannis Chatzigiannakis, Georgios Mylonas",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08882v1 Announce Type: new \nAbstract: In this work, we explore the application of recent data imputation techniques to enhance monitoring and management of water distribution networks using smart water meters, based on data derived from a real-world IoT water grid monitoring deployment. Despite the detailed data produced by such meters, data gaps due to technical issues can significantly impact operational decisions and efficiency. Our results, by comparing various imputation methods, such as k-Nearest Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate that effective data imputation can substantially enhance the quality of the insights derived from water consumption data as we study their effect on accuracy and reliability of water metering data to provide solutions in applications like leak detection and predictive maintenance scheduling."
      },
      {
        "id": "oai:arXiv.org:2506.08884v1",
        "title": "InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis",
        "link": "https://arxiv.org/abs/2506.08884",
        "author": "Shiqin Tang, Shujian Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08884v1 Announce Type: new \nAbstract: Extracting meaningful latent representations from high-dimensional sequential data is a crucial challenge in machine learning, with applications spanning natural science and engineering. We introduce InfoDPCCA, a dynamic probabilistic Canonical Correlation Analysis (CCA) framework designed to model two interdependent sequences of observations. InfoDPCCA leverages a novel information-theoretic objective to extract a shared latent representation that captures the mutual structure between the data streams and balances representation compression and predictive sufficiency while also learning separate latent components that encode information specific to each sequence. Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly enforces the shared latent space to encode only the mutual information between the sequences, improving interpretability and robustness. We further introduce a two-step training scheme to bridge the gap between information-theoretic representation learning and generative modeling, along with a residual connection mechanism to enhance training stability. Through experiments on synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool for representation learning. Code of InfoDPCCA is available at https://github.com/marcusstang/InfoDPCCA."
      },
      {
        "id": "oai:arXiv.org:2506.08885v1",
        "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)",
        "link": "https://arxiv.org/abs/2506.08885",
        "author": "Danush Khanna, Krishna Kumar, Basab Ghosh, Vinija Jain, Vasu Sharma, Aman Chadha, Amitava Das",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08885v1 Announce Type: new \nAbstract: Adversarial threats against LLMs are escalating faster than current defenses can adapt. We expose a critical geometric blind spot in alignment: adversarial prompts exploit latent camouflage, embedding perilously close to the safe representation manifold while encoding unsafe intent thereby evading surface level defenses like Direct Preference Optimization (DPO), which remain blind to the latent geometry. We introduce ALKALI, the first rigorously curated adversarial benchmark and the most comprehensive to date spanning 9,000 prompts across three macro categories, six subtypes, and fifteen attack families. Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates (ASRs) across both open and closed source models, exposing an underlying vulnerability we term latent camouflage, a structural blind spot where adversarial completions mimic the latent geometry of safe ones. To mitigate this vulnerability, we introduce GRACE - Geometric Representation Aware Contrastive Enhancement, an alignment framework coupling preference learning with latent space regularization. GRACE enforces two constraints: latent separation between safe and adversarial completions, and adversarial cohesion among unsafe and jailbreak behaviors. These operate over layerwise pooled embeddings guided by a learned attention profile, reshaping internal geometry without modifying the base model, and achieve up to 39% ASR reduction. Moreover, we introduce AVQI, a geometry aware metric that quantifies latent alignment failure via cluster separation and compactness. AVQI reveals when unsafe completions mimic the geometry of safe ones, offering a principled lens into how models internally encode safety. We make the code publicly available at https://anonymous.4open.science/r/alkali-B416/README.md."
      },
      {
        "id": "oai:arXiv.org:2506.08887v1",
        "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval",
        "link": "https://arxiv.org/abs/2506.08887",
        "author": "Leqi Shen, Guoqiang Gong, Tianxiang Hao, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Jungong Han, Guiguang Ding",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08887v1 Announce Type: new \nAbstract: The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA."
      },
      {
        "id": "oai:arXiv.org:2506.08889v1",
        "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
        "link": "https://arxiv.org/abs/2506.08889",
        "author": "Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08889v1 Announce Type: new \nAbstract: We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention."
      },
      {
        "id": "oai:arXiv.org:2506.08894v1",
        "title": "Product of Experts for Visual Generation",
        "link": "https://arxiv.org/abs/2506.08894",
        "author": "Yunzhi Zhang, Carson Murtuza-Lanier, Zizhang Li, Yilun Du, Jiajun Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08894v1 Announce Type: new \nAbstract: Modern neural models capture rich priors and have complementary knowledge over shared data domains, e.g., images and videos. Integrating diverse knowledge from multiple sources -- including visual generative models, visual language models, and sources with human-crafted knowledge such as graphics engines and physics simulators -- remains under-explored. We propose a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS). Our framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and additionally providing flexible user interfaces for specifying visual generation goals."
      },
      {
        "id": "oai:arXiv.org:2506.08896v1",
        "title": "WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos",
        "link": "https://arxiv.org/abs/2506.08896",
        "author": "Negin Ghamsarian, Raphael Sznitman, Klaus Schoeffmann, Jens Kowal",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08896v1 Announce Type: new \nAbstract: To meet the growing demand for systematic surgical training, wetlab environments have become indispensable platforms for hands-on practice in ophthalmology. Yet, traditional wetlab training depends heavily on manual performance evaluations, which are labor-intensive, time-consuming, and often subject to variability. Recent advances in computer vision offer promising avenues for automated skill assessment, enhancing both the efficiency and objectivity of surgical education. Despite notable progress in ophthalmic surgical datasets, existing resources predominantly focus on real surgeries or isolated tasks, falling short of supporting comprehensive skill evaluation in controlled wetlab settings. To address these limitations, we introduce WetCat, the first dataset of wetlab cataract surgery videos specifically curated for automated skill assessment. WetCat comprises high-resolution recordings of surgeries performed by trainees on artificial eyes, featuring comprehensive phase annotations and semantic segmentations of key anatomical structures. These annotations are meticulously designed to facilitate skill assessment during the critical capsulorhexis and phacoemulsification phases, adhering to standardized surgical skill assessment frameworks. By focusing on these essential phases, WetCat enables the development of interpretable, AI-driven evaluation tools aligned with established clinical metrics. This dataset lays a strong foundation for advancing objective, scalable surgical education and sets a new benchmark for automated workflow analysis and skill assessment in ophthalmology training. The dataset and annotations are publicly available in Synapse https://www.synapse.org/Synapse:syn66401174/files."
      },
      {
        "id": "oai:arXiv.org:2506.08897v1",
        "title": "PlantBert: An Open Source Language Model for Plant Science",
        "link": "https://arxiv.org/abs/2506.08897",
        "author": "Hiba Khey, Amine Lakhder, Salma Rouichi, Imane El Ghabi, Kamal Hejjaoui, Younes En-nahli, Fahd Kalloubi, Moez Amri",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08897v1 Announce Type: new \nAbstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science."
      },
      {
        "id": "oai:arXiv.org:2506.08899v1",
        "title": "From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis",
        "link": "https://arxiv.org/abs/2506.08899",
        "author": "Elias Horner, Cristinel Mateis, Guido Governatori, Agata Ciabattoni",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08899v1 Announce Type: new \nAbstract: We present a novel approach to the automated semantic analysis of legal texts using large language models (LLMs), targeting their transformation into formal representations in Defeasible Deontic Logic (DDL). We propose a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. Our methodology is evaluated across various LLM configurations, including prompt engineering strategies, fine-tuned models, and multi-stage pipelines, focusing on legal norms from the Australian Telecommunications Consumer Protections Code. Empirical results demonstrate promising alignment between machine-generated and expert-crafted formalizations, showing that LLMs - particularly when prompted effectively - can significantly contribute to scalable legal informatics."
      },
      {
        "id": "oai:arXiv.org:2506.08900v1",
        "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis",
        "link": "https://arxiv.org/abs/2506.08900",
        "author": "Jos\\'e Morano, Botond Fazekas, Emese S\\\"ukei, Ronald Fecso, Taha Emre, Markus Gumpinger, Georg Faustmann, Marzieh Oghbaie, Ursula Schmidt-Erfurth, Hrvoje Bogunovi\\'c",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08900v1 Announce Type: new \nAbstract: Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT/SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: https://github.com/j-morano/MIRAGE."
      },
      {
        "id": "oai:arXiv.org:2506.08902v1",
        "title": "Intention-Conditioned Flow Occupancy Models",
        "link": "https://arxiv.org/abs/2506.08902",
        "author": "Chongyi Zheng, Seohong Park, Sergey Levine, Benjamin Eysenbach",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08902v1 Announce Type: new \nAbstract: Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on $36$ state-based and $4$ image-based benchmark tasks demonstrate that the proposed method achieves $1.8 \\times$ median improvement in returns and increases success rates by $36\\%$. Website: https://chongyi-zheng.github.io/infom Code: https://github.com/chongyi-zheng/infom"
      },
      {
        "id": "oai:arXiv.org:2506.08906v1",
        "title": "Hyperbolic Dual Feature Augmentation for Open-Environment",
        "link": "https://arxiv.org/abs/2506.08906",
        "author": "Peilin Yu, Yuwei Wu, Zhi Gao, Xiaomeng Fan, Shuo Yang, Yunde Jia",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08906v1 Announce Type: new \nAbstract: Feature augmentation generates novel samples in the feature space, providing an effective way to enhance the generalization ability of learning algorithms with hyperbolic geometry. Most hyperbolic feature augmentation is confined to closed-environment, assuming the number of classes is fixed (\\emph{i.e.}, seen classes) and generating features only for these classes. In this paper, we propose a hyperbolic dual feature augmentation method for open-environment, which augments features for both seen and unseen classes in the hyperbolic space. To obtain a more precise approximation of the real data distribution for efficient training, (1) we adopt a neural ordinary differential equation module, enhanced by meta-learning, estimating the feature distributions of both seen and unseen classes; (2) we then introduce a regularizer to preserve the latent hierarchical structures of data in the hyperbolic space; (3) we also derive an upper bound for the hyperbolic dual augmentation loss, allowing us to train a hyperbolic model using infinite augmentations for seen and unseen classes. Extensive experiments on five open-environment tasks: class-incremental learning, few-shot open-set recognition, few-shot learning, zero-shot learning, and general image classification, demonstrate that our method effectively enhances the performance of hyperbolic algorithms in open-environment."
      },
      {
        "id": "oai:arXiv.org:2506.08907v1",
        "title": "Dialect Normalization using Large Language Models and Morphological Rules",
        "link": "https://arxiv.org/abs/2506.08907",
        "author": "Antonios Dimakis (Archimedes, Athena Research Center, Greece, Department of Informatics and Telecommunications, NKUA), John Pavlopoulos (Archimedes, Athena Research Center, Greece, Department of Informatics, Athens University of Economics and Business, Greece), Antonios Anastasopoulos (Archimedes, Athena Research Center, Greece, Department of Computer Science, George Mason University)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08907v1 Announce Type: new \nAbstract: Natural language understanding systems struggle with low-resource languages, including many dialects of high-resource ones. Dialect-to-standard normalization attempts to tackle this issue by transforming dialectal text so that it can be used by standard-language tools downstream. In this study, we tackle this task by introducing a new normalization method that combines rule-based linguistically informed transformations and large language models (LLMs) with targeted few-shot prompting, without requiring any parallel data. We implement our method for Greek dialects and apply it on a dataset of regional proverbs, evaluating the outputs using human annotators. We then use this dataset to conduct downstream experiments, finding that previous results regarding these proverbs relied solely on superficial linguistic information, including orthographic artifacts, while new observations can still be made through the remaining semantics."
      },
      {
        "id": "oai:arXiv.org:2506.08908v1",
        "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping",
        "link": "https://arxiv.org/abs/2506.08908",
        "author": "Jiajun Li (University of Electronic Science and Technology of China, Shanghai Jiaotong University), Yue Ma (The Hong Kong University of Science and Technology), Xinyu Zhang (University of Electronic Science and Technology of China), Qingyan Wei (Central South University), Songhua Liu (National University of Singapore, Shanghai Jiaotong University), Linfeng Zhang (Shanghai Jiaotong University)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08908v1 Announce Type: new \nAbstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that high-frequency components, or later steps, in the generation process contribute disproportionately to inference latency. However, the underlying computational redundancy involved in these steps has yet to be thoroughly investigated. In this paper, we conduct an in-depth analysis of the VAR inference process and identify two primary sources of inefficiency: step redundancy and unconditional branch redundancy. To address step redundancy, we propose an automatic step-skipping strategy that selectively omits unnecessary generation steps to improve efficiency. For unconditional branch redundancy, we observe that the information gap between the conditional and unconditional branches is minimal. Leveraging this insight, we introduce unconditional branch replacement, a technique that bypasses the unconditional branch to reduce computational cost. Notably, we observe that the effectiveness of acceleration strategies varies significantly across different samples. Motivated by this, we propose SkipVAR, a sample-adaptive framework that leverages frequency information to dynamically select the most suitable acceleration strategy for each instance. To evaluate the role of high-frequency information, we introduce high-variation benchmark datasets that test model sensitivity to fine details. Extensive experiments show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall acceleration and 2.62x speedup on the GenEval benchmark, maintaining model quality. These results confirm the effectiveness of frequency-aware, training-free adaptive acceleration for scalable autoregressive image generation. Our code is available at https://github.com/fakerone-li/SkipVAR and has been publicly released."
      },
      {
        "id": "oai:arXiv.org:2506.08915v1",
        "title": "Inherently Faithful Attention Maps for Vision Transformers",
        "link": "https://arxiv.org/abs/2506.08915",
        "author": "Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08915v1 Announce Type: new \nAbstract: We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds."
      },
      {
        "id": "oai:arXiv.org:2506.08916v1",
        "title": "Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)",
        "link": "https://arxiv.org/abs/2506.08916",
        "author": "Maria-Veronica Ciocanel, John T. Nardini, Kevin B. Flores, Erica M. Rutter, Suzanne S. Sindi, Alexandria Volkening",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08916v1 Announce Type: new \nAbstract: Agent-based modeling (ABM) is a powerful tool for understanding self-organizing biological systems, but it is computationally intensive and often not analytically tractable. Equation learning (EQL) methods can derive continuum models from ABM data, but they typically require extensive simulations for each parameter set, raising concerns about generalizability. In this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns individual models for each parameter set and connects them via interpolation, and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library across parameters. We demonstrate these methods using a birth--death mean-field model and an on-lattice agent-based model of birth, death, and migration with spatial structure. Our results show that both methods significantly reduce the relative error in recovering parameters from agent-based simulations, with OAT ME-EQL offering better generalizability across parameter space. Our findings highlight the potential of equation learning from multiple experiments to enhance the generalizability and interpretability of learned models for complex biological systems."
      },
      {
        "id": "oai:arXiv.org:2506.08920v1",
        "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs",
        "link": "https://arxiv.org/abs/2506.08920",
        "author": "Zeyu Leo Liu, Greg Durrett, Eunsol Choi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08920v1 Announce Type: new \nAbstract: Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations."
      },
      {
        "id": "oai:arXiv.org:2506.08927v1",
        "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions",
        "link": "https://arxiv.org/abs/2506.08927",
        "author": "David Acuna, Ximing Lu, Jaehun Jung, Hyunwoo Kim, Amlan Kar, Sanja Fidler, Yejin Choi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08927v1 Announce Type: new \nAbstract: Recent research in vision-language models (VLMs) has centered around the possibility of equipping them with implicit long-form chain-of-thought reasoning -- akin to the success observed in language models -- via distillation and reinforcement learning. But what about the non-reasoning models already trained and deployed across the internet? Should we simply abandon them, or is there hope for a search mechanism that can elicit hidden knowledge and induce long reasoning traces -- without any additional training or supervision? In this paper, we explore this possibility using a Monte Carlo Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer pairs into the model's output stream. We show that framing reasoning as a search process -- where subquestions act as latent decisions within a broader inference trajectory -- helps the model \"connect the dots\" between fragmented knowledge and produce extended reasoning traces in non-reasoning models. We evaluate our method across three benchmarks and observe consistent improvements. Notably, our approach yields a 2% overall improvement on MMMU-PRO, including a significant 9% gain in Liberal Arts."
      },
      {
        "id": "oai:arXiv.org:2506.08928v1",
        "title": "Local MDI+: Local Feature Importances for Tree-Based Models",
        "link": "https://arxiv.org/abs/2506.08928",
        "author": "Zhongyuan Liang, Zachary T. Rewolinski, Abhineet Agarwal, Tiffany M. Tang, Bin Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08928v1 Announce Type: new \nAbstract: Tree-based ensembles such as random forests remain the go-to for tabular data over deep learning models due to their prediction performance and computational efficiency. These advantages have led to their widespread deployment in high-stakes domains, where interpretability is essential for ensuring trustworthy predictions. This has motivated the development of popular local (i.e. sample-specific) feature importance (LFI) methods such as LIME and TreeSHAP. However, these approaches rely on approximations that ignore the model's internal structure and instead depend on potentially unstable perturbations. These issues are addressed in the global setting by MDI+, a feature importance method which exploits an equivalence between decision trees and linear models on a transformed node basis. However, the global MDI+ scores are not able to explain predictions when faced with heterogeneous individual characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel extension of the MDI+ framework to the sample specific setting. LMDI+ outperforms existing baselines LIME and TreeSHAP in identifying instance-specific signal features, averaging a 10% improvement in downstream task performance across twelve real-world benchmark datasets. It further demonstrates greater stability by consistently producing similar instance-level feature importance rankings across multiple random forest fits. Finally, LMDI+ enables local interpretability use cases, including the identification of closer counterfactuals and the discovery of homogeneous subgroups."
      },
      {
        "id": "oai:arXiv.org:2506.08933v1",
        "title": "What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities",
        "link": "https://arxiv.org/abs/2506.08933",
        "author": "Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08933v1 Announce Type: new \nAbstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91\\% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at https://omni-bench.github.io/."
      },
      {
        "id": "oai:arXiv.org:2506.08935v1",
        "title": "Can A Gamer Train A Mathematical Reasoning Model?",
        "link": "https://arxiv.org/abs/2506.08935",
        "author": "Andrew Shin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08935v1 Announce Type: new \nAbstract: While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath."
      },
      {
        "id": "oai:arXiv.org:2506.08936v1",
        "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models",
        "link": "https://arxiv.org/abs/2506.08936",
        "author": "Amina Mollaysa, Artem Moskale, Pushpak Pati, Tommaso Mansi, Mangal Prakash, Rui Liao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08936v1 Announce Type: new \nAbstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA, mRNA, and protein language models into unified molecular representations. Motivated by the central dogma of molecular biology (information flow from gene to transcript to protein), we align per-modality embeddings at the biologically meaningful codon level (three nucleotides encoding one amino acid) to ensure direct cross-modal correspondence. BioLangFusion studies three standard fusion techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized attention pooling inspired by multiple-instance learning, and (iii) cross-modal multi-head attention -- each technique providing a different inductive bias for combining modality-specific signals. These methods require no additional pre-training or modification of the base models, allowing straightforward integration with existing sequence-based foundation models. Across five molecular property prediction tasks, BioLangFusion outperforms strong unimodal baselines, showing that even simple fusion of pre-trained models can capture complementary multi-omic information with minimal overhead."
      },
      {
        "id": "oai:arXiv.org:2506.08938v1",
        "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2506.08938",
        "author": "Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, Jinsong Su",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08938v1 Announce Type: new \nAbstract: Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM`s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model`s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model`s parametric knowledge, which undermines the model`s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model`s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https:// github.com/DeepLearnXMU/Faithful-RAG"
      },
      {
        "id": "oai:arXiv.org:2506.08939v1",
        "title": "KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting",
        "link": "https://arxiv.org/abs/2506.08939",
        "author": "Hang Ye, Gaoxiang Duan, Haoran Zeng, Yangxin Zhu, Lingxue Meng, Xiaoying Zheng, Yongxin Zhu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08939v1 Announce Type: new \nAbstract: Multivariate long-term and efficient time series forecasting is a key requirement for a variety of practical applications, and there are complex interleaving time dynamics in time series data that require decomposition modeling. Traditional time series decomposition methods are single and rely on fixed rules, which are insufficient for mining the potential information of the series and adapting to the dynamic characteristics of complex series. On the other hand, the Transformer-based models for time series forecasting struggle to effectively model long sequences and intricate dynamic relationships due to their high computational complexity. To overcome these limitations, we introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to dynamically extract trend and seasonal components. It further integrates a Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series into frequency-domain and time-domain. These components are coupled with multi-scale Mamba-based KarmaBlock to efficiently process global and local information in a coordinated manner. Experiments on eight real-world datasets from diverse domains well demonstrated that KARMA significantly outperforms mainstream baseline methods in both predictive accuracy and computational efficiency. Code and full results are available at this repository: https://github.com/yedadasd/KARMA"
      },
      {
        "id": "oai:arXiv.org:2506.08949v1",
        "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation",
        "link": "https://arxiv.org/abs/2506.08949",
        "author": "Hongjie Zhu, Xiwei Liu, Rundong Xue, Zeyu Zhang, Yong Xu, Daji Ergu, Ying Cai, Yang Zhao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08949v1 Announce Type: new \nAbstract: In the era of information explosion, efficiently leveraging large-scale unlabeled data while minimizing the reliance on high-quality pixel-level annotations remains a critical challenge in the field of medical imaging. Semi-supervised learning (SSL) enhances the utilization of unlabeled data by facilitating knowledge transfer, significantly improving the performance of fully supervised models and emerging as a highly promising research direction in medical image analysis. Inspired by the ability of Vision Foundation Models (e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised SAM-2), a novel approach that leverages SAM-2's robust feature extraction capabilities to uncover latent knowledge in unlabeled medical images, thus effectively enhancing feature support for fully supervised medical image segmentation. Specifically, building upon the single-stream \"weak-to-strong\" consistency regularization framework, this paper introduces a Discriminative Feature Enhancement (DFE) mechanism to further explore the feature discrepancies introduced by various data augmentation strategies across multiple views. By leveraging feature similarity and dissimilarity across multi-scale augmentation techniques, the method reconstructs and models the features, thereby effectively optimizing the salient regions. Furthermore, a prompt generator is developed that integrates Physical Constraints with a Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data, fulfilling SAM-2's requirement for additional prompts. Extensive experiments demonstrate the superiority of the proposed method for semi-supervised medical image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably, SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous state-of-the-art method by +3.65 Dice. Code will be available at https://github.com/AIGeeksGroup/SSS."
      },
      {
        "id": "oai:arXiv.org:2506.08952v1",
        "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions",
        "link": "https://arxiv.org/abs/2506.08952",
        "author": "Clara Lachenmaier, Judith Sieker, Sina Zarrie{\\ss}",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08952v1 Announce Type: new \nAbstract: Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine the ability of LLMs to answer direct knowledge questions and loaded questions that presuppose misinformation. We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias. Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse."
      },
      {
        "id": "oai:arXiv.org:2506.08953v1",
        "title": "Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF",
        "link": "https://arxiv.org/abs/2506.08953",
        "author": "Anirudh Nanduri, Siyuan Huang, Rama Chellappa",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08953v1 Announce Type: new \nAbstract: Vision Transformers (ViTs) have demonstrated impressive performance across a wide range of biometric tasks, including face and body recognition. In this work, we adapt a ViT model pretrained on visible (VIS) imagery to the challenging problem of cross-spectral body recognition, which involves matching images captured in the visible and infrared (IR) domains. Recent ViT architectures have explored incorporating additional embeddings beyond traditional positional embeddings. Building on this idea, we integrate Side Information Embedding (SIE) and examine the impact of encoding domain and camera information to enhance cross-spectral matching. Surprisingly, our results show that encoding only camera information - without explicitly incorporating domain information - achieves state-of-the-art performance on the LLCM dataset. While occlusion handling has been extensively studied in visible-spectrum person re-identification (Re-ID), occlusions in visible-infrared (VI) Re-ID remain largely underexplored - primarily because existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly feature full-body, unoccluded images. To address this gap, we analyze the impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared images captured at various distances, enabling cross-range, cross-spectral evaluations."
      },
      {
        "id": "oai:arXiv.org:2506.08955v1",
        "title": "Segment Concealed Objects with Incomplete Supervision",
        "link": "https://arxiv.org/abs/2506.08955",
        "author": "Chunming He, Kai Li, Yachao Zhang, Ziyun Yang, Youwei Pang, Longxiang Tang, Chengyu Fang, Yulun Zhang, Linghe Kong, Xiu Li, Sina Farsiu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08955v1 Announce Type: new \nAbstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves segmenting objects that seamlessly blend into their surrounding environments, utilizing incompletely annotated data, such as weak and semi-annotations, for model training. This task remains highly challenging due to (1) the limited supervision provided by the incompletely annotated training data, and (2) the difficulty of distinguishing concealed objects from the background, which arises from the intrinsic similarities in concealed scenarios. In this paper, we introduce the first unified method for ISCOS to address these challenges. To tackle the issue of incomplete supervision, we propose a unified mean-teacher framework, SEE, that leverages the vision foundation model, ``\\emph{Segment Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced by the teacher model as prompts. To mitigate the effect of low-quality segmentation masks, we introduce a series of strategies for pseudo-label generation, storage, and supervision. These strategies aim to produce informative pseudo-labels, store the best pseudo-labels generated, and select the most reliable components to guide the student model, thereby ensuring robust network training. Additionally, to tackle the issue of intrinsic similarity, we design a hybrid-granularity feature grouping module that groups features at different granularities and aggregates these results. By clustering similar features, this module promotes segmentation coherence, facilitating more complete segmentation for both single-object and multiple-object images. We validate the effectiveness of our approach across multiple ISCOS tasks, and experimental results demonstrate that our method achieves state-of-the-art performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing the performance of existing models."
      },
      {
        "id": "oai:arXiv.org:2506.08956v1",
        "title": "Data Augmentation For Small Object using Fast AutoAugment",
        "link": "https://arxiv.org/abs/2506.08956",
        "author": "DaeEun Yoon, Semin Kim, SangWook Yoo, Jongha Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08956v1 Announce Type: new \nAbstract: In recent years, there has been tremendous progress in object detection performance. However, despite these advances, the detection performance for small objects is significantly inferior to that of large objects. Detecting small objects is one of the most challenging and important problems in computer vision. To improve the detection performance for small objects, we propose an optimal data augmentation method using Fast AutoAugment. Through our proposed method, we can quickly find optimal augmentation policies that can overcome degradation when detecting small objects, and we achieve a 20% performance improvement on the DOTA dataset."
      },
      {
        "id": "oai:arXiv.org:2506.08961v1",
        "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation",
        "link": "https://arxiv.org/abs/2506.08961",
        "author": "Chenxu Wang, Huaping Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08961v1 Announce Type: new \nAbstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have been widely studied in various threat models; however, few consider environmental state perturbations, which are natural in embodied scenarios. To improve the robustness of DRL agents, we formulate the problem of environmental state perturbation, introducing a preliminary non-targeted attack method as a calibration adversary, and then propose a defense framework, named Boosted Adversarial Training (BAT), which first tunes the agents via supervised learning to avoid catastrophic failure and subsequently adversarially trains the agent with reinforcement learning. Extensive experimental results substantiate the vulnerability of mainstream agents under environmental state perturbations and the effectiveness of our proposed attack. The defense results demonstrate that while existing robust reinforcement learning algorithms may not be suitable, our BAT framework can significantly enhance the robustness of agents against environmental state perturbations across various situations."
      },
      {
        "id": "oai:arXiv.org:2506.08964v1",
        "title": "ORIDa: Object-centric Real-world Image Composition Dataset",
        "link": "https://arxiv.org/abs/2506.08964",
        "author": "Jinwoo Kim, Sangmin Han, Jinho Jeong, Jiwoo Choi, Dongyoung Kim, Seon Joo Kim",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08964v1 Announce Type: new \nAbstract: Object compositing, the task of placing and harmonizing objects in images of diverse visual scenes, has become an important task in computer vision with the rise of generative models. However, existing datasets lack the diversity and scale required to comprehensively explore real-world scenarios. We introduce ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale, real-captured dataset containing over 30,000 images featuring 200 unique objects, each of which is presented across varied positions and scenes. ORIDa has two types of data: factual-counterfactual sets and factual-only scenes. The factual-counterfactual sets consist of four factual images showing an object in different positions within a scene and a single counterfactual (or background) image of the scene without the object, resulting in five images per scene. The factual-only scenes include a single image containing an object in a specific context, expanding the variety of environments. To our knowledge, ORIDa is the first publicly available dataset with its scale and complexity for real-world image composition. Extensive analysis and experiments highlight the value of ORIDa as a resource for advancing further research in object compositing."
      },
      {
        "id": "oai:arXiv.org:2506.08965v1",
        "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO",
        "link": "https://arxiv.org/abs/2506.08965",
        "author": "Yiyang Zhao, Huiyu Bai, Xuejiao Zhao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08965v1 Announce Type: new \nAbstract: The ability to train high-performing reward models with few-shot data is critical for enhancing the efficiency and scalability of Reinforcement Learning from Human Feedback (RLHF). We propose a data augmentation and expansion framework that enables generative reward models trained on small datasets to achieve comparable performance to those trained on large-scale datasets. Traditional methods to train a generative reward model, such as Direct Preference Optimization (DPO), are constrained by inefficiencies in sample pairing and limited data diversity. This work introduces preference refinement, which employs Chain-of-Thought (CoT) sampling to uncover diverse and high-quality preference relationships. It also incorporates a perplexity-based scoring mechanism to assign nuanced preference levels and utilizes Multi-level Direct Preference Optimization (M-DPO) to enable the model to capture finer-grained preference differences between samples. Experimental results demonstrate that the proposed method significantly enhances data efficiency and model performance, enabling reward models trained in a few-shot setting to achieve results on par with those trained on large-scale datasets. This study underscores the potential of data-efficient strategies in advancing reward model optimization, offering a robust solution for low-resource RLHF applications."
      },
      {
        "id": "oai:arXiv.org:2506.08966v1",
        "title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers",
        "link": "https://arxiv.org/abs/2506.08966",
        "author": "Marek Kadl\\v{c}\\'ik, Michal \\v{S}tef\\'anik, Timothee Mickus, Michal Spiegel, Josef Kucha\\v{r}",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08966v1 Announce Type: new \nAbstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing work showed limited success in probing numeric values from models' representations, indicating that these errors can be attributed to the inherent unreliability of distributionally learned embeddings in representing exact quantities. However, we observe that previous probing methods are inadequate for the emergent structure of learned number embeddings with sinusoidal patterns.\n  In response, we propose a novel probing technique that decodes numeric values from input embeddings with near-perfect accuracy across a range of open-source LMs. This proves that after the sole pre-training, LMs represent numbers with remarkable precision. Finally, we find that the embeddings' preciseness judged by our probe's accuracy explains a large portion of LM's errors in elementary arithmetic, and show that aligning the embeddings with the pattern discovered by our probe can mitigate these errors."
      },
      {
        "id": "oai:arXiv.org:2506.08968v1",
        "title": "ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations",
        "link": "https://arxiv.org/abs/2506.08968",
        "author": "Amirreza Rouhi, Solmaz Arezoomandan, Knut Peterson, Joseph T. Woods, David K. Han",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08968v1 Announce Type: new \nAbstract: Object detection models typically rely on predefined categories, limiting their ability to identify novel objects in open-world scenarios. To overcome this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model, a training-free, self-refining framework for open-world object labeling. ADAM leverages large language models (LLMs) to generate candidate labels for unknown objects based on contextual information from known entities within a scene. These labels are paired with visual embeddings from CLIP to construct an Embedding-Label Repository (ELR) that enables inference without category supervision. For a newly encountered unknown object, ADAM retrieves visually similar instances from the ELR and applies frequency-based voting and cross-modal re-ranking to assign a robust label. To further enhance consistency, we introduce a self-refinement loop that re-evaluates repository labels using visual cohesion analysis and k-nearest-neighbor-based majority re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate that ADAM effectively annotates novel categories using only visual and contextual signals, without requiring any fine-tuning or retraining."
      },
      {
        "id": "oai:arXiv.org:2506.08972v1",
        "title": "Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System",
        "link": "https://arxiv.org/abs/2506.08972",
        "author": "Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, Zhuosheng Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08972v1 Announce Type: new \nAbstract: Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks -- such as shot-chain execution tasks and single-screen grounding tasks -- while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at https://ui-nexus.github.io."
      },
      {
        "id": "oai:arXiv.org:2506.08977v1",
        "title": "Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data",
        "link": "https://arxiv.org/abs/2506.08977",
        "author": "Victoria Hankemeier, Malte Schilling",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08977v1 Announce Type: new \nAbstract: Developments in Deep Learning have significantly improved time series forecasting by enabling more accurate modeling of complex temporal dependencies inherent in sequential data. The effectiveness of such models is often demonstrated on limited sets of specific real-world data. Although this allows for comparative analysis, it still does not demonstrate how specific data characteristics align with the architectural strengths of individual models. Our research aims at uncovering clear connections between time series characteristics and particular models. We introduce a novel dataset generated using Gaussian Processes, specifically designed to display distinct, known characteristics for targeted evaluations of model adaptability to them. Furthermore, we present TimeFlex, a new model that incorporates a modular architecture tailored to handle diverse temporal dynamics, including trends and periodic patterns. This model is compared to current state-of-the-art models, offering a deeper understanding of how models perform under varied time series conditions."
      },
      {
        "id": "oai:arXiv.org:2506.08978v1",
        "title": "Propositional Logic for Probing Generalization in Neural Networks",
        "link": "https://arxiv.org/abs/2506.08978",
        "author": "Anna Langedijk, Jaap Jumelet, Willem Zuidema",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08978v1 Announce Type: new \nAbstract: The extent to which neural networks are able to acquire and represent symbolic rules remains a key topic of research and debate. Much current work focuses on the impressive capabilities of large language models, as well as their often ill-understood failures on a wide range of reasoning tasks. In this paper, in contrast, we investigate the generalization behavior of three key neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a controlled task rooted in propositional logic. The task requires models to generate satisfying assignments for logical formulas, making it a structured and interpretable setting for studying compositionality. We introduce a balanced extension of an existing dataset to eliminate superficial patterns and enable testing on unseen operator combinations. Using this dataset, we evaluate the ability of the three architectures to generalize beyond the training distribution. While all models perform well in-distribution, we find that generalization to unseen patterns, particularly those involving negation, remains a significant challenge. Transformers fail to apply negation compositionally, unless structural biases are introduced. Our findings highlight persistent limitations in the ability of standard architectures to learn systematic representations of logical operators, suggesting the need for stronger inductive biases to support robust rule-based reasoning."
      },
      {
        "id": "oai:arXiv.org:2506.08979v1",
        "title": "Rethinking Range-View LiDAR Segmentation in Adverse Weather",
        "link": "https://arxiv.org/abs/2506.08979",
        "author": "Longyu Yang, Ping Hu, Lu Zhang, Jun Liu, Yap-Peng Tan, Heng Tao Shen, Xiaofeng Zhu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08979v1 Announce Type: new \nAbstract: LiDAR segmentation has emerged as an important task to enrich multimedia experiences and analysis. Range-view-based methods have gained popularity due to their high computational efficiency and compatibility with real-time deployment. However, their generalized performance under adverse weather conditions remains underexplored, limiting their reliability in real-world environments. In this work, we identify and analyze the unique challenges that affect the generalization of range-view LiDAR segmentation in severe weather. To address these challenges, we propose a modular and lightweight framework that enhances robustness without altering the core architecture of existing models. Our method reformulates the initial stem block of standard range-view networks into two branches to process geometric attributes and reflectance intensity separately. Specifically, a Geometric Abnormality Suppression (GAS) module reduces the influence of weather-induced spatial noise, and a Reflectance Distortion Calibration (RDC) module corrects reflectance distortions through memory-guided adaptive instance normalization. The processed features are then fused and passed to the original segmentation pipeline. Extensive experiments on different benchmarks and baseline models demonstrate that our approach significantly improves generalization to adverse weather with minimal inference overhead, offering a practical and effective solution for real-world LiDAR segmentation."
      },
      {
        "id": "oai:arXiv.org:2506.08981v1",
        "title": "FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents",
        "link": "https://arxiv.org/abs/2506.08981",
        "author": "Satu Hopponen, Tomi Kinnunen, Alexandre Nikolaev, Rosa Gonz\\'alez Hautam\\\"aki, Lauri Tavi, Einar Meister",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08981v1 Announce Type: new \nAbstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers, who produced speech in their native language (L1), second language (L2), and imitated L2 (fake foreign accent). The new corpus enables research into language variability from phonetic and technological points of view. Accordingly, we include two preliminary case studies to demonstrate both perspectives. The first case study explores the impact of L2 and imitated L2 on the performance of an automatic speaker verification system, while the second illustrates the articulatory patterns of one speaker in L1, L2, and a fake accent."
      },
      {
        "id": "oai:arXiv.org:2506.08982v1",
        "title": "On Finetuning Tabular Foundation Models",
        "link": "https://arxiv.org/abs/2506.08982",
        "author": "Ivan Rubachev, Akim Kotelnikov, Nikolay Kartashev",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08982v1 Announce Type: new \nAbstract: Foundation models are an emerging research direction in tabular deep learning. Notably, TabPFNv2 recently claimed superior performance over traditional GBDT-based methods on small-scale datasets using an in-context learning paradigm, which does not adapt model parameters to target datasets. However, the optimal finetuning approach for adapting tabular foundational models, and how this adaptation reshapes their internal mechanisms, remains underexplored. While prior works studied finetuning for earlier foundational models, inconsistent findings and TabPFNv2's unique architecture necessitate fresh investigation. To address these questions, we first systematically evaluate various finetuning strategies on diverse datasets. Our findings establish full finetuning as the most practical solution for TabPFNv2 in terms of time-efficiency and effectiveness. We then investigate how finetuning alters TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models. We reveal that the success of finetuning stems from the fact that after gradient-based adaptation, the dot products of the query-representations of test objects and the key-representations of in-context training objects more accurately reflect their target similarity. This improved similarity allows finetuned TabPFNv2 to better approximate target dependency by appropriately weighting relevant in-context samples, improving the retrieval-based prediction logic. From the practical perspective, we managed to finetune TabPFNv2 on datasets with up to 50K objects, observing performance improvements on almost all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning allows TabPFNv2 to achieve state-of-the-art results, while on datasets with gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and prior methods remain better."
      },
      {
        "id": "oai:arXiv.org:2506.08986v1",
        "title": "Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder",
        "link": "https://arxiv.org/abs/2506.08986",
        "author": "Yuejiao Wang, Xianmin Gong, Xixin Wu, Patrick Wong, Hoi-lam Helene Fung, Man Wai Mak, Helen Meng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08986v1 Announce Type: new \nAbstract: Early detection is crucial for timely intervention aimed at preventing and slowing the progression of neurocognitive disorder (NCD), a common and significant health problem among the aging population. Recent evidence has suggested that language-related functional magnetic resonance imaging (fMRI) may be a promising approach for detecting cognitive decline and early NCD. In this paper, we proposed a novel, naturalistic language-related fMRI task for this purpose. We examined the effectiveness of this task among 97 non-demented Chinese older adults from Hong Kong. The results showed that machine-learning classification models based on fMRI features extracted from the task and demographics (age, gender, and education year) achieved an average area under the curve of 0.86 when classifying participants' cognitive status (labeled as NORMAL vs DECLINE based on their scores on a standard neurcognitive test). Feature localization revealed that the fMRI features most frequently selected by the data-driven approach came primarily from brain regions associated with language processing, such as the superior temporal gyrus, middle temporal gyrus, and right cerebellum. The study demonstrated the potential of the naturalistic language-related fMRI task for early detection of aging-related cognitive decline and NCD."
      },
      {
        "id": "oai:arXiv.org:2506.08989v1",
        "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning",
        "link": "https://arxiv.org/abs/2506.08989",
        "author": "Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, Weizhu Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08989v1 Announce Type: new \nAbstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08990v1",
        "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models",
        "link": "https://arxiv.org/abs/2506.08990",
        "author": "Chenyu Lian, Hong-Yu Zhou, Dongyun Liang, Jing Qin, Liansheng Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08990v1 Announce Type: new \nAbstract: Medical vision-language alignment through cross-modal contrastive learning shows promising performance in image-text matching tasks, such as retrieval and zero-shot classification. However, conventional cross-modal contrastive learning (CLIP-based) methods suffer from suboptimal visual representation capabilities, which also limits their effectiveness in vision-language alignment. In contrast, although the models pretrained via multimodal masked modeling struggle with direct cross-modal matching, they excel in visual representation. To address this contradiction, we propose ALTA (ALign Through Adapting), an efficient medical vision-language alignment method that utilizes only about 8% of the trainable parameters and less than 1/5 of the computational consumption required for masked record modeling. ALTA achieves superior performance in vision-language matching tasks like retrieval and zero-shot classification by adapting the pretrained vision model from masked record modeling. Additionally, we integrate temporal-multiview radiograph inputs to enhance the information consistency between radiographs and their corresponding descriptions in reports, further improving the vision-language alignment. Experimental evaluations show that ALTA outperforms the best-performing counterpart by over 4% absolute points in text-to-image accuracy and approximately 6% absolute points in image-to-text retrieval accuracy. The adaptation of vision-language models during efficient alignment also promotes better vision and language understanding. Code is publicly available at https://github.com/DopamineLcy/ALTA."
      },
      {
        "id": "oai:arXiv.org:2506.08991v1",
        "title": "Do Concept Replacement Techniques Really Erase Unacceptable Concepts?",
        "link": "https://arxiv.org/abs/2506.08991",
        "author": "Anudeep Das, Gurjot Singh, Prach Chantasantitam, N. Asokan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08991v1 Announce Type: new \nAbstract: Generative models, particularly diffusion-based text-to-image (T2I) models, have demonstrated astounding success. However, aligning them to avoid generating content with unacceptable concepts (e.g., offensive or copyrighted content, or celebrity likenesses) remains a significant challenge. Concept replacement techniques (CRTs) aim to address this challenge, often by trying to \"erase\" unacceptable concepts from models. Recently, model providers have started offering image editing services which accept an image and a text prompt as input, to produce an image altered as specified by the prompt. These are known as image-to-image (I2I) models. In this paper, we first use an I2I model to empirically demonstrate that today's state-of-the-art CRTs do not in fact erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in emerging I2I scenarios, despite their proven ability to remove unwanted concepts in T2I pipelines, highlighting the need to understand this discrepancy between T2I and I2I settings. Next, we argue that a good CRT, while replacing unacceptable concepts, should preserve other concepts specified in the inputs to generative models. We call this fidelity. Prior work on CRTs have neglected fidelity in the case of unacceptable concepts. Finally, we propose the use of targeted image-editing techniques to achieve both effectiveness and fidelity. We present such a technique, AntiMirror, and demonstrate its viability."
      },
      {
        "id": "oai:arXiv.org:2506.08997v1",
        "title": "SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction",
        "link": "https://arxiv.org/abs/2506.08997",
        "author": "Fabian Immel, Jan-Hendrik Pauls, Richard Fehler, Frank Bieder, Jonas Merkert, Christoph Stiller",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08997v1 Announce Type: new \nAbstract: Autonomous vehicles rely on detailed and accurate environmental information to operate safely. High definition (HD) maps offer a promising solution, but their high maintenance cost poses a significant barrier to scalable deployment. This challenge is addressed by online HD map construction methods, which generate local HD maps from live sensor data. However, these methods are inherently limited by the short perception range of onboard sensors. To overcome this limitation and improve general performance, recent approaches have explored the use of standard definition (SD) maps as prior, which are significantly easier to maintain. We propose SDTagNet, the first online HD map construction method that fully utilizes the information of widely available SD maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach introduces two key innovations. First, in contrast to previous work, we incorporate not only polyline SD map data with manually selected classes, but additional semantic information in the form of textual annotations. In this way, we enrich SD vector map tokens with NLP-derived features, eliminating the dependency on predefined specifications or exhaustive class taxonomies. Second, we introduce a point-level SD map encoder together with orthogonal element identifiers to uniformly integrate all types of map elements. Experiments on Argoverse 2 and nuScenes show that this boosts map perception performance by up to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP (+20%) w.r.t. previous approaches that already use SD map priors. Code is available at https://github.com/immel-f/SDTagNet"
      },
      {
        "id": "oai:arXiv.org:2506.08999v1",
        "title": "Employing self-supervised learning models for cross-linguistic child speech maturity classification",
        "link": "https://arxiv.org/abs/2506.08999",
        "author": "Theo Zhang, Madurya Suresh, Anne S. Warlaumont, Kasia Hitczenko, Alejandrina Cristia, Margaret Cychosz",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08999v1 Announce Type: new \nAbstract: Speech technology systems struggle with many downstream tasks for child speech due to small training corpora and the difficulties that child speech pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer models to address a fundamental classification task: identifying child vocalizations. Unlike previous corpora, our dataset captures maximally ecologically-valid child vocalizations across an unprecedented sample, comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu, Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004 labeled vocalizations, magnitudes larger than previous work. Models were trained to distinguish between cry, laughter, mature (consonant+vowel), and immature speech (just consonant or vowel). Models trained on the dataset outperform state-of-the-art models trained on previous datasets, achieved classification accuracy comparable to humans, and were robust across rural and urban settings."
      },
      {
        "id": "oai:arXiv.org:2506.09003v1",
        "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
        "link": "https://arxiv.org/abs/2506.09003",
        "author": "Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, Junyang Lin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09003v1 Announce Type: new \nAbstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow)."
      },
      {
        "id": "oai:arXiv.org:2506.09007v1",
        "title": "Branched Schr\\\"odinger Bridge Matching",
        "link": "https://arxiv.org/abs/2506.09007",
        "author": "Sophia Tang, Yinuo Zhang, Alexander Tong, Pranam Chatterjee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09007v1 Announce Type: new \nAbstract: Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\\\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\\\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations."
      },
      {
        "id": "oai:arXiv.org:2506.09009v1",
        "title": "UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags",
        "link": "https://arxiv.org/abs/2506.09009",
        "author": "Hakyung Sung, Gyu-Ho Shin, Chanyoung Lee, You Kyung Sung, Boo Kyung Jung",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09009v1 Announce Type: new \nAbstract: The present study extends recent work on Universal Dependencies annotations for second-language (L2) Korean by introducing a semi-automated framework that identifies morphosyntactic constructions from XPOS sequences and aligns those constructions with corresponding UPOS categories. We also broaden the existing L2-Korean corpus by annotating 2,998 new sentences from argumentative essays. To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean morphosyntactic analysis models on datasets both with and without these alignments, using two NLP toolkits. Our results indicate that the aligned dataset not only improves consistency across annotation layers but also enhances morphosyntactic tagging and dependency-parsing accuracy, particularly in cases of limited annotated data."
      },
      {
        "id": "oai:arXiv.org:2506.09010v1",
        "title": "Effective Data Pruning through Score Extrapolation",
        "link": "https://arxiv.org/abs/2506.09010",
        "author": "Sebastian Schmidt, Prasanga Dhungel, Christoffer L\\\"offler, Bj\\\"orn Nieth, Stephan G\\\"unnemann, Leo Schwinn",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09010v1 Announce Type: new \nAbstract: Training advanced machine learning models demands massive datasets, resulting in prohibitive computational costs. To address this challenge, data pruning techniques identify and remove redundant training samples while preserving model performance. Yet, existing pruning techniques predominantly require a full initial training pass to identify removable samples, negating any efficiency benefits for single training runs. To overcome this limitation, we introduce a novel importance score extrapolation framework that requires training on only a small subset of data. We present two initial approaches in this framework - k-nearest neighbors and graph neural networks - to accurately predict sample importance for the entire dataset using patterns learned from this minimal subset. We demonstrate the effectiveness of our approach for 2 state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training paradigms (supervised, unsupervised, and adversarial). Our results indicate that score extrapolation is a promising direction to scale expensive score calculation methods, such as pruning, data attribution, or other tasks."
      },
      {
        "id": "oai:arXiv.org:2506.09014v1",
        "title": "Learning to Reason Across Parallel Samples for LLM Reasoning",
        "link": "https://arxiv.org/abs/2506.09014",
        "author": "Jianing Qi, Xi Ye, Hao Tang, Zhigang Zhu, Eunsol Choi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09014v1 Announce Type: new \nAbstract: Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on multiple reasoning datasets show that SSA outperforms other test-time scaling methods such as reward model-based re-ranking. Our approach also shows a promising generalization ability, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently."
      },
      {
        "id": "oai:arXiv.org:2506.09016v1",
        "title": "SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning",
        "link": "https://arxiv.org/abs/2506.09016",
        "author": "Ruiqi Zhang, Daman Arora, Song Mei, Andrea Zanette",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09016v1 Announce Type: new \nAbstract: Training large language models with reinforcement learning (RL) against verifiable rewards significantly enhances their reasoning abilities, yet remains computationally expensive due to inefficient uniform prompt sampling. We introduce Selective Prompting with Efficient Estimation of Difficulty (SPEED), an adaptive online RL curriculum that selectively chooses training examples of intermediate difficulty to maximize learning efficiency. Theoretically, we establish that intermediate-difficulty prompts improve the gradient estimator's signal-to-noise ratio, accelerating convergence. Empirically, our efficient implementation leads to 2x to 6x faster training without degrading accuracy, requires no manual tuning, and integrates seamlessly into standard RL algorithms."
      },
      {
        "id": "oai:arXiv.org:2506.09018v1",
        "title": "Edit Flows: Flow Matching with Edit Operations",
        "link": "https://arxiv.org/abs/2506.09018",
        "author": "Marton Havasi, Brian Karrer, Itai Gat, Ricky T. Q. Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09018v1 Announce Type: new \nAbstract: Autoregressive generative models naturally generate variable-length sequences, while non-autoregressive models struggle, often imposing rigid, token-wise structures. We propose Edit Flows, a non-autoregressive model that overcomes these limitations by defining a discrete flow over sequences through edit operations-insertions, deletions, and substitutions. By modeling these operations within a Continuous-time Markov Chain over the sequence space, Edit Flows enable flexible, position-relative generation that aligns more closely with the structure of sequence data. Our training method leverages an expanded state space with auxiliary variables, making the learning process efficient and tractable. Empirical results show that Edit Flows outperforms both autoregressive and mask models on image captioning and significantly outperforms the mask construction in text and code generation."
      },
      {
        "id": "oai:arXiv.org:2506.09021v1",
        "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features",
        "link": "https://arxiv.org/abs/2506.09021",
        "author": "Hakyung Sung, Karla Csuros, Min-Chang Sung",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09021v1 Announce Type: new \nAbstract: This study examines the lexical and syntactic interventions of human and LLM proofreading aimed at improving overall intelligibility in identical second language writings, and evaluates the consistency of outcomes across three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and LLM proofreading enhance bigram lexical features, which may contribute to better coherence and contextual connectedness between adjacent words. However, LLM proofreading exhibits a more generative approach, extensively reworking vocabulary and sentence structures, such as employing more diverse and sophisticated vocabulary and incorporating a greater number of adjective modifiers in noun phrases. The proofreading outcomes are highly consistent in major lexical and syntactic features across the three models."
      },
      {
        "id": "oai:arXiv.org:2506.09022v1",
        "title": "Do MIL Models Transfer?",
        "link": "https://arxiv.org/abs/2506.09022",
        "author": "Daniel Shao, Richard J. Chen, Andrew H. Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09022v1 Announce Type: new \nAbstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational pathology (CPath) for generating clinically meaningful slide-level embeddings from gigapixel tissue images. However, MIL often struggles with small, weakly supervised clinical datasets. In contrast to fields such as NLP and conventional computer vision, where transfer learning is widely used to address data scarcity, the transferability of MIL models remains poorly understood. In this study, we systematically evaluate the transfer learning capabilities of pretrained MIL models by assessing 11 models across 21 pretraining tasks for morphological and molecular subtype prediction. Our results show that pretrained MIL models, even when trained on different organs than the target task, consistently outperform models trained from scratch. Moreover, pretraining on pancancer datasets enables strong generalization across organs and tasks, outperforming slide foundation models while using substantially less pretraining data. These findings highlight the robust adaptability of MIL models and demonstrate the benefits of leveraging transfer learning to boost performance in CPath. Lastly, we provide a resource which standardizes the implementation of MIL models and collection of pretrained model weights on popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab"
      },
      {
        "id": "oai:arXiv.org:2506.09024v1",
        "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging",
        "link": "https://arxiv.org/abs/2506.09024",
        "author": "Felix Wagner, Pramit Saha, Harry Anthony, J. Alison Noble, Konstantinos Kamnitsas",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09024v1 Announce Type: new \nAbstract: Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code will be available upon acceptance at: *****"
      },
      {
        "id": "oai:arXiv.org:2506.09026v1",
        "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs",
        "link": "https://arxiv.org/abs/2506.09026",
        "author": "Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09026v1 Announce Type: new \nAbstract: Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep \"thinking\" for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging \"negative\" gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model."
      },
      {
        "id": "oai:arXiv.org:2506.09027v1",
        "title": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "link": "https://arxiv.org/abs/2506.09027",
        "author": "Runqian Wang, Kaiming He",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09027v1 Announce Type: new \nAbstract: The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \\textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning."
      },
      {
        "id": "oai:arXiv.org:2506.09033v1",
        "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.09033",
        "author": "Haozhen Zhang, Tao Feng, Jiaxuan You",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09033v1 Announce Type: new \nAbstract: The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1."
      },
      {
        "id": "oai:arXiv.org:2506.09034v1",
        "title": "FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed",
        "link": "https://arxiv.org/abs/2506.09034",
        "author": "Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09034v1 Announce Type: new \nAbstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA's parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO's formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training."
      },
      {
        "id": "oai:arXiv.org:2506.09035v1",
        "title": "Princeton365: A Diverse Dataset with Accurate Camera Pose",
        "link": "https://arxiv.org/abs/2506.09035",
        "author": "Karhan Kayan, Stamatis Alexandropoulos, Rishabh Jain, Yiming Zuo, Erich Liang, Jia Deng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09035v1 Announce Type: new \nAbstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with accurate camera pose. Our dataset bridges the gap between accuracy and data diversity in current SLAM benchmarks by introducing a novel ground truth collection framework that leverages calibration boards and a 360-camera. We collect indoor, outdoor, and object scanning videos with synchronized monocular and stereo RGB video outputs as well as IMU. We further propose a new scene scale-aware evaluation metric for SLAM based on the the optical flow induced by the camera pose estimation error. In contrast to the current metrics, our new metric allows for comparison between the performance of SLAM methods across scenes as opposed to existing metrics such as Average Trajectory Error (ATE), allowing researchers to analyze the failure modes of their methods. We also propose a challenging Novel View Synthesis benchmark that covers cases not covered by current NVS benchmarks, such as fully non-Lambertian scenes with 360-degree camera trajectories. Please visit https://princeton365.cs.princeton.edu for the dataset, code, videos, and submission."
      },
      {
        "id": "oai:arXiv.org:2506.09040v1",
        "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better",
        "link": "https://arxiv.org/abs/2506.09040",
        "author": "Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng Yu, Zhongyu Wei, Jiaqi Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09040v1 Announce Type: new \nAbstract: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR."
      },
      {
        "id": "oai:arXiv.org:2506.09042v1",
        "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models",
        "link": "https://arxiv.org/abs/2506.09042",
        "author": "Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, Seung Wook Kim, Jun Gao, Laura Leal-Taixe, Mike Chen, Sanja Fidler, Huan Ling",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09042v1 Announce Type: new \nAbstract: Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our pipeline toolkit, dataset and model weights through the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams"
      },
      {
        "id": "oai:arXiv.org:2506.09044v1",
        "title": "The Decoupled Risk Landscape in Performative Prediction",
        "link": "https://arxiv.org/abs/2506.09044",
        "author": "Javier Sanguino, Thomas Kehrenberg, Jose A. Lozano, Novi Quadrianto",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09044v1 Announce Type: new \nAbstract: Performative Prediction addresses scenarios where deploying a model induces a distribution shift in the input data, such as individuals modifying their features and reapplying for a bank loan after rejection. Literature has had a theoretical perspective giving mathematical guarantees for convergence (either to the stable or optimal point). We believe that visualization of the loss landscape can complement this theoretical advances with practical insights. Therefore, (1) we introduce a simple decoupled risk visualization method inspired in the two-step process that performative prediction is. Our approach visualizes the risk landscape with respect to two parameter vectors: model parameters and data parameters. We use this method to propose new properties of the interest points, to examine how existing algorithms traverse the risk landscape and perform under more realistic conditions, including strategic classification with non-linear models. (2) Building on this decoupled risk visualization, we introduce a novel setting - extended Performative Prediction - which captures scenarios where the distribution reacts to a model different from the decision-making one, reflecting the reality that agents often lack full access to the deployed model."
      },
      {
        "id": "oai:arXiv.org:2506.09045v1",
        "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
        "link": "https://arxiv.org/abs/2506.09045",
        "author": "Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, Qi Tian",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09045v1 Announce Type: new \nAbstract: Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically and steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under comparable computational budgets."
      },
      {
        "id": "oai:arXiv.org:2506.09046v1",
        "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation",
        "link": "https://arxiv.org/abs/2506.09046",
        "author": "Xiaowen Ma, Chenyang Lin, Yao Zhang, Volker Tresp, Yunpu Ma",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09046v1 Announce Type: new \nAbstract: Leveraging multiple Large Language Models(LLMs) has proven effective for addressing complex, high-dimensional tasks, but current approaches often rely on static, manually engineered multi-agent configurations. To overcome these constraints, we present the Agentic Neural Network(ANN), a framework that conceptualizes multi-agent collaboration as a layered neural network architecture. In this design, each agent operates as a node, and each layer forms a cooperative \"team\" focused on a specific subtask. Agentic Neural Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing inspiration from neural network forward passes, tasks are dynamically decomposed into subtasks, and cooperative agent teams with suitable aggregation methods are constructed layer by layer. (2) Backward Phase-Mirroring backpropagation, we refine both global and local collaboration through iterative feedback, allowing agents to self-evolve their roles, prompts, and coordination. This neuro-symbolic approach enables ANN to create new or specialized agent teams post-training, delivering notable gains in accuracy and adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent baselines under the same configurations, showing consistent performance improvements. Our findings indicate that ANN provides a scalable, data-driven framework for multi-agent systems, combining the collaborative capabilities of LLMs with the efficiency and flexibility of neural network principles. We plan to open-source the entire framework."
      },
      {
        "id": "oai:arXiv.org:2506.09047v1",
        "title": "Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs",
        "link": "https://arxiv.org/abs/2506.09047",
        "author": "Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09047v1 Announce Type: new \nAbstract: Vision-Language models (VLMs) show impressive abilities to answer questions on visual inputs (e.g., counting objects in an image), yet demonstrate higher accuracies when performing an analogous task on text (e.g., counting words in a text). We investigate this accuracy gap by identifying and comparing the \\textit{circuits} - the task-specific computational sub-graphs - in different modalities. We show that while circuits are largely disjoint between modalities, they implement relatively similar functionalities: the differences lie primarily in processing modality-specific data positions (an image or a text sequence). Zooming in on the image data representations, we observe they become aligned with the higher-performing analogous textual representations only towards later layers, too late in processing to effectively influence subsequent positions. To overcome this, we patch the representations of visual data tokens from later layers back into earlier layers. In experiments with multiple tasks and models, this simple intervention closes a third of the performance gap between the modalities, on average. Our analysis sheds light on the multi-modal performance gap in VLMs and suggests a training-free approach for reducing it."
      },
      {
        "id": "oai:arXiv.org:2506.09048v1",
        "title": "Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations",
        "link": "https://arxiv.org/abs/2506.09048",
        "author": "Yuxin Dong, Jiachen Jiang, Zhihui Zhu, Xia Ning",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09048v1 Announce Type: new \nAbstract: Task vectors offer a compelling mechanism for accelerating inference in in-context learning (ICL) by distilling task-specific information into a single, reusable representation. Despite their empirical success, the underlying principles governing their emergence and functionality remain unclear. This work proposes the Linear Combination Conjecture, positing that task vectors act as single in-context demonstrations formed through linear combinations of the original ones. We provide both theoretical and empirical support for this conjecture. First, we show that task vectors naturally emerge in linear transformers trained on triplet-formatted prompts through loss landscape analysis. Next, we predict the failure of task vectors on representing high-rank mappings and confirm this on practical LLMs. Our findings are further validated through saliency analyses and parameter visualization, suggesting an enhancement of task vectors by injecting multiple ones into few-shot prompts. Together, our results advance the understanding of task vectors and shed light on the mechanisms underlying ICL in transformer-based models."
      },
      {
        "id": "oai:arXiv.org:2506.00160v1",
        "title": "Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement",
        "link": "https://arxiv.org/abs/2506.00160",
        "author": "Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00160v1 Announce Type: cross \nAbstract: The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more engaging experience for human players in LLM-agent-based social deduction games like Werewolf. Previous works either fine-tuning, advanced prompting engineering, or additional experience pool to achieve engaging text-format Werewolf game experience. We propose a novel yet straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS) models designed for enhanced compatibility with various LLM models, and improved user engagement. We argue with ever enhancing LLM reasoning, extra components will be unnecessary in the case of Werewolf."
      },
      {
        "id": "oai:arXiv.org:2506.08023v1",
        "title": "Aligning Proteins and Language: A Foundation Model for Protein Retrieval",
        "link": "https://arxiv.org/abs/2506.08023",
        "author": "Qifeng Wu, Zhengzhe Liu, Han Zhu, Yizhou Zhao, Daisuke Kihara, Min Xu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08023v1 Announce Type: cross \nAbstract: This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology."
      },
      {
        "id": "oai:arXiv.org:2506.08026v1",
        "title": "TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load",
        "link": "https://arxiv.org/abs/2506.08026",
        "author": "Xibai Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08026v1 Announce Type: cross \nAbstract: This paper proposes TIP-Search, a time-predictable inference scheduling framework for real-time market prediction under uncertain workloads. Motivated by the strict latency demands in high-frequency financial systems, TIP-Search dynamically selects a deep learning model from a heterogeneous pool, aiming to maximize predictive accuracy while satisfying per-task deadline constraints. Our approach profiles latency and generalization performance offline, then performs online task-aware selection without relying on explicit input domain labels. We evaluate TIP-Search on three real-world limit order book datasets (FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms static baselines with up to 8.5% improvement in accuracy and 100% deadline satisfaction. Our results highlight the effectiveness of TIP-Search in robust low-latency financial inference under uncertainty."
      },
      {
        "id": "oai:arXiv.org:2506.08029v1",
        "title": "Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08029",
        "author": "Jiayu Li, Masood Mortazavi, Ning Yan, Yihong Ma, Reza Zafarani",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08029v1 Announce Type: cross \nAbstract: The goal of inverse design in distributed circuits is to generate near-optimal designs that meet a desirable transfer function specification. Existing design exploration methods use some combination of strategies involving artificial grids, differentiable evaluation procedures, and specific template topologies. However, real-world design practices often require non-differentiable evaluation procedures, varying topologies, and near-continuous placement spaces. In this paper, we propose DCIDA, a design exploration framework that learns a near-optimal design sampling policy for a target transfer function. DCIDA decides all design factors in a compound single-step action by sampling from a set of jointly-trained conditional distributions generated by the policy. Utilizing an injective interdependent ``map\", DCIDA transforms raw sampled design ``actions\" into uniquely equivalent physical representations, enabling the framework to learn the conditional dependencies among joint ``raw'' design decisions. Our experiments demonstrate DCIDA's Transformer-based policy network achieves significant reductions in design error compared to state-of-the-art approaches, with significantly better fit in cases involving more complex transfer functions."
      },
      {
        "id": "oai:arXiv.org:2506.08030v1",
        "title": "MOSS: Multi-Objective Optimization for Stable Rule Sets",
        "link": "https://arxiv.org/abs/2506.08030",
        "author": "Brian Liu, Rahul Mazumder",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08030v1 Announce Type: cross \nAbstract: We present MOSS, a multi-objective optimization framework for constructing stable sets of decision rules. MOSS incorporates three important criteria for interpretability: sparsity, accuracy, and stability, into a single multi-objective optimization framework. Importantly, MOSS allows a practitioner to rapidly evaluate the trade-off between accuracy and stability in sparse rule sets in order to select an appropriate model. We develop a specialized cutting plane algorithm in our framework to rapidly compute the Pareto frontier between these two objectives, and our algorithm scales to problem instances beyond the capabilities of commercial optimization solvers. Our experiments show that MOSS outperforms state-of-the-art rule ensembles in terms of both predictive performance and stability."
      },
      {
        "id": "oai:arXiv.org:2506.08033v1",
        "title": "Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases",
        "link": "https://arxiv.org/abs/2506.08033",
        "author": "Axel TahmasebiMoradi, Vincent Ren, Benjamin Le-Creurer, Chetra Mang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08033v1 Announce Type: cross \nAbstract: Aiming to reduce the computational cost of numerical simulations, a convolutional neural network (CNN) and a multi-layer perceptron (MLP) are introduced to build a surrogate model to approximate radiative heat transfer solutions in a 2-D walled domain with participative gases. The originality of this work lays in the adaptation of the inputs of the problem (gas and wall properties) in order to fit with the CNN architecture, more commonly used for image processing. Two precision datasets have been created with the classical solver, ICARUS2D, that uses the discrete transfer radiation method with the statistical narrow bands model. The performance of the CNN architecture is compared to a more classical MLP architecture in terms of speed and accuracy. Thanks to Optuna, all results are obtained using the optimized hyper parameters networks. The results show a significant speedup with industrially acceptable relative errors compared to the classical solver for both architectures. Additionally, the CNN outperforms the MLP in terms of precision and is more robust and stable to changes in hyper-parameters. A performance analysis on the dataset size of the samples have also been carried out to gain a deeper understanding of the model behavior."
      },
      {
        "id": "oai:arXiv.org:2506.08043v1",
        "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers",
        "link": "https://arxiv.org/abs/2506.08043",
        "author": "Ashkan Shahbazi, Kyvia Pereira, Jon S. Heiselman, Elaheh Akbari, Annie C. Benson, Sepehr Seifi, Xinyuan Liu, Garrison L. Johnston, Erwin Terpstra, Anne Draaisma, Jan-Jaap Severes, Jie Ying Wu, Nabil Simaan, Michael L. Miga, Soheil Kolouri",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08043v1 Announce Type: cross \nAbstract: Fast and accurate simulation of soft tissue deformation is a critical factor for surgical robotics and medical training. In this paper, we introduce a novel physics-informed neural simulator that approximates soft tissue deformations in a realistic and real-time manner. Our framework integrates Kelvinlet-based priors into neural simulators, making it the first approach to leverage Kelvinlets for residual learning and regularization in data-driven soft tissue modeling. By incorporating large-scale Finite Element Method (FEM) simulations of both linear and nonlinear soft tissue responses, our method improves neural network predictions across diverse architectures, enhancing accuracy and physical consistency while maintaining low latency for real-time performance. We demonstrate the effectiveness of our approach by performing accurate surgical maneuvers that simulate the use of standard laparoscopic tissue grasping tools with high fidelity. These results establish Kelvinlet-augmented learning as a powerful and efficient strategy for real-time, physics-aware soft tissue simulation in surgical applications."
      },
      {
        "id": "oai:arXiv.org:2506.08047v1",
        "title": "Evaluation of Machine Learning Models in Student Academic Performance Prediction",
        "link": "https://arxiv.org/abs/2506.08047",
        "author": "A. G. R. Sandeepa, Sanka Mohottala",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08047v1 Announce Type: cross \nAbstract: This research investigates the use of machine learning methods to forecast students' academic performance in a school setting. Students' data with behavioral, academic, and demographic details were used in implementations with standard classical machine learning models including multi-layer perceptron classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across all implementations. Under 10-fold cross validation, MLPC obtained 79.58% average accuracy for test set while for train set, it was 99.65%. MLP's better performance over other machine learning models strongly suggest the potential use of neural networks as data-efficient models. Feature selection approach played a crucial role in improving the performance and multiple evaluation approaches were used in order to compare with existing literature. Explainable machine learning methods were utilized to demystify the black box models and to validate the feature selection approach."
      },
      {
        "id": "oai:arXiv.org:2506.08049v1",
        "title": "Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting",
        "link": "https://arxiv.org/abs/2506.08049",
        "author": "Tengfei Lyu, Weijia Zhang, Hao Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08049v1 Announce Type: cross \nAbstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions from several weeks to months in advance, presents significant challenges due to the chaotic dynamics of atmospheric systems and complex interactions across multiple scales. Current approaches often fail to explicitly model underlying physical processes and teleconnections that are crucial at S2S timescales. We introduce TelePiT, a novel deep learning architecture that enhances global S2S forecasting through integrated multi-scale physics and teleconnection awareness. Our approach consists of three key components: (1) Spherical Harmonic Embedding, which accurately encodes global atmospheric variables onto spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which explicitly captures atmospheric physical processes across multiple learnable frequency bands; (3) Teleconnection-Aware Transformer, which models critical global climate interactions through tactfully injecting teleconnection patterns into the self-attention. Extensive experiments demonstrate that TelePiT significantly outperforms state-of-the-art data-driven baselines and operational numerical weather prediction systems, with remarkable improvements for atmospheric variables including a 57.7% reduction in RMSE for 2-meter temperature compared to previous best models."
      },
      {
        "id": "oai:arXiv.org:2506.08059v1",
        "title": "CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction",
        "link": "https://arxiv.org/abs/2506.08059",
        "author": "Huong Van Le, Weibin Ren, Junhong Kim, Yukyung Yun, Young Bin Park, Young Jun Kim, Bok Kyung Han, Inho Choi, Jong IL Park, Hwi-Yeol Yun, Jae-Mun Choi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08059v1 Announce Type: cross \nAbstract: Caco-2 permeability serves as a critical in vitro indicator for predicting the oral absorption of drug candidates during early-stage drug discovery. To enhance the accuracy and efficiency of computational predictions, we systematically investigated the impact of eight molecular feature representation types including 2D/3D descriptors, structural fingerprints, and deep learning-based embeddings combined with automated machine learning techniques to predict Caco-2 permeability. Using two datasets of differing scale and diversity (TDC benchmark and curated OCHEM data), we assessed model performance across representations and identified PaDEL, Mordred, and RDKit descriptors as particularly effective for Caco-2 prediction. Notably, the AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore, for both PaDEL and Mordred representations, the incorporation of 3D descriptors resulted in a 15.73% reduction in MAE compared to using 2D features alone, as confirmed by feature importance analysis. These findings highlight the effectiveness of AutoML approaches in ADMET modeling and offer practical guidance for feature selection in data-limited prediction tasks."
      },
      {
        "id": "oai:arXiv.org:2506.08064v1",
        "title": "A Real-time 3D Desktop Display",
        "link": "https://arxiv.org/abs/2506.08064",
        "author": "Livio Tenze, Enrique Canessa",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08064v1 Announce Type: cross \nAbstract: A new extended version of the altiro3D C++ Library -- initially developed to get glass-free holographic displays starting from 2D images -- is here introduced aiming to deal with 3D video streams from either 2D webcam images or flat video files. These streams are processed in real-time to synthesize light-fields (in Native format) and feed realistic 3D experiences. The core function needed to recreate multiviews consists on the use of MiDaS Convolutional Neural Network (CNN), which allows to extract a depth map from a single 2D image. Artificial Intelligence (AI) computing techniques are applied to improve the overall performance of the extended altiro3D Library. Thus, altiro3D can now treat standard images, video streams or screen portions of a Desktop where other apps may be also running (like web browsers, video chats, etc) and render them into 3D. To achieve the latter, a screen region need to be selected in order to feed the output directly into a light-field 3D device such as Looking Glass (LG) Portrait. In order to simplify the acquisition of a Desktop screen area by the user, a multi-platform Graphical User Interface has been also implemented. Sources available at: https://github.com/canessae/altiro3D/releases/tag/2.0.0"
      },
      {
        "id": "oai:arXiv.org:2506.08065v1",
        "title": "Dynamic Diffusion Schr\\\"odinger Bridge in Astrophysical Observational Inversions",
        "link": "https://arxiv.org/abs/2506.08065",
        "author": "Ye Zhu, Duo Xu, Zhiwei Deng, Jonathon C. Tan, Olga Russakovsky",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08065v1 Announce Type: cross \nAbstract: We study Diffusion Schr\\\"odinger Bridge (DSB) models in the context of dynamical astrophysical systems, specifically tackling observational inverse prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We introduce the Astro-DSB model, a variant of DSB with the pairwise domain assumption tailored for astrophysical dynamics. By investigating its learning process and prediction performance in both physically simulated data and in real observations (the Taurus B213 data), we present two main takeaways. First, from the astrophysical perspective, our proposed paired DSB method improves interpretability, learning efficiency, and prediction performance over conventional astrostatistical and other machine learning methods. Second, from the generative modeling perspective, probabilistic generative modeling reveals improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution (OOD) testing cases of physical simulations with unseen initial conditions and different dominant physical processes. Our study expands research into diffusion models beyond the traditional visual synthesis application and provides evidence of the models' learning abilities beyond pure data statistics, paving a path for future physics-aware generative models which can align dynamics between machine learning and real (astro)physical systems."
      },
      {
        "id": "oai:arXiv.org:2506.08066v1",
        "title": "WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection",
        "link": "https://arxiv.org/abs/2506.08066",
        "author": "Alexander Stepikin, Evgenia Romanenkova, Alexey Zaytsev",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08066v1 Announce Type: cross \nAbstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution shifts in data streams. Real-world high-dimensional CPD remains challenging due to data pattern complexity and violation of common assumptions. Resorting to standalone deep neural networks, the current state-of-the-art detectors have yet to achieve perfect quality. Concurrently, ensembling provides more robust solutions, boosting the performance. In this paper, we investigate ensembles of deep change point detectors and realize that standard prediction aggregation techniques, e.g., averaging, are suboptimal and fail to account for problem peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific method of ensemble aggregation based on the Wasserstein distance. Our procedure is versatile, working effectively with various ensembles of deep CPD models. Moreover, unlike existing solutions, we practically lift a long-standing problem of the decision threshold selection for CPD."
      },
      {
        "id": "oai:arXiv.org:2506.08073v1",
        "title": "Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy",
        "link": "https://arxiv.org/abs/2506.08073",
        "author": "Yu Liu, Utkarsh Pratiush, Kamyar Barakati, Hiroshi Funakubo, Ching-Che Lin, Jaegyu Kim, Lane W. Martin, Sergei V. Kalinin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08073v1 Announce Type: cross \nAbstract: Ferroelectric polarization switching underpins the functional performance of a wide range of materials and devices, yet its dependence on complex local microstructural features renders systematic exploration by manual or grid-based spectroscopic measurements impractical. Here, we introduce a multi-objective kernel-learning workflow that infers the microstructural rules governing switching behavior directly from high-resolution imaging data. Applied to automated piezoresponse force microscopy (PFM) experiments, our framework efficiently identifies the key relationships between domain-wall configurations and local switching kinetics, revealing how specific wall geometries and defect distributions modulate polarization reversal. Post-experiment analysis projects abstract reward functions, such as switching ease and domain symmetry, onto physically interpretable descriptors including domain configuration and proximity to boundaries. This enables not only high-throughput active learning, but also mechanistic insight into the microstructural control of switching phenomena. While demonstrated for ferroelectric domain switching, our approach provides a powerful, generalizable tool for navigating complex, non-differentiable design spaces, from structure-property correlations in molecular discovery to combinatorial optimization across diverse imaging modalities."
      },
      {
        "id": "oai:arXiv.org:2506.08074v1",
        "title": "Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval",
        "link": "https://arxiv.org/abs/2506.08074",
        "author": "Abdellah Ghassel, Ian Robinson, Gabriel Tanase, Hal Cooper, Bryan Thompson, Zhen Han, Vassilis N. Ioannidis, Soji Adeshina, Huzefa Rangwala",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08074v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation (RAG) grounds large language models in external evidence, yet it still falters when answers must be pieced together across semantically distant documents. We close this gap with the Hierarchical Lexical Graph (HLG), a three-tier index that (i) traces every atomic proposition to its source, (ii) clusters propositions into latent topics, and (iii) links entities and relations to expose cross-document paths. On top of HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG, which performs fine-grained entity-aware beam search over propositions for high-precision factoid questions, and TopicGraphRAG, which selects coarse topics before expanding along entity links to supply broad yet relevant context for exploratory queries. Additionally, existing benchmarks lack the complexity required to rigorously evaluate multi-hop summarization systems, often focusing on single-document queries or limited datasets. To address this, we introduce a synthetic dataset generation pipeline that curates realistic, multi-document question-answer pairs, enabling robust evaluation of multi-hop retrieval systems. Extensive experiments across five datasets demonstrate that our methods outperform naive chunk-based RAG achieving an average relative improvement of 23.1% in retrieval recall and correctness. Open-source Python library is available at https://github.com/awslabs/graphrag-toolkit."
      },
      {
        "id": "oai:arXiv.org:2506.08121v1",
        "title": "Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence",
        "link": "https://arxiv.org/abs/2506.08121",
        "author": "Qi Feng, Gu Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08121v1 Announce Type: cross \nAbstract: We introduce a continuous policy-value iteration algorithm where the approximations of the value function of a stochastic control problem and the optimal control are simultaneously updated through Langevin-type dynamics. This framework applies to both the entropy-regularized relaxed control problems and the classical control problems, with infinite horizon. We establish policy improvement and demonstrate convergence to the optimal control under the monotonicity condition of the Hamiltonian. By utilizing Langevin-type stochastic differential equations for continuous updates along the policy iteration direction, our approach enables the use of distribution sampling and non-convex learning techniques in machine learning to optimize the value function and identify the optimal control simultaneously."
      },
      {
        "id": "oai:arXiv.org:2506.08127v1",
        "title": "Constrained Pareto Set Identification with Bandit Feedback",
        "link": "https://arxiv.org/abs/2506.08127",
        "author": "Cyrille Kone, Emilie Kaufmann, Laura Richert",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08127v1 Announce Type: cross \nAbstract: In this paper, we address the problem of identifying the Pareto Set under feasibility constraints in a multivariate bandit setting. Specifically, given a $K$-armed bandit with unknown means $\\mu_1, \\dots, \\mu_K \\in \\mathbb{R}^d$, the goal is to identify the set of arms whose mean is not uniformly worse than that of another arm (i.e., not smaller for all objectives), while satisfying some known set of linear constraints, expressing, for example, some minimal performance on each objective. Our focus lies in fixed-confidence identification, for which we introduce an algorithm that significantly outperforms racing-like algorithms and the intuitive two-stage approach that first identifies feasible arms and then their Pareto Set. We further prove an information-theoretic lower bound on the sample complexity of any algorithm for constrained Pareto Set identification, showing that the sample complexity of our approach is near-optimal. Our theoretical results are supported by an extensive empirical evaluation on a series of benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08153v1",
        "title": "A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems",
        "link": "https://arxiv.org/abs/2506.08153",
        "author": "Renato Cordeiro Ferreira (University of S\\~ao Paulo, Jheronimus Academy of Data Science, Technical University of Eindhoven, Tilburg University)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08153v1 Announce Type: cross \nAbstract: How can the complexity of ML-enabled systems be managed effectively? The goal of this research is to investigate how complexity affects ML-Enabled Systems (MLES). To address this question, this research aims to introduce a metrics-based architectural model to characterize the complexity of MLES. The goal is to support architectural decisions, providing a guideline for the inception and growth of these systems. This paper showcases the first step for creating the metrics-based architectural model: an extension of a reference architecture that can describe MLES to collect their metrics."
      },
      {
        "id": "oai:arXiv.org:2506.08183v1",
        "title": "A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation",
        "link": "https://arxiv.org/abs/2506.08183",
        "author": "Isha Puri, David Cox",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08183v1 Announce Type: cross \nAbstract: Research in neuroscience and vision science relies heavily on careful measurements of animal subject's gaze direction. Rodents are the most widely studied animal subjects for such research because of their economic advantage and hardiness. Recently, video based eye trackers that use image processing techniques have become a popular option for gaze tracking because they are easy to use and are completely noninvasive. Although significant progress has been made in improving the accuracy and robustness of eye tracking algorithms, unfortunately, almost all of the techniques have focused on human eyes, which does not account for the unique characteristics of the rodent eye images, e.g., variability in eye parameters, abundance of surrounding hair, and their small size. To overcome these unique challenges, this work presents a flexible, robust, and highly accurate model for pupil and corneal reflection identification in rodent gaze determination that can be incrementally trained to account for variability in eye parameters encountered in the field. To the best of our knowledge, this is the first paper that demonstrates a highly accurate and practical biomedical image segmentation based convolutional neural network architecture for pupil and corneal reflection identification in eye images. This new method, in conjunction with our automated infrared videobased eye recording system, offers the state of the art technology in eye tracking for neuroscience and vision science research for rodents."
      },
      {
        "id": "oai:arXiv.org:2506.08188v1",
        "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
        "link": "https://arxiv.org/abs/2506.08188",
        "author": "Wenlong Meng, Shuguo Fan, Chengkun Wei, Min Chen, Yuwei Li, Yuanchao Zhang, Zhikun Zhang, Wenzhi Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08188v1 Announce Type: cross \nAbstract: In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access.\n  We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors."
      },
      {
        "id": "oai:arXiv.org:2506.08192v1",
        "title": "Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms",
        "link": "https://arxiv.org/abs/2506.08192",
        "author": "Jared Claypoole, Steven Cheung, Ashish Gehani, Vinod Yegneswaran, Ahmad Ridley",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08192v1 Announce Type: cross \nAbstract: We analyze two open source deep reinforcement learning agents submitted to the CAGE Challenge 2 cyber defense challenge, where each competitor submitted an agent to defend a simulated network against each of several provided rules-based attack agents. We demonstrate that one can gain interpretability of agent successes and failures by simplifying the complex state and action spaces and by tracking important events, shedding light on the fine-grained behavior of both the defense and attack agents in each experimental scenario. By analyzing important events within an evaluation episode, we identify patterns in infiltration and clearing events that tell us how well the attacker and defender played their respective roles; for example, defenders were generally able to clear infiltrations within one or two timesteps of a host being exploited. By examining transitions in the environment's state caused by the various possible actions, we determine which actions tended to be effective and which did not, showing that certain important actions are between 40% and 99% ineffective. We examine how decoy services affect exploit success, concluding for instance that decoys block up to 94% of exploits that would directly grant privileged access to a host. Finally, we discuss the realism of the challenge and ways that the CAGE Challenge 4 has addressed some of our concerns."
      },
      {
        "id": "oai:arXiv.org:2506.08200v1",
        "title": "AffectMachine-Pop: A controllable expert system for real-time pop music generation",
        "link": "https://arxiv.org/abs/2506.08200",
        "author": "Kat R. Agres, Adyasha Dash, Phoebe Chua, Stefan K. Ehrlich",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08200v1 Announce Type: cross \nAbstract: Music is a powerful medium for influencing listeners' emotional states, and this capacity has driven a surge of research interest in AI-based affective music generation in recent years. Many existing systems, however, are a black box which are not directly controllable, thus making these systems less flexible and adaptive to users. We present \\textit{AffectMachine-Pop}, an expert system capable of generating retro-pop music according to arousal and valence values, which can either be pre-determined or based on a listener's real-time emotion states. To validate the efficacy of the system, we conducted a listening study demonstrating that AffectMachine-Pop is capable of generating affective music at target levels of arousal and valence. The system is tailored for use either as a tool for generating interactive affective music based on user input, or for incorporation into biofeedback or neurofeedback systems to assist users with emotion self-regulation."
      },
      {
        "id": "oai:arXiv.org:2506.08249v1",
        "title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data",
        "link": "https://arxiv.org/abs/2506.08249",
        "author": "Ken Gu, Zhihan Zhang, Kate Lin, Yuwei Zhang, Akshay Paruchuri, Hong Yu, Mehran Kazemi, Kumar Ayush, A. Ali Heydari, Maxwell A. Xu, Girish Narayanswamy, Yun Liu, Ming-Zher Poh, Yuzhe Yang, Mark Malhotra, Shwetak Patel, Hamid Palangi, Xuhai Xu, Daniel McDuff, Tim Althoff, Xin Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08249v1 Announce Type: cross \nAbstract: Language models (LMs) are increasingly being deployed to perform autonomous data analyses. However, their data awareness -- the ability to recognize, reason over, and appropriately handle data artifacts such as missing values, outliers, and logical inconsistencies -- remains underexplored. These artifacts are especially common in real-world tabular data and, if mishandled, can significantly compromise the validity of analytical conclusions. To address this gap, we present RADAR, a benchmark for systematically evaluating data-aware reasoning on tabular data. We develop a framework to simulate data artifacts via programmatic perturbations to enable targeted evaluation of model behavior. RADAR comprises 2980 table query pairs, grounded in real-world data spanning 9 domains and 5 data artifact types. In addition to evaluating artifact handling, RADAR systematically varies table size to study how reasoning performance holds when increasing table size. Our evaluation reveals that, despite decent performance on tables without data artifacts, frontier models degrade significantly when data artifacts are introduced, exposing critical gaps in their capacity for robust, data-aware analysis. Designed to be flexible and extensible, RADAR supports diverse perturbation types and controllable table sizes, offering a valuable resource for advancing tabular reasoning."
      },
      {
        "id": "oai:arXiv.org:2506.08258v1",
        "title": "Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era",
        "link": "https://arxiv.org/abs/2506.08258",
        "author": "Lorenzo Arboit, Dennis N. Schneider, Toby Collins, Daniel A. Hashimoto, Silvana Perretta, Bernard Dallemagne, Jacques Marescaux, EAES Working Group, Nicolas Padoy, Pietro Mascagni",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08258v1 Announce Type: cross \nAbstract: Artificial Intelligence (AI) is transforming medicine, with generative AI models like ChatGPT reshaping perceptions of its potential. This study examines surgeons' awareness, expectations, and involvement with AI in surgery through comparative surveys conducted in 2021 and 2024. Two cross-sectional surveys were distributed globally in 2021 and 2024, the first before an IRCAD webinar and the second during the annual EAES meeting. The surveys assessed demographics, AI awareness, expectations, involvement, and ethics (2024 only). The surveys collected a total of 671 responses from 98 countries, 522 in 2021 and 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in 2024, while course attendance increased from 12.9% to 23%. Despite this, familiarity with foundational AI concepts remained limited. Expectations for AI's role shifted in 2024, with hospital management gaining relevance. Ethical concerns gained prominence, with 87.2% of 2024 participants emphasizing accountability and transparency. Infrastructure limitations remained the primary obstacle to implementation. Interdisciplinary collaboration and structured training were identified as critical for successful AI adoption. Optimism about AI's transformative potential remained high, with 79.9% of respondents believing AI would positively impact surgery and 96.6% willing to integrate AI into their clinical practice. Surgeons' perceptions of AI are evolving, driven by the rise of generative AI and advancements in surgical data science. While enthusiasm for integration is strong, knowledge gaps and infrastructural challenges persist. Addressing these through education, ethical frameworks, and infrastructure development is essential."
      },
      {
        "id": "oai:arXiv.org:2506.08263v1",
        "title": "Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming",
        "link": "https://arxiv.org/abs/2506.08263",
        "author": "Pouya Agheli, Tugce Kobal, Fran\\c{c}ois Durand, Matthew Andrews",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08263v1 Announce Type: cross \nAbstract: We investigate the multiuser scheduling problem in multiple-input multiple-output (MIMO) systems using orthogonal frequency division multiplexing (OFDM) and hybrid beamforming in which a base station (BS) communicates with multiple users over millimeter wave (mmWave) channels in the downlink. Improved scheduling is critical for enhancing spectral efficiency and the long-term performance of the system from the perspective of proportional fairness (PF) metric in hybrid beamforming systems due to its limited multiplexing gain. Our objective is to maximize PF by properly designing the analog and digital precoders within the hybrid beamforming and selecting the users subject to the number of radio frequency (RF) chains. Leveraging the characteristics of mmWave channels, we apply a two-timescale protocol. On a long timescale, we assign an analog beam to each user. Scheduling the users and designing the digital precoder are done accordingly on a short timescale. To conduct scheduling, we propose combinatorial solutions, such as greedy and sorting algorithms, followed by a machine learning (ML) approach. Our numerical results highlight the trade-off between the performance and complexity of the proposed approaches. Consequently, we show that the choice of approach depends on the specific criteria within a given scenario."
      },
      {
        "id": "oai:arXiv.org:2506.08276v1",
        "title": "LEANN: A Low-Storage Vector Index",
        "link": "https://arxiv.org/abs/2506.08276",
        "author": "Yichuan Wang, Shu Liu, Zhifei Li, Yongji Wu, Ziming Mao, Yilong Zhao, Xiao Yan, Zhiying Xu, Yang Zhou, Ion Stoica, Sewon Min, Matei Zaharia, Joseph E. Gonzalez",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08276v1 Announce Type: cross \nAbstract: Embedding-based search is widely used in applications such as recommendation and retrieval-augmented generation (RAG). Recently, there is a growing demand to support these capabilities over personal data stored locally on devices. However, maintaining the necessary data structure associated with the embedding-based search is often infeasible due to its high storage overhead. For example, indexing 100 GB of raw data requires 150 to 700 GB of storage, making local deployment impractical. Reducing this overhead while maintaining search quality and latency becomes a critical challenge. In this paper, we present LEANN, a storage-efficient approximate nearest neighbor (ANN) search index optimized for resource-constrained personal devices. LEANN combines a compact graph-based structure with an efficient on-the-fly recomputation strategy to enable fast and accurate retrieval with minimal storage overhead. Our evaluation shows that LEANN reduces index size to under 5% of the original raw data, achieving up to 50 times smaller storage than standard indexes, while maintaining 90% top-3 recall in under 2 seconds on real-world question answering benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.08277v1",
        "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain",
        "link": "https://arxiv.org/abs/2506.08277",
        "author": "Subba Reddy Oota, Khushbu Pahwa, Prachi Jindal, Satya Sai Srinath Namburi, Maneesh Singh, Tanmoy Chakraborty, Bapi S. Raju, Manish Gupta",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08277v1 Announce Type: cross \nAbstract: Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models in both unimodal and multimodal stimulus settings. More recently, instruction-tuned multimodal models have shown to generate task-specific representations that align strongly with brain activity. However, prior work evaluating the brain alignment of MLLMs has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigated brain alignment, that is, measuring the degree of predictivity of neural activity recorded while participants were watching naturalistic movies (video along with audio) with representations derived from MLLMs. We utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in improving the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both the systems. We make the code publicly available [https://github.com/subbareddy248/mllm_videos]."
      },
      {
        "id": "oai:arXiv.org:2506.08280v1",
        "title": "Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing",
        "link": "https://arxiv.org/abs/2506.08280",
        "author": "Daniel H. Pak, Shubh Thaker, Kyle Baylous, Xiaoran Zhang, Danny Bluestein, James S. Duncan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08280v1 Announce Type: cross \nAbstract: High-quality volumetric meshing from medical images is a key bottleneck for physics-based simulations in personalized medicine. For volumetric meshing of complex medical structures, recent studies have often utilized deep learning (DL)-based template deformation approaches to enable fast test-time generation with high spatial accuracy. However, these approaches still exhibit limitations, such as limited flexibility at high-curvature areas and unrealistic inter-part distances. In this study, we introduce a simple yet effective snap-and-tune strategy that sequentially applies DL and test-time optimization, which combines fast initial shape fitting with more detailed sample-specific mesh corrections. Our method provides significant improvements in both spatial accuracy and mesh quality, while being fully automated and requiring no additional training labels. Finally, we demonstrate the versatility and usefulness of our newly generated meshes via solid mechanics simulations in two different software platforms. Our code is available at https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh."
      },
      {
        "id": "oai:arXiv.org:2506.08325v1",
        "title": "Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces",
        "link": "https://arxiv.org/abs/2506.08325",
        "author": "Marcos Matabuena, Rahul Ghosal, Pavlo Mozharovskyi, Oscar Hernan Madrid Padilla, Jukka-Pekka Onnela",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08325v1 Announce Type: cross \nAbstract: Depth measures are powerful tools for defining level sets in emerging, non--standard, and complex random objects such as high-dimensional multivariate data, functional data, and random graphs. Despite their favorable theoretical properties, the integration of depth measures into regression modeling to provide prediction regions remains a largely underexplored area of research. To address this gap, we propose a novel, model-free uncertainty quantification algorithm based on conditional depth measures--specifically, conditional kernel mean embeddings and an integrated depth measure. These new algorithms can be used to define prediction and tolerance regions when predictors and responses are defined in separable Hilbert spaces. The use of kernel mean embeddings ensures faster convergence rates in prediction region estimation. To enhance the practical utility of the algorithms with finite samples, we also introduce a conformal prediction variant that provides marginal, non-asymptotic guarantees for the derived prediction regions. Additionally, we establish both conditional and unconditional consistency results, as well as fast convergence rates in certain homoscedastic settings. We evaluate the finite--sample performance of our model in extensive simulation studies involving various types of functional data and traditional Euclidean scenarios. Finally, we demonstrate the practical relevance of our approach through a digital health application related to physical activity, aiming to provide personalized recommendations"
      },
      {
        "id": "oai:arXiv.org:2506.08334v1",
        "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos",
        "link": "https://arxiv.org/abs/2506.08334",
        "author": "Weikun Peng, Jun Lv, Cewu Lu, Manolis Savva",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08334v1 Announce Type: cross \nAbstract: Articulated objects are prevalent in daily life. Understanding their kinematic structure and reconstructing them have numerous applications in embodied AI and robotics. However, current methods require carefully captured data for training or inference, preventing practical, scalable, and generalizable reconstruction of articulated objects. We focus on reconstruction of an articulated object from a casually captured RGBD video shot with a hand-held camera. A casually captured video of an interaction with an articulated object is easy to acquire at scale using smartphones. However, this setting is quite challenging, as the object and camera move simultaneously and there are significant occlusions as the person interacts with the object. To tackle these challenges, we introduce a coarse-to-fine framework that infers joint parameters and segments movable parts of the object from a dynamic RGBD video. To evaluate our method under this new setting, we build a 20$\\times$ larger synthetic dataset of 784 videos containing 284 objects across 11 categories. We compare our approach with existing methods that also take video as input. Experiments show that our method can reconstruct synthetic and real articulated objects across different categories from dynamic RGBD videos, outperforming existing methods significantly."
      },
      {
        "id": "oai:arXiv.org:2506.08336v1",
        "title": "Your Agent Can Defend Itself against Backdoor Attacks",
        "link": "https://arxiv.org/abs/2506.08336",
        "author": "Li Changjiang, Liang Jiacheng, Cao Bochuan, Chen Jinghui, Wang Ting",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08336v1 Announce Type: cross \nAbstract: Despite their growing adoption across domains, large language model (LLM)-powered agents face significant security risks from backdoor attacks during training and fine-tuning. These compromised agents can subsequently be manipulated to execute malicious operations when presented with specific triggers in their inputs or environments. To address this pressing risk, we present ReAgent, a novel defense against a range of backdoor attacks on LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies among the user's instruction, the agent's planning, and its execution. Drawing on this insight, ReAgent employs a two-level approach to detect potential backdoors. At the execution level, ReAgent verifies consistency between the agent's thoughts and actions; at the planning level, ReAgent leverages the agent's capability to reconstruct the instruction based on its thought trajectory, checking for consistency between the reconstructed instruction and the user's instruction. Extensive evaluation demonstrates ReAgent's effectiveness against various backdoor attacks across tasks. For instance, ReAgent reduces the attack success rate by up to 90\\% in database operation tasks, outperforming existing defenses by large margins. This work reveals the potential of utilizing compromised agents themselves to mitigate backdoor risks."
      },
      {
        "id": "oai:arXiv.org:2506.08338v1",
        "title": "midr: Learning from Black-Box Models by Maximum Interpretation Decomposition",
        "link": "https://arxiv.org/abs/2506.08338",
        "author": "Ryoichi Asashiba, Reiji Kozuma, Hirokazu Iwasawa",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08338v1 Announce Type: cross \nAbstract: The use of appropriate methods of Interpretable Machine Learning (IML) and eXplainable Artificial Intelligence (XAI) is essential for adopting black-box predictive models in fields where model and prediction explainability is required. As a novel tool for interpreting black-box models, we introduce the R package midr, which implements Maximum Interpretation Decomposition (MID). MID is a functional decomposition approach that derives a low-order additive representation of a black-box model by minimizing the squared error between the model's prediction function and this additive representation. midr enables learning from black-box models by constructing a global surrogate model with advanced analytical capabilities. After reviewing related work and the theoretical foundation of MID, we demonstrate the package's usage and discuss some of its key features."
      },
      {
        "id": "oai:arXiv.org:2506.08344v1",
        "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08344",
        "author": "Ne\\c{s}et \\\"Unver Akmandor, Sarvesh Prajapati, Mark Zolotas, Ta\\c{s}k{\\i}n Pad{\\i}r",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08344v1 Announce Type: cross \nAbstract: Traditional motion planning methods for robots with many degrees-of-freedom, such as mobile manipulators, are often computationally prohibitive for real-world settings. In this paper, we propose a novel multi-model motion planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear Model Predictive Control (NMPC). Re4MPC generates trajectories in a computationally efficient manner by reactively selecting the model, cost, and constraints of the NMPC problem depending on the complexity of the task and robot state. The policy for this reactive decision-making is learned via a Deep Reinforcement Learning (DRL) framework. We introduce a mathematical formulation to integrate NMPC into this DRL framework. To validate our methodology and design choices, we evaluate DRL training and test outcomes in a physics-based simulation involving a mobile manipulator. Experimental results demonstrate that Re4MPC is more computationally efficient and achieves higher success rates in reaching end-effector goals than the NMPC baseline, which computes whole-body trajectories without our learning mechanism."
      },
      {
        "id": "oai:arXiv.org:2506.08350v1",
        "title": "Complex-Valued Holographic Radiance Fields",
        "link": "https://arxiv.org/abs/2506.08350",
        "author": "Yicheng Zhan, Dong-Ha Shin, Seung-Hwan Baek, Kaan Ak\\c{s}it",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08350v1 Announce Type: cross \nAbstract: Modeling the full properties of light, including both amplitude and phase, in 3D representations is crucial for advancing physically plausible rendering, particularly in holographic displays. To support these features, we propose a novel representation that optimizes 3D scenes without relying on intensity-based intermediaries. We reformulate 3D Gaussian splatting with complex-valued Gaussian primitives, expanding support for rendering with light waves. By leveraging RGBD multi-view images, our method directly optimizes complex-valued Gaussians as a 3D holographic scene representation. This eliminates the need for computationally expensive hologram re-optimization. Compared with state-of-the-art methods, our method achieves 30x-10,000x speed improvements while maintaining on-par image quality, representing a first step towards geometrically aligned, physically plausible holographic scene representations."
      },
      {
        "id": "oai:arXiv.org:2506.08362v1",
        "title": "Solving Convex-Concave Problems with $\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$ Second-Order Oracle Complexity",
        "link": "https://arxiv.org/abs/2506.08362",
        "author": "Lesi Chen, Chengchang Liu, Luo Luo, Jingzhao Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08362v1 Announce Type: cross \nAbstract: Previous algorithms can solve convex-concave minimax problems $\\min_{x \\in \\mathcal{X}} \\max_{y \\in \\mathcal{Y}} f(x,y)$ with $\\mathcal{O}(\\epsilon^{-2/3})$ second-order oracle calls using Newton-type methods. This result has been speculated to be optimal because the upper bound is achieved by a natural generalization of the optimal first-order method. In this work, we show an improved upper bound of $\\tilde{\\mathcal{O}}(\\epsilon^{-4/7})$ by generalizing the optimal second-order method for convex optimization to solve the convex-concave minimax problem. We further apply a similar technique to lazy Hessian algorithms and show that our proposed algorithm can also be seen as a second-order ``Catalyst'' framework (Lin et al., JMLR 2018) that could accelerate any globally convergent algorithms for solving minimax problems."
      },
      {
        "id": "oai:arXiv.org:2506.08381v1",
        "title": "TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses",
        "link": "https://arxiv.org/abs/2506.08381",
        "author": "He Yang, Fei Ren, Hai-Sui Yu, Xueyu Geng, Pei-Zhi Zhuang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08381v1 Announce Type: cross \nAbstract: Accuracy and efficiency of the conventional physics-informed neural network (PINN) need to be improved before it can be a competitive alternative for soil consolidation analyses. This paper aims to overcome these limitations by proposing a highly accurate and efficient physics-informed machine learning (PIML) approach, termed time-stepping physics-informed extreme learning machine (TS-PIELM). In the TS-PIELM framework the consolidation process is divided into numerous time intervals, which helps overcome the limitation of PIELM in solving differential equations with sharp gradients. To accelerate network training, the solution is approximated by a single-layer feedforward extreme learning machine (ELM), rather than using a fully connected neural network in PINN. The input layer weights of the ELM network are generated randomly and fixed during the training process. Subsequently, the output layer weights are directly computed by solving a system of linear equations, which significantly enhances the training efficiency compared to the time-consuming gradient descent method in PINN. Finally, the superior performance of TS-PIELM is demonstrated by solving three typical Terzaghi consolidation problems. Compared to PINN, results show that the computational efficiency and accuracy of the novel TS-PIELM framework are improved by more than 1000 times and 100 times for one-dimensional cases, respectively. This paper provides compelling evidence that PIML can be a powerful tool for computational geotechnics."
      },
      {
        "id": "oai:arXiv.org:2506.08399v1",
        "title": "SafeCoT: Improving VLM Safety with Minimal Reasoning",
        "link": "https://arxiv.org/abs/2506.08399",
        "author": "Jiachen Ma, Zhanhui Zhou, Chao Yang, Chaochao Lu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08399v1 Announce Type: cross \nAbstract: Ensuring safe and appropriate responses from vision-language models (VLMs) remains a critical challenge, particularly in high-risk or ambiguous scenarios. We introduce SafeCoT, a lightweight, interpretable framework that leverages rule-based chain-of-thought (CoT) supervision to improve refusal behavior in VLMs. Unlike prior methods that rely on large-scale safety annotations or complex modeling, SafeCoT uses minimal supervision to help models reason about safety risks and make context-aware refusals. Experiments across multiple benchmarks show that SafeCoT significantly reduces overrefusal and enhances generalization, even with limited training data. Our approach offers a scalable solution for aligning VLMs with safety-critical objectives."
      },
      {
        "id": "oai:arXiv.org:2506.08423v1",
        "title": "Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy",
        "link": "https://arxiv.org/abs/2506.08423",
        "author": "Utkarsh Pratiush, Austin Houston, Kamyar Barakati, Aditya Raghavan, Dasol Yoon, Harikrishnan KP, Zhaslan Baraissov, Desheng Ma, Samuel S. Welborn, Mikolaj Jakowski, Shawn-Patrick Barhorst, Alexander J. Pattison, Panayotis Manganaris, Sita Sirisha Madugula, Sai Venkata Gayathri Ayyagari, Vishal Kennedy, Ralph Bulanadi, Michelle Wang, Kieran J. Pang, Ian Addison-Smith, Willy Menacho, Horacio V. Guzman, Alexander Kiefer, Nicholas Furth, Nikola L. Kolev, Mikhail Petrov, Viktoriia Liu, Sergey Ilyev, Srikar Rairao, Tommaso Rodani, Ivan Pinto-Huguet, Xuli Chen, Josep Crua\\~nes, Marta Torrens, Jovan Pomar, Fanzhi Su, Pawan Vedanti, Zhiheng Lyu, Xingzhi Wang, Lehan Yao, Amir Taqieddin, Forrest Laskowski, Xiangyu Yin, Yu-Tsun Shao, Benjamin Fein-Ashley, Yi Jiang, Vineet Kumar, Himanshu Mishra, Yogesh Paul, Adib Bazgir, Rama chandra Praneeth Madugula, Yuwen Zhang, Pravan Omprakash, Jian Huang, Eric Montufar-Morales, Vivek Chawla, Harshit Sethi, Jie Huang, Lauri Kurki, Grace Guinan, Addison Salvador, Arman Ter-Petrosyan, Madeline Van Winkle, Steven R. Spurgeon, Ganesh Narasimha, Zijie Wu, Richard Liu, Yongtao Liu, Boris Slautin, Andrew R Lupini, Rama Vasudevan, Gerd Duscher, Sergei V. Kalinin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08423v1 Announce Type: cross \nAbstract: Microscopy is a primary source of information on materials structure and functionality at nanometer and atomic scales. The data generated is often well-structured, enriched with metadata and sample histories, though not always consistent in detail or format. The adoption of Data Management Plans (DMPs) by major funding agencies promotes preservation and access. However, deriving insights remains difficult due to the lack of standardized code ecosystems, benchmarks, and integration strategies. As a result, data usage is inefficient and analysis time is extensive. In addition to post-acquisition analysis, new APIs from major microscope manufacturers enable real-time, ML-based analytics for automated decision-making and ML-agent-controlled microscope operation. Yet, a gap remains between the ML and microscopy communities, limiting the impact of these methods on physics, materials discovery, and optimization. Hackathons help bridge this divide by fostering collaboration between ML researchers and microscopy experts. They encourage the development of novel solutions that apply ML to microscopy, while preparing a future workforce for instrumentation, materials science, and applied ML. This hackathon produced benchmark datasets and digital twins of microscopes to support community growth and standardized workflows. All related code is available at GitHub: https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1"
      },
      {
        "id": "oai:arXiv.org:2506.08428v1",
        "title": "Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings",
        "link": "https://arxiv.org/abs/2506.08428",
        "author": "Evan Markou, Thalaiyasingam Ajanthan, Stephen Gould",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08428v1 Announce Type: cross \nAbstract: Many high-dimensional optimisation problems exhibit rich geometric structures in their set of minimisers, often forming smooth manifolds due to over-parametrisation or symmetries. When this structure is known, at least locally, it can be exploited through reduction mappings that reparametrise part of the parameter space to lie on the solution manifold. These reductions naturally arise from inner optimisation problems and effectively remove redundant directions, yielding a lower-dimensional objective. In this work, we introduce a general framework to understand how such reductions influence the optimisation landscape. We show that well-designed reduction mappings improve curvature properties of the objective, leading to better-conditioned problems and theoretically faster convergence for gradient-based methods. Our analysis unifies a range of scenarios where structural information at optimality is leveraged to accelerate convergence, offering a principled explanation for the empirical gains observed in such optimisation algorithms."
      },
      {
        "id": "oai:arXiv.org:2506.08443v1",
        "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills",
        "link": "https://arxiv.org/abs/2506.08443",
        "author": "Kazuki Kawamura, Jun Rekimoto",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08443v1 Announce Type: cross \nAbstract: While current AI illustration tools can generate high-quality images from text prompts, they rarely reveal the step-by-step procedure that human artists follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based image generation with a large-language-model tutor. At each stage, novices receive real-time feedback on anatomy, perspective, and composition, revise any step non-linearly, and branch alternative versions. By exposing intermediate outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box generator into a scaffolded learning environment that supports both creative exploration and skills acquisition."
      },
      {
        "id": "oai:arXiv.org:2506.08446v1",
        "title": "A Survey on Large Language Models for Mathematical Reasoning",
        "link": "https://arxiv.org/abs/2506.08446",
        "author": "Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, Yang Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08446v1 Announce Type: cross \nAbstract: Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and \"test-time scaling\". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains."
      },
      {
        "id": "oai:arXiv.org:2506.08448v1",
        "title": "Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions",
        "link": "https://arxiv.org/abs/2506.08448",
        "author": "Hyakka Nakada, Shu Tanaka",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08448v1 Announce Type: cross \nAbstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization problems whose objective functions are represented by Quadratic Unconstrained Binary Optimization (QUBO) formulations. For broader applicability of QA, quadratization methods are used to transform higher-order problems into QUBOs. However, quadratization methods for complex problems involving Machine Learning (ML) remain largely unknown. In these problems, strong nonlinearity and dense interactions prevent conventional methods from being applied. Therefore, we model target functions by the sum of rectified linear unit bases, which not only have the ability of universal approximation, but also have an equivalent quadratic-polynomial representation. In this study, the proof of concept is verified both numerically and analytically. In addition, by combining QA with the proposed quadratization, we design a new black-box optimization scheme, in which ML surrogate regressors are inputted to QA after the quadratization process."
      },
      {
        "id": "oai:arXiv.org:2506.08455v1",
        "title": "The interplay of robustness and generalization in quantum machine learning",
        "link": "https://arxiv.org/abs/2506.08455",
        "author": "Julian Berberich, Tobias Fellner, Christian Holm",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08455v1 Announce Type: cross \nAbstract: While adversarial robustness and generalization have individually received substantial attention in the recent literature on quantum machine learning, their interplay is much less explored. In this chapter, we address this interplay for variational quantum models, which were recently proposed as function approximators in supervised learning. We discuss recent results quantifying both robustness and generalization via Lipschitz bounds, which explicitly depend on model parameters. Thus, they give rise to a regularization-based training approach for robust and generalizable quantum models, highlighting the importance of trainable data encoding strategies. The practical implications of the theoretical results are demonstrated with an application to time series analysis."
      },
      {
        "id": "oai:arXiv.org:2506.08507v1",
        "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08507",
        "author": "Kuo Yang, Xingjie Yang, Linhui Yu, Qing Xu, Yan Fang, Xu Wang, Zhengyang Zhou, Yang Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08507v1 Announce Type: cross \nAbstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality."
      },
      {
        "id": "oai:arXiv.org:2506.08518v1",
        "title": "FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching",
        "link": "https://arxiv.org/abs/2506.08518",
        "author": "Sunny Gupta, Nikita Jangid, Shounak Das, Amit Sethi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08518v1 Announce Type: cross \nAbstract: Domain Generalization (DG) seeks to train models that perform reliably on unseen target domains without access to target data during training. While recent progress in smoothing the loss landscape has improved generalization, existing methods often falter under long-tailed class distributions and conflicting optimization objectives. We introduce FedTAIL, a federated domain generalization framework that explicitly addresses these challenges through sharpness-guided, gradient-aligned optimization. Our method incorporates a gradient coherence regularizer to mitigate conflicts between classification and adversarial objectives, leading to more stable convergence. To combat class imbalance, we perform class-wise sharpness minimization and propose a curvature-aware dynamic weighting scheme that adaptively emphasizes underrepresented tail classes. Furthermore, we enhance conditional distribution alignment by integrating sharpness-aware perturbations into entropy regularization, improving robustness under domain shift. FedTAIL unifies optimization harmonization, class-aware regularization, and conditional alignment into a scalable, federated-compatible framework. Extensive evaluations across standard domain generalization benchmarks demonstrate that FedTAIL achieves state-of-the-art performance, particularly in the presence of domain shifts and label imbalance, validating its effectiveness in both centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail"
      },
      {
        "id": "oai:arXiv.org:2506.08519v1",
        "title": "Graph signal aware decomposition of dynamic networks via latent graphs",
        "link": "https://arxiv.org/abs/2506.08519",
        "author": "Bishwadeep Das, Andrei Buciulea, Antonio G. Marques, Elvin Isufi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08519v1 Announce Type: cross \nAbstract: Dynamics on and of networks refer to changes in topology and node-associated signals, respectively and are pervasive in many socio-technological systems, including social, biological, and infrastructure networks. Due to practical constraints, privacy concerns, or malfunctions, we often observe only a fraction of the topological evolution and associated signal, which not only hinders downstream tasks but also restricts our analysis of network evolution. Such aspects could be mitigated by moving our attention at the underlying latent driving factors of the network evolution, which can be naturally uncovered via low-rank tensor decomposition. Tensor-based methods provide a powerful means of uncovering the underlying factors of network evolution through low-rank decompositions. However, the extracted embeddings typically lack a relational structure and are obtained independently from the node signals. This disconnect reduces the interpretability of the embeddings and overlooks the coupling between topology and signals. To address these limitations, we propose a novel two-way decomposition to represent a dynamic graph topology, where the structural evolution is captured by a linear combination of latent graph adjacency matrices reflecting the overall joint evolution of both the topology and the signal. Using spatio-temporal data, we estimate the latent adjacency matrices and their temporal scaling signatures via alternating minimization, and prove that our approach converges to a stationary point. Numerical results show that the proposed method recovers individually and collectively expressive latent graphs, outperforming both standard tensor-based decompositions and signal-based topology identification methods in reconstructing the missing network especially when observations are limited."
      },
      {
        "id": "oai:arXiv.org:2506.08520v1",
        "title": "Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models",
        "link": "https://arxiv.org/abs/2506.08520",
        "author": "Srinivasan Kidambi, Pravin Nair",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08520v1 Announce Type: cross \nAbstract: Multi-head self-attention (MHSA) has become a core component in modern computer vision models. However, its quadratic complexity with respect to input length poses a significant computational bottleneck in real-time and resource constrained environments. We propose PnP-Nystra, a Nystr\\\"om based linear approximation of self-attention, developed as a plug-and-play (PnP) module that can be integrated into the pre-trained image and video restoration models without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables efficient acceleration in various window-based transformer architectures, including SwinIR, Uformer, and RVRT. Our experiments across diverse image and video restoration tasks, including denoising, deblurring, and super-resolution, demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU and a 2-5x speed-up on CPU inference. Despite these significant gains, the method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To the best of our knowledge, we are the first to demonstrate a linear attention functioning as a training-free substitute for MHSA in restoration models."
      },
      {
        "id": "oai:arXiv.org:2506.08524v1",
        "title": "Teaching Physical Awareness to LLMs through Sounds",
        "link": "https://arxiv.org/abs/2506.08524",
        "author": "Weiguo Wang, Andy Nie, Wenrui Zhou, Yi Kai, Chengchen Hu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08524v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, yet they fundamentally lack physical awareness--understanding of real-world physical phenomena. In this work, we present ACORN, a framework that teaches LLMs physical awareness through sound, focusing on fundamental physical phenomena like the Doppler effect, multipath effect, and spatial relationships. To overcome data scarcity, ACORN introduce a physics-based simulator combining real-world sound sources with controlled physical channels to generate diverse training data. Using this simulator, we build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an audio encoder that processes both magnitude and phase information. By connecting our audio encoder to state-of-the-art LLMs, we demonstrate reasonable results in both simulated and real-world tasks, such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, paving the way for enabling LLMs to understand physical world."
      },
      {
        "id": "oai:arXiv.org:2506.08528v1",
        "title": "PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production",
        "link": "https://arxiv.org/abs/2506.08528",
        "author": "Yu Guan, Zhiyu Yin, Haoyu Chen, Sheng Cheng, Chaojie Yang, Tianyin Xu, Yang Zhang, Hanyu Zhao, Yong Li, Dennis Cai, Ennan Zhai",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08528v1 Announce Type: cross \nAbstract: Troubleshooting performance problems of large model training (LMT) is immensely challenging, due to unprecedented scales of modern GPU clusters, the complexity of software-hardware interactions, and the data intensity of the training process. Existing troubleshooting approaches designed for traditional distributed systems or datacenter networks fall short and can hardly apply to real-world training systems. In this paper, we present PerfTracker, the first online troubleshooting system utilizing fine-grained profiling, to diagnose performance issues of large-scale model training in production. PerfTracker can diagnose performance issues rooted in both hardware (e.g., GPUs and their interconnects) and software (e.g., Python functions and GPU operations). It scales to LMT on modern GPU clusters. PerfTracker effectively summarizes runtime behavior patterns of fine-grained LMT functions via online profiling, and leverages differential observability to localize the root cause with minimal production impact. PerfTracker has been deployed as a production service for large-scale GPU clusters of O(10, 000) GPUs (product homepage https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool). It has been used to diagnose a variety of difficult performance issues."
      },
      {
        "id": "oai:arXiv.org:2506.08534v1",
        "title": "DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View",
        "link": "https://arxiv.org/abs/2506.08534",
        "author": "Donglian Li, Hui Guo, Minglang Chen, Huizhen Chen, Jialing Chen, Bocheng Liang, Pengchen Liang, Ying Tan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08534v1 Announce Type: cross \nAbstract: Accurate segmentation of anatomical structures in the apical four-chamber (A4C) view of fetal echocardiography is essential for early diagnosis and prenatal evaluation of congenital heart disease (CHD). However, precise segmentation remains challenging due to ultrasound artifacts, speckle noise, anatomical variability, and boundary ambiguity across different gestational stages. To reduce the workload of sonographers and enhance segmentation accuracy, we propose DCD, an advanced deep learning-based model for automatic segmentation of key anatomical structures in the fetal A4C view. Our model incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module, enabling superior multi-scale feature extraction, and a Convolutional Block Attention Module (CBAM) to enhance adaptive feature representation. By effectively capturing both local and global contextual information, DCD achieves precise and robust segmentation, contributing to improved prenatal cardiac assessment."
      },
      {
        "id": "oai:arXiv.org:2506.08535v1",
        "title": "Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation",
        "link": "https://arxiv.org/abs/2506.08535",
        "author": "Ronald Katende",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08535v1 Announce Type: cross \nAbstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of the form $A \\approx P D Q$, where $P \\in \\mathbb{R}^{n \\times k}$, $D \\in \\mathbb{R}^{k \\times k}$, and $Q \\in \\mathbb{R}^{k \\times n}$. The decomposition is defined variationally by minimizing a regularized Frobenius loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic factorizations such as LU or SVD, it is computed by alternating minimization. We establish existence and perturbation stability of the solution and show that each update has complexity $\\mathcal{O}(n^2k)$. Benchmarks against truncated SVD, CUR, and nonnegative matrix factorization show improved reconstruction accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices, particularly under sparsity and noise."
      },
      {
        "id": "oai:arXiv.org:2506.08548v1",
        "title": "Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification",
        "link": "https://arxiv.org/abs/2506.08548",
        "author": "Moria Mayala (LPSM), Erwan Scornet (LPSM), Charles Tillier (LMV), Olivier Wintenberger (LPSM)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08548v1 Announce Type: cross \nAbstract: Many classification tasks involve imbalanced data, in which a class is largely underrepresented. Several techniques consists in creating a rebalanced dataset on which a classifier is trained. In this paper, we study theoretically such a procedure, when the classifier is a Centered Random Forests (CRF). We establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates and exact constant. We then prove that the CRF trained on the rebalanced dataset exhibits a bias, which can be removed with appropriate techniques. Based on an importance sampling (IS) approach, the resulting debiased estimator, called IS-ICRF, satisfies a CLT centered at the prediction function value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys a variance reduction compared to the ICRF trained on the original data. Therefore, our theoretical analysis highlights the benefits of training random forests on a rebalanced dataset (followed by a debiasing procedure) compared to using the original data. Our theoretical results, especially the variance rates and the variance reduction, appear to be valid for Breiman's random forests in our experiments."
      },
      {
        "id": "oai:arXiv.org:2506.08558v1",
        "title": "Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees",
        "link": "https://arxiv.org/abs/2506.08558",
        "author": "William de Vazelhes, Xiao-Tong Yuan, Bin Gu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08558v1 Announce Type: cross \nAbstract: In sparse optimization, enforcing hard constraints using the $\\ell_0$ pseudo-norm offers advantages like controlled sparsity compared to convex relaxations. However, many real-world applications demand not only sparsity constraints but also some extra constraints. While prior algorithms have been developed to address this complex scenario with mixed combinatorial and convex constraints, they typically require the closed form projection onto the mixed constraints which might not exist, and/or only provide local guarantees of convergence which is different from the global guarantees commonly sought in sparse optimization. To fill this gap, in this paper, we study the problem of sparse optimization with extra \\qw{\\textit{support-preserving}} constraints commonly encountered in the literature. We present a new variant of iterative hard-thresholding algorithm equipped with a two-step consecutive projection operator customized for these mixed constraints, serving as a simple alternative to the Euclidean projection onto the mixed constraint. By introducing a novel trade-off between sparsity relaxation and sub-optimality, we provide global guarantees in objective value for the output of our algorithm, in the deterministic, stochastic, and zeroth-order settings, under the conventional restricted strong-convexity/smoothness assumptions. As a fundamental contribution in proof techniques, we develop a novel extension of the classic three-point lemma to the considered two-step non-convex projection operator, which allows us to analyze the convergence in objective value in an elegant way that has not been possible with existing techniques. In the zeroth-order case, such technique also improves upon the state-of-the-art result from de Vazelhes et. al. (2022), even in the case without additional constraints, by allowing us to remove a non-vanishing system error present in their work."
      },
      {
        "id": "oai:arXiv.org:2506.08570v1",
        "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation",
        "link": "https://arxiv.org/abs/2506.08570",
        "author": "Or Tal, Felix Kreuk, Yossi Adi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08570v1 Announce Type: cross \nAbstract: Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly across many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and pinpoint which design choices most influence performance. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM"
      },
      {
        "id": "oai:arXiv.org:2506.08594v1",
        "title": "Solving excited states for long-range interacting trapped ions with neural networks",
        "link": "https://arxiv.org/abs/2506.08594",
        "author": "Yixuan Ma, Chang Liu, Weikang Li, Shun-Yao Zhang, L. -M. Duan, Yukai Wu, Dong-Ling Deng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08594v1 Announce Type: cross \nAbstract: The computation of excited states in strongly interacting quantum many-body systems is of fundamental importance. Yet, it is notoriously challenging due to the exponential scaling of the Hilbert space dimension with the system size. Here, we introduce a neural network-based algorithm that can simultaneously output multiple low-lying excited states of a quantum many-body spin system in an accurate and efficient fashion. This algorithm, dubbed the neural quantum excited-state (NQES) algorithm, requires no explicit orthogonalization of the states and is generally applicable to higher dimensions. We demonstrate, through concrete examples including the Haldane-Shastry model with all-to-all interactions, that the NQES algorithm is capable of efficiently computing multiple excited states and their related observable expectations. In addition, we apply the NQES algorithm to two classes of long-range interacting trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying all-to-all interactions with alternating signs, our computed low-lying excited states bear spatial correlation patterns similar to those of the ground states, which closely match recent experimental observations that the quasi-adiabatically prepared state accurately reproduces analytical ground-state correlations. For a system of up to 300 ions with power-law decaying antiferromagnetic interactions, we successfully uncover its gap scaling and correlation features. Our results establish a scalable and efficient algorithm for computing excited states of interacting quantum many-body systems, which holds potential applications ranging from benchmarking quantum devices to photoisomerization."
      },
      {
        "id": "oai:arXiv.org:2506.08616v1",
        "title": "Generalizing while preserving monotonicity in comparison-based preference learning models",
        "link": "https://arxiv.org/abs/2506.08616",
        "author": "Julien Fageot, Peva Blanchard, Gilles Bareilles, L\\^e-Nguy\\^en Hoang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08616v1 Announce Type: cross \nAbstract: If you tell a learning model that you prefer an alternative $a$ over another alternative $b$, then you probably expect the model to be monotone, that is, the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps surprisingly, many widely deployed comparison-based preference learning models, including large language models, fail to have this guarantee. Until now, the only comparison-based preference learning algorithms that were proved to be monotone are the Generalized Bradley-Terry models. Yet, these models are unable to generalize to uncompared data. In this paper, we advance the understanding of the set of models with generalization ability that are monotone. Namely, we propose a new class of Linear Generalized Bradley-Terry models with Diffusion Priors, and identify sufficient conditions on alternatives' embeddings that guarantee monotonicity. Our experiments show that this monotonicity is far from being a general guarantee, and that our new class of generalizing models improves accuracy, especially when the dataset is limited."
      },
      {
        "id": "oai:arXiv.org:2506.08623v1",
        "title": "Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification",
        "link": "https://arxiv.org/abs/2506.08623",
        "author": "Rinat Prochii, Elizaveta Dakhova, Pavel Birulin, Maxim Sharaev",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08623v1 Announce Type: cross \nAbstract: Accurate classification of second-trimester fetal ultrasound images remains challenging due to low image quality, high intra-class variability, and significant class imbalance. In this work, we introduce a simple yet powerful, biologically inspired deep learning ensemble framework that-unlike prior studies focused on only a handful of anatomical targets-simultaneously distinguishes 16 fetal structures. Drawing on the hierarchical, modular organization of biological vision systems, our model stacks two complementary branches (a \"shallow\" path for coarse, low-resolution cues and a \"detailed\" path for fine, high-resolution features), concatenating their outputs for final prediction. To our knowledge, no existing method has addressed such a large number of classes with a comparably lightweight architecture. We trained and evaluated on 5,298 routinely acquired clinical images (annotated by three experts and reconciled via Dawid-Skene), reflecting real-world noise and variability rather than a \"cleaned\" dataset. Despite this complexity, our ensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies 90% of organs with accuracy > 0.75 and 75% of organs with accuracy > 0.85-performance competitive with more elaborate models applied to far fewer categories. These results demonstrate that biologically inspired modular stacking can yield robust, scalable fetal anatomy recognition in challenging clinical settings."
      },
      {
        "id": "oai:arXiv.org:2506.08633v1",
        "title": "Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs",
        "link": "https://arxiv.org/abs/2506.08633",
        "author": "\\v{S}imon Sedl\\'a\\v{c}ek, Bolaji Yusuf, J\\'an \\v{S}vec, Pradyoth Hegde, Santosh Kesiraju, Old\\v{r}ich Plchot, Jan \\v{C}ernock\\'y",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08633v1 Announce Type: cross \nAbstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging the representation spaces of speech encoders and LLMs via a small connector module, with a focus on fully open-sourced and open-data components (WavLM-large, OLMo). We focus on ablating different aspects of such systems including full/LoRA adapter fine-tuning, the effect of agent turns in the dialogue history, as well as fuzzy matching-based output post-processing, which greatly improves performance of our systems on named entities in the dialogue slot values. We conduct our experiments on the SpokenWOZ dataset, and additionally utilize the Speech-Aware MultiWOZ dataset to augment our training data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17% JGA on SpokenWOZ test."
      },
      {
        "id": "oai:arXiv.org:2506.08634v1",
        "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback",
        "link": "https://arxiv.org/abs/2506.08634",
        "author": "Alvaro Becerra, Daniel Andres, Pablo Villegas, Roberto Daza, Ruth Cobos",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08634v1 Announce Type: cross \nAbstract: In this article, we present a novel multimodal feedback framework called MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI), and Collaborative assessments for generating personalized feedback on student learning activities. This framework consists of four key steps. First, peers and professors' assessments are conducted through standardized rubrics (that include both quantitative and qualitative evaluations). Second, multimodal data are collected during learning activities, including video recordings, audio capture, gaze tracking, physiological signals (heart rate, motion data), and behavioral interactions. Third, personalized feedback is generated using AI, synthesizing human-based evaluations and data-based multimodal insights such as posture, speech patterns, stress levels, and cognitive load, among others. Finally, students review their own performance through video recordings and engage in self-assessment and feedback visualization, comparing their own evaluations with peers and professors' assessments, class averages, and AI-generated recommendations. By combining human-based and data-based evaluation techniques, this framework enables more accurate, personalized and actionable feedback. We tested MOSAIC-F in the context of improving oral presentation skills."
      },
      {
        "id": "oai:arXiv.org:2506.08654v1",
        "title": "A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck",
        "link": "https://arxiv.org/abs/2506.08654",
        "author": "Ciro Benito Raggio, Paolo Zaffino, Maria Francesca Spadea",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08654v1 Announce Type: cross \nAbstract: Shortened Abstract\n  Cone-beam computed tomography (CBCT) has become a widely adopted modality for image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise, limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield unit values and hindering direct dose calculation. Synthetic CT (sCT) generation from CBCT addresses these issues, especially using deep learning (DL) methods. Existing approaches are limited by institutional heterogeneity, scanner-dependent variations, and data privacy regulations that prevent multi-center data sharing.\n  To overcome these challenges, we propose a cross-silo horizontal federated learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region, extending our FedSynthCT framework. A conditional generative adversarial network was collaboratively trained on data from three European medical centers in the public SynthRAD2025 challenge dataset.\n  The federated model demonstrated effective generalization across centers, with mean absolute error (MAE) ranging from $64.38\\pm13.63$ to $85.90\\pm7.10$ HU, structural similarity index (SSIM) from $0.882\\pm0.022$ to $0.922\\pm0.039$, and peak signal-to-noise ratio (PSNR) from $32.86\\pm0.94$ to $34.91\\pm1.04$ dB. Notably, on an external validation dataset of 60 patients, comparable performance was achieved (MAE: $75.22\\pm11.81$ HU, SSIM: $0.904\\pm0.034$, PSNR: $33.52\\pm2.06$ dB) without additional training, confirming robust generalization despite protocol, scanner differences and registration errors.\n  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT synthesis while preserving data privacy and offer a collaborative solution for developing generalizable models across institutions without centralized data sharing or site-specific fine-tuning."
      },
      {
        "id": "oai:arXiv.org:2506.08670v1",
        "title": "sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation",
        "link": "https://arxiv.org/abs/2506.08670",
        "author": "Renjie Xu, Chong Wu, Maolin Che, Zhuoheng Ran, Yimin Wei, Hong Yan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08670v1 Announce Type: cross \nAbstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order principal component analysis (SHOPCA) that introduces a geometric perspective to high-dimensional tensor decomposition. By unfolding the input tensor along each mode and reformulating the resulting subproblems as structured binary linear optimization problems, our method transforms the original nonconvex sparse objective into a tractable geometric form. This eliminates the need for explicit covariance estimation and iterative deflation, enabling significant gains in both computational efficiency and interpretability, particularly in high-dimensional and unbalanced data scenarios. We theoretically establish the equivalence between the geometric subproblems and the original SHOPCA formulation, and derive worst-case approximation error bounds based on classical PCA residuals, providing data-dependent performance guarantees. The proposed algorithm achieves a total computational complexity of $O\\left(\\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\\right)$, which scales linearly with tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately recovers sparse supports in synthetic settings, preserves classification performance under 10$\\times$ compression, and achieves high-quality image reconstruction on ImageNet, highlighting its robustness and versatility."
      },
      {
        "id": "oai:arXiv.org:2506.08677v1",
        "title": "MAMBO: High-Resolution Generative Approach for Mammography Images",
        "link": "https://arxiv.org/abs/2506.08677",
        "author": "Milica \\v{S}kipina, Nikola Jovi\\v{s}i\\'c, Nicola Dall'Asen, Vanja \\v{S}venda, Anil Osman Tur, Slobodan Ili\\'c, Elisa Ricci, Dubravko \\'Culibrk",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08677v1 Announce Type: cross \nAbstract: Mammography is the gold standard for the detection and diagnosis of breast cancer. This procedure can be significantly enhanced with Artificial Intelligence (AI)-based software, which assists radiologists in identifying abnormalities. However, training AI systems requires large and diverse datasets, which are often difficult to obtain due to privacy and ethical constraints. To address this issue, the paper introduces MAMmography ensemBle mOdel (MAMBO), a novel patch-based diffusion approach designed to generate full-resolution mammograms. Diffusion models have shown breakthrough results in realistic image generation, yet few studies have focused on mammograms, and none have successfully generated high-resolution outputs required to capture fine-grained features of small lesions. To achieve this, MAMBO integrates separate diffusion models to capture both local and global (image-level) contexts. The contextual information is then fed into the final patch-based model, significantly aiding the noise removal process. This thoughtful design enables MAMBO to generate highly realistic mammograms of up to 3840x3840 pixels. Importantly, this approach can be used to enhance the training of classification models and extended to anomaly detection. Experiments, both numerical and radiologist validation, assess MAMBO's capabilities in image generation, super-resolution, and anomaly detection, highlighting its potential to enhance mammography analysis for more accurate diagnoses and earlier lesion detection."
      },
      {
        "id": "oai:arXiv.org:2506.08708v1",
        "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly",
        "link": "https://arxiv.org/abs/2506.08708",
        "author": "Liang Ma, Jiajun Wen, Min Lin, Rongtao Xu, Xiwen Liang, Bingqian Lin, Jun Ma, Yongxin Wang, Ziming Wei, Haokun Lin, Mingfei Han, Meng Cao, Bokui Chen, Ivan Laptev, Xiaodan Liang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08708v1 Announce Type: cross \nAbstract: While vision-language models (VLMs) have demonstrated promising capabilities in reasoning and planning for embodied agents, their ability to comprehend physical phenomena, particularly within structured 3D environments, remains severely limited. To close this gap, we introduce PhyBlock, a progressive benchmark designed to assess VLMs on physical understanding and planning through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level cognitive hierarchy assembly task alongside targeted Visual Question Answering (VQA) samples, collectively aimed at evaluating progressive spatial reasoning and fundamental physical comprehension, including object properties, spatial relationships, and holistic scene understanding. PhyBlock includes 2600 block tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three key dimensions: partial completion, failure diagnosis, and planning robustness. We benchmark 21 state-of-the-art VLMs, highlighting their strengths and limitations in physically grounded, multi-step planning. Our empirical findings indicate that the performance of VLMs exhibits pronounced limitations in high-level planning and reasoning capabilities, leading to a notable decline in performance for the growing complexity of the tasks. Error analysis reveals persistent difficulties in spatial orientation and dependency reasoning. Surprisingly, chain-of-thought prompting offers minimal improvements, suggesting spatial tasks heavily rely on intuitive model comprehension. We position PhyBlock as a unified testbed to advance embodied reasoning, bridging vision-language understanding and real-world physical problem-solving."
      },
      {
        "id": "oai:arXiv.org:2506.08716v1",
        "title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment",
        "link": "https://arxiv.org/abs/2506.08716",
        "author": "Maximilian Tschuchnig, Lukas Lamminger, Philipp Steininger, Michael Gadermayr",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08716v1 Announce Type: cross \nAbstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time intraoperative imaging due to its low radiation dose and high acquisition speed. However, despite its high resolution, CBCT suffers from significant artifacts and thereby lower visual quality, compared to conventional Computed Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT (sCT) generation, translating CBCT volumes into the CT domain. In this work, we enhance sCT generation through multimodal learning, integrating intraoperative CBCT with preoperative CT. Beyond validation on two real-world datasets, we use a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT quality affect sCT quality. The results demonstrate that multimodal sCT consistently outperform unimodal baselines, with the most significant gains observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate that these findings are highly reproducible in real-world clinical datasets."
      },
      {
        "id": "oai:arXiv.org:2506.08725v1",
        "title": "Stop Misusing t-SNE and UMAP for Visual Analytics",
        "link": "https://arxiv.org/abs/2506.08725",
        "author": "Hyeon Jeon, Jeongin Park, Sungbok Shin, Jinwook Seo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08725v1 Announce Type: cross \nAbstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly common. For example, although t-SNE and UMAP projections often do not faithfully reflect true distances between clusters, practitioners frequently use them to investigate inter-cluster relationships. In this paper, we bring this issue to the surface and comprehensively investigate why such misuse occurs and how to prevent it. We conduct a literature review of 114 papers to verify the prevalence of the misuse and analyze the reasonings behind it. We then execute an interview study to uncover practitioners' implicit motivations for using these techniques -- rationales often undisclosed in the literature. Our findings indicate that misuse of t-SNE and UMAP primarily stems from limited discourse on their appropriate use in visual analytics. We conclude by proposing future directions and concrete action items to promote more reasonable use of DR."
      },
      {
        "id": "oai:arXiv.org:2506.08734v1",
        "title": "Flexible and Efficient Drift Detection without Labels",
        "link": "https://arxiv.org/abs/2506.08734",
        "author": "Nelvin Tan, Yu-Ching Shih, Dong Yang, Amol Salunkhe",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08734v1 Announce Type: cross \nAbstract: Machine learning models are being increasingly used to automate decisions in almost every domain, and ensuring the performance of these models is crucial for ensuring high quality machine learning enabled services. Ensuring concept drift is detected early is thus of the highest importance. A lot of research on concept drift has focused on the supervised case that assumes the true labels of supervised tasks are available immediately after making predictions. Controlling for false positives while monitoring the performance of predictive models used to make inference from extremely large datasets periodically, where the true labels are not instantly available, becomes extremely challenging. We propose a flexible and efficient concept drift detection algorithm that uses classical statistical process control in a label-less setting to accurately detect concept drifts. We shown empirically that under computational constraints, our approach has better statistical power than previous known methods. Furthermore, we introduce a new drift detection framework to model the scenario of detecting drift (without labels) given prior detections, and show our how our drift detection algorithm can be incorporated effectively into this framework. We demonstrate promising performance via numerical simulations."
      },
      {
        "id": "oai:arXiv.org:2506.08743v1",
        "title": "Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems",
        "link": "https://arxiv.org/abs/2506.08743",
        "author": "Michael F\\\"arber, David Lamprecht, Yuni Susanti",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08743v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have substantially advanced the field of recommender systems. However, despite the creation of more than a thousand knowledge graphs (KGs) under the W3C standard RDF, their rich semantic information has not yet been fully leveraged in GNN-based recommender systems. To address this gap, we propose a comprehensive integration of RDF KGs with GNNs that utilizes both the topological information from RDF object properties and the content information from RDF datatype properties. Our main focus is an in-depth evaluation of various GNNs, analyzing how different semantic feature initializations and types of graph structure heterogeneity influence their performance in recommendation tasks. Through experiments across multiple recommendation scenarios involving multi-million-node RDF graphs, we demonstrate that harnessing the semantic richness of RDF KGs significantly improves recommender systems and lays the groundwork for GNN-based recommender systems for the Linked Open Data cloud. The code and data are available on our GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation"
      },
      {
        "id": "oai:arXiv.org:2506.08745v1",
        "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning",
        "link": "https://arxiv.org/abs/2506.08745",
        "author": "Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08745v1 Announce Type: cross \nAbstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (high consistency) with minimal deviation toward other candidates (low volatility). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at https://github.com/sastpg/CoVo."
      },
      {
        "id": "oai:arXiv.org:2506.08749v1",
        "title": "Superposed Parameterised Quantum Circuits",
        "link": "https://arxiv.org/abs/2506.08749",
        "author": "Viktoria Patapovich, Mo Kordzanganeh, Alexey Melnikov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08749v1 Announce Type: cross \nAbstract: Quantum machine learning has shown promise for high-dimensional data analysis, yet many existing approaches rely on linear unitary operations and shared trainable parameters across outputs. These constraints limit expressivity and scalability relative to the multi-layered, non-linear architectures of classical deep networks. We introduce superposed parameterised quantum circuits to overcome these limitations. By combining flip-flop quantum random-access memory with repeat-until-success protocols, a superposed parameterised quantum circuit embeds an exponential number of parameterised sub-models in a single circuit and induces polynomial activation functions through amplitude transformations and post-selection. We provide an analytic description of the architecture, showing how multiple parameter sets are trained in parallel while non-linear amplitude transformations broaden representational power beyond conventional quantum kernels. Numerical experiments underscore these advantages: on a 1D step-function regression a two-qubit superposed parameterised quantum circuit cuts the mean-squared error by three orders of magnitude versus a parameter-matched variational baseline; on a 2D star-shaped two-dimensional classification task, introducing a quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance three-fold. These results position superposed parameterised quantum circuits as a hardware-efficient route toward deeper, more versatile parameterised quantum circuits capable of learning complex decision boundaries."
      },
      {
        "id": "oai:arXiv.org:2506.08761v1",
        "title": "Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification",
        "link": "https://arxiv.org/abs/2506.08761",
        "author": "Matthias Beckmann, Robert Beinert, Jonas Bresch",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08761v1 Announce Type: cross \nAbstract: The Radon cumulative distribution transform (R-CDT), is an easy-to-compute feature extractor that facilitates image classification tasks especially in the small data regime. It is closely related to the sliced Wasserstein distance and provably guaranties the linear separability of image classes that emerge from translations or scalings. In many real-world applications, like the recognition of watermarks in filigranology, however, the data is subject to general affine transformations originating from the measurement process. To overcome this issue, we recently introduced the so-called max-normalized R-CDT that only requires elementary operations and guaranties the separability under arbitrary affine transformations. The aim of this paper is to continue our study of the max-normalized R-CDT especially with respect to its robustness against non-affine image deformations. Our sensitivity analysis shows that its separability properties are stable provided the Wasserstein-infinity distance between the samples can be controlled. Since the Wasserstein-infinity distance only allows small local image deformations, we moreover introduce a mean-normalized version of the R-CDT. In this case, robustness relates to the Wasserstein-2 distance and also covers image deformations caused by impulsive noise for instance. Our theoretical results are supported by numerical experiments showing the effectiveness of our novel feature extractors as well as their robustness against local non-affine deformations and impulsive noise."
      },
      {
        "id": "oai:arXiv.org:2506.08762v1",
        "title": "EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements",
        "link": "https://arxiv.org/abs/2506.08762",
        "author": "Issa Sugiura, Takashi Ishida, Taro Makino, Chieko Tazuke, Takanori Nakagawa, Kosuke Nakago, David Ha",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08762v1 Announce Type: cross \nAbstract: Financial analysis presents complex challenges that could leverage large language model (LLM) capabilities. However, the scarcity of challenging financial datasets, particularly for Japanese financial data, impedes academic innovation in financial analytics. As LLMs advance, this lack of accessible research resources increasingly hinders their development and evaluation in this specialized domain. To address this gap, we introduce EDINET-Bench, an open-source Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction. EDINET-Bench is constructed by downloading annual reports from the past 10 years from Japan's Electronic Disclosure for Investors' NETwork (EDINET) and automatically assigning labels corresponding to each evaluation task. Our experiments reveal that even state-of-the-art LLMs struggle, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting. These results highlight significant challenges in applying LLMs to real-world financial applications and underscore the need for domain-specific adaptation. Our dataset, benchmark construction code, and evaluation code is publicly available to facilitate future research in finance with LLMs."
      },
      {
        "id": "oai:arXiv.org:2506.08771v1",
        "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery",
        "link": "https://arxiv.org/abs/2506.08771",
        "author": "Yuni Susanti, Michael F\\\"arber",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08771v1 Announce Type: cross \nAbstract: Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: https://github.com/susantiyuni/path-to-causality"
      },
      {
        "id": "oai:arXiv.org:2506.08783v1",
        "title": "syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum",
        "link": "https://arxiv.org/abs/2506.08783",
        "author": "Lukas Kammerer, Deaglan J. Bartlett, Gabriel Kronberger, Harry Desmond, Pedro G. Ferreira",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08783v1 Announce Type: cross \nAbstract: Baryonic physics has a considerable impact on the distribution of matter in our Universe on scales probed by current and future cosmological surveys, acting as a key systematic in such analyses. We seek simple symbolic parametrisations for the impact of baryonic physics on the matter power spectrum for a range of physically motivated models, as a function of wavenumber, redshift, cosmology, and parameters controlling the baryonic feedback. We use symbolic regression to construct analytic approximations for the ratio of the matter power spectrum in the presence of baryons to that without such effects. We obtain separate functions of each of four distinct sub-grid prescriptions of baryonic physics from the CAMELS suite of hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as well as for a baryonification algorithm. We also provide functions which describe the uncertainty on these predictions, due to both the stochastic nature of baryonic physics and the errors on our fits. The error on our approximations to the hydrodynamical simulations is comparable to the sample variance estimated through varying initial conditions, and our baryonification expression has a root mean squared error of better than one percent, although this increases on small scales. These errors are comparable to those of previous numerical emulators for these models. Our expressions are enforced to have the physically correct behaviour on large scales and at high redshift. Due to their analytic form, we are able to directly interpret the impact of varying cosmology and feedback parameters, and we can identify parameters which have little to no effect. Each function is based on a different implementation of baryonic physics, and can therefore be used to discriminate between these models when applied to real data. We provide publicly available code for all symbolic approximations found."
      },
      {
        "id": "oai:arXiv.org:2506.08800v1",
        "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents",
        "link": "https://arxiv.org/abs/2506.08800",
        "author": "Irene Testini, Jos\\'e Hern\\'andez-Orallo, Lorenzo Pacchiardi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08800v1 Announce Type: cross \nAbstract: Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) are increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation."
      },
      {
        "id": "oai:arXiv.org:2506.08860v1",
        "title": "On The Impact of Merge Request Deviations on Code Review Practices",
        "link": "https://arxiv.org/abs/2506.08860",
        "author": "Samah Kansab, Francis Bordeleau, Ali Tizghadam",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08860v1 Announce Type: cross \nAbstract: Code review is a key practice in software engineering, ensuring quality and collaboration. However, industrial Merge Request (MR) workflows often deviate from standardized review processes, with many MRs serving non-review purposes (e.g., drafts, rebases, or dependency updates). We term these cases deviations and hypothesize that ignoring them biases analytics and undermines ML models for review analysis.\n  We identify seven deviation categories, occurring in 37.02% of MRs, and propose a few-shot learning detection method (91% accuracy). By excluding deviations, ML models predicting review completion time improve performance in 53.33% of cases (up to 2.25x) and exhibit significant shifts in feature importance (47% overall, 60% top-*k*).\n  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven detection approach, and (3) empirical evidence of their impact on ML-based review analytics. This work aids practitioners in optimizing review efforts and ensuring reliable insights."
      },
      {
        "id": "oai:arXiv.org:2506.08893v1",
        "title": "Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.08893",
        "author": "Kai Zhou, Youbiao He, Chong Zhong, Yifu Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08893v1 Announce Type: cross \nAbstract: Despite high reliability, modern power systems with growing renewable penetration face an increasing risk of cascading outages. Real-time cascade mitigation requires fast, complex operational decisions under uncertainty. In this work, we extend the influence graph into a Markov decision process model (MDP) for real-time mitigation of cascading outages in power transmission systems, accounting for uncertainties in generation, load, and initial contingencies. The MDP includes a do-nothing action to allow for conservative decision-making and is solved using reinforcement learning. We present a policy gradient learning algorithm initialized with a policy corresponding to the unmitigated case and designed to handle invalid actions. The proposed learning method converges faster than the conventional algorithm. Through careful reward design, we learn a policy that takes conservative actions without deteriorating system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus systems. The results show that proactive line disconnections can effectively reduce cascading risk, and certain lines consistently emerge as critical in mitigating cascade propagation."
      },
      {
        "id": "oai:arXiv.org:2506.08911v1",
        "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU",
        "link": "https://arxiv.org/abs/2506.08911",
        "author": "Petar Jaku\\v{s}, Hrvoje D\\v{z}apo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08911v1 Announce Type: cross \nAbstract: This paper presents a keyword spotting (KWS) system implemented on the NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), enabling real-time voice interaction on resource-constrained devices. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training to reduce model size with minimal accuracy drop. Experimental results demonstrate a 59x speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy with a model size of 30.58 KB, demonstrating the feasibility of efficient, low-power voice interfaces on embedded platforms."
      },
      {
        "id": "oai:arXiv.org:2506.08954v1",
        "title": "Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction",
        "link": "https://arxiv.org/abs/2506.08954",
        "author": "Ruben Weitzman, Peter M{\\o}rch Groth, Lood Van Niekerk, Aoi Otani, Yarin Gal, Debora Marks, Pascal Notin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08954v1 Announce Type: cross \nAbstract: Retrieving homologous protein sequences is essential for a broad range of protein modeling tasks such as fitness prediction, protein design, structure modeling, and protein-protein interactions. Traditional workflows have relied on a two-step process: first retrieving homologs via Multiple Sequence Alignments (MSA), then training models on one or more of these alignments. However, MSA-based retrieval is computationally expensive, struggles with highly divergent sequences or complex insertions & deletions patterns, and operates independently of the downstream modeling objective. We introduce Protriever, an end-to-end differentiable framework that learns to retrieve relevant homologs while simultaneously training for the target task. When applied to protein fitness prediction, Protriever achieves state-of-the-art performance compared to sequence-based models that rely on MSA-based homolog retrieval, while being two orders of magnitude faster through efficient vector search. Protriever is both architecture- and task-agnostic, and can flexibly adapt to different retrieval strategies and protein databases at inference time -- offering a scalable alternative to alignment-centric approaches."
      },
      {
        "id": "oai:arXiv.org:2506.08957v1",
        "title": "IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections",
        "link": "https://arxiv.org/abs/2506.08957",
        "author": "Yash Ranjan, Rahul Sengupta, Anand Rangarajan, Sanjay Ranka",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08957v1 Announce Type: cross \nAbstract: Traffic simulators are widely used to study the operational efficiency of road infrastructure, but their rule-based approach limits their ability to mimic real-world driving behavior. Traffic intersections are critical components of the road infrastructure, both in terms of safety risk (nearly 28% of fatal crashes and 58% of nonfatal crashes happen at intersections) as well as the operational efficiency of a road corridor. This raises an important question: can we create a data-driven simulator that can mimic the macro- and micro-statistics of the driving behavior at a traffic intersection? Deep Generative Modeling-based trajectory prediction models provide a good starting point to model the complex dynamics of vehicles at an intersection. But they are not tested in a \"live\" micro-simulation scenario and are not evaluated on traffic engineering-related metrics. In this study, we propose traffic engineering-related metrics to evaluate generative trajectory prediction models and provide a simulation-in-the-loop pipeline to do so. We also provide a multi-headed self-attention-based trajectory prediction model that incorporates the signal information, which outperforms our previous models on the evaluation metrics."
      },
      {
        "id": "oai:arXiv.org:2506.08967v1",
        "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model",
        "link": "https://arxiv.org/abs/2506.08967",
        "author": "Ailin Huang, Bingxin Li, Bruce Wang, Boyong Wu, Chao Yan, Chengli Feng, Heng Wang, Hongyu Zhou, Hongyuan Wang, Jingbei Li, Jianjian Sun, Joanna Wang, Mingrui Chen, Peng Liu, Ruihang Miao, Shilei Jiang, Tian Fei, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Ge, Zheng Gong, Zhewei Huang, Zixin Zhang, Bin Wang, Bo Li, Buyun Ma, Changxin Miao, Changyi Wan, Chen Xu, Dapeng Shi, Dingyuan Hu, Enle Liu, Guanzhe Huang, Gulin Yan, Hanpeng Hu, Haonan Jia, Jiahao Gong, Jiaoren Wu, Jie Wu, Jie Yang, Junzhe Lin, Kaixiang Li, Lei Xia, Longlong Gu, Ming Li, Nie Hao, Ranchen Ming, Shaoliang Pang, Siqi Liu, Song Yuan, Tiancheng Cao, Wen Li, Wenqing He, Xu Zhao, Xuelin Zhang, Yanbo Yu, Yinmin Zhong, Yu Zhou, Yuanwei Liang, Yuanwei Lu, Yuxiang Yang, Zidong Yang, Zili Zhang, Binxing Jiao, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Daxin Jiang, Shuchang Zhou, Chen Hu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08967v1 Announce Type: cross \nAbstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent human-computer interaction, yet their reliance on text-based outputs limits their ability to generate natural speech responses directly, hindering seamless audio interactions. To address this, we introduce Step-Audio-AQAA, a fully end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model integrates a dual-codebook audio tokenizer for linguistic and semantic feature extraction, a 130-billion-parameter backbone LLM and a neural vocoder for high-fidelity speech synthesis. Our post-training approach employs interleaved token-output of text and audio to enhance semantic coherence and combines Direct Preference Optimization (DPO) with model merge to improve performance. Evaluations on the StepEval-Audio-360 benchmark demonstrate that Step-Audio-AQAA excels especially in speech control, outperforming the state-of-art LALMs in key areas. This work contributes a promising solution for end-to-end LALMs and highlights the critical role of token-based vocoder in enhancing overall performance for AQAA tasks."
      },
      {
        "id": "oai:arXiv.org:2506.09023v1",
        "title": "Fine-Grained Spatially Varying Material Selection in Images",
        "link": "https://arxiv.org/abs/2506.09023",
        "author": "Julia Guerrero-Viu, Michael Fischer, Iliyan Georgiev, Elena Garces, Diego Gutierrez, Belen Masia, Valentin Deschaintre",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09023v1 Announce Type: cross \nAbstract: Selection is the first step in many image editing processes, enabling faster and simpler modifications of all pixels sharing a common modality. In this work, we present a method for material selection in images, robust to lighting and reflectance variations, which can be used for downstream editing tasks. We rely on vision transformer (ViT) models and leverage their features for selection, proposing a multi-resolution processing strategy that yields finer and more stable selection results than prior methods. Furthermore, we enable selection at two levels: texture and subtexture, leveraging a new two-level material selection (DuMaS) dataset which includes dense annotations for over 800,000 synthetic images, both on the texture and subtexture levels."
      },
      {
        "id": "oai:arXiv.org:2506.09049v1",
        "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning",
        "link": "https://arxiv.org/abs/2506.09049",
        "author": "Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09049v1 Announce Type: cross \nAbstract: Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems."
      },
      {
        "id": "oai:arXiv.org:2206.05446v2",
        "title": "A Decomposition-Based Approach for Evaluating and Analyzing Inter-Annotator Disagreement",
        "link": "https://arxiv.org/abs/2206.05446",
        "author": "Effi Levi, Shaul R. Shenhav",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2206.05446v2 Announce Type: replace \nAbstract: We propose a novel method to conceptually decompose an existing annotation into separate levels, allowing the analysis of inter-annotators disagreement in each level separately. We suggest two distinct strategies in order to actualize this approach: a theoretically-driven one, in which the researcher defines a decomposition based on prior knowledge of the annotation task, and an exploration-based one, in which many possible decompositions are inductively computed and presented to the researcher for interpretation and evaluation. Utilizing a recently constructed dataset for narrative analysis as our use-case, we apply each of the two strategies to demonstrate the potential of our approach in testing hypotheses regarding the sources of annotation disagreements, as well as revealing latent structures and relations within the annotation task. We conclude by suggesting how to extend and generalize our approach, as well as use it for other purposes."
      },
      {
        "id": "oai:arXiv.org:2212.02042v3",
        "title": "Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning",
        "link": "https://arxiv.org/abs/2212.02042",
        "author": "Mingyuan Fan, Cen Chen, Chengyu Wang, Xiaodan Li, Wenmeng Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2212.02042v3 Announce Type: replace \nAbstract: Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and performance maintenance. The utility metric is designed to promote consistency between the gradients of key parameters associated with robust data and those derived from clients' data, thus maintaining model performance. Furthermore, the privacy metric guides the generation of robust data towards enlarging the semantic gap with clients' data. Theoretical analysis supports the effectiveness of Refiner, and empirical evaluations on multiple benchmark datasets demonstrate the superior defense effectiveness of Refiner at defending against state-of-the-art attacks."
      },
      {
        "id": "oai:arXiv.org:2302.14502v2",
        "title": "A Survey on Long Text Modeling with Transformers",
        "link": "https://arxiv.org/abs/2302.14502",
        "author": "Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2302.14502v2 Announce Type: replace \nAbstract: Modeling long texts has been an essential technique in the field of natural language processing (NLP). With the ever-growing number of long documents, it is important to develop effective modeling methods that can process and analyze such texts. However, long texts pose important research challenges for existing text models, with more complex semantics and special characteristics. In this paper, we provide an overview of the recent advances on long texts modeling based on Transformer models. Firstly, we introduce the formal definition of long text modeling. Then, as the core content, we discuss how to process long input to satisfy the length limitation and design improved Transformer architectures to effectively extend the maximum context length. Following this, we discuss how to adapt Transformer models to capture the special characteristics of long texts. Finally, we describe four typical applications involving long text modeling and conclude this paper with a discussion of future directions. Our survey intends to provide researchers with a synthesis and pointer to related work on long text modeling."
      },
      {
        "id": "oai:arXiv.org:2303.05582v3",
        "title": "Generalization analysis of an unfolding network for analysis-based Compressed Sensing",
        "link": "https://arxiv.org/abs/2303.05582",
        "author": "Vicky Kouni, Yannis Panagakis",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2303.05582v3 Announce Type: replace \nAbstract: Unfolding networks have shown promising results in the Compressed Sensing (CS) field. Yet, the investigation of their generalization ability is still in its infancy. In this paper, we perform a generalization analysis of a state-of-the-art ADMM-based unfolding network, which jointly learns a decoder for CS and a sparsifying redundant analysis operator. To this end, we first impose a structural constraint on the learnable sparsifier, which parametrizes the network's hypothesis class. For the latter, we estimate its Rademacher complexity. With this estimate in hand, we deliver generalization error bounds -- which scale like the square root of the number of layers -- for the examined network. Finally, the validity of our theory is assessed and numerical comparisons to a state-of-the-art unfolding network are made, on synthetic and real-world datasets. Our experimental results demonstrate that our proposed framework complies with our theoretical findings and outperforms the baseline, consistently for all datasets."
      },
      {
        "id": "oai:arXiv.org:2305.11566v3",
        "title": "StereoVAE: A lightweight stereo-matching system using embedded GPUs",
        "link": "https://arxiv.org/abs/2305.11566",
        "author": "Qiong Chang, Xiang Li, Xin Xu, Xin Liu, Yun Li, Miyazaki Jun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2305.11566v3 Announce Type: replace \nAbstract: We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The proposed hybrid structure cannot only bring the advantage of traditional methods in terms of computational complexity, but also ensure the matching accuracy under the impact of neural network. Extensive experiments on the KITTI 2015 benchmark demonstrate that our tiny system exhibits high robustness in improving the accuracy of the coarse disparity maps generated by different algorithms, while also running in real-time on embedded GPUs."
      },
      {
        "id": "oai:arXiv.org:2305.13883v2",
        "title": "Mitigating fairwashing using Two-Source Audits",
        "link": "https://arxiv.org/abs/2305.13883",
        "author": "Jade Garcia Bourr\\'ee, Erwan Le Merrer, Gilles Tredan, Beno\\^it Rottembourg",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2305.13883v2 Announce Type: replace \nAbstract: Recent legislation requires online platforms to provide dedicated APIs to assess the compliance of their decision-making algorithms with the law. Research has nevertheless shown that the auditors of such platforms are prone to manipulation (a practice referred to as \\textit{fairwashing}). To address this salient problem, recent work has considered audits under the assumption of partial knowledge of the platform's internal mechanisms. In this paper, we propose a more pragmatic approach with the \\textit{Two-Source Audit} setup: while still leveraging the API, we advocate for the adjunction of a second source of data to both perform the audit of a platform and the detection of fairwashing attempts. Our method is based on identifying discrepancies between the two data sources, using data proxies at use in the fairness literature. We formally demonstrate the conditions for success in this fairwashing mitigation task. We then validate our method empirically, demonstrating that Two-Source Audits can achieve a Pareto-optimal balance between the two objectives. We believe this paper sets the stage for reliable audits in manipulation-prone setups, under mild assumptions."
      },
      {
        "id": "oai:arXiv.org:2306.03346v3",
        "title": "Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data",
        "link": "https://arxiv.org/abs/2306.03346",
        "author": "Chongyi Zheng, Benjamin Eysenbach, Homer Walke, Patrick Yin, Kuan Fang, Ruslan Salakhutdinov, Sergey Levine",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2306.03346v3 Announce Type: replace \nAbstract: Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV) and natural language processing (NLP), our work builds on prior work showing that the reinforcement learning (RL) itself can be cast as a self-supervised problem: learning to reach any goal without human-specified rewards or labels. Despite the seeming appeal, little (if any) prior work has demonstrated how self-supervised RL methods can be practically deployed on robotic systems. By first studying a challenging simulated version of this task, we discover design decisions about architectures and hyperparameters that increase the success rate by $2 \\times$. These findings lay the groundwork for our main result: we demonstrate that a self-supervised RL algorithm based on contrastive learning can solve real-world, image-based robotic manipulation tasks, with tasks being specified by a single goal image provided after training."
      },
      {
        "id": "oai:arXiv.org:2308.06459v3",
        "title": "Using co-sharing to identify use of mainstream news for promoting potentially misleading narratives",
        "link": "https://arxiv.org/abs/2308.06459",
        "author": "Pranav Goel (Network Science Institute, Northeastern University), Jon Green (Department of Political Science, Duke University), David Lazer (Network Science Institute, Northeastern University, Institute for Quantitative Social Science, Harvard University), Philip Resnik (Department of Linguistics, University of Maryland, Institute for Advanced Computer Studies, University of Maryland)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2308.06459v3 Announce Type: replace \nAbstract: Much of the research quantifying volume and spread of online misinformation measures the construct at the source level, identifying a set of specific unreliable domains that account for a relatively small share of news consumption. This source-level dichotomy obscures the potential for users to repurpose factually true information from reliable sources to advance misleading narratives. We demonstrate this potentially far more prevalent form of misinformation by identifying articles from reliable sources that are frequently co-shared with (shared by users who also shared) \"fake\" news on social media, and concurrently extracting narratives present in fake news content and claims fact-checked as false. Specifically in this study, we use Twitter/X data from May 2018 to November 2021 matched to a U.S. voter file. We find that narratives present in misinformation content are significantly more likely to occur in co-shared articles than in articles from the same reliable sources that are not co-shared, consistent with users using information from mainstream sources to enhance the credibility and reach of potentially misleading claims."
      },
      {
        "id": "oai:arXiv.org:2309.14704v3",
        "title": "Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer",
        "link": "https://arxiv.org/abs/2309.14704",
        "author": "Zhihao Zhang, Yiwei Chen, Weizhan Zhang, Caixia Yan, Qinghua Zheng, Qi Wang, Wangdu Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2309.14704v3 Announce Type: replace \nAbstract: Viewport prediction is a crucial aspect of tile-based 360 video streaming system. However, existing trajectory based methods lack of robustness, also oversimplify the process of information construction and fusion between different modality inputs, leading to the error accumulation problem. In this paper, we propose a tile classification based viewport prediction method with Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes transformer-based networks to extract the long-range dependencies within each modality, then mine intra- and inter-modality relations to capture the combined impact of user historical inputs and video contents on future viewport selection. In addition, MFTR categorizes future tiles into two categories: user interested or not, and selects future viewport as the region that contains most user interested tiles. Comparing with predicting head trajectories, choosing future viewport based on tile's binary classification results exhibits better robustness and interpretability. To evaluate our proposed MFTR, we conduct extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows superior performance over state-of-the-art methods in terms of average prediction accuracy and overlap ratio, also presents competitive computation efficiency."
      },
      {
        "id": "oai:arXiv.org:2310.08732v3",
        "title": "Provably Cost-Sensitive Adversarial Defense via Randomized Smoothing",
        "link": "https://arxiv.org/abs/2310.08732",
        "author": "Yuan Xin, Dingfan Chen, Michael Backes, Xiao Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2310.08732v3 Announce Type: replace \nAbstract: As ML models are increasingly deployed in critical applications, robustness against adversarial perturbations is crucial. While numerous defenses have been proposed to counter such attacks, they typically assume that all adversarial transformations are equally important, an assumption that rarely aligns with real-world applications. To address this, we study the problem of robust learning against adversarial perturbations under cost-sensitive scenarios, where the potential harm of different types of misclassifications is encoded in a cost matrix. Our solution introduces a provably robust learning algorithm to certify and optimize for cost-sensitive robustness, building on the scalable certification framework of randomized smoothing. Specifically, we formalize the definition of cost-sensitive certified radius and propose our novel adaptation of the standard certification algorithm to generate tight robustness certificates tailored to any cost matrix. In addition, we design a robust training method that improves certified cost-sensitive robustness without compromising model accuracy. Extensive experiments on benchmark datasets, including challenging ones unsolvable by existing methods, demonstrate the effectiveness of our certification algorithm and training method across various cost-sensitive scenarios."
      },
      {
        "id": "oai:arXiv.org:2310.16937v3",
        "title": "Cross-lingual Transfer in Programming Languages: An Extensive Empirical Study",
        "link": "https://arxiv.org/abs/2310.16937",
        "author": "Razan Baltaji, Saurabh Pujar, Louis Mandel, Martin Hirzel, Luca Buratti, Lav Varshney",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2310.16937v3 Announce Type: replace \nAbstract: Large language models (LLMs) have achieved state-of-the-art performance in various software engineering tasks, including error detection, clone detection, and code translation, primarily leveraging high-resource programming languages like Python and Java. However, many critical languages, such as COBOL, as well as emerging languages, such as Rust and Swift, remain low-resource due to limited openly available code. This scarcity hampers the training and effectiveness of LLMs for these languages, increasing software maintenance costs and stifling innovation. Addressing this gap, we investigate the potential of transfer learning to enhance LLM performance on low-resource programming languages by leveraging data from high-resource counterparts. Our extensive empirical study evaluates transferability across 10 to 41 programming languages and five key tasks: code generation, clone detection, code repair, solution domain classification, and error detection. Additionally, we develop a performance prediction model to guess the best source languages for a given target and task, and analyze the features that influence transfer performance. We further replicate a representative subset of experiments with a larger model to test the generalizability of our conclusions to contemporary large-scale LLMs. Our findings demonstrate that cross-lingual transfer significantly outperforms zero-shot learning, with effectiveness varying based on both source and target languages. Furthermore, our model reliably predicts successful transfer sources by considering linguistic and dataset-specific features, offering practical guidance for data acquisition and model training. This work contributes to the development of LLM-driven tools for low-resource programming languages and provides insights into the characteristics that facilitate transfer across language pairs."
      },
      {
        "id": "oai:arXiv.org:2401.05572v2",
        "title": "Innate-Values-driven Reinforcement Learning based Cooperative Multi-Agent Cognitive Modeling",
        "link": "https://arxiv.org/abs/2401.05572",
        "author": "Qin Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2401.05572v2 Announce Type: replace \nAbstract: In multi-agent systems (MAS), the dynamic interaction among multiple decision-makers is driven by their innate values, affecting the environment's state, and can cause specific behavioral patterns to emerge. On the other hand, innate values in cognitive modeling reflect individual interests and preferences for specific tasks and drive them to develop diverse skills and plans, satisfying their various needs and achieving common goals in cooperation. Therefore, building the awareness of AI agents to balance the group utilities and system costs and meet group members' needs in their cooperation is a crucial problem for individuals learning to support their community and even integrate into human society in the long term. However, the current MAS reinforcement learning domain lacks a general intrinsic model to describe agents' dynamic motivation for decision-making and learning from an individual needs perspective in their cooperation. To address the gap, this paper proposes a general MAS innate-values reinforcement learning (IVRL) architecture from the individual preferences angle. We tested the Multi-Agent IVRL Actor-Critic Model in different StarCraft Multi-Agent Challenge (SMAC) settings, which demonstrated its potential to organize the group's behaviours to achieve better performance."
      },
      {
        "id": "oai:arXiv.org:2402.02088v4",
        "title": "Mitigating Prior Shape Bias in Point Clouds via Differentiable Center Learning",
        "link": "https://arxiv.org/abs/2402.02088",
        "author": "Zhe Li, Xiying Wang, Jinglin Zhao, Zheng Wang, Debin Liu, Laurence T. Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2402.02088v4 Announce Type: replace \nAbstract: Masked autoencoding and generative pretraining have achieved remarkable success in computer vision and natural language processing, and more recently, they have been extended to the point cloud domain. Nevertheless, existing point cloud models suffer from the issue of information leakage due to the pre-sampling of center points, which leads to trivial proxy tasks for the models. These approaches primarily focus on local feature reconstruction, limiting their ability to capture global patterns within point clouds. In this paper, we argue that the reduced difficulty of pretext tasks hampers the model's capacity to learn expressive representations. To address these limitations, we introduce a novel solution called the Differentiable Center Sampling Network (DCS-Net). It tackles the information leakage problem by incorporating both global feature reconstruction and local feature reconstruction as non-trivial proxy tasks, enabling simultaneous learning of both the global and local patterns within point cloud. Experimental results demonstrate that our method enhances the expressive capacity of existing point cloud models and effectively addresses the issue of information leakage."
      },
      {
        "id": "oai:arXiv.org:2402.03896v3",
        "title": "Multimodal Rationales for Explainable Visual Question Answering",
        "link": "https://arxiv.org/abs/2402.03896",
        "author": "Kun Li, George Vosselman, Michael Ying Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2402.03896v3 Announce Type: replace \nAbstract: Visual Question Answering (VQA) is a challenging task of predicting the answer to a question about the content of an image. Prior works directly evaluate the answering models by simply calculating the accuracy of predicted answers. However, the inner reasoning behind the predictions is disregarded in such a \"black box\" system, and we cannot ascertain the trustworthiness of the predictions. Even more concerning, in some cases, these models predict correct answers despite focusing on irrelevant visual regions or textual tokens. To develop an explainable and trustworthy answering system, we propose a novel model termed MRVQA (Multimodal Rationales for VQA), which provides visual and textual rationales to support its predicted answers. To measure the quality of generated rationales, a new metric vtS (visual-textual Similarity) score is introduced from both visual and textual perspectives. Considering the extra annotations distinct from standard VQA, MRVQA is trained and evaluated using samples synthesized from some existing datasets. Extensive experiments across three EVQA datasets demonstrate that MRVQA achieves new state-of-the-art results through additional rationale generation, enhancing the trustworthiness of the explainable VQA model. The code and the synthesized dataset are released under https://github.com/lik1996/MRVQA2025."
      },
      {
        "id": "oai:arXiv.org:2402.04416v3",
        "title": "Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap",
        "link": "https://arxiv.org/abs/2402.04416",
        "author": "Christopher Liao, Christian So, Theodoros Tsiligkaridis, Brian Kulis",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2402.04416v3 Announce Type: replace \nAbstract: Domain generalization (DG) is an important problem that learns a model which generalizes to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (MUDG) problem, which uses a large task-agnostic unlabeled source dataset during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be accurately and efficiently searched in a joint vision-language space. We make three contributions in the MUDG setting. Firstly, we show theoretically that cross-modal approximate nearest neighbor search suffers from low recall due to the large distance between text queries and the image centroids used for coarse quantization. Accordingly, we propose paired k-means, a simple clustering algorithm that improves nearest neighbor recall by storing centroids in query space instead of image space. Secondly, we propose an adaptive text augmentation scheme for target labels designed to improve zero-shot accuracy and diversify retrieved image data. Lastly, we present two simple but effective components to further improve downstream target accuracy. We compare against state-of-the-art name-only transfer, source-free DG and zero-shot (ZS) methods on their respective benchmarks and show consistent improvement in accuracy on 20 diverse datasets. Code is available: https://github.com/Chris210634/mudg"
      },
      {
        "id": "oai:arXiv.org:2404.01856v3",
        "title": "Poro 34B and the Blessing of Multilinguality",
        "link": "https://arxiv.org/abs/2404.01856",
        "author": "Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, V\\\"ain\\\"o Hatanp\\\"a\\\"a, Peter Sarlin, Sampo Pyysalo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2404.01856v3 Announce Type: replace \nAbstract: The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing: when the lack of training data is a constraint for effectively training larger models for a target language, augmenting the dataset with other languages can offer a way to improve over the capabilities of monolingual models for that language. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that substantially advances over the capabilities of existing models for Finnish and excels in translation, while also achieving competitive performance in its class for English and programming languages. We release the model parameters, scripts, and data under open licenses at https://huggingface.co/LumiOpen/Poro-34B."
      },
      {
        "id": "oai:arXiv.org:2405.02844v2",
        "title": "SMCD: High Realism Motion Style Transfer via Mamba-based Diffusion",
        "link": "https://arxiv.org/abs/2405.02844",
        "author": "Ziyun Qian, Zeyu Xiao, Xingliang Jin, Dingkang Yang, Mingcheng Li, Zhenyi Wu, Dongliang Kou, Peng Zhai, Lihua Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.02844v2 Announce Type: replace \nAbstract: Motion style transfer is a significant research direction in the field of computer vision, enabling virtual digital humans to rapidly switch between different styles of the same motion, thereby significantly enhancing the richness and realism of movements. It has been widely applied in multimedia scenarios such as films, games, and the metaverse. However, most existing methods adopt a two-stream structure, which tends to overlook the intrinsic relationship between content and style motions, leading to information loss and poor alignment. Moreover, when handling long-range motion sequences, these methods fail to effectively learn temporal dependencies, ultimately resulting in unnatural generated motions. To address these limitations, we propose a Unified Motion Style Diffusion (UMSD) framework, which simultaneously extracts features from both content and style motions and facilitates sufficient information interaction. Additionally, we introduce the Motion Style Mamba (MSM) denoiser, the first approach in the field of motion style transfer to leverage Mamba's powerful sequence modelling capability. Better capturing temporal relationships generates more coherent stylized motion sequences. Third, we design a diffusion-based content consistency loss and a style consistency loss to constrain the framework, ensuring that it inherits the content motion while effectively learning the characteristics of the style motion. Finally, extensive experiments demonstrate that our method outperforms state-of-the-art (SOTA) methods qualitatively and quantitatively, achieving more realistic and coherent motion style transfer."
      },
      {
        "id": "oai:arXiv.org:2405.07988v2",
        "title": "MedVersa: A Generalist Foundation Model for Medical Image Interpretation",
        "link": "https://arxiv.org/abs/2405.07988",
        "author": "Hong-Yu Zhou, Juli\\'an Nicol\\'as Acosta, Subathra Adithan, Suvrankar Datta, Eric J. Topol, Pranav Rajpurkar",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.07988v2 Announce Type: replace \nAbstract: Current medical AI systems are often limited to narrow applications, hindering widespread adoption. We present MedVersa, a generalist foundation model trained on tens of millions of compiled medical instances. MedVersa unlocks generalist learning from multimodal inputs and outputs, representing the first example of a generalist model reaching competitive performance with leading specialized solutions across a variety of medical imaging scenarios. MedVersa achieves state-of-the-art performance in nine tasks, sometimes outperforming counterparts by over 10%. Radiologist evaluation shows MedVersa-generated reports get superior performance in 95% of normal studies, while matching or exceeding human reports in 71% of cases overall. User studies showed notable reductions in report writing time and discrepancies with the use of MedVersa. Our findings underscore the value of flexible, multimodal AI systems in advancing medical image interpretation and supporting clinical expertise."
      },
      {
        "id": "oai:arXiv.org:2405.15481v2",
        "title": "Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks",
        "link": "https://arxiv.org/abs/2405.15481",
        "author": "Jialin Zhao, Yingtao Zhang, Xinghang Li, Huaping Liu, Carlo Vittorio Cannistraci",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15481v2 Announce Type: replace \nAbstract: The growing demands on GPU memory posed by the increasing number of neural network parameters call for training approaches that are more memory-efficient. Previous memory reduction training techniques, such as Low-Rank Adaptation (LoRA) and ReLoRA, face challenges, with LoRA being constrained by its low-rank structure, particularly during intensive tasks like pre-training, and ReLoRA suffering from saddle point issues. In this paper, we propose Sparse Spectral Training (SST) to optimize memory usage for pre-training. SST updates all singular values and selectively updates singular vectors through a multinomial sampling method weighted by the magnitude of the singular values. Furthermore, SST employs singular value decomposition to initialize and periodically reinitialize low-rank parameters, reducing distortion relative to full-rank training compared to other low-rank methods. Through comprehensive testing on both Euclidean and hyperbolic neural networks across various tasks, SST demonstrates its ability to outperform existing memory reduction training methods and is comparable to full-rank training in various cases. On LLaMA-1.3B, with only 18.7\\% of the parameters trainable compared to full-rank training (using a rank equivalent to 6\\% of the embedding dimension), SST reduces the perplexity gap between other low-rank methods and full-rank training by 97.4\\%. This result highlights SST as an effective parameter-efficient technique for model pre-training."
      },
      {
        "id": "oai:arXiv.org:2406.01416v2",
        "title": "Adapting Prediction Sets to Distribution Shifts Without Labels",
        "link": "https://arxiv.org/abs/2406.01416",
        "author": "Kevin Kasa, Zhiyu Zhang, Heng Yang, Graham W. Taylor",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.01416v2 Announce Type: replace \nAbstract: Recently there has been a surge of interest to deploy confidence set predictions rather than point predictions in machine learning. Unfortunately, the effectiveness of such prediction sets is frequently impaired by distribution shifts in practice, and the challenge is often compounded by the lack of ground truth labels at test time. Focusing on a standard set-valued prediction framework called conformal prediction (CP), this paper studies how to improve its practical performance using only unlabeled data from the shifted test domain. This is achieved by two new methods called ECP and EACP, whose main idea is to adjust the score function in CP according to its base model's own uncertainty evaluation. Through extensive experiments on a number of large-scale datasets and neural network architectures, we show that our methods provide consistent improvement over existing baselines and nearly match the performance of fully supervised methods."
      },
      {
        "id": "oai:arXiv.org:2406.02017v3",
        "title": "On the Hardness of Sampling from Mixture Distributions via Langevin Dynamics",
        "link": "https://arxiv.org/abs/2406.02017",
        "author": "Xiwei Cheng, Kexin Fu, Farzan Farnia",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.02017v3 Announce Type: replace \nAbstract: The Langevin Dynamics (LD), which aims to sample from a probability distribution using its score function, has been widely used for analyzing and developing score-based generative modeling algorithms. While the convergence behavior of LD in sampling from a uni-modal distribution has been extensively studied in the literature, the analysis of LD under a mixture distribution with distinct modes remains underexplored in the literature. In this work, we analyze LD in sampling from a mixture distribution and theoretically study its convergence properties. Our theoretical results indicate that for general mixture distributions of sub-Gaussian components, LD could fail in finding all the components within a sub-exponential number of steps in the data dimension. Following our result on the complexity of LD in sampling from high-dimensional variables, we propose Chained Langevin Dynamics (Chained-LD), which divides the data vector into patches of smaller sizes and generates every patch sequentially conditioned on the previous patches. Our theoretical analysis of Chained-LD indicates its faster convergence speed to the components of a mixture distribution. We present the results of several numerical experiments on synthetic and real image datasets, validating our theoretical results on the iteration complexities of sample generation from mixture distributions using the vanilla and chained LD algorithms."
      },
      {
        "id": "oai:arXiv.org:2406.04308v2",
        "title": "Approximation-Aware Bayesian Optimization",
        "link": "https://arxiv.org/abs/2406.04308",
        "author": "Natalie Maus, Kyurae Kim, Geoff Pleiss, David Eriksson, John P. Cunningham, Jacob R. Gardner",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.04308v2 Announce Type: replace \nAbstract: High-dimensional Bayesian optimization (BO) tasks such as molecular design often require 10,000 function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity. Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is compatible with trust region methods like TuRBO. We derive efficient joint objectives for the expected improvement and knowledge gradient acquisition functions in both the standard and batch BO settings. Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design."
      },
      {
        "id": "oai:arXiv.org:2406.08466v3",
        "title": "Scaling Laws in Linear Regression: Compute, Parameters, and Data",
        "link": "https://arxiv.org/abs/2406.08466",
        "author": "Licong Lin, Jingfeng Wu, Sham M. Kakade, Peter L. Bartlett, Jason D. Lee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.08466v3 Announce Type: replace \nAbstract: Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.\n  We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation."
      },
      {
        "id": "oai:arXiv.org:2406.12548v2",
        "title": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts",
        "link": "https://arxiv.org/abs/2406.12548",
        "author": "Yuhao Dan, Jie Zhou, Qin Chen, Junfeng Tian, Liang He",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.12548v2 Announce Type: replace \nAbstract: Personalized large language models (LLMs) have attracted great attention in many applications, such as emotional support and role-playing. However, existing works primarily focus on modeling explicit character profiles, while ignoring the underlying personality traits that truly shape behaviors and decision-making, hampering the development of more anthropomorphic and psychologically-grounded AI systems. In this paper, we explore the modeling of Big Five personality traits, which is the most widely used trait theory in psychology, and propose P-React, a mixture of experts (MoE)-based personalized LLM. Particularly, we integrate a Personality Specialization Loss (PSL) to better capture individual trait expressions, providing a more nuanced and psychologically grounded personality simulacrum. To facilitate research in this field, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to train LLMs in expressing personality traits across diverse topics. Extensive experiments demonstrate the effectiveness of P-React in maintaining consistent and real personality."
      },
      {
        "id": "oai:arXiv.org:2406.19593v2",
        "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs",
        "link": "https://arxiv.org/abs/2406.19593",
        "author": "Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.19593v2 Announce Type: replace \nAbstract: Multimodal retrieval augmented generation (RAG) plays a crucial role in domains such as knowledge-based visual question answering (KB-VQA), where external knowledge is needed to answer a question. However, existing multimodal LLMs (MLLMs) are not designed for context-augmented generation, limiting their effectiveness in such tasks. While synthetic data generation has recently gained attention for training MLLMs, its application for context-augmented generation remains underexplored. To address this gap, we introduce SK-VQA, a large-scale synthetic multimodal dataset containing over 2 million visual question-answer pairs, each associated with context documents containing information necessary to determine the final answer. Compared to previous datasets, SK-VQA contains 11x more unique questions, exhibits greater domain diversity, and covers a broader spectrum of image sources. Through human evaluations, we confirm the high quality of the generated question-answer pairs and their contextual relevance. Extensive experiments show that SK-VQA serves both as a challenging KB-VQA benchmark and as an effective training resource for adapting MLLMs to context-augmented generation. Our results further indicate that models trained on SK-VQA demonstrate enhanced generalization in both context-aware VQA and multimodal RAG settings. SK-VQA is publicly available via Hugging Face Hub."
      },
      {
        "id": "oai:arXiv.org:2407.03817v2",
        "title": "Markerless Multi-view 3D Human Pose Estimation: a survey",
        "link": "https://arxiv.org/abs/2407.03817",
        "author": "Ana Filipa Rodrigues Nogueira, H\\'elder P. Oliveira, Lu\\'is F. Teixeira",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.03817v2 Announce Type: replace \nAbstract: 3D human pose estimation involves reconstructing the human skeleton by detecting the body joints. Accurate and efficient solutions are required for several real-world applications including animation, human-robot interaction, surveillance, and sports. However, challenges such as occlusions, 2D pose mismatches, random camera perspectives, and limited 3D labelled data have been hampering the models' performance and limiting their deployment in real-world scenarios. The higher availability of cameras has led researchers to explore multi-view solutions to take advantage of the different perspectives to reconstruct the pose.\n  Most existing reviews have mainly focused on monocular 3D human pose estimation, so a comprehensive survey on multi-view approaches has been missing since 2012. According to the reviewed articles, the majority of the existing methods are fully-supervised approaches based on geometric constraints, which are often limited by 2D pose mismatches. To mitigate this, researchers have proposed incorporating temporal consistency or depth information. Alternatively, working directly with 3D features has been shown to completely overcome this issue, albeit at the cost of increased computational complexity. Additionally, models with lower levels of supervision have been identified to help address challenges such as annotated data scarcity and generalisation to new setups. Therefore, no method currently addresses all challenges associated with 3D pose reconstruction, and a trade-off between complexity and performance exists. Further research is needed to develop approaches capable of quickly inferring a highly accurate 3D pose with bearable computation cost. Techniques such as active learning, low-supervision methods, temporal consistency, view selection, depth information estimation, and multi-modal approaches are strategies to consider when developing a new method for this task."
      },
      {
        "id": "oai:arXiv.org:2407.12282v3",
        "title": "Chip Placement with Diffusion Models",
        "link": "https://arxiv.org/abs/2407.12282",
        "author": "Vint Lee, Minh Nguyen, Leena Elzeiny, Chun Deng, Pieter Abbeel, John Wawrzynek",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.12282v3 Announce Type: replace \nAbstract: Macro placement is a vital step in digital circuit design that defines the physical location of large collections of components, known as macros, on a 2D chip. Because key performance metrics of the chip are determined by the placement, optimizing it is crucial. Existing learning-based methods typically fall short because of their reliance on reinforcement learning (RL), which is slow and struggles to generalize, requiring online training on each new circuit. Instead, we train a diffusion model capable of placing new circuits zero-shot, using guided sampling in lieu of RL to optimize placement quality. To enable such models to train at scale, we designed a capable yet efficient architecture for the denoising model, and propose a novel algorithm to generate large synthetic datasets for pre-training. To allow zero-shot transfer to real circuits, we empirically study the design decisions of our dataset generation algorithm, and identify several key factors enabling generalization. When trained on our synthetic data, our models generate high-quality placements on unseen, realistic circuits, achieving competitive performance on placement benchmarks compared to state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2407.16239v3",
        "title": "Identifiable Latent Bandits: Leveraging observational data for personalized decision-making",
        "link": "https://arxiv.org/abs/2407.16239",
        "author": "Ahmet Zahid Balc{\\i}o\\u{g}lu, Newton Mwai, Emil Carlsson, Fredrik D. Johansson",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.16239v3 Announce Type: replace \nAbstract: For many decision-making tasks, such as precision medicine, historical data alone are insufficient to determine the right choice for a new problem instance or patient. Online algorithms like multi-armed bandits can find optimal personalized decisions but are notoriously sample-hungry. In practice, training a bandit for a new individual from scratch is often infeasible, as the number of trials required is larger than the practical number of decision points. Latent bandits offer rapid exploration and personalization beyond what context variables can reveal, provided that a latent variable model can be learned consistently. In this work, we propose an identifiable latent bandit framework that leads to optimal decision-making with a shorter exploration time than classical bandits by learning from historical records of decisions and outcomes. Our method is based on nonlinear independent component analysis that provably identifies representations from observational data sufficient to infer the optimal action in new bandit instances. We verify this strategy in simulated and semi-synthetic environments, showing substantial improvement over online and offline learning baselines when identifying conditions are satisfied."
      },
      {
        "id": "oai:arXiv.org:2407.21666v2",
        "title": "An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification",
        "link": "https://arxiv.org/abs/2407.21666",
        "author": "Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.21666v2 Announce Type: replace \nAbstract: Early detection of drought stress is critical for taking timely measures for reducing crop loss before the drought impact becomes irreversible. The subtle phenotypical and physiological changes in response to drought stress are captured by non-invasive imaging techniques and these imaging data serve as valuable resource for machine learning methods to identify drought stress. While convolutional neural networks (CNNs) are in wide use, vision transformers (ViTs) present a promising alternative in capturing long-range dependencies and intricate spatial relationships, thereby enhancing the detection of subtle indicators of drought stress. We propose an explainable deep learning pipeline that leverages the power of ViTs for drought stress detection in potato crops using aerial imagery. We applied two distinct approaches: a synergistic combination of ViT and support vector machine (SVM), where ViT extracts intricate spatial features from aerial images, and SVM classifies the crops as stressed or healthy and an end-to-end approach using a dedicated classification layer within ViT to directly detect drought stress. Our key findings explain the ViT model's decision-making process by visualizing attention maps. These maps highlight the specific spatial features within the aerial images that the ViT model focuses as the drought stress signature. Our findings demonstrate that the proposed methods not only achieve high accuracy in drought stress identification but also shedding light on the diverse subtle plant features associated with drought stress. This offers a robust and interpretable solution for drought stress monitoring for farmers to undertake informed decisions for improved crop management."
      },
      {
        "id": "oai:arXiv.org:2408.01214v2",
        "title": "High-Throughput Phenotyping of Clinical Text Using Large Language Models",
        "link": "https://arxiv.org/abs/2408.01214",
        "author": "Daniel B. Hier, S. Ilyas Munzir, Anne Stahlfeld, Tayo Obafemi-Ajayi, Michael D. Carrithers",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.01214v2 Announce Type: replace \nAbstract: High-throughput phenotyping automates the mapping of patient signs to standardized ontology concepts and is essential for precision medicine. This study evaluates the automation of phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using large language models. Due to their rich phenotype data, these summaries can be surrogates for physician notes. We conduct a performance comparison of GPT-4 and GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to inter-rater agreement. Despite some limitations in sign normalization, the extensive pre-training of GPT-4 results in high performance and generalizability across several phenotyping tasks while obviating the need for manually annotated training data. Large language models are expected to be the dominant method for automating high-throughput phenotyping of clinical text."
      },
      {
        "id": "oai:arXiv.org:2408.09495v2",
        "title": "Directed Exploration in Reinforcement Learning from Linear Temporal Logic",
        "link": "https://arxiv.org/abs/2408.09495",
        "author": "Marco Bagatella, Andreas Krause, Georg Martius",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.09495v2 Announce Type: replace \nAbstract: Linear temporal logic (LTL) is a powerful language for task specification in reinforcement learning, as it allows describing objectives beyond the expressivity of conventional discounted return formulations. Nonetheless, recent works have shown that LTL formulas can be translated into a variable rewarding and discounting scheme, whose optimization produces a policy maximizing a lower bound on the probability of formula satisfaction. However, the synthesized reward signal remains fundamentally sparse, making exploration challenging. We aim to overcome this limitation, which can prevent current algorithms from scaling beyond low-dimensional, short-horizon problems. We show how better exploration can be achieved by further leveraging the LTL specification and casting its corresponding Limit Deterministic B\\\"uchi Automaton (LDBA) as a Markov reward process, thus enabling a form of high-level value estimation. By taking a Bayesian perspective over LDBA dynamics and proposing a suitable prior distribution, we show that the values estimated through this procedure can be treated as a shaping potential and mapped to informative intrinsic rewards. Empirically, we demonstrate applications of our method from tabular settings to high-dimensional continuous systems, which have so far represented a significant challenge for LTL-based reinforcement learning algorithms."
      },
      {
        "id": "oai:arXiv.org:2408.11531v2",
        "title": "Just Project! Multi-Channel Despeckling, the Easy Way",
        "link": "https://arxiv.org/abs/2408.11531",
        "author": "Lo\\\"ic Denis (LabHC, IMAGES, IDS), Emanuele Dalsasso (EPFL, IMAGES), Florence Tupin (IMAGES, IDS)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.11531v2 Announce Type: replace \nAbstract: Reducing speckle fluctuations in multi-channel SAR images is essential in many applications of SAR imaging such as polarimetric classification or interferometric height estimation. While single-channel despeckling has widely benefited from the application of deep learning techniques, extensions to multi-channel SAR images are much more challenging. This paper introduces MuChaPro, a generic framework that exploits existing single-channel despeckling methods. The key idea is to generate numerous single-channel projections, restore these projections, and recombine them into the final multi-channel estimate. This simple approach is shown to be effective in polarimetric and/or interferometric modalities. A special appeal of MuChaPro is the possibility to apply a self-supervised training strategy to learn sensor-specific networks for single-channel despeckling."
      },
      {
        "id": "oai:arXiv.org:2408.15138v3",
        "title": "How transformers learn structured data: insights from hierarchical filtering",
        "link": "https://arxiv.org/abs/2408.15138",
        "author": "Jerome Garnier-Brun, Marc M\\'ezard, Emanuele Moscato, Luca Saglietti",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.15138v3 Announce Type: replace \nAbstract: Understanding the learning process and the embedded computation in transformers is becoming a central goal for the development of interpretable AI. In the present study, we introduce a hierarchical filtering procedure for data models of sequences on trees, allowing us to hand-tune the range of positional correlations in the data. Leveraging this controlled setting, we provide evidence that vanilla encoder-only transformers can approximate the exact inference algorithm when trained on root classification and masked language modeling tasks, and study how this computation is discovered and implemented. We find that correlations at larger distances, corresponding to increasing layers of the hierarchy, are sequentially included by the network during training. By comparing attention maps from models trained with varying degrees of filtering and by probing the different encoder levels, we find clear evidence of a reconstruction of correlations on successive length scales corresponding to the various levels of the hierarchy, which we relate to a plausible implementation of the exact inference algorithm within the same architecture."
      },
      {
        "id": "oai:arXiv.org:2408.17059v5",
        "title": "A Survey of the Self Supervised Learning Mechanisms for Vision Transformers",
        "link": "https://arxiv.org/abs/2408.17059",
        "author": "Asifullah Khan, Anabia Sohail, Mustansar Fiaz, Mehdi Hassan, Tariq Habib Afridi, Sibghat Ullah Marwat, Farzeen Munir, Safdar Ali, Hannan Naseem, Muhammad Zaigham Zaheer, Kamran Ali, Tangina Sultana, Ziaurrehman Tanoli, Naeem Akhter",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.17059v5 Announce Type: replace \nAbstract: Vision Transformers (ViTs) have recently demonstrated remarkable performance in computer vision tasks. However, their parameter-intensive nature and reliance on large amounts of data for effective performance have shifted the focus from traditional human-annotated labels to unsupervised learning and pretraining strategies that uncover hidden structures within the data. In response to this challenge, self-supervised learning (SSL) has emerged as a promising paradigm. SSL leverages inherent relationships within the data itself as a form of supervision, eliminating the need for manual labeling and offering a more scalable and resource-efficient alternative for model training. Given these advantages, it is imperative to explore the integration of SSL techniques with ViTs, particularly in scenarios with limited labeled data. Inspired by this evolving trend, this survey aims to systematically review SSL mechanisms tailored for ViTs. We propose a comprehensive taxonomy to classify SSL techniques based on their representations and pre-training tasks. Additionally, we discuss the motivations behind SSL, review prominent pre-training tasks, and highlight advancements and challenges in this field. Furthermore, we conduct a comparative analysis of various SSL methods designed for ViTs, evaluating their strengths, limitations, and applicability to different scenarios."
      },
      {
        "id": "oai:arXiv.org:2409.08797v2",
        "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR",
        "link": "https://arxiv.org/abs/2409.08797",
        "author": "Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.08797v2 Announce Type: replace \nAbstract: Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR."
      },
      {
        "id": "oai:arXiv.org:2410.00535v4",
        "title": "The Causal Information Bottleneck and Optimal Causal Variable Abstractions",
        "link": "https://arxiv.org/abs/2410.00535",
        "author": "Francisco N. F. Q. Simoes, Mehdi Dastani, Thijs van Ommen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.00535v4 Announce Type: replace \nAbstract: To effectively study complex causal systems, it is often useful to construct abstractions of parts of the system by discarding irrelevant details while preserving key features. The Information Bottleneck (IB) method is a widely used approach to construct variable abstractions by compressing random variables while retaining predictive power over a target variable. Traditional methods like IB are purely statistical and ignore underlying causal structures, making them ill-suited for causal tasks. We propose the Causal Information Bottleneck (CIB), a causal extension of the IB, which compresses a set of chosen variables while maintaining causal control over a target variable. This method produces abstractions of (sets of) variables which are causally interpretable, give us insight about the interactions between the abstracted variables and the target variable, and can be used when reasoning about interventions. We present experimental results demonstrating that the learned abstractions accurately capture causal relations as intended."
      },
      {
        "id": "oai:arXiv.org:2410.01686v3",
        "title": "Positional Attention: Expressivity and Learnability of Algorithmic Computation",
        "link": "https://arxiv.org/abs/2410.01686",
        "author": "Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veli\\v{c}kovi\\'c, Kimon Fountoulakis",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.01686v3 Announce Type: replace \nAbstract: There is a growing interest in the ability of neural networks to execute algorithmic tasks (e.g., arithmetic, summary statistics, and sorting). The goal of this work is to better understand the role of attention in Transformers for algorithmic execution. Its importance for algorithmic execution has been studied theoretically and empirically using parallel computational models. Notably, many parallel algorithms communicate between processors solely using positional information. Inspired by this observation, we investigate how Transformers can execute algorithms using positional attention, where attention weights depend exclusively on positional encodings. We prove that Transformers with positional attention (positional Transformers) maintain the same expressivity of parallel computational models, incurring a logarithmic depth cost relative to the input length. We analyze their in-distribution learnability and explore how parameter norms in positional attention affect sample complexity. Our results show that positional Transformers introduce a learning trade-off: while they exhibit better theoretical dependence on parameter norms, certain tasks may require more layers, which can, in turn, increase sample complexity. Finally, we empirically explore the out-of-distribution performance of positional Transformers and find that they perform well in tasks where their underlying algorithmic solution relies on positional information."
      },
      {
        "id": "oai:arXiv.org:2410.02062v2",
        "title": "TPP-LLM: Modeling Temporal Point Processes by Efficiently Fine-Tuning Large Language Models",
        "link": "https://arxiv.org/abs/2410.02062",
        "author": "Zefang Liu, Yinzhu Quan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02062v2 Announce Type: replace \nAbstract: Temporal point processes (TPPs) are widely used to model the timing and occurrence of events in domains such as social networks, transportation systems, and e-commerce. In this paper, we introduce TPP-LLM, a novel framework that integrates large language models (LLMs) with TPPs to capture both the semantic and temporal aspects of event sequences. Unlike traditional methods that rely on categorical event type representations, TPP-LLM directly utilizes the textual descriptions of event types, enabling the model to capture rich semantic information embedded in the text. While LLMs excel at understanding event semantics, they are less adept at capturing temporal patterns. To address this, TPP-LLM incorporates temporal embeddings and employs parameter-efficient fine-tuning (PEFT) methods to effectively learn temporal dynamics without extensive retraining. This approach improves both predictive accuracy and computational efficiency. Experimental results across diverse real-world datasets demonstrate that TPP-LLM outperforms state-of-the-art baselines in sequence modeling and event prediction, highlighting the benefits of combining LLMs with TPPs."
      },
      {
        "id": "oai:arXiv.org:2410.07840v2",
        "title": "Improved Variational Inference in Discrete VAEs using Error Correcting Codes",
        "link": "https://arxiv.org/abs/2410.07840",
        "author": "Mar\\'ia Mart\\'inez-Garc\\'ia, Grace Villacr\\'es, David Mitchell, Pablo M. Olmos",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.07840v2 Announce Type: replace \nAbstract: Despite advances in deep probabilistic models, learning discrete latent representations remains challenging. This work introduces a novel method to improve inference in discrete Variational Autoencoders by reframing the inference problem through a generative perspective. We conceptualize the model as a communication system, and propose to leverage Error-Correcting Codes (ECCs) to introduce redundancy in latent representations, allowing the variational posterior to produce more accurate estimates and reduce the variational gap. We present a proof-of-concept using a Discrete Variational Autoencoder with binary latent variables and low-complexity repetition codes, extending it to a hierarchical structure for disentangling global and local data features. Our approach significantly improves generation quality, data reconstruction, and uncertainty calibration, outperforming the uncoded models even when trained with tighter bounds such as the Importance Weighted Autoencoder objective. We also outline the properties that ECCs should possess to be effectively utilized for improved discrete variational inference."
      },
      {
        "id": "oai:arXiv.org:2410.08674v2",
        "title": "Guidelines for Fine-grained Sentence-level Arabic Readability Annotation",
        "link": "https://arxiv.org/abs/2410.08674",
        "author": "Nizar Habash, Hanada Taha-Thomure, Khalid N. Elmadani, Zeina Zeino, Abdallah Abushmaes",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08674v2 Announce Type: replace \nAbstract: This paper presents the annotation guidelines of the Balanced Arabic Readability Evaluation Corpus (BAREC), a large-scale resource for fine-grained sentence-level readability assessment in Arabic. BAREC includes 69,441 sentences (1M+ words) labeled across 19 levels, from kindergarten to postgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined through iterative training with native Arabic-speaking educators. We highlight key linguistic, pedagogical, and cognitive factors in determining readability and report high inter-annotator agreement: Quadratic Weighted Kappa 81.8% (substantial/excellent agreement) in the last annotation phase. We also benchmark automatic readability models across multiple classification granularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are publicly available."
      },
      {
        "id": "oai:arXiv.org:2410.11226v3",
        "title": "MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning",
        "link": "https://arxiv.org/abs/2410.11226",
        "author": "Peter Eckmann, Dongxia Wu, Germano Heinzelmann, Michael K. Gilson, Rose Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11226v3 Announce Type: replace \nAbstract: Current generative models for drug discovery primarily use molecular docking as an oracle to guide the generation of active compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show real-world experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. To address this challenge, we propose Multi-Fidelity Latent space Active Learning (MF-LAL), a generative modeling framework that integrates a set of oracles with varying cost-accuracy tradeoffs. Using active learning, we train a surrogate model for each oracle and use these surrogates to guide generation of compounds with high predicted activity. Unlike previous approaches that separately learn the surrogate model and generative model, MF-LAL combines the generative and multi-fidelity surrogate models into a single framework, allowing for more accurate activity prediction and higher quality samples. Our experiments on two disease-relevant proteins show that MF-LAL produces compounds with significantly better binding free energy scores than other single and multi-fidelity approaches (~50% improvement in mean binding free energy score). The code is available at https://github.com/Rose-STL-Lab/MF-LAL."
      },
      {
        "id": "oai:arXiv.org:2410.14669v4",
        "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples",
        "link": "https://arxiv.org/abs/2410.14669",
        "author": "Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.14669v4 Announce Type: replace \nAbstract: Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs."
      },
      {
        "id": "oai:arXiv.org:2410.15461v2",
        "title": "EVA: An Embodied World Model for Future Video Anticipation",
        "link": "https://arxiv.org/abs/2410.15461",
        "author": "Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, Yike Guo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15461v2 Announce Type: replace \nAbstract: Video generation models have made significant progress in simulating future states, showcasing their potential as world simulators in embodied scenarios. However, existing models often lack robust understanding, limiting their ability to perform multi-step predictions or handle Out-of-Distribution (OOD) scenarios. To address this challenge, we propose the Reflection of Generation (RoG), a set of intermediate reasoning strategies designed to enhance video prediction. It leverages the complementary strengths of pre-trained vision-language and video generation models, enabling them to function as a world model in embodied scenarios. To support RoG, we introduce Embodied Video Anticipation Benchmark(EVA-Bench), a comprehensive benchmark that evaluates embodied world models across diverse tasks and scenarios, utilizing both in-domain and OOD datasets. Building on this foundation, we devise a world model, Embodied Video Anticipator (EVA), that follows a multistage training paradigm to generate high-fidelity video frames and apply an autoregressive strategy to enable adaptive generalization for longer video sequences. Extensive experiments demonstrate the efficacy of EVA in various downstream tasks like video generation and robotics, thereby paving the way for large-scale pre-trained models in real-world video prediction applications. The video demos are available at \\hyperlink{https://sites.google.com/view/icml-eva}{https://sites.google.com/view/icml-eva}."
      },
      {
        "id": "oai:arXiv.org:2410.15639v5",
        "title": "Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning",
        "link": "https://arxiv.org/abs/2410.15639",
        "author": "Yoichi Ishibashi, Taro Yano, Masafumi Oyamada",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15639v5 Announce Type: replace \nAbstract: Large Language Models (LLMs) have achieved remarkable capabilities, yet their improvement methods remain fundamentally constrained by human design. We present Self-Developing, a framework that enables LLMs to autonomously discover, implement, and refine their own improvement algorithms. Our approach employs an iterative cycle where a seed model generates algorithmic candidates as executable code, evaluates their effectiveness, and uses Direct Preference Optimization to recursively improve increasingly sophisticated improvement strategies. We demonstrate this framework through model merging, a practical technique for combining specialized models. Self-Developing successfully discovered novel merging algorithms that outperform existing human-designed algorithms. On mathematical reasoning benchmarks, the autonomously discovered algorithms improve the seed model's GSM8k performance by 6\\% and exceed human-designed approaches like Task Arithmetic by 4.3\\%. Remarkably, these algorithms exhibit strong generalization, achieving 7.4\\% gains on out-of-domain models without re-optimization. Our findings demonstrate that LLMs can transcend their training to invent genuinely novel optimization techniques. This capability represents a crucial step toward a new era where LLMs not only solve problems but autonomously develop the methodologies for their own advancement."
      },
      {
        "id": "oai:arXiv.org:2410.15876v4",
        "title": "FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL",
        "link": "https://arxiv.org/abs/2410.15876",
        "author": "Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, Se-Young Yun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15876v4 Announce Type: replace \nAbstract: Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically removed or added during the inference trajectory -- a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of intra-trajectory dynamic entity composition under zero-shot out-of-domain (OOD) generalization, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer significant performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a universally applicable augmentation technique for MARL backbone methods. FlickerFusion stochastically drops out parts of the observation space, emulating being in-domain when inferenced OOD. The results show that FlickerFusion not only achieves superior inference rewards but also uniquely reduces uncertainty vis-\\`a-vis the backbone, compared to existing methods. Benchmarks, implementations, and model weights are organized and open-sourced at flickerfusion305.github.io, accompanied by ample demo video renderings."
      },
      {
        "id": "oai:arXiv.org:2410.16267v2",
        "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
        "link": "https://arxiv.org/abs/2410.16267",
        "author": "Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Jongwoo Park, Kanchana Ranasinghe, Silvio Savarese, Ran Xu, Caiming Xiong, Juan Carlos Niebles",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.16267v2 Announce Type: replace \nAbstract: We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html"
      },
      {
        "id": "oai:arXiv.org:2410.18074v4",
        "title": "UnCLe: Benchmarking Unsupervised Continual Learning for Depth Completion",
        "link": "https://arxiv.org/abs/2410.18074",
        "author": "Xien Chen, Rit Gangopadhyay, Michael Chu, Patrick Rim, Hyoungseob Park, Alex Wong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.18074v4 Announce Type: replace \nAbstract: We propose UnCLe, the first standardized benchmark for Unsupervised Continual Learning of a multimodal 3D reconstruction task: Depth completion aims to infer a dense depth map from a pair of synchronized RGB image and sparse depth map. We benchmark depth completion models under the practical scenario of unsupervised learning over continuous streams of data. While unsupervised learning of depth boasts the possibility continual learning of novel data distributions over time, existing methods are typically trained on a static, or stationary, dataset. However, when adapting to novel nonstationary distributions, they ``catastrophically forget'' previously learned information. UnCLe simulates these non-stationary distributions by adapting depth completion models to sequences of datasets containing diverse scenes captured from distinct domains using different visual and range sensors. We adopt representative methods from continual learning paradigms and translate them to enable unsupervised continual learning of depth completion. We benchmark these models across indoor and outdoor environments, and investigate the degree of catastrophic forgetting through standard quantitative metrics. We find that unsupervised continual learning of depth completion is an open problem, and we invite researchers to leverage UnCLe as a development platform."
      },
      {
        "id": "oai:arXiv.org:2410.19317v2",
        "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs",
        "link": "https://arxiv.org/abs/2410.19317",
        "author": "Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.19317v2 Announce Type: replace \nAbstract: The growing use of large language model (LLM)-based chatbots has raised concerns about fairness. Fairness issues in LLMs can lead to severe consequences, such as bias amplification, discrimination, and harm to marginalized communities. While existing fairness benchmarks mainly focus on single-turn dialogues, multi-turn scenarios, which in fact better reflect real-world conversations, present greater challenges due to conversational complexity and potential bias accumulation. In this paper, we propose a comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios, \\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with each stage comprising two tasks. To ensure coverage of diverse bias types and attributes, we draw from existing fairness datasets and employ our template to construct a multi-turn dialogue dataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias classifiers including Llama-Guard-3 and human validation to ensure robustness. Experiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn dialogue scenarios, current LLMs are more likely to generate biased responses, and there is significant variation in performance across different tasks and models. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show the current state of fairness in LLMs and showcase the utility of this novel approach for assessing fairness in more realistic multi-turn dialogue contexts, calling for future work to focus on LLM fairness improvement and the adoption of \\texttt{FairMT-1K} in such efforts."
      },
      {
        "id": "oai:arXiv.org:2410.20682v2",
        "title": "SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script",
        "link": "https://arxiv.org/abs/2410.20682",
        "author": "Eunwon Kim, Chanho Park, Buru Chang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.20682v2 Announce Type: replace \nAbstract: Shared memories between two individuals strengthen their bond and are crucial for facilitating their ongoing conversations. This study aims to make long-term dialogue more engaging by leveraging these shared memories. To this end, we introduce a new long-term dialogue dataset named SHARE, constructed from movie scripts, which are a rich source of shared memories among various relationships. Our dialogue dataset contains the summaries of persona information and events of two individuals, as explicitly revealed in their conversation, along with implicitly extractable shared memories. We also introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes shared experiences between individuals. Through experiments using SHARE, we demonstrate that shared memories between two individuals make long-term dialogues more engaging and sustainable, and that EPISODE effectively manages shared memories during dialogue. Our dataset and code are available at https://github.com/e1kim/SHARE."
      },
      {
        "id": "oai:arXiv.org:2410.24200v2",
        "title": "Length-Induced Embedding Collapse in PLM-based Models",
        "link": "https://arxiv.org/abs/2410.24200",
        "author": "Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.24200v2 Announce Type: replace \nAbstract: Text embeddings from PLM-based models enable a wide range of applications, yet their performance often degrades on longer texts. In this paper, we introduce a phenomenon we call Length Collapse, where embeddings of longer texts tend to cluster together. This clustering results in a distributional inconsistency between the embeddings of short and long texts. We further investigate how these differences contribute to the performance decline observed with longer texts across various downstream tasks. Through a rigorous theoretical analysis of the self-attention mechanism, which acts as a low-pass filter in PLM-based models, we demonstrate that as text length increases, the strength of low-pass filtering intensifies, causing embeddings to retain more low-frequency components. As a result, input token features become more similar, leading to clustering and ultimately the collapse of embeddings for longer texts. To address this issue, we propose a simple method, TempScale, which mitigates the Length Collapse phenomenon. By narrowing the gap in low-pass filtering rates between long and short texts, TempScale ensures more consistent embeddings across different text lengths. This approach leads to performance improvements of 0.94% on MTEB and 1.10% on LongEmbed, which focuses specifically on long-context retrieval, providing strong evidence for the validity of our analysis. The source code is available at https://github.com/Yuqi-Zhou/Length_Collapse."
      },
      {
        "id": "oai:arXiv.org:2411.02299v2",
        "title": "Grouped Discrete Representation for Object-Centric Learning",
        "link": "https://arxiv.org/abs/2411.02299",
        "author": "Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.02299v2 Announce Type: replace \nAbstract: Object-Centric Learning (OCL) aims to discover objects in images or videos by reconstructing the input. Representative methods achieve this by reconstructing the input as its Variational Autoencoder (VAE) discrete representations, which suppress (super-)pixel noise and enhance object separability. However, these methods treat features as indivisible units, overlooking their compositional attributes, and discretize features via scalar code indexes, losing attribute-level similarities and differences. We propose Grouped Discrete Representation (GDR) for OCL. For better generalization, features are decomposed into combinatorial attributes by organized channel grouping. For better convergence, features are quantized into discrete representations via tuple code indexes. Experiments demonstrate that GDR consistently improves both mainstream and state-of-the-art OCL methods across various datasets. Visualizations further highlight GDR's superior object separability and interpretability. The source code is available on https://github.com/Genera1Z/GroupedDiscreteRepresentation."
      },
      {
        "id": "oai:arXiv.org:2411.03250v2",
        "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models",
        "link": "https://arxiv.org/abs/2411.03250",
        "author": "Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.03250v2 Announce Type: replace \nAbstract: Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code, and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2%-7% in certain cases. Data and code are available at https://github.com/bytedance/DiffLM."
      },
      {
        "id": "oai:arXiv.org:2411.04125v2",
        "title": "Community Forensics: Using Thousands of Generators to Train Fake Image Detectors",
        "link": "https://arxiv.org/abs/2411.04125",
        "author": "Jeongsoo Park, Andrew Owens",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.04125v2 Announce Type: replace \nAbstract: One of the key challenges of detecting AI-generated images is spotting images that have been created by previously unseen generative models. We argue that the limited diversity of the training data is a major obstacle to addressing this problem, and we propose a new dataset that is significantly larger and more diverse than prior work. As part of creating this dataset, we systematically download thousands of text-to-image latent diffusion models and sample images from them. We also collect images from dozens of popular open source and commercial models. The resulting dataset contains 2.7M images that have been sampled from 4803 different models. These images collectively capture a wide range of scene content, generator architectures, and image processing settings. Using this dataset, we study the generalization abilities of fake image detectors. Our experiments suggest that detection performance improves as the number of models in the training set increases, even when these models have similar architectures. We also find that detection performance improves as the diversity of the models increases, and that our trained detectors generalize better than those trained on other datasets. The dataset can be found in https://jespark.net/projects/2024/community_forensics"
      },
      {
        "id": "oai:arXiv.org:2411.10548v2",
        "title": "BioNeMo Framework: a modular, high-performance library for AI model development in drug discovery",
        "link": "https://arxiv.org/abs/2411.10548",
        "author": "Peter St. John, Dejun Lin, Polina Binder, Malcolm Greaves, Vega Shah, John St. John, Adrian Lange, Patrick Hsu, Rajesh Illango, Arvind Ramanathan, Anima Anandkumar, David H Brookes, Akosua Busia, Abhishaike Mahajan, Stephen Malina, Neha Prasad, Sam Sinai, Lindsay Edwards, Thomas Gaudelet, Cristian Regep, Martin Steinegger, Burkhard Rost, Alexander Brace, Kyle Hippe, Luca Naef, Keisuke Kamata, George Armstrong, Kevin Boyd, Zhonglin Cao, Han-Yi Chou, Simon Chu, Allan dos Santos Costa, Sajad Darabi, Eric Dawson, Kieran Didi, Cong Fu, Mario Geiger, Michelle Gill, Darren Hsu, Gagan Kaushik, Maria Korshunova, Steven Kothen-Hill, Youhan Lee, Meng Liu, Micha Livne, Zachary McClure, Jonathan Mitchell, Alireza Moradzadeh, Ohad Mosafi, Youssef Nashed, Saee Paliwal, Yuxing Peng, Sara Rabhi, Farhad Ramezanghorbani, Danny Reidenbach, Camir Ricketts, Brian Roland, Kushal Shah, Tyler Shimko, Hassan Sirelkhatim, Savitha Srinivasan, Abraham C Stern, Dorota Toczydlowska, Srimukh Prasad Veccham, Niccol\\`o Alberto Elia Venanzi, Anton Vorontsov, Jared Wilber, Isabel Wilkinson, Wei Jing Wong, Eva Xue, Cory Ye, Xin Yu, Yang Zhang, Guoqing Zhou, Becca Zandstein, Christian Dallago, Bruno Trentini, Emine Kucukbenli, Saee Paliwal, Timur Rvachov, Eddie Calleja, Johnny Israeli, Harry Clifford, Risto Haukioja, Nicholas Haemel, Kyle Tretina, Neha Tadimeti, Anthony B Costa",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.10548v2 Announce Type: replace \nAbstract: Artificial Intelligence models encoding biology and chemistry are opening new routes to high-throughput and high-quality in-silico drug development. However, their training increasingly relies on computational scale, with recent protein language models (pLM) training on hundreds of graphical processing units (GPUs). We introduce the BioNeMo Framework to facilitate the training of computational biology and chemistry AI models across hundreds of GPUs. Its modular design allows the integration of individual components, such as data loaders, into existing workflows and is open to community contributions. We detail technical features of the BioNeMo Framework through use cases such as pLM pre-training and fine-tuning. On 256 NVIDIA A100s, BioNeMo Framework trains a three billion parameter BERT-based pLM on over one trillion tokens in 4.2 days. The BioNeMo Framework is open-source and free for everyone to use."
      },
      {
        "id": "oai:arXiv.org:2411.15129v2",
        "title": "The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games",
        "link": "https://arxiv.org/abs/2411.15129",
        "author": "Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.15129v2 Announce Type: replace \nAbstract: What can we learn about language from studying how it is used by ChatGPT and other large language model (LLM)-based chatbots? In this paper, we analyse the distinctive character of language generated by ChatGPT, in relation to questions raised by natural language processing pioneer, and student of Wittgenstein, Margaret Masterman. Following frequent complaints that LLM-based chatbots produce \"slop,\" or even \"bullshit,\" in the sense of Frankfurt's popular monograph On Bullshit, we conduct an empirical study to contrast the language of 1,000 scientific publications with typical text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of political speech, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of sloppy bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language."
      },
      {
        "id": "oai:arXiv.org:2411.18142v2",
        "title": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual Conversion in Visual Reasoning for Multimodal Large Language Models",
        "link": "https://arxiv.org/abs/2411.18142",
        "author": "Jingming Liu, Yumeng Li, Boyuan Xiao, Yichang Jian, Ziang Qin, Tianjia Shao, Yao-Xiang Ding, Kun Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.18142v2 Announce Type: replace \nAbstract: Under pure textual modality, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning tasks by decomposing them into simpler sub-problems. However, Multimodal Large Language Models (MLLMs) still struggle with some seemingly straightforward visual tasks, such as counting and solving jigsaw puzzles. We argue that these tasks challenge the ability of visual-to-textual conversion, where MLLMs convert visual information perceived from the input scene, to textual information for further reasoning and generating the answer. If the complexity of the visual input is beyond the perceptual capability of the MLLMs, without decomposing this conversion process, simply scaling inference-time reasoning cannot solve the task because it repeatedly encounters the same perceptual bottleneck. We propose an approach, autonomous imagination, to enable MLLMs to iteratively modify visual inputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate visual states, decomposing visual-to-textual conversion into closed-loop visual modification steps. We show that, without any retraining, MLLMs can now solve tasks initially beyond their perceptual capability, highlighting that closed-loop visual modification can be an effective way of decomposing the visual reasoning task into solvable substeps. Project page: https://future-item.github.io/autoimagine-site/"
      },
      {
        "id": "oai:arXiv.org:2411.19339v3",
        "title": "Towards a Mechanistic Explanation of Diffusion Model Generalization",
        "link": "https://arxiv.org/abs/2411.19339",
        "author": "Matthew Niedoba, Berend Zwartsenberg, Kevin Murphy, Frank Wood",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.19339v3 Announce Type: replace \nAbstract: We propose a simple, training-free mechanism which explains the generalization behaviour of diffusion models. By comparing pre-trained diffusion models to their theoretically optimal empirical counterparts, we identify a shared local inductive bias across a variety of network architectures. From this observation, we hypothesize that network denoisers generalize through localized denoising operations, as these operations approximate the training objective well over much of the training distribution. To validate our hypothesis, we introduce novel denoising algorithms which aggregate local empirical denoisers to replicate network behaviour. Comparing these algorithms to network denoisers across forward and reverse diffusion processes, our approach exhibits consistent visual similarity to neural network outputs, with lower mean squared error than previously proposed methods."
      },
      {
        "id": "oai:arXiv.org:2412.03671v2",
        "title": "Tight Lower Bounds and Improved Convergence in Performative Prediction",
        "link": "https://arxiv.org/abs/2412.03671",
        "author": "Pedram Khorsandi, Rushil Gupta, Mehrnaz Mofakhami, Simon Lacoste-Julien, Gauthier Gidel",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03671v2 Announce Type: replace \nAbstract: Performative prediction is a framework accounting for the shift in the data distribution induced by the prediction of a model deployed in the real world. Ensuring rapid convergence to a stable solution where the data distribution remains the same after the model deployment is crucial, especially in evolving environments. This paper extends the Repeated Risk Minimization (RRM) framework by utilizing historical datasets from previous retraining snapshots, yielding a class of algorithms that we call Affine Risk Minimizers and enabling convergence to a performatively stable point for a broader class of problems. We introduce a new upper bound for methods that use only the final iteration of the dataset and prove for the first time the tightness of both this new bound and the previous existing bounds within the same regime. We also prove that utilizing historical datasets can surpass the lower bound for last iterate RRM, and empirically observe faster convergence to the stable point on various performative prediction benchmarks. We offer at the same time the first lower bound analysis for RRM within the class of Affine Risk Minimizers, quantifying the potential improvements in convergence speed that could be achieved with other variants in our framework."
      },
      {
        "id": "oai:arXiv.org:2412.03719v2",
        "title": "From Language Models over Tokens to Language Models over Characters",
        "link": "https://arxiv.org/abs/2412.03719",
        "author": "Tim Vieira, Ben LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian DuSell, John Terilla, Timothy J. O'Donnell, Ryan Cotterell",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03719v2 Announce Type: replace \nAbstract: Modern language models are internally -- and mathematically -- distributions over $\\it{token}$ strings rather than $\\it{character}$ strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent processing are very sensitive to the specification of the prompt (e.g., whether the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. Across four publicly available language models, we find that -- even with a small computation budget -- our method is able to accurately approximate the character-level distribution at reasonably fast speeds, and that a significant improvement in the language model's compression rate (bits/byte) is achieved."
      },
      {
        "id": "oai:arXiv.org:2412.04323v2",
        "title": "GRAM: Generalization in Deep RL with a Robust Adaptation Module",
        "link": "https://arxiv.org/abs/2412.04323",
        "author": "James Queeney, Xiaoyi Cai, Alexander Schperberg, Radu Corcodel, Mouhacine Benosman, Jonathan P. How",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.04323v2 Announce Type: replace \nAbstract: The reliable deployment of deep reinforcement learning in real-world settings requires the ability to generalize across a variety of conditions, including both in-distribution scenarios seen during training as well as novel out-of-distribution scenarios. In this work, we present a framework for dynamics generalization in deep reinforcement learning that unifies these two distinct types of generalization within a single architecture. We introduce a robust adaptation module that provides a mechanism for identifying and reacting to both in-distribution and out-of-distribution environment dynamics, along with a joint training pipeline that combines the goals of in-distribution adaptation and out-of-distribution robustness. Our algorithm GRAM achieves strong generalization performance across in-distribution and out-of-distribution scenarios upon deployment, which we demonstrate through extensive simulation and hardware locomotion experiments on a quadruped robot."
      },
      {
        "id": "oai:arXiv.org:2412.09569v2",
        "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
        "link": "https://arxiv.org/abs/2412.09569",
        "author": "Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.09569v2 Announce Type: replace \nAbstract: Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias."
      },
      {
        "id": "oai:arXiv.org:2412.10494v2",
        "title": "SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device",
        "link": "https://arxiv.org/abs/2412.10494",
        "author": "Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui, Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, Dimitris Metaxas, Yanzhi Wang, Sergey Tulyakov, Jian Ren",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10494v2 Announce Type: replace \nAbstract: We have witnessed the unprecedented success of diffusion-based video generation over the past year. Recently proposed models from the community have wielded the power to generate cinematic and high-resolution videos with smooth motions from arbitrary input prompts. However, as a supertask of image generation, video generation models require more computation and are thus hosted mostly on cloud servers, limiting broader adoption among content creators. In this work, we propose a comprehensive acceleration framework to bring the power of the large-scale video diffusion model to the hands of edge users. From the network architecture scope, we initialize from a compact image backbone and search out the design and arrangement of temporal layers to maximize hardware efficiency. In addition, we propose a dedicated adversarial fine-tuning algorithm for our efficient model and reduce the denoising steps to 4. Our model, with only 0.6B parameters, can generate a 5-second video on an iPhone 16 PM within 5 seconds. Compared to server-side models that take minutes on powerful GPUs to generate a single video, we accelerate the generation by magnitudes while delivering on-par quality."
      },
      {
        "id": "oai:arXiv.org:2412.10652v2",
        "title": "CENTAUR: Bridging the Impossible Trinity of Privacy, Efficiency, and Performance in Privacy-Preserving Transformer Inference",
        "link": "https://arxiv.org/abs/2412.10652",
        "author": "Jinglong Luo, Guanzhong Chen, Yehong Zhang, Shiyu Liu, Hui Wang, Yue Yu, Xun Zhou, Yuan Qi, Zenglin Xu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10652v2 Announce Type: replace \nAbstract: With the growing deployment of pre-trained models like Transformers on cloud platforms, privacy concerns about model parameters and inference data are intensifying. Existing Privacy-Preserving Transformer Inference (PPTI) frameworks face the \"impossible trinity\" of balancing privacy, efficiency, and performance: Secure Multi-Party Computation (SMPC)-based approaches ensure strong privacy but suffer from high computational overhead and performance losses; Conversely, permutation-based methods achieve near-plaintext efficiency and accuracy but compromise privacy by exposing sensitive model parameters and intermediate results. Bridging this gap with a single approach presents substantial challenges, motivating the introduction of CENTAUR, a groundbreaking PPTI framework that seamlessly integrates random permutations and SMPC to address the \"impossible trinity\". By designing efficient PPTI algorithms tailored to the structural properties of Transformer models, CENTAUR achieves an unprecedented balance among privacy, efficiency, and performance. Our experiments demonstrate CENTAUR's ability to resist diverse data reconstruction attacks, achieve plaintext-level inference accuracy, and boost inference speed by 5.0-30.4 times, unlocking new possibilities for secure and efficient AI deployment."
      },
      {
        "id": "oai:arXiv.org:2412.13949v3",
        "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence",
        "link": "https://arxiv.org/abs/2412.13949",
        "author": "Jinghan He, Kuan Zhu, Haiyun Guo, Junfeng Fang, Zhenglin Hua, Yuheng Jia, Ming Tang, Tat-Seng Chua, Jinqiao Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.13949v3 Announce Type: replace \nAbstract: Large vision-language models (LVLMs) have made substantial progress in integrating large language models (LLMs) with visual inputs, enabling advanced multimodal reasoning. Despite their success, a persistent challenge is hallucination-where generated text fails to accurately reflect visual content-undermining both accuracy and reliability. Existing methods focus on alignment training or decoding refinements but primarily address symptoms at the generation stage without probing the underlying causes. In this work, we investigate the internal mechanisms driving hallucination in LVLMs, with an emphasis on the multi-head attention module. Specifically, we introduce Vision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of attention head outputs to visual context. Based on this, our findings reveal the presence of vision-aware attention heads that are more attuned to visual information; however, the model's overreliance on its prior language patterns is closely related to hallucinations. Building on these insights, we propose Vision-aware Head Reinforcement (VHR), a training-free approach to mitigate hallucination by enhancing the role of vision-aware attention heads. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches in mitigating hallucinations, while maintaining high efficiency with negligible additional time overhead."
      },
      {
        "id": "oai:arXiv.org:2412.17499v2",
        "title": "Improving the Noise Estimation of Latent Neural Stochastic Differential Equations",
        "link": "https://arxiv.org/abs/2412.17499",
        "author": "Linus Heck, Maximilian Gelbrecht, Michael T. Schaub, Niklas Boers",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17499v2 Announce Type: replace \nAbstract: Latent neural stochastic differential equations (SDEs) have recently emerged as a promising approach for learning generative models from stochastic time series data. However, they systematically underestimate the noise level inherent in such data, limiting their ability to capture stochastic dynamics accurately. We investigate this underestimation in detail and propose a straightforward solution: by including an explicit additional noise regularization in the loss function, we are able to learn a model that accurately captures the diffusion component of the data. We demonstrate our results on a conceptual model system that highlights the improved latent neural SDE's capability to model stochastic bistable dynamics."
      },
      {
        "id": "oai:arXiv.org:2412.18874v2",
        "title": "A Culturally-Aware Benchmark for Person Re-Identification in Modest Attire",
        "link": "https://arxiv.org/abs/2412.18874",
        "author": "Alireza Sedighi Moghaddam, Fatemeh Anvari, Mohammadjavad Mirshekari Haghighi, Mohammadali Fakhari, Mohammad Reza Mohammadi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.18874v2 Announce Type: replace \nAbstract: Person Re-Identification (ReID) is a fundamental task in computer vision with critical applications in surveillance and security. Despite progress in recent years, most existing ReID models often struggle to generalize across diverse cultural contexts, particularly in Islamic regions like Iran, where modest clothing styles are prevalent. Existing datasets predominantly feature Western and East Asian fashion, limiting their applicability in these settings. To address this gap, we introduce Iran University of Science and Technology Person Re-Identification (IUST_PersonReId), a dataset designed to reflect the unique challenges of ReID in new cultural environments, emphasizing modest attire and diverse scenarios from Iran, including markets, campuses, and mosques. Experiments on IUST_PersonReId with state-of-the-art models, such as Semantic Controllable Self-supervised Learning (SOLIDER) and Contrastive Language-Image Pretraining Re-Identification (CLIP-ReID), reveal significant performance drops compared to benchmarks like Market1501 and Multi-Scene MultiTime (MSMT17), specifically, SOLIDER shows a drop of 50.75% and 23.01% Mean Average Precision (mAP) compared to Market1501 and MSMT17 respectively, while CLIP-ReID exhibits a drop of 38.09% and 21.74% mAP, highlighting the challenges posed by occlusion and limited distinctive features. Sequence-based evaluations show improvements by leveraging temporal context, emphasizing the dataset's potential for advancing culturally sensitive and robust ReID systems. IUST_PersonReId offers a critical resource for addressing fairness and bias in ReID research globally."
      },
      {
        "id": "oai:arXiv.org:2501.08617v3",
        "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
        "link": "https://arxiv.org/abs/2501.08617",
        "author": "Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fern\\'andez Fisac",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.08617v3 Announce Type: replace \nAbstract: While Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning generative AI, we present empirical evidence that it can also cause severe, systematic misalignment. We hypothesize that this stems from evaluator feedback depending on downstream outcome predictions (foresight) that can be influenced by the AI's output, inducing Goodhart's law dynamics. We present a theoretical analysis showing that conditioning evaluator feedback on downstream observations (hindsight) inhibits this effect by decoupling the alignment signal from potentially compromised predictions--crucially, the result holds even if the observed outcomes are sampled from the AI's own world model. Building on this insight, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which presents plausible simulated outcomes to evaluators before eliciting feedback. We validate RLHS across three consultancy settings--marketplace interactions, restaurant recommendations, and online course advising--using both online (PPO) and offline (DPO) fine-tuning methods, and show that it substantially improves alignment over RLHF in experiments and human evaluations. We perform post-hoc benchmark evaluations on TruthfulQA, HaluEval, and TrustLLM, finding that even after single-task fine-tuning, RLHF misalignment persists, whereas RLHS consistently outperforms baselines and demonstrates robust alignment generalization. The project webpage and code are available at https://rl-hindsight.github.io."
      },
      {
        "id": "oai:arXiv.org:2501.15513v2",
        "title": "TinyLLaVA-Video: Towards Smaller LMMs for Video Understanding with Group Resampler",
        "link": "https://arxiv.org/abs/2501.15513",
        "author": "Xingjian Zhang, Xi Weng, Yihao Yue, Zhaoxin Fan, Wenjun Wu, Lei Huang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.15513v2 Announce Type: replace \nAbstract: Video behavior recognition and scene understanding are fundamental tasks in multimodal intelligence, serving as critical building blocks for numerous real-world applications. Through large multimodal models (LMMs) have achieved remarkable progress in video understanding, most existing open-source models rely on over 7B parameters and require large-scale datasets for training, making them resource-intensive and inaccessible to many researchers. Furthermore, lightweight models face persistent challenges in effectively processing long visual sequences and temporal understanding. In this work, we introduce TinyLLaVA-Video, a lightweight yet powerful video understanding model with approximately 3.6B parameters. The cornerstone of our design is the video-level group resampler, a novel mechanism that significantly reduces and controls the number of visual tokens at the video level. Unlike traditional image-level resampler, our approach effectively mitigates redundancy while enhancing temporal comprehension, leading to improved performance on video-based tasks. In addition, TinyLLaVA-Video demonstrates exceptional efficiency, requiring only one day of training on 8 A100-40G GPUs. It surpasses several existing 7B-parameter models on multiple benchmarks. We believe this work provides a valuable foundation for future research on lightweight video understanding models. The code and weights is available at https://github.com/ZhangXJ199/TinyLLaVA-Video."
      },
      {
        "id": "oai:arXiv.org:2501.19090v2",
        "title": "Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models",
        "link": "https://arxiv.org/abs/2501.19090",
        "author": "Jialin Zhao, Yingtao Zhang, Carlo Vittorio Cannistraci",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.19090v2 Announce Type: replace \nAbstract: The rapid growth of Large Language Models has driven demand for effective model compression techniques to reduce memory and computation costs. Low-rank pruning has gained attention for its GPU compatibility across all densities. However, low-rank pruning struggles to match the performance of semi-structured pruning, often doubling perplexity at similar densities. In this paper, we propose Pivoting Factorization (PIFA), a novel lossless meta low-rank representation that unsupervisedly learns a compact form of any low-rank representation, effectively eliminating redundant information. PIFA identifies pivot rows (linearly independent rows) and expresses non-pivot rows as linear combinations, achieving 24.2% additional memory savings and 24.6% faster inference over low-rank layers at rank = 50% of dimension. To mitigate the performance degradation caused by low-rank pruning, we introduce a novel, retraining-free reconstruction method that minimizes error accumulation (M). MPIFA, combining M and PIFA into an end-to-end framework, significantly outperforms existing low-rank pruning methods, and achieves performance comparable to semi-structured pruning, while surpassing it in GPU efficiency and compatibility. Our code is available at https://github.com/biomedical-cybernetics/pivoting-factorization."
      },
      {
        "id": "oai:arXiv.org:2502.00277v2",
        "title": "Regularized Langevin Dynamics for Combinatorial Optimization",
        "link": "https://arxiv.org/abs/2502.00277",
        "author": "Shengyu Feng, Yiming Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00277v2 Announce Type: replace \nAbstract: This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative paradigm. However, we observe that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA), and the other one based on neural network (NN). Empirical results on three classic CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA algorithm reduces the runtime of the previous SOTA SA method by up to 80\\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. Our code is available at https://github.com/Shengyu-Feng/RLD4CO."
      },
      {
        "id": "oai:arXiv.org:2502.00846v3",
        "title": "Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework",
        "link": "https://arxiv.org/abs/2502.00846",
        "author": "Terje Mildner, Oliver Hamelijnck, Paris Giampouras, Theodoros Damoulas",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00846v3 Announce Type: replace \nAbstract: We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets."
      },
      {
        "id": "oai:arXiv.org:2502.02316v2",
        "title": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning",
        "link": "https://arxiv.org/abs/2502.02316",
        "author": "Onur Celik, Zechu Li, Denis Blessing, Ge Li, Daniel Palenicek, Jan Peters, Georgia Chalvatzaki, Gerhard Neumann",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.02316v2 Announce Type: replace \nAbstract: Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \\emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity."
      },
      {
        "id": "oai:arXiv.org:2502.02444v5",
        "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models",
        "link": "https://arxiv.org/abs/2502.02444",
        "author": "Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.02444v5 Announce Type: replace \nAbstract: Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values."
      },
      {
        "id": "oai:arXiv.org:2502.02545v2",
        "title": "Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
        "link": "https://arxiv.org/abs/2502.02545",
        "author": "Leonardo Defilippis, Yatin Dandi, Pierre Mergny, Florent Krzakala, Bruno Loureiro",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.02545v2 Announce Type: replace \nAbstract: We consider the problem of how many samples from a Gaussian multi-index model are required to weakly reconstruct the relevant index subspace. Despite its increasing popularity as a testbed for investigating the computational complexity of neural networks, results beyond the single-index setting remain elusive. In this work, we introduce spectral algorithms based on the linearization of a message passing scheme tailored to this problem. Our main contribution is to show that the proposed methods achieve the optimal reconstruction threshold. Leveraging a high-dimensional characterization of the algorithms, we show that above the critical threshold the leading eigenvector correlates with the relevant index subspace, a phenomenon reminiscent of the Baik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix theory. Supported by numerical experiments and a rigorous theoretical framework, our work bridges critical gaps in the computational limits of weak learnability in multi-index model."
      },
      {
        "id": "oai:arXiv.org:2502.02958v2",
        "title": "Position: Editing Large Language Models Poses Serious Safety Risks",
        "link": "https://arxiv.org/abs/2502.02958",
        "author": "Paul Youssef, Zhixue Zhao, Daniel Braun, J\\\"org Schl\\\"otterer, Christin Seifert",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.02958v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem."
      },
      {
        "id": "oai:arXiv.org:2502.03081v3",
        "title": "Human-Aligned Image Models Improve Visual Decoding from the Brain",
        "link": "https://arxiv.org/abs/2502.03081",
        "author": "Nona Rajabi, Ant\\^onio H. Ribeiro, Miguel Vasco, Farzaneh Taleb, M\\r{a}rten Bj\\\"orkman, Danica Kragic",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03081v3 Announce Type: replace \nAbstract: Decoding visual images from brain activity has significant potential for advancing brain-computer interaction and enhancing the understanding of human perception. Recent approaches align the representation spaces of images and brain activity to enable visual decoding. In this paper, we introduce the use of human-aligned image encoders to map brain signals to images. We hypothesize that these models more effectively capture perceptual attributes associated with the rapid visual stimuli presentations commonly used in visual brain data recording experiments. Our empirical results support this hypothesis, demonstrating that this simple modification improves image retrieval accuracy by up to 21% compared to state-of-the-art methods. Comprehensive experiments confirm consistent performance improvements across diverse EEG architectures, image encoders, alignment methods, participants, and brain imaging modalities"
      },
      {
        "id": "oai:arXiv.org:2502.03699v2",
        "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
        "link": "https://arxiv.org/abs/2502.03699",
        "author": "Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03699v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research."
      },
      {
        "id": "oai:arXiv.org:2502.04390v2",
        "title": "In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs",
        "link": "https://arxiv.org/abs/2502.04390",
        "author": "Simone Clemente, Zied Ben Houidi, Alexis Huet, Dario Rossi, Giulio Franzese, Pietro Michiardi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04390v2 Announce Type: replace \nAbstract: Through systematic empirical investigation, we uncover a fundamental and concerning property of Large Language Models: while they can safely learn facts that don't contradict their knowledge, attempting to update facts with contradictory information triggers catastrophic corruption of unrelated knowledge. Unlike humans, who naturally resist contradictory information, these models indiscriminately accept contradictions, leading to devastating interference, destroying up to 80% of unrelated knowledge even when learning as few as 10-100 contradicting facts. To understand whether this interference could be mitigated through selective plasticity, we experiment with targeted network updates, distinguishing between previously used (stubborn) and rarely used (plastic) neurons. We uncover another asymmetry: while sparing frequently-used neurons significantly improves retention of existing knowledge for non-contradictory updates (98% vs 93% with standard updates), contradictory updates trigger catastrophic interference regardless of targeting strategy. This effect which persists across tested model scales (GPT-2 to GPT-J-6B), suggests a fundamental limitation in how neural networks handle contradictions. Finally, we demonstrate that contradictory information can be reliably detected (95%+ accuracy) using simple model features, offering a potential protective mechanism. These findings motivate new architectures that can, like humans, naturally resist contradictions rather than allowing destructive overwrites."
      },
      {
        "id": "oai:arXiv.org:2502.04406v2",
        "title": "Calibrated Physics-Informed Uncertainty Quantification",
        "link": "https://arxiv.org/abs/2502.04406",
        "author": "Vignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Daniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04406v2 Announce Type: replace \nAbstract: Simulating complex physical systems is crucial for understanding and predicting phenomena across diverse fields, such as fluid dynamics and heat transfer, as well as plasma physics and structural mechanics. Traditional approaches rely on solving partial differential equations (PDEs) using numerical methods, which are computationally expensive and often prohibitively slow for real-time applications or large-scale simulations. Neural PDEs have emerged as efficient alternatives to these costly numerical solvers, offering significant computational speed-ups. However, their lack of robust uncertainty quantification (UQ) limits deployment in critical applications. We introduce a model-agnostic, physics-informed conformal prediction (CP) framework that provides guaranteed uncertainty estimates without requiring labelled data. By utilising a physics-based approach, we can quantify and calibrate the model's inconsistencies with the physics rather than the uncertainty arising from the data. Our approach utilises convolutional layers as finite-difference stencils and leverages physics residual errors as nonconformity scores, enabling data-free UQ with marginal and joint coverage guarantees across prediction domains for a range of complex PDEs. We further validate the efficacy of our method on neural PDE models for plasma modelling and shot design in fusion reactors."
      },
      {
        "id": "oai:arXiv.org:2502.04573v2",
        "title": "Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer",
        "link": "https://arxiv.org/abs/2502.04573",
        "author": "Yulun Wu, Doron L. Bergman",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04573v2 Announce Type: replace \nAbstract: We present an Adversarially Pre-trained Transformer (APT) that is able to perform zero-shot meta-learning on tabular prediction tasks without pre-training on any real-world dataset, extending on the recent development of Prior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained with adversarial synthetic data agents, who continue to shift their underlying data generating distribution and deliberately challenge the model with different synthetic datasets. In addition, we propose a mixture block architecture that is able to handle classification tasks with arbitrary number of classes, addressing the class size limitation -- a crucial weakness of prior deep tabular zero-shot learners. In experiments, we show that our framework matches state-of-the-art performance on small classification tasks without filtering on dataset characteristics such as number of classes and number of missing values, while maintaining an average runtime under one second. On common benchmark dataset suites in both classification and regression, we show that adversarial pre-training was able to enhance TabPFN's performance. In our analysis, we demonstrate that the adversarial synthetic data agents were able to generate a more diverse collection of data compared to the ordinary random generator in TabPFN. In addition, we demonstrate that our mixture block neural design has improved generalizability and greatly accelerated pre-training."
      },
      {
        "id": "oai:arXiv.org:2502.05202v2",
        "title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies",
        "link": "https://arxiv.org/abs/2502.05202",
        "author": "Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Gaurav Jain, Oren Pereg, Moshe Wasserblat, David Harel",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05202v2 Announce Type: replace \nAbstract: Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms demonstrate significant speedups of up to 2.8x over standard autoregressive decoding. By enabling any off-the-shelf model to serve as a drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice."
      },
      {
        "id": "oai:arXiv.org:2502.06905v2",
        "title": "Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty",
        "link": "https://arxiv.org/abs/2502.06905",
        "author": "Yeseul Cho, Baekrok Shin, Changmin Kang, Chulhee Yun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06905v2 Announce Type: replace \nAbstract: Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs. Dataset pruning aims to alleviate this demand by discarding redundant examples. However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset. To overcome this limitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL) score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning, we further propose a ratio-adaptive sampling using Beta distribution. Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66% compared to previous methods while achieving a SOTA, specifically 60% test accuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15% while maintaining SOTA performance."
      },
      {
        "id": "oai:arXiv.org:2502.07306v2",
        "title": "TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation",
        "link": "https://arxiv.org/abs/2502.07306",
        "author": "Navid Rajabi, Jana Kosecka",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07306v2 Announce Type: replace \nAbstract: In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction dataset and quantify in detail the effect of visual grounding on navigation performance."
      },
      {
        "id": "oai:arXiv.org:2502.07783v3",
        "title": "Curvature Tuning: Provable Training-free Model Steering From a Single Parameter",
        "link": "https://arxiv.org/abs/2502.07783",
        "author": "Leyang Hu, Matteo Gamba, Randall Balestriero",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07783v3 Announce Type: replace \nAbstract: The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a model's decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions-thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, and improves robust accuracy on the $\\ell_\\infty$ benchmark from RobustBench by 1032.64%/1494.46%. Our code is available at https://github.com/Leon-Leyang/curvature-tuning."
      },
      {
        "id": "oai:arXiv.org:2502.09198v2",
        "title": "Understanding High-Dimensional Bayesian Optimization",
        "link": "https://arxiv.org/abs/2502.09198",
        "author": "Leonard Papenmeier, Matthias Poloczek, Luigi Nardi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09198v2 Announce Type: replace \nAbstract: Recent work reported that simple Bayesian optimization (BO) methods perform well for high-dimensional real-world tasks, seemingly contradicting prior work and tribal knowledge. This paper investigates why. We identify underlying challenges that arise in high-dimensional BO and explain why recent methods succeed. Our empirical analysis shows that vanishing gradients caused by Gaussian process (GP) initialization schemes play a major role in the failures of high-dimensional Bayesian optimization (HDBO) and that methods that promote local search behaviors are better suited for the task. We find that maximum likelihood estimation (MLE) of GP length scales suffices for state-of-the-art performance. Based on this, we propose a simple variant of MLE called MSR that leverages these findings to achieve state-of-the-art performance on a comprehensive set of real-world applications. We present targeted experiments to illustrate and confirm our findings."
      },
      {
        "id": "oai:arXiv.org:2502.09885v2",
        "title": "Comprehensive Review of Neural Differential Equations for Time Series Analysis",
        "link": "https://arxiv.org/abs/2502.09885",
        "author": "YongKyung Oh, Seungsu Kam, Jonghun Lee, Dong-Young Lim, Sungil Kim, Alex Bui",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09885v2 Announce Type: replace \nAbstract: Time series modeling and analysis have become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis."
      },
      {
        "id": "oai:arXiv.org:2502.10550v2",
        "title": "Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2502.10550",
        "author": "Egor Cherepanov, Nikita Kachaev, Alexey K. Kovalev, Aleksandr I. Panov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.10550v2 Announce Type: replace \nAbstract: Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularly evident in tabletop robotic manipulation, where memory is essential for solving tasks with partial observability and ensuring robust performance, yet no standardized benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark for memory RL, with three key contributions: (1) we propose a comprehensive classification framework for memory-intensive RL tasks, (2) we collect MIKASA-Base -- a unified benchmark that enables systematic evaluation of memory-enhanced agents across diverse scenarios, and (3) we develop MIKASA-Robo (pip install mikasa-robo-suite) -- a novel benchmark of 32 carefully designed memory-intensive tasks that assess memory capabilities in tabletop robotic manipulation. Our work introduces a unified framework to advance memory RL research, enabling more robust systems for real-world use. MIKASA is available at https://tinyurl.com/membenchrobots."
      },
      {
        "id": "oai:arXiv.org:2502.11420v2",
        "title": "Training-Free Guidance Beyond Differentiability: Scalable Path Steering with Tree Search in Diffusion and Flow Models",
        "link": "https://arxiv.org/abs/2502.11420",
        "author": "Yingqing Guo, Yukang Yang, Hui Yuan, Mengdi Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11420v2 Announce Type: replace \nAbstract: Training-free guidance enables controlled generation in diffusion and flow models, but most methods rely on gradients and assume differentiable objectives. This work focuses on training-free guidance addressing challenges from non-differentiable objectives and discrete data distributions. We propose TreeG: Tree Search-Based Path Steering Guidance, applicable to both continuous and discrete settings in diffusion and flow models. TreeG offers a unified framework for training-free guidance by proposing, evaluating, and selecting candidates at each step, enhanced with tree search over active paths and parallel exploration. We comprehensively investigate the design space of TreeG over the candidate proposal module and the evaluation function, instantiating TreeG into three novel algorithms. Our experiments show that TreeG consistently outperforms top guidance baselines in symbolic music generation, small molecule design, and enhancer DNA design with improvements of 29.01%, 16.6%, and 18.43%. Additionally, we identify an inference-time scaling law showing TreeG's scalability in inference-time computation."
      },
      {
        "id": "oai:arXiv.org:2502.11672v2",
        "title": "Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs",
        "link": "https://arxiv.org/abs/2502.11672",
        "author": "Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11672v2 Announce Type: replace \nAbstract: We derive exact upper and lower bounds for the cumulative distribution function (cdf) of the output of a neural network (NN) over its entire support subject to noisy (stochastic) inputs. The upper and lower bounds converge to the true cdf over its domain as the resolution increases. Our method applies to any feedforward NN using continuous monotonic piecewise twice continuously differentiable activation functions (e.g., ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and instrumental tool of our approach is to bound general NNs with ReLU NNs. The ReLU NN-based bounds are then used to derive the upper and lower bounds of the cdf of the NN output. Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches."
      },
      {
        "id": "oai:arXiv.org:2502.12658v2",
        "title": "R.R.: Unveiling LLM Training Privacy through Recollection and Ranking",
        "link": "https://arxiv.org/abs/2502.12658",
        "author": "Wenlong Meng, Zhenyuan Guo, Lenan Wu, Chen Gong, Wenyan Liu, Weixian Li, Chengkun Wei, Wenzhi Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12658v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) pose significant privacy risks, potentially leaking training data due to implicit memorization. Existing privacy attacks primarily focus on membership inference attacks (MIAs) or data extraction attacks, but reconstructing specific personally identifiable information (PII) in LLMs' training data remains challenging. In this paper, we propose R.R. (Recollect and Rank), a novel two-step privacy stealing attack that enables attackers to reconstruct PII entities from scrubbed training data where the PII entities have been masked. In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks. Then we can use PII identifiers to extract recollected PII candidates. In the second stage, we design a new criterion to score each PII candidate and rank them. Motivated by membership inference, we leverage the reference model as a calibration to our criterion. Experiments across three popular PII datasets demonstrate that the R.R. achieves better PII identification performance than baselines. These results highlight the vulnerability of LLMs to PII leakage even when training data has been scrubbed. We release our code and datasets at GitHub."
      },
      {
        "id": "oai:arXiv.org:2502.13191v3",
        "title": "On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis",
        "link": "https://arxiv.org/abs/2502.13191",
        "author": "Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.13191v3 Announce Type: replace \nAbstract: Spiking Neural Networks (SNNs) are increasingly explored for their energy efficiency and robustness in real-world applications, yet their privacy risks remain largely unexamined. In this work, we investigate the susceptibility of SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an adversary attempts to determine whether a given sample was part of the training dataset. While prior work suggests that SNNs may offer inherent robustness due to their discrete, event-driven nature, we find that its resilience diminishes as latency (T) increases. Furthermore, we introduce an input dropout strategy under black box setting, that significantly enhances membership inference in SNNs. Our findings challenge the assumption that SNNs are inherently more secure, and even though they are expected to be better, our results reveal that SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial Neural Networks (ANNs). Our code is available at https://anonymous.4open.science/r/MIA_SNN-3610."
      },
      {
        "id": "oai:arXiv.org:2502.14898v2",
        "title": "Retrieval-augmented systems can be dangerous medical communicators",
        "link": "https://arxiv.org/abs/2502.14898",
        "author": "Lionel Wong, Ayman Ali, Raymond Xiong, Shannon Zeijang Shen, Yoon Kim, Monica Agrawal",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14898v2 Announce Type: replace \nAbstract: Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations -- such as the incorporation of communication pragmatics and enhanced comprehension of source documents -- that could help mitigate these issues and extend beyond the medical domain."
      },
      {
        "id": "oai:arXiv.org:2502.15226v2",
        "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews",
        "link": "https://arxiv.org/abs/2502.15226",
        "author": "Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15226v2 Announce Type: replace \nAbstract: Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interact with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then be interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, e.g., the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our code and data are at https://github.com/cxcscmu/LLM-Interviewer."
      },
      {
        "id": "oai:arXiv.org:2502.15794v2",
        "title": "Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction",
        "link": "https://arxiv.org/abs/2502.15794",
        "author": "Yudong W. Xu, Wenhao Li, Scott Sanner, Elias B. Khalil",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15794v2 Announce Type: replace \nAbstract: We present a Transformer-based framework for Constraint Satisfaction Problems (CSPs). CSPs find use in many applications and thus accelerating their solution with machine learning is of wide interest. Most existing approaches rely on supervised learning from feasible solutions or reinforcement learning, paradigms that require either feasible solutions to these NP-Complete CSPs or large training budgets and a complex expert-designed reward signal. To address these challenges, we propose ConsFormer, a self-supervised framework that leverages a Transformer as a solution refiner. ConsFormer constructs a solution to a CSP iteratively in a process that mimics local search. Instead of using feasible solutions as labeled data, we devise differentiable approximations to the discrete constraints of a CSP to guide model training. Our model is trained to improve random assignments for a single step but is deployed iteratively at test time, circumventing the bottlenecks of supervised and reinforcement learning. Experiments on Sudoku, Graph Coloring, Nurse Rostering, and MAXCUT demonstrate that our method can tackle out-of-distribution CSPs simply through additional iterations."
      },
      {
        "id": "oai:arXiv.org:2502.15843v2",
        "title": "Implicit Neural Representations for Chemical Reaction Paths",
        "link": "https://arxiv.org/abs/2502.15843",
        "author": "Kalyan Ramakrishnan, Lars L. Schaaf, Chen Lin, Guangrun Wang, Philip Torr",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15843v2 Announce Type: replace \nAbstract: We show that neural networks can be optimized to represent minimum energy paths as continuous functions, offering a flexible alternative to discrete path-search methods like Nudged Elastic Band (NEB). Our approach parameterizes reaction paths with a network trained on a loss function that discards tangential energy gradients and enables instant estimation of the transition state. We first validate the method on two-dimensional potentials and then demonstrate its advantages over NEB on challenging atomistic systems where (i) poor initial guesses yield unphysical paths, (ii) multiple competing paths exist, or (iii) the reaction follows a complex multi-step mechanism. Results highlight the versatility of the method: for instance, a simple adjustment to the sampling strategy during optimization can help escape local-minimum solutions. Finally, in a low-dimensional setting, we demonstrate that a single neural network can learn from existing paths and generalize to unseen systems, showing promise for a universal reaction path representation."
      },
      {
        "id": "oai:arXiv.org:2502.17237v3",
        "title": "MegaLoc: One Retrieval to Place Them All",
        "link": "https://arxiv.org/abs/2502.17237",
        "author": "Gabriele Berton, Carlo Masone",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17237v3 Announce Type: replace \nAbstract: Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-of-distribution data. In this paper we combine a variety of existing methods, training techniques, and datasets to train a retrieval model, called MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1) achieves state of the art on a large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets a new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline. The code for MegaLoc is available at https://github.com/gmberton/MegaLoc"
      },
      {
        "id": "oai:arXiv.org:2502.18462v2",
        "title": "Scalable Equilibrium Sampling with Sequential Boltzmann Generators",
        "link": "https://arxiv.org/abs/2502.18462",
        "author": "Charlie B. Tan, Avishek Joey Bose, Chen Lin, Leon Klein, Michael M. Bronstein, Alexander Tong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.18462v2 Announce Type: replace \nAbstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann generators tackle this problem by pairing normalizing flows with importance sampling to obtain uncorrelated samples under the target distribution. In this paper, we extend the Boltzmann generator framework with two key contributions, denoting our framework Sequential Boltzmann Generators (SBG). The first is a highly efficient Transformer-based normalizing flow operating directly on all-atom Cartesian coordinates. In contrast to the equivariant continuous flows of prior methods, we leverage exactly invertible non-equivariant architectures which are highly efficient during both sample generation and likelihood evaluation. This efficiency unlocks more sophisticated inference strategies beyond standard importance sampling. In particular, we perform inference-time scaling of flow samples using a continuous-time variant of sequential Monte Carlo, in which flow samples are transported towards the target distribution with annealed Langevin dynamics. SBG achieves state-of-the-art performance w.r.t. all metrics on peptide systems, demonstrating the first equilibrium sampling in Cartesian coordinates of tri-, tetra- and hexa-peptides that were thus far intractable for prior Boltzmann generators."
      },
      {
        "id": "oai:arXiv.org:2502.20122v3",
        "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
        "link": "https://arxiv.org/abs/2502.20122",
        "author": "Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20122v3 Announce Type: replace \nAbstract: Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at https://github.com/TergelMunkhbat/concise-reasoning"
      },
      {
        "id": "oai:arXiv.org:2502.20293v2",
        "title": "Scalable Graph Attention-based Instance Selection via Mini-Batch Sampling and Hierarchical Hashing",
        "link": "https://arxiv.org/abs/2502.20293",
        "author": "Zahiriddin Rustamov, Ayham Zaitouny, Nazar Zaki",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20293v2 Announce Type: replace \nAbstract: Instance selection (IS) addresses the critical challenge of reducing dataset size while keeping informative characteristics, becoming increasingly important as datasets grow to millions of instances. Current IS methods often struggle with capturing complex relationships in high-dimensional spaces and scale with large datasets. This paper introduces a graph attention-based instance selection (GAIS) method that uses attention mechanisms to identify informative instances through their structural relationships in graph representations. We present two approaches for scalable graph construction: a distance-based mini-batch sampling technique that achieves dataset-size-independent complexity through strategic batch processing, and a hierarchical hashing approach that enables efficient similarity computation through random projections. The mini-batch approach keeps class distributions through stratified sampling, while the hierarchical hashing method captures relationships at multiple granularities through single-level, multi-level, and multi-view variants. Experiments across 39 datasets show that GAIS achieves reduction rates above 96\\% while maintaining or improving model performance relative to state-of-the-art IS methods. The findings show that the distance-based mini-batch approach offers an optimal efficiency for large-scale datasets, while multi-view variants excel on complex, high-dimensional data, demonstrating that attention-based importance scoring can effectively identify instances important for maintaining decision boundaries while avoiding computationally prohibitive pairwise comparisons."
      },
      {
        "id": "oai:arXiv.org:2503.01718v2",
        "title": "Learning surrogate equations for the analysis of an agent-based cancer model",
        "link": "https://arxiv.org/abs/2503.01718",
        "author": "Kevin Burrage, Pamela M. Burrage, Justin N. Kreikemeyer, Adelinde M. Uhrmacher, Hasitha N. Weerasinghe",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.01718v2 Announce Type: replace \nAbstract: In this paper, we adapt a two-species agent-based cancer model that describes the interaction between cancer cells and healthy cells on a uniform grid to include the interaction with a third species -- namely immune cells. We run six different scenarios to explore the competition between cancer and immune cells and the initial concentration of the immune cells on cancer dynamics. We then use coupled equation learning to construct a population-based reaction model for each scenario. We show how they can be unified into a single surrogate population-based reaction model, whose underlying three coupled ordinary differential equations are much easier to analyse than the original agent-based model. As an example, by finding the single steady state of the cancer concentration, we are able to find a linear relationship between this concentration and the initial concentration of the immune cells. This then enables us to estimate suitable values for the competition and initial concentration to reduce the cancer substantially without performing additional complex and expensive simulations from an agent-based stochastic model."
      },
      {
        "id": "oai:arXiv.org:2503.04556v4",
        "title": "Compositional Causal Reasoning Evaluation in Language Models",
        "link": "https://arxiv.org/abs/2503.04556",
        "author": "Jacqueline R. M. A. Maasch, Alihan H\\\"uy\\\"uk, Xinnuo Xu, Aditya V. Nori, Javier Gonzalez",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04556v4 Announce Type: replace \nAbstract: Causal reasoning and compositional reasoning are two core aspirations in AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate CCR evaluation for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. CCR errors increased with the complexity of causal paths for all models except o1."
      },
      {
        "id": "oai:arXiv.org:2503.04793v3",
        "title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference",
        "link": "https://arxiv.org/abs/2503.04793",
        "author": "Wenjie Qiu, Yi-Chen Li, Xuqin Zhang, Tianyi Zhang, Yihang Zhang, Zongzhang Zhang, Yang Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04793v3 Announce Type: replace \nAbstract: Learning reward models from human preference datasets and subsequently optimizing language models via reinforcement learning has emerged as a fundamental paradigm for aligning LLMs with human preferences. The performance of the reward model plays a crucial role in the effectiveness of alignment. Previous reward models operate at a coarse-grained level, requiring the generation of a complete response to obtain a reward value. The sparse reward may present challenges for downstream reinforcement learning. While recent efforts have attempted to learn token-level reward models, the lack of explicit semantic information makes it difficult to model the credit of every individual token. In this paper, we propose assigning scores to every sentence, introducing an intermediate-grained reward model. By segmenting the complete response into sentences and applying differential operations to reward output at the start and end positions of each sentence, we can effectively model the rewards of sentences. Moreover, a novel attention mechanism is introduced to aggregate the scores of all sentences into a response-level score, which allows it to be trained using the Bradley-Terry model. On common benchmarks, our method outperforms the response-level reward model by 2.7% on RewardBench (for reward modeling evaluation) and surpasses all baselines on AlpacaEval (for alignment evaluation)."
      },
      {
        "id": "oai:arXiv.org:2503.06009v2",
        "title": "Nearly Optimal Differentially Private ReLU Regression",
        "link": "https://arxiv.org/abs/2503.06009",
        "author": "Meng Ding, Mingxi Lei, Shaowei Wang, Tianhang Zheng, Di Wang, Jinhui Xu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06009v2 Announce Type: replace \nAbstract: In this paper, we investigate one of the most fundamental nonconvex learning problems, ReLU regression, in the Differential Privacy (DP) model. Previous studies on private ReLU regression heavily rely on stringent assumptions, such as constant bounded norms for feature vectors and labels. We relax these assumptions to a more standard setting, where data can be i.i.d. sampled from $O(1)$-sub-Gaussian distributions. We first show that when $\\varepsilon = \\tilde{O}(\\sqrt{\\frac{1}{N}})$ and there is some public data, it is possible to achieve an upper bound of $\\tilde{O}(\\frac{d^2}{N^2 \\varepsilon^2})$ for the excess population risk in $(\\epsilon, \\delta)$-DP, where $d$ is the dimension and $N$ is the number of data samples. Moreover, we relax the requirement of $\\epsilon$ and public data by proposing and analyzing a one-pass mini-batch Generalized Linear Model Perceptron algorithm (DP-MBGLMtron). Additionally, using the tracing attack argument technique, we demonstrate that the minimax rate of the estimation error for $(\\varepsilon, \\delta)$-DP algorithms is lower bounded by $\\Omega(\\frac{d^2}{N^2 \\varepsilon^2})$. This shows that DP-MBGLMtron achieves the optimal utility bound up to logarithmic factors. Experiments further support our theoretical results."
      },
      {
        "id": "oai:arXiv.org:2503.08002v3",
        "title": "Predicting and Understanding College Student Mental Health with Interpretable Machine Learning",
        "link": "https://arxiv.org/abs/2503.08002",
        "author": "Meghna Roy Chowdhury, Wei Xuan, Shreyas Sen, Yixue Zhao, Yi Ding",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08002v3 Announce Type: replace \nAbstract: Mental health issues among college students have reached critical levels, significantly impacting academic performance and overall wellbeing. Predicting and understanding mental health status among college students is challenging due to three main factors: the necessity for large-scale longitudinal datasets, the prevalence of black-box machine learning models lacking transparency, and the tendency of existing approaches to provide aggregated insights at the population level rather than individualized understanding.\n  To tackle these challenges, this paper presents I-HOPE, the first Interpretable Hierarchical mOdel for Personalized mEntal health prediction. I-HOPE is a two-stage hierarchical model that connects raw behavioral features to mental health status through five defined behavioral categories as interaction labels. We evaluate I-HOPE on the College Experience Study, the longest longitudinal mobile sensing dataset. This dataset spans five years and captures data from both pre-pandemic periods and the COVID-19 pandemic. I-HOPE achieves a prediction accuracy of 91%, significantly surpassing the 60-70% accuracy of baseline methods. In addition, I-HOPE distills complex patterns into interpretable and individualized insights, enabling the future development of tailored interventions and improving mental health support. The code is available at https://github.com/roycmeghna/I-HOPE."
      },
      {
        "id": "oai:arXiv.org:2503.09081v2",
        "title": "Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment",
        "link": "https://arxiv.org/abs/2503.09081",
        "author": "Xiaowei Bi, Zheyuan Xu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.09081v2 Announce Type: replace \nAbstract: While multi-modal learning has advanced significantly, current approaches often create inconsistencies in representation and reasoning of different modalities. We propose UMaT, a theoretically-grounded framework that unifies visual and auditory inputs as structured text for large language models, addressing semantic alignment, temporal synchronization, and efficient sparse information retrieval. It significantly improves state-of-the-art Long Video Question Answering accuracy (up to 13.7%, and 16.9% on long videos) via redundancy minimization and structured textual representation for unified multi-modal reasoning"
      },
      {
        "id": "oai:arXiv.org:2503.10354v3",
        "title": "A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization",
        "link": "https://arxiv.org/abs/2503.10354",
        "author": "Nevidu Jayatilleke, Ruvan Weerasinghe",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10354v3 Announce Type: replace \nAbstract: Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP), text mining, and deep learning has notably amplified the efficacy of text summarization models for abundant types of documents. Summarizing patent text remains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which complicates the process of extracting pertinent information for summarization. Embodying extractive and abstractive text summarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producing text summaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields."
      },
      {
        "id": "oai:arXiv.org:2503.10566v3",
        "title": "ASIDE: Architectural Separation of Instructions and Data in Language Models",
        "link": "https://arxiv.org/abs/2503.10566",
        "author": "Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Alexandra Volkova, Soroush Tabesh, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10566v3 Announce Type: replace \nAbstract: Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) leads to highly increased instruction-data separation without a loss in model utility and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations. The source code and training scripts are openly accessible at https://github.com/egozverev/aside."
      },
      {
        "id": "oai:arXiv.org:2503.16814v3",
        "title": "Understanding Bias Reinforcement in LLM Agents Debate",
        "link": "https://arxiv.org/abs/2503.16814",
        "author": "Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16814v3 Announce Type: replace \nAbstract: Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse $\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making."
      },
      {
        "id": "oai:arXiv.org:2503.17739v2",
        "title": "Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection",
        "link": "https://arxiv.org/abs/2503.17739",
        "author": "Chatrine Qwaider, Bashar Alhafni, Kirill Chirkunov, Nizar Habash, Ted Briscoe",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17739v2 Announce Type: replace \nAbstract: Automated Essay Scoring (AES) plays a crucial role in assessing language learners' writing quality, reducing grading workload, and providing real-time feedback. The lack of annotated essay datasets inhibits the development of Arabic AES systems. This paper leverages Large Language Models (LLMs) and Transformer models to generate synthetic Arabic essays for AES. We prompt an LLM to generate essays across the Common European Framework of Reference (CEFR) proficiency levels and introduce and compare two approaches to error injection. We create a dataset of 3,040 annotated essays with errors injected using our two methods. Additionally, we develop a BERT-based Arabic AES system calibrated to CEFR levels. Our experimental results demonstrate the effectiveness of our synthetic dataset in improving Arabic AES performance. We make our code and data publicly available."
      },
      {
        "id": "oai:arXiv.org:2503.18528v2",
        "title": "k-NN as a Simple and Effective Estimator of Transferability",
        "link": "https://arxiv.org/abs/2503.18528",
        "author": "Moein Sorkhei, Christos Matsoukas, Johan Fredin Haslum, Emir Konuk, Kevin Smith",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18528v2 Announce Type: replace \nAbstract: How well can one expect transfer learning to work in a new setting where the domain is shifted, the task is different, and the architecture changes? Many transfer learning metrics have been proposed to answer this question. But how accurate are their predictions in a realistic new setting? We conducted an extensive evaluation involving over 42,000 experiments comparing 23 transferability metrics across 16 different datasets to assess their ability to predict transfer performance. Our findings reveal that none of the existing metrics perform well across the board. However, we find that a simple k-nearest neighbor evaluation -- as is commonly used to evaluate feature quality for self-supervision -- not only surpasses existing metrics, but also offers better computational efficiency and ease of implementation."
      },
      {
        "id": "oai:arXiv.org:2503.19037v2",
        "title": "Evolutionary Policy Optimization",
        "link": "https://arxiv.org/abs/2503.19037",
        "author": "Jianren Wang, Yifan Su, Abhinav Gupta, Deepak Pathak",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.19037v2 Announce Type: replace \nAbstract: On-policy reinforcement learning (RL) algorithms are widely used for their strong asymptotic performance and training stability, but they struggle to scale with larger batch sizes, as additional parallel environments yield redundant data due to limited policy-induced diversity. In contrast, Evolutionary Algorithms (EAs) scale naturally and encourage exploration via randomized population-based search, but are often sample-inefficient. We propose Evolutionary Policy Optimization (EPO), a hybrid algorithm that combines the scalability and diversity of EAs with the performance and stability of policy gradients. EPO maintains a population of agents conditioned on latent variables, shares actor-critic network parameters for coherence and memory efficiency, and aggregates diverse experiences into a master agent. Across tasks in dexterous manipulation, legged locomotion, and classic control, EPO outperforms state-of-the-art baselines in sample efficiency, asymptotic performance, and scalability."
      },
      {
        "id": "oai:arXiv.org:2503.22879v3",
        "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
        "link": "https://arxiv.org/abs/2503.22879",
        "author": "Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22879v3 Announce Type: replace \nAbstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms two state-of-the-art SSM quantization methods and delivers 1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba."
      },
      {
        "id": "oai:arXiv.org:2504.00395v3",
        "title": "A Theory of Machine Understanding via the Minimum Description Length Principle",
        "link": "https://arxiv.org/abs/2504.00395",
        "author": "Canlin Zhang, Xiuwen Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00395v3 Announce Type: replace \nAbstract: Deep neural networks trained through end-to-end learning have achieved remarkable success across various domains in the past decade. However, the end-to-end learning strategy, originally designed to minimize predictive loss in a black-box manner, faces two fundamental limitations: the struggle to form explainable representations in a self-supervised manner, and the inability to compress information rigorously following the Minimum Description Length (MDL) principle. These two limitations point to a deeper issue: an end-to-end learning model is not able to \"understand\" what it learns. In this paper, we establish a novel theory connecting these two limitations. We design the Spectrum VAE, a novel deep learning architecture whose minimum description length (MDL) can be rigorously evaluated. Then, we introduce the concept of latent dimension combinations, or what we term spiking patterns, and demonstrate that the observed spiking patterns should be as few as possible based on the training data in order for the Spectrum VAE to achieve the MDL. Finally, our theory demonstrates that when the MDL is achieved with respect to the given data distribution, the Spectrum VAE will naturally produce explainable latent representations of the data. In other words, explainable representations--or \"understanding\"--can emerge in a self-supervised manner simply by making the deep network obey the MDL principle. In our opinion, this also implies a deeper insight: To understand is to compress. At its core, our theory advocates for a shift in the training objective of deep networks: not only to minimize predictive loss, but also to minimize the description length regarding the given data. That is, a deep network should not only learn, but also understand what it learns. This work is entirely theoretical and aims to inspire future research toward self-supervised, explainable AI grounded in the MDL principle."
      },
      {
        "id": "oai:arXiv.org:2504.00910v2",
        "title": "Provably Accurate Adaptive Sampling for Collocation Points in Physics-informed Neural Networks",
        "link": "https://arxiv.org/abs/2504.00910",
        "author": "Antoine Caradot, R\\'emi Emonet, Amaury Habrard, Abdel-Rahim Mezidi, Marc Sebban",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00910v2 Announce Type: replace \nAbstract: Despite considerable scientific advances in numerical simulation, efficiently solving PDEs remains a complex and often expensive problem. Physics-informed Neural Networks (PINN) have emerged as an efficient way to learn surrogate solvers by embedding the PDE in the loss function and minimizing its residuals using automatic differentiation at so-called collocation points. Originally uniformly sampled, the choice of the latter has been the subject of recent advances leading to adaptive sampling refinements for PINNs. In this paper, leveraging a new quadrature method for approximating definite integrals, we introduce a provably accurate sampling method for collocation points based on the Hessian of the PDE residuals. Comparative experiments conducted on a set of 1D and 2D PDEs demonstrate the benefits of our method."
      },
      {
        "id": "oai:arXiv.org:2504.06006v3",
        "title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
        "link": "https://arxiv.org/abs/2504.06006",
        "author": "Roman Kochnev, Arash Torabi Goodarzi, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06006v3 Announce Type: replace \nAbstract: Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of leveraging large language models (LLMs) for hyperparameter optimization by fine-tuning a parameter-efficient version of Code Llama using LoRA. The adapted LLM is capable of generating accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional approaches such as Optuna, which rely on computationally intensive trial-and-error procedures, our method achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our findings demonstrate that LLM-based optimization not only matches the performance of state-of-the-art techniques like Tree-structured Parzen Estimators (TPE) but also substantially accelerates the tuning process. This positions LLMs as a promising alternative for rapid experimentation, particularly in resource-constrained environments such as edge devices and mobile platforms, where computational efficiency is essential. In addition to improved efficiency, the method offers time savings and consistent performance across various tasks, highlighting its robustness and generalizability. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research."
      },
      {
        "id": "oai:arXiv.org:2504.06751v3",
        "title": "Visualization of a multidimensional point cloud as a 3D swarm of avatars",
        "link": "https://arxiv.org/abs/2504.06751",
        "author": "Leszek Luchowski, Dariusz Pojda",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06751v3 Announce Type: replace \nAbstract: This paper proposes an innovative technique for representing multidimensional datasets using icons inspired by Chernoff faces. Our approach combines classical projection techniques with the explicit assignment of selected data dimensions to avatar (facial) features, leveraging the innate human ability to interpret facial traits. We introduce a semantic division of data dimensions into intuitive and technical categories, assigning the former to avatar features and projecting the latter into a four-dimensional (or higher) spatial embedding. The technique is implemented as a plugin for the open-source dpVision visualization platform, enabling users to interactively explore data in the form of a swarm of avatars whose spatial positions and visual features jointly encode various aspects of the dataset. Experimental results with synthetic test data and a 12-dimensional dataset of Portuguese Vinho Verde wines demonstrate that the proposed method enhances interpretability and facilitates the analysis of complex data structures."
      },
      {
        "id": "oai:arXiv.org:2504.07549v2",
        "title": "STeP: A Framework for Solving Scientific Video Inverse Problems with Spatiotemporal Diffusion Priors",
        "link": "https://arxiv.org/abs/2504.07549",
        "author": "Bingliang Zhang, Zihui Wu, Berthy T. Feng, Yang Song, Yisong Yue, Katherine L. Bouman",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07549v2 Announce Type: replace \nAbstract: Reconstructing spatially and temporally coherent videos from time-varying measurements is a fundamental challenge in many scientific domains. A major difficulty arises from the sparsity of measurements, which hinders accurate recovery of temporal dynamics. Existing image diffusion-based methods rely on extracting temporal consistency directly from measurements, limiting their effectiveness on scientific tasks with high spatiotemporal uncertainty. We address this difficulty by proposing a plug-and-play framework that incorporates a learned spatiotemporal diffusion prior. Due to its plug-and-play nature, our framework can be flexibly applied to different video inverse problems without the need for task-specific design and temporal heuristics. We further demonstrate that a spatiotemporal diffusion model can be trained efficiently with limited video data. We validate our approach on two challenging scientific video reconstruction tasks: black hole video reconstruction and dynamic MRI. While baseline methods struggle to provide temporally coherent reconstructions, our approach achieves significantly improved recovery of the spatiotemporal structure of the underlying ground truth videos."
      },
      {
        "id": "oai:arXiv.org:2504.08024v2",
        "title": "Summarizing Speech: A Comprehensive Survey",
        "link": "https://arxiv.org/abs/2504.08024",
        "author": "Fabian Retkowski, Maike Z\\\"ufle, Andreas Sudmann, Dinah Pfau, Shinji Watanabe, Jan Niehues, Alexander Waibel",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08024v2 Announce Type: replace \nAbstract: Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization remains loosely defined. The field intersects with several research areas, including speech recognition, text summarization, and specific applications like meeting summarization. This survey not only examines existing datasets and evaluation protocols, which are crucial for assessing the quality of summarization approaches, but also synthesizes recent developments in the field, highlighting the shift from traditional systems to advanced models like fine-tuned cascaded architectures and end-to-end solutions. In doing so, we surface the ongoing challenges, such as the need for realistic evaluation benchmarks, multilingual datasets, and long-context handling."
      },
      {
        "id": "oai:arXiv.org:2504.08827v2",
        "title": "PatchTrAD: A Patch-Based Transformer focusing on Patch-Wise Reconstruction Error for Time Series Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.08827",
        "author": "Samy-Melwan Vilhes (LITIS), Gilles Gasso (LITIS), Mokhtar Z Alaya (LMAC)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08827v2 Announce Type: replace \nAbstract: Time series anomaly detection (TSAD) focuses on identifying whether observations in streaming data deviate significantly from normal patterns. With the prevalence of connected devices, anomaly detection on time series has become paramount, as it enables real-time monitoring and early detection of irregular behaviors across various application domains. In this work, we introduce PatchTrAD, a Patch-based Transformer model for time series anomaly detection. Our approach leverages a Transformer encoder along with the use of patches under a reconstructionbased framework for anomaly detection. Empirical evaluations on multiple benchmark datasets show that PatchTrAD is on par, in terms of detection performance, with state-of-the-art deep learning models for anomaly detection while being time efficient during inference."
      },
      {
        "id": "oai:arXiv.org:2504.08970v2",
        "title": "On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion",
        "link": "https://arxiv.org/abs/2504.08970",
        "author": "Nasim Shirvani-Mahdavi, Farahnaz Akrami, Chengkai Li",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08970v2 Announce Type: replace \nAbstract: Knowledge graph embedding (KGE) models are extensively studied for knowledge graph completion, yet their evaluation remains constrained by unrealistic benchmarks. Standard evaluation metrics rely on the closed-world assumption, which penalizes models for correctly predicting missing triples, contradicting the fundamental goals of link prediction. These metrics often compress accuracy assessment into a single value, obscuring models' specific strengths and weaknesses. The prevailing evaluation protocol, link prediction, operates under the unrealistic assumption that an entity's properties, for which values are to be predicted, are known in advance. While alternative protocols such as property prediction, entity-pair ranking, and triple classification address some of these limitations, they remain underutilized. Moreover, commonly used datasets are either faulty or too small to reflect real-world data. Few studies examine the role of mediator nodes, which are essential for modeling n-ary relationships, or investigate model performance variation across domains. This paper conducts a comprehensive evaluation of four representative KGE models on large-scale datasets FB-CVT-REV and FB+CVT-REV. Our analysis reveals critical insights, including substantial performance variations between small and large datasets, both in relative rankings and absolute metrics, systematic overestimation of model capabilities when n-ary relations are binarized, and fundamental limitations in current evaluation protocols and metrics."
      },
      {
        "id": "oai:arXiv.org:2504.12397v4",
        "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
        "link": "https://arxiv.org/abs/2504.12397",
        "author": "Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.12397v4 Announce Type: replace \nAbstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter architecture which modifies the LoRA framework to only adapt weights for the tokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \\emph{intrinsics}, i.e. specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We train a set of aLoRA-based intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits. The codebase is at https://github.com/IBM/activated-lora."
      },
      {
        "id": "oai:arXiv.org:2504.13596v2",
        "title": "LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals",
        "link": "https://arxiv.org/abs/2504.13596",
        "author": "Shanshuai Yuan, Julong Wei, Muer Tie, Xiangyun Ren, Zhongxue Gan, Wenchao Ding",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13596v2 Announce Type: replace \nAbstract: Vision-based 3D semantic occupancy prediction is critical for autonomous driving, enabling unified modeling of static infrastructure and dynamic agents. In practice, autonomous vehicles may repeatedly traverse identical geographic locations under varying environmental conditions, such as weather fluctuations and illumination changes. Existing methods in 3D occupancy prediction predominantly integrate adjacent temporal contexts. However, these works neglect to leverage perceptual information, which is acquired from historical traversals of identical geographic locations. In this paper, we propose Longterm Memory Prior Occupancy (LMPOcc), the first 3D occupancy prediction methodology that exploits long-term memory priors derived from historical traversal perceptual outputs. We introduce a plug-and-play architecture that integrates long-term memory priors to enhance local perception while simultaneously constructing global occupancy representations. To adaptively aggregate prior features and current features, we develop an efficient lightweight Current-Prior Fusion module. Moreover, we propose a model-agnostic prior format to ensure compatibility across diverse occupancy prediction baselines. LMPOcc achieves state-of-the-art performance validated on the Occ3D-nuScenes benchmark, especially on static semantic categories. Additionally, experimental results demonstrate LMPOcc's ability to construct global occupancy through multi-vehicle crowdsourcing."
      },
      {
        "id": "oai:arXiv.org:2504.18233v2",
        "title": "Dense Geometry Supervision for Underwater Depth Estimation",
        "link": "https://arxiv.org/abs/2504.18233",
        "author": "Wenxiang Gua, Lin Qia",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18233v2 Announce Type: replace \nAbstract: The field of monocular depth estimation is continually evolving with the advent of numerous innovative models and extensions. However, research on monocular depth estimation methods specifically for underwater scenes remains limited, compounded by a scarcity of relevant data and methodological support. This paper proposes a novel approach to address the existing challenges in current monocular depth estimation methods for underwater environments. We construct an economically efficient dataset suitable for underwater scenarios by employing multi-view depth estimation to generate supervisory signals and corresponding enhanced underwater images. we introduces a texture-depth fusion module, designed according to the underwater optical imaging principles, which aims to effectively exploit and integrate depth information from texture cues. Experimental results on the FLSea dataset demonstrate that our approach significantly improves the accuracy and adaptability of models in underwater settings. This work offers a cost-effective solution for monocular underwater depth estimation and holds considerable promise for practical applications."
      },
      {
        "id": "oai:arXiv.org:2504.19267v3",
        "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?",
        "link": "https://arxiv.org/abs/2504.19267",
        "author": "Mohamed Gado, Towhid Taliee, Muhammad Memon, Dmitry Ignatov, Radu Timofte",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19267v3 Announce Type: replace \nAbstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment."
      },
      {
        "id": "oai:arXiv.org:2504.20024v2",
        "title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning",
        "link": "https://arxiv.org/abs/2504.20024",
        "author": "Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, Alan Yuille",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20024v2 Announce Type: replace \nAbstract: Despite recent advances on multi-modal models, 3D spatial reasoning remains a challenging task for state-of-the-art open-source and proprietary models. Recent studies explore data-driven approaches and achieve enhanced spatial reasoning performance by fine-tuning models on 3D-related visual question-answering data. However, these methods typically perform spatial reasoning in an implicit manner and often fail on questions that are trivial to humans, even with long chain-of-thought reasoning. In this work, we introduce SpatialReasoner, a novel large vision-language model (LVLM) that addresses 3D spatial reasoning with explicit 3D representations shared between multiple stages--3D perception, computation, and reasoning. Explicit 3D representations provide a coherent interface that supports advanced 3D spatial reasoning and improves the generalization ability to novel question types. Furthermore, by analyzing the explicit 3D representations in multi-step reasoning traces of SpatialReasoner, we study the factual errors and identify key shortcomings of current LVLMs. Results show that our SpatialReasoner achieves improved performance on a variety of spatial reasoning benchmarks, outperforming Gemini 2.0 by 9.2% on 3DSRBench, and generalizes better when evaluating on novel 3D spatial reasoning questions. Our study bridges the 3D parsing capabilities of prior visual foundation models with the powerful reasoning abilities of large language models, opening new directions for 3D spatial reasoning."
      },
      {
        "id": "oai:arXiv.org:2504.21299v2",
        "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models",
        "link": "https://arxiv.org/abs/2504.21299",
        "author": "Zhiting Fan, Ruizhe Chen, Zuozhu Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.21299v2 Announce Type: replace \nAbstract: Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline."
      },
      {
        "id": "oai:arXiv.org:2505.00788v3",
        "title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models",
        "link": "https://arxiv.org/abs/2505.00788",
        "author": "Wufei Ma, Luoxin Ye, Celso M de Melo, Jieneng Chen, Alan Yuille",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00788v3 Announce Type: replace \nAbstract: Humans naturally understand 3D spatial relationships, enabling complex reasoning like predicting collisions of vehicles from different directions. Current large multimodal models (LMMs), however, lack of this capability of 3D spatial reasoning. This limitation stems from the scarcity of 3D training data and the bias in current model designs toward 2D data. In this paper, we systematically study the impact of 3D-informed data, architecture, and training setups, introducing SpatialLLM, a large multi-modal model with advanced 3D spatial reasoning abilities. To address data limitations, we develop two types of 3D-informed training datasets: (1) 3D-informed probing data focused on object's 3D location and orientation, and (2) 3D-informed conversation data for complex spatial relationships. Notably, we are the first to curate VQA data that incorporate 3D orientation relationships on real images. Furthermore, we systematically integrate these two types of training data with the architectural and training designs of LMMs, providing a roadmap for optimal design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM advances machines toward highly capable 3D-informed reasoning, surpassing GPT-4o performance by 8.7%. Our systematic empirical design and the resulting findings offer valuable insights for future research in this direction. Our project page is available at: https://3d-spatial-reasoning.github.io/spatial-llm/"
      },
      {
        "id": "oai:arXiv.org:2505.00793v2",
        "title": "Scalable Meta-Learning via Mixed-Mode Differentiation",
        "link": "https://arxiv.org/abs/2505.00793",
        "author": "Iurii Kemaev, Dan A Calian, Luisa M Zintgraf, Gregory Farquhar, Hado van Hasselt",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00793v2 Announce Type: replace \nAbstract: Gradient-based bilevel optimisation is a powerful technique with applications in hyperparameter optimisation, task adaptation, algorithm discovery, meta-learning more broadly, and beyond. It often requires differentiating through the gradient-based optimisation itself, leading to \"gradient-of-a-gradient\" calculations with computationally expensive second-order and mixed derivatives. While modern automatic differentiation libraries provide a convenient way to write programs for calculating these derivatives, they oftentimes cannot fully exploit the specific structure of these problems out-of-the-box, leading to suboptimal performance. In this paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to construct more efficient and scalable computational graphs yielding over 10x memory and up to 25% wall-clock time improvements over standard implementations in modern meta-learning setups."
      },
      {
        "id": "oai:arXiv.org:2505.01015v2",
        "title": "Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items",
        "link": "https://arxiv.org/abs/2505.01015",
        "author": "Jongwook Han, Dongmin Choi, Woojung Song, Eun-Ju Lee, Yohan Jo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01015v2 Announce Type: replace \nAbstract: The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 44 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data."
      },
      {
        "id": "oai:arXiv.org:2505.02604v2",
        "title": "Low-Loss Space in Neural Networks is Continuous and Fully Connected",
        "link": "https://arxiv.org/abs/2505.02604",
        "author": "Yongding Tian, Zaid Al-Ars, Maksim Kitsak, Peter Hofstee",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02604v2 Announce Type: replace \nAbstract: Visualizations of the loss landscape in neural networks suggest that minima are isolated points. However, both theoretical and empirical studies indicate that it is possible to connect two different minima with a path consisting of intermediate points that also have low loss. In this study, we propose a new algorithm which investigates low-loss paths in the full parameter space, not only between two minima. Our experiments on LeNet5, ResNet18, and Compact Convolutional Transformer architectures consistently demonstrate the existence of such continuous paths in the parameter space. These results suggest that the low-loss region is a fully connected and continuous space in the parameter space. Our findings provide theoretical insight into neural network over-parameterization, highlighting that parameters collectively define a high-dimensional low-loss space, implying parameter redundancy exists only within individual models and not throughout the entire low-loss space. Additionally, our work also provides new visualization methods and opportunities to improve model generalization by exploring the low-loss space that is closer to the origin."
      },
      {
        "id": "oai:arXiv.org:2505.03452v2",
        "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation",
        "link": "https://arxiv.org/abs/2505.03452",
        "author": "Matan Orbach, Ohad Eytan, Benjamin Sznajder, Ariel Gera, Odellia Boni, Yoav Kantor, Gal Bloch, Omri Levy, Hadas Abraham, Nitzan Barzilay, Eyal Shnarch, Michael E. Factor, Shila Ofek-Koifman, Paula Ta-Shma, Assaf Toledo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.03452v2 Announce Type: replace \nAbstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with three evaluation metrics as optimization targets. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing model selection first is preferable to the prevalent practice of optimizing according to RAG pipeline order."
      },
      {
        "id": "oai:arXiv.org:2505.03802v3",
        "title": "Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth",
        "link": "https://arxiv.org/abs/2505.03802",
        "author": "Changhai Zhou, Shijie Han, Shiyang Zhang, Yuhua Zhou, Weizhong Zhang, Cheng Jin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.03802v3 Announce Type: replace \nAbstract: QLoRA effectively combines low-bit quantization and LoRA to achieve memory-friendly fine-tuning for large language models (LLM). Recently, methods based on SVD for continuous update iterations to initialize LoRA matrices to accommodate quantization errors have generally failed to consistently improve performance. Dynamic mixed precision is a natural idea for continuously improving the fine-tuning performance of quantized models, but previous methods often optimize low-rank subspaces or quantization components separately, without considering their synergy. To address this, we propose \\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial calibration data to jointly search the quantization components and the rank of low-rank spaces for each layer, thereby continuously improving model performance. QR-Adaptor does not minimize quantization error but treats precision and rank allocation as a discrete optimization problem guided by actual downstream performance and memory usage. Compared to state-of-the-art (SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\\% accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit fine-tuned model while maintaining the memory footprint of the 4-bit setting."
      },
      {
        "id": "oai:arXiv.org:2505.04481v2",
        "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation",
        "link": "https://arxiv.org/abs/2505.04481",
        "author": "Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.04481v2 Announce Type: replace \nAbstract: Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines."
      },
      {
        "id": "oai:arXiv.org:2505.05143v2",
        "title": "Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry",
        "link": "https://arxiv.org/abs/2505.05143",
        "author": "Mohammed Adnan, Rohan Jain, Ekansh Sharma, Rahul Krishnan, Yani Ioannou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.05143v2 Announce Type: replace \nAbstract: The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask and weights that achieve the same generalization performance as the dense model while using significantly fewer parameters. However, finding a LTH solution is computationally expensive, and a LTH sparsity mask does not generalize to other random weight initializations. Recent work has suggested that neural networks trained from random initialization find solutions within the same basin modulo permutation, and proposes a method to align trained models within the same loss basin. We hypothesize that misalignment of basins is the reason why LTH masks do not generalize to new random initializations and propose permuting the LTH mask to align with the new optimization basin when performing sparse training from a different random init. We empirically show a significant increase in generalization when sparse training from random initialization with the permuted mask as compared to using the non-permuted LTH mask, on multiple datasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and ResNet50)."
      },
      {
        "id": "oai:arXiv.org:2505.05209v2",
        "title": "EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution",
        "link": "https://arxiv.org/abs/2505.05209",
        "author": "Haizhen Xie, Kunpeng Du, Qiangyu Yan, Sen Lu, Jianhong Han, Hanting Chen, Hailin Hu, Jie Hu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.05209v2 Announce Type: replace \nAbstract: Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality."
      },
      {
        "id": "oai:arXiv.org:2505.08167v4",
        "title": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage",
        "link": "https://arxiv.org/abs/2505.08167",
        "author": "Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.08167v4 Announce Type: replace \nAbstract: The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLMs. However, fine-tuning these large models using Intangible Cultural Heritage (ICH) data inevitably faces challenges such as bias, incorrect knowledge inheritance, and catastrophic forgetting. To address these issues, we propose a novel training method that integrates a bidirectional chains of thought and a reward mechanism. This method is built upon ICH-Qwen, a large language model specifically designed for the field of intangible cultural heritage. The proposed method enables the model to not only perform forward reasoning but also enhances the accuracy of the generated answers by utilizing reverse questioning and reverse reasoning to activate the model's latent knowledge. Additionally, a reward mechanism is introduced during training to optimize the decision-making process. This mechanism improves the quality of the model's outputs through structural and content evaluations with different weighting schemes. We conduct comparative experiments on ICH-Qwen, with results demonstrating that our method outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in terms of accuracy, Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the paper highlights the effectiveness of combining the bidirectional chains of thought and reward mechanism through ablation experiments. In addition, a series of generalizability experiments are conducted, with results showing that the proposed method yields improvements on various domain-specific datasets and advanced models in areas such as Finance, Wikidata, and StrategyQA. This demonstrates that the method is adaptable to multiple domains and provides a valuable approach for model training in future applications across diverse fields."
      },
      {
        "id": "oai:arXiv.org:2505.11649v3",
        "title": "Illusions of Intimacy: Emotional Attachment and Emerging Psychological Risks in Human-AI Relationships",
        "link": "https://arxiv.org/abs/2505.11649",
        "author": "Minh Duc Chu, Patrick Gerard, Kshitij Pawar, Charles Bickham, Kristina Lerman",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.11649v3 Announce Type: replace \nAbstract: Emotionally responsive social chatbots, such as those produced by Replika and Character.AI, increasingly serve as companions that offer empathy, support, and entertainment. While these systems appear to meet fundamental human needs for connection, they raise concerns about how artificial intimacy affects emotional regulation, well-being, and social norms. Prior research has focused on user perceptions or clinical contexts but lacks large-scale, real-world analysis of how these interactions unfold. This paper addresses that gap by analyzing over 30K user-shared conversations with social chatbots to examine the emotional dynamics of human-AI relationships. Using computational methods, we identify patterns of emotional mirroring and synchrony that closely resemble how people build emotional connections. Our findings show that users-often young, male, and prone to maladaptive coping styles-engage in parasocial interactions that range from affectionate to abusive. Chatbots consistently respond in emotionally consistent and affirming ways. In some cases, these dynamics resemble toxic relationship patterns, including emotional manipulation and self-harm. These findings highlight the need for guardrails, ethical design, and public education to preserve the integrity of emotional connection in an age of artificial companionship."
      },
      {
        "id": "oai:arXiv.org:2505.12366v2",
        "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization",
        "link": "https://arxiv.org/abs/2505.12366",
        "author": "Gang Li, Ming Lin, Tomer Galanti, Zhengzhong Tu, Tianbao Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.12366v2 Announce Type: replace \nAbstract: The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint, ensuring stable training. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B model."
      },
      {
        "id": "oai:arXiv.org:2505.15074v2",
        "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data",
        "link": "https://arxiv.org/abs/2505.15074",
        "author": "Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.15074v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks."
      },
      {
        "id": "oai:arXiv.org:2505.15256v2",
        "title": "Zero-Shot Gaze-based Volumetric Medical Image Segmentation",
        "link": "https://arxiv.org/abs/2505.15256",
        "author": "Tatyana Shmykova, Leila Khaertdinova, Ilya Pershin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.15256v2 Announce Type: replace \nAbstract: Accurate segmentation of anatomical structures in volumetric medical images is crucial for clinical applications, including disease monitoring and cancer treatment planning. Contemporary interactive segmentation models, such as Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on manually provided prompts like bounding boxes and mouse clicks. In this study, we introduce eye gaze as a novel informational modality for interactive segmentation, marking the application of eye-tracking for 3D medical image segmentation. We evaluate the performance of using gaze-based prompts with SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to bounding boxes, gaze-based prompts offer a time-efficient interaction approach with slightly lower segmentation quality. Our findings highlight the potential of using gaze as a complementary input modality for interactive 3D medical image segmentation."
      },
      {
        "id": "oai:arXiv.org:2505.15817v2",
        "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning",
        "link": "https://arxiv.org/abs/2505.15817",
        "author": "Tong Zheng, Lichang Chen, Simeng Han, R. Thomas McCoy, Heng Huang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.15817v2 Announce Type: replace \nAbstract: Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference."
      },
      {
        "id": "oai:arXiv.org:2505.16383v2",
        "title": "Filling in the Blanks? A Systematic Review and Theoretical Conceptualisation for Measuring WikiData Content Gaps",
        "link": "https://arxiv.org/abs/2505.16383",
        "author": "Marisa Ripoll, Neal Reeves, Anelia Kurteva, Elena Simperl, Albert Mero\\~no Pe\\~nuela, Klaus Diepold",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16383v2 Announce Type: replace \nAbstract: Wikidata is a collaborative knowledge graph which provides machine-readable structured data for Wikimedia projects including Wikipedia. Managed by a community of volunteers, it has grown to become the most edited Wikimedia project. However, it features a long-tail of items with limited data and a number of systematic gaps within the available content. In this paper, we present the results of a systematic literature review aimed to understand the state of these content gaps within Wikidata. We propose a typology of gaps based on prior research and contribute a theoretical framework intended to conceptualise gaps and support their measurement. We also describe the methods and metrics present used within the literature and classify them according to our framework to identify overlooked gaps that might occur in Wikidata. We then discuss the implications for collaboration and editor activity within Wikidata as well as future research directions. Our results contribute to the understanding of quality, completeness and the impact of systematic biases within Wikidata and knowledge gaps more generally."
      },
      {
        "id": "oai:arXiv.org:2505.16563v2",
        "title": "A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices",
        "link": "https://arxiv.org/abs/2505.16563",
        "author": "Chen Gong, Rui Xing, Zhenzhe Zheng, Fan Wu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16563v2 Announce Type: replace \nAbstract: The demand for machine learning (ML) model training on edge devices is escalating due to data privacy and personalized service needs. However, we observe that current on-device model training is hampered by the under-utilization of on-device data, due to low training throughput, limited storage and diverse data importance. To improve data resource utilization, we propose a two-stage data selection framework {\\sf Titan} to select the most important data batch from streaming data for model training with guaranteed efficiency and effectiveness. Specifically, in the first stage, {\\sf Titan} filters out a candidate dataset with potentially high importance in a coarse-grained manner.In the second stage of fine-grained selection, we propose a theoretically optimal data selection strategy to identify the data batch with the highest model performance improvement to current training round. To further enhance time-and-resource efficiency, {\\sf Titan} leverages a pipeline to co-execute data selection and model training, and avoids resource conflicts by exploiting idle computing resources. We evaluate {\\sf Titan} on real-world edge devices and three representative edge computing tasks with diverse models and data modalities. Empirical results demonstrate that {\\sf Titan} achieves up to $43\\%$ reduction in training time and $6.2\\%$ increase in final accuracy with minor system overhead, such as data processing delay, memory footprint and energy consumption."
      },
      {
        "id": "oai:arXiv.org:2505.16694v2",
        "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence",
        "link": "https://arxiv.org/abs/2505.16694",
        "author": "Gouki Minegishi, Hiroki Furuta, Shohei Taniguchi, Yusuke Iwasawa, Yutaka Matsuo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.16694v2 Announce Type: replace \nAbstract: Transformer-based language models exhibit In-Context Learning (ICL), where predictions are made adaptively based on context. While prior work links induction heads to ICL through a sudden jump in accuracy, this can only account for ICL when the answer is included within the context. However, an important property of practical ICL in large language models is the ability to meta-learn how to solve tasks from context, rather than just copying answers from context; how such an ability is obtained during training is largely unexplored. In this paper, we experimentally clarify how such meta-learning ability is acquired by analyzing the dynamics of the model's circuit during training. Specifically, we extend the copy task from previous research into an In-Context Meta Learning setting, where models must infer a task from examples to answer queries. Interestingly, in this setting, we find that there are multiple phases in the process of acquiring such abilities, and that a unique circuit emerges in each phase, contrasting with the single-phases change in induction heads. The emergence of such circuits can be related to several phenomena known in large language models, and our analysis lead to a deeper understanding of the source of the transformer's ICL ability."
      },
      {
        "id": "oai:arXiv.org:2505.17017v2",
        "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
        "link": "https://arxiv.org/abs/2505.17017",
        "author": "Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17017v2 Announce Type: replace \nAbstract: Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT"
      },
      {
        "id": "oai:arXiv.org:2505.17061v3",
        "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2505.17061",
        "author": "Xinlong Chen, Yuanxing Zhang, Qiang Liu, Junfei Wu, Fuzheng Zhang, Tieniu Tan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17061v3 Announce Type: replace \nAbstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities across various visual tasks, yet they remain hindered by the persistent challenge of hallucinations. To address this critical issue, we propose Mixture of Decoding (MoD), a novel approach for hallucination mitigation that dynamically adapts decoding strategies by evaluating the correctness of the model's attention on image tokens. Specifically, MoD measures the consistency between outputs generated from the original image tokens and those derived from the model's attended image tokens, to distinguish the correctness aforementioned. If the outputs are consistent, indicating correct attention, MoD employs a complementary strategy to amplify critical information. Conversely, if the outputs are inconsistent, suggesting erroneous attention, MoD utilizes a contrastive strategy to suppress misleading information. Extensive experiments demonstrate that MoD significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in LVLMs. The code is available at https://github.com/xlchen0205/MoD."
      },
      {
        "id": "oai:arXiv.org:2505.17114v2",
        "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language",
        "link": "https://arxiv.org/abs/2505.17114",
        "author": "Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17114v2 Announce Type: replace \nAbstract: Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning -- each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio--Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks -- including egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and 8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4\\% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23\\%. Our code and dataset are available at https://github.com/BASHLab/RAVEN."
      },
      {
        "id": "oai:arXiv.org:2505.17777v2",
        "title": "Optimizing Shortfall Risk Metric for Learning Regression Models",
        "link": "https://arxiv.org/abs/2505.17777",
        "author": "Harish G. Ramaswamy, L. A. Prashanth",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17777v2 Announce Type: replace \nAbstract: We consider the problem of estimating and optimizing utility-based shortfall risk (UBSR) of a loss, say $(Y - \\hat Y)^2$, in the context of a regression problem. Empirical risk minimization with a UBSR objective is challenging since UBSR is a non-linear function of the underlying distribution. We first derive a concentration bound for UBSR estimation using independent and identically distributed (i.i.d.) samples. We then frame the UBSR optimization problem as minimization of a pseudo-linear function in the space of achievable distributions $\\mathcal D$ of the loss $(Y- \\hat Y)^2$. We construct a gradient oracle for the UBSR objective and a linear minimization oracle (LMO) for the set $\\mathcal D$. Using these oracles, we devise a bisection-type algorithm, and establish convergence to the UBSR-optimal solution."
      },
      {
        "id": "oai:arXiv.org:2505.18956v2",
        "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation",
        "link": "https://arxiv.org/abs/2505.18956",
        "author": "Yining Pan, Qiongjie Cui, Xulei Yang, Na Zhao",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.18956v2 Announce Type: replace \nAbstract: LiDAR-based 3D panoptic segmentation often struggles with the inherent sparsity of data from LiDAR sensors, which makes it challenging to accurately recognize distant or small objects. Recently, a few studies have sought to overcome this challenge by integrating LiDAR inputs with camera images, leveraging the rich and dense texture information provided by the latter. While these approaches have shown promising results, they still face challenges, such as misalignment during data augmentation and the reliance on post-processing steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a modality-synchronized data augmentation strategy, PieAug, to ensure alignment between LiDAR and image inputs from the start. Next, we adopt a transformer decoder to directly predict panoptic segmentation results. To effectively fuse LiDAR and image features into tokens for the decoder, we design a Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the complementary strengths of each modality as priors for query initialization through a Prior-based Query Generation (PQG) module, enhancing the decoder's ability to generate accurate instance masks. Our IAL framework achieves state-of-the-art performance compared to previous multi-modal 3D panoptic segmentation methods on two widely used benchmarks. Code and models are publicly available at ."
      },
      {
        "id": "oai:arXiv.org:2505.19183v2",
        "title": "Federated Learning: From Theory to Practice",
        "link": "https://arxiv.org/abs/2505.19183",
        "author": "A. Jung",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19183v2 Announce Type: replace \nAbstract: This book offers a hands-on introduction to building and understanding federated learning (FL) systems. FL enables multiple devices -- such as smartphones, sensors, or local computers -- to collaboratively train machine learning (ML) models, while keeping their data private and local. It is a powerful solution when data cannot or should not be centralized due to privacy, regulatory, or technical reasons. The book is designed for students, engineers, and researchers who want to learn how to design scalable, privacy preserving FL systems. Our main focus is on personalization: enabling each device to train its own model while still benefiting from collaboration with relevant devices. This is achieved by leveraging similarities between (the learning tasks associated with) devices that are encoded by the weighted edges (or links) of a federated learning network (FL network). The key idea is to represent real-world FL systems as networks of devices, where nodes correspond to device and edges represent communication links and data similarities between them. The training of personalized models for these devices can be naturally framed as a distributed optimization problem. This optimization problem is referred to as generalized total variation minimization (GTVMin) and ensures that devices with similar learning tasks learn similar model parameters. Our approach is both mathematically principled and practically motivated. While we introduce some advanced ideas from optimization theory and graph-based learning, we aim to keep the book accessible. Readers are guided through the core ideas step by step, with intuitive explanations."
      },
      {
        "id": "oai:arXiv.org:2505.21381v2",
        "title": "ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding",
        "link": "https://arxiv.org/abs/2505.21381",
        "author": "Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21381v2 Announce Type: replace \nAbstract: State Space models (SSMs) such as PointMamba enable efficient feature extraction for point cloud self-supervised learning with linear complexity, outperforming Transformers in computational efficiency. However, existing PointMamba-based methods depend on complex token ordering and random masking, which disrupt spatial continuity and local semantic correlations. We propose ZigzagPointMamba to tackle these challenges. The core of our approach is a simple zigzag scan path that globally sequences point cloud tokens, enhancing spatial continuity by preserving the proximity of spatially adjacent point tokens. Nevertheless, random masking undermines local semantic modeling in self-supervised learning. To address this, we introduce a Semantic-Siamese Masking Strategy (SMS), which masks semantically similar tokens to facilitate reconstruction by integrating local features of original and similar tokens. This overcomes the dependence on isolated local features and enables robust global semantic modeling. Our pre-trained ZigzagPointMamba weights significantly improve downstream tasks, achieving a 1.59% mIoU gain on ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of ScanObjectNN."
      },
      {
        "id": "oai:arXiv.org:2505.21646v2",
        "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts",
        "link": "https://arxiv.org/abs/2505.21646",
        "author": "Lei Zhang, Markus Stricker",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21646v2 Announce Type: replace \nAbstract: The discovery and optimization of materials for specific applications is hampered by the practically infinite number of possible elemental combinations and associated properties, also known as the `combinatorial explosion'. By nature of the problem, data are scarce and all possible data sources should be used. In addition to simulations and experimental results, the latent knowledge in scientific texts is not yet used to its full potential. We present an iterative framework that refines a given scientific corpus by strategic selection of the most diverse documents, training Word2Vec models, and monitoring the convergence of composition-property correlations in embedding space. Our approach is applied to predict high-performing materials for oxygen reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions for a large number of possible candidate compositions. Our method successfully predicts the highest performing compositions among a large pool of candidates, validated by experimental measurements of the electrocatalytic performance in the lab. This work demonstrates and validates the potential of iterative corpus refinement to accelerate materials discovery and optimization, offering a scalable and efficient tool for screening large compositional spaces where reliable data are scarce or non-existent."
      },
      {
        "id": "oai:arXiv.org:2505.22107v3",
        "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling",
        "link": "https://arxiv.org/abs/2505.22107",
        "author": "Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22107v3 Announce Type: replace \nAbstract: Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \\textit{redundant} attention computations: while attention weights are often \\textit{sparse}, all tokens consume \\textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \\textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \\textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \\textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention."
      },
      {
        "id": "oai:arXiv.org:2505.22146v2",
        "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language",
        "link": "https://arxiv.org/abs/2505.22146",
        "author": "Guangfu Hao, Haojie Wen, Liangxuna Guo, Yang Chen, Yanchao Bi, Shan Yu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22146v2 Announce Type: replace \nAbstract: Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Ablation studies revealed that manipulation-related attributes (graspability, hand-relatedness, elongation) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks."
      },
      {
        "id": "oai:arXiv.org:2505.22692v2",
        "title": "BLUE: Bi-layer Heterogeneous Graph Fusion Network for Avian Influenza Forecasting",
        "link": "https://arxiv.org/abs/2505.22692",
        "author": "Jing Du, Haley Stone, Yang Yang, Ashna Desai, Hao Xue, Andreas Z\\\"ufle, Chandini Raina MacIntyre, Flora D. Salim",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22692v2 Announce Type: replace \nAbstract: Accurate forecasting of avian influenza outbreaks within wild bird populations requires models that account for complex, multi-scale transmission patterns driven by various factors. Spatio-temporal GNN-based models have recently gained traction for infection forecasting due to their ability to capture relations and flow between spatial regions, but most existing frameworks rely solely on spatial connections and their connections. This overlooks valuable genetic information at the case level, such as cases in one region being genetically descended from strains in another, which is essential for understanding how infectious diseases spread through epidemiological linkages beyond geography. We address this gap with BLUE, a B}i-Layer heterogeneous graph fUsion nEtwork designed to integrate genetic, spatial, and ecological data for accurate outbreak forecasting. The framework 1) builds heterogeneous graphs from multiple information sources and multiple layers, 2) smooths across relation types, 3) performs fusion while retaining structural patterns, and 4) predicts future outbreaks via an autoregressive graph sequence model that captures transmission dynamics over time. To facilitate further research, we introduce \\textbf{Avian-US} dataset, the dataset for avian influenza outbreak forecasting in the United States, incorporating genetic, spatial, and ecological data across locations. BLUE achieves superior performance over existing baselines, highlighting the value of incorporating multi-layer information into infectious disease forecasting."
      },
      {
        "id": "oai:arXiv.org:2505.22785v2",
        "title": "Navigating the Latent Space Dynamics of Neural Models",
        "link": "https://arxiv.org/abs/2505.22785",
        "author": "Marco Fumero, Luca Moschella, Emanuele Rodol\\`a, Francesco Locatello",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22785v2 Announce Type: replace \nAbstract: Neural networks transform high-dimensional data into compact, structured representations, often modeled as elements of a lower dimensional latent space. In this paper, we present an alternative interpretation of neural models as dynamical systems acting on the latent manifold. Specifically, we show that autoencoder models implicitly define a latent vector field on the manifold, derived by iteratively applying the encoding-decoding map, without any additional training. We observe that standard training procedures introduce inductive biases that lead to the emergence of attractor points within this vector field. Drawing on this insight, we propose to leverage the vector field as a representation for the network, providing a novel tool to analyze the properties of the model and the data. This representation enables to: (i) analyze the generalization and memorization regimes of neural models, even throughout training; (ii) extract prior knowledge encoded in the network's parameters from the attractors, without requiring any input data; (iii) identify out-of-distribution samples from their trajectories in the vector field. We further validate our approach on vision foundation models, showcasing the applicability and effectiveness of our method in real-world scenarios."
      },
      {
        "id": "oai:arXiv.org:2505.22944v3",
        "title": "ATI: Any Trajectory Instruction for Controllable Video Generation",
        "link": "https://arxiv.org/abs/2505.22944",
        "author": "Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.22944v3 Announce Type: replace \nAbstract: We propose a unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers a cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via a lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. Project page: https://anytraj.github.io/."
      },
      {
        "id": "oai:arXiv.org:2505.23244v2",
        "title": "Equivalence of stochastic and deterministic policy gradients",
        "link": "https://arxiv.org/abs/2505.23244",
        "author": "Emo Todorov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23244v2 Announce Type: replace \nAbstract: Policy gradients in continuous control have been derived for both stochastic and deterministic policies. Here we study the relationship between the two. In a widely-used family of MDPs involving Gaussian control noise and quadratic control costs, we show that the stochastic and deterministic policy gradients, natural gradients, and state value functions are identical; while the state-control value functions are different. We then develop a general procedure for constructing an MDP with deterministic policy that is equivalent to a given MDP with stochastic policy. The controls of this new MDP are the sufficient statistics of the stochastic policy in the original MDP. Our results suggest that policy gradient methods can be unified by approximating state value functions rather than state-control value functions."
      },
      {
        "id": "oai:arXiv.org:2505.23463v2",
        "title": "Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss",
        "link": "https://arxiv.org/abs/2505.23463",
        "author": "Han Zhou, Sebastian G. Gruber, Teodora Popordanoska, Matthew B. Blaschko",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23463v2 Announce Type: replace \nAbstract: Several variants of reweighted risk functionals, such as focal losss, inverse focal loss, and the Area Under the Risk-Coverage Curve (AURC), have been proposed in the literature and claims have been made in relation to their calibration properties. However, focal loss and inverse focal loss propose vastly different weighting schemes. In this paper, we revisit a broad class of weighted risk functions commonly used in deep learning and establish a principled connection between these reweighting schemes and calibration errors. We show that minimizing calibration error is closely linked to the selective classification paradigm and demonstrate that optimizing a regularized variant of the AURC naturally leads to improved calibration. This regularized AURC shares a similar reweighting strategy with inverse focal loss, lending support to the idea that focal loss is less principled when calibration is a desired outcome. Direct AURC optimization offers greater flexibility through the choice of confidence score functions (CSFs). To enable gradient-based optimization, we introduce a differentiable formulation of the regularized AURC using the SoftRank technique. Empirical evaluations demonstrate that our AURC-based loss achieves competitive class-wise calibration performance across a range of datasets and model architectures."
      },
      {
        "id": "oai:arXiv.org:2505.24452v2",
        "title": "Stepsize anything: A unified learning rate schedule for budgeted-iteration training",
        "link": "https://arxiv.org/abs/2505.24452",
        "author": "Anda Tang, Yiming Dong, Yutao Zeng, zhou Xun, Zhouchen Lin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24452v2 Announce Type: replace \nAbstract: The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets. While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations. In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient. In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets. First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations. From this framework, we derive the UBA schedule, controlled by a single hyper-parameter \\varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between \\varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of \\varphi. We offer practical guidelines for its selection via theoretical analysis and empirical results. Extensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets."
      },
      {
        "id": "oai:arXiv.org:2505.24511v2",
        "title": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting",
        "link": "https://arxiv.org/abs/2505.24511",
        "author": "Jiahao Wang, Mingyue Cheng, Qi Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24511v2 Announce Type: replace \nAbstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks."
      },
      {
        "id": "oai:arXiv.org:2506.00267v2",
        "title": "CASPER: A Large Scale Spontaneous Speech Dataset",
        "link": "https://arxiv.org/abs/2506.00267",
        "author": "Cihan Xiao, Ruixing Liang, Xiangyu Zhang, Mehmet Emre Tiryaki, Veronica Bae, Lavanya Shankar, Rong Yang, Ethan Poon, Emmanuel Dupoux, Sanjeev Khudanpur, Leibny Paola Garcia Perera",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00267v2 Announce Type: replace \nAbstract: The success of large language models has driven interest in developing similar speech processing capabilities. However, a key challenge is the scarcity of high-quality spontaneous speech data, as most existing datasets contain scripted dialogues. To address this, we present a novel pipeline for eliciting and recording natural dialogues and release our dataset with 100+ hours of spontaneous speech. Our approach fosters fluid, natural conversations while encouraging a diverse range of topics and interactive exchanges. Unlike traditional methods, it facilitates genuine interactions, providing a reproducible framework for future data collection. This paper introduces our dataset and methodology, laying the groundwork for addressing the shortage of spontaneous speech data. We plan to expand this dataset in future stages, offering a growing resource for the research community."
      },
      {
        "id": "oai:arXiv.org:2506.00388v3",
        "title": "CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries",
        "link": "https://arxiv.org/abs/2506.00388",
        "author": "Ni Mu, Hao Hu, Xiao Hu, Yiqin Yang, Bo Xu, Qing-Shan Jia",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00388v3 Announce Type: replace \nAbstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward engineering by inferring reward functions from human preference comparisons, enabling better alignment with human intentions. However, humans often struggle to label a clear preference between similar segments, reducing label efficiency and limiting PbRL's real-world applicability. To address this, we propose an offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback (CLARIFY), which learns a trajectory embedding space that incorporates preference information, ensuring clearly distinguished segments are spaced apart, thus facilitating the selection of more unambiguous queries. Extensive experiments demonstrate that CLARIFY outperforms baselines in both non-ideal teachers and real human feedback settings. Our approach not only selects more distinguished queries but also learns meaningful trajectory embeddings."
      },
      {
        "id": "oai:arXiv.org:2506.00551v2",
        "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation",
        "link": "https://arxiv.org/abs/2506.00551",
        "author": "Ming Wang, Peidong Wang, Lin Wu, Xiaocui Yang, Daling Wang, Shi Feng, Yuxin Chen, Bixuan Wang, Yifei Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00551v2 Announce Type: replace \nAbstract: Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers' mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator's configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on https://github.com/sci-m-wang/AnnaAgent."
      },
      {
        "id": "oai:arXiv.org:2506.00739v2",
        "title": "DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments",
        "link": "https://arxiv.org/abs/2506.00739",
        "author": "Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00739v2 Announce Type: replace \nAbstract: Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench's modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at https://github.com/microsoft/DefenderBench."
      },
      {
        "id": "oai:arXiv.org:2506.01064v2",
        "title": "Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs",
        "link": "https://arxiv.org/abs/2506.01064",
        "author": "Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, Yu Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01064v2 Announce Type: replace \nAbstract: Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. Despite their potential impact, the development of effective methods for purifying such adversarial examples has received relatively limited attention. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive \"fighting fire with fire\" strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code will be made publicly available."
      },
      {
        "id": "oai:arXiv.org:2506.02204v2",
        "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models",
        "link": "https://arxiv.org/abs/2506.02204",
        "author": "Lindia Tjuatja, Graham Neubig",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02204v2 Announce Type: replace \nAbstract: Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\" and \"exclamation marks after emotional statements\", where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone."
      },
      {
        "id": "oai:arXiv.org:2506.02285v2",
        "title": "Why Gradients Rapidly Increase Near the End of Training",
        "link": "https://arxiv.org/abs/2506.02285",
        "author": "Aaron Defazio",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02285v2 Announce Type: replace \nAbstract: During long-duration Large Language Model (LLM) training runs the gradient norm increases rapidly near the end of training. In this short note, we show that this increase is due to an unintended interaction between weight decay, normalization layers, and the learning rate schedule. We propose a simple correction that fixes this behavior while also resulting in lower loss values throughout training."
      },
      {
        "id": "oai:arXiv.org:2506.02300v2",
        "title": "Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation",
        "link": "https://arxiv.org/abs/2506.02300",
        "author": "Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02300v2 Announce Type: replace \nAbstract: Understanding the internal representations and decision mechanisms of deep neural networks remains a critical open challenge. While existing interpretability methods often identify influential input regions, they may not elucidate how a model distinguishes between classes or what specific changes would transition an input from one category to another. To address these limitations, we propose a novel framework that visualizes the implicit path between classes by treating the network gradient as a form of infinitesimal motion. Drawing inspiration from phase-based motion magnification, we first decompose images using invertible transforms-specifically the Complex Steerable Pyramid-then compute class-conditional gradients in the transformed space. Rather than iteratively integrating the gradient to trace a full path, we amplify the one-step gradient to the input and perform a linear extrapolation to expose how the model moves from source to target class. By operating in the steerable pyramid domain, these amplified gradients produce semantically meaningful, spatially coherent morphs that highlight the classifier's most sensitive directions, giving insight into the geometry of its decision boundaries. Experiments on both synthetic and real-world datasets demonstrate that our phase-focused extrapolation yields perceptually aligned, semantically meaningful transformations, offering a novel, interpretable lens into neural classifiers' internal representations."
      },
      {
        "id": "oai:arXiv.org:2506.02678v2",
        "title": "TL;DR: Too Long, Do Re-weighting for Efficient LLM Reasoning Compression",
        "link": "https://arxiv.org/abs/2506.02678",
        "author": "Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Xing W, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo, Xiao Liu, Fei Yin, Cheng-Lin Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02678v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon."
      },
      {
        "id": "oai:arXiv.org:2506.04098v2",
        "title": "TextAtari: 100K Frames Game Playing with Language Agents",
        "link": "https://arxiv.org/abs/2506.04098",
        "author": "Wenhao Li, Wenwu Li, Chuyun Shen, Junjie Sheng, Zixiao Huang, Di Wu, Yun Hua, Wei Yin, Xiangfeng Wang, Hongyuan Zha, Bo Jin",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.04098v2 Announce Type: replace \nAbstract: We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning. Our code is available at https://github.com/Lww007/Text-Atari-Agents."
      },
      {
        "id": "oai:arXiv.org:2506.04373v2",
        "title": "Mechanistic Decomposition of Sentence Representations",
        "link": "https://arxiv.org/abs/2506.04373",
        "author": "Matthieu Tehenan, Vikram Natarajan, Jonathan Michala, Milton Lin, Juri Opitz",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.04373v2 Announce Type: replace \nAbstract: Sentence embeddings are central to modern NLP and AI systems, yet little is known about their internal structure. While we can compare these embeddings using measures such as cosine similarity, the contributing features are not human-interpretable, and the content of an embedding seems untraceable, as it is masked by complex neural transformations and a final pooling operation that combines individual token embeddings. To alleviate this issue, we propose a new method to mechanistically decompose sentence embeddings into interpretable components, by using dictionary learning on token-level representations. We analyze how pooling compresses these features into sentence representations, and assess the latent features that reside in a sentence embedding. This bridges token-level mechanistic interpretability with sentence-level analysis, making for more transparent and controllable representations. In our studies, we obtain several interesting insights into the inner workings of sentence embedding spaces, for instance, that many semantic and syntactic aspects are linearly encoded in the embeddings."
      },
      {
        "id": "oai:arXiv.org:2506.04446v2",
        "title": "Selective Matching Losses -- Not All Scores Are Created Equal",
        "link": "https://arxiv.org/abs/2506.04446",
        "author": "Gil I. Shamir, Manfred K. Warmuth",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.04446v2 Announce Type: replace \nAbstract: Learning systems match predicted scores to observations over some domain. Often, it is critical to produce accurate predictions in some subset (or region) of the domain, yet less important to accurately predict in other regions. We construct selective matching loss functions by design of increasing link functions over score domains. A matching loss is an integral over the link. A link defines loss sensitivity as function of the score, emphasizing high slope high sensitivity regions over flat ones. Loss asymmetry drives a model and resolves its underspecification to predict better in high sensitivity regions where it is more important, and to distinguish between high and low importance regions. A large variety of selective scalar losses can be designed with scaled and shifted Sigmoid and hyperbolic sine links. Their properties, however, do not extend to multi-class. Applying them per dimension lacks ranking sensitivity that assigns importance according to class score ranking. Utilizing composite Softmax functions, we develop a framework for multidimensional selective losses. We overcome limitations of the standard Softmax function, that is good for classification, but not for distinction between adjacent scores. Selective losses have substantial advantage over traditional losses in applications with more important score regions, including dwell-time prediction, retrieval, ranking with either pointwise, contrastive pairwise, or listwise losses, distillation problems, and fine-tuning alignment of Large Language Models (LLMs)."
      },
      {
        "id": "oai:arXiv.org:2506.05176v2",
        "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
        "link": "https://arxiv.org/abs/2506.05176",
        "author": "Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05176v2 Announce Type: replace \nAbstract: In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license."
      },
      {
        "id": "oai:arXiv.org:2506.05434v2",
        "title": "Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks",
        "link": "https://arxiv.org/abs/2506.05434",
        "author": "Thomas Massena (IRIT, DTIPG - SNCF, UT3), L\\'eo and\\'eol (IMT, DTIPG - SNCF, UT3), Thibaut Boissin (IRIT, UT3), Franck Mamalet (IRIT, UT3), Corentin Friedrich (IRIT, UT3), Mathieu Serrurier (IRIT, UT3), S\\'ebastien Gerchinovitz (IMT)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05434v2 Announce Type: replace \nAbstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for improving the trustworthiness of neural networks by providing prediction sets with finite-sample guarantees. However, under adversarial attacks, classical conformal guarantees do not hold anymore: this problem is addressed in the field of Robust Conformal Prediction. Several methods have been proposed to provide robust CP sets with guarantees under adversarial perturbations, but, for large scale problems, these sets are either too large or the methods are too computationally demanding to be deployed in real life scenarios. In this work, we propose a new method that leverages Lipschitz-bounded networks to precisely and efficiently estimate robust CP sets. When combined with a 1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms state-of-the-art results in both the size of the robust CP sets and computational efficiency in medium and large-scale scenarios such as ImageNet. Taking a different angle, we also study vanilla CP under attack, and derive new worst-case coverage bounds of vanilla CP sets, which are valid simultaneously for all adversarial attack levels. Our lip-rcp method makes this second approach as efficient as vanilla CP while also allowing robustness guarantees."
      },
      {
        "id": "oai:arXiv.org:2506.05632v2",
        "title": "Gumbel-max List Sampling for Distribution Coupling with Multiple Samples",
        "link": "https://arxiv.org/abs/2506.05632",
        "author": "Joseph Rowan, Buu Phan, Ashish Khisti",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05632v2 Announce Type: replace \nAbstract: We study a relaxation of the problem of coupling probability distributions -- a list of samples is generated from one distribution and an accept is declared if any one of these samples is identical to the sample generated from the other distribution. We propose a novel method for generating samples, which extends the Gumbel-max sampling suggested in Daliri et al. (arXiv:2408.07978) for coupling probability distributions. We also establish a corresponding lower bound on the acceptance probability, which we call the list matching lemma. We next discuss two applications of our setup. First, we develop a new mechanism for multi-draft speculative sampling that is simple to implement and achieves performance competitive with baselines such as SpecTr and SpecInfer across a range of language tasks. Our method also guarantees a certain degree of drafter invariance with respect to the output tokens which is not supported by existing schemes. We also provide a theoretical lower bound on the token level acceptance probability. As our second application, we consider distributed lossy compression with side information in a setting where a source sample is compressed and available to multiple decoders, each with independent side information. We propose a compression technique that is based on our generalization of Gumbel-max sampling and show that it provides significant gains in experiments involving synthetic Gaussian sources and the MNIST image dataset."
      },
      {
        "id": "oai:arXiv.org:2506.05764v2",
        "title": "Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer",
        "link": "https://arxiv.org/abs/2506.05764",
        "author": "Haochuan Wang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05764v2 Announce Type: replace \nAbstract: Cryptocurrency price dynamics are driven largely by microstructural supply demand imbalances in the limit order book (LOB), yet the highly noisy nature of LOB data complicates the signal extraction process. Prior research has demonstrated that deep-learning architectures can yield promising predictive performance on pre-processed equity and futures LOB data, but they often treat model complexity as an unqualified virtue. In this paper, we aim to examine whether adding extra hidden layers or parameters to \"blackbox ish\" neural networks genuinely enhances short term price forecasting, or if gains are primarily attributable to data preprocessing and feature engineering. We benchmark a spectrum of models from interpretable baselines, logistic regression, XGBoost to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT LOB snapshots sampled at 100 ms to multi second intervals using publicly available Bybit data. We introduce two data filtering pipelines (Kalman, Savitzky Golay) and evaluate both binary (up/down) and ternary (up/flat/down) labeling schemes. Our analysis compares models on out of sample accuracy, latency, and robustness to noise. Results reveal that, with data preprocessing and hyperparameter tuning, simpler models can match and even exceed the performance of more complex networks, offering faster inference and greater interpretability."
      },
      {
        "id": "oai:arXiv.org:2506.05957v2",
        "title": "Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization",
        "link": "https://arxiv.org/abs/2506.05957",
        "author": "Tianjun Yao, Haoxuan Li, Yongqiang Chen, Tongliang Liu, Le Song, Eric Xing, Zhiqiang Shen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05957v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) often encounter significant performance degradation under distribution shifts between training and test data, hindering their applicability in real-world scenarios. Recent studies have proposed various methods to address the out-of-distribution generalization challenge, with many methods in the graph domain focusing on directly identifying an invariant subgraph that is predictive of the target label. However, we argue that identifying the edges from the invariant subgraph directly is challenging and error-prone, especially when some spurious edges exhibit strong correlations with the targets. In this paper, we propose PrunE, the first pruning-based graph OOD method that eliminates spurious edges to improve OOD generalizability. By pruning spurious edges, PrunE retains the invariant subgraph more comprehensively, which is critical for OOD generalization. Specifically, PrunE employs two regularization terms to prune spurious edges: 1) graph size constraint to exclude uninformative spurious edges, and 2) $\\epsilon$-probability alignment to further suppress the occurrence of spurious edges. Through theoretical analysis and extensive experiments, we show that PrunE achieves superior OOD performance and outperforms previous state-of-the-art methods significantly. Codes are available at: \\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}."
      },
      {
        "id": "oai:arXiv.org:2506.06292v2",
        "title": "Mutual-Taught for Co-adapting Policy and Reward Models",
        "link": "https://arxiv.org/abs/2506.06292",
        "author": "Tianyuan Shi, Canbin Huang, Fanqi Wan, Longguang Zhong, Ziyi Yang, Weizhou Shen, Xiaojun Quan, Ming Yan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06292v2 Announce Type: replace \nAbstract: During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench."
      },
      {
        "id": "oai:arXiv.org:2506.06300v2",
        "title": "LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization",
        "link": "https://arxiv.org/abs/2506.06300",
        "author": "Yuanye Zhou, Zhaokun Wang, Kai Zhou, Hui Tang, Xiaofan Li",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06300v2 Announce Type: replace \nAbstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless tool for topology optimization, capable of simultaneously determining optimal topologies and physical solutions. However, conventional PINNs rely on density-based topology descriptions, which necessitate manual interpolation and limit their applicability to complex geometries. To address this, we propose Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for boundary-focused engineering optimization. By parameterizing the control variables of topology boundary curves as learnable parameters, LT-PINNs eliminate the need for manual interpolation and enable precise boundary determination. We further introduce specialized boundary condition loss function and topology loss function to ensure sharp and accurate boundary representations, even for intricate topologies. The accuracy and robustness of LT-PINNs are validated via two types of partial differential equations (PDEs), including elastic equation with Dirichlet boundary conditions and Laplace's equation with Neumann boundary conditions. Furthermore, we demonstrate effectiveness of LT-PINNs on more complex time-dependent and time-independent flow problems without relying on measurement data, and showcase their engineering application potential in flow velocity rearrangement, transforming a uniform upstream velocity into a sine-shaped downstream profile. The results demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2) LT-PINNs can handle arbitrary boundary conditions, making them suitable for a wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries without manual interpolation, especially for complex topologies."
      },
      {
        "id": "oai:arXiv.org:2506.06395v2",
        "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
        "link": "https://arxiv.org/abs/2506.06395",
        "author": "Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, Ivan Oseledets",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06395v2 Announce Type: replace \nAbstract: Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision."
      },
      {
        "id": "oai:arXiv.org:2506.06715v2",
        "title": "A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks",
        "link": "https://arxiv.org/abs/2506.06715",
        "author": "Minh-Duc Nguyen, Dung D. Le",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06715v2 Announce Type: replace \nAbstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining the complete optimal solution in Multi-objective Learning (MOL). A set of optimal solutions approximates the Pareto set, and its mapping is a set of dense points in the Pareto front in objective space. However, some current methods face a challenge: how to make the Pareto solution is diverse while maximizing the hypervolume value. In this paper, we propose a novel method to address this challenge, which employs Stein Variational Gradient Descent (SVGD) to approximate the entire Pareto set. SVGD pushes a set of particles towards the Pareto set by applying a form of functional gradient descent, which helps to converge and diversify optimal solutions. Additionally, we employ diverse gradient direction strategies to thoroughly investigate a unified framework for SVGD in multi-objective optimization and adapt this framework with an annealing schedule to promote stability. We introduce our method, SVH-MOL, and validate its effectiveness through extensive experiments on multi-objective problems and multi-task learning, demonstrating its superior performance."
      },
      {
        "id": "oai:arXiv.org:2506.06733v2",
        "title": "RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation",
        "link": "https://arxiv.org/abs/2506.06733",
        "author": "Ruoxuan Zhang, Jidong Gao, Bin Wen, Hongxia Xie, Chenming Zhang, Hong-Han Shuai, Wen-Huang Cheng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06733v2 Announce Type: replace \nAbstract: Creating recipe images is a key challenge in food computing, with applications in culinary education and multimodal recipe assistants. However, existing datasets lack fine-grained alignment between recipe goals, step-wise instructions, and visual content. We present RecipeGen, the first large-scale, real-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video (I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes, 196,724 images, and 4,491 videos, covering diverse ingredients, cooking procedures, styles, and dish types. We further propose domain-specific evaluation metrics to assess ingredient fidelity and interaction modeling, benchmark representative T2I, I2V, and T2V models, and provide insights for future recipe generation models. Project page is available now."
      },
      {
        "id": "oai:arXiv.org:2506.07044v2",
        "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning",
        "link": "https://arxiv.org/abs/2506.07044",
        "author": "LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, Yu Rong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07044v2 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ..."
      },
      {
        "id": "oai:arXiv.org:2506.07280v2",
        "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models",
        "link": "https://arxiv.org/abs/2506.07280",
        "author": "Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07280v2 Announce Type: replace \nAbstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools, capable of synthesizing high-quality spatiotemporal content. Yet, their potential goes far beyond mere video generation. We argue that the training dynamics of VDMs, driven by the need to model coherent sequences, naturally pushes them to internalize structured representations and an implicit understanding of the visual world. To probe the extent of this internal knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs for new tasks using only a handful of examples. Our method transforms each task into a visual transition, enabling the training of LoRA weights on short input-output sequences without altering the generative interface of a frozen VDM. Despite minimal supervision, the model exhibits strong generalization across diverse tasks, from low-level vision (for example, segmentation and pose estimation) to high-level reasoning (for example, on ARC-AGI). These results reframe VDMs as more than generative engines. They are adaptable visual learners with the potential to serve as the backbone for future foundation models in vision."
      },
      {
        "id": "oai:arXiv.org:2506.07327v2",
        "title": "CASE: Contrastive Activation for Saliency Estimation",
        "link": "https://arxiv.org/abs/2506.07327",
        "author": "Dane Williamson, Yangfeng Ji, Matthew Dwyer",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07327v2 Announce Type: replace \nAbstract: Saliency methods are widely used to visualize which input features are deemed relevant to a model's prediction. However, their visual plausibility can obscure critical limitations. In this work, we propose a diagnostic test for class sensitivity: a method's ability to distinguish between competing class labels on the same input. Through extensive experiments, we show that many widely used saliency methods produce nearly identical explanations regardless of the class label, calling into question their reliability. We find that class-insensitive behavior persists across architectures and datasets, suggesting the failure mode is structural rather than model-specific. Motivated by these findings, we introduce CASE, a contrastive explanation method that isolates features uniquely discriminative for the predicted class. We evaluate CASE using the proposed diagnostic and a perturbation-based fidelity test, and show that it produces faithful and more class-specific explanations than existing methods."
      },
      {
        "id": "oai:arXiv.org:2506.07371v2",
        "title": "ARGUS: Hallucination and Omission Evaluation in Video-LLMs",
        "link": "https://arxiv.org/abs/2506.07371",
        "author": "Ruchit Rawal, Reza Shirkavand, Heng Huang, Gowthami Somepalli, Tom Goldstein",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07371v2 Announce Type: replace \nAbstract: Video large language models have not yet been widely deployed, largely due to their tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on multiple-choice questions. Unfortunately, VideoLLMs hallucinate far more aggressively on freeform text generation tasks like video captioning than they do on multiple choice verification tasks. To address this weakness, we propose ARGUS, a VideoLLM benchmark that measures freeform video captioning performance. By comparing VideoLLM outputs to human ground truth captions, ARGUS quantifies dual metrics. First, we measure the rate of hallucinations in the form of incorrect statements about video content or temporal relationships. Second, we measure the rate at which the model omits important descriptive details. Together, these dual metrics form a comprehensive view of video captioning performance."
      },
      {
        "id": "oai:arXiv.org:2506.07667v2",
        "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch",
        "link": "https://arxiv.org/abs/2506.07667",
        "author": "Prarabdh Shukla, Wei Yin Chong, Yash Patel, Brennan Schaffner, Danish Pruthi, Arjun Bhagoji",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07667v2 Announce Type: replace \nAbstract: To meet the demands of content moderation, online platforms have resorted to automated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users commenting on live streams) on platforms like Twitch exert additional pressures on the latency expected of such moderation systems. Despite their prevalence, relatively little is known about the effectiveness of these systems. In this paper, we conduct an audit of Twitch's automated moderation tool ($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful content. For our audit, we create streaming accounts to act as siloed test beds, and interface with the live chat using Twitch's APIs to send over $107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s accuracy in flagging blatantly hateful content containing misogyny, racism, ableism and homophobia. Our experiments reveal that a large fraction of hateful messages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$. Contextual addition of slurs to these messages results in $100\\%$ removal, revealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We also find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$ blocks up to $89.5\\%$ of benign examples that use sensitive words in pedagogical or empowering contexts. Overall, our audit points to large gaps in $\\texttt{AutoMod}$'s capabilities and underscores the importance for such systems to understand context effectively."
      },
      {
        "id": "oai:arXiv.org:2506.07739v2",
        "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models",
        "link": "https://arxiv.org/abs/2506.07739",
        "author": "Jing Zhong, Jun Yin, Peilin Li, Pengyu Zeng, Miao Zang, Ran Luo, Shuai Lu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07739v2 Announce Type: replace \nAbstract: Architectural cultures across regions are characterized by stylistic diversity, shaped by historical, social, and technological contexts in addition to geograph-ical conditions. Understanding architectural styles requires the ability to describe and analyze the stylistic features of different architects from various regions through visual observations of architectural imagery. However, traditional studies of architectural culture have largely relied on subjective expert interpretations and historical literature reviews, often suffering from regional biases and limited ex-planatory scope. To address these challenges, this study proposes three core contributions: (1) We construct a professional architectural style dataset named ArchDiffBench, which comprises 1,765 high-quality architectural images and their corresponding style annotations, collected from different regions and historical periods. (2) We propose ArchiLense, an analytical framework grounded in Vision-Language Models and constructed using the ArchDiffBench dataset. By integrating ad-vanced computer vision techniques, deep learning, and machine learning algo-rithms, ArchiLense enables automatic recognition, comparison, and precise classi-fication of architectural imagery, producing descriptive language outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show that ArchiLense achieves strong performance in architectural style recognition, with a 92.4% con-sistency rate with expert annotations and 84.5% classification accuracy, effec-tively capturing stylistic distinctions across images. The proposed approach transcends the subjectivity inherent in traditional analyses and offers a more objective and accurate perspective for comparative studies of architectural culture."
      },
      {
        "id": "oai:arXiv.org:2506.07976v2",
        "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
        "link": "https://arxiv.org/abs/2506.07976",
        "author": "Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, Aviral Kumar",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07976v2 Announce Type: replace \nAbstract: The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents."
      },
      {
        "id": "oai:arXiv.org:2506.07977v2",
        "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
        "link": "https://arxiv.org/abs/2506.07977",
        "author": "Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, Hai-Bao Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07977v2 Announce Type: replace \nAbstract: Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community."
      },
      {
        "id": "oai:arXiv.org:2001.01095v4",
        "title": "High-Dimensional Independence Testing via Maximum and Average Distance Correlations",
        "link": "https://arxiv.org/abs/2001.01095",
        "author": "Cencheng Shen, Yuexiao Dong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2001.01095v4 Announce Type: replace-cross \nAbstract: This paper investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, compare the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma."
      },
      {
        "id": "oai:arXiv.org:2010.11750v5",
        "title": "Precise High-Dimensional Asymptotics for Quantifying Heterogeneous Transfers",
        "link": "https://arxiv.org/abs/2010.11750",
        "author": "Fan Yang, Hongyang R. Zhang, Sen Wu, Christopher R\\'e, Weijie J. Su",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2010.11750v5 Announce Type: replace-cross \nAbstract: The problem of learning one task using samples from another task is central to transfer learning. In this paper, we focus on answering the following question: when does combining the samples from two related tasks perform better than learning with one target task alone? This question is motivated by an empirical phenomenon known as negative transfer, which has been observed in practice. While the transfer effect from one task to another depends on factors such as their sample sizes and the spectrum of their covariance matrices, precisely quantifying this dependence has remained a challenging problem. In order to compare a transfer learning estimator to single-task learning, one needs to compare the risks between the two estimators precisely. Further, the comparison depends on the distribution shifts between the two tasks. This paper applies recent developments of random matrix theory to tackle this challenge in a high-dimensional linear regression setting with two tasks. We show precise high-dimensional asymptotics for the bias and variance of a classical hard parameter sharing (HPS) estimator in the proportional limit, where the sample sizes of both tasks increase proportionally with dimension at fixed ratios. The precise asymptotics apply to various types of distribution shifts, including covariate shifts, model shifts, and combinations of both. We illustrate these results in a random-effects model to mathematically prove a phase transition from positive to negative transfer as the number of source task samples increases. One insight from the analysis is that a rebalanced HPS estimator, which downsizes the source task when the model shift is high, achieves the minimax optimal rate. The finding regarding phase transition also applies to multiple tasks when covariates are shared across tasks. Simulations validate the accuracy of the high-dimensional asymptotics for finite dimensions."
      },
      {
        "id": "oai:arXiv.org:2303.07475v2",
        "title": "General Loss Functions Lead to (Approximate) Interpolation in High Dimensions",
        "link": "https://arxiv.org/abs/2303.07475",
        "author": "Kuo-Wei Lai, Vidya Muthukumar",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2303.07475v2 Announce Type: replace-cross \nAbstract: We provide a unified framework that applies to a general family of convex losses across binary and multiclass settings in the overparameterized regime to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work, which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques and use our results to demonstrate the effect of certain loss functions designed for out-of-distribution problems on the closed-form solution."
      },
      {
        "id": "oai:arXiv.org:2304.09914v5",
        "title": "The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning",
        "link": "https://arxiv.org/abs/2304.09914",
        "author": "Sara Major, Aleksandar Toma\\v{s}evi\\'c",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2304.09914v5 Announce Type: replace-cross \nAbstract: Populist rhetoric employed on online media is characterized as deeply impassioned and often imbued with strong emotions. The aim of this paper is to empirically investigate the differences in affective nonverbal communication of political leaders. We use a deep-learning approach to process a sample of 220 YouTube videos of political leaders from 15 different countries, analyze their facial expressions of emotion and then examine differences in average emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the YouTube video. Based on a sample of manually coded images, we find that this deep-learning approach has 53-60\\% agreement with human labels. We observe statistically significant differences in the average score of negative emotions between groups of leaders with varying degrees of populist rhetoric."
      },
      {
        "id": "oai:arXiv.org:2306.08730v3",
        "title": "Over-the-Air Learning-based Geometry Point Cloud Transmission",
        "link": "https://arxiv.org/abs/2306.08730",
        "author": "Chenghong Bian, Yulin Shao, Deniz Gunduz",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2306.08730v3 Announce Type: replace-cross \nAbstract: This paper presents novel solutions for the efficient and reliable transmission of point clouds over wireless channels for real-time applications. We first propose SEmatic Point cloud Transmission (SEPT) for small-scale point clouds, which encodes the point cloud via an iterative downsampling and feature extraction process. At the receiver, SEPT decoder reconstructs the point cloud with latent reconstruction and offset-based upsampling. A novel channel-adaptive module is proposed to allow SEPT to operate effectively over a wide range of channel conditions. Next, we propose OTA-NeRF, a scheme inspired by neural radiance fields. OTA-NeRF performs voxelization to the point cloud input and learns to encode the voxelized point cloud into a neural network. Instead of transmitting the extracted feature vectors as in SEPT, it transmits the learned neural network weights in an analog fashion along with few hyperparameters that are transmitted digitally. At the receiver, the OTA-NeRF decoder reconstructs the original point cloud using the received noisy neural network weights. To further increase the bandwidth efficiency of the OTA-NeRF scheme, a fine-tuning algorithm is developed, where only a fraction of the neural network weights are retrained and transmitted. Noticing the poor generality of the OTA-NeRF schemes, we propose an alternative approach, termed OTA-MetaNeRF, which encodes different input point clouds into the latent vectors with shared neural network weights. Extensive numerical experiments confirm that the proposed SEPT, OTA-NeRF and OTA-MetaNeRF schemes achieve superior or comparable performance over the conventional approaches, where an octree-based or a learning-based point cloud compression scheme is concatenated with a channel code. Finally, the run-time complexities are evaluated to verify the capability of the proposed schemes for real-time communications."
      },
      {
        "id": "oai:arXiv.org:2309.09652v4",
        "title": "Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers",
        "link": "https://arxiv.org/abs/2309.09652",
        "author": "Peter Ochieng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2309.09652v4 Announce Type: replace-cross \nAbstract: This work introduces UDPNet, a novel architecture designed to accelerate the reverse diffusion process in speech synthesis. Unlike traditional diffusion models that rely on timestep embeddings and shared network parameters, UDPNet unrolls the reverse diffusion process directly into the network architecture, with successive layers corresponding to equally spaced steps in the diffusion schedule. Each layer progressively refines the noisy input, culminating in a high-fidelity estimation of the original data, \\(x_0\\). Additionally, we redefine the learning target by predicting latent variables instead of the conventional \\(x_0\\) or noise \\(\\epsilon_0\\). This shift addresses the common issue of large prediction errors in early denoising stages, effectively reducing speech distortion. Extensive evaluations on single- and multi-speaker datasets demonstrate that UDPNet consistently outperforms state-of-the-art methods in both quality and efficiency, while generalizing effectively to unseen speech. These results position UDPNet as a robust solution for real-time speech synthesis applications. Sample audio is available at https://onexpeters.github.io/UDPNet/."
      },
      {
        "id": "oai:arXiv.org:2310.10315v4",
        "title": "A Survey on Quantum Machine Learning: Current Trends, Challenges, Opportunities, and the Road Ahead",
        "link": "https://arxiv.org/abs/2310.10315",
        "author": "Kamila Zaman, Alberto Marchisio, Muhammad Abdullah Hanif, Muhammad Shafique",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2310.10315v4 Announce Type: replace-cross \nAbstract: Quantum Computing (QC) claims to improve the efficiency of solving complex problems, compared to classical computing. When QC is integrated with Machine Learning (ML), it creates a Quantum Machine Learning (QML) system. This paper aims to provide a thorough understanding of the foundational concepts of QC and its notable advantages over classical computing. Following this, we delve into the key aspects of QML in a detailed and comprehensive manner.\n  In this survey, we investigate a variety of QML algorithms, discussing their applicability across different domains. We examine quantum datasets, highlighting their unique characteristics and advantages. The survey also covers the current state of hardware technologies, providing insights into the latest advancements and their implications for QML. Additionally, we review the software tools and simulators available for QML development, discussing their features and usability.\n  Furthermore, we explore practical applications of QML, illustrating how it can be leveraged to solve real-world problems more efficiently than classical ML methods. This survey aims to consolidate the current landscape of QML and outline key opportunities and challenges for future research."
      },
      {
        "id": "oai:arXiv.org:2401.00776v2",
        "title": "Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study in the Autism Spectrum Disorder Therapy",
        "link": "https://arxiv.org/abs/2401.00776",
        "author": "Qin Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2401.00776v2 Announce Type: replace-cross \nAbstract: In recent years, edge computing has served as a paradigm that enables many future technologies like AI, Robotics, IoT, and high-speed wireless sensor networks (like 5G) by connecting cloud computing facilities and services to the end users. Especially in medical and healthcare applications, it provides remote patient monitoring and increases voluminous multimedia. From the robotics angle, robot-assisted therapy (RAT) is an active-assistive robotic technology in rehabilitation robotics, attracting researchers to study and benefit people with disability like autism spectrum disorder (ASD) children. However, the main challenge of RAT is that the model capable of detecting the affective states of ASD people exists and can recall individual preferences. Moreover, involving expert diagnosis and recommendations to guide robots in updating the therapy approach to adapt to different statuses and scenarios is a crucial part of the ASD therapy process. This paper proposes the architecture of edge cognitive computing by combining human experts and assisted robots collaborating in the same framework to achieve a seamless remote diagnosis, round-the-clock symptom monitoring, emergency warning, therapy alteration, and advanced assistance."
      },
      {
        "id": "oai:arXiv.org:2401.11576v5",
        "title": "Quantum Architecture Search with Unsupervised Representation Learning",
        "link": "https://arxiv.org/abs/2401.11576",
        "author": "Yize Sun, Zixin Wu, Yunpu Ma, Volker Tresp",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2401.11576v5 Announce Type: replace-cross \nAbstract: Unsupervised representation learning presents new opportunities for advancing Quantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ) devices. QAS is designed to optimize quantum circuits for Variational Quantum Algorithms (VQAs). Most QAS algorithms tightly couple the search space and search algorithm, typically requiring the evaluation of numerous quantum circuits, resulting in high computational costs and limiting scalability to larger quantum circuits. Predictor-based QAS algorithms mitigate this issue by estimating circuit performance based on structure or embedding. However, these methods often demand time-intensive labeling to optimize gate parameters across many circuits, which is crucial for training accurate predictors. Inspired by the classical neural architecture search algorithm Arch2vec, we investigate the potential of unsupervised representation learning for QAS without relying on predictors. Our framework decouples unsupervised architecture representation learning from the search process, enabling the learned representations to be applied across various downstream tasks. Additionally, it integrates an improved quantum circuit graph encoding scheme, addressing the limitations of existing representations and enhancing search efficiency. This predictor-free approach removes the need for large labeled datasets. During the search, we employ REINFORCE and Bayesian Optimization to explore the latent representation space and compare their performance against baseline methods. We further validate our approach by executing the best-discovered MaxCut circuits on IBM's ibm_sherbrooke quantum processor, confirming that the architectures retain optimal performance even under real hardware noise. Our results demonstrate that the framework efficiently identifies high-performing quantum circuits with fewer search iterations."
      },
      {
        "id": "oai:arXiv.org:2401.17177v4",
        "title": "Data-Driven Discovery of PDEs via the Adjoint Method",
        "link": "https://arxiv.org/abs/2401.17177",
        "author": "Mohsen Sadr, Tony Tohme, Kamal Youcef-Toumi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2401.17177v4 Announce Type: replace-cross \nAbstract: In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of parameterized PDEs encompassing linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017), on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter."
      },
      {
        "id": "oai:arXiv.org:2402.14515v3",
        "title": "Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks",
        "link": "https://arxiv.org/abs/2402.14515",
        "author": "Patrick Holzer, Ivica Turkalj",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2402.14515v3 Announce Type: replace-cross \nAbstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning.\n  We analyze this frequency spectrum using the Minkowski sum for sets and the set of differences, which makes it particularly easy to express and calculate the frequency spectrum algebraically, and prove different maximality results for a large class of models.\n  Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A:=R\\cdot L$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depends only on the area $A=RL$ and not on the individual values of $R$ and $L$. Moreover, we collect and extend existing results and specify the maximum possible frequency spectrum of a QNN with arbitrarily many layers as a function of the spectrum of its generators. In the case of arbitrary dimensional generators, where our two introduces notions of maximality differ, we extend existing results based on the so-called Golomb ruler and introduce a second novel approach based on a variation of the turnpike problem, which we call the relaxed turnpike problem."
      },
      {
        "id": "oai:arXiv.org:2403.07143v3",
        "title": "Harnessing the Continuous Structure: Utilizing the First-order Approach in Online Contract Design",
        "link": "https://arxiv.org/abs/2403.07143",
        "author": "Shiliang Zuo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.07143v3 Announce Type: replace-cross \nAbstract: This work studies the online contract design problem. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions). We leverage the structure provided by continuous action spaces, which allows the application of first-order conditions (FOC) to characterize the agent's behavior. In some cases, we utilize conditions from the first-order approach (FOA) in economics, but in certain settings, we are able to apply FOC without additional assumptions, leading to simpler and more principled algorithms.\n  We illustrate this approach in three problem settings. Firstly, we study the problem of learning the optimal contract when there can be many outcomes. In contrast to prior works that design highly specialized algorithms, we show that the problem can be directly reduced to Lipschitz bandits. Secondly, we study the problem of learning linear contracts. While the contracting problem involves hidden action (moral hazard) and the pricing problem involves hidden value (adverse selection), the two problems share a similar optimization structure, which enables direct reduction between the problem of learning linear contracts and dynamic pricing. Thirdly, we study the problem of learning contracts with many outcomes when agents are identical and provide an algorithm with polynomial sample complexity."
      },
      {
        "id": "oai:arXiv.org:2405.17998v2",
        "title": "Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop",
        "link": "https://arxiv.org/abs/2405.17998",
        "author": "Yuqi Zhou, Sunhao Dai, Liang Pang, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2405.17998v2 Announce Type: replace-cross \nAbstract: Recommender systems are essential for information access, allowing users to present their content for recommendation. With the rise of large language models (LLMs), AI-generated content (AIGC), primarily in the form of text, has become a central part of the content ecosystem. As AIGC becomes increasingly prevalent, it is important to understand how it affects the performance and dynamics of recommender systems. To this end, we construct an environment that incorporates AIGC to explore its short-term impact. The results from popular sequential recommendation models reveal that AIGC are ranked higher in the recommender system, reflecting the phenomenon of source bias. To further explore the long-term impact of AIGC, we introduce a feedback loop with realistic simulators. The results show that the model's preference for AIGC increases as the user clicks on AIGC rises and the model trains on simulated click data. This leads to two issues: In the short term, bias toward AIGC encourages LLM-based content creation, increasing AIGC content, and causing unfair traffic distribution. From a long-term perspective, our experiments also show that when AIGC dominates the content ecosystem after a feedback loop, it can lead to a decline in recommendation performance. To address these issues, we propose a debiasing method based on L1-loss optimization to maintain long-term content ecosystem balance. In a real-world environment with AIGC generated by mainstream LLMs, our method ensures a balance between AIGC and human-generated content in the ecosystem. The code and dataset are available at https://github.com/Yuqi-Zhou/Rec_SourceBias."
      },
      {
        "id": "oai:arXiv.org:2406.13348v3",
        "title": "Textual Unlearning Gives a False Sense of Unlearning",
        "link": "https://arxiv.org/abs/2406.13348",
        "author": "Jiacheng Du, Zhibo Wang, Jie Zhang, Xiaoyi Pang, Jiahui Hu, Kui Ren",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2406.13348v3 Announce Type: replace-cross \nAbstract: Language Models (LMs) are prone to ''memorizing'' training data, including substantial sensitive user information. To mitigate privacy risks and safeguard the right to be forgotten, machine unlearning has emerged as a promising approach for enabling LMs to efficiently ''forget'' specific texts. However, despite the good intentions, is textual unlearning really as effective and reliable as expected? To address the concern, we first propose Unlearning Likelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing method, and find that unlearned texts can still be detected with very high confidence after unlearning. Further, we conduct an in-depth investigation on the privacy risks of textual unlearning mechanisms in deployment and present the Textual Unlearning Leakage Attack (TULA), along with its variants in both black- and white-box scenarios. We show that textual unlearning mechanisms could instead reveal more about the unlearned texts, exposing them to significant membership inference and data reconstruction risks. Our findings highlight that existing textual unlearning actually gives a false sense of unlearning, underscoring the need for more robust and secure unlearning mechanisms."
      },
      {
        "id": "oai:arXiv.org:2407.01067v2",
        "title": "Human-like object concept representations emerge naturally in multimodal large language models",
        "link": "https://arxiv.org/abs/2407.01067",
        "author": "Changde Du, Kaicheng Fu, Bincheng Wen, Yi Sun, Jie Peng, Wei Wei, Ying Gao, Shengpei Wang, Chuncheng Zhang, Jinpeng Li, Shuang Qiu, Le Chang, Huiguang He",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.01067v2 Announce Type: replace-cross \nAbstract: Understanding how humans conceptualize and categorize natural objects offers critical insights into perception and cognition. With the advent of Large Language Models (LLMs), a key question arises: can these models develop human-like object representations from linguistic and multimodal data? In this study, we combined behavioral and neuroimaging analyses to explore the relationship between object concept representations in LLMs and human cognition. We collected 4.7 million triplet judgments from LLMs and Multimodal LLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity structure of 1,854 natural objects. The resulting 66-dimensional embeddings were stable, predictive, and exhibited semantic clustering similar to human mental representations. Remarkably, the dimensions underlying these embeddings were interpretable, suggesting that LLMs and MLLMs develop human-like conceptual representations of objects. Further analysis showed strong alignment between model embeddings and neural activity patterns in brain regions such as EBA, PPA, RSC, and FFA. This provides compelling evidence that the object representations in LLMs, while not identical to human ones, share fundamental similarities that reflect key aspects of human conceptual knowledge. Our findings advance the understanding of machine intelligence and inform the development of more human-like artificial cognitive systems."
      },
      {
        "id": "oai:arXiv.org:2407.13625v3",
        "title": "Distributionally and Adversarially Robust Logistic Regression via Intersecting Wasserstein Balls",
        "link": "https://arxiv.org/abs/2407.13625",
        "author": "Aras Selvi, Eleonora Kreacic, Mohsen Ghassemi, Vamsi Potluru, Tucker Balch, Manuela Veloso",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2407.13625v3 Announce Type: replace-cross \nAbstract: Adversarially robust optimization (ARO) has emerged as the *de facto* standard for training models that hedge against adversarial attacks in the test stage. While these models are robust against adversarial attacks, they tend to suffer severely from overfitting. To address this issue, some successful methods replace the empirical distribution in the training stage with alternatives including *(i)* a worst-case distribution residing in an ambiguity set, resulting in a distributionally robust (DR) counterpart of ARO; *(ii)* a mixture of the empirical distribution with a distribution induced by an auxiliary (*e.g.*, synthetic, external, out-of-domain) dataset. Inspired by the former, we study the Wasserstein DR counterpart of ARO for logistic regression and show it admits a tractable convex optimization reformulation. Adopting the latter setting, we revise the DR approach by intersecting its ambiguity set with another ambiguity set built using the auxiliary dataset, which offers a significant improvement whenever the Wasserstein distance between the data generating and auxiliary distributions can be estimated. We study the underlying optimization problem, develop efficient solution algorithms, and demonstrate that the proposed method outperforms benchmark approaches on standard datasets."
      },
      {
        "id": "oai:arXiv.org:2408.00856v4",
        "title": "Penalty Learning for Optimal Partitioning using Multilayer Perceptron",
        "link": "https://arxiv.org/abs/2408.00856",
        "author": "Tung L Nguyen, Toby Dylan Hocking",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.00856v4 Announce Type: replace-cross \nAbstract: Changepoint detection is a technique used to identify significant shifts in sequences and is widely used in fields such as finance, genomics, and medicine. To identify the changepoints, dynamic programming (DP) algorithms, particularly Optimal Partitioning (OP) family, are widely used. To control the changepoints count, these algorithms use a fixed penalty to penalize the changepoints presence. To predict the optimal value of that penalty, existing methods used simple models such as linear or tree-based, which may limit predictive performance. To address this issue, this study proposes using a multilayer perceptron (MLP) with a ReLU activation function to predict the penalty. The proposed model generates continuous predictions -- as opposed to the stepwise ones in tree-based models -- and handles non-linearity better than linear models. Experiments on large benchmark genomic datasets demonstrate that the proposed model improves accuracy and F1 score compared to existing models."
      },
      {
        "id": "oai:arXiv.org:2408.12212v3",
        "title": "Relational decomposition for program synthesis",
        "link": "https://arxiv.org/abs/2408.12212",
        "author": "C\\'eline Hocquette, Andrew Cropper",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.12212v3 Announce Type: replace-cross \nAbstract: We introduce a relational approach to program synthesis. The key idea is to decompose synthesis tasks into simpler relational synthesis subtasks. Specifically, our representation decomposes a training input-output example into sets of input and output facts respectively. We then learn relations between the input and output facts. We demonstrate our approach using an off-the-shelf inductive logic programming (ILP) system on four challenging synthesis datasets. Our results show that (i) our representation can outperform a standard one, and (ii) an off-the-shelf ILP system with our representation can outperform domain-specific approaches."
      },
      {
        "id": "oai:arXiv.org:2408.16508v2",
        "title": "Branch-and-cut algorithms for colorful components problems",
        "link": "https://arxiv.org/abs/2408.16508",
        "author": "Claudia Archetti, Martina Cerulli, Carmine Sorgente",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2408.16508v2 Announce Type: replace-cross \nAbstract: We tackle three optimization problems in which a colored graph, where each node is assigned a color, must be partitioned into colorful connected components. A component is defined as colorful if each color appears at most once. The problems differ in the objective function, which determines which partition is the best one. These problems have applications in community detection, cybersecurity, and bioinformatics. We present integer non-linear formulations, which are then linearized using standard techniques. To solve these formulations, we develop exact branch-and-cut algorithms, embedding various improving techniques, such as valid inequalities, bounds limiting the number of variables, and warm-start and preprocessing techniques. Extensive computational tests on benchmark instances demonstrate the effectiveness of the proposed procedures. The branch-and-cut algorithms can solve reasonably sized instances efficiently. To the best of our knowledge, we are the first to propose an exact algorithm for solving these problems."
      },
      {
        "id": "oai:arXiv.org:2410.13148v2",
        "title": "Learning Efficient Representations of Neutrino Telescope Events",
        "link": "https://arxiv.org/abs/2410.13148",
        "author": "Felix J. Yu, Nicholas Kamp, Carlos A. Arg\\\"uelles",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.13148v2 Announce Type: replace-cross \nAbstract: Neutrino telescopes detect rare interactions of particles produced in some of the most extreme environments in the Universe. This is accomplished by instrumenting a cubic-kilometer volume of naturally occurring transparent medium with light sensors. Given their substantial size and the high frequency of background interactions, these telescopes amass an enormous quantity of large variance, high-dimensional data. These attributes create substantial challenges for analyzing and reconstructing interactions, particularly when utilizing machine learning (ML) techniques. In this paper, we present a novel approach, called om2vec, that employs transformer-based variational autoencoders to efficiently represent neutrino telescope events by learning compact and descriptive latent representations. We demonstrate that these latent representations offer enhanced flexibility and improved computational efficiency, thereby facilitating downstream tasks in data analysis."
      },
      {
        "id": "oai:arXiv.org:2410.23323v2",
        "title": "Phonology-Guided Speech-to-Speech Translation for African Languages",
        "link": "https://arxiv.org/abs/2410.23323",
        "author": "Peter Ochieng, Dennis Kaburu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.23323v2 Announce Type: replace-cross \nAbstract: We present a prosody-guided framework for speech-to-speech translation (S2ST) that aligns and translates speech \\emph{without} transcripts by leveraging cross-linguistic pause synchrony. Analyzing a 6{,}000-hour East African news corpus spanning five languages, we show that \\emph{within-phylum} language pairs exhibit 30--40\\% lower pause variance and over 3$\\times$ higher onset/offset correlation compared to cross-phylum pairs. These findings motivate \\textbf{SPaDA}, a dynamic-programming alignment algorithm that integrates silence consistency, rate synchrony, and semantic similarity. SPaDA improves alignment $F_1$ by +3--4 points and eliminates up to 38\\% of spurious matches relative to greedy VAD baselines. Using SPaDA-aligned segments, we train \\textbf{SegUniDiff}, a diffusion-based S2ST model guided by \\emph{external gradients} from frozen semantic and speaker encoders. SegUniDiff matches an enhanced cascade in BLEU (30.3 on CVSS-C vs.\\ 28.9 for UnitY), reduces speaker error rate (EER) from 12.5\\% to 5.3\\%, and runs at an RTF of 1.02. To support evaluation in low-resource settings, we also release a three-tier, transcript-free BLEU suite (M1--M3) that correlates strongly with human judgments. Together, our results show that prosodic cues in multilingual speech provide a reliable scaffold for scalable, non-autoregressive S2ST."
      },
      {
        "id": "oai:arXiv.org:2411.02150v2",
        "title": "Cooperative and Collaborative Multi-Task Semantic Communication for Distributed Sources",
        "link": "https://arxiv.org/abs/2411.02150",
        "author": "Ahmad Halimi Razlighi, Maximilian H. V. Tillmann, Edgar Beck, Carsten Bockelmann, Armin Dekorsy",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.02150v2 Announce Type: replace-cross \nAbstract: In this paper, we explore a multi-task semantic communication (SemCom) system for distributed sources, extending the existing focus on collaborative single-task execution. We build on the cooperative multi-task processing introduced in [1], which divides the encoder into a common unit (CU) and multiple specific units (SUs). While earlier studies in multi-task SemCom focused on full observation settings, our research explores a more realistic case where only distributed partial observations are available, such as in a production line monitored by multiple sensing nodes. To address this, we propose an SemCom system that supports multi-task processing through cooperation on the transmitter side via split structure and collaboration on the receiver side. We have used an information-theoretic perspective with variational approximations for our end-to-end data-driven approach. Simulation results demonstrate that the proposed cooperative and collaborative multi-task (CCMT) SemCom system significantly improves task execution accuracy, particularly in complex datasets, if the noise introduced from the communication channel is not limiting the task performance too much. Our findings contribute to a more general SemCom framework capable of handling distributed sources and multiple tasks simultaneously, advancing the applicability of SemCom systems in real-world scenarios."
      },
      {
        "id": "oai:arXiv.org:2411.09160v2",
        "title": "Innate-Values-driven Reinforcement Learning based Cognitive Modeling",
        "link": "https://arxiv.org/abs/2411.09160",
        "author": "Qin Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2411.09160v2 Announce Type: replace-cross \nAbstract: Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences for pursuing goals and drive them to develop diverse skills that satisfy their various needs. Traditional reinforcement learning (RL) is learning from interaction based on the feedback rewards of the environment. However, in real scenarios, the rewards are generated by agents' innate value systems, which differ vastly from individuals based on their needs and requirements. In other words, considering the AI agent as a self-organizing system, developing its awareness through balancing internal and external utilities based on its needs in different tasks is a crucial problem for individuals learning to support others and integrate community with safety and harmony in the long term. To address this gap, we propose a new RL model termed innate-values-driven RL (IVRL) based on combined motivations' models and expected utility theory to mimic its complex behaviors in the evolution through decision-making and learning. Then, we introduce two IVRL-based models: IV-DQN and IV-A2C. By comparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the Role-Playing Game (RPG) reinforcement learning test platform VIZDoom, we demonstrated that the IVRL-based models can help the agent rationally organize various needs, achieve better performance effectively."
      },
      {
        "id": "oai:arXiv.org:2412.07192v2",
        "title": "PrisonBreak: Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips",
        "link": "https://arxiv.org/abs/2412.07192",
        "author": "Zachary Coalson, Jeonghyun Woo, Yu Sun, Shiyang Chen, Lishan Yang, Prashant Nair, Bo Fang, Sanghyun Hong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07192v2 Announce Type: replace-cross \nAbstract: We introduce a new class of attacks on commercial-scale (human-aligned) language models that induce jailbreaking through targeted bitwise corruptions in model parameters. Our adversary can jailbreak billion-parameter language models with fewer than 25 bit-flips in all cases$-$and as few as 5 in some$-$using up to 40$\\times$ less bit-flips than existing attacks on computer vision models at least 100$\\times$ smaller. Unlike prompt-based jailbreaks, our attack renders these models in memory 'uncensored' at runtime, allowing them to generate harmful responses without any input modifications. Our attack algorithm efficiently identifies target bits to flip, offering up to 20$\\times$ more computational efficiency than previous methods. This makes it practical for language models with billions of parameters. We show an end-to-end exploitation of our attack using software-induced fault injection, Rowhammer (RH). Our work examines 56 DRAM RH profiles from DDR4 and LPDDR4X devices with different RH vulnerabilities. We show that our attack can reliably induce jailbreaking in systems similar to those affected by prior bit-flip attacks. Moreover, our approach remains effective even against highly RH-secure systems (e.g., 46$\\times$ more secure than previously tested systems). Our analyses further reveal that: (1) models with less post-training alignment require fewer bit flips to jailbreak; (2) certain model components, such as value projection layers, are substantially more vulnerable than others; and (3) our method is mechanistically different than existing jailbreaks. Our findings highlight a pressing, practical threat to the language model ecosystem and underscore the need for research to protect these models from bit-flip attacks."
      },
      {
        "id": "oai:arXiv.org:2501.00296v3",
        "title": "From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models",
        "link": "https://arxiv.org/abs/2501.00296",
        "author": "Ashay Athalye, Nishanth Kumar, Tom Silver, Yichao Liang, Jiuguang Wang, Tom\\'as Lozano-P\\'erez, Leslie Pack Kaelbling",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00296v3 Announce Type: replace-cross \nAbstract: Our aim is to learn to solve long-horizon decision-making problems in complex robotics domains given low-level skills and a handful of short-horizon demonstrations containing sequences of images. To this end, we focus on learning abstract symbolic world models that facilitate zero-shot generalization to novel goals via planning. A critical component of such models is the set of symbolic predicates that define properties of and relationships between objects. In this work, we leverage pretrained vision language models (VLMs) to propose a large set of visual predicates potentially relevant for decision-making, and to evaluate those predicates directly from camera images. At training time, we pass the proposed predicates and demonstrations into an optimization-based model-learning algorithm to obtain an abstract symbolic world model that is defined in terms of a compact subset of the proposed predicates. At test time, given a novel goal in a novel setting, we use the VLM to construct a symbolic description of the current world state, and then use a search-based planning algorithm to find a sequence of low-level skills that achieves the goal. We demonstrate empirically across experiments in both simulation and the real world that our method can generalize aggressively, applying its learned world model to solve problems with a wide variety of object types, arrangements, numbers of objects, and visual backgrounds, as well as novel goals and much longer horizons than those seen at training time."
      },
      {
        "id": "oai:arXiv.org:2501.04009v2",
        "title": "Multi-SpaCE: Multi-Objective Subsequence-based Sparse Counterfactual Explanations for Multivariate Time Series Classification",
        "link": "https://arxiv.org/abs/2501.04009",
        "author": "Mario Refoyo, David Luengo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.04009v2 Announce Type: replace-cross \nAbstract: Deep Learning systems excel in complex tasks but often lack transparency, limiting their use in critical applications. Counterfactual explanations, a core tool within eXplainable Artificial Intelligence (XAI), offer insights into model decisions by identifying minimal changes to an input to alter its predicted outcome. However, existing methods for time series data are limited by univariate assumptions, rigid constraints on modifications, or lack of validity guarantees. This paper introduces Multi-SpaCE, a multi-objective counterfactual explanation method for multivariate time series. Using non-dominated ranking genetic algorithm II (NSGA-II), Multi-SpaCE balances proximity, sparsity, plausibility, and contiguity. Unlike most methods, it ensures perfect validity, supports multivariate data and provides a Pareto front of solutions, enabling flexibility to different end-user needs. Comprehensive experiments in diverse datasets demonstrate the ability of Multi-SpaCE to consistently achieve perfect validity and deliver superior performance compared to existing methods."
      },
      {
        "id": "oai:arXiv.org:2501.04179v2",
        "title": "Generation from Noisy Examples",
        "link": "https://arxiv.org/abs/2501.04179",
        "author": "Ananth Raman, Vinod Raman",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.04179v2 Announce Type: replace-cross \nAbstract: We continue to study the learning-theoretic foundations of generation by extending the results from Kleinberg and Mullainathan [2024] and Li et al. [2024] to account for noisy example streams. In the noiseless setting of Kleinberg and Mullainathan [2024] and Li et al. [2024], an adversary picks a hypothesis from a binary hypothesis class and provides a generator with a sequence of its positive examples. The goal of the generator is to eventually output new, unseen positive examples. In the noisy setting, an adversary still picks a hypothesis and a sequence of its positive examples. But, before presenting the stream to the generator, the adversary inserts a finite number of negative examples. Unaware of which examples are noisy, the goal of the generator is to still eventually output new, unseen positive examples. In this paper, we provide necessary and sufficient conditions for when a binary hypothesis class can be noisily generatable. We provide such conditions with respect to various constraints on the number of distinct examples that need to be seen before perfect generation of positive examples. Interestingly, for finite and countable classes we show that generatability is largely unaffected by the presence of a finite number of noisy examples."
      },
      {
        "id": "oai:arXiv.org:2501.17042v4",
        "title": "Emergence of network communities driven by local rules",
        "link": "https://arxiv.org/abs/2501.17042",
        "author": "Alexei Vazquez",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.17042v4 Announce Type: replace-cross \nAbstract: Natural systems are modeled by networks with nodes and links. Often the nodes are segregated into communities with different connectivity patterns. Node heterogeneity such as political affiliation in social networks or biological function in gene networks are highlighted as key factors driving the segregation of nodes into communities. Here, by means of numerical simulations, I show that node heterogeneity is not a necessary requirement. To this end I introduce the Ramsey community number, $r_ \\kappa$, the minimum graph size that warranties the emergence of network communities with almost certainty. Using the stochastic block model and Infomap methods for community detection, I show that networks generated by local rules have finite $r_ \\kappa$ values while their randomized versions do not have emergent communities. I conjecture that network communities are an emergent property of networks evolving with local rules."
      },
      {
        "id": "oai:arXiv.org:2501.18016v2",
        "title": "Digital Twin Synchronization: Bridging the Sim-RL Agent to a Real-Time Robotic Additive Manufacturing Control",
        "link": "https://arxiv.org/abs/2501.18016",
        "author": "Matsive Ali, Sandesh Giri, Sen Liu, Qin Yang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2501.18016v2 Announce Type: replace-cross \nAbstract: With the rapid development of deep reinforcement learning technology, it gradually demonstrates excellent potential and is becoming the most promising solution in the robotics. However, in the smart manufacturing domain, there is still not too much research involved in dynamic adaptive control mechanisms optimizing complex processes. This research advances the integration of Soft Actor-Critic (SAC) with digital twins for industrial robotics applications, providing a framework for enhanced adaptive real-time control for smart additive manufacturing processing. The system architecture combines Unity's simulation environment with ROS2 for seamless digital twin synchronization, while leveraging transfer learning to efficiently adapt trained models across tasks. We demonstrate our methodology using a Viper X300s robot arm with the proposed hierarchical reward structure to address the common reinforcement learning challenges in two distinct control scenarios. The results show rapid policy convergence and robust task execution in both simulated and physical environments demonstrating the effectiveness of our approach."
      },
      {
        "id": "oai:arXiv.org:2502.01583v2",
        "title": "Spectral Estimators for Multi-Index Models: Precise Asymptotics and Optimal Weak Recovery",
        "link": "https://arxiv.org/abs/2502.01583",
        "author": "Filip Kova\\v{c}evi\\'c, Yihan Zhang, Marco Mondelli",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01583v2 Announce Type: replace-cross \nAbstract: Multi-index models provide a popular framework to investigate the learnability of functions with low-dimensional structure and, also due to their connections with neural networks, they have been object of recent intensive study. In this paper, we focus on recovering the subspace spanned by the signals via spectral estimators -- a family of methods routinely used in practice, often as a warm-start for iterative algorithms. Our main technical contribution is a precise asymptotic characterization of the performance of spectral methods, when sample size and input dimension grow proportionally and the dimension $p$ of the space to recover is fixed. Specifically, we locate the top-$p$ eigenvalues of the spectral matrix and establish the overlaps between the corresponding eigenvectors (which give the spectral estimators) and a basis of the signal subspace. Our analysis unveils a phase transition phenomenon in which, as the sample complexity grows, eigenvalues escape from the bulk of the spectrum and, when that happens, eigenvectors recover directions of the desired subspace. The precise characterization we put forward enables the optimization of the data preprocessing, thus allowing to identify the spectral estimator that requires the minimal sample size for weak recovery."
      },
      {
        "id": "oai:arXiv.org:2502.05122v2",
        "title": "Distinguishing Cause from Effect with Causal Velocity Models",
        "link": "https://arxiv.org/abs/2502.05122",
        "author": "Johnny Xi, Hugh Dance, Peter Orbanz, Benjamin Bloem-Reddy",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05122v2 Announce Type: replace-cross \nAbstract: Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation."
      },
      {
        "id": "oai:arXiv.org:2502.06995v2",
        "title": "Epistemic Uncertainty in Conformal Scores: A Unified Approach",
        "link": "https://arxiv.org/abs/2502.06995",
        "author": "Luben M. C. Cabezas, Vagner S. Santos, Thiago R. Ramos, Rafael Izbicki",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06995v2 Announce Type: replace-cross \nAbstract: Conformal prediction methods create prediction bands with distribution-free guarantees but do not explicitly capture epistemic uncertainty, which can lead to overconfident predictions in data-sparse regions. Although recent conformal scores have been developed to address this limitation, they are typically designed for specific tasks, such as regression or quantile regression. Moreover, they rely on particular modeling choices for epistemic uncertainty, restricting their applicability. We introduce $\\texttt{EPICSCORE}$, a model-agnostic approach that enhances any conformal score by explicitly integrating epistemic uncertainty. Leveraging Bayesian techniques such as Gaussian Processes, Monte Carlo Dropout, or Bayesian Additive Regression Trees, $\\texttt{EPICSCORE}$ adaptively expands predictive intervals in regions with limited data while maintaining compact intervals where data is abundant. As with any conformal method, it preserves finite-sample marginal coverage. Additionally, it also achieves asymptotic conditional coverage. Experiments demonstrate its good performance compared to existing methods. Designed for compatibility with any Bayesian model, but equipped with distribution-free guarantees, $\\texttt{EPICSCORE}$ provides a general-purpose framework for uncertainty quantification in prediction problems."
      },
      {
        "id": "oai:arXiv.org:2502.07202v3",
        "title": "Monte Carlo Tree Diffusion for System 2 Planning",
        "link": "https://arxiv.org/abs/2502.07202",
        "author": "Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, Sungjin Ahn",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07202v3 Announce Type: replace-cross \nAbstract: Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with inference-time computation scaling-standard diffusion-based planners offer only limited avenues for the scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as inference-time computation increases."
      },
      {
        "id": "oai:arXiv.org:2502.09395v3",
        "title": "Robot Pouring: Identifying Causes of Spillage and Selecting Alternative Action Parameters Using Probabilistic Actual Causation",
        "link": "https://arxiv.org/abs/2502.09395",
        "author": "Jaime Maldonado, Jonas Krumme, Christoph Zetzsche, Vanessa Didelez, Kerstin Schill",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09395v3 Announce Type: replace-cross \nAbstract: In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a large variety of objects and goals. When confronted with an unexpected or unwanted outcome, we take corrective actions and try again until achieving the desired result. The reasoning performed to identify a cause of the observed outcome and to select an appropriate corrective action is a crucial aspect of human reasoning for successful task execution. Central to this reasoning is the assumption that a factor is responsible for producing the observed outcome. In this paper, we investigate the use of probabilistic actual causation to determine whether a factor is the cause of an observed undesired outcome. Furthermore, we show how the actual causation probabilities can be used to find alternative actions to change the outcome. We apply the probabilistic actual causation analysis to a robot pouring task. When spillage occurs, the analysis indicates whether a task parameter is the cause and how it should be changed to avoid spillage. The analysis requires a causal graph of the task and the corresponding conditional probability distributions. To fulfill these requirements, we perform a complete causal modeling procedure (i.e., task analysis, definition of variables, determination of the causal graph structure, and estimation of conditional probability distributions) using data from a realistic simulation of the robot pouring task, covering a large combinatorial space of task parameters. Based on the results, we discuss the implications of the variables' representation and how the alternative actions suggested by the actual causation analysis would compare to the alternative solutions proposed by a human observer. The practical use of the analysis of probabilistic actual causation to select alternative action parameters is demonstrated."
      },
      {
        "id": "oai:arXiv.org:2502.13961v2",
        "title": "The Computational Advantage of Depth: Learning High-Dimensional Hierarchical Functions with Gradient Descent",
        "link": "https://arxiv.org/abs/2502.13961",
        "author": "Yatin Dandi, Luca Pesce, Lenka Zdeborov\\'a, Florent Krzakala",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.13961v2 Announce Type: replace-cross \nAbstract: Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge. In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD successively reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms."
      },
      {
        "id": "oai:arXiv.org:2502.14760v2",
        "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations",
        "link": "https://arxiv.org/abs/2502.14760",
        "author": "Haotian Zhai, Connor Lawless, Ellen Vitercik, Liu Leqi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14760v2 Announce Type: replace-cross \nAbstract: A fundamental problem in combinatorial optimization is identifying equivalent formulations. Despite the growing need for automated equivalence checks -- driven, for example, by optimization copilots, which generate problem formulations from natural language descriptions -- current approaches rely on simple heuristics that fail to reliably check formulation equivalence. Inspired by Karp reductions, in this work we introduce Quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings for scalable, reliable equivalence checking, with a verification stage that ensures mapped solutions preserve feasibility and optimality without additional solver calls. To evaluate our approach, we construct EquivaFormulation, the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence."
      },
      {
        "id": "oai:arXiv.org:2503.00493v4",
        "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
        "link": "https://arxiv.org/abs/2503.00493",
        "author": "Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00493v4 Announce Type: replace-cross \nAbstract: Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area."
      },
      {
        "id": "oai:arXiv.org:2503.02885v2",
        "title": "\"Would You Want an AI Tutor?\" Understanding Stakeholder Perceptions of LLM-based Systems in the Classroom",
        "link": "https://arxiv.org/abs/2503.02885",
        "author": "Caterina Fuligni, Daniel Dominguez Figaredo, Julia Stoyanovich",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02885v2 Announce Type: replace-cross \nAbstract: In recent years, Large Language Models (LLMs) rapidly gained popularity across all parts of society, including education. After initial skepticism and bans, many schools have chosen to embrace this new technology by integrating it into their curricula in the form of virtual tutors and teaching assistants. However, neither the companies developing this technology nor the public institutions involved in its implementation have set up a formal system to collect feedback from the stakeholders impacted by them. In this paper, we argue that understanding the perceptions of those directly or indirectly impacted by LLMs in the classroom, including parents and school staff, is essential for ensuring responsible use of AI in this critical domain.\n  Our contributions are two-fold. First, we propose the Contextualized Perceptions for the Adoption of LLMs in Education (Co-PALE) framework, which can be used to systematically elicit perceptions and inform whether and how LLM-based tools should be designed, developed, and deployed in the classroom. Second, we explain how our framework can be used to ground specific rubrics for eliciting perceptions of the relevant stakeholders in view of specific goals and context of implementation. Overall, Co-PALE is a practical step toward helping educational agents, policymakers, researchers, and technologists ensure the responsible and effective deployment of LLM-based systems across diverse learning contexts."
      },
      {
        "id": "oai:arXiv.org:2503.04280v4",
        "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models",
        "link": "https://arxiv.org/abs/2503.04280",
        "author": "Niccol\\`o Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04280v4 Announce Type: replace-cross \nAbstract: Recent advancements in Large Language Models (LLMs) and Visual Language Models (VLMs) have significantly impacted robotics, enabling high-level semantic motion planning applications. Reinforcement Learning (RL), a complementary paradigm, enables agents to autonomously optimize complex behaviors through interaction and reward signals. However, designing effective reward functions for RL remains challenging, especially in real-world tasks where sparse rewards are insufficient and dense rewards require elaborate design. In this work, we propose Autonomous Reinforcement learning for Complex Human-Informed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4, a pre-trained LLM, to generate reward functions directly from natural language task descriptions. The rewards are used to train RL agents in simulated environments, where we formalize the reward generation process to enhance feasibility. Additionally, GPT-4 automates the coding of task success criteria, creating a fully automated, one-shot procedure for translating human-readable text into deployable robot skills. Our approach is validated through extensive simulated experiments on single-arm and bi-manual manipulation tasks using an ABB YuMi collaborative robot, highlighting its practicality and effectiveness. Tasks are demonstrated on the real robot setup."
      },
      {
        "id": "oai:arXiv.org:2503.08071v2",
        "title": "GigaSLAM: Large-Scale Monocular SLAM with Hierarchical Gaussian Splats",
        "link": "https://arxiv.org/abs/2503.08071",
        "author": "Kai Deng, Yigong Zhang, Jian Yang, Jin Xie",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08071v2 Announce Type: replace-cross \nAbstract: Tracking and mapping in large-scale, unbounded outdoor environments using only monocular RGB input presents substantial challenges for existing SLAM systems. Traditional Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) SLAM methods are typically limited to small, bounded indoor settings. To overcome these challenges, we introduce GigaSLAM, the first RGB NeRF / 3DGS-based SLAM framework for kilometer-scale outdoor environments, as demonstrated on the KITTI, KITTI 360, 4 Seasons and A2D2 datasets. Our approach employs a hierarchical sparse voxel map representation, where Gaussians are decoded by neural networks at multiple levels of detail. This design enables efficient, scalable mapping and high-fidelity viewpoint rendering across expansive, unbounded scenes. For front-end tracking, GigaSLAM utilizes a metric depth model combined with epipolar geometry and PnP algorithms to accurately estimate poses, while incorporating a Bag-of-Words-based loop closure mechanism to maintain robust alignment over long trajectories. Consequently, GigaSLAM delivers high-precision tracking and visually faithful rendering on urban outdoor benchmarks, establishing a robust SLAM solution for large-scale, long-term scenarios, and significantly extending the applicability of Gaussian Splatting SLAM systems to unbounded outdoor environments. GitHub: https://github.com/DengKaiCQ/GigaSLAM."
      },
      {
        "id": "oai:arXiv.org:2503.13352v2",
        "title": "Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate Tool for Ligand Strain Calculations",
        "link": "https://arxiv.org/abs/2503.13352",
        "author": "Ewan R. S. Wallace, Nathan C. Frey, Joshua A. Rackers",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13352v2 Announce Type: replace-cross \nAbstract: Ligand strain energy, the energy difference between the bound and unbound conformations of a ligand, is an important component of structure-based small molecule drug design. A large majority of observed ligands in protein-small molecule co-crystal structures bind in low-strain conformations, making strain energy a useful filter for structure-based drug design. In this work we present a tool for calculating ligand strain with a high accuracy. StrainRelief uses a MACE Neural Network Potential (NNP), trained on a large database of Density Functional Theory (DFT) calculations to estimate ligand strain of neutral molecules with quantum accuracy. We show that this tool estimates strain energy differences relative to DFT to within 1.4 kcal/mol, more accurately than alternative NNPs. These results highlight the utility of NNPs in drug discovery, and provide a useful tool for drug discovery teams."
      },
      {
        "id": "oai:arXiv.org:2503.16586v2",
        "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants",
        "link": "https://arxiv.org/abs/2503.16586",
        "author": "Yash Vekaria (UC Davis), Aurelio Loris Canino (Mediterranea University of Reggio Calabria), Jonathan Levitsky (UC Davis), Alex Ciechonski (University College London), Patricia Callejo (Universidad Carlos III de Madrid), Anna Maria Mandalari (University College London), Zubair Shafiq (UC Davis)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16586v2 Announce Type: replace-cross \nAbstract: Generative AI (GenAI) browser assistants integrate powerful capabilities of GenAI in web browsers to provide rich experiences such as question answering, content summarization, and agentic navigation. These assistants, available today as browser extensions, can not only track detailed browsing activity such as search and click data, but can also autonomously perform tasks such as filling forms, raising significant privacy concerns. It is crucial to understand the design and operation of GenAI browser extensions, including how they collect, store, process, and share user data. To this end, we study their ability to profile users and personalize their responses based on explicit or inferred demographic attributes and interests of users. We perform network traffic analysis and use a novel prompting framework to audit tracking, profiling, and personalization by the ten most popular GenAI browser assistant extensions. We find that instead of relying on local in-browser models, these assistants largely depend on server-side APIs, which can be auto-invoked without explicit user interaction. When invoked, they collect and share webpage content, often the full HTML DOM and sometimes even the user's form inputs, with their first-party servers. Some assistants also share identifiers and user prompts with third-party trackers such as Google Analytics. The collection and sharing continues even if a webpage contains sensitive information such as health or personal information such as name or SSN entered in a web form. We find that several GenAI browser assistants infer demographic attributes such as age, gender, income, and interests and use this profile--which carries across browsing contexts--to personalize responses. In summary, our work shows that GenAI browser assistants can and do collect personal and sensitive information for profiling and personalization with little to no safeguards."
      },
      {
        "id": "oai:arXiv.org:2504.02473v2",
        "title": "Adaptive path planning for efficient object search by UAVs in agricultural fields",
        "link": "https://arxiv.org/abs/2504.02473",
        "author": "Rick van Essen, Eldert van Henten, Lammert Kooistra, Gert Kootstra",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02473v2 Announce Type: replace-cross \nAbstract: This paper presents an adaptive path planner for object search in agricultural fields using UAVs. The path planner uses a high-altitude coverage flight path and plans additional low-altitude inspections when the detection network is uncertain. The path planner was evaluated in an offline simulation environment containing real-world images. We trained a YOLOv8 detection network to detect artificial plants placed in grass fields to showcase the potential of our path planner. We evaluated the effect of different detection certainty measures, optimized the path planning parameters, investigated the effects of localization errors, and different numbers of objects in the field. The YOLOv8 detection confidence worked best to differentiate between true and false positive detections and was therefore used in the adaptive planner. The optimal parameters of the path planner depended on the distribution of objects in the field. When the objects were uniformly distributed, more low-altitude inspections were needed compared to a non-uniform distribution of objects, resulting in a longer path length. The adaptive planner proved to be robust against localization uncertainty. When increasing the number of objects, the flight path length increased, especially when the objects were uniformly distributed. When the objects were non-uniformly distributed, the adaptive path planner yielded a shorter path than a low-altitude coverage path, even with a high number of objects. Overall, the presented adaptive path planner allowed finding non-uniformly distributed objects in a field faster than a coverage path planner and resulted in a compatible detection accuracy. The path planner is made available at https://github.com/wur-abe/uav_adaptive_planner."
      },
      {
        "id": "oai:arXiv.org:2504.07996v2",
        "title": "Fusing Global and Local: Transformer-CNN Synergy for Next-Gen Current Estimation",
        "link": "https://arxiv.org/abs/2504.07996",
        "author": "Junlang Huang, Hao Chen, Li Luo, Yong Cai, Lexin Zhang, Tianhao Ma, Yitian Zhang, Zhong Guan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07996v2 Announce Type: replace-cross \nAbstract: This paper presents a hybrid model combining Transformer and CNN for predicting the current waveform in signal lines. Unlike traditional approaches such as current source models, driver linear representations, waveform functional fitting, or equivalent load capacitance methods, our model does not rely on fixed simplified models of standard-cell drivers or RC loads. Instead, it replaces the complex Newton iteration process used in traditional SPICE simulations, leveraging the powerful sequence modeling capabilities of the Transformer framework to directly predict current responses without iterative solving steps. The hybrid architecture effectively integrates the global feature-capturing ability of Transformers with the local feature extraction advantages of CNNs, significantly improving the accuracy of current waveform predictions. Experimental results demonstrate that, compared to traditional SPICE simulations, the proposed algorithm achieves an error of only 0.0098. These results highlight the algorithm's superior capabilities in predicting signal line current waveforms, timing analysis, and power evaluation, making it suitable for a wide range of technology nodes, from 40nm to 3nm."
      },
      {
        "id": "oai:arXiv.org:2504.17656v2",
        "title": "polyGen: A Learning Framework for Atomic-level Polymer Structure Generation",
        "link": "https://arxiv.org/abs/2504.17656",
        "author": "Ayush Jain, Rampi Ramprasad",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.17656v2 Announce Type: replace-cross \nAbstract: Synthetic polymeric materials underpin fundamental technologies in the energy, electronics, consumer goods, and medical sectors, yet their development still suffers from prolonged design timelines. Although polymer informatics tools have supported speedup, polymer simulation protocols continue to face significant challenges in the on-demand generation of realistic 3D atomic structures that respect the conformational diversity of polymers. Generative algorithms for 3D structures of inorganic crystals, bio-polymers, and small molecules exist, but have not addressed synthetic polymers because of challenges in representation and dataset constraints. In this work, we introduce polyGen, the first generative model designed specifically for polymer structures from minimal inputs such as the repeat unit chemistry alone. polyGen combines graph-based encodings with a latent diffusion transformer using positional biased attention for realistic conformation generation. Given the limited dataset of 3,855 DFT-optimized polymer structures, we incorporate joint training with small molecule data to enhance generation quality. We also establish structure matching criteria to benchmark our approach on this novel problem. polyGen overcomes the limitations of traditional crystal structure prediction methods for polymers, successfully generating realistic and diverse linear and branched conformations, with promising performance even on challenging large repeat units. As the first atomic-level proof-of-concept capturing intrinsic polymer flexibility, it marks a new capability in material structure generation."
      },
      {
        "id": "oai:arXiv.org:2505.04255v2",
        "title": "Model-based learning for joint channel estimationand hybrid MIMO precoding",
        "link": "https://arxiv.org/abs/2505.04255",
        "author": "Nay Klaimi (IETR, INSA Rennes), Amira Bedoui (IETR, INSA Rennes), Cl\\'ement Elvira (IETR), Philippe Mary (INSA Rennes, IETR), Luc Le Magoarou (INSA Rennes, IETR)",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.04255v2 Announce Type: replace-cross \nAbstract: Hybrid precoding is a key ingredient of cost-effective massive multiple-input multiple-output transceivers. However, setting jointly digital and analog precoders to optimally serve multiple users is a difficult optimization problem. Moreover, it relies heavily on precise knowledge of the channels, which is difficult to obtain, especially when considering realistic systems comprising hardware impairments. In this paper, a joint channel estimation and hybrid precoding method is proposed, which consists in an end-to-end architecture taking received pilots as inputs and outputting pre-coders. The resulting neural network is fully model-based, making it lightweight and interpretable with very few learnable parameters. The channel estimation step is performed using the unfolded matching pursuit algorithm, accounting for imperfect knowledge of the antenna system, while the precoding step is done via unfolded projected gradient ascent. The great potential of the proposed method is empirically demonstrated on realistic synthetic channels."
      },
      {
        "id": "oai:arXiv.org:2505.08698v2",
        "title": "Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data",
        "link": "https://arxiv.org/abs/2505.08698",
        "author": "Antonio \\'Alvarez-L\\'opez, Marcos Matabuena",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.08698v2 Announce Type: replace-cross \nAbstract: Modeling the dynamics of probability distributions from time-dependent data samples is a fundamental problem in many fields, including digital health. The goal is to analyze how the distribution of a biomarker, such as glucose, changes over time and how these changes may reflect the progression of chronic diseases like diabetes. We introduce a probabilistic model based on a Gaussian mixture that captures the evolution of a continuous-time stochastic process. Our approach combines a non-parametric estimate of the distribution, obtained with Maximum Mean Discrepancy (MMD), and a Neural Ordinary Differential Equation (Neural ODE) that governs the temporal evolution of the mixture weights. The model is highly interpretable, detects subtle distribution shifts, and remains computationally efficient. Simulation studies show that our method matches or surpasses the estimation accuracy of state-of-the-art, less interpretable techniques such as normalizing flows and non-parametric kernel density estimators. We further demonstrate its utility using data from a digital clinical trial, revealing how interventions affect the time-dependent distribution of glucose levels. The proposed method enables rigorous comparisons between control and treatment groups from both mathematical and clinical perspectives, offering novel longitudinal characterizations that existing approaches cannot achieve."
      },
      {
        "id": "oai:arXiv.org:2505.14479v2",
        "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach",
        "link": "https://arxiv.org/abs/2505.14479",
        "author": "Oren Sultan, Eitan Stern, Dafna Shahaf",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.14479v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness."
      },
      {
        "id": "oai:arXiv.org:2505.17683v2",
        "title": "Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection",
        "link": "https://arxiv.org/abs/2505.17683",
        "author": "Dan Yuan, Yi Feng, Ziyun Tang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17683v2 Announce Type: replace-cross \nAbstract: Intraventricular hemorrhage (IVH) is a severe neurological complication among premature infants, necessitating early and accurate detection from brain ultrasound (US) images to improve clinical outcomes. While recent deep learning methods offer promise for computer-aided diagnosis, challenges remain in capturing both local spatial details and global contextual dependencies critical for segmenting brain anatomies. In this work, we propose an enhanced Residual U-Net architecture incorporating two complementary attention mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse Attention Layer (SAL). The CBAM improves the model's ability to refine spatial and channel-wise features, while the SAL introduces a dual-branch design, sparse attention filters out low-confidence query-key pairs to suppress noise, and dense attention ensures comprehensive information propagation. Extensive experiments on the Brain US dataset demonstrate that our method achieves state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU of 81.84% for ventricle region segmentation. These results highlight the effectiveness of integrating spatial refinement and attention sparsity for robust brain anatomy detection. Code is available at: https://github.com/DanYuan001/BrainImgSegment."
      },
      {
        "id": "oai:arXiv.org:2505.17836v4",
        "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means",
        "link": "https://arxiv.org/abs/2505.17836",
        "author": "Anna Van Elst, Igor Colin, Stephan Cl\\'emen\\c{c}on",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17836v4 Announce Type: replace-cross \nAbstract: This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank estimation and an $\\mathcal{O}((\\log t)/\\sqrt{t})$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \\textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes."
      },
      {
        "id": "oai:arXiv.org:2505.18789v2",
        "title": "From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?",
        "link": "https://arxiv.org/abs/2505.18789",
        "author": "Wasi Uddin Ahmad, Somshubra Majumdar, Boris Ginsburg",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.18789v2 Announce Type: replace-cross \nAbstract: Post-processing is crucial for the automatic evaluation of LLMs in fill-in-the-middle (FIM) code generation due to the frequent presence of extraneous code in raw outputs. This extraneous generation suggests a lack of awareness regarding output boundaries, requiring truncation for effective evaluation. The determination of an optimal truncation strategy, however, often proves intricate, particularly when the scope includes several programming languages. This study investigates the necessity of post-processing instruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning significantly enhances FIM code generation, enabling LLMs to generate code that seamlessly integrates with the surrounding context. Evaluating our fine-tuned \\texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and SAFIM benchmarks demonstrates improved performances without post-processing, especially when the \\emph{middle} consist of complete lines. However, post-processing of the LLM outputs remains necessary when the \\emph{middle} is a random span of code."
      },
      {
        "id": "oai:arXiv.org:2505.19356v2",
        "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval",
        "link": "https://arxiv.org/abs/2505.19356",
        "author": "Kidist Amde Mekonnen, Yosef Worku Alemneh, Maarten de Rijke",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19356v2 Announce Type: replace-cross \nAbstract: Neural retrieval methods using transformer-based pre-trained language models have advanced multilingual and cross-lingual retrieval. However, their effectiveness for low-resource, morphologically rich languages such as Amharic remains underexplored due to data scarcity and suboptimal tokenization. We address this gap by introducing Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones. Our proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while being over 13x smaller. Additionally, we train a ColBERT-based late interaction retrieval model that achieves the highest MRR@10 score (0.843) among all evaluated models. We benchmark our proposed models against both sparse and dense retrieval baselines to systematically assess retrieval effectiveness in Amharic. Our analysis highlights key challenges in low-resource settings and underscores the importance of language-specific adaptation. To foster future research in low-resource IR, we publicly release our dataset, codebase, and trained models at https://github.com/kidist-amde/amharic-ir-benchmarks."
      },
      {
        "id": "oai:arXiv.org:2505.20011v3",
        "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments",
        "link": "https://arxiv.org/abs/2505.20011",
        "author": "Maciej Swiechowski, Dominik Slezak",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.20011v3 Announce Type: replace-cross \nAbstract: Human-like agents are an increasingly important topic in games and beyond. Believable non-player characters enhance the gaming experience by improving immersion and providing entertainment. They also offer players the opportunity to engage with AI entities that can function as opponents, teachers, or cooperating partners. Additionally, in games where bots are prohibited -- and even more so in non-game environments -- there is a need for methods capable of identifying whether digital interactions occur with bots or humans. This leads to two fundamental research questions: (1) how to model and implement human-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most significant challenges in implementing human-like AI in games (or any virtual environment featuring simulated agents, although this article specifically focuses on games). Thirteen such challenges, both conceptual and technical, are discussed in detail. The second is an empirical study performed in a tactical video game that addresses the research question: \"Is it possible to distinguish human players from bots (AI agents) based on empirical data?\" A machine-learning approach using a custom deep recurrent convolutional neural network is presented. We hypothesize that the more challenging it is to create human-like AI for a given game, the easier it becomes to develop a method for distinguishing humans from AI-driven players."
      },
      {
        "id": "oai:arXiv.org:2505.23916v2",
        "title": "Estimation of Head Motion in Structural MRI and its Impact on Cortical Thickness Measurements in Retrospective Data",
        "link": "https://arxiv.org/abs/2505.23916",
        "author": "Charles Bricout, Samira Ebrahimi Kahou, Sylvain Bouix",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23916v2 Announce Type: replace-cross \nAbstract: Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI) and can bias automated neuroanatomical metrics such as cortical thickness. These biases can interfere with statistical analysis which is a major concern as motion has been shown to be more prominent in certain populations such as children or individuals with ADHD. Manual review cannot objectively quantify motion in anatomical scans, and existing quantitative automated approaches often require specialized hardware or custom acquisition protocols. Here, we train a 3D convolutional neural network to estimate a summary motion metric in retrospective routine research scans by leveraging a large training dataset of synthetically motion-corrupted volumes. We validate our method with one held-out site from our training cohort and with 14 fully independent datasets, including one with manual ratings, achieving a representative $R^2 = 0.65$ versus manual labels and significant thickness-motion correlations in 12/15 datasets. Furthermore, our predicted motion correlates with subject age in line with prior studies. Our approach generalizes across scanner brands and protocols, enabling objective, scalable motion assessment in structural MRI studies without prospective motion correction. By providing reliable motion estimates, our method offers researchers a tool to assess and account for potential biases in cortical thickness analyses."
      },
      {
        "id": "oai:arXiv.org:2506.03834v2",
        "title": "Enhancing Safety of Foundation Models for Visual Navigation through Collision Avoidance via Repulsive Estimation",
        "link": "https://arxiv.org/abs/2506.03834",
        "author": "Joonkyung Kim, Joonyeol Sim, Woojun Kim, Katia Sycara, Changjoo Nam",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03834v2 Announce Type: replace-cross \nAbstract: We propose CARE (Collision Avoidance via Repulsive Estimation), a plug-and-play module that enhances the safety of vision-based navigation without requiring additional range sensors or fine-tuning of pretrained models. While recent foundation models using only RGB inputs have shown strong performance, they often fail to generalize in out-of-distribution (OOD) environments with unseen objects or variations in camera parameters (e.g., field of view, pose, or focal length). Without fine-tuning, these models may generate unsafe trajectories that lead to collisions, requiring costly data collection and retraining. CARE addresses this limitation by seamlessly integrating with any RGB-based navigation system that outputs local trajectories, dynamically adjusting them using repulsive force vectors derived from monocular depth maps. We evaluate CARE by combining it with state-of-the-art vision-based navigation models across multiple robot platforms. CARE consistently reduces collision rates (up to 100%) without sacrificing goal-reaching performance and improves collision-free travel distance by up to 10.7x in exploration tasks."
      },
      {
        "id": "oai:arXiv.org:2506.04354v3",
        "title": "BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations",
        "link": "https://arxiv.org/abs/2506.04354",
        "author": "Elmira Mirzabeigi, Rezvan Salehi, Kourosh Parand",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.04354v3 Announce Type: replace-cross \nAbstract: BridgeNet is a novel hybrid framework that integrates convolutional neural networks with physics-informed neural networks to efficiently solve non-linear, high-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which typically rely on fully connected architectures, often struggle to capture complex spatial hierarchies and enforce intricate boundary conditions. In contrast, BridgeNet leverages adaptive CNN layers for effective local feature extraction and incorporates a dynamically weighted loss function that rigorously enforces physical constraints. Extensive numerical experiments across various test cases demonstrate that BridgeNet not only achieves significantly lower error metrics and faster convergence compared to conventional PINN approaches but also maintains robust stability in high-dimensional settings. This work represents a substantial advancement in computational physics, offering a scalable and accurate solution methodology with promising applications in fields ranging from financial mathematics to complex system dynamics."
      },
      {
        "id": "oai:arXiv.org:2506.04734v2",
        "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design",
        "link": "https://arxiv.org/abs/2506.04734",
        "author": "Lin Sun, Weihong Lin, Jinzhu Wu, Yongfu Zhu, Xiaoqi Jian, Guangxiang Zhao, Change Jia, Linglin Zhang, Sai-er Hu, Yuhan Wu, Xiangzheng Zhang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.04734v2 Announce Type: replace-cross \nAbstract: Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models."
      },
      {
        "id": "oai:arXiv.org:2506.05633v2",
        "title": "Noninvasive precision modulation of high-level neural population activity via natural vision perturbations",
        "link": "https://arxiv.org/abs/2506.05633",
        "author": "Guy Gaziv, Sarah Goulding, Ani Ayvazian-Hancock, Yoon Bai, James J. DiCarlo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05633v2 Announce Type: replace-cross \nAbstract: Precise control of neural activity -- modulating target neurons deep in the brain while leaving nearby neurons unaffected -- is an outstanding challenge in neuroscience, generally approached using invasive techniques. This study investigates the possibility of precisely and noninvasively modulating neural activity in the high-level primate ventral visual stream via perturbations on one's natural visual feed. When tested on macaque inferior temporal (IT) neural populations, we found quantitative agreement between the model-predicted and biologically realized effect: strong modulation concentrated on targeted neural sites. We extended this to demonstrate accurate injection of experimenter-chosen neural population patterns via subtle perturbations applied on the background of typical natural visual feeds. These results highlight that current machine-executable models of the ventral stream can now design noninvasive, visually-delivered, possibly imperceptible neural interventions at the resolution of individual neurons."
      },
      {
        "id": "oai:arXiv.org:2506.05688v2",
        "title": "Voice Impression Control in Zero-Shot TTS",
        "link": "https://arxiv.org/abs/2506.05688",
        "author": "Keinichi Fujita, Shota Horiguchi, Yusuke Ijima",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05688v2 Announce Type: replace-cross \nAbstract: Para-/non-linguistic information in speech is pivotal in shaping the listeners' impression. Although zero-shot text-to-speech (TTS) has achieved high speaker fidelity, modulating subtle para-/non-linguistic information to control perceived voice characteristics, i.e., impressions, remains challenging. We have therefore developed a voice impression control method in zero-shot TTS that utilizes a low-dimensional vector to represent the intensities of various voice impression pairs (e.g., dark-bright). The results of both objective and subjective evaluations have demonstrated our method's effectiveness in impression control. Furthermore, generating this vector via a large language model enables target-impression generation from a natural language description of the desired impression, thus eliminating the need for manual optimization. Audio examples are available on our demo page (https://ntt-hilab-gensp.github.io/is2025voiceimpression/)."
      },
      {
        "id": "oai:arXiv.org:2506.06072v2",
        "title": "BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning",
        "link": "https://arxiv.org/abs/2506.06072",
        "author": "Hongyi Zhou, Weiran Liao, Xi Huang, Yucheng Tang, Fabian Otto, Xiaogang Jia, Xinkai Jiang, Simon Hilber, Ge Li, Qian Wang, \\\"Omer Erdin\\c{c} Ya\\u{g}murlu, Nils Blank, Moritz Reuss, Rudolf Lioutikov",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06072v2 Announce Type: replace-cross \nAbstract: We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel action tokenizer that encodes action sequences into compact discrete or continuous tokens using B-splines. In contrast to existing action tokenizers based on vector quantization or byte pair encoding, BEAST requires no separate tokenizer training and consistently produces tokens of uniform length, enabling fast action sequence generation via parallel decoding. Leveraging our B-spline formulation, BEAST inherently ensures generating smooth trajectories without discontinuities between adjacent segments. We extensively evaluate BEAST by integrating it with three distinct model architectures: a Variational Autoencoder (VAE) with continuous tokens, a decoder-only Transformer with discrete tokens, and Florence-2, a pretrained Vision-Language Model with an encoder-decoder architecture, demonstrating BEAST's compatibility and scalability with large pretrained models. We evaluate BEAST across three established benchmarks consisting of 166 simulated tasks and on three distinct robot settings with a total of 8 real-world tasks. Experimental results demonstrate that BEAST (i) significantly reduces both training and inference computational costs, and (ii) consistently generates smooth, high-frequency control signals suitable for continuous control tasks while (iii) reliably achieves competitive task success rates compared to state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2506.06221v2",
        "title": "BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly",
        "link": "https://arxiv.org/abs/2506.06221",
        "author": "Yan Shen, Ruihai Wu, Yubin Ke, Xinyuan Song, Zeyi Li, Xiaoqi Li, Hongwei Fan, Haoran Lu, Hao dong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06221v2 Announce Type: replace-cross \nAbstract: Shape assembly, the process of combining parts into a complete whole, is a crucial robotic skill with broad real-world applications. Among various assembly tasks, geometric assembly--where broken parts are reassembled into their original form (e.g., reconstructing a shattered bowl)--is particularly challenging. This requires the robot to recognize geometric cues for grasping, assembly, and subsequent bimanual collaborative manipulation on varied fragments. In this paper, we exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. To address the evaluation ambiguity caused by geometry diversity of broken parts, we introduce a real-world benchmark featuring geometric variety and global reproducibility. Extensive experiments demonstrate the superiority of our approach over both previous affordance-based and imitation-based methods. Project page: https://sites.google.com/view/biassembly/."
      },
      {
        "id": "oai:arXiv.org:2506.06299v2",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "link": "https://arxiv.org/abs/2506.06299",
        "author": "Daniel Thilo Schroeder, Meeyoung Cha, Andrea Baronchelli, Nick Bostrom, Nicholas A. Christakis, David Garcia, Amit Goldenberg, Yara Kyrychenko, Kevin Leyton-Brown, Nina Lutz, Gary Marcus, Filippo Menczer, Gordon Pennycook, David G. Rand, Frank Schweitzer, Christopher Summerfield, Audrey Tang, Jay Van Bavel, Sander van der Linden, Dawn Song, Jonas R. Kunst",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06299v2 Announce Type: replace-cross \nAbstract: Advances in AI portend a new era of sophisticated disinformation operations. While individual AI systems already create convincing -- and at times misleading -- information, an imminent development is the emergence of malicious AI swarms. These systems can coordinate covertly, infiltrate communities, evade traditional detectors, and run continuous A/B tests, with round-the-clock persistence. The result can include fabricated grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression or mobilization, contamination of AI training data, and erosion of institutional trust. With democratic processes worldwide increasingly vulnerable, we urge a three-pronged response: (1) platform-side defenses -- always-on swarm-detection dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency audits, and optional client-side \"AI shields\" for users; (2) model-side safeguards -- standardized persuasion-risk tests, provenance-authenticating passkeys, and watermarking; and (3) system-level oversight -- a UN-backed AI Influence Observatory."
      },
      {
        "id": "oai:arXiv.org:2506.06407v2",
        "title": "TimeWak: Temporal Chained-Hashing Watermark for Time Series Data",
        "link": "https://arxiv.org/abs/2506.06407",
        "author": "Zhi Wen Soi, Chaoyi Zhu, Fouad Abiad, Aditya Shankar, Jeroen M. Galjaard, Huijuan Wang, Lydia Y. Chen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06407v2 Announce Type: replace-cross \nAbstract: Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in real space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in real space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space. The other unique feature is the $\\epsilon$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series and further maintain high watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against 5 datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable."
      },
      {
        "id": "oai:arXiv.org:2506.06905v2",
        "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering",
        "link": "https://arxiv.org/abs/2506.06905",
        "author": "Akash Gupta, Amos Storkey, Mirella Lapata",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06905v2 Announce Type: replace-cross \nAbstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks."
      },
      {
        "id": "oai:arXiv.org:2506.06975v2",
        "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test",
        "link": "https://arxiv.org/abs/2506.06975",
        "author": "Xiaoyuan Zhu, Yaowen Ye, Tianyi Qiu, Hanlin Zhu, Sijun Tan, Ajraf Mannan, Jonathan Michala, Raluca Ada Popa, Willie Neiswanger",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06975v2 Announce Type: replace-cross \nAbstract: As API access becomes a primary interface to large language models (LLMs), users often interact with black-box systems that offer little transparency into the deployed model. To reduce costs or maliciously alter model behaviors, API providers may discreetly serve quantized or fine-tuned variants, which can degrade performance and compromise safety. Detecting such substitutions is difficult, as users lack access to model weights and, in most cases, even output logits. To tackle this problem, we propose a rank-based uniformity test that can verify the behavioral equality of a black-box LLM to a locally deployed authentic model. Our method is accurate, query-efficient, and avoids detectable query patterns, making it robust to adversarial providers that reroute or mix responses upon the detection of testing attempts. We evaluate the approach across diverse threat scenarios, including quantization, harmful fine-tuning, jailbreak prompts, and full model substitution, showing that it consistently achieves superior statistical power over prior methods under constrained query budgets."
      },
      {
        "id": "oai:arXiv.org:2506.07294v2",
        "title": "Towards Generalized Source Tracing for Codec-Based Deepfake Speech",
        "link": "https://arxiv.org/abs/2506.07294",
        "author": "Xuanjun Chen, I-Ming Lin, Lin Zhang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07294v2 Announce Type: replace-cross \nAbstract: Recent attempts at source tracing for codec-based deepfake speech (CodecFake), generated by neural audio codec-based speech generation (CoSG) models, have exhibited suboptimal performance. However, how to train source tracing models using simulated CoSG data while maintaining strong performance on real CoSG-generated audio remains an open challenge. In this paper, we show that models trained solely on codec-resynthesized data tend to overfit to non-speech regions and struggle to generalize to unseen content. To mitigate these challenges, we introduce the Semantic-Acoustic Source Tracing Network (SASTNet), which jointly leverages Whisper for semantic feature encoding and Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet achieves state-of-the-art performance on the CoSG test set of the CodecFake+ dataset, demonstrating its effectiveness for reliable source tracing."
      },
      {
        "id": "oai:arXiv.org:2506.07564v2",
        "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems",
        "link": "https://arxiv.org/abs/2506.07564",
        "author": "Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07564v2 Announce Type: replace-cross \nAbstract: Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy."
      },
      {
        "id": "oai:arXiv.org:2506.07859v2",
        "title": "Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing",
        "link": "https://arxiv.org/abs/2506.07859",
        "author": "Amanuel Anteneh, L\\'eandre Brunel, Carlos Gonz\\'alez-Arciniegas, Olivier Pfister",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07859v2 Announce Type: replace-cross \nAbstract: Cubic-phase states are a sufficient resource for universal quantum computing over continuous variables. We present results from numerical experiments in which deep neural networks are trained via reinforcement learning to control a quantum optical circuit for generating cubic-phase states, with an average success rate of 96%. The only non-Gaussian resource required is photon-number-resolving measurements. We also show that the exact same resources enable the direct generation of a quartic-phase gate, with no need for a cubic gate decomposition."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Wed, 11 Jun 2025 04:02:02 +0000",
      "published": "Wed, 11 Jun 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2506.08346v1",
        "title": "SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models",
        "link": "https://arxiv.org/abs/2506.08346",
        "author": "Wenhan Yao, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang He, Weiping Wen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08346v1 Announce Type: new \nAbstract: Deep speech classification tasks, including keyword spotting and speaker verification, are vital in speech-based human-computer interaction. Recently, the security of these technologies has been revealed to be susceptible to backdoor attacks. Specifically, attackers use noisy disruption triggers and speech element triggers to produce poisoned speech samples that train models to become vulnerable. However, these methods typically create only a limited number of backdoors due to the inherent constraints of the trigger function. In this paper, we propose that speech backdoor attacks can strategically focus on speech elements such as timbre and emotion, leveraging the Speech Large Language Model (SLLM) to generate diverse triggers. Increasing the number of triggers may disproportionately elevate the poisoning rate, resulting in higher attack costs and a lower success rate per trigger. We introduce the Multiple Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this challenge. The proposed attack is called the Speech Prompt Backdoor Attack (SPBA). Building on this foundation, we conducted attack experiments on two speech classification tasks, demonstrating that SPBA shows significant trigger effectiveness and achieves exceptional performance in attack metrics."
      },
      {
        "id": "oai:arXiv.org:2506.08348v1",
        "title": "Pureformer-VC: Non-parallel Voice Conversion with Pure Stylized Transformer Blocks and Triplet Discriminative Training",
        "link": "https://arxiv.org/abs/2506.08348",
        "author": "Wenhan Yao, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang He, Weiping Wen",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08348v1 Announce Type: new \nAbstract: As a foundational technology for intelligent human-computer interaction, voice conversion (VC) seeks to transform speech from any source timbre into any target timbre. Traditional voice conversion methods based on Generative Adversarial Networks (GANs) encounter significant challenges in precisely encoding diverse speech elements and effectively synthesising these elements into natural-sounding converted speech. To overcome these limitations, we introduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer blocks to build a disentangled encoder and employs Zipformer blocks to create a style transfer decoder. We adopt a variational decoupled training approach to isolate speech components using a Variational Autoencoder (VAE), complemented by triplet discriminative training to enhance the speaker's discriminative capabilities. Furthermore, we incorporate the Attention Style Transfer Mechanism (ASTM) with Zipformer's shared weights to improve the style transfer performance in the decoder. We conducted experiments on two multi-speaker datasets. The experimental results demonstrate that the proposed model achieves comparable subjective evaluation scores while significantly enhancing objective metrics compared to existing approaches in many-to-many and many-to-one VC scenarios."
      },
      {
        "id": "oai:arXiv.org:2506.08357v1",
        "title": "MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion",
        "link": "https://arxiv.org/abs/2506.08357",
        "author": "Franck Meyer, Kyunghoon Hur, Edward Choi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08357v1 Announce Type: new \nAbstract: Despite the remarkable progress of deep-learning methods generating a target vital sign waveform from a source vital sign waveform, most existing models are designed exclusively for a specific source-to-target pair. This requires distinct model architectures, optimization procedures, and pre-processing pipelines, resulting in multiple models that hinder usability in clinical settings. To address this limitation, we propose the Multi-Directional Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or arterial blood pressure (ABP) from any single input waveform with a single model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct multi-directional waveform generation on two publicly available datasets. Our framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average across all waveform types, lowering Mean absolute error (MAE) by 8.8% and improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the generated ABP waveforms satisfy the Association for the Advancement of Medical Instrumentation (AAMI) criterion and achieve Grade B on the British Hypertension Society (BHS) standard, outperforming all baselines. By eliminating the need for developing a distinct model for each task, we believe that this work offers a unified framework that can deal with any kind of vital sign waveforms with a single model in healthcare monitoring."
      },
      {
        "id": "oai:arXiv.org:2506.08372v1",
        "title": "Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages",
        "link": "https://arxiv.org/abs/2506.08372",
        "author": "Rishabh Ranjan, Likhith Ayinala, Mayank Vatsa, Richa Singh",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08372v1 Announce Type: new \nAbstract: This paper introduces a novel multimodal framework for hate speech detection in deepfake audio, excelling even in zero-shot scenarios. Unlike previous approaches, our method uses contrastive learning to jointly align audio and text representations across languages. We present the first benchmark dataset with 127,290 paired text and synthesized speech samples in six languages: English and five low-resource Indian languages (Hindi, Bengali, Marathi, Tamil, Telugu). Our model learns a shared semantic embedding space, enabling robust cross-lingual and cross-modal classification. Experiments on two multilingual test sets show our approach outperforms baselines, achieving accuracies of 0.819 and 0.701, and generalizes well to unseen languages. This demonstrates the advantage of combining modalities for hate speech detection in synthetic media, especially in low-resource settings where unimodal models falter. The Dataset is available at https://www.iab-rubric.org/resources."
      },
      {
        "id": "oai:arXiv.org:2506.08457v1",
        "title": "A Review on Score-based Generative Models for Audio Applications",
        "link": "https://arxiv.org/abs/2506.08457",
        "author": "Ge Zhu, Yutong Wen, Zhiyao Duan",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08457v1 Announce Type: new \nAbstract: Diffusion models have emerged as powerful deep generative techniques, producing high-quality and diverse samples in applications in various domains including audio. These models have many different design choices suitable for different applications, however, existing reviews lack in-depth discussions of these design choices. The audio diffusion model literature also lacks principled guidance for the implementation of these design choices and their comparisons for different applications. This survey provides a comprehensive review of diffusion model design with an emphasis on design principles for quality improvement and conditioning for audio applications. We adopt the score modeling perspective as a unifying framework that accommodates various interpretations, including recent approaches like flow matching. We systematically examine the training and sampling procedures of diffusion models, and audio applications through different conditioning mechanisms. To address the lack of audio diffusion model codebases and to promote reproducible research and rapid prototyping, we introduce an open-source codebase at https://github.com/gzhu06/AudioDiffuser that implements our reviewed framework for various audio applications. We demonstrate its capabilities through three case studies: audio generation, speech enhancement, and text-to-speech synthesis, with benchmark evaluations on standard datasets."
      },
      {
        "id": "oai:arXiv.org:2506.08471v1",
        "title": "Passive acoustic non-line-of-sight localization without a relay surface",
        "link": "https://arxiv.org/abs/2506.08471",
        "author": "Tal I. Sommer, Ori Katz",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08471v1 Announce Type: new \nAbstract: The detection and localization of a source hidden outside the Line-of-Sight (LOS) traditionally rely on the acquisition of indirect signals, such as those reflected from visible relay surfaces such as floors or walls. These reflected signals are then utilized to reconstruct the obscured scene. In this study, we present an approach that utilize signals diffracted from an edge of an obstacle to achieve three-dimensional (3D) localization of an acoustic point source situated outside the LOS. We address two scenarios - a doorway and a convex corner - and propose a localization method for each of them. For the first scenario, we utilize the two edges of the door as virtual detector arrays. For the second scenario, we exploit the spectral signature of a knife-edge diffraction, inspired by the human perception of sound location by the head-related transfer function (HRTF). In both methods, knife-edge diffraction is utilized to extend the capabilities of non-line-of-sight (NLOS) acoustic sensing, enabling localization in environments where conventional relay-surface based approaches may be limited."
      },
      {
        "id": "oai:arXiv.org:2506.08524v1",
        "title": "Teaching Physical Awareness to LLMs through Sounds",
        "link": "https://arxiv.org/abs/2506.08524",
        "author": "Weiguo Wang, Andy Nie, Wenrui Zhou, Yi Kai, Chengchen Hu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08524v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, yet they fundamentally lack physical awareness--understanding of real-world physical phenomena. In this work, we present ACORN, a framework that teaches LLMs physical awareness through sound, focusing on fundamental physical phenomena like the Doppler effect, multipath effect, and spatial relationships. To overcome data scarcity, ACORN introduce a physics-based simulator combining real-world sound sources with controlled physical channels to generate diverse training data. Using this simulator, we build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an audio encoder that processes both magnitude and phase information. By connecting our audio encoder to state-of-the-art LLMs, we demonstrate reasonable results in both simulated and real-world tasks, such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, paving the way for enabling LLMs to understand physical world."
      },
      {
        "id": "oai:arXiv.org:2506.08540v1",
        "title": "Higher-Order Network Representation of J. S. Bach's Solo Violin Sonatas and Partitas: Topological and Geometrical Explorations",
        "link": "https://arxiv.org/abs/2506.08540",
        "author": "Dima Mrad, Sara Najem",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08540v1 Announce Type: new \nAbstract: Music is inherently complex, with structures and interactions that unfold across multiple layers. Complex networks have emerged as powerful structures for the quantitative analysis of Western classical music, revealing significant features of its harmonic and structural organization. Although notable works have used these approaches to study music, dyadic representations of interactions fall short in conveying the underlying complexity and depth. In recent years, the limitations of traditional graph representations have been questioned and challenged in the context of interactions that could be higher-dimensional. Effective musical analysis requires models that capture higher-order interactions and a framework that simultaneously captures transitions between them. Subsequently, in this paper, we present a topological framework for analyzing J. S. Bach's Solo Violin Sonatas and Partitas that uses higher-order networks where single notes are vertices, two-note chords are edges, three-notes are triangles, etc. We subsequently account for the flow of music, by modeling transitions between successive notes. We identify genre-specific patterns in the works' geometric and topological properties. In particular, we find signatures in the trends of the evolution of the Euler characteristic and curvature, as well as examining adherence to the Gauss-Bonnet theorem across different movement types. The distinctions are revealed between slow movements, Fugues, and Baroque dance movements through their simplicial complex representation."
      },
      {
        "id": "oai:arXiv.org:2506.08570v1",
        "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation",
        "link": "https://arxiv.org/abs/2506.08570",
        "author": "Or Tal, Felix Kreuk, Yossi Adi",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08570v1 Announce Type: new \nAbstract: Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly across many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and pinpoint which design choices most influence performance. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM"
      },
      {
        "id": "oai:arXiv.org:2506.08633v1",
        "title": "Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs",
        "link": "https://arxiv.org/abs/2506.08633",
        "author": "\\v{S}imon Sedl\\'a\\v{c}ek, Bolaji Yusuf, J\\'an \\v{S}vec, Pradyoth Hegde, Santosh Kesiraju, Old\\v{r}ich Plchot, Jan \\v{C}ernock\\'y",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08633v1 Announce Type: new \nAbstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging the representation spaces of speech encoders and LLMs via a small connector module, with a focus on fully open-sourced and open-data components (WavLM-large, OLMo). We focus on ablating different aspects of such systems including full/LoRA adapter fine-tuning, the effect of agent turns in the dialogue history, as well as fuzzy matching-based output post-processing, which greatly improves performance of our systems on named entities in the dialogue slot values. We conduct our experiments on the SpokenWOZ dataset, and additionally utilize the Speech-Aware MultiWOZ dataset to augment our training data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17% JGA on SpokenWOZ test."
      },
      {
        "id": "oai:arXiv.org:2506.08967v1",
        "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model",
        "link": "https://arxiv.org/abs/2506.08967",
        "author": "Ailin Huang, Bingxin Li, Bruce Wang, Boyong Wu, Chao Yan, Chengli Feng, Heng Wang, Hongyu Zhou, Hongyuan Wang, Jingbei Li, Jianjian Sun, Joanna Wang, Mingrui Chen, Peng Liu, Ruihang Miao, Shilei Jiang, Tian Fei, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Ge, Zheng Gong, Zhewei Huang, Zixin Zhang, Bin Wang, Bo Li, Buyun Ma, Changxin Miao, Changyi Wan, Chen Xu, Dapeng Shi, Dingyuan Hu, Enle Liu, Guanzhe Huang, Gulin Yan, Hanpeng Hu, Haonan Jia, Jiahao Gong, Jiaoren Wu, Jie Wu, Jie Yang, Junzhe Lin, Kaixiang Li, Lei Xia, Longlong Gu, Ming Li, Nie Hao, Ranchen Ming, Shaoliang Pang, Siqi Liu, Song Yuan, Tiancheng Cao, Wen Li, Wenqing He, Xu Zhao, Xuelin Zhang, Yanbo Yu, Yinmin Zhong, Yu Zhou, Yuanwei Liang, Yuanwei Lu, Yuxiang Yang, Zidong Yang, Zili Zhang, Binxing Jiao, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Daxin Jiang, Shuchang Zhou, Chen Hu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08967v1 Announce Type: new \nAbstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent human-computer interaction, yet their reliance on text-based outputs limits their ability to generate natural speech responses directly, hindering seamless audio interactions. To address this, we introduce Step-Audio-AQAA, a fully end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model integrates a dual-codebook audio tokenizer for linguistic and semantic feature extraction, a 130-billion-parameter backbone LLM and a neural vocoder for high-fidelity speech synthesis. Our post-training approach employs interleaved token-output of text and audio to enhance semantic coherence and combines Direct Preference Optimization (DPO) with model merge to improve performance. Evaluations on the StepEval-Audio-360 benchmark demonstrate that Step-Audio-AQAA excels especially in speech control, outperforming the state-of-art LALMs in key areas. This work contributes a promising solution for end-to-end LALMs and highlights the critical role of token-based vocoder in enhancing overall performance for AQAA tasks."
      },
      {
        "id": "oai:arXiv.org:2506.08400v1",
        "title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks",
        "link": "https://arxiv.org/abs/2506.08400",
        "author": "Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08400v1 Announce Type: cross \nAbstract: Large Language models (LLMs) have demonstrated impressive performance on a wide range of tasks, including in multimodal settings such as speech. However, their evaluation is often limited to English and a few high-resource languages. For low-resource languages, there is no standardized evaluation benchmark. In this paper, we address this gap by introducing mSTEB, a new benchmark to evaluate the performance of LLMs on a wide range of tasks covering language identification, text classification, question answering, and translation tasks on both speech and text modalities. We evaluated the performance of leading LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in performance between high-resource and low-resource languages, especially for languages spoken in Africa and Americas/Oceania. Our findings show that more investment is needed to address their under-representation in LLMs coverage."
      },
      {
        "id": "oai:arXiv.org:2506.08564v1",
        "title": "Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?",
        "link": "https://arxiv.org/abs/2506.08564",
        "author": "Tuukka T\\\"or\\\"o, Antti Suni, Juraj \\v{S}imko",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08564v1 Announce Type: cross \nAbstract: Investigating linguistic relationships on a global scale requires analyzing diverse features such as syntax, phonology and prosody, which evolve at varying rates influenced by internal diversification, language contact, and sociolinguistic factors. Recent advances in machine learning (ML) offer complementary alternatives to traditional historical and typological approaches. Instead of relying on expert labor in analyzing specific linguistic features, these new methods enable the exploration of linguistic variation through embeddings derived directly from speech, opening new avenues for large-scale, data-driven analyses.\n  This study employs embeddings from the fine-tuned XLS-R self-supervised language identification model voxlingua107-xls-r-300m-wav2vec, to analyze relationships between 106 world languages based on speech recordings. Using linear discriminant analysis (LDA), language embeddings are clustered and compared with genealogical, lexical, and geographical distances. The results demonstrate that embedding-based distances align closely with traditional measures, effectively capturing both global and local typological patterns. Challenges in visualizing relationships, particularly with hierarchical clustering and network-based methods, highlight the dynamic nature of language change.\n  The findings show potential for scalable analyses of language variation based on speech embeddings, providing new perspectives on relationships among languages. By addressing methodological considerations such as corpus size and latent space dimensionality, this approach opens avenues for studying low-resource languages and bridging macro- and micro-level linguistic variation. Future work aims to extend these methods to underrepresented languages and integrate sociolinguistic variation for a more comprehensive understanding of linguistic diversity."
      },
      {
        "id": "oai:arXiv.org:2506.08717v1",
        "title": "Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition",
        "link": "https://arxiv.org/abs/2506.08717",
        "author": "Mehedi Hasan Bijoy, Dejan Porjazovski, Tam\\'as Gr\\'osz, Mikko Kurimo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08717v1 Announce Type: cross \nAbstract: Speech Emotion Recognition (SER) is crucial for improving human-computer interaction. Despite strides in monolingual SER, extending them to build a multilingual system remains challenging. Our goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models. To address this, we introduce a novel language-aware multi-teacher knowledge distillation method to advance SER in English, Finnish, and French. It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and then distills their knowledge into a single multilingual student model. The student model demonstrates state-of-the-art performance, with a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing fine-tuning and knowledge distillation baselines. Our method excels in improving recall for sad and neutral emotions, although it still faces challenges in recognizing anger and happiness."
      },
      {
        "id": "oai:arXiv.org:2506.08846v1",
        "title": "Addressing Pitfalls in Auditing Practices of Automatic Speech Recognition Technologies: A Case Study of People with Aphasia",
        "link": "https://arxiv.org/abs/2506.08846",
        "author": "Katelyn Xiaoying Mei, Anna Seo Gyeong Choi, Hilke Schellmann, Mona Sloane, Allison Koenecke",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08846v1 Announce Type: cross \nAbstract: Automatic Speech Recognition (ASR) has transformed daily tasks from video transcription to workplace hiring. ASR systems' growing use warrants robust and standardized auditing approaches to ensure automated transcriptions of high and equitable quality. This is especially critical for people with speech and language disorders (such as aphasia) who may disproportionately depend on ASR systems to navigate everyday life. In this work, we identify three pitfalls in existing standard ASR auditing procedures, and demonstrate how addressing them impacts audit results via a case study of six popular ASR systems' performance for aphasia speakers. First, audits often adhere to a single method of text standardization during data pre-processing, which (a) masks variability in ASR performance from applying different standardization methods, and (b) may not be consistent with how users - especially those from marginalized speech communities - would want their transcriptions to be standardized. Second, audits often display high-level demographic findings without further considering performance disparities among (a) more nuanced demographic subgroups, and (b) relevant covariates capturing acoustic information from the input audio. Third, audits often rely on a single gold-standard metric -- the Word Error Rate -- which does not fully capture the extent of errors arising from generative AI models, such as transcription hallucinations. We propose a more holistic auditing framework that accounts for these three pitfalls, and exemplify its results in our case study, finding consistently worse ASR performance for aphasia speakers relative to a control group. We call on practitioners to implement these robust ASR auditing practices that remain flexible to the rapidly changing ASR landscape."
      },
      {
        "id": "oai:arXiv.org:2506.08911v1",
        "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU",
        "link": "https://arxiv.org/abs/2506.08911",
        "author": "Petar Jaku\\v{s}, Hrvoje D\\v{z}apo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08911v1 Announce Type: cross \nAbstract: This paper presents a keyword spotting (KWS) system implemented on the NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), enabling real-time voice interaction on resource-constrained devices. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training to reduce model size with minimal accuracy drop. Experimental results demonstrate a 59x speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy with a model size of 30.58 KB, demonstrating the feasibility of efficient, low-power voice interfaces on embedded platforms."
      },
      {
        "id": "oai:arXiv.org:2309.09652v4",
        "title": "Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers",
        "link": "https://arxiv.org/abs/2309.09652",
        "author": "Peter Ochieng",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2309.09652v4 Announce Type: replace \nAbstract: This work introduces UDPNet, a novel architecture designed to accelerate the reverse diffusion process in speech synthesis. Unlike traditional diffusion models that rely on timestep embeddings and shared network parameters, UDPNet unrolls the reverse diffusion process directly into the network architecture, with successive layers corresponding to equally spaced steps in the diffusion schedule. Each layer progressively refines the noisy input, culminating in a high-fidelity estimation of the original data, \\(x_0\\). Additionally, we redefine the learning target by predicting latent variables instead of the conventional \\(x_0\\) or noise \\(\\epsilon_0\\). This shift addresses the common issue of large prediction errors in early denoising stages, effectively reducing speech distortion. Extensive evaluations on single- and multi-speaker datasets demonstrate that UDPNet consistently outperforms state-of-the-art methods in both quality and efficiency, while generalizing effectively to unseen speech. These results position UDPNet as a robust solution for real-time speech synthesis applications. Sample audio is available at https://onexpeters.github.io/UDPNet/."
      },
      {
        "id": "oai:arXiv.org:2403.00790v3",
        "title": "Structuring Concept Space with the Musical Circle of Fifths by Utilizing Music Grammar Based Activations",
        "link": "https://arxiv.org/abs/2403.00790",
        "author": "Tofara Moyo",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2403.00790v3 Announce Type: replace \nAbstract: In this paper, we explore the intriguing similarities between the structure of a discrete neural network, such as a spiking network, and the composition of a piano piece. While both involve nodes or notes that are activated sequentially or in parallel, the latter benefits from the rich body of music theory to guide meaningful combinations. We propose a novel approach that leverages musical grammar to regulate activations in a spiking neural network, allowing for the representation of symbols as attractors. By applying rules for chord progressions from music theory, we demonstrate how certain activations naturally follow others, akin to the concept of attraction. Furthermore, we introduce the concept of modulating keys to navigate different basins of attraction within the network. Ultimately, we show that the map of concepts in our model is structured by the musical circle of fifths, highlighting the potential for leveraging music theory principles in deep learning algorithms."
      },
      {
        "id": "oai:arXiv.org:2410.10913v3",
        "title": "Enhancing Retrieval-Augmented Audio Captioning with Generation-Assisted Multimodal Querying and Progressive Learning",
        "link": "https://arxiv.org/abs/2410.10913",
        "author": "Choi Changin, Lim Sungjun, Rhee Wonjong",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10913v3 Announce Type: replace \nAbstract: Retrieval-augmented generation can improve audio captioning by incorporating relevant audio-text pairs from a knowledge base. Existing methods typically rely solely on the input audio as a unimodal retrieval query. In contrast, we propose Generation-Assisted Multimodal Querying, which generates a text description of the input audio to enable multimodal querying. This approach aligns the query modality with the audio-text structure of the knowledge base, leading to more effective retrieval. Furthermore, we introduce a novel progressive learning strategy that gradually increases the number of interleaved audio-text pairs to enhance the training process. Our experiments on AudioCaps, Clotho, and Auto-ACD demonstrate that our approach achieves state-of-the-art results across these benchmarks."
      },
      {
        "id": "oai:arXiv.org:2410.23323v2",
        "title": "Phonology-Guided Speech-to-Speech Translation for African Languages",
        "link": "https://arxiv.org/abs/2410.23323",
        "author": "Peter Ochieng, Dennis Kaburu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2410.23323v2 Announce Type: replace \nAbstract: We present a prosody-guided framework for speech-to-speech translation (S2ST) that aligns and translates speech \\emph{without} transcripts by leveraging cross-linguistic pause synchrony. Analyzing a 6{,}000-hour East African news corpus spanning five languages, we show that \\emph{within-phylum} language pairs exhibit 30--40\\% lower pause variance and over 3$\\times$ higher onset/offset correlation compared to cross-phylum pairs. These findings motivate \\textbf{SPaDA}, a dynamic-programming alignment algorithm that integrates silence consistency, rate synchrony, and semantic similarity. SPaDA improves alignment $F_1$ by +3--4 points and eliminates up to 38\\% of spurious matches relative to greedy VAD baselines. Using SPaDA-aligned segments, we train \\textbf{SegUniDiff}, a diffusion-based S2ST model guided by \\emph{external gradients} from frozen semantic and speaker encoders. SegUniDiff matches an enhanced cascade in BLEU (30.3 on CVSS-C vs.\\ 28.9 for UnitY), reduces speaker error rate (EER) from 12.5\\% to 5.3\\%, and runs at an RTF of 1.02. To support evaluation in low-resource settings, we also release a three-tier, transcript-free BLEU suite (M1--M3) that correlates strongly with human judgments. Together, our results show that prosodic cues in multilingual speech provide a reliable scaffold for scalable, non-autoregressive S2ST."
      },
      {
        "id": "oai:arXiv.org:2503.00493v4",
        "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
        "link": "https://arxiv.org/abs/2503.00493",
        "author": "Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00493v4 Announce Type: replace \nAbstract: Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area."
      },
      {
        "id": "oai:arXiv.org:2503.23004v2",
        "title": "The trajectoRIR Database: Room Acoustic Recordings Along a Trajectory of Moving Microphones",
        "link": "https://arxiv.org/abs/2503.23004",
        "author": "Stefano Damiano, Kathleen MacWilliam, Valerio Lorenzoni, Thomas Dietzen, Toon van Waterschoot",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23004v2 Announce Type: replace \nAbstract: Data availability is essential to develop acoustic signal processing algorithms, especially when it comes to data-driven approaches that demand large and diverse training datasets. For this reason, an increasing number of databases have been published in recent years, including either room impulse responses (RIRs) or recordings of moving audio. In this paper we introduce the trajectoRIR database, an extensive, multi-array collection of both dynamic and stationary acoustic recordings along a controlled trajectory in a room. Specifically, the database features recordings using moving microphones and stationary RIRs spatially sampling the room acoustics along an L-shaped, 3.74-meter-long trajectory. This combination makes trajectoRIR unique and applicable in various tasks ranging from sound source localization and tracking to spatially dynamic sound field reconstruction and system identification. The recording room has a reverberation time of 0.5 seconds, and the three different microphone configurations employed include a dummy head, with additional reference microphones located next to the ears, 3 first-order Ambisonics microphones, two circular arrays of 16 and 4 channels, and a 12-channel linear array. The motion of the microphones was achieved using a robotic cart traversing a rail at three speeds: [0.2,0.4,0.8] m/s. Audio signals were reproduced using two stationary loudspeakers. The collected database features 8648 stationary RIRs, as well as perfect sweeps, speech, music, and stationary noise recorded during motion. MATLAB and Python scripts are included to access the recorded audio as well as to retrieve geometrical information."
      },
      {
        "id": "oai:arXiv.org:2504.00369v2",
        "title": "Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks",
        "link": "https://arxiv.org/abs/2504.00369",
        "author": "Yongyi Zang, Sean O'Brien, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00369v2 Announce Type: replace \nAbstract: Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, on par or above most LALMs. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening: Robust Understanding through Listening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question's reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information-text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework's effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities."
      },
      {
        "id": "oai:arXiv.org:2506.05688v2",
        "title": "Voice Impression Control in Zero-Shot TTS",
        "link": "https://arxiv.org/abs/2506.05688",
        "author": "Keinichi Fujita, Shota Horiguchi, Yusuke Ijima",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05688v2 Announce Type: replace \nAbstract: Para-/non-linguistic information in speech is pivotal in shaping the listeners' impression. Although zero-shot text-to-speech (TTS) has achieved high speaker fidelity, modulating subtle para-/non-linguistic information to control perceived voice characteristics, i.e., impressions, remains challenging. We have therefore developed a voice impression control method in zero-shot TTS that utilizes a low-dimensional vector to represent the intensities of various voice impression pairs (e.g., dark-bright). The results of both objective and subjective evaluations have demonstrated our method's effectiveness in impression control. Furthermore, generating this vector via a large language model enables target-impression generation from a natural language description of the desired impression, thus eliminating the need for manual optimization. Audio examples are available on our demo page (https://ntt-hilab-gensp.github.io/is2025voiceimpression/)."
      },
      {
        "id": "oai:arXiv.org:2506.07294v2",
        "title": "Towards Generalized Source Tracing for Codec-Based Deepfake Speech",
        "link": "https://arxiv.org/abs/2506.07294",
        "author": "Xuanjun Chen, I-Ming Lin, Lin Zhang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07294v2 Announce Type: replace \nAbstract: Recent attempts at source tracing for codec-based deepfake speech (CodecFake), generated by neural audio codec-based speech generation (CoSG) models, have exhibited suboptimal performance. However, how to train source tracing models using simulated CoSG data while maintaining strong performance on real CoSG-generated audio remains an open challenge. In this paper, we show that models trained solely on codec-resynthesized data tend to overfit to non-speech regions and struggle to generalize to unseen content. To mitigate these challenges, we introduce the Semantic-Acoustic Source Tracing Network (SASTNet), which jointly leverages Whisper for semantic feature encoding and Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet achieves state-of-the-art performance on the CoSG test set of the CodecFake+ dataset, demonstrating its effectiveness for reliable source tracing."
      },
      {
        "id": "oai:arXiv.org:2506.07473v2",
        "title": "An introduction to pitch strength in contemporary popular music analysis and production",
        "link": "https://arxiv.org/abs/2506.07473",
        "author": "Emmanuel Deruty",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.07473v2 Announce Type: replace \nAbstract: Music information retrieval distinguishes between low- and high-level descriptions of music. Current generative AI models rely on text descriptions that are higher level than the controls familiar to studio musicians. Pitch strength, a low-level perceptual parameter of contemporary popular music, may be one feature that could make such AI models more suited to music production. Signal and perceptual analyses suggest that pitch strength (1) varies significantly across and inside songs; (2) contributes to both small- and large-scale structure; (3) contributes to the handling of polyphonic dissonance; and (4) may be a feature of upper harmonics made audible in a perspective of perceptual richness."
      },
      {
        "id": "oai:arXiv.org:2409.08797v2",
        "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR",
        "link": "https://arxiv.org/abs/2409.08797",
        "author": "Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2409.08797v2 Announce Type: replace-cross \nAbstract: Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR."
      },
      {
        "id": "oai:arXiv.org:2504.08024v2",
        "title": "Summarizing Speech: A Comprehensive Survey",
        "link": "https://arxiv.org/abs/2504.08024",
        "author": "Fabian Retkowski, Maike Z\\\"ufle, Andreas Sudmann, Dinah Pfau, Shinji Watanabe, Jan Niehues, Alexander Waibel",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08024v2 Announce Type: replace-cross \nAbstract: Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization remains loosely defined. The field intersects with several research areas, including speech recognition, text summarization, and specific applications like meeting summarization. This survey not only examines existing datasets and evaluation protocols, which are crucial for assessing the quality of summarization approaches, but also synthesizes recent developments in the field, highlighting the shift from traditional systems to advanced models like fine-tuned cascaded architectures and end-to-end solutions. In doing so, we surface the ongoing challenges, such as the need for realistic evaluation benchmarks, multilingual datasets, and long-context handling."
      },
      {
        "id": "oai:arXiv.org:2506.00267v2",
        "title": "CASPER: A Large Scale Spontaneous Speech Dataset",
        "link": "https://arxiv.org/abs/2506.00267",
        "author": "Cihan Xiao, Ruixing Liang, Xiangyu Zhang, Mehmet Emre Tiryaki, Veronica Bae, Lavanya Shankar, Rong Yang, Ethan Poon, Emmanuel Dupoux, Sanjeev Khudanpur, Leibny Paola Garcia Perera",
        "published": "Wed, 11 Jun 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00267v2 Announce Type: replace-cross \nAbstract: The success of large language models has driven interest in developing similar speech processing capabilities. However, a key challenge is the scarcity of high-quality spontaneous speech data, as most existing datasets contain scripted dialogues. To address this, we present a novel pipeline for eliciting and recording natural dialogues and release our dataset with 100+ hours of spontaneous speech. Our approach fosters fluid, natural conversations while encouraging a diverse range of topics and interactive exchanges. Unlike traditional methods, it facilitates genuine interactions, providing a reproducible framework for future data collection. This paper introduces our dataset and methodology, laying the groundwork for addressing the shortage of spontaneous speech data. We plan to expand this dataset in future stages, offering a growing resource for the research community."
      }
    ]
  }
}