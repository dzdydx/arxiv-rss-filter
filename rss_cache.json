{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Wed, 30 Apr 2025 04:09:53 +0000",
      "published": "Wed, 30 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.20049v1",
        "title": "It's the same but not the same: Do LLMs distinguish Spanish varieties?",
        "link": "https://arxiv.org/abs/2504.20049",
        "author": "Marina Mayor-Rocher, Cristina Pozo, Nina Melero, Gonzalo Mart\\'inez, Mar\\'ia Grandury, Pedro Reviriego",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20049v1 Announce Type: new \nAbstract: In recent years, large language models (LLMs) have demonstrated a high capacity for understanding and generating text in Spanish. However, with five hundred million native speakers, Spanish is not a homogeneous language but rather one rich in diatopic variations spanning both sides of the Atlantic. For this reason, in this study, we evaluate the ability of nine language models to identify and distinguish the morphosyntactic and lexical peculiarities of seven varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean, Peninsular, Mexican and Central American and Rioplatense) through a multiple-choice test. The results indicate that the Peninsular Spanish variety is the best identified by all models and that, among them, GPT-4o is the only model capable of recognizing the variability of the Spanish language.\n  --\n  En los \\'ultimos a\\~nos, los grandes modelos de lenguaje (LLMs, por sus siglas en ingl\\'es) han demostrado una alta capacidad para comprender y generar texto en espa\\~nol. Sin embargo, con quinientos millones de hablantes nativos, la espa\\~nola no es una lengua homog\\'enea, sino rica en variedades diat\\'opicas que se extienden a ambos lados del Atl\\'antico. Por todo ello, evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de identificar y discernir las peculiaridades morfosint\\'acticas y l\\'exicas de siete variedades de espa\\~nol (andino, antillano, caribe\\~no continental, chileno, espa\\~nol peninsular, mexicano y centroamericano y rioplatense) mediante un test de respuesta m\\'ultiple. Los resultados obtenidos indican que la variedad de espa\\~nol peninsular es la mejor identificada por todos los modelos y que, de entre todos, GPT-4o es el \\'unico modelo capaz de identificar la variabilidad de la lengua espa\\~nola."
      },
      {
        "id": "oai:arXiv.org:2504.20051v1",
        "title": "Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts",
        "link": "https://arxiv.org/abs/2504.20051",
        "author": "Frances Laureano De Leon, Harish Tayyar Madabushi, Mark G. Lee",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20051v1 Announce Type: new \nAbstract: Multiword expressions, characterised by non-compositional meanings and syntactic irregularities, are an example of nuanced language. These expressions can be used literally or idiomatically, leading to significant changes in meaning. While large language models have demonstrated strong performance across many tasks, their ability to handle such linguistic subtleties remains uncertain. Therefore, this study evaluates how state-of-the-art language models process the ambiguity of potentially idiomatic multiword expressions, particularly in contexts that are less frequent, where models are less likely to rely on memorisation. By evaluating models across in Portuguese and Galician, in addition to English, and using a novel code-switched dataset and a novel task, we find that large language models, despite their strengths, struggle with nuanced language. In particular, we find that the latest models, including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both detection and semantic tasks, with especially poor performance on the novel tasks we introduce, despite its similarity to existing tasks. Overall, our results demonstrate that multiword expressions, especially those which are ambiguous, continue to be a challenge to models."
      },
      {
        "id": "oai:arXiv.org:2504.20052v1",
        "title": "Can Geometry Save Central Views for Sports Field Registration?",
        "link": "https://arxiv.org/abs/2504.20052",
        "author": "Floriane Magera, Thomas Hoyoux, Martin Castin, Olivier Barnich, Anthony Cioppa, Marc Van Droogenbroeck",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20052v1 Announce Type: new \nAbstract: Single-frame sports field registration often serves as the foundation for extracting 3D information from broadcast videos, enabling applications related to sports analytics, refereeing, or fan engagement. As sports fields have rigorous specifications in terms of shape and dimensions of their line, circle and point components, sports field markings are commonly used as calibration targets for this task. However, because of the sparse and uneven distribution of field markings, close-up camera views around central areas of the field often depict only line and circle markings. On these views, sports field registration is challenging for the vast majority of existing methods, as they focus on leveraging line field markings and their intersections. It is indeed a challenge to include circle correspondences in a set of linear equations. In this work, we propose a novel method to derive a set of points and lines from circle correspondences, enabling the exploitation of circle correspondences for both sports field registration and image annotation. In our experiments, we illustrate the benefits of our bottom-up geometric method against top-performing detectors and show that our method successfully complements them, enabling sports field registration in difficult scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.20054v1",
        "title": "Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment",
        "link": "https://arxiv.org/abs/2504.20054",
        "author": "Jiayang Sun, Hongbo Wang, Jie Cao, Huaibo Huang, Ran He",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20054v1 Announce Type: new \nAbstract: While diffusion models excel at generating high-quality images, they often struggle with accurate counting, attributes, and spatial relationships in complex multi-object scenes. To address these challenges, we propose Marmot, a novel and generalizable framework that employs Multi-Agent Reasoning for Multi-Object Self-Correcting, enhancing image-text alignment and facilitating more coherent multi-object image editing. Our framework adopts a divide-and-conquer strategy that decomposes the self-correction task into three critical dimensions (counting, attributes, and spatial relationships), and further divided into object-level subtasks. We construct a multi-agent editing system featuring a decision-execution-verification mechanism, effectively mitigating inter-object interference and enhancing editing reliability. To resolve the problem of subtask integration, we propose a Pixel-Domain Stitching Smoother that employs mask-guided two-stage latent space optimization. This innovation enables parallel processing of subtask results, thereby enhancing runtime efficiency while eliminating multi-stage distortion accumulation. Extensive experiments demonstrate that Marmot significantly improves accuracy in object counting, attribute assignment, and spatial relationships for image generation tasks."
      },
      {
        "id": "oai:arXiv.org:2504.20055v1",
        "title": "A constraints-based approach to fully interpretable neural networks for detecting learner behaviors",
        "link": "https://arxiv.org/abs/2504.20055",
        "author": "Juan D. Pinto, Luc Paquette",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20055v1 Announce Type: new \nAbstract: The increasing use of complex machine learning models in education has led to concerns about their interpretability, which in turn has spurred interest in developing explainability techniques that are both faithful to the model's inner workings and intelligible to human end-users. In this paper, we describe a novel approach to creating a neural-network-based behavior detection model that is interpretable by design. Our model is fully interpretable, meaning that the parameters we extract for our explanations have a clear interpretation, fully capture the model's learned knowledge about the learner behavior of interest, and can be used to create explanations that are both faithful and intelligible. We achieve this by implementing a series of constraints to the model that both simplify its inference process and bring it closer to a human conception of the task at hand. We train the model to detect gaming-the-system behavior, evaluate its performance on this task, and compare its learned patterns to those identified by human experts. Our results show that the model is successfully able to learn patterns indicative of gaming-the-system behavior while providing evidence for fully interpretable explanations. We discuss the implications of our approach and suggest ways to evaluate explainability using a human-grounded approach."
      },
      {
        "id": "oai:arXiv.org:2504.20069v1",
        "title": "A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives",
        "link": "https://arxiv.org/abs/2504.20069",
        "author": "Junhong Lai, Jiyu Wei, Lin Yao, Yueming Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20069v1 Announce Type: new \nAbstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain activity and diagnosing neurological disorders. This review focuses on the recent development of EEG foundation models(EEG-FMs), which have shown great potential in processing and analyzing EEG data. We discuss various EEG-FMs, including their architectures, pre-training strategies, their pre-training and downstream datasets and other details. The review also highlights the challenges and future directions in this field, aiming to provide a comprehensive overview for researchers and practitioners interested in EEG analysis and related EEG-FMs."
      },
      {
        "id": "oai:arXiv.org:2504.20070v1",
        "title": "Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization",
        "link": "https://arxiv.org/abs/2504.20070",
        "author": "Altun Shukurlu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20070v1 Announce Type: new \nAbstract: Deep Knowledge Tracing (DKT) models student learning behavior by using Recurrent Neural Networks (RNNs) to predict future performance based on historical interaction data. However, the original implementation relied on standard RNNs in the Lua-based Torch framework, which limited extensibility and reproducibility. In this work, we revisit the DKT model from two perspectives: architectural improvements and optimization efficiency. First, we enhance the model using gated recurrent units, specifically Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRU), which better capture long-term dependencies and help mitigate vanishing gradient issues. Second, we re-implement DKT using the PyTorch framework, enabling a modular and accessible infrastructure compatible with modern deep learning workflows. We also benchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and AdamW to evaluate their impact on convergence speed and predictive accuracy in educational modeling tasks. Experiments on the Synthetic-5 and Khan Academy datasets show that GRUs and LSTMs achieve higher accuracy and improved training stability compared to basic RNNs, while adaptive optimizers such as Adam and AdamW consistently outperform SGD in both early-stage learning and final model performance. Our open-source PyTorch implementation provides a reproducible and extensible foundation for future research in neural knowledge tracing and personalized learning systems."
      },
      {
        "id": "oai:arXiv.org:2504.20073v1",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.20073",
        "author": "Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, Manling Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20073v1 Announce Type: new \nAbstract: Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN."
      },
      {
        "id": "oai:arXiv.org:2504.20077v1",
        "title": "Edge-Based Learning for Improved Classification Under Adversarial Noise",
        "link": "https://arxiv.org/abs/2504.20077",
        "author": "Manish Kansana, Keyan Alexander Rahimi, Elias Hossain, Iman Dehzangi, Noorbakhsh Amiri Golilarz",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20077v1 Announce Type: new \nAbstract: Adversarial noise introduces small perturbations in images, misleading deep learning models into misclassification and significantly impacting recognition accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method (FGSM) adversarial noise on image classification and investigated whether training on specific image features can improve robustness. We hypothesize that while adversarial noise perturbs various regions of an image, edges may remain relatively stable and provide essential structural information for classification. To test this, we conducted a series of experiments using brain tumor and COVID datasets. Initially, we trained the models on clean images and then introduced subtle adversarial perturbations, which caused deep learning models to significantly misclassify the images. Retraining on a combination of clean and noisy images led to improved performance. To evaluate the robustness of the edge features, we extracted edges from the original/clean images and trained the models exclusively on edge-based representations. When noise was introduced to the images, the edge-based models demonstrated greater resilience to adversarial attacks compared to those trained on the original or clean images. These results suggest that while adversarial noise is able to exploit complex non-edge regions significantly more than edges, the improvement in the accuracy after retraining is marginally more in the original data as compared to the edges. Thus, leveraging edge-based learning can improve the resilience of deep learning models against adversarial perturbations."
      },
      {
        "id": "oai:arXiv.org:2504.20078v1",
        "title": "Low-Rank Matrix Approximation for Neural Network Compression",
        "link": "https://arxiv.org/abs/2504.20078",
        "author": "Kalyan Cherukuri, Aarav Lala",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20078v1 Announce Type: new \nAbstract: Deep Neural Networks (DNNs) are often constrained by their large memories and computational restrictions. In this paper, we introduce a novel adaptive-rank Singular Value Decomposition (ARSVD) that dynamically chooses the rank increase of the fully connected layers below a certain threshold in energy expenditure. Unlike conventional SVD compression methods that apply a fixed rank reduction in all layers, our ARSVD method uses energy distribution to adaptively select rank per layer while retaining accuracy. This is done for each layer in an effort to use as much energy as possible while maintaining the lowest accuracy loss. Such accuracy-adaptive approaches outperform traditional static rank reduction methods by providing an improved balance between compression and model performance. We first train a simple Multi-Layer Perceptron (MLP) on the MNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using accuracy and F1-score. After applying ARSVD, our results demonstrate that the technique can achieve substantial model compression without compromising classification accuracy. These results illustrate the usefulness of ARSVD in computing scenarios where both computational and memory resources are scarce."
      },
      {
        "id": "oai:arXiv.org:2504.20079v1",
        "title": "FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking",
        "link": "https://arxiv.org/abs/2504.20079",
        "author": "Xuan Rao, Bo Zhao, Derong Liu, Cesare Alippi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20079v1 Announce Type: new \nAbstract: Strong priors are imposed on the search space of Differentiable Architecture Search (DARTS), such that cells of the same type share the same topological structure and each intermediate node retains two operators from distinct nodes. While these priors reduce optimization difficulties and improve the applicability of searched architectures, they hinder the subsequent development of automated machine learning (Auto-ML) and prevent the optimization algorithm from exploring more powerful neural networks through improved architectural flexibility. This paper aims to reduce these prior constraints by eliminating restrictions on cell topology and modifying the discretization mechanism for super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which leverages an Entropy-based Super-Network Shrinking (ESS) framework, is presented to address the challenges arising from the elimination of prior constraints. Notably, FX-DARTS enables the derivation of neural architectures without strict prior rules while maintaining the stability in the enlarged search space. Experimental results on image classification benchmarks demonstrate that FX-DARTS is capable of exploring a set of neural architectures with competitive trade-offs between performance and computational complexity within a single search procedure."
      },
      {
        "id": "oai:arXiv.org:2504.20080v1",
        "title": "DNAD: Differentiable Neural Architecture Distillation",
        "link": "https://arxiv.org/abs/2504.20080",
        "author": "Xuan Rao, Bo Zhao, Derong Liu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20080v1 Announce Type: new \nAbstract: To meet the demand for designing efficient neural networks with appropriate trade-offs between model performance (e.g., classification accuracy) and computational complexity, the differentiable neural architecture distillation (DNAD) algorithm is developed based on two cores, namely search by deleting and search by imitating. Primarily, to derive neural architectures in a space where cells of the same type no longer share the same topology, the super-network progressive shrinking (SNPS) algorithm is developed based on the framework of differentiable architecture search (DARTS), i.e., search by deleting. Unlike conventional DARTS-based approaches which yield neural architectures with simple structures and derive only one architecture during the search procedure, SNPS is able to derive a Pareto-optimal set of architectures with flexible structures by forcing the dynamic super-network shrink from a dense structure to a sparse one progressively. Furthermore, since knowledge distillation (KD) has shown great effectiveness to train a compact network with the assistance of an over-parameterized model, we integrate SNPS with KD to formulate the DNAD algorithm, i.e., search by imitating. By minimizing behavioral differences between the super-network and teacher network, the over-fitting of one-level DARTS is avoided and well-performed neural architectures are derived. Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both SNPS and DNAD are able to derive a set of architectures which achieve similar or lower error rates with fewer parameters and FLOPs. Particularly, DNAD achieves the top-1 error rate of 23.7% on ImageNet classification with a model of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods."
      },
      {
        "id": "oai:arXiv.org:2504.20086v1",
        "title": "Understanding and Mitigating Risks of Generative AI in Financial Services",
        "link": "https://arxiv.org/abs/2504.20086",
        "author": "Sebastian Gehrmann, Claire Huang, Xian Teng, Sergei Yurovski, Iyanuoluwa Shode, Chirag S. Patel, Arjun Bhorkar, Naveen Thomas, John Doucette, David Rosenberg, Mark Dredze, David Rabinowitz",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20086v1 Announce Type: new \nAbstract: To responsibly develop Generative AI (GenAI) products, it is critical to define the scope of acceptable inputs and outputs. What constitutes a \"safe\" response is an actively debated question. Academic work puts an outsized focus on evaluating models by themselves for general purpose aspects such as toxicity, bias, and fairness, especially in conversational applications being used by a broad audience. In contrast, less focus is put on considering sociotechnical systems in specialized domains. Yet, those specialized systems can be subject to extensive and well-understood legal and regulatory scrutiny. These product-specific considerations need to be set in industry-specific laws, regulations, and corporate governance requirements. In this paper, we aim to highlight AI content safety considerations specific to the financial services domain and outline an associated AI content risk taxonomy. We compare this taxonomy to existing work in this space and discuss implications of risk category violations on various stakeholders. We evaluate how existing open-source technical guardrail solutions cover this taxonomy by assessing them on data collected via red-teaming activities. Our results demonstrate that these guardrails fail to detect most of the content risks we discuss."
      },
      {
        "id": "oai:arXiv.org:2504.20091v1",
        "title": "VideoMultiAgents: A Multi-Agent Framework for Video Question Answering",
        "link": "https://arxiv.org/abs/2504.20091",
        "author": "Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Masamoto Tanabiki, Kazuki Kozuka, Ehsan Adeli",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20091v1 Announce Type: new \nAbstract: Video Question Answering (VQA) inherently relies on multimodal reasoning, integrating visual, temporal, and linguistic cues to achieve a deeper understanding of video content. However, many existing methods rely on feeding frame-level captions into a single model, making it difficult to adequately capture temporal and interactive contexts. To address this limitation, we introduce VideoMultiAgents, a framework that integrates specialized agents for vision, scene graph analysis, and text processing. It enhances video understanding leveraging complementary multimodal reasoning from independently operating agents. Our approach is also supplemented with a question-guided caption generation, which produces captions that highlight objects, actions, and temporal transitions directly relevant to a given query, thus improving the answer accuracy. Experimental results demonstrate that our method achieves state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA), EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%)."
      },
      {
        "id": "oai:arXiv.org:2504.20096v1",
        "title": "Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis",
        "link": "https://arxiv.org/abs/2504.20096",
        "author": "Damien Martins Gomes",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20096v1 Announce Type: new \nAbstract: First-order optimization methods remain the standard for training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by preconditioning the stochastic gradient with a diagonal matrix. Despite the widespread adoption of first-order methods, second-order optimization algorithms often exhibit superior convergence compared to methods like Adam and SGD. However, their practicality in training DNNs is still limited by a significantly higher per-iteration computational cost compared to first-order methods. In this thesis, we present AdaFisher, a novel adaptive second-order optimizer that leverages a diagonal block-Kronecker approximation of the Fisher information matrix to adaptively precondition gradients. AdaFisher aims to bridge the gap between the improved convergence and generalization of second-order methods and the computational efficiency needed for training DNNs. Despite the traditionally slower speed of second-order optimizers, AdaFisher is effective for tasks such as image classification and language modeling, exhibiting remarkable stability and robustness during hyperparameter tuning. We demonstrate that AdaFisher outperforms state-of-the-art optimizers in both accuracy and convergence speed. The code is available from https://github.com/AtlasAnalyticsLab/AdaFisher."
      },
      {
        "id": "oai:arXiv.org:2504.20097v1",
        "title": "Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments",
        "link": "https://arxiv.org/abs/2504.20097",
        "author": "Junran Guo, Tonglin Mu, Keyuan Li, Jianing Li, Ziyang Luo, Ye Chen, Xiaodong Fan, Jinquan Huang, Minjie Liu, Jinbei Zhang, Ruoyang Qi, Naiting Gu, Shihai Sun",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20097v1 Announce Type: new \nAbstract: Detecting small objects, such as drones, over long distances presents a significant challenge with broad implications for security, surveillance, environmental monitoring, and autonomous systems. Traditional imaging-based methods rely on high-resolution image acquisition, but are often constrained by range, power consumption, and cost. In contrast, data-driven single-photon-single-pixel light detection and ranging (\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}) provides an imaging-free alternative, directly enabling target identification while reducing system complexity and cost. However, its detection range has been limited to a few hundred meters. Here, we introduce a novel integration of residual neural networks (ResNet) with \\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}, incorporating a refined observation model to extend the detection range to 5~\\si{\\kilo\\meter} in an intracity environment while enabling high-accuracy identification of drone poses and types. Experimental results demonstrate that our approach not only outperforms conventional imaging-based recognition systems, but also achieves 94.93\\% pose identification accuracy and 97.99\\% type classification accuracy, even under weak signal conditions with long distances and low signal-to-noise ratios (SNRs). These findings highlight the potential of imaging-free methods for robust long-range detection of small targets in real-world scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.20099v1",
        "title": "Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics",
        "link": "https://arxiv.org/abs/2504.20099",
        "author": "Inmaculada Santamaria-Valenzuela, Victor Rodriguez-Fernandez, Javier Huertas-Tato, Jong Hyuk Park, David Camacho",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20099v1 Announce Type: new \nAbstract: The present study explores the interpretability of latent spaces produced by time series foundation models, focusing on their potential for visual analysis tasks. Specifically, we evaluate the MOMENT family of models, a set of transformer-based, pre-trained architectures for multivariate time series tasks such as: imputation, prediction, classification, and anomaly detection. We evaluate the capacity of these models on five datasets to capture the underlying structures in time series data within their latent space projection and validate whether fine tuning improves the clarity of the resulting embedding spaces. Notable performance improvements in terms of loss reduction were observed after fine tuning. Visual analysis shows limited improvement in the interpretability of the embeddings, requiring further work. Results suggest that, although Time Series Foundation Models such as MOMENT are robust, their latent spaces may require additional methodological refinements to be adequately interpreted, such as alternative projection techniques, loss functions, or data preprocessing strategies. Despite the limitations of MOMENT, foundation models supose a big reduction in execution time and so a great advance for interactive visual analytics."
      },
      {
        "id": "oai:arXiv.org:2504.20102v1",
        "title": "HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction",
        "link": "https://arxiv.org/abs/2504.20102",
        "author": "Qingzhi Yu, Shuai Yan, Wenfeng Dai, Xiang Cheng",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20102v1 Announce Type: new \nAbstract: Protein-protein interactions (PPIs) are fundamental for deciphering cellular functions,disease pathways,and drug discovery.Although existing neural networks and machine learning methods have achieved high accuracy in PPI prediction,their black-box nature leads to a lack of causal interpretation of the prediction results and difficulty in capturing hierarchical geometries and multi-scale dynamic interaction patterns among proteins.To address these challenges, we propose HyboWaveNet,a novel deep learning framework that collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale graphical wavelet transform for robust PPI prediction. Mapping protein features to Lorentz space simulates hierarchical topological relationships among biomolecules via a hyperbolic distance metric,enabling node feature representations that better fit biological a priori.HyboWaveNet inherently simulates hierarchical and scale-free biological relationships, while the integration of wavelet transforms enables adaptive extraction of local and global interaction features across different resolutions. Our framework generates node feature representations via a graph neural network under the Lorenz model and generates pairs of positive samples under multiple different views for comparative learning, followed by further feature extraction via multi-scale graph wavelet transforms to predict potential PPIs. Experiments on public datasets show that HyboWaveNet improves over both existing state-of-the-art methods. We also demonstrate through ablation experimental studies that the multi-scale graph wavelet transform module improves the predictive performance and generalization ability of HyboWaveNet. This work links geometric deep learning and signal processing to advance PPI prediction, providing a principled approach for analyzing complex biological systems"
      },
      {
        "id": "oai:arXiv.org:2504.20104v1",
        "title": "An on-production high-resolution longitudinal neonatal fingerprint database in Brazil",
        "link": "https://arxiv.org/abs/2504.20104",
        "author": "Luiz F. P. Southier, Marcelo Filipak, Luiz A. Zanlorensi, Ildefonso Wasilevski, Fabio Favarim, Jefferson T. Oliva, Marcelo Teixeira, Dalcimar Casanova",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20104v1 Announce Type: new \nAbstract: The neonatal period is critical for survival, requiring accurate and early identification to enable timely interventions such as vaccinations, HIV treatment, and nutrition programs. Biometric solutions offer potential for child protection by helping to prevent baby swaps, locate missing children, and support national identity systems. However, developing effective biometric identification systems for newborns remains a major challenge due to the physiological variability caused by finger growth, weight changes, and skin texture alterations during early development. Current literature has attempted to address these issues by applying scaling factors to emulate growth-induced distortions in minutiae maps, but such approaches fail to capture the complex and non-linear growth patterns of infants. A key barrier to progress in this domain is the lack of comprehensive, longitudinal biometric datasets capturing the evolution of neonatal fingerprints over time. This study addresses this gap by focusing on designing and developing a high-quality biometric database of neonatal fingerprints, acquired at multiple early life stages. The dataset is intended to support the training and evaluation of machine learning models aimed at emulating the effects of growth on biometric features. We hypothesize that such a dataset will enable the development of more robust and accurate Deep Learning-based models, capable of predicting changes in the minutiae map with higher fidelity than conventional scaling-based methods. Ultimately, this effort lays the groundwork for more reliable biometric identification systems tailored to the unique developmental trajectory of newborns."
      },
      {
        "id": "oai:arXiv.org:2504.20106v1",
        "title": "Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors",
        "link": "https://arxiv.org/abs/2504.20106",
        "author": "Ren-Wei Liang, Chin-Ting Hsu, Chan-Hung Yu, Saransh Agrawal, Shih-Cheng Huang, Shang-Tse Chen, Kuan-Hao Huang, Shao-Hua Sun",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20106v1 Announce Type: new \nAbstract: Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), attempt to balance these trade-offs but suffer from performance conflicts, limited controllability, and poor extendability. To address these issues, we propose Preference Vector, a novel framework inspired by task arithmetic. Instead of optimizing multiple preferences within a single objective, we train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time. This modular approach enables fine-grained, user-controllable preference adjustments and facilitates seamless integration of new preferences without retraining. Experiments show that our proposed Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment."
      },
      {
        "id": "oai:arXiv.org:2504.20108v1",
        "title": "Swapped Logit Distillation via Bi-level Teacher Alignment",
        "link": "https://arxiv.org/abs/2504.20108",
        "author": "Stephen Ekaputra Limantoro, Jhe-Hao Lin, Chih-Yu Wang, Yi-Lung Tsai, Hong-Han Shuai, Ching-Chun Huang, Wen-Huang Cheng",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20108v1 Announce Type: new \nAbstract: Knowledge distillation (KD) compresses the network capacity by transferring knowledge from a large (teacher) network to a smaller one (student). It has been mainstream that the teacher directly transfers knowledge to the student with its original distribution, which can possibly lead to incorrect predictions. In this article, we propose a logit-based distillation via swapped logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed under two assumptions: (1) the wrong prediction occurs when the prediction label confidence is not the maximum; (2) the \"natural\" limit of probability remains uncertain as the best value addition to the target cannot be determined. To address these issues, we propose a swapped logit processing scheme. Through this approach, we find that the swap method can be effectively extended to teacher and student outputs, transforming into two teachers. We further introduce loss scheduling to boost the performance of two teachers' alignment. Extensive experiments on image classification tasks demonstrate that SLD consistently performs best among previous state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.20110v1",
        "title": "Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling",
        "link": "https://arxiv.org/abs/2504.20110",
        "author": "Yu-hsuan Chen, Jing Bi, Cyril Ngo Ngoc, Victor Oancea, Jonathan Cagan, Levent Burak Kara",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20110v1 Announce Type: new \nAbstract: AI-driven surrogate modeling has become an increasingly effective alternative to physics-based simulations for 3D design, analysis, and manufacturing. These models leverage data-driven methods to predict physical quantities traditionally requiring computationally expensive simulations. However, the scarcity of labeled CAD-to-simulation datasets has driven recent advancements in self-supervised and foundation models, where geometric representation learning is performed offline and later fine-tuned for specific downstream tasks. While these approaches have shown promise, their effectiveness is limited in applications requiring fine-scale geometric detail preservation. This work introduces a self-supervised geometric representation learning method designed to capture fine-scale geometric features from non-parametric 3D models. Unlike traditional end-to-end surrogate models, this approach decouples geometric feature extraction from downstream physics tasks, learning a latent space embedding guided by geometric reconstruction losses. Key elements include the essential use of near-zero level sampling and the innovative batch-adaptive attention-weighted loss function, which enhance the encoding of intricate design features. The proposed method is validated through case studies in structural mechanics, demonstrating strong performance in capturing design features and enabling accurate few-shot physics predictions. Comparisons with traditional parametric surrogate modeling highlight its potential to bridge the gap between geometric and physics-based representations, providing an effective solution for surrogate modeling in data-scarce scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.20111v1",
        "title": "Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image",
        "link": "https://arxiv.org/abs/2504.20111",
        "author": "Anubhav Jain, Yuya Kobayashi, Naoki Murata, Yuhta Takida, Takashi Shibuya, Yuki Mitsufuji, Niv Cohen, Nasir Memon, Julian Togelius",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20111v1 Announce Type: new \nAbstract: Watermarking techniques are vital for protecting intellectual property and preventing fraudulent use of media. Most previous watermarking schemes designed for diffusion models embed a secret key in the initial noise. The resulting pattern is often considered hard to remove and forge into unrelated images. In this paper, we propose a black-box adversarial attack without presuming access to the diffusion model weights. Our attack uses only a single watermarked example and is based on a simple observation: there is a many-to-one mapping between images and initial noises. There are regions in the clean image latent space pertaining to each watermark that get mapped to the same initial noise when inverted. Based on this intuition, we propose an adversarial attack to forge the watermark by introducing perturbations to the images such that we can enter the region of watermarked images. We show that we can also apply a similar approach for watermark removal by learning perturbations to exit this region. We report results on multiple watermarking schemes (Tree-Ring, RingID, WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0). Our results demonstrate the effectiveness of the attack and expose vulnerabilities in the watermarking methods, motivating future research on improving them."
      },
      {
        "id": "oai:arXiv.org:2504.20112v1",
        "title": "Supervised Pretraining for Material Property Prediction",
        "link": "https://arxiv.org/abs/2504.20112",
        "author": "Chowdhury Mohammad Abid Rahman, Aldo H. Romero, Prashnna K. Gyawali",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20112v1 Announce Type: new \nAbstract: Accurate prediction of material properties facilitates the discovery of novel materials with tailored functionalities. Deep learning models have recently shown superior accuracy and flexibility in capturing structure-property relationships. However, these models often rely on supervised learning, which requires large, well-annotated datasets an expensive and time-consuming process. Self-supervised learning (SSL) offers a promising alternative by pretraining on large, unlabeled datasets to develop foundation models that can be fine-tuned for material property prediction. In this work, we propose supervised pretraining, where available class information serves as surrogate labels to guide learning, even when downstream tasks involve unrelated material properties. We evaluate this strategy on two state-of-the-art SSL models and introduce a novel framework for supervised pretraining. To further enhance representation learning, we propose a graph-based augmentation technique that injects noise to improve robustness without structurally deforming material graphs. The resulting foundation models are fine-tuned for six challenging material property predictions, achieving significant performance gains over baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE) and establishing a new benchmark in material property prediction. This study represents the first exploration of supervised pertaining with surrogate labels in material property prediction, advancing methodology and application in the field."
      },
      {
        "id": "oai:arXiv.org:2504.20121v1",
        "title": "Benchmarking Transferability: A Framework for Fair and Robust Evaluation",
        "link": "https://arxiv.org/abs/2504.20121",
        "author": "Alireza Kazemi, Helia Rezvani, Mahsa Baktashmotlagh",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20121v1 Announce Type: new \nAbstract: Transferability scores aim to quantify how well a model trained on one domain generalizes to a target domain. Despite numerous methods proposed for measuring transferability, their reliability and practical usefulness remain inconclusive, often due to differing experimental setups, datasets, and assumptions. In this paper, we introduce a comprehensive benchmarking framework designed to systematically evaluate transferability scores across diverse settings. Through extensive experiments, we observe variations in how different metrics perform under various scenarios, suggesting that current evaluation practices may not fully capture each method's strengths and limitations. Our findings underscore the value of standardized assessment protocols, paving the way for more reliable transferability measures and better-informed model selection in cross-domain applications. Additionally, we achieved a 3.5\\% improvement using our proposed metric for the head-training fine-tuning experimental setup. Our code is available in this repository: https://github.com/alizkzm/pert_robust_platform."
      },
      {
        "id": "oai:arXiv.org:2504.20131v1",
        "title": "LZ Penalty: An information-theoretic repetition penalty for autoregressive language models",
        "link": "https://arxiv.org/abs/2504.20131",
        "author": "Antonio A. Ginart, Naveen Kodali, Jason Lee, Caiming Xiong, Silvio Savarese, John R. Emmons",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20131v1 Announce Type: new \nAbstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate repetitions in autoregressive language models without loss of capability. The penalty is based on the codelengths in the LZ77 universal lossless compression algorithm. Through the lens of the prediction-compression duality, decoding the LZ penalty has the interpretation of sampling from the residual distribution after removing the information that is highly compressible. We demonstrate the LZ penalty enables state-of-the-art open-source reasoning models to operate with greedy (temperature zero) decoding without loss of capability and without instances of degenerate repetition. Both the industry-standard frequency penalty and repetition penalty are ineffective, incurring degenerate repetition rates of up to 4%."
      },
      {
        "id": "oai:arXiv.org:2504.20157v1",
        "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models",
        "link": "https://arxiv.org/abs/2504.20157",
        "author": "Zae Myung Kim, Chanwoo Park, Vipul Raheja, Dongyeop Kang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20157v1 Announce Type: new \nAbstract: Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared."
      },
      {
        "id": "oai:arXiv.org:2504.20168v1",
        "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools",
        "link": "https://arxiv.org/abs/2504.20168",
        "author": "Nishant Subramani, Jason Eisner, Justin Svegliato, Benjamin Van Durme, Yu Su, Sam Thomson",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20168v1 Announce Type: new \nAbstract: Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats."
      },
      {
        "id": "oai:arXiv.org:2504.20172v1",
        "title": "Causal Identification in Time Series Models",
        "link": "https://arxiv.org/abs/2504.20172",
        "author": "Erik Jahn, Karthik Karnik, Leonard J. Schulman",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20172v1 Announce Type: new \nAbstract: In this paper, we analyze the applicability of the Causal Identification algorithm to causal time series graphs with latent confounders. Since these graphs extend over infinitely many time steps, deciding whether causal effects across arbitrary time intervals are identifiable appears to require computation on graph segments of unbounded size. Even for deciding the identifiability of intervention effects on variables that are close in time, no bound is known on how many time steps in the past need to be considered. We give a first bound of this kind that only depends on the number of variables per time step and the maximum time lag of any direct or latent causal effect. More generally, we show that applying the Causal Identification algorithm to a constant-size segment of the time series graph is sufficient to decide identifiability of causal effects, even across unbounded time intervals."
      },
      {
        "id": "oai:arXiv.org:2504.20178v1",
        "title": "A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals",
        "link": "https://arxiv.org/abs/2504.20178",
        "author": "Zhe Cui, Yuli Li, Le-Nam Tran",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20178v1 Announce Type: new \nAbstract: Current crowd-counting models often rely on single-modal inputs, such as visual images or wireless signal data, which can result in significant information loss and suboptimal recognition performance. To address these shortcomings, we propose TransFusion, a novel multimodal fusion-based crowd- counting model that integrates Channel State Information (CSI) with image data. By leveraging the powerful capabilities of Transformer networks, TransFusion effectively combines these two distinct data modalities, enabling the capture of comprehen- sive global contextual information that is critical for accurate crowd estimation. However, while transformers are well capable of capturing global features, they potentially fail to identify finer- grained, local details essential for precise crowd counting. To mitigate this, we incorporate Convolutional Neural Networks (CNNs) into the model architecture, enhancing its ability to extract detailed local features that complement the global context provided by the Transformer. Extensive experimental evaluations demonstrate that TransFusion achieves high accuracy with minimal counting errors while maintaining superior efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.20179v1",
        "title": "Integration Flow Models",
        "link": "https://arxiv.org/abs/2504.20179",
        "author": "Jingjing Wang, Dan Zhang, Joshua Luo, Yin Yang, Feng Luo",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20179v1 Announce Type: new \nAbstract: Ordinary differential equation (ODE) based generative models have emerged as a powerful approach for producing high-quality samples in many applications. However, the ODE-based methods either suffer the discretization error of numerical solvers of ODE, which restricts the quality of samples when only a few NFEs are used, or struggle with training instability. In this paper, we proposed Integration Flow, which directly learns the integral of ODE-based trajectory paths without solving the ODE functions. Moreover, Integration Flow explicitly incorporates the target state $\\mathbf{x}_0$ as the anchor state in guiding the reverse-time dynamics. We have theoretically proven this can contribute to both stability and accuracy. To the best of our knowledge, Integration Flow is the first model with a unified structure to estimate ODE-based generative models and the first to show the exact straightness of 1-Rectified Flow without reflow. Through theoretical analysis and empirical evaluations, we show that Integration Flows achieve improved performance when it is applied to existing ODE-based models, such as diffusion models, Rectified Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model, 3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without reflow and 4.15 for PFGM++."
      },
      {
        "id": "oai:arXiv.org:2504.20187v1",
        "title": "AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.20187",
        "author": "Weihao Sun, Heeseung Bang, Andreas A. Malikopoulos",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20187v1 Announce Type: new \nAbstract: In this paper, we present an adherence-aware reinforcement learning (RL) approach aimed at seeking optimal lane-changing recommendations within a semi-autonomous driving environment to enhance a single vehicle's travel efficiency. The problem is framed within a Markov decision process setting and is addressed through an adherence-aware deep Q network, which takes into account the partial compliance of human drivers with the recommended actions. This approach is evaluated within CARLA's driving environment under realistic scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.20193v1",
        "title": "ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition",
        "link": "https://arxiv.org/abs/2504.20193",
        "author": "Zhe Cui, Shuxian Zhang, Kangzhi Lou, Le-Nam Tran",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20193v1 Announce Type: new \nAbstract: This paper presents ProFi-Net, a novel few-shot learning framework for WiFi-based gesture recognition that overcomes the chal- lenges of limited training data and sparse feature representations. ProFi- Net employs a prototype-based metric learning architecture enhanced with a feature-level attention mechanism, which dynamically refines the Euclidean distance by emphasizing the most discriminative feature di- mensions. Additionally, our approach introduces a curriculum-inspired data augmentation strategy exclusively on the query set. By progressively incorporating Gaussian noise of increasing magnitude, the model is ex- posed to a broader range of challenging variations, thereby improving its generalization and robustness to overfitting. Extensive experiments con- ducted across diverse real-world environments demonstrate that ProFi- Net significantly outperforms conventional prototype networks and other state-of-the-art few-shot learning methods in terms of classification ac- curacy and training efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.20197v1",
        "title": "Representation Learning on a Random Lattice",
        "link": "https://arxiv.org/abs/2504.20197",
        "author": "Aryeh Brill",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20197v1 Announce Type: new \nAbstract: Decomposing a deep neural network's learned representations into interpretable features could greatly enhance its safety and reliability. To better understand features, we adopt a geometric perspective, viewing them as a learned coordinate system for mapping an embedded data distribution. We motivate a model of a generic data distribution as a random lattice and analyze its properties using percolation theory. Learned features are categorized into context, component, and surface features. The model is qualitatively consistent with recent findings in mechanistic interpretability and suggests directions for future research."
      },
      {
        "id": "oai:arXiv.org:2504.20199v1",
        "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains",
        "link": "https://arxiv.org/abs/2504.20199",
        "author": "Juntian Zhang, Chuanqi cheng, Yuhan Liu, Wei Liu, Jian Luan, Rui Yan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20199v1 Announce Type: new \nAbstract: Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs'perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.20203v1",
        "title": "Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies",
        "link": "https://arxiv.org/abs/2504.20203",
        "author": "Vladyslav Polushko, Damjan Hatic, Ronald R\\\"osch, Thomas M\\\"arz, Markus Rauhut, Andreas Weinmann",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20203v1 Announce Type: new \nAbstract: Floods cause serious problems around the world. Responding quickly and effectively requires accurate and timely information about the affected areas. The effective use of Remote Sensing images for accurate flood detection requires specific detection methods. Typically, Deep Neural Networks are employed, which are trained on specific datasets. For the purpose of river flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here explore the use of different augmentation strategies, ranging from basic approaches to more complex techniques, including optical distortion. By identifying effective strategies, we aim to refine the training process of state-of-the-art Deep Learning segmentation networks."
      },
      {
        "id": "oai:arXiv.org:2504.20213v1",
        "title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework",
        "link": "https://arxiv.org/abs/2504.20213",
        "author": "Yuan Xia, Akanksha Atrey, Fadoua Khmaissia, Kedar S. Namjoshi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20213v1 Announce Type: new \nAbstract: This paper investigates the logical reasoning capabilities of large language models (LLMs). For a precisely defined yet tractable formulation, we choose the conceptually simple but technically complex task of constructing proofs in Boolean logic. A trained LLM receives as input a set of assumptions and a goal, and produces as output a proof that formally derives the goal from the assumptions. Incorrect proofs are caught by an automated proof checker. A critical obstacle for training is the scarcity of real-world proofs. We propose an efficient, randomized procedure for synthesizing valid proofs and introduce Template Transformation, a data augmentation technique that enhances the model's ability to handle complex logical expressions. The central evaluation question is whether an LLM has indeed learned to reason. We propose tests to measure the reasoning ability of a black-box LLM. By these measures, experiments demonstrate strong reasoning capabilities for assertions with short proofs, which decline with proof complexity. Notably, template transformation improves accuracy even for smaller models, suggesting its effectiveness across model scales."
      },
      {
        "id": "oai:arXiv.org:2504.20220v1",
        "title": "A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports",
        "link": "https://arxiv.org/abs/2504.20220",
        "author": "Henning Sch\\\"afer, Cynthia S. Schmidt, Johannes Wutzkowsky, Kamil Lorek, Lea Reinartz, Johannes R\\\"uckert, Christian Temme, Britta B\\\"ockmann, Peter A. Horn, Christoph M. Friedrich",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20220v1 Announce Type: new \nAbstract: Despite the growing adoption of electronic health records, many processes still rely on paper documents, reflecting the heterogeneous real-world conditions in which healthcare is delivered. The manual transcription process is time-consuming and prone to errors when transferring paper-based data to digital formats. To streamline this workflow, this study presents an open-source pipeline that extracts and categorizes checkbox data from scanned documents. Demonstrated on transfusion reaction reports, the design supports adaptation to other checkbox-rich document types. The proposed method integrates checkbox detection, multilingual optical character recognition (OCR) and multilingual vision-language models (VLMs). The pipeline achieves high precision and recall compared against annually compiled gold-standards from 2017 to 2024. The result is a reduction in administrative workload and accurate regulatory reporting. The open-source availability of this pipeline encourages self-hosted parsing of checkbox forms."
      },
      {
        "id": "oai:arXiv.org:2504.20222v1",
        "title": "FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations",
        "link": "https://arxiv.org/abs/2504.20222",
        "author": "Naoko Sawada, Pedro Miraldo, Suhas Lohit, Tim K. Marks, Moitreya Chatterjee",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20222v1 Announce Type: new \nAbstract: Neural implicit surface representation techniques are in high demand for advancing technologies in augmented reality/virtual reality, digital twins, autonomous navigation, and many other fields. With their ability to model object surfaces in a scene as a continuous function, such techniques have made remarkable strides recently, especially over classical 3D surface reconstruction methods, such as those that use voxels or point clouds. However, these methods struggle with scenes that have varied and complex surfaces principally because they model any given scene with a single encoder network that is tasked to capture all of low through high-surface frequency information in the scene simultaneously. In this work, we propose a novel, neural implicit surface representation approach called FreBIS to overcome this challenge. FreBIS works by stratifying the scene based on the frequency of surfaces into multiple frequency levels, with each level (or a group of levels) encoded by a dedicated encoder. Moreover, FreBIS encourages these encoders to capture complementary information by promoting mutual dissimilarity of the encoded features via a novel, redundancy-aware weighting module. Empirical evaluations on the challenging BlendedMVS dataset indicate that replacing the standard encoder in an off-the-shelf neural surface reconstruction method with our frequency-stratified encoders yields significant improvements. These enhancements are evident both in the quality of the reconstructed 3D surfaces and in the fidelity of their renderings from any viewpoint."
      },
      {
        "id": "oai:arXiv.org:2504.20234v1",
        "title": "Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters",
        "link": "https://arxiv.org/abs/2504.20234",
        "author": "Bartosz Ptak, Marek Kraft",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20234v1 Announce Type: new \nAbstract: Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method."
      },
      {
        "id": "oai:arXiv.org:2504.20241v1",
        "title": "Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts",
        "link": "https://arxiv.org/abs/2504.20241",
        "author": "Kamirul Kamirul, Odysseas Pappas, Alin Achim",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20241v1 Announce Type: new \nAbstract: Detecting ship presence via wake signatures in SAR imagery is attracting considerable research interest, but limited annotated data availability poses significant challenges for supervised learning. Physics-based simulations are commonly used to address this data scarcity, although they are slow and constrain end-to-end learning. In this work, we explore a new direction for more efficient and end-to-end SAR ship wake simulation using a diffusion model trained on data generated by a physics-based simulator. The training dataset is built by pairing images produced by the simulator with text prompts derived from simulation parameters. Experimental result show that the model generates realistic Kelvin wake patterns and achieves significantly faster inference than the physics-based simulator. These results highlight the potential of diffusion models for fast and controllable wake image generation, opening new possibilities for end-to-end downstream tasks in maritime SAR analysis."
      },
      {
        "id": "oai:arXiv.org:2504.20249v1",
        "title": "Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena",
        "link": "https://arxiv.org/abs/2504.20249",
        "author": "W. Diab, M. Al-Kobaisi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20249v1 Announce Type: new \nAbstract: Neural Operators (NOs) are machine learning models designed to solve partial differential equations (PDEs) by learning to map between function spaces. Neural Operators such as the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO) have demonstrated excellent generalization properties when mapping between spatial function spaces. However, they struggle in mapping the temporal dynamics of time-dependent PDEs, especially for time steps not explicitly seen during training. This limits their temporal accuracy as they do not leverage these dynamics in the training process. In addition, most NOs tend to be prohibitively costly to train, especially for higher-dimensional PDEs. In this paper, we propose the Temporal Neural Operator (TNO), an efficient neural operator specifically designed for spatio-temporal operator learning for time-dependent PDEs. TNO achieves this by introducing a temporal-branch to the DeepONet framework, leveraging the best architectural design choices from several other NOs, and a combination of training strategies including Markov assumption, teacher forcing, temporal bundling, and the flexibility to condition the output on the current state or past states. Through extensive benchmarking and an ablation study on a diverse set of example problems we demonstrate the TNO long range temporal extrapolation capabilities, robustness to error accumulation, resolution invariance, and flexibility to handle multiple input functions."
      },
      {
        "id": "oai:arXiv.org:2504.20250v1",
        "title": "Financial Data Analysis with Robust Federated Logistic Regression",
        "link": "https://arxiv.org/abs/2504.20250",
        "author": "Kun Yang, Nikhil Krishnan, Sanjeev R. Kulkarni",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20250v1 Announce Type: new \nAbstract: In this study, we focus on the analysis of financial data in a federated setting, wherein data is distributed across multiple clients or locations, and the raw data never leaves the local devices. Our primary focus is not only on the development of efficient learning frameworks (for protecting user data privacy) in the field of federated learning but also on the importance of designing models that are easier to interpret. In addition, we care about the robustness of the framework to outliers. To achieve these goals, we propose a robust federated logistic regression-based framework that strives to strike a balance between these goals. To verify the feasibility of our proposed framework, we carefully evaluate its performance not only on independently identically distributed (IID) data but also on non-IID data, especially in scenarios involving outliers. Extensive numerical results collected from multiple public datasets demonstrate that our proposed method can achieve comparable performance to those of classical centralized algorithms, such as Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary and multi-class classification tasks."
      },
      {
        "id": "oai:arXiv.org:2504.20251v1",
        "title": "A Platform for Generating Educational Activities to Teach English as a Second Language",
        "link": "https://arxiv.org/abs/2504.20251",
        "author": "Aiala Ros\\'a, Santiago G\\'ongora, Juan Pablo Filevich, Ignacio Sastre, Laura Musto, Brian Carpenter, Luis Chiruzzo",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20251v1 Announce Type: new \nAbstract: We present a platform for the generation of educational activities oriented to teaching English as a foreign language. The different activities --games and language practice exercises-- are strongly based on Natural Language Processing techniques. The platform offers the possibility of playing out-of-the-box games, generated from resources created semi-automatically and then manually curated. It can also generate games or exercises of greater complexity from texts entered by teachers, providing a stage of review and edition of the generated content before use. As a way of expanding the variety of activities in the platform, we are currently experimenting with image and text generation. In order to integrate them and improve the performance of other neural tools already integrated, we are working on migrating the platform to a more powerful server. In this paper we describe the development of our platform and its deployment for end users, discussing the challenges faced and how we overcame them, and also detail our future work plans."
      },
      {
        "id": "oai:arXiv.org:2504.20271v1",
        "title": "Investigating task-specific prompts and sparse autoencoders for activation monitoring",
        "link": "https://arxiv.org/abs/2504.20271",
        "author": "Henk Tillman, Dan Mossing",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20271v1 Announce Type: new \nAbstract: Language models can behave in unexpected and unsafe ways, and so it is valuable to monitor their outputs. Internal activations of language models encode additional information that could be useful for this. The baseline approach for activation monitoring is some variation of linear probing on a particular layer: starting from a labeled dataset, train a logistic regression classifier on that layer's activations. Recent work has proposed several approaches which may improve on naive linear probing, by leveraging additional computation. One class of techniques, which we call \"prompted probing,\" leverages test time computation to improve monitoring by (1) prompting the model with a description of the monitoring task, and (2) applying a learned linear probe to resulting activations. Another class of techniques uses computation at train time: training sparse autoencoders offline to identify an interpretable basis for the activations, and e.g. max-pooling activations across tokens using that basis before applying a linear probe. However, one can also prompt the model with a description of the monitoring task and use its output directly. We develop and test novel refinements of these methods and compare them against each other. We find asking the model zero-shot is a reasonable baseline when inference-time compute is not limited; however, activation probing methods can substantially outperform this baseline given sufficient training data. Specifically, we recommend prompted probing when inference-time compute is available, due to its superior data efficiency and good generalization performance. Alternatively, if inference-time compute is limited, we find SAE-based probing methods outperform raw activation probing."
      },
      {
        "id": "oai:arXiv.org:2504.20276v1",
        "title": "Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi",
        "link": "https://arxiv.org/abs/2504.20276",
        "author": "Dandan Chen Kaptur, Yue Huang, Xuejun Ryan Ji, Yanhui Guo, Bradley Kaptur",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20276v1 Announce Type: new \nAbstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs), for systematic reviews. We evaluated their performance by comparing LLM-generated codes with human-generated codes from a peer-reviewed systematic review on assessment. Our findings suggested that the performance of LLMs fluctuates by data volume and question complexity for systematic reviews."
      },
      {
        "id": "oai:arXiv.org:2504.20277v1",
        "title": "Generative Diffusion Models for Resource Allocation in Wireless Networks",
        "link": "https://arxiv.org/abs/2504.20277",
        "author": "Yigit Berkay Uslu, Samar Hadou, Shirin Saeedi Bidokhti, Alejandro Ribeiro",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20277v1 Announce Type: new \nAbstract: This paper proposes a supervised training algorithm for learning stochastic resource allocation policies with generative diffusion models (GDMs). We formulate the allocation problem as the maximization of an ergodic utility function subject to ergodic Quality of Service (QoS) constraints. Given samples from a stochastic expert policy that yields a near-optimal solution to the problem, we train a GDM policy to imitate the expert and generate new samples from the optimal distribution. We achieve near-optimal performance through sequential execution of the generated samples. To enable generalization to a family of network configurations, we parameterize the backward diffusion process with a graph neural network (GNN) architecture. We present numerical results in a case study of power control in multi-user interference networks."
      },
      {
        "id": "oai:arXiv.org:2504.20282v1",
        "title": "FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting",
        "link": "https://arxiv.org/abs/2504.20282",
        "author": "Michael A. Helcig, Stefan Nastic",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20282v1 Announce Type: new \nAbstract: Privacy-preserving distributed model training is crucial for modern machine learning applications, yet existing Federated Learning approaches struggle with heterogeneous data distributions and varying computational capabilities. Traditional solutions either treat all participants uniformly or require costly dynamic clustering during training, leading to reduced efficiency and delayed model specialization. We present FedCCL (Federated Clustered Continual Learning), a framework specifically designed for environments with static organizational characteristics but dynamic client availability. By combining static pre-training clustering with an adapted asynchronous FedAvg algorithm, FedCCL enables new clients to immediately profit from specialized models without prior exposure to their data distribution, while maintaining reduced coordination overhead and resilience to client disconnections. Our approach implements an asynchronous Federated Learning protocol with a three-tier model topology - global, cluster-specific, and local models - that efficiently manages knowledge sharing across heterogeneous participants. Evaluation using photovoltaic installations across central Europe demonstrates that FedCCL's location-based clustering achieves an energy prediction error of 3.93% (+-0.21%), while maintaining data privacy and showing that the framework maintains stability for population-independent deployments, with 0.14 percentage point degradation in performance for new installations. The results demonstrate that FedCCL offers an effective framework for privacy-preserving distributed learning, maintaining high accuracy and adaptability even with dynamic participant populations."
      },
      {
        "id": "oai:arXiv.org:2504.20288v1",
        "title": "Image Interpolation with Score-based Riemannian Metrics of Diffusion Models",
        "link": "https://arxiv.org/abs/2504.20288",
        "author": "Shinnosuke Saito, Takashi Matsubara",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20288v1 Announce Type: new \nAbstract: Diffusion models excel in content generation by implicitly learning the data manifold, yet they lack a practical method to leverage this manifold - unlike other deep generative models equipped with latent spaces. This paper introduces a novel framework that treats the data space of pre-trained diffusion models as a Riemannian manifold, with a metric derived from the score function. Experiments with MNIST and Stable Diffusion show that this geometry-aware approach yields image interpolations that are more realistic, less noisy, and more faithful to prompts than existing methods, demonstrating its potential for improved content generation and editing."
      },
      {
        "id": "oai:arXiv.org:2504.20293v1",
        "title": "Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results",
        "link": "https://arxiv.org/abs/2504.20293",
        "author": "Stefan Kober",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20293v1 Announce Type: new \nAbstract: Traditional k-means clustering underperforms on non-convex shapes and requires the number of clusters k to be specified in advance. We propose a simple geometric enhancement: after standard k-means, each cluster center is assigned a radius (the distance to its farthest assigned point), and clusters whose radii overlap are merged. This post-processing step loosens the requirement for exact k: as long as k is overestimated (but not excessively), the method can often reconstruct non-convex shapes through meaningful merges. We also show that this approach supports recursive partitioning: clustering can be performed independently on tiled regions of the feature space, then globally merged, making the method scalable and suitable for distributed systems. Implemented as a lightweight post-processing step atop scikit-learn's k-means, the algorithm performs well on benchmark datasets, achieving high accuracy with minimal additional computation."
      },
      {
        "id": "oai:arXiv.org:2504.20295v1",
        "title": "The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting",
        "link": "https://arxiv.org/abs/2504.20295",
        "author": "Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon-Gutierrez, Andres Caro",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20295v1 Announce Type: new \nAbstract: Digital twins (DTs) are improving water distribution systems by using real-time data, analytics, and prediction models to optimize operations. This paper presents a DT platform designed for a Spanish water supply network, utilizing Long Short-Term Memory (LSTM) networks to predict water consumption. However, machine learning models are vulnerable to adversarial attacks, such as the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). These attacks manipulate critical model parameters, injecting subtle distortions that degrade forecasting accuracy. To further exploit these vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based approach that dynamically adjusts perturbations, making adversarial attacks more difficult to detect. Experimental results show that this approach significantly impacts prediction reliability, causing the Mean Absolute Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack strategies amplify this effect, highlighting cybersecurity risks in AI-driven DTs. These findings emphasize the urgent need for robust defenses, including adversarial training, anomaly detection, and secure data pipelines."
      },
      {
        "id": "oai:arXiv.org:2504.20303v1",
        "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes",
        "link": "https://arxiv.org/abs/2504.20303",
        "author": "Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Siqi Lu, Quan Liu, Ruining Deng, Can Cui, Jialin Yue, Yizhe Lin, Tianyuan Yao, Juming Xiong, Junchao Zhu, Chongyu Qu, Yuechen Yang, Mitchell Wilkes, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, Yuankai Huo",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20303v1 Announce Type: new \nAbstract: By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on https://github.com/geopacha/DeepAndes."
      },
      {
        "id": "oai:arXiv.org:2504.20304v1",
        "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions",
        "link": "https://arxiv.org/abs/2504.20304",
        "author": "Xiulin Yang, Zhuoxuan Ju, Lanni Bu, Zoey Liu, Nathan Schneider",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20304v1 Announce Type: new \nAbstract: CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research."
      },
      {
        "id": "oai:arXiv.org:2504.20306v1",
        "title": "Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis",
        "link": "https://arxiv.org/abs/2504.20306",
        "author": "Teja Krishna Cherukuri, Nagur Shareef Shaik, Sribhuvan Reddy Yellu, Jun-Won Chung, Dong Hye Ye",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20306v1 Announce Type: new \nAbstract: Colorectal polyps are key indicators for early detection of colorectal cancer. However, traditional endoscopic imaging often struggles with accurate polyp localization and lacks comprehensive contextual awareness, which can limit the explainability of diagnoses. To address these issues, we propose the Dynamic Contextual Attention Network (DCAN). This novel approach transforms spatial representations into adaptive contextual insights, using an attention mechanism that enhances focus on critical polyp regions without explicit localization modules. By integrating contextual awareness into the classification process, DCAN improves decision interpretability and overall diagnostic performance. This advancement in imaging could lead to more reliable colorectal cancer detection, enabling better patient outcomes."
      },
      {
        "id": "oai:arXiv.org:2504.20307v1",
        "title": "FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization",
        "link": "https://arxiv.org/abs/2504.20307",
        "author": "Hui Chen, Xuhui Fan, Zhangkai Wu, Longbing Cao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20307v1 Announce Type: new \nAbstract: Bayesian optimization is a powerful technique for optimizing expensive-to-evaluate black-box functions, consisting of two main components: a surrogate model and an acquisition function. In recent years, myopic acquisition functions have been widely adopted for their simplicity and effectiveness. However, their lack of look-ahead capability limits their performance. To address this limitation, we propose FigBO, a generalized acquisition function that incorporates the future impact of candidate points on global information gain. FigBO is a plug-and-play method that can integrate seamlessly with most existing myopic acquisition functions. Theoretically, we analyze the regret bound and convergence rate of FigBO when combined with the myopic base acquisition function expected improvement (EI), comparing them to those of standard EI. Empirically, extensive experimental results across diverse tasks demonstrate that FigBO achieves state-of-the-art performance and significantly faster convergence compared to existing methods."
      },
      {
        "id": "oai:arXiv.org:2504.20310v1",
        "title": "A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning",
        "link": "https://arxiv.org/abs/2504.20310",
        "author": "Greg Gluch, Shafi Goldwasser",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20310v1 Announce Type: new \nAbstract: In this paper, we initiate a cryptographically inspired theoretical study of detection versus mitigation of adversarial inputs produced by attackers of Machine Learning algorithms during inference time.\n  We formally define defense by detection (DbD) and defense by mitigation (DbM). Our definitions come in the form of a 3-round protocol between two resource-bounded parties: a trainer/defender and an attacker. The attacker aims to produce inference-time inputs that fool the training algorithm. We define correctness, completeness, and soundness properties to capture successful defense at inference time while not degrading (too much) the performance of the algorithm on inputs from the training distribution.\n  We first show that achieving DbD and achieving DbM are equivalent for ML classification tasks. Surprisingly, this is not the case for ML generative learning tasks, where there are many possible correct outputs that can be generated for each input. We show a separation between DbD and DbM by exhibiting a generative learning task for which is possible to defend by mitigation but is provably impossible to defend by detection under the assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE), publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation phase uses significantly fewer samples than the initial training algorithm."
      },
      {
        "id": "oai:arXiv.org:2504.20314v1",
        "title": "Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training",
        "link": "https://arxiv.org/abs/2504.20314",
        "author": "Qitao Tan, Sung-En Chang, Rui Xia, Huidong Ji, Chence Yang, Ci Zhang, Jun Liu, Zheng Zhan, Zhou Zou, Yanzhi Wang, Jin Lu, Geng Yuan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20314v1 Announce Type: new \nAbstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN) training paradigm that offers computational simplicity and memory savings. However, this seemingly promising approach faces a significant and long-ignored challenge. ZO requires generating a substantial number of Gaussian random numbers, which poses significant difficulties and even makes it infeasible for hardware platforms, such as FPGAs and ASICs. In this paper, we identify this critical issue, which arises from the mismatch between algorithm and hardware designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO framework. Specifically, we design random number reuse strategies to significantly reduce the demand for random number generation and introduce a hardware-friendly adaptive scaling method to replace the costly Gaussian distribution with a uniform distribution. Our experiments show that PeZO reduces the required LUTs and FFs for random number generation by 48.6\\% and 12.7\\%, and saves at maximum 86\\% power consumption, all without compromising training performance, making ZO optimization feasible for on-device training. To the best of our knowledge, we are the first to explore the potential of on-device ZO optimization, providing valuable insights for future research."
      },
      {
        "id": "oai:arXiv.org:2504.20319v1",
        "title": "Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach",
        "link": "https://arxiv.org/abs/2504.20319",
        "author": "Huchen Yang, Xinghao Dong, Jin-Long Wu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20319v1 Announce Type: new \nAbstract: Bayesian experimental design (BED) offers a principled framework for optimizing data acquisition by leveraging probabilistic inference. However, practical implementations of BED are often compromised by model discrepancy, i.e., the mismatch between predictive models and true physical systems, which can potentially lead to biased parameter estimates. While data-driven approaches have been recently explored to characterize the model discrepancy, the resulting high-dimensional parameter space poses severe challenges for both Bayesian updating and design optimization. In this work, we propose a hybrid BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI) that addresses these challenges by providing a computationally efficient, gradient-free alternative to estimate the information gain for high-dimensional network parameters. The AD-EKI allows a differentiable evaluation of the utility function in BED and thus facilitates the use of standard gradient-based methods for design optimization. In the proposed hybrid framework, we iteratively optimize experimental designs, decoupling the inference of low-dimensional physical parameters handled by standard BED methods, from the high-dimensional model discrepancy handled by AD-EKI. The identified optimal designs for the model discrepancy enable us to systematically collect informative data for its calibration. The performance of the proposed method is studied by a classical convection-diffusion BED example, and the hybrid framework enabled by AD-EKI efficiently identifies informative data to calibrate the model discrepancy and robustly infers the unknown physical parameters in the modeled system. Besides addressing the challenges of BED with model discrepancy, AD-EKI also potentially fosters efficient and scalable frameworks in many other areas with bilevel optimization, such as meta-learning and structure optimization."
      },
      {
        "id": "oai:arXiv.org:2504.20322v1",
        "title": "Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training",
        "link": "https://arxiv.org/abs/2504.20322",
        "author": "Sumit Mamtani, Yash Thesia",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20322v1 Announce Type: new \nAbstract: Fine-grained visual classification aims to recognize objects belonging to multiple subordinate categories within a super-category. However, this remains a challenging problem, as appearance information alone is often insufficient to accurately differentiate between fine-grained visual categories. To address this, we propose a novel and unified framework that leverages meta-information to assist fine-grained identification. We tackle the joint learning of visual and meta-information through cross-contrastive pre-training. In the first stage, we employ three encoders for images, text, and meta-information, aligning their projected embeddings to achieve better representations. We then fine-tune the image and meta-information encoders for the classification task. Experiments on the NABirds dataset demonstrate that our framework effectively utilizes meta-information to enhance fine-grained recognition performance. With the addition of meta-information, our framework surpasses the current baseline on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the NABirds dataset, outperforming many existing state-of-the-art approaches that utilize meta-information."
      },
      {
        "id": "oai:arXiv.org:2504.20323v1",
        "title": "Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation",
        "link": "https://arxiv.org/abs/2504.20323",
        "author": "Chao-Lin Liu, Po-Hsien Wu, Yi-Ting Yu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20323v1 Announce Type: new \nAbstract: This report addresses the challenge of limited labeled datasets for developing legal recommender systems, particularly in specialized domains like labor disputes. We propose a new approach leveraging the co-citation of legal articles within cases to establish similarity and enable algorithmic annotation. This method draws a parallel to the concept of case co-citation, utilizing cited precedents as indicators of shared legal issues. To evaluate the labeled results, we employ a system that recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation demonstrates that the recommender, with finetuned text embedding models and a reasonable BiLSTM module can recommend labor cases whose similarity was measured by the co-citation of the legal articles. This research contributes to the development of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases."
      },
      {
        "id": "oai:arXiv.org:2504.20343v1",
        "title": "MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation",
        "link": "https://arxiv.org/abs/2504.20343",
        "author": "Amaan Izhar, Nurul Japar, Norisma Idris, Ting Dang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20343v1 Announce Type: new \nAbstract: Medical image reporting (MIR) aims to generate structured clinical descriptions from radiological images. Existing methods struggle with fine-grained feature extraction, multimodal alignment, and generalization across diverse imaging types, often relying on vanilla transformers and focusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language mixture-of-experts model with gated cross-aligned fusion, designed to address these limitations. Our architecture includes: (i) a multiscale vision encoder (MSVE) for capturing anatomical details at varying resolutions, (ii) a multihead dual-branch latent attention (MDLA) module for vision-language alignment through latent bottleneck representations, and (iii) a modulated mixture-of-experts (MoE) decoder for adaptive expert specialization. We extend MIR to CT scans, retinal imaging, MRI scans, and gross pathology images, reporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets. Extensive experiments and ablations confirm improved clinical accuracy, cross-modal alignment, and model interpretability. Code is available at https://github.com/AI-14/micar-vl-moe."
      },
      {
        "id": "oai:arXiv.org:2504.20355v1",
        "title": "Local Prompt Optimization",
        "link": "https://arxiv.org/abs/2504.20355",
        "author": "Yash Jain, Vishal Chowdhary",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20355v1 Announce Type: new \nAbstract: In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods."
      },
      {
        "id": "oai:arXiv.org:2504.20356v1",
        "title": "What Causes Knowledge Loss in Multilingual Language Models?",
        "link": "https://arxiv.org/abs/2504.20356",
        "author": "Maria Khelli, Samuel Cahyawijaya, Ayu Purwarianti, Genta Indra Winata",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20356v1 Announce Type: new \nAbstract: Cross-lingual transfer in natural language processing (NLP) models enhances multilingual performance by leveraging shared linguistic knowledge. However, traditional methods that process all data simultaneously often fail to mimic real-world scenarios, leading to challenges like catastrophic forgetting, where fine-tuning on new tasks degrades performance on previously learned ones. Our study explores this issue in multilingual contexts, focusing on linguistic differences affecting representational learning rather than just model parameters. We experiment with 52 languages using LoRA adapters of varying ranks to evaluate non-shared, partially shared, and fully shared parameters. Our aim is to see if parameter sharing through adapters can mitigate forgetting while preserving prior knowledge. We find that languages using non-Latin scripts are more susceptible to catastrophic forgetting, whereas those written in Latin script facilitate more effective cross-lingual transfer."
      },
      {
        "id": "oai:arXiv.org:2504.20362v1",
        "title": "TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots",
        "link": "https://arxiv.org/abs/2504.20362",
        "author": "Qinhua Xie, Hao Tang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20362v1 Announce Type: new \nAbstract: With the increasing use of surgical robots in clinical practice, enhancing their ability to process multimodal medical images has become a key research challenge. Although traditional medical image fusion methods have made progress in improving fusion accuracy, they still face significant challenges in real-time performance, fine-grained feature extraction, and edge preservation.In this paper, we introduce TTTFusion, a Test-Time Training (TTT)-based image fusion strategy that dynamically adjusts model parameters during inference to efficiently fuse multimodal medical images. By adapting the model during the test phase, our method optimizes the parameters based on the input image data, leading to improved accuracy and better detail preservation in the fusion results.Experimental results demonstrate that TTTFusion significantly enhances the fusion quality of multimodal images compared to traditional fusion methods, particularly in fine-grained feature extraction and edge preservation. This approach not only improves image fusion accuracy but also offers a novel technical solution for real-time image processing in surgical robots."
      },
      {
        "id": "oai:arXiv.org:2504.20370v1",
        "title": "ABO: Abandon Bayer Filter for Adaptive Edge Offloading in Responsive Augmented Reality",
        "link": "https://arxiv.org/abs/2504.20370",
        "author": "Yongxuan Han, Shengzhong Liu, Fan Wu, Guihai Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20370v1 Announce Type: new \nAbstract: Bayer-patterned color filter array (CFA) has been the go-to solution for color image sensors. In augmented reality (AR), although color interpolation (i.e., demosaicing) of pre-demosaic RAW images facilitates a user-friendly rendering, it creates no benefits in offloaded DNN analytics but increases the image channels by 3 times inducing higher transmission overheads. The potential optimization in frame preprocessing of DNN offloading is yet to be investigated. To that end, we propose ABO, an adaptive RAW frame offloading framework that parallelizes demosaicing with DNN computation. Its contributions are three-fold: First, we design a configurable tile-wise RAW image neural codec to compress frame sizes while sustaining downstream DNN accuracy under bandwidth constraints. Second, based on content-aware tiles-in-frame selection and runtime bandwidth estimation, a dynamic transmission controller adaptively calibrates codec configurations to maximize the DNN accuracy. Third, we further optimize the system pipelining to achieve lower end-to-end frame processing latency and higher throughput. Through extensive evaluations on a prototype platform, ABO consistently achieves 40% more frame processing throughput and 30% less end-to-end latency while improving the DNN accuracy by up to 15% than SOTA baselines. It also exhibits improved robustness against dim lighting and motion blur situations."
      },
      {
        "id": "oai:arXiv.org:2504.20371v1",
        "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation",
        "link": "https://arxiv.org/abs/2504.20371",
        "author": "Zhibo Man, Yuanmeng Chen, Yujie Zhang, Yufeng Chen, Jinan Xu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20371v1 Announce Type: new \nAbstract: Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory; the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three critical aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompting templates, and (3) we design precise disambiguation metrics, and study the efficacy of various prompting strategies on multiple state-of-the-art LLMs. Our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.20375v1",
        "title": "Generative Learning for Slow Manifolds and Bifurcation Diagrams",
        "link": "https://arxiv.org/abs/2504.20375",
        "author": "Ellis R. Crabtree, Dimitris G. Giovanis, Nikolaos Evangelou, Juan M. Bello-Rivas, Ioannis G. Kevrekidis",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20375v1 Announce Type: new \nAbstract: In dynamical systems characterized by separation of time scales, the approximation of so called ``slow manifolds'', on which the long term dynamics lie, is a useful step for model reduction. Initializing on such slow manifolds is a useful step in modeling, since it circumvents fast transients, and is crucial in multiscale algorithms alternating between fine scale (fast) and coarser scale (slow) simulations. In a similar spirit, when one studies the infinite time dynamics of systems depending on parameters, the system attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling these manifolds gives us representative attractors (here, steady states of ODEs or PDEs) at different parameter values. Algorithms for the systematic construction of these manifolds are required parts of the ``traditional'' numerical nonlinear dynamics toolkit.\n  In more recent years, as the field of Machine Learning develops, conditional score-based generative models (cSGMs) have demonstrated capabilities in generating plausible data from target distributions that are conditioned on some given label. It is tempting to exploit such generative models to produce samples of data distributions conditioned on some quantity of interest (QoI). In this work, we present a framework for using cSGMs to quickly (a) initialize on a low-dimensional (reduced-order) slow manifold of a multi-time-scale system consistent with desired value(s) of a QoI (a ``label'') on the manifold, and (b) approximate steady states in a bifurcation diagram consistent with a (new, out-of-sample) parameter value. This conditional sampling can help uncover the geometry of the reduced slow-manifold and/or approximately ``fill in'' missing segments of steady states in a bifurcation diagram."
      },
      {
        "id": "oai:arXiv.org:2504.20376v1",
        "title": "Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems",
        "link": "https://arxiv.org/abs/2504.20376",
        "author": "Shiqian Zhao, Jiayang Liu, Yiming Li, Runyi Hu, Xiaojun Jia, Wenshu Fan, Xinfeng Li, Jie Zhang, Wei Dong, Tianwei Zhang, Luu Anh Tuan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20376v1 Announce Type: new \nAbstract: Currently, the memory mechanism has been widely and successfully exploited in online text-to-image (T2I) generation systems ($e.g.$, DALL$\\cdot$E 3) for alleviating the growing tokenization burden and capturing key information in multi-turn interactions. Despite its practicality, its security analyses have fallen far behind. In this paper, we reveal that this mechanism exacerbates the risk of jailbreak attacks. Different from previous attacks that fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or may generate non-unsafe images due to under- or over-optimization, we propose Inception, the first multi-turn jailbreak attack against the memory mechanism in real-world text-to-image generation systems. Inception embeds the malice at the inception of the chat session turn by turn, leveraging the mechanism that T2I generation systems retrieve key information in their memory. Specifically, Inception mainly consists of two modules. It first segments the unsafe prompt into chunks, which are subsequently fed to the system in multiple turns, serving as pseudo-gradients for directive optimization. Specifically, we develop a series of segmentation policies that ensure the images generated are semantically consistent with the target prompt. Secondly, after segmentation, to overcome the challenge of the inseparability of minimum unsafe words, we propose recursion, a strategy that makes minimum unsafe words subdivisible. Collectively, segmentation and recursion ensure that all the request prompts are benign but can lead to malicious outcomes. We conduct experiments on the real-world text-to-image generation system ($i.e.$, DALL$\\cdot$E 3) to validate the effectiveness of Inception. The results indicate that Inception surpasses the state-of-the-art by a 14\\% margin in attack success rate."
      },
      {
        "id": "oai:arXiv.org:2504.20378v1",
        "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views",
        "link": "https://arxiv.org/abs/2504.20378",
        "author": "Jiang Wu, Rui Li, Yu Zhu, Rong Guo, Jinqiu Sun, Yanning Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20378v1 Announce Type: new \nAbstract: We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\\times$ faster than the NeRF-based fine-tuning approach."
      },
      {
        "id": "oai:arXiv.org:2504.20379v1",
        "title": "GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.20379",
        "author": "Jongwon Lee, Timothy Bretl",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20379v1 Announce Type: new \nAbstract: In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5{\\deg} in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset."
      },
      {
        "id": "oai:arXiv.org:2504.20383v1",
        "title": "Neural Stereo Video Compression with Hybrid Disparity Compensation",
        "link": "https://arxiv.org/abs/2504.20383",
        "author": "Shiyin Jiang, Zhenghao Chen, Minghao Han, Xingyu Zhou, Leheng Zhang, Shuhang Gu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20383v1 Announce Type: new \nAbstract: Disparity compensation represents the primary strategy in stereo video compression (SVC) for exploiting cross-view redundancy. These mechanisms can be broadly categorized into two types: one that employs explicit horizontal shifting, and another that utilizes an implicit cross-attention mechanism to reduce cross-view disparity redundancy. In this work, we propose a hybrid disparity compensation (HDC) strategy that leverages explicit pixel displacement as a robust prior feature to simplify optimization and perform implicit cross-attention mechanisms for subsequent warping operations, thereby capturing a broader range of disparity information. Specifically, HDC first computes a similarity map by fusing the horizontally shifted cross-view features to capture pixel displacement information. This similarity map is then normalized into an \"explicit pixel-wise attention score\" to perform the cross-attention mechanism, implicitly aligning features from one view to another. Building upon HDC, we introduce a novel end-to-end optimized neural stereo video compression framework, which integrates HDC-based modules into key coding operations, including cross-view feature extraction and reconstruction (HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on SVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both autonomous driving and general scenes, demonstrate that our framework outperforms both neural and traditional SVC methodologies."
      },
      {
        "id": "oai:arXiv.org:2504.20384v1",
        "title": "FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding",
        "link": "https://arxiv.org/abs/2504.20384",
        "author": "Yanan Guo, Wenhui Dong, Jun Song, Shiding Zhu, Xuan Zhang, Hanqing Yang, Yingbo Wang, Yang Du, Xianing Chen, Bo Zheng",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20384v1 Announce Type: new \nAbstract: Recent advancements in video understanding within visual large language models (VLLMs) have led to notable progress. However, the complexity of video data and contextual processing limitations still hinder long-video comprehension. A common approach is video feature compression to reduce token input to large language models, yet many methods either fail to prioritize essential features, leading to redundant inter-frame information, or introduce computationally expensive modules.To address these issues, we propose FiLA(Fine-grained Vision Language Model)-Video, a novel framework that leverages a lightweight dynamic-weight multi-frame fusion strategy, which adaptively integrates multiple frames into a single representation while preserving key video information and reducing computational costs. To enhance frame selection for fusion, we introduce a keyframe selection strategy, effectively identifying informative frames from a larger pool for improved summarization. Additionally, we present a simple yet effective long-video training data generation strategy, boosting model performance without extensive manual annotation. Experimental results demonstrate that FiLA-Video achieves superior efficiency and accuracy in long-video comprehension compared to existing methods."
      },
      {
        "id": "oai:arXiv.org:2504.20390v1",
        "title": "Manifold Clustering with Schatten p-norm Maximization",
        "link": "https://arxiv.org/abs/2504.20390",
        "author": "Fangfang Li, Quanxue Gao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20390v1 Announce Type: new \nAbstract: Manifold clustering, with its exceptional ability to capture complex data structures, holds a pivotal position in cluster analysis. However, existing methods often focus only on finding the optimal combination between K-means and manifold learning, and overlooking the consistency between the data structure and labels. To address this issue, we deeply explore the relationship between K-means and manifold learning, and on this basis, fuse them to develop a new clustering framework. Specifically, the algorithm uses labels to guide the manifold structure and perform clustering on it, which ensures the consistency between the data structure and labels. Furthermore, in order to naturally maintain the class balance in the clustering process, we maximize the Schatten p-norm of labels, and provide a theoretical proof to support this. Additionally, our clustering framework is designed to be flexible and compatible with many types of distance functions, which facilitates efficient processing of nonlinear separable data. The experimental results of several databases confirm the superiority of our proposed model."
      },
      {
        "id": "oai:arXiv.org:2504.20408v1",
        "title": "FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation",
        "link": "https://arxiv.org/abs/2504.20408",
        "author": "Jae Yong Lee, Gwang Jae Jung, Byung Chan Lim, Hyung Ju Hwang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20408v1 Announce Type: new \nAbstract: The Boltzmann equation, a fundamental model in kinetic theory, describes the evolution of particle distribution functions through a nonlinear, high-dimensional collision operator. However, its numerical solution remains computationally demanding, particularly for inelastic collisions and high-dimensional velocity domains. In this work, we propose the Fourier Neural Spectral Network (FourierSpecNet), a hybrid framework that integrates the Fourier spectral method with deep learning to approximate the collision operator in Fourier space efficiently. FourierSpecNet achieves resolution-invariant learning and supports zero-shot super-resolution, enabling accurate predictions at unseen resolutions without retraining. Beyond empirical validation, we establish a consistency result showing that the trained operator converges to the spectral solution as the discretization is refined. We evaluate our method on several benchmark cases, including Maxwellian and hard-sphere molecular models, as well as inelastic collision scenarios. The results demonstrate that FourierSpecNet offers competitive accuracy while significantly reducing computational cost compared to traditional spectral solvers. Our approach provides a robust and scalable alternative for solving the Boltzmann equation across both elastic and inelastic regimes."
      },
      {
        "id": "oai:arXiv.org:2504.20409v1",
        "title": "GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation",
        "link": "https://arxiv.org/abs/2504.20409",
        "author": "Jingfeng Guo, Jinnan Chen, Weikai Chen, Zhenyu Sun, Lanjiong Li, Baozhu Zhao, Lingting Zhu, Xin Wang, Qi Liu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20409v1 Announce Type: new \nAbstract: This work presents GarmentX, a novel framework for generating diverse, high-fidelity, and wearable 3D garments from a single input image. Traditional garment reconstruction methods directly predict 2D pattern edges and their connectivity, an overly unconstrained approach that often leads to severe self-intersections and physically implausible garment structures. In contrast, GarmentX introduces a structured and editable parametric representation compatible with GarmentCode, ensuring that the decoded sewing patterns always form valid, simulation-ready 3D garments while allowing for intuitive modifications of garment shape and style. To achieve this, we employ a masked autoregressive model that sequentially predicts garment parameters, leveraging autoregressive modeling for structured generation while mitigating inconsistencies in direct pattern prediction. Additionally, we introduce GarmentX dataset, a large-scale dataset of 378,682 garment parameter-image pairs, constructed through an automatic data generation pipeline that synthesizes diverse and high-quality garment images conditioned on parametric garment representations. Through integrating our method with GarmentX dataset, we achieve state-of-the-art performance in geometric fidelity and input image alignment, significantly outperforming prior approaches. We will release GarmentX dataset upon publication."
      },
      {
        "id": "oai:arXiv.org:2504.20411v1",
        "title": "ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes",
        "link": "https://arxiv.org/abs/2504.20411",
        "author": "Amartya Mukherjee, Ruizhi Deng, He Zhao, Yuzhen Mao, Leonid Sigal, Frederick Tung",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20411v1 Announce Type: new \nAbstract: This work introduces a novel approach to modeling temporal point processes using diffusion models with an asynchronous noise schedule. At each step of the diffusion process, the noise schedule injects noise of varying scales into different parts of the data. With a careful design of the noise schedules, earlier events are generated faster than later ones, thus providing stronger conditioning for forecasting the more distant future. We derive an objective to effectively train these models for a general family of noise schedules based on conditional flow matching. Our method models the joint distribution of the latent representations of events in a sequence and achieves state-of-the-art results in predicting both the next inter-event time and event type on benchmark datasets. Additionally, it flexibly accommodates varying lengths of observation and prediction windows in different forecasting settings by adjusting the starting and ending points of the generation process. Finally, our method shows superior performance in long-horizon prediction tasks, outperforming existing baseline methods."
      },
      {
        "id": "oai:arXiv.org:2504.20419v1",
        "title": "Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks",
        "link": "https://arxiv.org/abs/2504.20419",
        "author": "Konstantinos I. Roumeliotis, Ranjan Sapkota, Manoj Karkee, Nikolaos D. Tselikas, Dimitrios K. Nasiopoulos",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20419v1 Announce Type: new \nAbstract: Automation in agriculture plays a vital role in addressing challenges related to crop monitoring and disease management, particularly through early detection systems. This study investigates the effectiveness of combining multimodal Large Language Models (LLMs), specifically GPT-4o, with Convolutional Neural Networks (CNNs) for automated plant disease classification using leaf imagery. Leveraging the PlantVillage dataset, we systematically evaluate model performance across zero-shot, few-shot, and progressive fine-tuning scenarios. A comparative analysis between GPT-4o and the widely used ResNet-50 model was conducted across three resolutions (100, 150, and 256 pixels) and two plant species (apple and corn). Results indicate that fine-tuned GPT-4o models achieved slightly better performance compared to the performance of ResNet-50, achieving up to 98.12% classification accuracy on apple leaf images, compared to 96.88% achieved by ResNet-50, with improved generalization and near-zero training loss. However, zero-shot performance of GPT-4o was significantly lower, underscoring the need for minimal training. Additional evaluations on cross-resolution and cross-plant generalization revealed the models' adaptability and limitations when applied to new domains. The findings highlight the promise of integrating multimodal LLMs into automated disease detection pipelines, enhancing the scalability and intelligence of precision agriculture systems while reducing the dependence on large, labeled datasets and high-resolution sensor infrastructure. Large Language Models, Vision Language Models, LLMs and CNNs, Disease Detection with Vision Language Models, VLMs"
      },
      {
        "id": "oai:arXiv.org:2504.20421v1",
        "title": "Understanding GNNs and Homophily in Dynamic Node Classification",
        "link": "https://arxiv.org/abs/2504.20421",
        "author": "Michael Ito, Danai Koutra, Jenna Wiens",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20421v1 Announce Type: new \nAbstract: Homophily, as a measure, has been critical to increasing our understanding of graph neural networks (GNNs). However, to date this measure has only been analyzed in the context of static graphs. In our work, we explore homophily in dynamic settings. Focusing on graph convolutional networks (GCNs), we demonstrate theoretically that in dynamic settings, current GCN discriminative performance is characterized by the probability that a node's future label is the same as its neighbors' current labels. Based on this insight, we propose dynamic homophily, a new measure of homophily that applies in the dynamic setting. This new measure correlates with GNN discriminative performance and sheds light on how to potentially design more powerful GNNs for dynamic graphs. Leveraging a variety of dynamic node classification datasets, we demonstrate that popular GNNs are not robust to low dynamic homophily. Going forward, our work represents an important step towards understanding homophily and GNN performance in dynamic node classification."
      },
      {
        "id": "oai:arXiv.org:2504.20430v1",
        "title": "Learning Laplacian Positional Encodings for Heterophilous Graphs",
        "link": "https://arxiv.org/abs/2504.20430",
        "author": "Michael Ito, Jiong Zhu, Dexiong Chen, Danai Koutra, Jenna Wiens",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20430v1 Announce Type: new \nAbstract: In this work, we theoretically demonstrate that current graph positional encodings (PEs) are not beneficial and could potentially hurt performance in tasks involving heterophilous graphs, where nodes that are close tend to have different labels. This limitation is critical as many real-world networks exhibit heterophily, and even highly homophilous graphs can contain local regions of strong heterophily. To address this limitation, we propose Learnable Laplacian Positional Encodings (LLPE), a new PE that leverages the full spectrum of the graph Laplacian, enabling them to capture graph structure on both homophilous and heterophilous graphs. Theoretically, we prove LLPE's ability to approximate a general class of graph distances and demonstrate its generalization properties. Empirically, our evaluation on 12 benchmarks demonstrates that LLPE improves accuracy across a variety of GNNs, including graph transformers, by up to 35% and 14% on synthetic and real-world graphs, respectively. Going forward, our work represents a significant step towards developing PEs that effectively capture complex structures in heterophilous graphs."
      },
      {
        "id": "oai:arXiv.org:2504.20435v1",
        "title": "AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries",
        "link": "https://arxiv.org/abs/2504.20435",
        "author": "Love Panta, Suraj Prasai, Karishma Malla Vaidya, Shyam Shrestha, Suresh Manandhar",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20435v1 Announce Type: new \nAbstract: Cervical cancer remains a significant health challenge, with high incidence and mortality rates, particularly in transitioning countries. Conventional Liquid-Based Cytology(LBC) is a labor-intensive process, requires expert pathologists and is highly prone to errors, highlighting the need for more efficient screening methods. This paper introduces an innovative approach that integrates low-cost biological microscopes with our simple and efficient AI algorithms for automated whole-slide analysis. Our system uses a motorized microscope to capture cytology images, which are then processed through an AI pipeline involving image stitching, cell segmentation, and classification. We utilize the lightweight UNet-based model involving human-in-the-loop approach to train our segmentation model with minimal ROIs. CvT-based classification model, trained on the SIPaKMeD dataset, accurately categorizes five cell types. Our framework offers enhanced accuracy and efficiency in cervical cancer screening compared to various state-of-art methods, as demonstrated by different evaluation metrics."
      },
      {
        "id": "oai:arXiv.org:2504.20437v1",
        "title": "GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection",
        "link": "https://arxiv.org/abs/2504.20437",
        "author": "DiJia Su, Andrew Gu, Jane Xu, Yuandong Tian, Jiawei Zhao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20437v1 Announce Type: new \nAbstract: Large language models (LLMs) have revolutionized natural language understanding and generation but face significant memory bottlenecks during training. GaLore, Gradient Low-Rank Projection, addresses this issue by leveraging the inherent low-rank structure of weight gradients, enabling substantial memory savings without sacrificing performance. Recent works further extend GaLore from various aspects, including low-bit quantization and higher-order tensor structures. However, there are several remaining challenges for GaLore, such as the computational overhead of SVD for subspace updates and the integration with state-of-the-art training parallelization strategies (e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable GaLore framework that addresses these challenges and incorporates recent advancements. In addition, we demonstrate the scalability of GaLore 2 by pre-training Llama 7B from scratch using up to 500 billion training tokens, highlighting its potential impact on real LLM pre-training scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.20438v1",
        "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
        "link": "https://arxiv.org/abs/2504.20438",
        "author": "Ziyang Xu, Kangsheng Duan, Xiaolei Shen, Zhifeng Ding, Wenyu Liu, Xiaohu Ruan, Xiaoxin Chen, Xinggang Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20438v1 Announce Type: new \nAbstract: Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/projects/PixelHacker."
      },
      {
        "id": "oai:arXiv.org:2504.20442v1",
        "title": "Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework",
        "link": "https://arxiv.org/abs/2504.20442",
        "author": "Yuchen Wang, Pengfei Jia, Zhitao Shu, Keyan Liu, Abdul Rashid Mohamed Shariff",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20442v1 Announce Type: new \nAbstract: With the intensification of global climate change, accurate prediction of weather indicators is of great significance in disaster prevention and mitigation, agricultural production, and transportation. Precipitation, as one of the key meteorological indicators, plays a crucial role in water resource management, agricultural production, and urban flood control. This study proposes a multidimensional precipitation index prediction model based on a CNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation forecasts. The dataset is sourced from Pune, Maharashtra, India, covering monthly mean precipitation data from 1972 to 2002. This dataset includes nearly 31 years (1972-2002) of monthly average precipitation, reflecting the long-term fluctuations and seasonal variations of precipitation in the region. By analyzing these time series data, the CNN-LSTM model effectively captures local features and long-term dependencies. Experimental results show that the model achieves a root mean square error (RMSE) of 6.752, which demonstrates a significant advantage over traditional time series prediction methods in terms of prediction accuracy and generalization ability. Furthermore, this study provides new research ideas for precipitation prediction. However, the model requires high computational resources when dealing with large-scale datasets, and its predictive ability for multidimensional precipitation data still needs improvement. Future research could extend the model to support and predict multidimensional precipitation data, thereby promoting the development of more accurate and efficient meteorological prediction technologies."
      },
      {
        "id": "oai:arXiv.org:2504.20444v1",
        "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?",
        "link": "https://arxiv.org/abs/2504.20444",
        "author": "Mika H\\\"am\\\"al\\\"ainen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20444v1 Announce Type: new \nAbstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and Claude. We do this by repurposing the famous experiment Asch (1946) conducted using human subjects. The experiment is simple, given two candidates with equal descriptions which one is preferred if one description has positive adjectives first before negative ones and another description has negative adjectives followed by positive ones. We test this in two experiments. In one experiment, LLMs are given both candidates simultaneously in the same prompt, and in another experiment, LLMs are given both candidates separately. We test all the models with 200 candidate pairs. We found that, in the first experiment, ChatGPT preferred the candidate with positive adjectives listed first, while Gemini preferred both equally often. Claude refused to make a choice. In the second experiment, ChatGPT and Claude were most likely to rank both candidates equally. In the case where they did not give an equal rating, both showed a clear preference to a candidate that had negative adjectives listed first. Gemini was most likely to prefer a candidate with negative adjectives listed first."
      },
      {
        "id": "oai:arXiv.org:2504.20446v1",
        "title": "FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks",
        "link": "https://arxiv.org/abs/2504.20446",
        "author": "Wenjing Xiao, Wenhao Song, Miaojiang Chen, Ruikun Luo, Min Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20446v1 Announce Type: new \nAbstract: Intelligent fault-tolerant (FT) computing has recently demonstrated significant advantages of predicting and diagnosing faults in advance, enabling reliable service delivery. However, due to heterogeneity of fault knowledge and complex dependence relationships of time series log data, existing deep learning-based FT algorithms further improve detection performance relying on single neural network model with difficulty. To this end, we propose FT-MoE, a sustainable-learning mixture-of-experts model for fault-tolerant computing with multiple tasks, which enables different parameters learning distinct fault knowledge to achieve high-reliability for service system. Firstly, we use decoder-based transformer models to obtain fault prototype vectors of decoupling long-distance dependencies. Followed by, we present a dual mixture of experts networks for high-accurate prediction for both fault detection and classification tasks. Then, we design a two-stage optimization scheme of offline training and online tuning, which allows that in operation FT-MoE can also keep learning to adapt to dynamic service environments. Finally, to verify the effectiveness of FT-MoE, we conduct extensive experiments on the FT benchmark. Experimental results show that FT-MoE achieves superior performance compared to the state-of-the-art methods. Code will be available upon publication."
      },
      {
        "id": "oai:arXiv.org:2504.20451v1",
        "title": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs",
        "link": "https://arxiv.org/abs/2504.20451",
        "author": "Daniel Lee, Harsh Sharma, Jieun Han, Sunny Jeong, Alice Oh, Vered Shwartz",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20451v1 Announce Type: new \nAbstract: Translating knowledge-intensive and entity-rich text between English and Korean requires transcreation to preserve language-specific and cultural nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13 models (LLMs and MT models) using automatic metrics and human assessment by bilingual annotators. Our findings show LLMs outperform traditional MT systems but struggle with entity translation requiring cultural adaptation. By constructing an error taxonomy, we identify incorrect responses and entity name errors as key issues, with performance varying by entity type and popularity level. This work exposes gaps in automatic evaluation metrics and hope to enable future work in completing culturally-nuanced machine translation."
      },
      {
        "id": "oai:arXiv.org:2504.20456v1",
        "title": "Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding",
        "link": "https://arxiv.org/abs/2504.20456",
        "author": "Gabe Guo, Stefano Ermon",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20456v1 Announce Type: new \nAbstract: In arbitrary-order language models, it is an open question how to sample tokens in parallel from the correct joint distribution. With discrete diffusion models, the more tokens they generate in parallel, the less their predicted distributions adhere to the originally learned data distribution, as they rely on a conditional independence assumption that only works with infinitesimally small timesteps. We find that a different class of models, any-subset autoregressive models (AS-ARMs), holds the solution. As implied by the name, AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs support parallelized joint probability density estimation, allowing them to correct their own parallel-generated token distributions, via our Any-Subset Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of tokens from the correct joint distribution, with the number of neural network calls upper bounded by the number of tokens predicted. We empirically verify that ASSD speeds up language generation, without sacrificing quality. Furthermore, we provide a mathematically justified scheme for training AS-ARMs for generation, and show that AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling benchmark tasks, and nearly match the performance of models 50X larger on code generation. Our theoretical and empirical results indicate that the once-forgotten AS-ARMs are a promising direction of language modeling."
      },
      {
        "id": "oai:arXiv.org:2504.20466v1",
        "title": "LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs",
        "link": "https://arxiv.org/abs/2504.20466",
        "author": "Woo Yi Yang, Jiarui Wang, Sijing Wu, Huiyu Duan, Yuxin Zhu, Liu Yang, Kang Fu, Guangtao Zhai, Xiongkuo Min",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20466v1 Announce Type: new \nAbstract: The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication."
      },
      {
        "id": "oai:arXiv.org:2504.20468v1",
        "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception",
        "link": "https://arxiv.org/abs/2504.20468",
        "author": "Yuanchen Wu, Lu Zhang, Hang Yao, Junlong Du, Ke Yan, Shouhong Ding, Yunsheng Wu, Xiaoqiang Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20468v1 Announce Type: new \nAbstract: Large Vision-Language Models (LVLMs) have achieved impressive results across various cross-modal tasks. However, hallucinations, i.e., the models generating counterfactual responses, remain a challenge. Though recent studies have attempted to alleviate object perception hallucinations, they focus on the models' response generation, and overlooking the task question itself. This paper discusses the vulnerability of LVLMs in solving counterfactual presupposition questions (CPQs), where the models are prone to accept the presuppositions of counterfactual objects and produce severe hallucinatory responses. To this end, we introduce \"Antidote\", a unified, synthetic data-driven post-training framework for mitigating both types of hallucination above. It leverages synthetic data to incorporate factual priors into questions to achieve self-correction, and decouple the mitigation process into a preference optimization problem. Furthermore, we construct \"CP-Bench\", a novel benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce factual responses. Applied to the LLaVA series, Antidote can simultaneously enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR by 30-50%, all without relying on external supervision from stronger LVLMs or human feedback and introducing noticeable catastrophic forgetting issues."
      },
      {
        "id": "oai:arXiv.org:2504.20469v1",
        "title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models",
        "link": "https://arxiv.org/abs/2504.20469",
        "author": "Enfa Fane, Mihai Surdeanu, Eduardo Blanco, Steven R. Corman",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20469v1 Announce Type: new \nAbstract: Understanding how news narratives frame entities is crucial for studying media's impact on societal perceptions of events. In this paper, we evaluate the zero-shot capabilities of large language models (LLMs) in classifying framing roles. Through systematic experimentation, we assess the effects of input context, prompting strategies, and task decomposition. Our findings show that a hierarchical approach of first identifying broad roles and then fine-grained roles, outperforms single-step classification. We also demonstrate that optimal input contexts and prompts vary across task levels, highlighting the need for subtask-specific strategies. We achieve a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our approach. Our findings emphasize the importance of tailored prompt design and input context optimization for improving LLM performance in entity framing."
      },
      {
        "id": "oai:arXiv.org:2504.20471v1",
        "title": "The Estimation of Continual Causal Effect for Dataset Shifting Streams",
        "link": "https://arxiv.org/abs/2504.20471",
        "author": "Baining Chen, Yiming Zhang, Yuqiao Han, Ruyue Zhang, Ruihuan Du, Zhishuo Zhou, Zhengdan Zhu, Xun Liu, Jiecheng Guo",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20471v1 Announce Type: new \nAbstract: Causal effect estimation has been widely used in marketing optimization. The framework of an uplift model followed by a constrained optimization algorithm is popular in practice. To enhance performance in the online environment, the framework needs to be improved to address the complexities caused by temporal dataset shift. This paper focuses on capturing the dataset shift from user behavior and domain distribution changing over time. We propose an Incremental Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle this challenge. The ICE-PKD framework includes two components: (i) a multi-treatment uplift network that eliminates confounding bias using counterfactual regression; (ii) an incremental training strategy that adapts to the temporal dataset shift by updating with the latest data and protects generalization via replay-based knowledge distillation. We also revisit the uplift modeling metrics and introduce a novel metric for more precise online evaluation in multiple treatment scenarios. Extensive experiments on both simulated and online datasets show that the proposed framework achieves better performance. The ICE-PKD framework has been deployed in the marketing system of Huaxiaozhu, a ride-hailing platform in China."
      },
      {
        "id": "oai:arXiv.org:2504.20482v1",
        "title": "Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias",
        "link": "https://arxiv.org/abs/2504.20482",
        "author": "Chao Li, Changhua Zhou, Jia Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20482v1 Announce Type: new \nAbstract: Knowledge distillation typically transfers knowledge from a teacher model to a student model by minimizing differences between their output distributions. However, existing distillation approaches largely focus on mimicking absolute probabilities and neglect the valuable relational inductive biases embedded in the teacher's relative predictions, leading to exposure bias. In this paper, we propose Group Relative Knowledge Distillation (GRKD), a novel framework that distills teacher knowledge by learning the relative ranking among classes, rather than directly fitting the absolute distribution. Specifically, we introduce a group relative loss that encourages the student model to preserve the pairwise preference orderings provided by the teacher's outputs. Extensive experiments on classification benchmarks demonstrate that GRKD achieves superior generalization compared to existing methods, especially in tasks requiring fine-grained class differentiation. Our method provides a new perspective on exploiting teacher knowledge, focusing on relational structure rather than absolute likelihood."
      },
      {
        "id": "oai:arXiv.org:2504.20484v1",
        "title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training",
        "link": "https://arxiv.org/abs/2504.20484",
        "author": "Linjuan Wu, Haoran Wei, Huan Lin, Tianhao Li, Baosong Yang, Weiming Lu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20484v1 Announce Type: new \nAbstract: Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation."
      },
      {
        "id": "oai:arXiv.org:2504.20492v1",
        "title": "Triadic Closure-Heterogeneity-Harmony GCN for Link Prediction",
        "link": "https://arxiv.org/abs/2504.20492",
        "author": "Ke-ke Shang, Junfan Yi, Michael Small, Yijie Zhou",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20492v1 Announce Type: new \nAbstract: Link prediction aims to estimate the likelihood of connections between pairs of nodes in complex networks, which is beneficial to many applications from friend recommendation to metabolic network reconstruction. Traditional heuristic-based methodologies in the field of complex networks typically depend on predefined assumptions about node connectivity, limiting their generalizability across diverse networks. While recent graph neural network (GNN) approaches capture global structural features effectively, they often neglect node attributes and intrinsic structural relationships between node pairs. To address this, we propose TriHetGCN, an extension of traditional Graph Convolutional Networks (GCNs) that incorporates explicit topological indicators -- triadic closure and degree heterogeneity. TriHetGCN consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. The topology feature module constructs node features using shortest path distances to anchor nodes, enhancing global structure perception. The graph structural module integrates topological indicators into the GCN framework to model triadic closure and heterogeneity. The connection probability module uses deep learning to predict links. Evaluated on nine real-world datasets, from traditional networks without node attributes to large-scale networks with rich features, TriHetGCN achieves state-of-the-art performance, outperforming mainstream methods. This highlights its strong generalization across diverse network types, offering a promising framework that bridges statistical physics and graph deep learning."
      },
      {
        "id": "oai:arXiv.org:2504.20496v1",
        "title": "Large-scale visual SLAM for in-the-wild videos",
        "link": "https://arxiv.org/abs/2504.20496",
        "author": "Shuo Sun, Torsten Sattler, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20496v1 Announce Type: new \nAbstract: Accurate and robust 3D scene reconstruction from casual, in-the-wild videos can significantly simplify robot deployment to new environments. However, reliable camera pose estimation and scene reconstruction from such unconstrained videos remains an open challenge. Existing visual-only SLAM methods perform well on benchmark datasets but struggle with real-world footage which often exhibits uncontrolled motion including rapid rotations and pure forward movements, textureless regions, and dynamic objects. We analyze the limitations of current methods and introduce a robust pipeline designed to improve 3D reconstruction from casual videos. We build upon recent deep visual odometry methods but increase robustness in several ways. Camera intrinsics are automatically recovered from the first few frames using structure-from-motion. Dynamic objects and less-constrained areas are masked with a predictive model. Additionally, we leverage monocular depth estimates to regularize bundle adjustment, mitigating errors in low-parallax situations. Finally, we integrate place recognition and loop closure to reduce long-term drift and refine both intrinsics and pose estimates through global bundle adjustment. We demonstrate large-scale contiguous 3D models from several online videos in various environments. In contrast, baseline methods typically produce locally inconsistent results at several points, producing separate segments or distorted maps. In lieu of ground-truth pose data, we evaluate map consistency, execution time and visual accuracy of re-rendered NeRF models. Our proposed system establishes a new baseline for visual reconstruction from casual uncontrolled videos found online, demonstrating more consistent reconstructions over longer sequences of in-the-wild videos than previously achieved."
      },
      {
        "id": "oai:arXiv.org:2504.20498v1",
        "title": "Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection",
        "link": "https://arxiv.org/abs/2504.20498",
        "author": "Jianhong Han, Yupei Wang, Liang Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20498v1 Announce Type: new \nAbstract: Single-source Domain Generalization (SDG) in object detection aims to develop a detector using only data from a source domain that can exhibit strong generalization capability when applied to unseen target domains. Existing methods are built upon CNN-based detectors and primarily improve robustness by employing carefully designed data augmentation strategies integrated with feature alignment techniques. However, data augmentation methods have inherent drawbacks; they are only effective when the augmented sample distribution approximates or covers the unseen scenarios, thus failing to enhance generalization across all unseen domains. Furthermore, while the recent Detection Transformer (DETR) has demonstrated superior generalization capability in domain adaptation tasks due to its efficient global information extraction, its potential in SDG tasks remains unexplored. To this end, we introduce a strong DETR-based detector named the Style-Adaptive Detection Transformer (SA-DETR) for SDG in object detection. Specifically, we present a domain style adapter that projects the style representation of the unseen target domain into the training domain, enabling dynamic style adaptation. Then, we propose an object-aware contrastive learning module to guide the detector in extracting domain-invariant features through contrastive learning. By using object-aware gating masks to constrain feature aggregation in both spatial and semantic dimensions, this module achieves cross-domain contrast of instance-level features, thereby enhancing generalization. Extensive experiments demonstrate the superior performance and generalization capability of SA-DETR across five different weather scenarios. Code is released at https://github.com/h751410234/SA-DETR."
      },
      {
        "id": "oai:arXiv.org:2504.20500v1",
        "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation",
        "link": "https://arxiv.org/abs/2504.20500",
        "author": "Huimin Lu, Masaru Isonuma, Junichiro Mori, Ichiro Sakata",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20500v1 Announce Type: new \nAbstract: We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs). Previous detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the trade-off between detoxification efficacy and language modeling performance. In contrast, UniDetox provides a detoxification technique that can be universally applied to a wide range of LLMs without the need for separate model-specific tuning. Specifically, we propose a novel and efficient dataset distillation technique for detoxification using contrastive decoding. This approach distills detoxifying representations in the form of synthetic text data, enabling universal detoxification of any LLM through fine-tuning with the distilled text. Our experiments demonstrate that the detoxifying text distilled from GPT-2 can effectively detoxify larger models, including OPT, Falcon, and LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter tuning for each model, as a single hyperparameter configuration can be seamlessly applied across different models. Additionally, analysis of the detoxifying text reveals a reduction in politically biased content, providing insights into the attributes necessary for effective detoxification of LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.20509v1",
        "title": "MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification",
        "link": "https://arxiv.org/abs/2504.20509",
        "author": "Yichu Xu, Di Wang, Hongzan Jiao, Lefei Zhang, Liangpei Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20509v1 Announce Type: new \nAbstract: The Mamba model has recently demonstrated strong potential in hyperspectral image (HSI) classification, owing to its ability to perform context modeling with linear computational complexity. However, existing Mamba-based methods usually neglect the spectral and spatial directional characteristics related to heterogeneous objects in hyperspectral scenes, leading to limited classification performance. To address these issues, we propose MambaMoE, a novel spectral-spatial mixture-of-experts framework, representing the first MoE-based approach in the HSI classification community. Specifically, we design a Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation to enable adaptive spectral-spatial modeling. Furthermore, we introduce an uncertainty-guided corrective learning (UGCL) strategy to encourage the model's attention toward complex regions prone to prediction ambiguity. Extensive experiments on multiple public HSI benchmarks demonstrate that MambaMoE achieves state-of-the-art performance in both accuracy and efficiency compared to existing advanced approaches, especially for Mamba-based methods. Code will be released."
      },
      {
        "id": "oai:arXiv.org:2504.20510v1",
        "title": "SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects",
        "link": "https://arxiv.org/abs/2504.20510",
        "author": "Irina Ruzavina, Lisa Sophie Theis, Jesse Lemeer, Rutger de Groen, Leo Ebeling, Andrej Hulak, Jouaria Ali, Guangzhi Tang, Rico Mockel",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20510v1 Announce Type: new \nAbstract: Automating the quality control of shot-blasted steel surfaces is crucial for improving manufacturing efficiency and consistency. This study presents a dataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as either \"ready for paint\" or \"needs shot-blasting.\" The dataset captures real-world surface defects, including discoloration, welding lines, scratches and corrosion, making it well-suited for training computer vision models. Additionally, three classification approaches were evaluated: Compact Convolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50 feature extraction, and a Convolutional Autoencoder (CAE). The supervised methods (CCT and SVM) achieve 95% classification accuracy on the test set, with CCT leveraging transformer-based attention mechanisms and SVM offering a computationally efficient alternative. The CAE approach, while less effective, establishes a baseline for unsupervised quality control. We present interpretable decision-making by all three neural networks, allowing industry users to visually pinpoint problematic regions and understand the model's rationale. By releasing the dataset and baseline codes, this work aims to support further research in defect detection, advance the development of interpretable computer vision models for quality control, and encourage the adoption of automated inspection systems in industrial applications."
      },
      {
        "id": "oai:arXiv.org:2504.20518v1",
        "title": "Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models",
        "link": "https://arxiv.org/abs/2504.20518",
        "author": "Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20518v1 Announce Type: new \nAbstract: Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the $<$EOS$>$ token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens' attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across five representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at https://github.com/Robin-WZQ/DAA."
      },
      {
        "id": "oai:arXiv.org:2504.20522v1",
        "title": "Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification",
        "link": "https://arxiv.org/abs/2504.20522",
        "author": "Gissel Velarde, Tillman Weyde, David Meredith",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20522v1 Announce Type: new \nAbstract: The aim of this study is to evaluate a machine-learning method in which symbolic representations of folk songs are segmented and classified into tune families with Haar-wavelet filtering. The method is compared with previously proposed Gestalt-based method. Melodies are represented as discrete symbolic pitch-time signals. We apply the continuous wavelet transform (CWT) with the Haar wavelet at specific scales, obtaining filtered versions of melodies emphasizing their information at particular time-scales. We use the filtered signal for representation and segmentation, using the wavelet coefficients' local maxima to indicate local boundaries and classify segments by means of k-nearest neighbours based on standard vector-metrics (Euclidean, cityblock), and compare the results to a Gestalt-based segmentation method and metrics applied directly to the pitch signal. We found that the wavelet based segmentation and wavelet-filtering of the pitch signal lead to better classification accuracy in cross-validated evaluation when the time-scale and other parameters are optimized."
      },
      {
        "id": "oai:arXiv.org:2504.20525v1",
        "title": "Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection",
        "link": "https://arxiv.org/abs/2504.20525",
        "author": "Huan Zheng, Wencheng Han, Tianyi Yan, Cheng-zhong Xu, Jianbing Shen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20525v1 Announce Type: new \nAbstract: Monocular 3D lane detection aims to estimate 3D position of lanes from frontal-view (FV) images. However, current monocular 3D lane detection methods suffer from two limitations, including inaccurate geometric information of the predicted 3D lanes and difficulties in maintaining lane integrity. To address these issues, we seek to fully exploit the potential of multiple input frames. First, we aim at enhancing the ability to perceive the geometry of scenes by leveraging temporal geometric consistency. Second, we strive to improve the integrity of lanes by revealing more instance information from temporal sequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation Network (GTA-Net) for monocular 3D lane detection. On one hand, we develop the Temporal Geometry Enhancement Module (TGEM), which exploits geometric consistency across successive frames, facilitating effective geometry perception. On the other hand, we present the Temporal Instance-aware Query Generation (TIQG), which strategically incorporates temporal cues into query generation, thereby enabling the exploration of comprehensive instance information. Experiments demonstrate that our GTA-Net achieves SoTA results, surpassing existing monocular 3D lane detection solutions."
      },
      {
        "id": "oai:arXiv.org:2504.20530v1",
        "title": "Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer",
        "link": "https://arxiv.org/abs/2504.20530",
        "author": "Wenxuan Liu, Xian Zhong, Zhuo Zhou, Siyuan Yang, Chia-Wen Lin, Alex Chichung Kot",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20530v1 Announce Type: new \nAbstract: Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges due to significant view variations along the vertical spatial axis. Unlike traditional ground-based settings, UAVs capture actions from a wide range of altitudes, resulting in considerable appearance discrepancies. We introduce a multi-view formulation tailored to varying UAV altitudes and empirically observe a partial order among views, where recognition accuracy consistently decreases as the altitude increases. This motivates a novel approach that explicitly models the hierarchical structure of UAV views to improve recognition performance across altitudes. To this end, we propose the Partial Order Guided Multi-View Network (POG-MVNet), designed to address drastic view variations by effectively leveraging view-dependent information across different altitude levels. The framework comprises three key components: a View Partition (VP) module, which uses the head-to-body ratio to group views by altitude; an Order-aware Feature Decoupling (OFD) module, which disentangles action-relevant and view-specific features under partial order guidance; and an Action Partial Order Guide (APOG), which leverages the partial order to transfer informative knowledge from easier views to support learning in more challenging ones. We conduct experiments on Drone-Action, MOD20, and UAV datasets, demonstrating that POG-MVNet significantly outperforms competing methods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action dataset and a 3.5% improvement on UAV dataset compared to state-of-the-art methods ASAT and FAR. The code for POG-MVNet will be made available soon."
      },
      {
        "id": "oai:arXiv.org:2504.20532v1",
        "title": "TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution",
        "link": "https://arxiv.org/abs/2504.20532",
        "author": "Yue Li, Weizhi Liu, Dongdong Lin",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20532v1 Announce Type: new \nAbstract: The emergence of diffusion models has facilitated the generation of speech with reinforced fidelity and naturalness. While deepfake detection technologies have manifested the ability to identify AI-generated content, their efficacy decreases as generative models become increasingly sophisticated. Furthermore, current research in the field has not adequately addressed the necessity for robust watermarking to safeguard the intellectual property rights associated with synthetic speech and generative models. To remedy this deficiency, we propose a \\textbf{ro}bust generative \\textbf{s}peech wat\\textbf{e}rmarking method (TriniMark) for authenticating the generated content and safeguarding the copyrights by enabling the traceability of the diffusion model. We first design a structure-lightweight watermark encoder that embeds watermarks into the time-domain features of speech and reconstructs the waveform directly. A temporal-aware gated convolutional network is meticulously designed in the watermark decoder for bit-wise watermark recovery. Subsequently, the waveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion model, which leverages the transferability of watermarks and enables the diffusion model to incorporate watermark knowledge effectively. When an attacker trains a surrogate model using the outputs of the target model, the embedded watermark can still be learned by the surrogate model and correctly extracted. Comparative experiments with state-of-the-art methods demonstrate the superior robustness of our method, particularly in countering compound attacks."
      },
      {
        "id": "oai:arXiv.org:2504.20535v1",
        "title": "DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction",
        "link": "https://arxiv.org/abs/2504.20535",
        "author": "Chris Child, Lam Ngo",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20535v1 Announce Type: new \nAbstract: The DeeP-Mod framework builds an environment model using features from a Deep Dynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While Deep Q-Learning is effective in decision-making, state information is lost in deeper DQN layers due to mixed state-action representations. We address this by using Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures the output represents state values, not state-action pairs. Extracting features from the DDPN preserves state information, enabling task and action set independence. We show that a reduced DDPN can be trained using features extracted from the original DDPN trained on an identical problem. This reduced DDPN achieves faster convergence under noise and outperforms the original DDPN. Finally, we introduce the DeeP-Mod framework, which creates an environment model using the evolution of features extracted from a DDPN in response to actions. A second DDPN, which learns directly from this feature model rather than raw states, can learn an effective feature-value representation and thus optimal policy. A key advantage of DeeP-Mod is that an externally defined environment model is not needed at any stage, making DDPN applicable to a wide range of environments."
      },
      {
        "id": "oai:arXiv.org:2504.20541v1",
        "title": "Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study",
        "link": "https://arxiv.org/abs/2504.20541",
        "author": "Daniele Pannone, Danilo Avola",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20541v1 Announce Type: new \nAbstract: This paper introduces a deep learning framework for generating point clouds from WiFi Channel State Information data. We employ a two-stage autoencoder approach: a PointNet autoencoder with convolutional layers for point cloud generation, and a Convolutional Neural Network autoencoder to map CSI data to a matching latent space. By aligning these latent spaces, our method enables accurate environmental point cloud reconstruction from WiFi data. Experimental results validate the effectiveness of our approach, highlighting its potential for wireless sensing and environmental mapping applications."
      },
      {
        "id": "oai:arXiv.org:2504.20547v1",
        "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records",
        "link": "https://arxiv.org/abs/2504.20547",
        "author": "Jesus Lovon (IRIT-IRIS), Thouria Ben-Haddi (IRIT-IRIS), Jules Di Scala (IRIT-IRIS), Jose G. Moreno (IRIT-IRIS), Lynda Tamine (IRIT-IRIS)",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20547v1 Announce Type: new \nAbstract: The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an openly available MIMIC-IV benchmark for electronic health records (EHRs) to address this issue. First, we integrate the MIMIC-IV data within the Hugging Face datasets library to allow an easy share and use of this collection. Second, we investigate the application of templates to convert EHR tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the mortality of patients task show that fine-tuned text-based models are competitive against robust tabular classifiers. In contrast, zero-shot LLMs struggle to leverage EHR representations. This study underlines the potential of text-based approaches in the medical field and highlights areas for further improvement."
      },
      {
        "id": "oai:arXiv.org:2504.20552v1",
        "title": "BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters",
        "link": "https://arxiv.org/abs/2504.20552",
        "author": "Baz Roland (LIASD), Kristina Malyseva (LIASD), Anna Pappa (LIASD), Tristan Cazenave (APA)",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20552v1 Announce Type: new \nAbstract: This project introduces BrAIcht, an AI conversational agent that creates dialogues in the distinctive style of the famous German playwright Bertolt Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7 billion parameters and a modified version of the base Llama2 suitable for German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of other German plays that are stylistically similar to Bertolt Brecht are used to form a more di-erse dataset. Due to the limited memory capacity, a parameterefficient fine-tuning technique called QLoRA is implemented to train the large language model. The results, based on BLEU score and perplexity, show very promising performance of BrAIcht in generating dialogues in the style of Bertolt Brecht."
      },
      {
        "id": "oai:arXiv.org:2504.20566v1",
        "title": "Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning",
        "link": "https://arxiv.org/abs/2504.20566",
        "author": "Shunjie Wen, Thomas Heinis, Dong-Wan Choi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20566v1 Announce Type: new \nAbstract: Online class-incremental learning (OCIL) focuses on gradually learning new classes (called plasticity) from a stream of data in a single-pass, while concurrently preserving knowledge of previously learned classes (called stability). The primary challenge in OCIL lies in maintaining a good balance between the knowledge of old and new classes within the continually updated model. Most existing methods rely on explicit knowledge interaction through experience replay, and often employ exclusive training separation to address bias problems. Nevertheless, it still remains a big challenge to achieve a well-balanced learner, as these methods often exhibit either reduced plasticity or limited stability due to difficulties in continually integrating knowledge in the OCIL setting. In this paper, we propose a novel replay-based method, called Balanced Online Incremental Learning (BOIL), which can achieve both high plasticity and stability, thus ensuring more balanced performance in OCIL. Our BOIL method proposes an inclusive training separation strategy using dual classifiers so that knowledge from both old and new classes can effectively be integrated into the model, while introducing implicit approaches for transferring knowledge across the two classifiers. Extensive experimental evaluations over three widely-used OCIL benchmark datasets demonstrate the superiority of BOIL, showing more balanced yet better performance compared to state-of-the-art replay-based OCIL methods."
      },
      {
        "id": "oai:arXiv.org:2504.20568v1",
        "title": "Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network",
        "link": "https://arxiv.org/abs/2504.20568",
        "author": "Danilo Avola, Federica Bruni, Gian Luca Foresti, Daniele Pannone, Amedeo Ranaldi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20568v1 Announce Type: new \nAbstract: Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze environments, enabling tasks such as tracking people, detecting intrusions, and recognizing gestures. The rise of this technology is driven by the IEEE 802.11bf standard and growing demand for tools that can ensure privacy and operate through obstacles. However, the performance of Wi-Fi sensing is heavily influenced by environmental conditions, especially when extracting spatial and temporal features from the surrounding scene. A key challenge is achieving robust generalization across domains, ensuring stable performance even when the sensing environment changes significantly. This paper introduces a novel deep learning model for cross-domain adaptation of Wi-Fi signals, inspired by physical signal shielding. The model uses a Relativistic average Generative Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM) architectures for both the generator and discriminator. To simulate physical shielding, an acrylic box lined with electromagnetic shielding fabric was constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from various materials both inside (domain-free) and outside (domain-dependent) the box to train the model. A multi-class Support Vector Machine (SVM) was trained on domain-free spectra and tested on signals denoised by the RaGAN. The system achieved 96% accuracy and demonstrated strong material discrimination capabilities, offering potential for use in security applications to identify concealed objects based on their composition."
      },
      {
        "id": "oai:arXiv.org:2504.20571v1",
        "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example",
        "link": "https://arxiv.org/abs/2504.20571",
        "author": "Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20571v1 Announce Type: new \nAbstract: We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR"
      },
      {
        "id": "oai:arXiv.org:2504.20579v1",
        "title": "Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects",
        "link": "https://arxiv.org/abs/2504.20579",
        "author": "Praharsh Nanavati, Ranjitha Prasad, Karthikeyan Shanmugam",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20579v1 Announce Type: new \nAbstract: Estimating treatment effects from observational data is challenging due to two main reasons: (a) hidden confounding, and (b) covariate mismatch (control and treatment groups not having identical distributions). Long lines of works exist that address only either of these issues. To address the former, conventional techniques that require detailed knowledge in the form of causal graphs have been proposed. For the latter, covariate matching and importance weighting methods have been used. Recently, there has been progress in combining testable independencies with partial side information for tackling hidden confounding. A common framework to address both hidden confounding and selection bias is missing. We propose neural architectures that aim to learn a representation of pre-treatment covariates that is a valid adjustment and also satisfies covariate matching constraints. We combine two different neural architectures: one based on gradient matching across domains created by subsampling a suitable anchor variable that assumes causal side information, followed by the other, a covariate matching transformation. We prove that approximately invariant representations yield approximate valid adjustment sets which would enable an interval around the true causal effect. In contrast to usual sensitivity analysis, where an unknown nuisance parameter is varied, we have a testable approximation yielding a bound on the effect estimate. We also outperform various baselines with respect to ATE and PEHE errors on causal benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd Management dataset."
      },
      {
        "id": "oai:arXiv.org:2504.20581v1",
        "title": "ClonEval: An Open Voice Cloning Benchmark",
        "link": "https://arxiv.org/abs/2504.20581",
        "author": "Iwona Christop, Tomasz Kuczy\\'nski, Marek Kubis",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20581v1 Announce Type: new \nAbstract: We present a novel benchmark for voice cloning text-to-speech models. The benchmark consists of an evaluation protocol, an open-source library for assessing the performance of voice cloning models, and an accompanying leaderboard. The paper discusses design considerations and presents a detailed description of the evaluation procedure. The usage of the software library is explained, along with the organization of results on the leaderboard."
      },
      {
        "id": "oai:arXiv.org:2504.20593v1",
        "title": "Independent Learning in Performative Markov Potential Games",
        "link": "https://arxiv.org/abs/2504.20593",
        "author": "Rilind Sahitaj, Paulius Sasnauskas, Yi\\u{g}it Yal{\\i}n, Debmalya Mandal, Goran Radanovi\\'c",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20593v1 Announce Type: new \nAbstract: Performative Reinforcement Learning (PRL) refers to a scenario in which the deployed policy changes the reward and transition dynamics of the underlying environment. In this work, we study multi-agent PRL by incorporating performative effects into Markov Potential Games (MPGs). We introduce the notion of a performatively stable equilibrium (PSE) and show that it always exists under a reasonable sensitivity assumption. We then provide convergence results for state-of-the-art algorithms used to solve MPGs. Specifically, we show that independent policy gradient ascent (IPGA) and independent natural policy gradient (INPG) converge to an approximate PSE in the best-iterate sense, with an additional term that accounts for the performative effects. Furthermore, we show that INPG asymptotically converges to a PSE in the last-iterate sense. As the performative effects vanish, we recover the convergence rates from prior work. For a special case of our game, we provide finite-time last-iterate convergence results for a repeated retraining approach, in which agents independently optimize a surrogate objective. We conduct extensive experiments to validate our theoretical findings."
      },
      {
        "id": "oai:arXiv.org:2504.20599v1",
        "title": "PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders",
        "link": "https://arxiv.org/abs/2504.20599",
        "author": "Qiaochu Wang, Chufeng Xiao, Manfred Lau, Hongbo Fu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20599v1 Announce Type: new \nAbstract: Learning-based methods to understand and model hand-object interactions (HOI) require a large amount of high-quality HOI data. One way to create HOI data is to transfer hand poses from a source object to another based on the objects' geometry. However, current methods for transferring hand poses between objects rely on shape matching, limiting the ability to transfer poses across different categories due to differences in their shapes and sizes. We observe that HOI often involves specific semantic parts of objects, which often have more consistent shapes across categories. In addition, constructing size-invariant correspondences between these parts is important for cross-category transfer. Based on these insights, we introduce a novel method PartHOI for part-based HOI transfer. Using a generalized cylinder representation to parameterize an object parts' geometry, PartHOI establishes a robust geometric correspondence between object parts, and enables the transfer of contact points. Given the transferred points, we optimize a hand pose to fit the target object well. Qualitative and quantitative results demonstrate that our method can generalize HOI transfers well even for cross-category objects, and produce high-fidelity results that are superior to the existing methods."
      },
      {
        "id": "oai:arXiv.org:2504.20602v1",
        "title": "Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection",
        "link": "https://arxiv.org/abs/2504.20602",
        "author": "Siwei Wang, Zhiwei Chen, Liujuan Cao, Rongrong Ji",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20602v1 Announce Type: new \nAbstract: Small object detection is a broadly investigated research task and is commonly conceptualized as a \"pipeline-style\" engineering process. In the upstream, images serve as raw materials for processing in the detection pipeline, where pre-trained models are employed to generate initial feature maps. In the midstream, an assigner selects training positive and negative samples. Subsequently, these samples and features are fed into the downstream for classification and regression. Previous small object detection methods often focused on improving isolated stages of the pipeline, thereby neglecting holistic optimization and consequently constraining overall performance gains. To address this issue, we have optimized three key aspects, namely Purifying, Labeling, and Utilizing, in this pipeline, proposing a high-quality Small object detection framework termed PLUSNet. Specifically, PLUSNet comprises three sequential components: the Hierarchical Feature Purifier (HFP) for purifying upstream features, the Multiple Criteria Label Assignment (MCLA) for improving the quality of midstream training samples, and the Frequency Decoupled Head (FDHead) for more effectively exploiting information to accomplish downstream tasks. The proposed PLUS modules are readily integrable into various object detectors, thus enhancing their detection capabilities in multi-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet consistently achieves significant and consistent improvements across multiple datasets for small object detection."
      },
      {
        "id": "oai:arXiv.org:2504.20605v1",
        "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models",
        "link": "https://arxiv.org/abs/2504.20605",
        "author": "Mihai Nadas, Laura Diosan, Andrei Piscoran, Andreea Tomescu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20605v1 Announce Type: new \nAbstract: Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models."
      },
      {
        "id": "oai:arXiv.org:2504.20607v1",
        "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian",
        "link": "https://arxiv.org/abs/2504.20607",
        "author": "Hao Tian, Rui Liu, Wen Shen, Yilong Hu, Zhihao Zheng, Xiaolin Qin",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20607v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model's performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians."
      },
      {
        "id": "oai:arXiv.org:2504.20609v1",
        "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks",
        "link": "https://arxiv.org/abs/2504.20609",
        "author": "Xinyu Yao, Mengdi Wang, Bo Chen, Xiaobing Zhao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20609v1 Announce Type: new \nAbstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial role in the inheritance and study of ancient literature. However, existing natural language processing models primarily optimize for Modern Chinese, resulting in inadequate performance on Classical Chinese. This paper presents a comprehensive solution for Classical Chinese language processing. By continuing pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we construct a large language model, WenyanGPT, which is specifically designed for Classical Chinese tasks. Additionally, we develop an evaluation benchmark dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that WenyanGPT significantly outperforms current advanced LLMs in various Classical Chinese tasks. We make the model's training data, instruction fine-tuning data\\footnote, and evaluation benchmark dataset publicly available to promote further research and development in the field of Classical Chinese processing."
      },
      {
        "id": "oai:arXiv.org:2504.20629v1",
        "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
        "link": "https://arxiv.org/abs/2504.20629",
        "author": "Jeongsoo Choi, Ji-Hoon Kim, Kim Sung-Bin, Tae-Hyun Oh, Joon Son Chung",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20629v1 Announce Type: new \nAbstract: In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT ."
      },
      {
        "id": "oai:arXiv.org:2504.20635v1",
        "title": "Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation",
        "link": "https://arxiv.org/abs/2504.20635",
        "author": "Bradley Segal, Joshua Fieggen, David Clifton, Lei Clifton",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20635v1 Announce Type: new \nAbstract: Ensuring the generalisability of clinical machine learning (ML) models across diverse healthcare settings remains a significant challenge due to variability in patient demographics, disease prevalence, and institutional practices. Existing model evaluation approaches often rely on real-world datasets, which are limited in availability, embed confounding biases, and lack the flexibility needed for systematic experimentation. Furthermore, while generative models aim for statistical realism, they often lack transparency and explicit control over factors driving distributional shifts. In this work, we propose a novel structured synthetic data framework designed for the controlled benchmarking of model robustness, fairness, and generalisability. Unlike approaches focused solely on mimicking observed data, our framework provides explicit control over the data generating process, including site-specific prevalence variations, hierarchical subgroup effects, and structured feature interactions. This enables targeted investigation into how models respond to specific distributional shifts and potential biases. Through controlled experiments, we demonstrate the framework's ability to isolate the impact of site variations, support fairness-aware audits, and reveal generalisation failures, particularly highlighting how model complexity interacts with site-specific effects. This work contributes a reproducible, interpretable, and configurable tool designed to advance the reliable deployment of ML in clinical settings."
      },
      {
        "id": "oai:arXiv.org:2504.20642v1",
        "title": "Decision-centric fairness: Evaluation and optimization for resource allocation problems",
        "link": "https://arxiv.org/abs/2504.20642",
        "author": "Simon De Vos, Jente Van Belle, Andres Algaba, Wouter Verbeke, Sam Verboven",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20642v1 Announce Type: new \nAbstract: Data-driven decision support tools play an increasingly central role in decision-making across various domains. In this work, we focus on binary classification models for predicting positive-outcome scores and deciding on resource allocation, e.g., credit scores for granting loans or churn propensity scores for targeting customers with a retention campaign. Such models may exhibit discriminatory behavior toward specific demographic groups through their predicted scores, potentially leading to unfair resource allocation. We focus on demographic parity as a fairness metric to compare the proportions of instances that are selected based on their positive outcome scores across groups. In this work, we propose a decision-centric fairness methodology that induces fairness only within the decision-making region -- the range of relevant decision thresholds on the score that may be used to decide on resource allocation -- as an alternative to a global fairness approach that seeks to enforce parity across the entire score distribution. By restricting the induction of fairness to the decision-making region, the proposed decision-centric approach avoids imposing overly restrictive constraints on the model, which may unnecessarily degrade the quality of the predicted scores. We empirically compare our approach to a global fairness approach on multiple (semi-synthetic) datasets to identify scenarios in which focusing on fairness where it truly matters, i.e., decision-centric fairness, proves beneficial."
      },
      {
        "id": "oai:arXiv.org:2504.20643v1",
        "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations",
        "link": "https://arxiv.org/abs/2504.20643",
        "author": "Moran Mizrahi, Chen Shani, Gabriel Stanovsky, Dan Jurafsky, Dafna Shahaf",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20643v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with creativity. In this paper, we introduce a novel approach that couples LLMs with structured representations and cognitively inspired manipulations to generate more creative and diverse ideas. Our notion of creativity goes beyond superficial token-level variations; rather, we explicitly recombine structured representations of existing ideas, allowing our algorithm to effectively explore the more abstract landscape of ideas. We demonstrate our approach in the culinary domain with DishCOVER, a model that generates creative recipes. Experiments comparing our model's results to those of GPT-4o show greater diversity. Domain expert evaluations reveal that our outputs, which are mostly coherent and feasible culinary creations, significantly surpass GPT-4o in terms of novelty, thus outperforming it in creative generation. We hope our work inspires further research into structured creativity in AI."
      },
      {
        "id": "oai:arXiv.org:2504.20644v1",
        "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection",
        "link": "https://arxiv.org/abs/2504.20644",
        "author": "Ziqing Fan, Siyuan Du, Shengchao Hu, Pingjie Wang, Li Shen, Ya Zhang, Dacheng Tao, Yanfeng Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20644v1 Announce Type: new \nAbstract: Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e.dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance. To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of $\\gamma$-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.20645v1",
        "title": "LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping",
        "link": "https://arxiv.org/abs/2504.20645",
        "author": "Weiqin Jiao, Hao Cheng, George Vosselman, Claudio Persello",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20645v1 Announce Type: new \nAbstract: Polygonal road outline extraction from high-resolution aerial images is an important task in large-scale topographic mapping, where roads are represented as vectorized polygons, capturing essential geometric features with minimal vertex redundancy. Despite its importance, no existing method has been explicitly designed for this task. While polygonal building outline extraction has been extensively studied, the unique characteristics of roads, such as branching structures and topological connectivity, pose challenges to these methods. To address this gap, we introduce LDPoly, the first dedicated framework for extracting polygonal road outlines from high-resolution aerial images. Our method leverages a novel Dual-Latent Diffusion Model with a Channel-Embedded Fusion Module, enabling the model to simultaneously generate road masks and vertex heatmaps. A tailored polygonization method is then applied to obtain accurate vectorized road polygons with minimal vertex redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which contains detailed polygonal annotations for various topographic objects in several Dutch regions. Our experiments include both in-region and cross-region evaluations, with the latter designed to assess the model's generalization performance on unseen regions. Quantitative and qualitative results demonstrate that LDPoly outperforms state-of-the-art polygon extraction methods across various metrics, including pixel-level coverage, vertex efficiency, polygon regularity, and road connectivity. We also design two new metrics to assess polygon simplicity and boundary smoothness. Moreover, this work represents the first application of diffusion models for extracting precise vectorized object outlines without redundant vertices from remote-sensing imagery, paving the way for future advancements in this field."
      },
      {
        "id": "oai:arXiv.org:2504.20648v1",
        "title": "SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data",
        "link": "https://arxiv.org/abs/2504.20648",
        "author": "Michael Ogezi, Freda Shi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20648v1 Announce Type: new \nAbstract: Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial relations are generally rare in widely used VL datasets, with only a few being well represented, while most form a long tail of underrepresented relations. This gap leaves VLMs ill-equipped to handle diverse spatial relationships. To bridge it, we construct a synthetic VQA dataset focused on spatial reasoning generated from hyper-detailed image descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset consists of 455k samples containing 3.4 million QA pairs. Trained on this dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements on spatial reasoning benchmarks, achieving up to a 49% performance gain on the What's Up benchmark, while maintaining strong results on general tasks. Our work narrows the gap between human and VLM spatial reasoning and makes VLMs more capable in real-world tasks such as robotics and navigation."
      },
      {
        "id": "oai:arXiv.org:2504.20650v1",
        "title": "RuleKit 2: Faster and simpler rule learning",
        "link": "https://arxiv.org/abs/2504.20650",
        "author": "Adam Gudy\\'s, Cezary Maszczyk, Joanna Badura, Adam Grzelak, Marek Sikora, {\\L}ukasz Wr\\'obel",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20650v1 Announce Type: new \nAbstract: Rules offer an invaluable combination of predictive and descriptive capabilities. Our package for rule-based data analysis, RuleKit, has proven its effectiveness in classification, regression, and survival problems. Here we present its second version. New algorithms and optimized implementations of those previously included, significantly improved the computational performance of our suite, reducing the analysis time of some data sets by two orders of magnitude. The usability of RuleKit 2 is provided by two new components: Python package and browser application with a graphical user interface. The former complies with scikit-learn, the most popular data mining library for Python, allowing RuleKit 2 to be straightforwardly integrated into existing data analysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license (https://github.com/adaa-polsl/RuleKit)"
      },
      {
        "id": "oai:arXiv.org:2504.20656v1",
        "title": "Federated learning, ethics, and the double black box problem in medical AI",
        "link": "https://arxiv.org/abs/2504.20656",
        "author": "Joshua Hatherley, Anders S{\\o}gaard, Angela Ballantyne, Ruben Pauwels",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20656v1 Announce Type: new \nAbstract: Federated learning (FL) is a machine learning approach that allows multiple devices or institutions to collaboratively train a model without sharing their local data with a third-party. FL is considered a promising way to address patient privacy concerns in medical artificial intelligence. The ethical risks of medical FL systems themselves, however, have thus far been underexamined. This paper aims to address this gap. We argue that medical FL presents a new variety of opacity -- federation opacity -- that, in turn, generates a distinctive double black box problem in healthcare AI. We highlight several instances in which the anticipated benefits of medical FL may be exaggerated, and conclude by highlighting key challenges that must be overcome to make FL ethically feasible in medicine."
      },
      {
        "id": "oai:arXiv.org:2504.20657v1",
        "title": "Image deidentification in the XNAT ecosystem: use cases and solutions",
        "link": "https://arxiv.org/abs/2504.20657",
        "author": "Alex Michie, Simon J Doran",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20657v1 Announce Type: new \nAbstract: XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT \"ecosystem\". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be \"over-aggressive\" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the \"answer key\" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\\%. (Abridged from original for arXiv submission)"
      },
      {
        "id": "oai:arXiv.org:2504.20658v1",
        "title": "TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks",
        "link": "https://arxiv.org/abs/2504.20658",
        "author": "Stefano Dell'Anna (University of Trento, Trento, Italy), Andrea Montibeller (University of Trento, Trento, Italy), Giulia Boato (University of Trento, Trento, Italy, Truebees srl, Trento, Italy)",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20658v1 Announce Type: new \nAbstract: AI-generated synthetic media are increasingly used in real-world scenarios, often with the purpose of spreading misinformation and propaganda through social media platforms, where compression and other processing can degrade fake detection cues. Currently, many forensic tools fail to account for these in-the-wild challenges. In this work, we introduce TrueFake, a large-scale benchmarking dataset of 600,000 images including top notch generative techniques and sharing via three different social networks. This dataset allows for rigorous evaluation of state-of-the-art fake image detectors under very realistic and challenging conditions. Through extensive experimentation, we analyze how social media sharing impacts detection performance, and identify current most effective detection and training strategies. Our findings highlight the need for evaluating forensic models in conditions that mirror real-world use."
      },
      {
        "id": "oai:arXiv.org:2504.20660v1",
        "title": "Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems",
        "link": "https://arxiv.org/abs/2504.20660",
        "author": "Sahil Tomar, Shamshe Alam, Sandeep Kumar, Amit Mathur",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20660v1 Announce Type: new \nAbstract: In this paper, a novel quantum classical hybrid framework is proposed that synergizes quantum with Classical Reinforcement Learning. By leveraging the inherent parallelism of quantum computing, the proposed approach generates robust Q tables and specialized turn cost estimations, which are then integrated with a classical Reinforcement Learning pipeline. The Classical Quantum fusion results in rapid convergence of training, reducing the training time significantly and improved adaptability in scenarios featuring static, dynamic, and moving obstacles. Simulator based evaluations demonstrate significant enhancements in path efficiency, trajectory smoothness, and mission success rates, underscoring the potential of framework for real time, autonomous navigation in complex and unpredictable environments. Furthermore, the proposed framework was tested beyond simulations on practical scenarios, including real world map data such as the IIT Delhi campus, reinforcing its potential for real time, autonomous navigation in complex and unpredictable environments."
      },
      {
        "id": "oai:arXiv.org:2504.20666v1",
        "title": "SFi-Former: Sparse Flow Induced Attention for Graph Transformer",
        "link": "https://arxiv.org/abs/2504.20666",
        "author": "Zhonghao Li, Ji Shi, Xinming Zhang, Miao Zhang, Bo Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20666v1 Announce Type: new \nAbstract: Graph Transformers (GTs) have demonstrated superior performance compared to traditional message-passing graph neural networks in many studies, especially in processing graph data with long-range dependencies. However, GTs tend to suffer from weak inductive bias, overfitting and over-globalizing problems due to the dense attention. In this paper, we introduce SFi-attention, a novel attention mechanism designed to learn sparse pattern by minimizing an energy function based on network flows with l1-norm regularization, to relieve those issues caused by dense attention. Furthermore, SFi-Former is accordingly devised which can leverage the sparse attention pattern of SFi-attention to generate sparse network flows beyond adjacency matrix of graph data. Specifically, SFi-Former aggregates features selectively from other nodes through flexible adaptation of the sparse attention, leading to a more robust model. We validate our SFi-Former on various graph datasets, especially those graph data exhibiting long-range dependencies. Experimental results show that our SFi-Former obtains competitive performance on GNN Benchmark datasets and SOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally, our model gives rise to smaller generalization gaps, which indicates that it is less prone to over-fitting. Click here for codes."
      },
      {
        "id": "oai:arXiv.org:2504.20667v1",
        "title": "Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability",
        "link": "https://arxiv.org/abs/2504.20667",
        "author": "Simone Piaggesi, Riccardo Guidotti, Fosca Giannotti, Dino Pedreschi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20667v1 Announce Type: new \nAbstract: Post-hoc explainability is essential for understanding black-box machine learning models. Surrogate-based techniques are widely used for local and global model-agnostic explanations but have significant limitations. Local surrogates capture non-linearities but are computationally expensive and sensitive to parameters, while global surrogates are more efficient but struggle with complex local behaviors. In this paper, we present ILLUME, a flexible and interpretable framework grounded in representation learning, that can be integrated with various surrogate models to provide explanations for any black-box classifier. Specifically, our approach combines a globally trained surrogate with instance-specific linear transformations learned with a meta-encoder to generate both local and global explanations. Through extensive empirical evaluations, we demonstrate the effectiveness of ILLUME in producing feature attributions and decision rules that are not only accurate but also robust and faithful to the black-box, thus providing a unified explanation framework that effectively addresses the limitations of traditional surrogate methods."
      },
      {
        "id": "oai:arXiv.org:2504.20668v1",
        "title": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages",
        "link": "https://arxiv.org/abs/2504.20668",
        "author": "Ivan Vykopal, Martin Hyben, Robert Moro, Michal Gregor, Jakub Simko",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20668v1 Announce Type: new \nAbstract: Online disinformation poses a global challenge, placing significant demands on fact-checkers who must verify claims efficiently to prevent the spread of false information. A major issue in this process is the redundant verification of already fact-checked claims, which increases workload and delays responses to newly emerging claims. This research introduces an approach that retrieves previously fact-checked claims, evaluates their relevance to a given input, and provides supplementary information to support fact-checkers. Our method employs large language models (LLMs) to filter irrelevant fact-checks and generate concise summaries and explanations, enabling fact-checkers to faster assess whether a claim has been verified before. In addition, we evaluate our approach through both automatic and human assessments, where humans interact with the developed tool to review its effectiveness. Our results demonstrate that LLMs are able to filter out many irrelevant fact-checks and, therefore, reduce effort and streamline the fact-checking process."
      },
      {
        "id": "oai:arXiv.org:2504.20669v1",
        "title": "Advance Fake Video Detection via Vision Transformers",
        "link": "https://arxiv.org/abs/2504.20669",
        "author": "Joy Battocchio (University of Trento, Trento, Italy), Stefano Dell'Anna (University of Trento, Trento, Italy), Andrea Montibeller (University of Trento, Trento, Italy), Giulia Boato (University of Trento, Trento, Italy, Truebees srl, Trento, Italy)",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20669v1 Announce Type: new \nAbstract: Recent advancements in AI-based multimedia generation have enabled the creation of hyper-realistic images and videos, raising concerns about their potential use in spreading misinformation. The widespread accessibility of generative techniques, which allow for the production of fake multimedia from prompts or existing media, along with their continuous refinement, underscores the urgent need for highly accurate and generalizable AI-generated media detection methods, underlined also by new regulations like the European Digital AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based fake image detection and extend this idea to video. We propose an {original} %innovative framework that effectively integrates ViT embeddings over time to enhance detection performance. Our method shows promising accuracy, generalization, and few-shot learning capabilities across a new, large and diverse dataset of videos generated using five open source generative techniques from the state-of-the-art, as well as a separate dataset containing videos produced by proprietary generative methods."
      },
      {
        "id": "oai:arXiv.org:2504.20670v1",
        "title": "FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection",
        "link": "https://arxiv.org/abs/2504.20670",
        "author": "Yao Xiao, Tingfa Xu, Yu Xin, Jianan Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20670v1 Announce Type: new \nAbstract: Embedded flight devices with visual capabilities have become essential for a wide range of applications. In aerial image detection, while many existing methods have partially addressed the issue of small target detection, challenges remain in optimizing small target detection and balancing detection accuracy with efficiency. These issues are key obstacles to the advancement of real-time aerial image detection. In this paper, we propose a new family of real-time detectors for aerial image detection, named FBRT-YOLO, to address the imbalance between detection accuracy and efficiency. Our method comprises two lightweight modules: Feature Complementary Mapping Module (FCM) and Multi-Kernel Perception Unit(MKP), designed to enhance object perception for small targets in aerial images. FCM focuses on alleviating the problem of information imbalance caused by the loss of small target information in deep networks. It aims to integrate spatial positional information of targets more deeply into the network,better aligning with semantic information in the deeper layers to improve the localization of small targets. We introduce MKP, which leverages convolutions with kernels of different sizes to enhance the relationships between targets of various scales and improve the perception of targets at different scales. Extensive experimental results on three major aerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that FBRT-YOLO outperforms various real-time detectors in terms of performance and speed."
      },
      {
        "id": "oai:arXiv.org:2504.20677v1",
        "title": "Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset",
        "link": "https://arxiv.org/abs/2504.20677",
        "author": "Paola Natalia Ca\\~nas, Alexander Diez, David Galva\\~n, Marcos Nieto, Igor Rodr\\'iguez",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20677v1 Announce Type: new \nAbstract: This paper presents a robust, occlusion-aware driver monitoring system (DMS) utilizing the Driver Monitoring Dataset (DMD). The system performs driver identification, gaze estimation by regions, and face occlusion detection under varying lighting conditions, including challenging low-light scenarios. Aligned with EuroNCAP recommendations, the inclusion of occlusion detection enhances situational awareness and system trustworthiness by indicating when the system's performance may be degraded. The system employs separate algorithms trained on RGB and infrared (IR) images to ensure reliable functioning. We detail the development and integration of these algorithms into a cohesive pipeline, addressing the challenges of working with different sensors and real-car implementation. Evaluation on the DMD and in real-world scenarios demonstrates the effectiveness of the proposed system, highlighting the superior performance of RGB-based models and the pioneering contribution of robust occlusion detection in DMS."
      },
      {
        "id": "oai:arXiv.org:2504.20678v1",
        "title": "Non-native Children's Automatic Speech Assessment Challenge (NOCASA)",
        "link": "https://arxiv.org/abs/2504.20678",
        "author": "Yaroslav Getman, Tam\\'as Gr\\'osz, Mikko Kurimo, Giampiero Salvi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20678v1 Announce Type: new \nAbstract: This paper presents the \"Non-native Children's Automatic Speech Assessment\" (NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA challenges participants to develop new systems that can assess single-word pronunciations of young second language (L2) learners as part of a gamified pronunciation training app. To achieve this, several issues must be addressed, most notably the limited nature of available training data and the highly unbalanced distribution among the pronunciation level categories. To expedite the development, we provide a pseudo-anonymized training data (TeflonNorL2), containing 10,334 recordings from 44 speakers attempting to pronounce 205 distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that should be given in the game). In addition to the data, two already trained systems are released as official baselines: an SVM classifier trained on the ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%."
      },
      {
        "id": "oai:arXiv.org:2504.20679v1",
        "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?",
        "link": "https://arxiv.org/abs/2504.20679",
        "author": "Wing Yan Li, Zeqiang Wang, Jon Johnson, Suparna De",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20679v1 Announce Type: new \nAbstract: Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science."
      },
      {
        "id": "oai:arXiv.org:2504.20682v1",
        "title": "OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation",
        "link": "https://arxiv.org/abs/2504.20682",
        "author": "Long Liu, Cihui Yang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20682v1 Announce Type: new \nAbstract: Table structure recognition is a key task in document analysis. However, the geometric deformation in deformed tables causes a weak correlation between content information and structure, resulting in downstream tasks not being able to obtain accurate content information. To obtain fine-grained spatial coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge response by Gradient Orientation-aware Extractor, combines a Heterogeneous Kernel Cross Fusion module and a scale-aware loss function to adapt to multi-scale objective features, and introduces mask-driven non-maximal suppression in the post-processing, which replaces the traditional bounding box suppression mechanism. Furthermore, we also propose a data generator, filling the gap in the dataset for fine-grained deformation table cell spatial coordinate localization, and derive a large-scale dataset named Deformation Wired Table (DWTAL). Experiments show that our proposed model demonstrates excellent segmentation accuracy on all mainstream instance segmentation models. The dataset and the source code are open source: https://github.com/justliulong/OGHFYOLO."
      },
      {
        "id": "oai:arXiv.org:2504.20685v1",
        "title": "Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion",
        "link": "https://arxiv.org/abs/2504.20685",
        "author": "Zesheng Wang, Alexandre Bruckert, Patrick Le Callet, Guangtao Zhai",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20685v1 Announce Type: new \nAbstract: Generating realistic listener facial motions in dyadic conversations remains challenging due to the high-dimensional action space and temporal dependency requirements. Existing approaches usually consider extracting 3D Morphable Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes the computational speed of the 3DMM a bottleneck, making it difficult to achieve real-time interactive responses. To tackle this problem, we propose Facial Action Diffusion (FAD), which introduces the diffusion methods from the field of image generation to achieve efficient facial action generation. We further build the Efficient Listener Network (ELNet) specially designed to accommodate both the visual and audio information of the speaker as input. Considering of FAD and ELNet, the proposed method learns effective listener facial motion representations and leads to improvements of performance over the state-of-the-art methods while reducing 99% computational time."
      },
      {
        "id": "oai:arXiv.org:2504.20687v1",
        "title": "What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models",
        "link": "https://arxiv.org/abs/2504.20687",
        "author": "Jan Kapar, Niklas Koenen, Martin Jullum",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20687v1 Announce Type: new \nAbstract: Evaluating synthetic tabular data is challenging, since they can differ from the real data in so many ways. There exist numerous metrics of synthetic data quality, ranging from statistical distances to predictive performance, often providing conflicting results. Moreover, they fail to explain or pinpoint the specific weaknesses in the synthetic data. To address this, we apply explainable AI (XAI) techniques to a binary detection classifier trained to distinguish real from synthetic data. While the classifier identifies distributional differences, XAI concepts such as feature importance and feature effects, analyzed through methods like permutation feature importance, partial dependence plots, Shapley values and counterfactual explanations, reveal why synthetic data are distinguishable, highlighting inconsistencies, unrealistic dependencies, or missing patterns. This interpretability increases transparency in synthetic data evaluation and provides deeper insights beyond conventional metrics, helping diagnose and improve synthetic data quality. We apply our approach to two tabular datasets and generative models, showing that it uncovers issues overlooked by standard evaluation techniques."
      },
      {
        "id": "oai:arXiv.org:2504.20690v1",
        "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
        "link": "https://arxiv.org/abs/2504.20690",
        "author": "Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20690v1 Announce Type: new \nAbstract: Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/."
      },
      {
        "id": "oai:arXiv.org:2504.20699v1",
        "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?",
        "link": "https://arxiv.org/abs/2504.20699",
        "author": "Evangelia Gogoulou, Shorouq Zahra, Liane Guillou, Luise D\\\"urlich, Joakim Nivre",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20699v1 Announce Type: new \nAbstract: A frequently observed problem with LLMs is their tendency to generate output that is nonsensical, illogical, or factually incorrect, often referred to broadly as hallucination. Building on the recently proposed HalluciGen task for hallucination detection and generation, we evaluate a suite of open-access LLMs on their ability to detect intrinsic hallucinations in two conditional generation tasks: translation and paraphrasing. We study how model performance varies across tasks and language and we investigate the impact of model size, instruction tuning, and prompt choice. We find that performance varies across models but is consistent across prompts. Finally, we find that NLI models perform comparably well, suggesting that LLM-based detectors are not the only viable option for this specific task."
      },
      {
        "id": "oai:arXiv.org:2504.20703v1",
        "title": "BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification",
        "link": "https://arxiv.org/abs/2504.20703",
        "author": "Foteini Papadopoulou, Osman Mutlu, Neris \\\"Ozen, Bas H. M. van der Velden, Iris Hendrickx, Ali H\\\"urriyeto\\u{g}lu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20703v1 Announce Type: new \nAbstract: This paper presents our system developed for the SemEval-2025 Task 9: The Food Hazard Detection Challenge. The shared task's objective is to evaluate explainable classification systems for classifying hazards and products in two levels of granularity from food recall incident reports. In this work, we propose text augmentation techniques as a way to improve poor performance on minority classes and compare their effect for each category on various transformer and machine learning models. We explore three word-level data augmentation techniques, namely synonym replacement, random word swapping, and contextual word insertion. The results show that transformer models tend to have a better overall performance. None of the three augmentation techniques consistently improved overall performance for classifying hazards and products. We observed a statistically significant improvement (P < 0.05) in the fine-grained categories when using the BERT model to compare the baseline with each augmented model. Compared to the baseline, the contextual words insertion augmentation improved the accuracy of predictions for the minority hazard classes by 6%. This suggests that targeted augmentation of minority classes can improve the performance of transformer models."
      },
      {
        "id": "oai:arXiv.org:2504.20708v1",
        "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think",
        "link": "https://arxiv.org/abs/2504.20708",
        "author": "Hasan Abed Al Kader Hammoud, Hani Itani, Bernard Ghanem",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20708v1 Announce Type: new \nAbstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner."
      },
      {
        "id": "oai:arXiv.org:2504.20733v1",
        "title": "Unsupervised Surrogate Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.20733",
        "author": "Simon Kl\\\"uttermann, Tim Katzke, Emmanuel M\\\"uller",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20733v1 Announce Type: new \nAbstract: In this paper, we study unsupervised anomaly detection algorithms that learn a neural network representation, i.e. regular patterns of normal data, which anomalies are deviating from. Inspired by a similar concept in engineering, we refer to our methodology as surrogate anomaly detection. We formalize the concept of surrogate anomaly detection into a set of axioms required for optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121 benchmark datasets, demonstrating its competitive performance against 19 existing methods, as well as the scalability and reliability of our method."
      },
      {
        "id": "oai:arXiv.org:2504.20734v1",
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities",
        "link": "https://arxiv.org/abs/2504.20734",
        "author": "Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20734v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines."
      },
      {
        "id": "oai:arXiv.org:2504.20735v1",
        "title": "Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency",
        "link": "https://arxiv.org/abs/2504.20735",
        "author": "Tariq Qayyum, Asadullah Tariq, Muhammad Ali, Mohamed Adel Serhani, Zouheir Trabelsi, Maite L\\'opez-S\\'anchez",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20735v1 Announce Type: new \nAbstract: Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation systems, enabling vehicles to offload computational tasks to nearby roadside units (RSUs) and mobile edge computing (MEC) servers for real-time processing. However, the highly dynamic nature of VANETs introduces challenges, such as unpredictable network conditions, high latency, energy inefficiency, and task failure. This research addresses these issues by proposing a hybrid AI framework that integrates supervised learning, reinforcement learning, and Particle Swarm Optimization (PSO) for intelligent task offloading and resource allocation. The framework leverages supervised models for predicting optimal offloading strategies, reinforcement learning for adaptive decision-making, and PSO for optimizing latency and energy consumption. Extensive simulations demonstrate that the proposed framework achieves significant reductions in latency and energy usage while improving task success rates and network throughput. By offering an efficient, and scalable solution, this framework sets the foundation for enhancing real-time applications in dynamic vehicular environments."
      },
      {
        "id": "oai:arXiv.org:2504.20752v1",
        "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers",
        "link": "https://arxiv.org/abs/2504.20752",
        "author": "Roman Abramov, Felix Steinbauer, Gjergji Kasneci",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20752v1 Announce Type: new \nAbstract: Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models."
      },
      {
        "id": "oai:arXiv.org:2504.20754v1",
        "title": "DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs",
        "link": "https://arxiv.org/abs/2504.20754",
        "author": "Hao Luan, See-Kiong Ng, Chun Kai Ling",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20754v1 Announce Type: new \nAbstract: Diffusion models form an important class of generative models today, accounting for much of the state of the art in cutting edge AI research. While numerous extensions beyond image and video generation exist, few of such approaches address the issue of explicit constraints in the samples generated. In this paper, we study the problem of generating paths in a layered graph (a variant of a directed acyclic graph) using discrete diffusion models, while guaranteeing that our generated samples are indeed paths. Our approach utilizes a simple yet effective representation for paths which we call the padded adjacency-list matrix (PALM). In addition, we show how to effectively perform classifier guidance, which helps steer the sampled paths to specific preferred edges without any retraining of the diffusion model. Our preliminary results show that empirically, our method outperforms alternatives which do not explicitly account for path constraints."
      },
      {
        "id": "oai:arXiv.org:2504.20769v1",
        "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption",
        "link": "https://arxiv.org/abs/2504.20769",
        "author": "Wenxiao Wang, Parsa Hosseini, Soheil Feizi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20769v1 Announce Type: new \nAbstract: Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%."
      },
      {
        "id": "oai:arXiv.org:2504.20770v1",
        "title": "JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation",
        "link": "https://arxiv.org/abs/2504.20770",
        "author": "Ji Shi, Chengxun Xie, Zhonghao Li, Xinming Zhang, Miao Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20770v1 Announce Type: new \nAbstract: The discovery of new molecules based on the original chemical molecule distributions is of great importance in medicine. The graph transformer, with its advantages of high performance and scalability compared to traditional graph networks, has been widely explored in recent research for applications of graph structures. However, current transformer-based graph decoders struggle to effectively utilize graph information, which limits their capacity to leverage only sequences of nodes rather than the complex topological structures of molecule graphs. This paper focuses on building a graph transformer-based framework for molecular generation, which we call \\textbf{JTreeformer} as it transforms graph generation into junction tree generation. It combines GCN parallel with multi-head attention as the encoder. It integrates a directed acyclic GCN into a graph-based Transformer to serve as a decoder, which can iteratively synthesize the entire molecule by leveraging information from the partially constructed molecular structure at each step. In addition, a diffusion model is inserted in the latent space generated by the encoder, to enhance the efficiency and effectiveness of sampling further. The empirical results demonstrate that our novel framework outperforms existing molecule generation methods, thus offering a promising tool to advance drug discovery (https://anonymous.4open.science/r/JTreeformer-C74C)."
      },
      {
        "id": "oai:arXiv.org:2504.20771v1",
        "title": "Turing Machine Evaluation for Large Language Model",
        "link": "https://arxiv.org/abs/2504.20771",
        "author": "Haitao Wu, Zongbo Han, Huaxi Huang, Changqing Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20771v1 Announce Type: new \nAbstract: With the rapid development and widespread application of Large Language Models (LLMs), rigorous evaluation has become particularly crucial. This research adopts a novel perspective, focusing on evaluating the core computational reasoning ability of LLMs, defined as the capacity of model to accurately understand rules, and execute logically computing operations. This capability assesses the reliability of LLMs as precise executors, and is critical to advanced tasks such as complex code generation and multi-step problem-solving. We propose an evaluation framework based on Universal Turing Machine (UTM) simulation. This framework requires LLMs to strictly follow instructions and track dynamic states, such as tape content and read/write head position, during multi-step computations. To enable standardized evaluation, we developed TMBench, a benchmark for systematically studying the computational reasoning capabilities of LLMs. TMBench provides several key advantages, including knowledge-agnostic evaluation, adjustable difficulty, foundational coverage through Turing machine encoding, and unlimited capacity for instance generation, ensuring scalability as models continue to evolve. We find that model performance on TMBench correlates strongly with performance on other recognized reasoning benchmarks (Pearson correlation coefficient is 0.73), clearly demonstrating that computational reasoning is a significant dimension for measuring the deep capabilities of LLMs. Code and data are available at https://github.com/HaitaoWuTJU/Turing-Machine-Bench."
      },
      {
        "id": "oai:arXiv.org:2504.20789v1",
        "title": "Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM",
        "link": "https://arxiv.org/abs/2504.20789",
        "author": "Collin Beaudoin, Swaroop Ghosh",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20789v1 Announce Type: new \nAbstract: Identifying molecular properties, including side effects, is a critical yet time-consuming step in drug development. Failing to detect these side effects before regulatory submission can result in significant financial losses and production delays, and overlooking them during the regulatory review can lead to catastrophic consequences. This challenge presents an opportunity for innovative machine learning approaches, particularly hybrid quantum-classical models like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network. The QK-LSTM integrates quantum kernel functions into the classical LSTM framework, enabling the capture of complex, non-linear patterns in sequential data. By mapping input data into a high-dimensional quantum feature space, the QK-LSTM model reduces the need for large parameter sets, allowing for model compression without sacrificing accuracy in sequence-based tasks. Recent advancements have been made in the classical domain using augmented variations of the Simplified Molecular Line-Entry System (SMILES). However, to the best of our knowledge, no research has explored the impact of augmented SMILES in the quantum domain, nor the role of augmented Self-Referencing Embedded Strings (SELFIES) in either classical or hybrid quantum-classical settings. This study presents the first analysis of these approaches, providing novel insights into their potential for enhancing molecular property prediction and side effect identification. Results reveal that augmenting SELFIES yields in statistically significant improvements from SMILES by a 5.97% improvement for the classical domain and a 5.91% improvement for the hybrid quantum-classical domain."
      },
      {
        "id": "oai:arXiv.org:2504.20794v1",
        "title": "Q-Fusion: Diffusing Quantum Circuits",
        "link": "https://arxiv.org/abs/2504.20794",
        "author": "Collin Beaudoin, Swaroop Ghosh",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20794v1 Announce Type: new \nAbstract: Quantum computing holds great potential for solving socially relevant and computationally complex problems. Furthermore, quantum machine learning (QML) promises to rapidly improve our current machine learning capabilities. However, current noisy intermediate-scale quantum (NISQ) devices are constrained by limitations in the number of qubits and gate counts, which hinder their full capabilities. Furthermore, the design of quantum algorithms remains a laborious task, requiring significant domain expertise and time. Quantum Architecture Search (QAS) aims to streamline this process by automatically generating novel quantum circuits, reducing the need for manual intervention. In this paper, we propose a diffusion-based algorithm leveraging the LayerDAG framework to generate new quantum circuits. This method contrasts with other approaches that utilize large language models (LLMs), reinforcement learning (RL), variational autoencoders (VAE), and similar techniques. Our results demonstrate that the proposed model consistently generates 100% valid quantum circuit outputs."
      },
      {
        "id": "oai:arXiv.org:2504.20800v1",
        "title": "Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining",
        "link": "https://arxiv.org/abs/2504.20800",
        "author": "Weizhen He, Yunfeng Yan, Shixiang Tang, Yiheng Deng, Yangyang Zhong, Pengxin Luo, Donglian Qi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20800v1 Announce Type: new \nAbstract: Human-centric perception is the core of diverse computer vision tasks and has been a long-standing research focus. However, previous research studied these human-centric tasks individually, whose performance is largely limited to the size of the public task-specific datasets. Recent human-centric methods leverage the additional modalities, e.g., depth, to learn fine-grained semantic information, which limits the benefit of pretraining models due to their sensitivity to camera views and the scarcity of RGB-D data on the Internet. This paper improves the data scalability of human-centric pretraining methods by discarding depth information and exploring semantic information of RGB images in the frequency space by Discrete Cosine Transform (DCT). We further propose new annotation denoising auxiliary tasks with keypoints and DCT maps to enforce the RGB image extractor to learn fine-grained semantic information of human bodies. Our extensive experiments show that when pretrained on large-scale datasets (COCO and AIC datasets) without depth annotation, our model achieves better performance than state-of-the-art methods by +0.5 mAP on COCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by +4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on SHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for crowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for person ReID. We also validate the effectiveness of our method on MPII+NTURGBD datasets"
      },
      {
        "id": "oai:arXiv.org:2504.20821v1",
        "title": "The When and How of Target Variable Transformations",
        "link": "https://arxiv.org/abs/2504.20821",
        "author": "Loren Nuyts, Jesse Davis",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20821v1 Announce Type: new \nAbstract: The machine learning pipeline typically involves the iterative process of (1) collecting the data, (2) preparing the data, (3) learning a model, and (4) evaluating a model. Practitioners recognize the importance of the data preparation phase in terms of its impact on the ability to learn accurate models. In this regard, significant attention is often paid to manipulating the feature set (e.g., selection, transformations, dimensionality reduction). A point that is less well appreciated is that transformations on the target variable can also have a large impact on whether it is possible to learn a suitable model. These transformations may include accounting for subject-specific biases (e.g., in how someone uses a rating scale), contexts (e.g., population size effects), and general trends (e.g., inflation). However, this point has received a much more cursory treatment in the existing literature. The goal of this paper is three-fold. First, we aim to highlight the importance of this problem by showing when transforming the target variable has been useful in practice. Second, we will provide a set of generic ``rules of thumb'' that indicate situations when transforming the target variable may be needed. Third, we will discuss which transformations should be considered in a given situation."
      },
      {
        "id": "oai:arXiv.org:2504.20822v1",
        "title": "An approach to melodic segmentation and classification based on filtering with the Haar-wavelet",
        "link": "https://arxiv.org/abs/2504.20822",
        "author": "Gissel Velarde, Tillman Weyde, David Meredith",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20822v1 Announce Type: new \nAbstract: We present a novel method of classification and segmentation of melodies in symbolic representation. The method is based on filtering pitch as a signal over time with the Haar-wavelet, and we evaluate it on two tasks. The filtered signal corresponds to a single-scale signal ws from the continuous Haar wavelet transform. The melodies are first segmented using local maxima or zero-crossings of w_s. The segments of w_s are then classified using the k-nearest neighbour algorithm with Euclidian and city-block distances. The method proves more effective than using unfiltered pitch signals and Gestalt-based segmentation when used to recognize the parent works of segments from Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch folk tunes into 26 tune families, the performance of the method is comparable to the use of pitch signals, but not as good as that of string-matching methods based on multiple features."
      },
      {
        "id": "oai:arXiv.org:2504.20823v1",
        "title": "Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction",
        "link": "https://arxiv.org/abs/2504.20823",
        "author": "Olga Tsurkan, Aleksandra Konstantinova, Aleksandr Sedykh, Dmitrii Zhiganov, Arsenii Senokosov, Daniil Tarpanov, Matvei Anoshin, Leonid Fedichkin",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20823v1 Announce Type: new \nAbstract: Predictive maintenance in aerospace heavily relies on accurate estimation of the remaining useful life of jet engines. In this paper, we introduce a Hybrid Quantum Recurrent Neural Network frame- work, combining Quantum Long Short-Term Memory layers with classical dense layers for Remaining Useful Life forecasting on NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each Quantum Long Short-Term Memory gate replaces conventional linear transformations with Quantum Depth-Infused circuits, allowing the network to learn high-frequency components more effectively. Experimental results demonstrate that, despite having fewer trainable parameters, the Hybrid Quantum Recurrent Neural Network achieves up to a 5% improvement over a Recurrent Neural Network based on stacked Long Short-Term Memory layers in terms of mean root mean squared error and mean absolute error. Moreover, a thorough comparison of our method with established techniques, including Random Forest, Convolutional Neural Network, and Multilayer Perceptron, demonstrates that our approach, which achieves a Root Mean Squared Error of 15.46, surpasses these baselines by approximately 13.68%, 16.21%, and 7.87%, respectively. Nevertheless, it remains outperformed by certain advanced joint architectures. Our findings highlight the poten- tial of hybrid quantum-classical approaches for robust time-series forecasting under limited data conditions, offering new avenues for enhancing reliability in predictive maintenance tasks."
      },
      {
        "id": "oai:arXiv.org:2504.20829v1",
        "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion",
        "link": "https://arxiv.org/abs/2504.20829",
        "author": "Jiaxin Hong, Sixu Chen, Shuoyang Sun, Hongyao Yu, Hao Fang, Yuqi Tan, Bin Chen, Shuhan Qi, Jiawei Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20829v1 Announce Type: new \nAbstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability."
      },
      {
        "id": "oai:arXiv.org:2504.20830v1",
        "title": "CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation",
        "link": "https://arxiv.org/abs/2504.20830",
        "author": "Jianyu Wu, Yizhou Wang, Xiangyu Yue, Xinzhu Ma, Jingyang Guo, Dongzhan Zhou, Wanli Ouyang, Shixiang Tang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20830v1 Announce Type: new \nAbstract: While accurate and user-friendly Computer-Aided Design (CAD) is crucial for industrial design and manufacturing, existing methods still struggle to achieve this due to their over-simplified representations or architectures incapable of supporting multimodal design requirements. In this paper, we attempt to tackle this problem from both methods and datasets aspects. First, we propose a cascade MAR with topology predictor (CMT), the first multimodal framework for CAD generation based on Boundary Representation (B-Rep). Specifically, the cascade MAR can effectively capture the ``edge-counters-surface'' priors that are essential in B-Reps, while the topology predictor directly estimates topology in B-Reps from the compact tokens in MAR. Second, to facilitate large-scale training, we develop a large-scale multimodal CAD dataset, mmABC, which includes over 1.3 million B-Rep models with multimodal annotations, including point clouds, text descriptions, and multi-view images. Extensive experiments show the superior of CMT in both conditional and unconditional CAD generation tasks. For example, we improve Coverage and Valid ratio by +10.68% and +10.3%, respectively, compared to state-of-the-art methods on ABC in unconditional generation. CMT also improves +4.01 Chamfer on image conditioned CAD generation on mmABC. The dataset, code and pretrained network shall be released."
      },
      {
        "id": "oai:arXiv.org:2504.20834v1",
        "title": "Reinforcement Learning for LLM Reasoning Under Memory Constraints",
        "link": "https://arxiv.org/abs/2504.20834",
        "author": "Alan Lee, Harry Tong",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20834v1 Announce Type: new \nAbstract: We explore reinforcement learning (RL) techniques to enhance reasoning within targeted problem spaces in large language models (LLMs) under memory and compute constraints. Our focus is on critic-free methods compatible with LoRA fine-tuning on a single 40GB GPU, a common limitation in academic settings. We introduce S-GRPO, a memory-efficient variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching strategy for fine-grained credit assignment. Despite limited resources, when used to fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in multi-digit multiplication tasks, underscoring the potential of RL fine-tuning under hardware constraints. Additionally, we find that our full-token GRPO baseline under LoRA fine-tuning did not improve model performance (compared to base model) on either task, suggesting that our memory-efficient methods may act as a form of regularization that stabilizes training when only a small subset of parameters are updated."
      },
      {
        "id": "oai:arXiv.org:2504.20837v1",
        "title": "RadSAM: Segmenting 3D radiological images with a 2D promptable model",
        "link": "https://arxiv.org/abs/2504.20837",
        "author": "Julien Khlaut, Elodie Ferreres, Daniel Tordjman, H\\'el\\`ene Philippe, Tom Boeken, Pierre Manceron, Corentin Dancette",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20837v1 Announce Type: new \nAbstract: Medical image segmentation is a crucial and time-consuming task in clinical care, where mask precision is extremely important. The Segment Anything Model (SAM) offers a promising approach, as it provides an interactive interface based on visual prompting and edition to refine an initial segmentation. This model has strong generalization capabilities, does not rely on predefined classes, and adapts to diverse objects; however, it is pre-trained on natural images and lacks the ability to process medical data effectively. In addition, this model is built for 2D images, whereas a whole medical domain is based on 3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging are based on 2D models, thus requiring one prompt per slice to segment 3D objects, making the segmentation process tedious. They also lack important features such as editing. To bridge this gap, we propose RadSAM, a novel method for segmenting 3D objects with a 2D model from a single prompt. In practice, we train a 2D model using noisy masks as initial prompts, in addition to bounding boxes and points. We then use this novel prompt type with an iterative inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a benchmark to evaluate the model's ability to segment 3D objects in CT images from a single prompt and evaluate the models' out-of-domain transfer and edition capabilities. We demonstrate the effectiveness of our approach against state-of-the-art models on this benchmark using the AMOS abdominal organ segmentation dataset."
      },
      {
        "id": "oai:arXiv.org:2504.20839v1",
        "title": "Universal language model with the intervention of quantum theory",
        "link": "https://arxiv.org/abs/2504.20839",
        "author": "D. -F. Qin",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20839v1 Announce Type: new \nAbstract: This paper examines language modeling based on the theory of quantum mechanics. It focuses on the introduction of quantum mechanics into the symbol-meaning pairs of language in order to build a representation model of natural language. At the same time, it is realized that word embedding, which is widely used as a basic technique for statistical language modeling, can be explained and improved by the mathematical framework of quantum mechanics. On this basis, this paper continues to try to use quantum statistics and other related theories to study the mathematical representation, natural evolution and statistical properties of natural language. It is also assumed that the source of such quantum properties is the physicality of information. The feasibility of using quantum theory to model natural language is pointed out through the construction of a experimental code. The paper discusses, in terms of applications, the possible help of the theory in constructing generative models that are popular nowadays. A preliminary discussion of future applications of the theory to quantum computers is also presented."
      },
      {
        "id": "oai:arXiv.org:2504.20848v1",
        "title": "Mitigating the Structural Bias in Graph Adversarial Defenses",
        "link": "https://arxiv.org/abs/2504.20848",
        "author": "Junyuan Fang, Huimin Liu, Han Yang, Jiajing Wu, Zibin Zheng, Chi K. Tse",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20848v1 Announce Type: new \nAbstract: In recent years, graph neural networks (GNNs) have shown great potential in addressing various graph structure-related downstream tasks. However, recent studies have found that current GNNs are susceptible to malicious adversarial attacks. Given the inevitable presence of adversarial attacks in the real world, a variety of defense methods have been proposed to counter these attacks and enhance the robustness of GNNs. Despite the commendable performance of these defense methods, we have observed that they tend to exhibit a structural bias in terms of their defense capability on nodes with low degree (i.e., tail nodes), which is similar to the structural bias of traditional GNNs on nodes with low degree in the clean graph. Therefore, in this work, we propose a defense strategy by including hetero-homo augmented graph construction, $k$NN augmented graph construction, and multi-view node-wise attention modules to mitigate the structural bias of GNNs against adversarial attacks. Notably, the hetero-homo augmented graph consists of removing heterophilic links (i.e., links connecting nodes with dissimilar features) globally and adding homophilic links (i.e., links connecting nodes with similar features) for nodes with low degree. To further enhance the defense capability, an attention mechanism is adopted to adaptively combine the representations from the above two kinds of graph views. We conduct extensive experiments to demonstrate the defense and debiasing effect of the proposed strategy on benchmark datasets."
      },
      {
        "id": "oai:arXiv.org:2504.20849v1",
        "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry",
        "link": "https://arxiv.org/abs/2504.20849",
        "author": "Anum Afzal, Alexandre Mercier, Florian Matthes",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20849v1 Announce Type: new \nAbstract: Online platforms are increasingly interested in using Data-to-Text technologies to generate content and help their users. Unfortunately, traditional generative methods often fall into repetitive patterns, resulting in monotonous galleries of texts after only a few iterations. In this paper, we investigate LLM-based data-to-text approaches to automatically generate marketing texts that are of sufficient quality and diverse enough for broad adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in conjunction with fine-tuning, few-shot, and zero-shot approaches to set a baseline for diverse marketing texts. We also introduce a metric JaccDiv to evaluate the diversity of a set of texts. This research extends its relevance beyond the music industry, proving beneficial in various fields where repetitive automated content generation is prevalent."
      },
      {
        "id": "oai:arXiv.org:2504.20860v1",
        "title": "FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models",
        "link": "https://arxiv.org/abs/2504.20860",
        "author": "Mainak Singha, Subhankar Roy, Sarthak Mehrotra, Ankit Jha, Moloud Abdar, Biplab Banerjee, Elisa Ricci",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20860v1 Announce Type: new \nAbstract: Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated learning by tuning lightweight input tokens (or prompts) on local client data, while keeping network weights frozen. Post training, only the prompts are shared by the clients with the central server for aggregation. However, textual prompt tuning often struggles with overfitting to known concepts and may be overly reliant on memorized text features, limiting its adaptability to unseen concepts. To address this limitation, we propose Federated Multimodal Visual Prompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual information -- image-conditioned features and textual attribute features of a class -- that is multimodal in nature. At the core of FedMVP is a PromptFormer module that synergistically aligns textual and visual features through cross-attention, enabling richer contexual integration. The dynamically generated multimodal visual prompts are then input to the frozen vision encoder of CLIP, and trained with a combination of CLIP similarity loss and a consistency loss. Extensive evaluation on 20 datasets spanning three generalization settings demonstrates that FedMVP not only preserves performance on in-distribution classes and domains, but also displays higher generalizability to unseen classes and domains when compared to state-of-the-art methods. Codes will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2504.20862v1",
        "title": "Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data",
        "link": "https://arxiv.org/abs/2504.20862",
        "author": "Dayananda Herurkar, J\\\"orn Hees, Vesselin Tzvetkov, Andreas Dengel",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20862v1 Announce Type: new \nAbstract: The remarkable success of Deep Learning approaches is often based and demonstrated on large public datasets. However, when applying such approaches to internal, private datasets, one frequently faces challenges arising from structural differences in the datasets, domain shift, and the lack of labels. In this work, we introduce Tabular Data Adapters (TDA), a novel method for generating soft labels for unlabeled tabular data in outlier detection tasks. By identifying statistically similar public datasets and transforming private data (based on a shared autoencoder) into a format compatible with state-of-the-art public models, our approach enables the generation of weak labels. It thereby can help to mitigate the cold start problem of labeling by basing on existing outlier detection models for public datasets. In experiments on 50 tabular datasets across different domains, we demonstrate that our method is able to provide more accurate annotations than baseline approaches while reducing computational time. Our approach offers a scalable, efficient, and cost-effective solution, to bridge the gap between public research models and real-world industrial applications."
      },
      {
        "id": "oai:arXiv.org:2504.20865v1",
        "title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection",
        "link": "https://arxiv.org/abs/2504.20865",
        "author": "Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20865v1 Announce Type: new \nAbstract: The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators."
      },
      {
        "id": "oai:arXiv.org:2504.20869v1",
        "title": "Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks",
        "link": "https://arxiv.org/abs/2504.20869",
        "author": "Junyuan Fang, Han Yang, Haixian Wen, Jiajing Wu, Zibin Zheng, Chi K. Tse",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20869v1 Announce Type: new \nAbstract: Graph neural networks have been widely utilized to solve graph-related tasks because of their strong learning power in utilizing the local information of neighbors. However, recent studies on graph adversarial attacks have proven that current graph neural networks are not robust against malicious attacks. Yet much of the existing work has focused on the optimization objective based on attack performance to obtain (near) optimal perturbations, but paid less attention to the strength quantification of each perturbation such as the injection of a particular node/link, which makes the choice of perturbations a black-box model that lacks interpretability. In this work, we propose the concept of noise to quantify the attack strength of each adversarial link. Furthermore, we propose three attack strategies based on the defined noise and classification margins in terms of single and multiple steps optimization. Extensive experiments conducted on benchmark datasets against three representative graph neural networks demonstrate the effectiveness of the proposed attack strategies. Particularly, we also investigate the preferred patterns of effective adversarial perturbations by analyzing the corresponding properties of the selected perturbation nodes."
      },
      {
        "id": "oai:arXiv.org:2504.20872v1",
        "title": "FLIM-based Salient Object Detection Networks with Adaptive Decoders",
        "link": "https://arxiv.org/abs/2504.20872",
        "author": "Gilson Junior Soares, Matheus Abrantes Cerqueira, Jancarlo F. Gomes, Laurent Najman, Silvio Jamil F. Guimar\\~aes, Alexandre Xavier Falc\\~ao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20872v1 Announce Type: new \nAbstract: Salient Object Detection (SOD) methods can locate objects that stand out in an image, assign higher values to their pixels in a saliency map, and binarize the map outputting a predicted segmentation mask. A recent tendency is to investigate pre-trained lightweight models rather than deep neural networks in SOD tasks, coping with applications under limited computational resources. In this context, we have investigated lightweight networks using a methodology named Feature Learning from Image Markers (FLIM), which assumes that the encoder's kernels can be estimated from marker pixels on discriminative regions of a few representative images. This work proposes flyweight networks, hundreds of times lighter than lightweight models, for SOD by combining a FLIM encoder with an adaptive decoder, whose weights are estimated for each input image by a given heuristic function. Such FLIM networks are trained from three to four representative images only and without backpropagation, making the models suitable for applications under labeled data constraints as well. We study five adaptive decoders; two of them are introduced here. Differently from the previous ones that rely on one neuron per pixel with shared weights, the heuristic functions of the new adaptive decoders estimate the weights of each neuron per pixel. We compare FLIM models with adaptive decoders for two challenging SOD tasks with three lightweight networks from the state-of-the-art, two FLIM networks with decoders trained by backpropagation, and one FLIM network whose labeled markers define the decoder's weights. The experiments demonstrate the advantages of the proposed networks over the baselines, revealing the importance of further investigating such methods in new applications."
      },
      {
        "id": "oai:arXiv.org:2504.20887v1",
        "title": "Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation",
        "link": "https://arxiv.org/abs/2504.20887",
        "author": "Harry Mead, Clarissa Costen, Bruno Lacerda, Nick Hawes",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20887v1 Announce Type: new \nAbstract: When optimising for conditional value at risk (CVaR) using policy gradients (PG), current methods rely on discarding a large proportion of trajectories, resulting in poor sample efficiency. We propose a reformulation of the CVaR optimisation problem by capping the total return of trajectories used in training, rather than simply discarding them, and show that this is equivalent to the original problem if the cap is set appropriately. We show, with empirical results in an number of environments, that this reformulation of the problem results in consistently improved performance compared to baselines."
      },
      {
        "id": "oai:arXiv.org:2504.20894v1",
        "title": "Does Feedback Help in Bandits with Arm Erasures?",
        "link": "https://arxiv.org/abs/2504.20894",
        "author": "Merve Karakas, Osama Hanna, Lin F. Yang, Christina Fragouli",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20894v1 Announce Type: new \nAbstract: We study a distributed multi-armed bandit (MAB) problem over arm erasure channels, motivated by the increasing adoption of MAB algorithms over communication-constrained networks. In this setup, the learner communicates the chosen arm to play to an agent over an erasure channel with probability $\\epsilon \\in [0,1)$; if an erasure occurs, the agent continues pulling the last successfully received arm; the learner always observes the reward of the arm pulled. In past work, we considered the case where the agent cannot convey feedback to the learner, and thus the learner does not know whether the arm played is the requested or the last successfully received one. In this paper, we instead consider the case where the agent can send feedback to the learner on whether the arm request was received, and thus the learner exactly knows which arm was played. Surprisingly, we prove that erasure feedback does not improve the worst-case regret upper bound order over the previously studied no-feedback setting. In particular, we prove a regret lower bound of $\\Omega(\\sqrt{KT} + K / (1 - \\epsilon))$, where $K$ is the number of arms and $T$ the time horizon, that matches no-feedback upper bounds up to logarithmic factors. We note however that the availability of feedback enables simpler algorithm designs that may achieve better constants (albeit not better order) regret bounds; we design one such algorithm and evaluate its performance numerically."
      },
      {
        "id": "oai:arXiv.org:2504.20900v1",
        "title": "Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking",
        "link": "https://arxiv.org/abs/2504.20900",
        "author": "Dayananda Herurkar, Ahmad Ali, Andreas Dengel",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20900v1 Announce Type: new \nAbstract: Generative models have revolutionized multiple domains, yet their application to tabular data remains underexplored. Evaluating generative models for tabular data presents unique challenges due to structural complexity, large-scale variability, and mixed data types, making it difficult to intuitively capture intricate patterns. Existing evaluation metrics offer only partial insights, lacking a comprehensive measure of generative performance. To address this limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS. Our extensive experimental analysis, conducted on three standard network intrusion detection datasets, compares these metrics with established evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results demonstrate that FAED effectively captures generative modeling issues overlooked by existing metrics. While FPCAD exhibits promising performance, further refinements are necessary to enhance its reliability. Our proposed framework provides a robust and practical approach for assessing generative models in tabular data applications."
      },
      {
        "id": "oai:arXiv.org:2504.20902v1",
        "title": "Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers",
        "link": "https://arxiv.org/abs/2504.20902",
        "author": "Quentin Guimard, Moreno D'Inc\\`a, Massimiliano Mancini, Elisa Ricci",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20902v1 Announce Type: new \nAbstract: A person downloading a pre-trained model from the web should be aware of its biases. Existing approaches for bias identification rely on datasets containing labels for the task of interest, something that a non-expert may not have access to, or may not have the necessary resources to collect: this greatly limits the number of tasks where model biases can be identified. In this work, we present Classifier-to-Bias (C2B), the first bias discovery framework that works without access to any labeled data: it only relies on a textual description of the classification task to identify biases in the target classification model. This description is fed to a large language model to generate bias proposals and corresponding captions depicting biases together with task-specific target labels. A retrieval model collects images for those captions, which are then used to assess the accuracy of the model w.r.t. the given biases. C2B is training-free, does not require any annotations, has no constraints on the list of biases, and can be applied to any pre-trained model on any classification task. Experiments on two publicly available datasets show that C2B discovers biases beyond those of the original datasets and outperforms a recent state-of-the-art bias detection baseline that relies on task-specific annotations, being a promising first step toward addressing task-agnostic unsupervised bias detection."
      },
      {
        "id": "oai:arXiv.org:2504.20908v1",
        "title": "MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability",
        "link": "https://arxiv.org/abs/2504.20908",
        "author": "Wenxin Chen, Weishen Pan, Kyra Gan, Fei Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20908v1 Announce Type: new \nAbstract: Identifying subgroups that benefit from specific treatments using observational data is a critical challenge in personalized medicine. Most existing approaches solely focus on identifying a subgroup with an improved treatment effect. However, practical considerations, such as ensuring a minimum subgroup size for representativeness or achieving sufficient confounder balance for reliability, are also important for making findings clinically meaningful and actionable. While some studies address these constraints individually, none offer a unified approach to handle them simultaneously. To bridge this gap, we propose a model-agnostic framework for optimal subgroup identification under multiple constraints. We reformulate this combinatorial problem as an unconstrained min-max optimization problem with novel modifications and solve it by a gradient descent ascent algorithm. We further prove its convergence to a feasible and locally optimal solution. Our method is stable and highly flexible, supporting various models and techniques for estimating and optimizing treatment effectiveness with observational data. Extensive experiments on both synthetic and real-world datasets demonstrate its effectiveness in identifying subgroups that satisfy multiple constraints, achieving higher treatment effects and better confounder balancing results across different group sizes."
      },
      {
        "id": "oai:arXiv.org:2504.20915v1",
        "title": "Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome",
        "link": "https://arxiv.org/abs/2504.20915",
        "author": "Milad Leyli-abadi, Jean-Patrick Brunet, Axel Tahmasebimoradi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20915v1 Announce Type: new \nAbstract: Based on recent studies, some COVID-19 symptoms can persist for months after infection, leading to what is termed long COVID. Factors such as vaccination timing, patient characteristics, and symptoms during the acute phase of infection may contribute to the prolonged effects and intensity of long COVID. Each patient, based on their unique combination of factors, develops a specific risk or intensity of long COVID. In this work, we aim to achieve two objectives: (1) conduct a statistical analysis to identify relationships between various factors and long COVID, and (2) perform predictive analysis of long COVID intensity using these factors. We benchmark and interpret various data-driven approaches, including linear models, random forests, gradient boosting, and neural networks, using data from the Lifelines COVID-19 cohort. Our results show that Neural Networks (NN) achieve the best performance in terms of MAPE, with predictions averaging 19\\% error. Additionally, interpretability analysis reveals key factors such as loss of smell, headache, muscle pain, and vaccination timing as significant predictors, while chronic disease and gender are critical risk factors. These insights provide valuable guidance for understanding long COVID and developing targeted interventions."
      },
      {
        "id": "oai:arXiv.org:2504.20922v1",
        "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures",
        "link": "https://arxiv.org/abs/2504.20922",
        "author": "Miguel Nogales, Matteo Gambella, Manuel Roveri",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20922v1 Announce Type: new \nAbstract: Early exits (EEs) offer a promising approach to reducing computational costs and latency by dynamically terminating inference once a satisfactory prediction confidence on a data sample is achieved. Although many works integrate EEs into encoder-only Transformers, their application to decoder-only architectures and, more importantly, Mamba models, a novel family of state-space architectures in the LLM realm, remains insufficiently explored. This work introduces DYNAMAX, the first framework to exploit the unique properties of Mamba architectures for early exit mechanisms. We not only integrate EEs into Mamba but also repurpose Mamba as an efficient EE classifier for both Mamba-based and transformer-based LLMs, showcasing its versatility. Our experiments employ the Mistral 7B transformer compared to the Codestral 7B Mamba model, using data sets such as TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and consistency. The results highlight the adaptability of Mamba as a powerful EE classifier and its efficiency in balancing computational cost and performance quality across NLP tasks. By leveraging Mamba's inherent design for dynamic processing, we open pathways for scalable and efficient inference in embedded applications and resource-constrained environments. This study underscores the transformative potential of Mamba in redefining dynamic computing paradigms for LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.20932v1",
        "title": "Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity",
        "link": "https://arxiv.org/abs/2504.20932",
        "author": "Taisuke Kobayashi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20932v1 Announce Type: new \nAbstract: Continual learning is the one of the most essential abilities for autonomous agents, which can incrementally learn daily-life skills. For this ultimate goal, a simple but powerful method, dark experience replay (DER), has been proposed recently. DER mitigates catastrophic forgetting, in which the skills acquired in the past are unintentionally forgotten, by stochastically storing the streaming data in a reservoir sampling (RS) buffer and by relearning them or retaining the past outputs for them. However, since DER considers multiple objectives, it will not function properly without appropriate weighting of them. In addition, the ability to retain past outputs inhibits learning if the past outputs are incorrect due to distribution shift or other effects. This is due to a tradeoff between memory consolidation and plasticity. The tradeoff is hidden even in the RS buffer, which gradually stops storing new data for new skills in it as data is continuously passed to it. To alleviate the tradeoff and achieve better balance, this paper proposes improvement strategies to each of DER and RS. Specifically, DER is improved with automatic adaptation of weights, block of replaying erroneous data, and correction of past outputs. RS is also improved with generalization of acceptance probability, stratification of plural buffers, and intentional omission of unnecessary data. These improvements are verified through multiple benchmarks including regression, classification, and reinforcement learning problems. As a result, the proposed methods achieve steady improvements in learning performance by balancing the memory consolidation and plasticity."
      },
      {
        "id": "oai:arXiv.org:2504.20938v1",
        "title": "Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition",
        "link": "https://arxiv.org/abs/2504.20938",
        "author": "Zhengfu He, Junxuan Wang, Rui Lin, Xuyang Ge, Wentao Shu, Qiong Tang, Junping Zhang, Xipeng Qiu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20938v1 Announce Type: new \nAbstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of Transformer attention layers to disentangle original Multi Head Self Attention (MHSA) into individually comprehensible components. Lorsa is designed to address the challenge of attention superposition to understand attention-mediated interaction between features in different token positions. We show that Lorsa heads find cleaner and finer-grained versions of previously discovered MHSA behaviors like induction heads, successor heads and attention sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse Autoencoder (SAE) are both sparse dictionary learning methods applied to different Transformer components, and lead to consistent findings in many ways. For instance, we discover a comprehensive family of arithmetic-specific Lorsa heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated interpretability analysis indicates that Lorsa achieves parity with SAE in interpretability while Lorsa exhibits superior circuit discovery properties, especially for features computed collectively by multiple MHSA heads. We also conduct extensive experiments on architectural design ablation, Lorsa scaling law and error analysis."
      },
      {
        "id": "oai:arXiv.org:2504.20942v1",
        "title": "Scenario-based Compositional Verification of Autonomous Systems with Neural Perception",
        "link": "https://arxiv.org/abs/2504.20942",
        "author": "Christopher Watson, Rajeev Alur, Divya Gopinath, Ravi Mangal, Corina S. Pasareanu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20942v1 Announce Type: new \nAbstract: Recent advances in deep learning have enabled the development of autonomous systems that use deep neural networks for perception. Formal verification of these systems is challenging due to the size and complexity of the perception DNNs as well as hard-to-quantify, changing environment conditions. To address these challenges, we propose a probabilistic verification framework for autonomous systems based on the following key concepts: (1) Scenario-based Modeling: We decompose the task (e.g., car navigation) into a composition of scenarios, each representing a different environment condition. (2) Probabilistic Abstractions: For each scenario, we build a compact abstraction of perception based on the DNN's performance on an offline dataset that represents the scenario's environment condition. (3) Symbolic Reasoning and Acceleration: The abstractions enable efficient compositional verification of the autonomous system via symbolic reasoning and a novel acceleration proof rule that bounds the error probability of the system under arbitrary variations of environment conditions. We illustrate our approach on two case studies: an experimental autonomous system that guides airplanes on taxiways using high-dimensional perception DNNs and a simulation model of an F1Tenth autonomous car using LiDAR observations."
      },
      {
        "id": "oai:arXiv.org:2504.20944v1",
        "title": "Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements",
        "link": "https://arxiv.org/abs/2504.20944",
        "author": "Kleanthis Avramidis, Woojae Jeong, Aditya Kommineni, Sudarsana R. Kadiri, Marcus Ma, Colin McDaniel, Myzelle Hughes, Thomas McGee, Elsi Kaiser, Dani Byrd, Assal Habibi, B. Rael Cahn, Idan A. Blank, Kristina Lerman, Takfarinas Medani, Richard M. Leahy, Shrikanth Narayanan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20944v1 Announce Type: new \nAbstract: Identifying physiological and behavioral markers for mental health conditions is a longstanding challenge in psychiatry. Depression and suicidal ideation, in particular, lack objective biomarkers, with screening and diagnosis primarily relying on self-reports and clinical interviews. Here, we investigate eye tracking as a potential marker modality for screening purposes. Eye movements are directly modulated by neuronal networks and have been associated with attentional and mood-related patterns; however, their predictive value for depression and suicidality remains unclear. We recorded eye-tracking sequences from 126 young adults as they read and responded to affective sentences, and subsequently developed a deep learning framework to predict their clinical status. The proposed model included separate branches for trials of positive and negative sentiment, and used 2D time-series representations to account for both intra-trial and inter-trial variations. We were able to identify depression and suicidal ideation with an area under the receiver operating curve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and suicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also exhibited moderate, yet significant, accuracy in differentiating depressed from suicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative patterns emerge more strongly when assessing the data relative to response generation than relative to the onset time of the final word of the sentences. The most pronounced effects were observed for negative-sentiment sentences, that are congruent to depressed and suicidal participants. Our findings highlight eye tracking as an objective tool for mental health assessment and underscore the modulatory impact of emotional stimuli on cognitive processes affecting oculomotor control."
      },
      {
        "id": "oai:arXiv.org:2504.20946v1",
        "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models",
        "link": "https://arxiv.org/abs/2504.20946",
        "author": "Tyler McDonald, Ali Emami",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20946v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks, prompt engineering remains an active field of contribution within computational linguistics, particularly in domains requiring specialized knowledge such as arithmetic reasoning. While these LLMs are optimized for a variety of tasks, their exhaustive employment may become computationally or financially cumbersome for small teams. Additionally, complete reliance on proprietary, closed-source models often limits customization and adaptability, posing significant challenges in research and application scalability. Instead, by leveraging open-source models at or below 7 billion parameters, we can optimize our resource usage while still observing remarkable gains over standard prompting approaches. To cultivate this notion, we introduce Trace-of-Thought Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to create observable subproblems using critical problem-solving, specifically designed to enhance arithmetic reasoning capabilities. When applied to open-source models in tandem with GPT-4, we observe that Trace-of-Thought not only allows novel insight into the problem-solving process but also introduces performance gains as large as 125% on language models at or below 7 billion parameters. This approach underscores the potential of open-source initiatives in democratizing AI research and improving the accessibility of high-quality computational linguistics applications."
      },
      {
        "id": "oai:arXiv.org:2504.20948v1",
        "title": "DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition",
        "link": "https://arxiv.org/abs/2504.20948",
        "author": "Yanghui Song, Chengfu Yang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20948v1 Announce Type: new \nAbstract: Given the severe challenges confronting the global growth security of economic crops, precise identification and prevention of plant diseases has emerged as a critical issue in artificial intelligence-enabled agricultural technology. To address the technical challenges in plant disease recognition, including small-sample learning, leaf occlusion, illumination variations, and high inter-class similarity, this study innovatively proposes a Dynamic Dual-Stream Fusion Network (DS_FusionNet). The network integrates a dual-backbone architecture, deformable dynamic fusion modules, and bidirectional knowledge distillation strategy, significantly enhancing recognition accuracy. Experimental results demonstrate that DS_FusionNet achieves classification accuracies exceeding 90% using only 10% of the PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the complex PlantWild dataset, exhibiting exceptional generalization capabilities. This research not only provides novel technical insights for fine-grained image classification but also establishes a robust foundation for precise identification and management of agricultural diseases."
      },
      {
        "id": "oai:arXiv.org:2504.20951v1",
        "title": "Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models",
        "link": "https://arxiv.org/abs/2504.20951",
        "author": "Maryna Vyshnyvetska",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20951v1 Announce Type: new \nAbstract: We propose a theoretical model called \"information gravity\" to describe the text generation process in large language models (LLMs). The model uses physical apparatus from field theory and spacetime geometry to formalize the interaction between user queries and the probability distribution of generated tokens. A query is viewed as an object with \"information mass\" that curves the semantic space of the model, creating gravitational potential wells that \"attract\" tokens during generation. This model offers a mechanism to explain several observed phenomena in LLM behavior, including hallucinations (emerging from low-density semantic voids), sensitivity to query formulation (due to semantic field curvature changes), and the influence of sampling temperature on output diversity."
      },
      {
        "id": "oai:arXiv.org:2504.20964v1",
        "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification",
        "link": "https://arxiv.org/abs/2504.20964",
        "author": "Shangyu Li, Juyong Jiang, Tiancheng Zhao, Jiasi Shen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20964v1 Announce Type: new \nAbstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench."
      },
      {
        "id": "oai:arXiv.org:2504.20965v1",
        "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security",
        "link": "https://arxiv.org/abs/2504.20965",
        "author": "Zikui Cai, Shayan Shabihi, Bang An, Zora Che, Brian R. Bartoldson, Bhavya Kailkhura, Tom Goldstein, Furong Huang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20965v1 Announce Type: new \nAbstract: We introduce AegisLLM, a cooperative multi-agent defense against adversarial attacks and information leakage. In AegisLLM, a structured workflow of autonomous agents - orchestrator, deflector, responder, and evaluator - collaborate to ensure safe and compliant LLM outputs, while self-improving over time through prompt optimization. We show that scaling agentic reasoning system at test-time - both by incorporating additional agent roles and by leveraging automated prompt optimization (such as DSPy)- substantially enhances robustness without compromising model utility. This test-time defense enables real-time adaptability to evolving attacks, without requiring model retraining. Comprehensive evaluations across key threat scenarios, including unlearning and jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning benchmark, AegisLLM achieves near-perfect unlearning with only 20 training examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve 51% improvement compared to the base model on StrongReject, with false refusal rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our results highlight the advantages of adaptive, agentic reasoning over static defenses, establishing AegisLLM as a strong runtime alternative to traditional approaches based on model modifications. Code is available at https://github.com/zikuicai/aegisllm"
      },
      {
        "id": "oai:arXiv.org:2504.20966v1",
        "title": "Softpick: No Attention Sink, No Massive Activations with Rectified Softmax",
        "link": "https://arxiv.org/abs/2504.20966",
        "author": "Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20966v1 Announce Type: new \nAbstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention."
      },
      {
        "id": "oai:arXiv.org:2504.20970v1",
        "title": "SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features",
        "link": "https://arxiv.org/abs/2504.20970",
        "author": "Mete Erdogan, Sebnem Demirtas",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20970v1 Announce Type: new \nAbstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications."
      },
      {
        "id": "oai:arXiv.org:2504.20972v1",
        "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap",
        "link": "https://arxiv.org/abs/2504.20972",
        "author": "Yifan Wei, Xiaoyan Yu, Ran Song, Hao Peng, Angsheng Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20972v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel in tasks such as retrieval and question answering but require updates to incorporate new knowledge and reduce inaccuracies and hallucinations. Traditional updating methods, like fine-tuning and incremental learning, face challenges such as overfitting and high computational costs. Knowledge Editing (KE) provides a promising alternative but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where multiple triplets share common elements, leading to editing conflicts. We identify the prevalence of KEO in existing KE datasets and show its significant impact on current KE methods, causing performance degradation in handling such triplets. To address this, we propose a new formulation, Knowledge Set Editing (KSE), and introduce SetKE, a method that edits sets of triplets simultaneously. Experimental results demonstrate that SetKE outperforms existing methods in KEO scenarios on mainstream LLMs. Additionally, we introduce EditSet, a dataset containing KEO triplets, providing a comprehensive benchmark."
      },
      {
        "id": "oai:arXiv.org:2504.20974v1",
        "title": "Equivariant non-linear maps for neural networks on homogeneous spaces",
        "link": "https://arxiv.org/abs/2504.20974",
        "author": "Elias Nyholm, Oscar Carlsson, Maurice Weiler, Daniel Persson",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20974v1 Announce Type: new \nAbstract: This paper presents a novel framework for non-linear equivariant neural network layers on homogeneous spaces. The seminal work of Cohen et al. on equivariant $G$-CNNs on homogeneous spaces characterized the representation theory of such layers in the linear setting, finding that they are given by convolutions with kernels satisfying so-called steerability constraints. Motivated by the empirical success of non-linear layers, such as self-attention or input dependent kernels, we set out to generalize these insights to the non-linear setting. We derive generalized steerability constraints that any such layer needs to satisfy and prove the universality of our construction. The insights gained into the symmetry-constrained functional dependence of equivariant operators on feature maps and group elements informs the design of future equivariant neural network layers. We demonstrate how several common equivariant network architectures - $G$-CNNs, implicit steerable kernel networks, conventional and relative position embedded attention based transformers, and LieTransformers - may be derived from our framework."
      },
      {
        "id": "oai:arXiv.org:2504.20988v1",
        "title": "Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning",
        "link": "https://arxiv.org/abs/2504.20988",
        "author": "Atul Sharma, Kavindu Herath, Saurabh Bagchi, Chaoyue Liu, Somali Chaterji",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20988v1 Announce Type: new \nAbstract: We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm for collaborative machine learning that combines the strengths of Federated Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier communication structure that avoids the single point of failure inherent in FL and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local (ELL). At equal communication budgets (total edges), HSL achieves higher performance than ELL, while at significantly lower communication budgets, it can match ELL's performance. For instance, with only 400 edges, HSL reaches the same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL also achieves stronger consensus among nodes after mixing, resulting in improved performance with fewer training rounds. We substantiate these claims through rigorous theoretical analyses and extensive experimental results, showcasing HSL's practicality for large-scale collaborative learning."
      },
      {
        "id": "oai:arXiv.org:2504.20995v1",
        "title": "TesserAct: Learning 4D Embodied World Models",
        "link": "https://arxiv.org/abs/2504.20995",
        "author": "Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, Chuang Gan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20995v1 Announce Type: new \nAbstract: This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models."
      },
      {
        "id": "oai:arXiv.org:2504.20996v1",
        "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
        "link": "https://arxiv.org/abs/2504.20996",
        "author": "Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20996v1 Announce Type: new \nAbstract: We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models."
      },
      {
        "id": "oai:arXiv.org:2504.20997v1",
        "title": "Toward Efficient Exploration by Large Language Model Agents",
        "link": "https://arxiv.org/abs/2504.20997",
        "author": "Dilip Arumugam, Thomas L. Griffiths",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20997v1 Announce Type: new \nAbstract: A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration."
      },
      {
        "id": "oai:arXiv.org:2504.20998v1",
        "title": "YoChameleon: Personalized Vision and Language Generation",
        "link": "https://arxiv.org/abs/2504.20998",
        "author": "Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, Yuheng Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20998v1 Announce Type: new \nAbstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive\" image generation approach to enhance image quality in a few-shot setting."
      },
      {
        "id": "oai:arXiv.org:2504.20058v1",
        "title": "Predictive AI with External Knowledge Infusion for Stocks",
        "link": "https://arxiv.org/abs/2504.20058",
        "author": "Ambedkar Dukkipati, Kawin Mayilvaghanan, Naveen Kumar Pallekonda, Sai Prakash Hadnoor, Ranga Shaarad Ayyagari",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20058v1 Announce Type: cross \nAbstract: Fluctuations in stock prices are influenced by a complex interplay of factors that go beyond mere historical data. These factors, themselves influenced by external forces, encompass inter-stock dynamics, broader economic factors, various government policy decisions, outbreaks of wars, etc. Furthermore, all of these factors are dynamic and exhibit changes over time. In this paper, for the first time, we tackle the forecasting problem under external influence by proposing learning mechanisms that not only learn from historical trends but also incorporate external knowledge from temporal knowledge graphs. Since there are no such datasets or temporal knowledge graphs available, we study this problem with stock market data, and we construct comprehensive temporal knowledge graph datasets. In our proposed approach, we model relations on external temporal knowledge graphs as events of a Hawkes process on graphs. With extensive experiments, we show that learned dynamic representations effectively rank stocks based on returns across multiple holding periods, outperforming related baselines on relevant metrics."
      },
      {
        "id": "oai:arXiv.org:2504.20059v1",
        "title": "Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence",
        "link": "https://arxiv.org/abs/2504.20059",
        "author": "Joey Chan, Qiao Jin, Nicholas Wan, Charalampos S. Floudas, Elisabetta Xue, Zhiyong Lu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20059v1 Announce Type: cross \nAbstract: Clinical trials are crucial for assessing new treatments; however, recruitment challenges - such as limited awareness, complex eligibility criteria, and referral barriers - hinder their success. With the growth of online platforms, patients increasingly turn to social media and health communities for support, research, and advocacy, expanding recruitment pools and established enrollment pathways. Recognizing this potential, we utilized TrialGPT, a framework that leverages a large language model (LLM) as its backbone, to match 50 online patient cases (collected from published case reports and a social media website) to clinical trials and evaluate performance against traditional keyword-based searches. Our results show that TrialGPT outperforms traditional methods by 46% in identifying eligible trials, with each patient, on average, being eligible for around 7 trials. Additionally, our outreach efforts to case authors and trial organizers regarding these patient-trial matches yielded highly positive feedback, which we present from both perspectives."
      },
      {
        "id": "oai:arXiv.org:2504.20064v1",
        "title": "Against Opacity: Explainable AI and Large Language Models for Effective Digital Advertising",
        "link": "https://arxiv.org/abs/2504.20064",
        "author": "Qi Yang, Marlo Ongpin, Sergey Nikolenko, Alfred Huang, Aleksandr Farseev",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20064v1 Announce Type: cross \nAbstract: The opaqueness of modern digital advertising, exemplified by platforms such as Meta Ads, raises concerns regarding their autonomous control over audience targeting, pricing structures, and ad relevancy assessments. Locked in their leading positions by network effects, ``Metas and Googles of the world'' attract countless advertisers who rely on intuition, with billions of dollars lost on ineffective social media ads. The platforms' algorithms use huge amounts of data unavailable to advertisers, and the algorithms themselves are opaque as well. This lack of transparency hinders the advertisers' ability to make informed decisions and necessitates efforts to promote transparency, standardize industry metrics, and strengthen regulatory frameworks. In this work, we propose novel ways to assist marketers in optimizing their advertising strategies via machine learning techniques designed to analyze and evaluate content, in particular, predict the click-through rates (CTR) of novel advertising content. Another important problem is that large volumes of data available in the competitive landscape, e.g., competitors' ads, impede the ability of marketers to derive meaningful insights. This leads to a pressing need for a novel approach that would allow us to summarize and comprehend complex data. Inspired by the success of ChatGPT in bridging the gap between large language models (LLMs) and a broader non-technical audience, we propose a novel system that facilitates marketers in data interpretation, called SODA, that merges LLMs with explainable AI, enabling better human-AI collaboration with an emphasis on the domain of digital marketing and advertising. By combining LLMs and explainability features, in particular modern text-image models, we aim to improve the synergy between human marketers and AI systems."
      },
      {
        "id": "oai:arXiv.org:2504.20065v1",
        "title": "A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts",
        "link": "https://arxiv.org/abs/2504.20065",
        "author": "Robert Becker, Aron Culotta",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20065v1 Announce Type: cross \nAbstract: We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works."
      },
      {
        "id": "oai:arXiv.org:2504.20068v1",
        "title": "Tempo: Application-aware LLM Serving with Mixed SLO Requirements",
        "link": "https://arxiv.org/abs/2504.20068",
        "author": "Wei Zhang, Zhiyu Wu, Yi Mu, Banruo Liu, Myungjin Lee, Fan Lai",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20068v1 Announce Type: cross \nAbstract: The integration of Large Language Models (LLMs) into diverse applications, ranging from interactive chatbots and cloud AIOps to intelligent agents, has introduced a wide spectrum of Service Level Objectives (SLOs) for responsiveness. These workloads include latency-sensitive requests focused on per-token latency in streaming chat, throughput-intensive requests that require rapid full responses to invoke tools, and collective requests with dynamic dependencies arising from self-reflection or agent-based reasoning. This workload diversity, amplified by unpredictable request information such as response lengths and runtime dependencies, makes existing schedulers inadequate even within their design envelopes.\n  In this paper, we define service gain as the useful service delivered by completing requests. We observe that as SLO directly reflects the actual performance needs of requests, completing a request much faster than its SLO (e.g., deadline) yields limited additional service gain. Based on this insight, we introduce Tempo, the first systematic SLO-aware scheduler designed to maximize service gain across diverse LLM workloads. Tempo allocates just enough serving bandwidth to meet each SLO, maximizing residual capacity for others best-effort workloads. Instead of assuming request information or none at all, it adopts a hybrid scheduling strategy: using quantile-based response upper bounds and dependency-graph matching for conservative initial estimates, prioritizing requests by service gain density, and refining decisions online as generation progresses. Our evaluation across diverse workloads, including chat, reasoning, and agentic pipelines, shows that Tempo improves end-to-end service gain by up to 8.3$\\times$ and achieves up to 10.3$\\times$ SLO goodput compared to state-of-the-art designs"
      },
      {
        "id": "oai:arXiv.org:2504.20074v1",
        "title": "EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures",
        "link": "https://arxiv.org/abs/2504.20074",
        "author": "Khurram Khalil, Khaza Anuarul Hoque",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20074v1 Announce Type: cross \nAbstract: The increasing adoption of approximate computing in deep neural network accelerators (AxDNNs) promises significant energy efficiency gains. However, permanent faults in AxDNNs can severely degrade their performance compared to their accurate counterparts (AccDNNs). Traditional fault detection and mitigation approaches, while effective for AccDNNs, introduce substantial overhead and latency, making them impractical for energy-constrained real-time deployment. To address this, we introduce EPSILON, a lightweight framework that leverages pre-computed statistical signatures and layer-wise importance metrics for efficient fault detection and mitigation in AxDNNs. Our framework introduces a novel non-parametric pattern-matching algorithm that enables constant-time fault detection without interrupting normal execution while dynamically adapting to different network architectures and fault patterns. EPSILON maintains model accuracy by intelligently adjusting mitigation strategies based on a statistical analysis of weight distribution and layer criticality while preserving the energy benefits of approximate computing. Extensive evaluations across various approximate multipliers, AxDNN architectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and fault scenarios demonstrate that EPSILON maintains 80.05\\% accuracy while offering 22\\% improvement in inference time and 28\\% improvement in energy efficiency, establishing EPSILON as a practical solution for deploying reliable AxDNNs in safety-critical edge applications."
      },
      {
        "id": "oai:arXiv.org:2504.20084v1",
        "title": "AI Awareness",
        "link": "https://arxiv.org/abs/2504.20084",
        "author": "Xiaojian Li, Haoyuan Shi, Rongwu Xu, Wei Xu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20084v1 Announce Type: cross \nAbstract: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness, not as a philosophical question of consciousness, but as a measurable, functional capacity. In this review, we explore the emerging landscape of AI awareness, which includes meta-cognition (the ability to represent and reason about its own state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents), and situational awareness (assessing and responding to the context in which it operates).\n  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.\n  AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raises concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow. On the whole, our interdisciplinary review provides a roadmap for future research and aims to clarify the role of AI awareness in the ongoing development of intelligent machines."
      },
      {
        "id": "oai:arXiv.org:2504.20088v1",
        "title": "Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks",
        "link": "https://arxiv.org/abs/2504.20088",
        "author": "Joao Felipe Gueiros, Hemanth Chandravamsi, Steven H. Frankel",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20088v1 Announce Type: cross \nAbstract: This paper explores the use of deep residual networks for pricing European options on Petrobras, one of the world's largest oil and gas producers, and compares its performance with the Black-Scholes (BS) model. Using eight years of historical data from B3 (Brazilian Stock Exchange) collected via web scraping, a deep learning model was trained using a custom built hybrid loss function that incorporates market data and analytical pricing. The data for training and testing were drawn between the period spanning November 2016 to January 2025, using an 80-20 train-test split. The test set consisted of data from the final three months: November, December, and January 2025. The deep residual network model achieved a 64.3\\% reduction in the mean absolute error for the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes model on the test set. Furthermore, unlike the Black-Scholes solution, which tends to decrease its accuracy for longer periods of time, the deep learning model performed accurately for longer expiration periods. These findings highlight the potential of deep learning in financial modeling, with future work focusing on specialized models for different price ranges."
      },
      {
        "id": "oai:arXiv.org:2504.20090v1",
        "title": "Spark: A System for Scientifically Creative Idea Generation",
        "link": "https://arxiv.org/abs/2504.20090",
        "author": "Aishik Sanyal, Samuel Schapiro, Sumuk Shashidhar, Royce Moon, Lav R. Varshney, Dilek Hakkani-Tur",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20090v1 Announce Type: cross \nAbstract: Recently, large language models (LLMs) have shown promising abilities to generate novel research ideas in science, a direction which coincides with many foundational principles in computational creativity (CC). In light of these developments, we present an idea generation system named Spark that couples retrieval-augmented idea generation using LLMs with a reviewer model named Judge trained on 600K scientific reviews from OpenReview. Our work is both a system demonstration and intended to inspire other CC researchers to explore grounding the generation and evaluation of scientific ideas within foundational CC principles. To this end, we release the annotated dataset used to train Judge, inviting other researchers to explore the use of LLMs for idea generation and creative evaluations."
      },
      {
        "id": "oai:arXiv.org:2504.20094v1",
        "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?",
        "link": "https://arxiv.org/abs/2504.20094",
        "author": "Zheng Hui, Xiaokai Wei, Yexi Jiang, Kevin Gao, Chen Wang, Frank Ong, Se-eun Yoon, Rachit Pareek, Michelle Gong",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20094v1 Announce Type: cross \nAbstract: In this paper, we propose a multi-agent collaboration framework called MATCHA for conversational recommendation system, leveraging large language models (LLMs) to enhance personalization and user engagement. Users can request recommendations via free-form text and receive curated lists aligned with their interests, preferences, and constraints. Our system introduces specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards. These agents collaboratively improve recommendations accuracy, diversity, and safety. On eight metrics, our model achieves superior or comparable performance to the current state-of-the-art. Through comparisons with six baseline models, our approach addresses key challenges in conversational recommendation systems for game recommendations, including: (1) handling complex, user-specific requests, (2) enhancing personalization through multi-agent collaboration, (3) empirical evaluation and deployment, and (4) ensuring safe and trustworthy interactions."
      },
      {
        "id": "oai:arXiv.org:2504.20103v1",
        "title": "Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning",
        "link": "https://arxiv.org/abs/2504.20103",
        "author": "Wenfeng Dai, Yanhong Wang, Shuai Yan, Qingzhi Yu, Xiang Cheng",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20103v1 Announce Type: cross \nAbstract: Drug-target interaction (DTI) prediction is a core task in drug development and precision medicine in the biomedical field. However, traditional machine learning methods generally have the black box problem, which makes it difficult to reveal the deep correlation between the model decision mechanism and the interaction pattern between biological molecules. This study proposes a heterogeneous network drug target interaction prediction framework, integrating graph neural network and multi scale signal processing technology to construct a model with both efficient prediction and multi level interpretability. Its technical breakthroughs are mainly reflected in the following three dimensions:Local global feature collaborative perception module. Based on heterogeneous graph convolutional neural network (HGCN), a multi order neighbor aggregation strategy is designed.Multi scale graph signal decomposition and biological interpretation module. A deep hierarchical node feature transform (GWT) architecture is proposed.Contrastive learning combining multi dimensional perspectives and hierarchical representations. By comparing the learning models, the node representations from the two perspectives of HGCN and GWT are aligned and fused, so that the model can integrate multi dimensional information and improve the prediction robustness. Experimental results show that our framework shows excellent prediction performance on all datasets. This study provides a complete solution for drug target discovery from black box prediction to mechanism decoding, and its methodology has important reference value for modeling complex biomolecular interaction systems."
      },
      {
        "id": "oai:arXiv.org:2504.20109v1",
        "title": "Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems",
        "link": "https://arxiv.org/abs/2504.20109",
        "author": "Rajeev Gupta, Suhani Gupta, Ronak Parikh, Divya Gupta, Amir Javaheri, Jairaj Singh Shaktawat",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20109v1 Announce Type: cross \nAbstract: Artificial Intelligence has made remarkable advancements in recent years, primarily driven by increasingly large deep learning models. However, achieving true Artificial General Intelligence (AGI) demands fundamentally new architectures rather than merely scaling up existing models. Current approaches largely depend on expanding model parameters, which improves task-specific performance but falls short in enabling continuous, adaptable, and generalized learning. Achieving AGI capable of continuous learning and personalization on resource-constrained edge devices is an even bigger challenge.\n  This paper reviews the state of continual learning and neuroscience-inspired AI, and proposes a novel architecture for Personalized AGI that integrates brain-like learning mechanisms for edge deployment. We review literature on continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss key neuroscience principles of human learning, including Synaptic Pruning, Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for AI systems. Building on these insights, we outline an AI architecture that features complementary fast-and-slow learning modules, synaptic self-optimization, and memory-efficient model updates to support on-device lifelong adaptation.\n  Conceptual diagrams of the proposed architecture and learning processes are provided. We address challenges such as catastrophic forgetting, memory efficiency, and system scalability, and present application scenarios for mobile AI assistants and embodied AI systems like humanoid robots. We conclude with key takeaways and future research directions toward truly continual, personalized AGI on the edge. While the architecture is theoretical, it synthesizes diverse findings and offers a roadmap for future implementation."
      },
      {
        "id": "oai:arXiv.org:2504.20113v1",
        "title": "Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI",
        "link": "https://arxiv.org/abs/2504.20113",
        "author": "Lingbo Li, Anuradha Mathrani, Teo Susnjak",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20113v1 Announce Type: cross \nAbstract: Exponential growth in scientific literature has heightened the demand for efficient evidence-based synthesis, driving the rise of the field of Automated Meta-analysis (AMA) powered by natural language processing and machine learning. This PRISMA systematic review introduces a structured framework for assessing the current state of AMA, based on screening 978 papers from 2006 to 2024, and analyzing 54 studies across diverse domains. Findings reveal a predominant focus on automating data processing (57%), such as extraction and statistical modeling, while only 17% address advanced synthesis stages. Just one study (2%) explored preliminary full-process automation, highlighting a critical gap that limits AMA's capacity for comprehensive synthesis. Despite recent breakthroughs in large language models (LLMs) and advanced AI, their integration into statistical modeling and higher-order synthesis, such as heterogeneity assessment and bias evaluation, remains underdeveloped. This has constrained AMA's potential for fully autonomous meta-analysis. From our dataset spanning medical (67%) and non-medical (33%) applications, we found that AMA has exhibited distinct implementation patterns and varying degrees of effectiveness in actually improving efficiency, scalability, and reproducibility. While automation has enhanced specific meta-analytic tasks, achieving seamless, end-to-end automation remains an open challenge. As AI systems advance in reasoning and contextual understanding, addressing these gaps is now imperative. Future efforts must focus on bridging automation across all meta-analysis stages, refining interpretability, and ensuring methodological robustness to fully realize AMA's potential for scalable, domain-agnostic synthesis."
      },
      {
        "id": "oai:arXiv.org:2504.20114v1",
        "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering",
        "link": "https://arxiv.org/abs/2504.20114",
        "author": "Zhonghao Li, Kunpeng Zhang, Jinghuai Ou, Shuliang Liu, Xuming Hu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20114v1 Announce Type: cross \nAbstract: Retrieval-augmented generation (RAG) systems face significant challenges in multi-hop question answering (MHQA), where complex queries require synthesizing information across multiple document chunks. Existing approaches typically rely on iterative LLM-based query rewriting and routing, resulting in high computational costs due to repeated LLM invocations and multi-stage processes. To address these limitations, we propose TreeHop, an embedding-level framework without the need for LLMs in query refinement. TreeHop dynamically updates query embeddings by fusing semantic information from prior queries and retrieved documents, enabling iterative retrieval through embedding-space operations alone. This method replaces the traditional \"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined \"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead. Moreover, a rule-based stop criterion is introduced to further prune redundant retrievals, balancing efficiency and recall rate. Experimental results show that TreeHop rivals advanced RAG methods across three open-domain MHQA datasets, achieving comparable performance with only 5\\%-0.4\\% of the model parameter size and reducing the query latency by approximately 99\\% compared to concurrent approaches. This makes TreeHop a faster and more cost-effective solution for deployment in a range of knowledge-intensive applications. For reproducibility purposes, codes and data are available here: https://github.com/allen-li1231/TreeHop."
      },
      {
        "id": "oai:arXiv.org:2504.20117v1",
        "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies",
        "link": "https://arxiv.org/abs/2504.20117",
        "author": "Shubham Gandhi, Dhruv Shah, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20117v1 Announce Type: cross \nAbstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research."
      },
      {
        "id": "oai:arXiv.org:2504.20126v1",
        "title": "Enhancing Cell Counting through MLOps: A Structured Approach for Automated Cell Analysis",
        "link": "https://arxiv.org/abs/2504.20126",
        "author": "Matteo Testi, Luca Clissa, Matteo Ballabio, Salvatore Ricciardi, Federico Baldo, Emanuele Frontoni, Sara Moccia, Gennario Vessio",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20126v1 Announce Type: cross \nAbstract: Machine Learning (ML) models offer significant potential for advancing cell counting applications in neuroscience, medical research, pharmaceutical development, and environmental monitoring. However, implementing these models effectively requires robust operational frameworks. This paper introduces Cell Counting Machine Learning Operations (CC-MLOps), a comprehensive framework that streamlines the integration of ML in cell counting workflows. CC-MLOps encompasses data access and preprocessing, model training, monitoring, explainability features, and sustainability considerations. Through a practical use case, we demonstrate how MLOps principles can enhance model reliability, reduce human error, and enable scalable Cell Counting solutions. This work provides actionable guidance for researchers and laboratory professionals seeking to implement machine learning (ML)- powered cell counting systems."
      },
      {
        "id": "oai:arXiv.org:2504.20127v1",
        "title": "Learning Hierarchical Interaction for Accurate Molecular Property Prediction",
        "link": "https://arxiv.org/abs/2504.20127",
        "author": "Huiyang Hong, Xinkai Wu, Hongyu Sun, Qi Wang, Yuquan Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20127v1 Announce Type: cross \nAbstract: Discovering molecules with desirable molecular properties, including ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of great importance in drug discovery. Existing approaches typically employ deep learning models, such as Graph Neural Networks (GNNs) and Transformers, to predict these molecular properties by learning from diverse chemical information. However, these models often fail to efficiently capture and utilize the hierarchical nature of molecular structures, and lack mechanisms for effective interaction among multi-level features. To address these limitations, we propose a Hierarchical Interaction Message Passing Mechanism, which serves as the foundation of our novel model, HimNet. Our method enables interaction-aware representation learning across atomic, motif, and molecular levels via hierarchical attention-guided message passing. This design allows HimNet to effectively balance global and local information, ensuring rich and task-relevant feature extraction for downstream property prediction tasks, such as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple benchmark datasets demonstrate that HimNet achieves the best or near-best performance in most molecular property prediction tasks. Furthermore, our method exhibits promising hierarchical interpretability, aligning well with chemical intuition on representative molecules. We believe that HimNet offers an accurate and efficient solution for molecular activity and ADMET property prediction, contributing significantly to advanced decision-making in the early stages of drug discovery."
      },
      {
        "id": "oai:arXiv.org:2504.20129v1",
        "title": "A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States",
        "link": "https://arxiv.org/abs/2504.20129",
        "author": "Arun M. Saranathan, Mahmoud Saeedimoghaddam, Brandon Smith, Deepthi Raghunandan, Grey Nearing, Craig Pelissier",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20129v1 Announce Type: cross \nAbstract: Snow is an essential input for various land surface models. Seasonal snow estimates are available as snow water equivalent (SWE) from process-based reanalysis products or locally from in situ measurements. While the reanalysis products are computationally expensive and available at only fixed spatial and temporal resolutions, the in situ measurements are highly localized and sparse. To address these issues and enable the analysis of the effect of a large suite of physical, morphological, and geological conditions on the presence and amount of snow, we build a Long Short-Term Memory (LSTM) network, which is able to estimate the SWE based on time series input of the various physical/meteorological factors as well static spatial/morphological factors. Specifically, this model breaks down the SWE estimation into two separate tasks: (i) a classification task that indicates the presence/absence of snow on a specific day and (ii) a regression task that indicates the height of the SWE on a specific day in the case of snow presence. The model is trained using physical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows in the western United States. We will show that trained LSTM models have a classification accuracy of $\\geq 93\\%$ for the presence of snow and a coefficient of correlation of $\\sim 0.9$ concerning their SWE estimates. We will also demonstrate that the models can generalize both spatially and temporally to previously unseen data."
      },
      {
        "id": "oai:arXiv.org:2504.20186v1",
        "title": "Communication in Agile Software Development - A Mapping Study",
        "link": "https://arxiv.org/abs/2504.20186",
        "author": "Suddhasvatta Das, Kevin Gary",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20186v1 Announce Type: cross \nAbstract: Software industry is a fast-moving industry and to keep up with this pace the development process also needs to be fast and efficient and Agile software development (ASD) is the answer to this problem. Even though ASD has been in there for over two decades there are still multiple unknown questions tied to ASD that need to be addressed. In this study we are going to address one of the most critical factors of ASD i.e. Communication. We conducted a review of 14 studies and found the areas under ASD communication that the community is interested in as well as research gaps."
      },
      {
        "id": "oai:arXiv.org:2504.20194v1",
        "title": "Coreset selection for the Sinkhorn divergence and generic smooth divergences",
        "link": "https://arxiv.org/abs/2504.20194",
        "author": "Alex Kokot, Alex Luedtke",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20194v1 Announce Type: cross \nAbstract: We introduce CO2, an efficient algorithm to produce convexly-weighted coresets with respect to generic smooth divergences. By employing a functional Taylor expansion, we show a local equivalence between sufficiently regular losses and their second order approximations, reducing the coreset selection problem to maximum mean discrepancy minimization. We apply CO2 to the Sinkhorn divergence, providing a novel sampling procedure that requires logarithmically many data points to match the approximation guarantees of random sampling. To show this, we additionally verify several new regularity properties for entropically regularized optimal transport of independent interest. Our approach leads to a new perspective linking coreset selection and kernel quadrature to classical statistical methods such as moment and score matching. We showcase this method with a practical application of subsampling image data, and highlight key directions to explore for improved algorithmic efficiency and theoretical guarantees."
      },
      {
        "id": "oai:arXiv.org:2504.20198v1",
        "title": "Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems",
        "link": "https://arxiv.org/abs/2504.20198",
        "author": "Alireza Furutanpey, Carmen Walser, Philipp Raith, Pantelis A. Frangoudis, Schahram Dustdar",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20198v1 Announce Type: cross \nAbstract: This work presents a comprehensive evaluation of neural network graph compilers across heterogeneous hardware platforms, addressing the critical gap between theoretical optimization techniques and practical deployment scenarios. We demonstrate how vendor-specific optimizations can invalidate relative performance comparisons between architectural archetypes, with performance advantages sometimes completely reversing after compilation. Our systematic analysis reveals that graph compilers exhibit performance patterns highly dependent on both neural architecture and batch sizes. Through fine-grained block-level experimentation, we establish that vendor-specific compilers can leverage repeated patterns in simple architectures, yielding disproportionate throughput gains as model depth increases. We introduce novel metrics to quantify a compiler's ability to mitigate performance friction as batch size increases. Our methodology bridges the gap between academic research and practical deployment by incorporating compiler effects throughout the research process, providing actionable insights for practitioners navigating complex optimization landscapes across heterogeneous hardware environments."
      },
      {
        "id": "oai:arXiv.org:2504.20238v1",
        "title": "Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model",
        "link": "https://arxiv.org/abs/2504.20238",
        "author": "P. Trent Vonich, Gregory J. Hakim",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20238v1 Announce Type: cross \nAbstract: Atmospheric predictability research has long held that the limit of skillful deterministic weather forecasts is about 14 days. We challenge this limit using GraphCast, a machine-learning weather model, by optimizing forecast initial conditions using gradient-based techniques for twice-daily forecasts spanning 2020. This approach yields an average error reduction of 86% at 10 days, with skill lasting beyond 30 days. Mean optimal initial-condition perturbations reveal large-scale, spatially coherent corrections to ERA5, primarily reflecting an intensification of the Hadley circulation. Forecasts using GraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21% error reduction, peaking at 4 days, indicating that analysis corrections reflect a combination of both model bias and a reduction in analysis error. These results demonstrate that, given accurate initial conditions, skillful deterministic forecasts are consistently achievable far beyond two weeks, challenging long-standing assumptions about the limits of atmospheric predictability."
      },
      {
        "id": "oai:arXiv.org:2504.20256v1",
        "title": "Optimizing Hard Thresholding for Sparse Model Discovery",
        "link": "https://arxiv.org/abs/2504.20256",
        "author": "Derek W. Jollie, Scott G. McCalla",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20256v1 Announce Type: cross \nAbstract: Many model selection algorithms rely on sparse dictionary learning to provide interpretable and physics-based governing equations. The optimization algorithms typically use a hard thresholding process to enforce sparse activations in the model coefficients by removing library elements from consideration. By introducing an annealing scheme that reactivates a fraction of the removed terms with a cooling schedule, we are able to improve the performance of these sparse learning algorithms. We concentrate on two approaches to the optimization, SINDy, and an alternative using hard thresholding pursuit. We see in both cases that annealing can improve model accuracy. The effectiveness of annealing is demonstrated through comparisons on several nonlinear systems pulled from convective flows, excitable systems, and population dynamics. Finally we apply these algorithms to experimental data for projectile motion."
      },
      {
        "id": "oai:arXiv.org:2504.20266v1",
        "title": "A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems",
        "link": "https://arxiv.org/abs/2504.20266",
        "author": "Mohammadhossein Homaei, Agustin Di Bartolo, Oscar Mogollon-Gutierrez, Fernando Broncano Morgado, Pablo Garcia Rodriguez",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20266v1 Announce Type: cross \nAbstract: Digital twins (DTs) help improve real-time monitoring and decision-making in water distribution systems. However, their connectivity makes them easy targets for cyberattacks such as scanning, denial-of-service (DoS), and unauthorized access. Small and medium-sized enterprises (SMEs) that manage these systems often do not have enough budget or staff to build strong cybersecurity teams. To solve this problem, we present a Virtual Cybersecurity Department (VCD), an affordable and automated framework designed for SMEs. The VCD uses open-source tools like Zabbix for real-time monitoring, Suricata for network intrusion detection, Fail2Ban to block repeated login attempts, and simple firewall settings. To improve threat detection, we also add a machine-learning-based IDS trained on the OD-IDS2022 dataset using an improved ensemble model. This model detects cyber threats such as brute-force attacks, remote code execution (RCE), and network flooding, with 92\\% accuracy and fewer false alarms. Our solution gives SMEs a practical and efficient way to secure water systems using low-cost and easy-to-manage tools."
      },
      {
        "id": "oai:arXiv.org:2504.20275v1",
        "title": "Smart Water Security with AI and Blockchain-Enhanced Digital Twins",
        "link": "https://arxiv.org/abs/2504.20275",
        "author": "Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon Gutierrez, Ruben Molano Gomez, Andres Caro",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20275v1 Announce Type: cross \nAbstract: Water distribution systems in rural areas face serious challenges such as a lack of real-time monitoring, vulnerability to cyberattacks, and unreliable data handling. This paper presents an integrated framework that combines LoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection System (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure and transparent water management. The IDS filters anomalous or spoofed data using a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before validated data is logged via smart contracts on a private Ethereum blockchain using Proof of Authority (PoA) consensus. The verified data feeds into a real-time DT model supporting leak detection, consumption forecasting, and predictive maintenance. Experimental results demonstrate that the system achieves over 80 transactions per second (TPS) with under 2 seconds of latency while remaining cost-effective and scalable for up to 1,000 smart meters. This work demonstrates a practical and secure architecture for decentralized water infrastructure in under-connected rural environments."
      },
      {
        "id": "oai:arXiv.org:2504.20294v1",
        "title": "mrCAD: Multimodal Refinement of Computer-aided Designs",
        "link": "https://arxiv.org/abs/2504.20294",
        "author": "William P. McCarthy, Saujas Vaduguru, Karl D. D. Willis, Justin Matejka, Judith E. Fan, Daniel Fried, Yewen Pu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20294v1 Announce Type: cross \nAbstract: A key feature of human collaboration is the ability to iteratively refine the concepts we have communicated. In contrast, while generative AI excels at the \\textit{generation} of content, it often struggles to make specific language-guided \\textit{modifications} of its prior outputs. To bridge the gap between how humans and machines perform edits, we present mrCAD, a dataset of multimodal instructions in a communication game. In each game, players created computer aided designs (CADs) and refined them over several rounds to match specific target designs. Only one player, the Designer, could see the target, and they must instruct the other player, the Maker, using text, drawing, or a combination of modalities. mrCAD consists of 6,082 communication games, 15,163 instruction-execution rounds, played between 1,092 pairs of human players. We analyze the dataset and find that generation and refinement instructions differ in their composition of drawing and text. Using the mrCAD task as a benchmark, we find that state-of-the-art VLMs are better at following generation instructions than refinement instructions. These results lay a foundation for analyzing and modeling a multimodal language of refinement that is not represented in previous datasets."
      },
      {
        "id": "oai:arXiv.org:2504.20339v1",
        "title": "DRO: Doppler-Aware Direct Radar Odometry",
        "link": "https://arxiv.org/abs/2504.20339",
        "author": "Cedric Le Gentil, Leonardo Brizi, Daniil Lisus, Xinyuan Qiao, Giorgio Grisetti, Timothy D. Barfoot",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20339v1 Announce Type: cross \nAbstract: A renaissance in radar-based sensing for mobile robotic applications is underway. Compared to cameras or lidars, millimetre-wave radars have the ability to `see' through thin walls, vegetation, and adversarial weather conditions such as heavy rain, fog, snow, and dust. In this paper, we propose a novel SE(2) odometry approach for spinning frequency-modulated continuous-wave radars. Our method performs scan-to-local-map registration of the incoming radar data in a direct manner using all the radar intensity information without the need for feature or point cloud extraction. The method performs locally continuous trajectory estimation and accounts for both motion and Doppler distortion of the radar scans. If the radar possesses a specific frequency modulation pattern that makes radial Doppler velocities observable, an additional Doppler-based constraint is formulated to improve the velocity estimate and enable odometry in geometrically feature-deprived scenarios (e.g., featureless tunnels). Our method has been validated on over 250km of on-road data sourced from public datasets (Boreas and MulRan) and collected using our automotive platform. With the aid of a gyroscope, it outperforms state-of-the-art methods and achieves an average relative translation error of 0.26% on the Boreas leaderboard. When using data with the appropriate Doppler-enabling frequency modulation pattern, the translation error is reduced to 0.18% in similar environments. We also benchmarked our algorithm using 1.5 hours of data collected with a mobile robot in off-road environments with various levels of structure to demonstrate its versatility. Our real-time implementation is publicly available: https://github.com/utiasASRL/dro."
      },
      {
        "id": "oai:arXiv.org:2504.20340v1",
        "title": "A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks",
        "link": "https://arxiv.org/abs/2504.20340",
        "author": "Khoi Trinh, Scott Seidenberger, Raveen Wijewickrama, Murtuza Jadliwala, Anindya Maiti",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20340v1 Announce Type: cross \nAbstract: With AI-generated content becoming ubiquitous across the web, social media, and other digital platforms, it is vital to examine how such content are inspired and generated. The creation of AI-generated images often involves refining the input prompt iteratively to achieve desired visual outcomes. This study focuses on the relatively underexplored concept of image regeneration using AI, in which a human operator attempts to closely recreate a specific target image by iteratively refining their prompt. Image regeneration is distinct from normal image generation, which lacks any predefined visual reference. A separate challenge lies in determining whether existing image similarity metrics (ISMs) can provide reliable, objective feedback in iterative workflows, given that we do not fully understand if subjective human judgments of similarity align with these metrics. Consequently, we must first validate their alignment with human perception before assessing their potential as a feedback mechanism in the iterative prompt refinement process. To address these research gaps, we present a structured user study evaluating how iterative prompt refinement affects the similarity of regenerated images relative to their targets, while also examining whether ISMs capture the same improvements perceived by human observers. Our findings suggest that incremental prompt adjustments substantially improve alignment, verified through both subjective evaluations and quantitative measures, underscoring the broader potential of iterative workflows to enhance generative AI content creation across various application domains."
      },
      {
        "id": "oai:arXiv.org:2504.20368v1",
        "title": "AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury",
        "link": "https://arxiv.org/abs/2504.20368",
        "author": "David Gordon, Panayiotis Petousis, Susanne B. Nicholas, Alex A. T. Bui",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20368v1 Announce Type: cross \nAbstract: Diagnostic reasoning entails a physician's local (mental) model based on an assumed or known shared perspective (global model) to explain patient observations with evidence assigned towards a clinical assessment. But in several (complex) medical situations, multiple experts work together as a team to optimize health evaluation and decision-making by leveraging different perspectives. Such consensus-driven reasoning reflects individual knowledge contributing toward a broader perspective on the patient. In this light, we introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework automating the learning of these global models and their incorporation as prior beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof of concept with a prosocial MAS application for predicting acute kidney injuries (AKIs). In this case, we found that incorporating a global structure enabled multiple agents to achieve better performance (average precision, AP) in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT, AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs. baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180) for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents with higher recall scores reported lower confidence levels in the initial round on true positive and false negative cases. But after explicit interactions, their confidence in their decisions increased (suggesting reinforced belief). In contrast, the SF-FT agent with the lowest recall decreased its confidence in true positive and false negative cases (suggesting a new belief). This approach suggests that learning and leveraging global structures in MAS is necessary prior to achieving competitive classification and diagnostic reasoning performance."
      },
      {
        "id": "oai:arXiv.org:2504.20395v1",
        "title": "Partial Answer of How Transformers Learn Automata",
        "link": "https://arxiv.org/abs/2504.20395",
        "author": "Tiantian (Crystal),  Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20395v1 Announce Type: cross \nAbstract: We introduce a novel framework for simulating finite automata using representation-theoretic semidirect products and Fourier modules, achieving more efficient Transformer-based implementations."
      },
      {
        "id": "oai:arXiv.org:2504.20401v1",
        "title": "Nonlinear Computation with Linear Optics via Source-Position Encoding",
        "link": "https://arxiv.org/abs/2504.20401",
        "author": "N. Richardson, C. Bosch, R. P. Adams",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20401v1 Announce Type: cross \nAbstract: Optical computing systems provide an alternate hardware model which appears to be aligned with the demands of neural network workloads. However, the challenge of implementing energy efficient nonlinearities in optics -- a key requirement for realizing neural networks -- is a conspicuous missing link. In this work we introduce a novel method to achieve nonlinear computation in fully linear media. Our method can operate at low power and requires only the ability to drive the optical system at a data-dependent spatial position. Leveraging this positional encoding, we formulate a fully automated, topology-optimization-based hardware design framework for extremely specialized optical neural networks, drawing on modern advancements in optimization and machine learning. We evaluate our optical designs on machine learning classification tasks: demonstrating significant improvements over linear methods, and competitive performance when compared to standard artificial neural networks."
      },
      {
        "id": "oai:arXiv.org:2504.20403v1",
        "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.20403",
        "author": "Hanxi Liu, Yifang Men, Zhouhui Lian",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20403v1 Announce Type: cross \nAbstract: Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars."
      },
      {
        "id": "oai:arXiv.org:2504.20405v1",
        "title": "SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses",
        "link": "https://arxiv.org/abs/2504.20405",
        "author": "Sahil Sethi, Sai Reddy, Mansi Sakarvadia, Jordan Serotte, Darlington Nwaudo, Nicholas Maassen, Lewis Shi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20405v1 Announce Type: cross \nAbstract: While deep learning has shown strong performance in musculoskeletal imaging, existing work has largely focused on pathologies where diagnosis is not a clinical challenge, leaving more difficult problems underexplored, such as detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard MRIs. Diagnosing these lesions is challenging due to their subtle imaging features, often leading to reliance on invasive MRI arthrograms (MRAs). This study introduces ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies, and presents a deep learning (DL) framework for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes 586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent arthroscopy. Ground truth labels were derived from intraoperative findings, the gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were trained using a combination of CNNs and transformers. Predictions from sagittal, axial, and coronal views were ensembled to optimize performance. The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71 standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83% and 94%, and specificity of 91% and 86% for standard MRIs and MRAs, respectively. Notably, model performance on non-invasive standard MRIs matched or surpassed radiologists interpreting MRAs. External validation demonstrated initial generalizability across imaging protocols. This study demonstrates that DL models can achieve radiologist-level diagnostic performance on standard MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular codebase for training and evaluating deep learning models on 3D medical imaging data, we aim to accelerate research in musculoskeletal imaging and support the development of new datasets for clinically challenging diagnostic tasks."
      },
      {
        "id": "oai:arXiv.org:2504.20454v1",
        "title": "LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight",
        "link": "https://arxiv.org/abs/2504.20454",
        "author": "Jiajun Ding, Beiyao Zhu, Xiaosheng Liu, Lishen Zhang, Zhao Liu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20454v1 Announce Type: cross \nAbstract: This study integrates PET metabolic information with CT anatomical structures to establish a 3D multimodal segmentation dataset for lymphoma based on whole-body FDG PET/CT examinations, which bridges the gap of the lack of standardised multimodal segmentation datasets in the field of haematological malignancies. We retrospectively collected 483 examination datasets acquired between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were rigorously de-identified. Complete 3D structural information was preserved during data acquisition, preprocessing and annotation, and a high-quality dataset was constructed based on the nnUNet format. By systematic technical validation and evaluation of the preprocessing process, annotation quality and automatic segmentation algorithm, the deep learning model trained based on this dataset is verified to achieve accurate segmentation of lymphoma lesions in PET/CT images with high accuracy, good robustness and reproducibility, which proves the applicability and stability of this dataset in accurate segmentation and quantitative analysis. The deep fusion of PET/CT images achieved with this dataset not only significantly improves the accurate portrayal of the morphology, location and metabolic features of tumour lesions, but also provides solid data support for early diagnosis, clinical staging and personalized treatment, and promotes the development of automated image segmentation and precision medicine based on deep learning. The dataset and related resources are available at https://github.com/SuperD0122/LymphAtlas-."
      },
      {
        "id": "oai:arXiv.org:2504.20458v1",
        "title": "Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User",
        "link": "https://arxiv.org/abs/2504.20458",
        "author": "Xiaolei Wang, Chunxuan Xia, Junyi Li, Fanzhe Meng, Lei Huang, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20458v1 Announce Type: cross \nAbstract: Conversational recommendation systems (CRSs) use multi-turn interaction to capture user preferences and provide personalized recommendations. A fundamental challenge in CRSs lies in effectively understanding user preferences from conversations. User preferences can be multifaceted and complex, posing significant challenges for accurate recommendations even with access to abundant external knowledge. While interaction with users can clarify their true preferences, frequent user involvement can lead to a degraded user experience.\n  To address this problem, we propose a generative reward model based simulated user, named GRSU, for automatic interaction with CRSs. The simulated user provides feedback to the items recommended by CRSs, enabling them to better capture intricate user preferences through multi-turn interaction. Inspired by generative reward models, we design two types of feedback actions for the simulated user: i.e., generative item scoring, which offers coarse-grained feedback, and attribute-based item critique, which provides fine-grained feedback. To ensure seamless integration, these feedback actions are unified into an instruction-based format, allowing the development of a unified simulated user via instruction tuning on synthesized data. With this simulated user, automatic multi-turn interaction with CRSs can be effectively conducted. Furthermore, to strike a balance between effectiveness and efficiency, we draw inspiration from the paradigm of reward-guided search in complex reasoning tasks and employ beam search for the interaction process. On top of this, we propose an efficient candidate ranking method to improve the recommendation results derived from interaction. Extensive experiments on public datasets demonstrate the effectiveness, efficiency, and transferability of our approach."
      },
      {
        "id": "oai:arXiv.org:2504.20479v1",
        "title": "Full-field surrogate modeling of cardiac function encoding geometric variability",
        "link": "https://arxiv.org/abs/2504.20479",
        "author": "Elena Martinez, Beatrice Moscoloni, Matteo Salvador, Fanwei Kong, Mathias Peirlinck, Alison Lesley Marsden",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20479v1 Announce Type: cross \nAbstract: Combining physics-based modeling with data-driven methods is critical to enabling the translation of computational methods to clinical use in cardiology. The use of rigorous differential equations combined with machine learning tools allows for model personalization with uncertainty quantification in time frames compatible with clinical practice. However, accurate and efficient surrogate models of cardiac function, built from physics-based numerical simulation, are still mostly geometry-specific and require retraining for different patients and pathological conditions. We propose a novel computational pipeline to embed cardiac anatomies into full-field surrogate models. We generate a dataset of electrophysiology simulations using a complex multi-scale mathematical model coupling partial and ordinary differential equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective scientific machine learning method to encode activation maps extracted from physics-based numerical simulations into a neural network. Leveraging large deformation diffeomorphic metric mappings, we build a biventricular anatomical atlas and parametrize the anatomical variability of a small and challenging cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a novel statistical shape modeling based z-score sampling approach to generate a new synthetic cohort of 52 biventricular geometries that are compatible with the original geometrical variability. This synthetic cohort acts as the training set for BLNMs. Our surrogate model demonstrates robustness and great generalization across the complex original patient cohort, achieving an average adimensional mean squared error of 0.0034. The Python implementation of our BLNM model is publicly available under MIT License at https://github.com/StanfordCBCL/BLNM."
      },
      {
        "id": "oai:arXiv.org:2504.20501v1",
        "title": "SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation",
        "link": "https://arxiv.org/abs/2504.20501",
        "author": "Jia Wang, Yunan Mei, Jiarui Liu, Xin Fan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20501v1 Announce Type: cross \nAbstract: One-shot medical image segmentation (MIS) is crucial for medical analysis due to the burden of medical experts on manual annotation. The recent emergence of the segment anything model (SAM) has demonstrated remarkable adaptation in MIS but cannot be directly applied to one-shot medical image segmentation (MIS) due to its reliance on labor-intensive user interactions and the high computational cost. To cope with these limitations, we propose a novel SAM-guided robust representation learning framework, named RRL-MedSAM, to adapt SAM to one-shot 3D MIS, which exploits the strong generalization capabilities of the SAM encoder to learn better feature representation. We devise a dual-stage knowledge distillation (DSKD) strategy to distill general knowledge between natural and medical images from the foundation model to train a lightweight encoder, and then adopt a mutual exponential moving average (mutual-EMA) to update the weights of the general lightweight encoder and medical-specific encoder. Specifically, pseudo labels from the registration network are used to perform mutual supervision for such two encoders. Moreover, we introduce an auto-prompting (AP) segmentation decoder which adopts the mask generated from the general lightweight model as a prompt to assist the medical-specific model in boosting the final segmentation performance. Extensive experiments conducted on three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed RRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both segmentation and registration tasks. Especially, our lightweight encoder uses only 3\\% of the parameters compared to the encoder of SAM-Base."
      },
      {
        "id": "oai:arXiv.org:2504.20504v1",
        "title": "Quality-factor inspired deep neural network solver for solving inverse scattering problems",
        "link": "https://arxiv.org/abs/2504.20504",
        "author": "Yutong Du, Zicheng Liu, Miao Cao, Zupeng Liang, Yali Zong, Changyou Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20504v1 Announce Type: cross \nAbstract: Deep neural networks have been applied to address electromagnetic inverse scattering problems (ISPs) and shown superior imaging performances, which can be affected by the training dataset, the network architecture and the applied loss function. Here, the quality of data samples is cared and valued by the defined quality factor. Based on the quality factor, the composition of the training dataset is optimized. The network architecture is integrated with the residual connections and channel attention mechanism to improve feature extraction. A loss function that incorporates data-fitting error, physical-information constraints and the desired feature of the solution is designed and analyzed to suppress the background artifacts and improve the reconstruction accuracy. Various numerical analysis are performed to demonstrate the superiority of the proposed quality-factor inspired deep neural network (QuaDNN) solver and the imaging performance is finally verified by experimental imaging test."
      },
      {
        "id": "oai:arXiv.org:2504.20560v1",
        "title": "Generate more than one child in your co-evolutionary semi-supervised learning GAN",
        "link": "https://arxiv.org/abs/2504.20560",
        "author": "Francisco Sede\\~no, Jamal Toutouh, Francisco Chicano",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20560v1 Announce Type: cross \nAbstract: Generative Adversarial Networks (GANs) are very useful methods to address semi-supervised learning (SSL) datasets, thanks to their ability to generate samples similar to real data. This approach, called SSL-GAN has attracted many researchers in the last decade. Evolutionary algorithms have been used to guide the evolution and training of SSL-GANs with great success. In particular, several co-evolutionary approaches have been applied where the two networks of a GAN (the generator and the discriminator) are evolved in separate populations. The co-evolutionary approaches published to date assume some spatial structure of the populations, based on the ideas of cellular evolutionary algorithms. They also create one single individual per generation and follow a generational replacement strategy in the evolution. In this paper, we re-consider those algorithmic design decisions and propose a new co-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN), with panmictic population, elitist replacement, and more than one individual in the offspring. We evaluate the performance of our proposed method using three standard benchmark datasets. The results show that creating more than one offspring per population and using elitism improves the results in comparison with a classical SSL-GAN."
      },
      {
        "id": "oai:arXiv.org:2504.20584v1",
        "title": "Hydra: Marker-Free RGB-D Hand-Eye Calibration",
        "link": "https://arxiv.org/abs/2504.20584",
        "author": "Martin Huber, Huanyu Tian, Christopher E. Mower, Lucas-Raphael M\\\"uller, S\\'ebastien Ourselin, Christos Bergeles, Tom Vercauteren",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20584v1 Announce Type: cross \nAbstract: This work presents an RGB-D imaging-based approach to marker-free hand-eye calibration using a novel implementation of the iterative closest point (ICP) algorithm with a robust point-to-plane (PTP) objective formulated on a Lie algebra. Its applicability is demonstrated through comprehensive experiments using three well known serial manipulators and two RGB-D cameras. With only three randomly chosen robot configurations, our approach achieves approximately 90% successful calibrations, demonstrating 2-3x higher convergence rates to the global optimum compared to both marker-based and marker-free baselines. We also report 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9 robot configurations over other marker-free methods. Our method exhibits significantly improved accuracy (5 mm in task space) over classical approaches (7 mm in task space) whilst being marker-free. The benchmarking dataset and code are open sourced under Apache 2.0 License, and a ROS 2 integration with robot abstraction is provided to facilitate deployment."
      },
      {
        "id": "oai:arXiv.org:2504.20595v1",
        "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
        "link": "https://arxiv.org/abs/2504.20595",
        "author": "Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, Luke Zettlemoyer",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20595v1 Announce Type: cross \nAbstract: We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model."
      },
      {
        "id": "oai:arXiv.org:2504.20617v1",
        "title": "Sobolev norm inconsistency of kernel interpolation",
        "link": "https://arxiv.org/abs/2504.20617",
        "author": "Yunfei Yang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20617v1 Announce Type: cross \nAbstract: We study the consistency of minimum-norm interpolation in reproducing kernel Hilbert spaces corresponding to bounded kernels. Our main result give lower bounds for the generalization error of the kernel interpolation measured in a continuous scale of norms that interpolate between $L^2$ and the hypothesis space. These lower bounds imply that kernel interpolation is always inconsistent, when the smoothness index of the norm is larger than a constant that depends only on the embedding index of the hypothesis space and the decay rate of the eigenvalues."
      },
      {
        "id": "oai:arXiv.org:2504.20620v1",
        "title": "Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal",
        "link": "https://arxiv.org/abs/2504.20620",
        "author": "Abhishek Pasula, Deepak N. Subramani",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20620v1 Announce Type: cross \nAbstract: Climate change alters ocean conditions, notably temperature and sea level. In the Bay of Bengal, these changes influence monsoon precipitation and marine productivity, critical to the Indian economy. In Phase 6 of the Coupled Model Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different shared socioeconomic pathways (SSPs) to obtain future climate projections. However, significant discrepancies are observed between these models and the reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean square error (RMSE) between the climate model output and the Ocean Reanalysis System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the dynamic sea level (DSL). We introduce a new data-driven deep learning model to correct for this bias. The deep neural model for each variable is trained using pairs of climatology-removed monthly climate projections as input and the corresponding month's ORAS5 as output. This model is trained with historical data (1950 to 2014), validated with future projection data from 2015 to 2020, and tested with future projections from 2021 to 2023. Compared to the conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical method for bias correction in climate models, our approach decreases RMSE by 0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the projections for 2024-2100. A detailed analysis of the monthly, seasonal, and decadal means and variability is performed to underscore the implications of the novel dynamics uncovered in our corrected projections."
      },
      {
        "id": "oai:arXiv.org:2504.20630v1",
        "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
        "link": "https://arxiv.org/abs/2504.20630",
        "author": "Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20630v1 Announce Type: cross \nAbstract: Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo."
      },
      {
        "id": "oai:arXiv.org:2504.20634v1",
        "title": "On Stochastic Rounding with Few Random Bits",
        "link": "https://arxiv.org/abs/2504.20634",
        "author": "Andrew Fitzgibbon, Stephen Felix",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20634v1 Announce Type: cross \nAbstract: Large-scale numerical computations make increasing use of low-precision (LP) floating point formats and mixed precision arithmetic, which can be enhanced by the technique of stochastic rounding (SR), that is, rounding an intermediate high-precision value up or down randomly as a function of the value's distance to the two rounding candidates. Stochastic rounding requires, in addition to the high-precision input value, a source of random bits. As the provision of high-quality random bits is an additional computational cost, it is of interest to require as few bits as possible while maintaining the desirable properties of SR in a given computation, or computational domain. This paper examines a number of possible implementations of few-bit stochastic rounding (FBSR), and shows how several natural implementations can introduce sometimes significant bias into the rounding process, which are not present in the case of infinite-bit, infinite-precision examinations of these implementations. The paper explores the impact of these biases in machine learning examples, and hence opens another class of configuration parameters of which practitioners should be aware when developing or adopting low-precision floating point. Code is available at http://github.com/graphcore-research/arith25-stochastic-rounding."
      },
      {
        "id": "oai:arXiv.org:2504.20651v1",
        "title": "Learning and Generalization with Mixture Data",
        "link": "https://arxiv.org/abs/2504.20651",
        "author": "Harsh Vardhan, Avishek Ghosh, Arya Mazumdar",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20651v1 Announce Type: cross \nAbstract: In many, if not most, machine learning applications the training data is naturally heterogeneous (e.g. federated learning, adversarial attacks and domain adaptation in neural net training). Data heterogeneity is identified as one of the major challenges in modern day large-scale learning. A classical way to represent heterogeneous data is via a mixture model. In this paper, we study generalization performance and statistical rates when data is sampled from a mixture distribution. We first characterize the heterogeneity of the mixture in terms of the pairwise total variation distance of the sub-population distributions. Thereafter, as a central theme of this paper, we characterize the range where the mixture may be treated as a single (homogeneous) distribution for learning. In particular, we study the generalization performance under the classical PAC framework and the statistical error rates for parametric (linear regression, mixture of hyperplanes) as well as non-parametric (Lipschitz, convex and H\\\"older-smooth) regression problems. In order to do this, we obtain Rademacher complexity and (local) Gaussian complexity bounds with mixture data, and apply them to get the generalization and convergence rates respectively. We observe that as the (regression) function classes get more complex, the requirement on the pairwise total variation distance gets stringent, which matches our intuition. We also do a finer analysis for the case of mixed linear regression and provide a tight bound on the generalization error in terms of heterogeneity."
      },
      {
        "id": "oai:arXiv.org:2504.20720v1",
        "title": "Learning a General Model: Folding Clothing with Topological Dynamics",
        "link": "https://arxiv.org/abs/2504.20720",
        "author": "Yiming Liu, Lijun Han, Enlin Gu, Hesheng Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20720v1 Announce Type: cross \nAbstract: The high degrees of freedom and complex structure of garments present significant challenges for clothing manipulation. In this paper, we propose a general topological dynamics model to fold complex clothing. By utilizing the visible folding structure as the topological skeleton, we design a novel topological graph to represent the clothing state. This topological graph is low-dimensional and applied for complex clothing in various folding states. It indicates the constraints of clothing and enables predictions regarding clothing movement. To extract graphs from self-occlusion, we apply semantic segmentation to analyze the occlusion relationships and decompose the clothing structure. The decomposed structure is then combined with keypoint detection to generate the topological graph. To analyze the behavior of the topological graph, we employ an improved Graph Neural Network (GNN) to learn the general dynamics. The GNN model can predict the deformation of clothing and is employed to calculate the deformation Jacobi matrix for control. Experiments using jackets validate the algorithm's effectiveness to recognize and fold complex clothing with self-occlusion."
      },
      {
        "id": "oai:arXiv.org:2504.20726v1",
        "title": "Enhancing Vulnerability Reports with Automated and Augmented Description Summarization",
        "link": "https://arxiv.org/abs/2504.20726",
        "author": "Hattan Althebeiti, Mohammed Alkinoon, Manar Mohaisen, Saeed Salem, DaeHun Nyang, David Mohaisen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20726v1 Announce Type: cross \nAbstract: Public vulnerability databases, such as the National Vulnerability Database (NVD), document vulnerabilities and facilitate threat information sharing. However, they often suffer from short descriptions and outdated or insufficient information. In this paper, we introduce Zad, a system designed to enrich NVD vulnerability descriptions by leveraging external resources. Zad consists of two pipelines: one collects and filters supplementary data using two encoders to build a detailed dataset, while the other fine-tunes a pre-trained model on this dataset to generate enriched descriptions. By addressing brevity and improving content quality, Zad produces more comprehensive and cohesive vulnerability descriptions. We evaluate Zad using standard summarization metrics and human assessments, demonstrating its effectiveness in enhancing vulnerability information."
      },
      {
        "id": "oai:arXiv.org:2504.20736v1",
        "title": "A Survey on Event-based Optical Marker Systems",
        "link": "https://arxiv.org/abs/2504.20736",
        "author": "Nafiseh Jabbari Tofighi, Maxime Robic, Fabio Morbidi, Pascal Vasseur",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20736v1 Announce Type: cross \nAbstract: The advent of event-based cameras, with their low latency, high dynamic range, and reduced power consumption, marked a significant change in robotic vision and machine perception. In particular, the combination of these neuromorphic sensors with widely-available passive or active optical markers (e.g. AprilTags, arrays of blinking LEDs), has recently opened up a wide field of possibilities. This survey paper provides a comprehensive review on Event-Based Optical Marker Systems (EBOMS). We analyze the basic principles and technologies on which these systems are based, with a special focus on their asynchronous operation and robustness against adverse lighting conditions. We also describe the most relevant applications of EBOMS, including object detection and tracking, pose estimation, and optical communication. The article concludes with a discussion of possible future research directions in this rapidly-emerging and multidisciplinary field."
      },
      {
        "id": "oai:arXiv.org:2504.20741v1",
        "title": "In defence of post-hoc explanations in medical AI",
        "link": "https://arxiv.org/abs/2504.20741",
        "author": "Joshua Hatherley, Lauritz Munch, Jens Christian Bjerring",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20741v1 Announce Type: cross \nAbstract: Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a \"silver bullet\" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI."
      },
      {
        "id": "oai:arXiv.org:2504.20784v1",
        "title": "Approximate Lifted Model Construction",
        "link": "https://arxiv.org/abs/2504.20784",
        "author": "Malte Luttermann, Jan Speller, Marcel Gehrke, Tanya Braun, Ralf M\\\"oller, Mattis Hartwig",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20784v1 Announce Type: cross \nAbstract: Probabilistic relational models such as parametric factor graphs enable efficient (lifted) inference by exploiting the indistinguishability of objects. In lifted inference, a representative of indistinguishable objects is used for computations. To obtain a relational (i.e., lifted) representation, the Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP algorithm, however, requires underlying distributions, encoded as potential-based factorisations, to exactly match to identify and exploit indistinguishabilities. Hence, ACP is unsuitable for practical applications where potentials learned from data inevitably deviate even if associated objects are indistinguishable. To mitigate this problem, we introduce the $\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which allows for a deviation of potentials depending on a hyperparameter $\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits indistinguishabilities that are not exact. We prove that the approximation error induced by $\\varepsilon$-ACP is strictly bounded and our experiments show that the approximation error is close to zero in practice."
      },
      {
        "id": "oai:arXiv.org:2504.20808v1",
        "title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings",
        "link": "https://arxiv.org/abs/2504.20808",
        "author": "Florian Vahl, J\\\"orn Griepenburg, Jan Gutsche, Jasper G\\\"uldenstein, Jianwei Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20808v1 Announce Type: cross \nAbstract: This paper introduces SoccerDiffusion, a transformer-based diffusion model designed to learn end-to-end control policies for humanoid robot soccer directly from real-world gameplay recordings. Using data collected from RoboCup competitions, the model predicts joint command trajectories from multi-modal sensor inputs, including vision, proprioception, and game state. We employ a distillation technique to enable real-time inference on embedded platforms that reduces the multi-step diffusion process to a single step. Our results demonstrate the model's ability to replicate complex motion behaviors such as walking, kicking, and fall recovery both in simulation and on physical robots. Although high-level tactical behavior remains limited, this work provides a robust foundation for subsequent reinforcement learning or preference optimization methods. We release the dataset, pretrained models, and code under: https://bit-bots.github.io/SoccerDiffusion"
      },
      {
        "id": "oai:arXiv.org:2504.20859v1",
        "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation",
        "link": "https://arxiv.org/abs/2504.20859",
        "author": "Guy Hadad, Haggai Roitman, Yotam Eshel, Bracha Shapira, Lior Rokach",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20859v1 Announce Type: cross \nAbstract: As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2504.20877v1",
        "title": "Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms",
        "link": "https://arxiv.org/abs/2504.20877",
        "author": "Meltem Tatl{\\i}, Arpan Mukherjee, Prashanth L. A., Karthikeyan Shanmugam, Ali Tajer",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20877v1 Announce Type: cross \nAbstract: The objective of canonical multi-armed bandits is to identify and repeatedly select an arm with the largest reward, often in the form of the expected value of the arm's probability distribution. Such a utilitarian perspective and focus on the probability models' first moments, however, is agnostic to the distributions' tail behavior and their implications for variability and risks in decision-making. This paper introduces a principled framework for shifting from expectation-based evaluation to an alternative reward formulation, termed a preference metric (PM). The PMs can place the desired emphasis on different reward realization and can encode a richer modeling of preferences that incorporate risk aversion, robustness, or other desired attitudes toward uncertainty. A fundamentally distinct observation in such a PM-centric perspective is that designing bandit algorithms will have a significantly different principle: as opposed to the reward-based models in which the optimal sampling policy converges to repeatedly sampling from the single best arm, in the PM-centric framework the optimal policy converges to selecting a mix of arms based on specific mixing weights. Designing such mixture policies departs from the principles for designing bandit algorithms in significant ways, primarily because of uncountable mixture possibilities. The paper formalizes the PM-centric framework and presents two algorithm classes (horizon-dependent and anytime) that learn and track mixtures in a regret-efficient fashion. These algorithms have two distinctions from their canonical counterparts: (i) they involve an estimation routine to form reliable estimates of optimal mixtures, and (ii) they are equipped with tracking mechanisms to navigate arm selection fractions to track the optimal mixtures. These algorithms' regret guarantees are investigated under various algebraic forms of the PMs."
      },
      {
        "id": "oai:arXiv.org:2504.20879v1",
        "title": "The Leaderboard Illusion",
        "link": "https://arxiv.org/abs/2504.20879",
        "author": "Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet \\\"Ust\\\"un, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20879v1 Announce Type: cross \nAbstract: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field"
      },
      {
        "id": "oai:arXiv.org:2504.20883v1",
        "title": "Guessing Efficiently for Constrained Subspace Approximation",
        "link": "https://arxiv.org/abs/2504.20883",
        "author": "Aditya Bhaskara, Sepideh Mahabadi, Madhusudhan Reddy Pittu, Ali Vakilian, David P. Woodruff",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20883v1 Announce Type: cross \nAbstract: In this paper we study constrained subspace approximation problem. Given a set of $n$ points $\\{a_1,\\ldots,a_n\\}$ in $\\mathbb{R}^d$, the goal of the {\\em subspace approximation} problem is to find a $k$ dimensional subspace that best approximates the input points. More precisely, for a given $p\\geq 1$, we aim to minimize the $p$th power of the $\\ell_p$ norm of the error vector $(\\|a_1-\\bm{P}a_1\\|,\\ldots,\\|a_n-\\bm{P}a_n\\|)$, where $\\bm{P}$ denotes the projection matrix onto the subspace and the norms are Euclidean. In \\emph{constrained} subspace approximation (CSA), we additionally have constraints on the projection matrix $\\bm{P}$. In its most general form, we require $\\bm{P}$ to belong to a given subset $\\mathcal{S}$ that is described explicitly or implicitly.\n  We introduce a general framework for constrained subspace approximation. Our approach, that we term coreset-guess-solve, yields either $(1+\\varepsilon)$-multiplicative or $\\varepsilon$-additive approximations for a variety of constraints. We show that it provides new algorithms for partition-constrained subspace approximation with applications to {\\it fair} subspace approximation, $k$-means clustering, and projected non-negative matrix factorization, among others. Specifically, while we reconstruct the best known bounds for $k$-means clustering in Euclidean spaces, we improve the known results for the remainder of the problems."
      },
      {
        "id": "oai:arXiv.org:2504.20898v1",
        "title": "CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models",
        "link": "https://arxiv.org/abs/2504.20898",
        "author": "Hasan Md Tusfiqur Alam, Devansh Srivastav, Abdulrahman Mohamed Selim, Md Abdul Kadir, Md Moktadiurl Hoque Shuvo, Daniel Sonntag",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20898v1 Announce Type: cross \nAbstract: Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights."
      },
      {
        "id": "oai:arXiv.org:2504.20904v1",
        "title": "Dual Explanations via Subgraph Matching for Malware Detection",
        "link": "https://arxiv.org/abs/2504.20904",
        "author": "Hossein Shokouhinejad, Roozbeh Razavi-Far, Griffin Higgins, Ali A. Ghorbani",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20904v1 Announce Type: cross \nAbstract: Interpretable malware detection is crucial for understanding harmful behaviors and building trust in automated security systems. Traditional explainable methods for Graph Neural Networks (GNNs) often highlight important regions within a graph but fail to associate them with known benign or malicious behavioral patterns. This limitation reduces their utility in security contexts, where alignment with verified prototypes is essential. In this work, we introduce a novel dual prototype-driven explainable framework that interprets GNN-based malware detection decisions. This dual explainable framework integrates a base explainer (a state-of-the-art explainer) with a novel second-level explainer which is designed by subgraph matching technique, called SubMatch explainer. The proposed explainer assigns interpretable scores to nodes based on their association with matched subgraphs, offering a fine-grained distinction between benign and malicious regions. This prototype-guided scoring mechanism enables more interpretable, behavior-aligned explanations. Experimental results demonstrate that our method preserves high detection performance while significantly improving interpretability in malware analysis."
      },
      {
        "id": "oai:arXiv.org:2504.20906v1",
        "title": "GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems",
        "link": "https://arxiv.org/abs/2504.20906",
        "author": "Sarad Venugopalan, Sridhar Adepu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20906v1 Announce Type: cross \nAbstract: The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. Further, the time complexity of the anomaly detection scenario/problem at hand is lowered using dimensionality reduction of the actuator(s) in relationship with a sensor. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies and provide explainability; that are not simultaneously achieved by other state of the art AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we pin-point the sensor(s) and its actuation state for which anomaly was detected."
      },
      {
        "id": "oai:arXiv.org:2504.20923v1",
        "title": "End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation",
        "link": "https://arxiv.org/abs/2504.20923",
        "author": "Andrea Di Pierno (IMT School of Advanced Studies, Lucca, Italy, Department of Mathematics and Computer Science, University of Catania, Italy), Luca Guarnera (Department of Mathematics and Computer Science, University of Catania, Italy), Dario Allegra (Department of Mathematics and Computer Science, University of Catania, Italy), Sebastiano Battiato (Department of Mathematics and Computer Science, University of Catania, Italy)",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20923v1 Announce Type: cross \nAbstract: Audio deepfakes represent a growing threat to digital security and trust, leveraging advanced generative models to produce synthetic speech that closely mimics real human voices. Detecting such manipulations is especially challenging under open-world conditions, where spoofing methods encountered during testing may differ from those seen during training. In this work, we propose an end-to-end deep learning framework for audio deepfake detection that operates directly on raw waveforms. Our model, RawNetLite, is a lightweight convolutional-recurrent architecture designed to capture both spectral and temporal features without handcrafted preprocessing. To enhance robustness, we introduce a training strategy that combines data from multiple domains and adopts Focal Loss to emphasize difficult or ambiguous samples. We further demonstrate that incorporating codec-based manipulations and applying waveform-level audio augmentations (e.g., pitch shifting, noise, and time stretching) leads to significant generalization improvements under realistic acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging out-of-distribution test set (AVSpoof2021 + CodecFake). These findings highlight the importance of diverse training data, tailored objective functions and audio augmentations in building resilient and generalizable audio forgery detectors. Code and pretrained models are available at https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/."
      },
      {
        "id": "oai:arXiv.org:2504.20927v1",
        "title": "Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR",
        "link": "https://arxiv.org/abs/2504.20927",
        "author": "Shahbaz P Qadri Syed, He Bai",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20927v1 Announce Type: cross \nAbstract: Developing scalable and efficient reinforcement learning algorithms for cooperative multi-agent control has received significant attention over the past years. Existing literature has proposed inexact decompositions of local Q-functions based on empirical information structures between the agents. In this paper, we exploit inter-agent coupling information and propose a systematic approach to exactly decompose the local Q-function of each agent. We develop an approximate least square policy iteration algorithm based on the proposed decomposition and identify two architectures to learn the local Q-function for each agent. We establish that the worst-case sample complexity of the decomposition is equal to the centralized case and derive necessary and sufficient graphical conditions on the inter-agent couplings to achieve better sample efficiency. We demonstrate the improved sample efficiency and computational efficiency on numerical examples."
      },
      {
        "id": "oai:arXiv.org:2504.20930v1",
        "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification",
        "link": "https://arxiv.org/abs/2504.20930",
        "author": "Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20930v1 Announce Type: cross \nAbstract: Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs."
      },
      {
        "id": "oai:arXiv.org:2504.20940v1",
        "title": "Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data",
        "link": "https://arxiv.org/abs/2504.20940",
        "author": "Maximilian Stupp, P. S. Koutsourelakis",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20940v1 Announce Type: cross \nAbstract: Coarse-grained (CG) models offer an effective route to reducing the complexity of molecular simulations, yet conventional approaches depend heavily on long all-atom molecular dynamics (MD) trajectories to adequately sample configurational space. This data-driven dependence limits their accuracy and generalizability, as unvisited configurations remain excluded from the resulting CG model. We introduce a data-free generative framework for coarse-graining that directly targets the all-atom Boltzmann distribution. Our model defines a structured latent space comprising slow collective variables, which are statistically associated with multimodal marginal densities capturing metastable states, and fast variables, which represent the remaining degrees of freedom with simple, unimodal conditional distributions. A potentially learnable, bijective map from the full latent space to the all-atom configuration space enables automatic and accurate reconstruction of molecular structures. The model is trained using an energy-based objective that minimizes the reverse Kullback-Leibler divergence, relying solely on the interatomic potential rather than sampled trajectories. A tempering scheme is used to stabilize training and promote exploration of diverse configurations. Once trained, the model can generate unbiased, one-shot equilibrium all-atom samples. We validate the method on two synthetic systems-a double-well potential and a Gaussian mixture-as well as on the benchmark alanine dipeptide. The model captures all relevant modes of the Boltzmann distribution, accurately reconstructs atomic configurations, and learns physically meaningful coarse-grained representations, all without any simulation data."
      },
      {
        "id": "oai:arXiv.org:2504.20969v1",
        "title": "XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search",
        "link": "https://arxiv.org/abs/2504.20969",
        "author": "Yiting Zhang, Shichen Li, Elena Shrestha",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20969v1 Announce Type: cross \nAbstract: Mechanical search (MS) in cluttered environments remains a significant challenge for autonomous manipulators, requiring long-horizon planning and robust state estimation under occlusions and partial observability. In this work, we introduce XPG-RL, a reinforcement learning framework that enables agents to efficiently perform MS tasks through explainable, priority-guided decision-making based on raw sensory inputs. XPG-RL integrates a task-driven action prioritization mechanism with a learned context-aware switching strategy that dynamically selects from a discrete set of action primitives such as target grasping, occlusion removal, and viewpoint adjustment. Within this strategy, a policy is optimized to output adaptive threshold values that govern the discrete selection among action primitives. The perception module fuses RGB-D inputs with semantic and geometric features to produce a structured scene representation for downstream decision-making. Extensive experiments in both simulation and real-world settings demonstrate that XPG-RL consistently outperforms baseline methods in task success rates and motion efficiency, achieving up to 4.5$\\times$ higher efficiency in long-horizon tasks. These results underscore the benefits of integrating domain knowledge with learnable decision-making policies for robust and efficient robotic manipulation."
      },
      {
        "id": "oai:arXiv.org:2504.20982v1",
        "title": "Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling",
        "link": "https://arxiv.org/abs/2504.20982",
        "author": "Tyler Chen, Archan Ray, Akshay Seshadri, Dylan Herman, Bao Bach, Pranav Deshpande, Abhishek Som, Niraj Kumar, Marco Pistoia",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20982v1 Announce Type: cross \nAbstract: The $k$-means algorithm (Lloyd's algorithm) is a widely used method for clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that each iteration requires time linear in the number of data points, which can be expensive in big data applications. This was improved in recent works proposing quantum and quantum-inspired classical algorithms to approximate the $k$-means algorithm locally, in time depending only logarithmically on the number of data points (along with data dependent parameters) [$q$-means: A quantum algorithm for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash, NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this work, we describe a simple randomized mini-batch $k$-means algorithm and a quantum algorithm inspired by the classical algorithm. We prove worse-case guarantees that significantly improve upon the bounds for previous algorithms. Our improvements are due to a careful use of uniform sampling, which preserves certain symmetries of the $k$-means problem that are not preserved in previous algorithms that use data norm-based sampling."
      },
      {
        "id": "oai:arXiv.org:2504.20984v1",
        "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
        "link": "https://arxiv.org/abs/2504.20984",
        "author": "Evan Li, Tushin Mallick, Evan Rose, William Robertson, Alina Oprea, Cristina Nita-Rotaru",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20984v1 Announce Type: cross \nAbstract: LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.\n  In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that our system is secure against attacks from the INJECAGENT benchmark, a standard benchmark for control flow integrity in the face of indirect prompt injection attacks, and our newly introduced attacks. Our architecture represents a significant advancement towards hardening LLM-based systems containing system facilities of varying levels of trustworthiness."
      },
      {
        "id": "oai:arXiv.org:2205.12379v4",
        "title": "Gaussian Pre-Activations in Neural Networks: Myth or Reality?",
        "link": "https://arxiv.org/abs/2205.12379",
        "author": "Pierre Wolinski, Julyan Arbel",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2205.12379v4 Announce Type: replace \nAbstract: The study of feature propagation at initialization in neural networks lies at the root of numerous initialization designs. An assumption very commonly made in the field states that the pre-activations are Gaussian. Although this convenient Gaussian hypothesis can be justified when the number of neurons per layer tends to infinity, it is challenged by both theoretical and experimental works for finite-width neural networks. Our major contribution is to construct a family of pairs of activation functions and initialization distributions that ensure that the pre-activations remain Gaussian throughout the network's depth, even in narrow neural networks. In the process, we discover a set of constraints that a neural network should fulfill to ensure Gaussian pre-activations. Additionally, we provide a critical review of the claims of the Edge of Chaos line of works and build an exact Edge of Chaos analysis. We also propose a unified view on pre-activations propagation, encompassing the framework of several well-known initialization procedures. Finally, our work provides a principled framework for answering the much-debated question: is it desirable to initialize the training of a neural network whose pre-activations are ensured to be Gaussian? Our code is available on GitHub: https://github.com/p-wol/gaussian-preact/ ."
      },
      {
        "id": "oai:arXiv.org:2302.00671v3",
        "title": "QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing",
        "link": "https://arxiv.org/abs/2302.00671",
        "author": "Grace Zhang, Ayush Jain, Injune Hwang, Shao-Hua Sun, Joseph J. Lim",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2302.00671v3 Announce Type: replace \nAbstract: Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeled data between tasks. In this work, we introduce a new framework for sharing behavioral policies across tasks, which can be used in addition to existing MTRL methods. The key idea is to improve each task's off-policy data collection by employing behaviors from other task policies. Selectively sharing helpful behaviors acquired in one task to collect training data for another task can lead to higher-quality trajectories, leading to more sample-efficient MTRL. Thus, we introduce a simple and principled framework called Q-switch mixture of policies (QMP) that selectively shares behavior between different task policies by using the task's Q-function to evaluate and select useful shareable behaviors. We theoretically analyze how QMP improves the sample efficiency of the underlying RL algorithm. Our experiments show that QMP's behavioral policy sharing provides complementary gains over many popular MTRL algorithms and outperforms alternative ways to share behaviors in various manipulation, locomotion, and navigation environments. Videos are available at https://qmp-mtrl.github.io."
      },
      {
        "id": "oai:arXiv.org:2303.01903v4",
        "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
        "link": "https://arxiv.org/abs/2303.01903",
        "author": "Zhou Yu, Xuecheng Ouyang, Zhenwei Shao, Meng Wang, Jun Yu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2303.01903v4 Announce Type: replace \nAbstract: Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the \\emph{blind} LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. The two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. Prophet is general that can be instantiated with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones). Moreover, Prophet can also be integrated with modern large multimodal models in different stages, which is named Prophet++, to further improve the capabilities on knowledge-based VQA tasks."
      },
      {
        "id": "oai:arXiv.org:2303.16078v4",
        "title": "Practical solutions to the relative pose of three calibrated cameras",
        "link": "https://arxiv.org/abs/2303.16078",
        "author": "Charalambos Tzamos, Viktor Kocur, Yaqing Ding, Daniel Barath, Zuzana Berger Haladova, Torsten Sattler, Zuzana Kukelova",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2303.16078v4 Announce Type: replace \nAbstract: We study the challenging problem of estimating the relative pose of three calibrated cameras from four point correspondences. We propose novel efficient solutions to this problem that are based on the simple idea of using four correspondences to estimate an approximate geometry of the first two views. We model this geometry either as an affine or a fully perspective geometry estimated using one additional approximate correspondence. We generate such an approximate correspondence using a very simple and efficient strategy, where the new point is the mean point of three corresponding input points. The new solvers are efficient and easy to implement, since they are based on existing efficient minimal solvers, i.e., the 4-point affine fundamental matrix, the well-known 5-point relative pose solver, and the P3P solver. Extensive experiments on real data show that the proposed solvers, when properly coupled with local optimization, achieve state-of-the-art results, with the novel solver based on approximate mean-point correspondences being more robust and accurate than the affine-based solver."
      },
      {
        "id": "oai:arXiv.org:2306.07520v5",
        "title": "Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions",
        "link": "https://arxiv.org/abs/2306.07520",
        "author": "Weizhen He, Yiheng Deng, Shixiang Tang, Qihao Chen, Qingsong Xie, Yizhou Wang, Lei Bai, Feng Zhu, Rui Zhao, Wanli Ouyang, Donglian Qi, Yunfeng Yan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2306.07520v5 Announce Type: replace \nAbstract: Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a new instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Our instruct-ReID is a more general ReID setting, where existing 6 ReID tasks can be viewed as special cases by designing different instructions. We propose a large-scale OmniReID benchmark and an adaptive triplet loss as a baseline method to facilitate research in this new setting. Experimental results show that the proposed multi-purpose ReID model, trained on our OmniReID benchmark without fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17, CUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC for clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template based clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+ real2 for our newly defined language-instructed ReID, +4.3% on LLCM for visible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The datasets, the model, and code will be available at https://github.com/hwz-zju/Instruct-ReID."
      },
      {
        "id": "oai:arXiv.org:2307.13586v4",
        "title": "Settling the Sample Complexity of Online Reinforcement Learning",
        "link": "https://arxiv.org/abs/2307.13586",
        "author": "Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.13586v4 Announce Type: replace \nAbstract: A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.\n  We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \\cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \\begin{equation*}\n  \\min\\big\\{ \\sqrt{SAH^3K}, \\,HK \\big\\}, \\end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size $K\\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\\varepsilon$-accuracy) of $\\frac{SAH^3}{\\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\\varepsilon$-range.\n  Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency -- a long-standing challenge facing the analysis of online RL in the sample-hungry regime."
      },
      {
        "id": "oai:arXiv.org:2308.09138v2",
        "title": "Semantic Consistency for Assuring Reliability of Large Language Models",
        "link": "https://arxiv.org/abs/2308.09138",
        "author": "Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2308.09138v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold."
      },
      {
        "id": "oai:arXiv.org:2310.03903v3",
        "title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models",
        "link": "https://arxiv.org/abs/2310.03903",
        "author": "Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.03903v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination."
      },
      {
        "id": "oai:arXiv.org:2310.12059v4",
        "title": "Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education",
        "link": "https://arxiv.org/abs/2310.12059",
        "author": "Duc-Vu Nguyen, Quoc-Nam Nguyen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.12059v4 Announce Type: replace \nAbstract: In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only."
      },
      {
        "id": "oai:arXiv.org:2310.18511v3",
        "title": "3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition",
        "link": "https://arxiv.org/abs/2310.18511",
        "author": "Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, Mohamed Elhoseiny",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.18511v3 Announce Type: replace \nAbstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized at CVPR2023, showcasing the winning method's utilization of a modified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative techniques for GCR enhancement. We hope our work will help ease future research on compositional 3D Vision."
      },
      {
        "id": "oai:arXiv.org:2401.10266v2",
        "title": "Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies",
        "link": "https://arxiv.org/abs/2401.10266",
        "author": "Maryam Ahang, Todd Charter, Oluwaseyi Ogunfowora, Maziyar Khadivi, Mostafa Abbasi, Homayoun Najjaran",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.10266v2 Announce Type: replace \nAbstract: Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process (TEP) is conducted. This research will be beneficial for both researchers who are new to the field and experts, as it covers the literature on condition monitoring and state-of-the-art methods alongside the challenges and possible solutions to them."
      },
      {
        "id": "oai:arXiv.org:2403.14562v2",
        "title": "Agentic AI: The Era of Semantic Decoding",
        "link": "https://arxiv.org/abs/2403.14562",
        "author": "Maxime Peyrard, Martin Josifoski, Robert West",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.14562v2 Announce Type: replace \nAbstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation."
      },
      {
        "id": "oai:arXiv.org:2403.16184v3",
        "title": "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement",
        "link": "https://arxiv.org/abs/2403.16184",
        "author": "Yuxuan Wang, Xiaoyuan Liu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.16184v3 Announce Type: replace \nAbstract: Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG's representation, and significantly improve the performance."
      },
      {
        "id": "oai:arXiv.org:2405.01480v2",
        "title": "Common pitfalls to avoid while using multiobjective optimization in machine learning",
        "link": "https://arxiv.org/abs/2405.01480",
        "author": "Junaid Akhter, Paul David F\\\"ahrmann, Konstantin Sonntag, Sebastian Peitz, Daniel Schwietert",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.01480v2 Announce Type: replace \nAbstract: Recently, there has been an increasing interest in the application of multiobjective optimization (MOO) in machine learning (ML). This interest is driven by the numerous real-life situations where multiple objectives must be optimized simultaneously. A key aspect of MOO is the existence of a Pareto set, rather than a single optimal solution, which represents the optimal trade-offs between different objectives. Despite its potential, there is a noticeable lack of satisfactory literature serving as an entry-level guide for ML practitioners aiming to apply MOO effectively. In this paper, our goal is to provide such a resource and highlight pitfalls to avoid. We begin by establishing the groundwork for MOO, focusing on well-known approaches such as the weighted sum (WS) method, alongside more advanced techniques like the multiobjective gradient descent algorithm (MGDA). We critically review existing studies across various ML fields where MOO has been applied and identify challenges that can lead to incorrect interpretations. One of these fields is physics informed neural networks (PINNs), which we use as a guiding example to carefully construct experiments illustrating these pitfalls. By comparing WS and MGDA with one of the most common evolutionary algorithms, NSGA-II, we demonstrate that difficulties can arise regardless of the specific MOO method used. We emphasize the importance of understanding the specific problem, the objective space, and the selected MOO method, while also noting that neglecting factors such as convergence criteria can result in misleading experiments."
      },
      {
        "id": "oai:arXiv.org:2405.17790v2",
        "title": "Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification",
        "link": "https://arxiv.org/abs/2405.17790",
        "author": "Weizhen He, Yiheng Deng, Yunfeng Yan, Feng Zhu, Yizhou Wang, Lei Bai, Qingsong Xie, Donglian Qi, Wanli Ouyang, Shixiang Tang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.17790v2 Announce Type: replace \nAbstract: Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a novel instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Instruct-ReID is the first exploration of a general ReID setting, where existing 6 ReID tasks can be viewed as special cases by assigning different instructions. To facilitate research in this new instruct-ReID task, we propose a large-scale OmniReID++ benchmark equipped with diverse data and comprehensive evaluation methods e.g., task specific and task-free evaluation settings. In the task-specific evaluation setting, gallery sets are categorized according to specific ReID tasks. We propose a novel baseline model, IRM, with an adaptive triplet loss to handle various retrieval tasks within a unified framework. For task-free evaluation setting, where target person images are retrieved from task-agnostic gallery sets, we further propose a new method called IRM++ with novel memory bank-assisted learning. Extensive evaluations of IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our proposed methods, achieving state-of-the-art performance on 10 test sets. The datasets, the model, and the code will be available at https://github.com/hwz-zju/Instruct-ReID"
      },
      {
        "id": "oai:arXiv.org:2405.18554v3",
        "title": "Scalable Surrogate Verification of Image-based Neural Network Control Systems using Composition and Unrolling",
        "link": "https://arxiv.org/abs/2405.18554",
        "author": "Feiyang Cai, Chuchu Fan, Stanley Bak",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.18554v3 Announce Type: replace \nAbstract: Verifying safety of neural network control systems that use images as input is a difficult problem because, from a given system state, there is no known way to mathematically model what images are possible in the real-world. We build on recent work that considers a surrogate verification approach, training a conditional generative adversarial network (cGAN) as an image generator in place of the real world. This enables set-based formal analysis of the closed-loop system, providing analysis beyond simulation and testing. While existing work is effective on small examples, excessive overapproximation both within a single control period and across multiple control periods limits its scalability. We propose approaches to overcome these two sources of error. First, we overcome one-step error by composing the system's dynamics along with the cGAN and neural network controller, without losing the dependencies between input states and the control outputs as in the monotonic analysis of the system dynamics. Second, we reduce multi-step error by repeating the single-step composition, essentially unrolling multiple steps of the control loop into a large neural network. We then leverage existing network verification tools to compute accurate reachable sets for multiple steps, avoiding the accumulation of abstraction error at each step. We demonstrate the effectiveness of our approach in terms of both accuracy and scalability using two case studies: an autonomous aircraft taxiing system and an advanced emergency braking system. On the aircraft taxiing system, the converged reachable set is 175% larger using the prior baseline method compared with our proposed approach. On the emergency braking system, with 24x the number of image output variables from the cGAN, the baseline method fails to prove any states are safe, whereas our improvements enable set-based safety analysis."
      },
      {
        "id": "oai:arXiv.org:2407.03821v2",
        "title": "Seamless Monitoring of Stress Levels Leveraging a Universal Model for Time Sequences",
        "link": "https://arxiv.org/abs/2407.03821",
        "author": "Davide Gabrielli, Bardh Prenkaj, Paola Velardi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.03821v2 Announce Type: replace \nAbstract: Monitoring the stress level in patients with neurodegenerative diseases can help manage symptoms, improve patient's quality of life, and provide insight into disease progression. In the literature, ECG, actigraphy, speech, voice, and facial analysis have proven effective at detecting patients' emotions. On the other hand, these tools are invasive and do not integrate smoothly into the patient's daily life. HRV has also been proven to effectively indicate stress conditions, especially in combination with other signals. However, when HRV is derived from less invasive devices than the ECG, like wristbands and smartwatches, the quality of measurements significantly degrades. This paper presents a methodology for stress detection from a wristband based on a universal model for time series, UniTS, which we finetuned for the task and equipped with explainability features. We cast the problem as anomaly detection rather than classification to favor model adaptation to individual patients and allow the clinician to maintain greater control over the system's predictions. We demonstrate that our proposed model considerably surpasses 12 top-performing methods on three benchmark datasets. Furthermore, unlike other state-of-the-art systems, UniTS enables seamless monitoring, as it shows comparable performance when using signals from invasive or lightweight devices."
      },
      {
        "id": "oai:arXiv.org:2407.08010v2",
        "title": "A Self-organizing Interval Type-2 Fuzzy Neural Network for Multi-Step Time Series Prediction",
        "link": "https://arxiv.org/abs/2407.08010",
        "author": "Fulong Yao, Wanqing Zhao, Matthew Forshaw, Yang Song",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.08010v2 Announce Type: replace \nAbstract: Data uncertainty is inherent in many real-world applications and poses significant challenges for accurate time series predictions. The interval type 2 fuzzy neural network (IT2FNN) has shown exceptional performance in uncertainty modelling for single-step prediction tasks. However, extending it for multi-step ahead predictions introduces further issues in uncertainty handling as well as model interpretability and accuracy. To address these issues, this paper proposes a new selforganizing interval type-2 fuzzy neural network with multiple outputs (SOIT2FNN-MO). Differing from the traditional six-layer IT2FNN, a nine-layer network architecture is developed. First, a new co-antecedent layer and a modified consequent layer are devised to improve the interpretability of the fuzzy model for multi-step time series prediction problems. Second, a new link layer is created to improve the accuracy by building temporal connections between multi-step predictions. Third, a new transformation layer is designed to address the problem of the vanishing rule strength caused by high-dimensional inputs. Furthermore, a two-stage, self-organizing learning mechanism is developed to automatically extract fuzzy rules from data and optimize network parameters. Experimental results on chaotic and microgrid prediction problems demonstrate that SOIT2FNN-MO outperforms state-of-the-art methods, by achieving a better accuracy ranging from 1.6% to 30% depending on the level of noises in data. Additionally, the proposed model is more interpretable, offering deeper insights into the prediction process."
      },
      {
        "id": "oai:arXiv.org:2407.15229v2",
        "title": "A Practical Analysis of Human Alignment with *PO",
        "link": "https://arxiv.org/abs/2407.15229",
        "author": "Kian Ahrabian, Xihui Lin, Barun Patra, Vishrav Chaudhary, Alon Benhaim, Jay Pujara, Xia Song",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.15229v2 Announce Type: replace \nAbstract: At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario."
      },
      {
        "id": "oai:arXiv.org:2407.20152v2",
        "title": "Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems: An application on Hydrological Systems",
        "link": "https://arxiv.org/abs/2407.20152",
        "author": "Rahul Ghosh, Arvind Renganathan, Zac McEachran, Kelly Lindsay, Somya Sharma, Michael Steinbach, John Nieber, Christopher Duffy, Vipin Kumar",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.20152v2 Announce Type: replace \nAbstract: We present a framework for modeling multi-scale processes, and study its performance in the context of streamflow forecasting in hydrology. Specifically, we propose a novel hierarchical recurrent neural architecture that factorizes the system dynamics at multiple temporal scales and captures their interactions. This framework consists of an inverse and a forward model. The inverse model is used to empirically resolve the system's temporal modes from data (physical model simulations, observed data, or a combination of them from the past), and these states are then used in the forward model to predict streamflow. Experiments on several catchments from the National Weather Service North Central River Forecast Center show that FHNN outperforms standard baselines, including physics-based models and transformer-based approaches. The model demonstrates particular effectiveness in catchments with low runoff ratios and colder climates. We further validate FHNN on the CAMELS (Catchment Attributes and MEteorology for Large-sample Studies), which is a widely used continental-scale hydrology benchmark dataset, confirming consistent performance improvements for 1-7 day streamflow forecasts across diverse hydrological conditions. Additionally, we show that FHNN can maintain accuracy even with limited training data through effective pre-training strategies and training global models."
      },
      {
        "id": "oai:arXiv.org:2408.00137v2",
        "title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment",
        "link": "https://arxiv.org/abs/2408.00137",
        "author": "Sangwon Yu, Jongyoon Song, Bongkyu Hwang, Hoyoung Kang, Sooah Cho, Junhwa Choi, Seongho Joe, Taehee Lee, Youngjune L. Gwon, Sungroh Yoon",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.00137v2 Announce Type: replace \nAbstract: A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities."
      },
      {
        "id": "oai:arXiv.org:2408.13509v3",
        "title": "Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation",
        "link": "https://arxiv.org/abs/2408.13509",
        "author": "Ying Jin, Jinlong Peng, Qingdong He, Teng Hu, Jiafu Wu, Hao Chen, Haoxuan Wang, Wenbing Zhu, Mingmin Chi, Jun Liu, Yabiao Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.13509v3 Announce Type: replace \nAbstract: The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks."
      },
      {
        "id": "oai:arXiv.org:2408.16005v4",
        "title": "Many-Worlds Inverse Rendering",
        "link": "https://arxiv.org/abs/2408.16005",
        "author": "Ziyi Zhang, Nicolas Roussel, Wenzel Jakob",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.16005v4 Announce Type: replace \nAbstract: Discontinuous visibility changes remain a major bottleneck when optimizing surfaces within a physically-based inverse renderer. Many previous works have proposed sophisticated algorithms and data structures to sample visibility silhouettes more efficiently.\n  Our work presents another solution: instead of differentiating a tentative surface locally, we differentiate a volumetric perturbation of a surface. We refer this as a many-worlds representation because it models a non-interacting superposition of conflicting explanations (worlds) of the input dataset. Each world is optically isolated from others, leading to a new transport law that distinguishes our method from prior work based on exponential random media.\n  The resulting Monte Carlo algorithm is simpler and more efficient than prior methods. We demonstrate that our method promotes rapid convergence, both in terms of the total iteration count and the cost per iteration."
      },
      {
        "id": "oai:arXiv.org:2408.16690v4",
        "title": "Generic Objects as Pose Probes for Few-shot View Synthesis",
        "link": "https://arxiv.org/abs/2408.16690",
        "author": "Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.16690v4 Announce Type: replace \nAbstract: Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as \"pose probes\". The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \\href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this https URL}"
      },
      {
        "id": "oai:arXiv.org:2409.07394v2",
        "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge",
        "link": "https://arxiv.org/abs/2409.07394",
        "author": "Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.07394v2 Announce Type: replace \nAbstract: Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Across four LLMs, six question-answering (QA) and three summarization datasets, we demonstrate that ADACAD consistently outperforms other decoding baselines with average QA accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt performance when conflict is absent, ADACAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not."
      },
      {
        "id": "oai:arXiv.org:2409.16902v4",
        "title": "Underwater Camouflaged Object Tracking Meets Vision-Language SAM2",
        "link": "https://arxiv.org/abs/2409.16902",
        "author": "Chunhui Zhang, Li Liu, Guanjie Huang, Zhipeng Zhang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.16902v4 Announce Type: replace \nAbstract: Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \\eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Experimental results demonstrate that our VL-SAM2 achieves state-of-the-art performance on the UW-COT220 dataset. The dataset and codes are available at~\\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\\color{magenta}{here}}."
      },
      {
        "id": "oai:arXiv.org:2410.02102v2",
        "title": "Racing Thoughts: Explaining Contextualization Errors in Large Language Models",
        "link": "https://arxiv.org/abs/2410.02102",
        "author": "Michael A. Lepori, Michael C. Mozer, Asma Ghandeharioun",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02102v2 Announce Type: replace \nAbstract: The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt \"John is going fishing, so he walks over to the bank. Can he make an ATM transaction?\", a model may incorrectly respond \"Yes\" if it has not properly contextualized \"bank\" as a geographical feature, rather than a financial institution. We propose the LLM Race Conditions Hypothesis as an explanation of contextualization errors of this form. This hypothesis identifies dependencies between tokens (e.g., \"bank\" must be properly contextualized before the final token, \"?\", integrates information from \"bank\"), and claims that contextualization errors are a result of violating these dependencies. Using a variety of techniques from mechanistic intepretability, we provide correlational and causal evidence in support of the hypothesis, and suggest inference-time interventions to address it."
      },
      {
        "id": "oai:arXiv.org:2410.03883v2",
        "title": "DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction",
        "link": "https://arxiv.org/abs/2410.03883",
        "author": "Xinwei Zhang, Zhiqi Bu, Borja Balle, Mingyi Hong, Meisam Razaviyayn, Vahab Mirrokni",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03883v2 Announce Type: replace \nAbstract: Differential privacy (DP) offers a robust framework for safeguarding individual data privacy. To utilize DP in training modern machine learning models, differentially private optimizers have been widely used in recent years. A popular approach to privatize an optimizer is to clip the individual gradients and add sufficiently large noise to the clipped gradient. This approach led to the development of DP optimizers that have comparable performance with their non-private counterparts in fine-tuning tasks or in tasks with a small number of training parameters. However, a significant performance drop is observed when these optimizers are applied to large-scale training. This degradation stems from the substantial noise injection required to maintain DP, which disrupts the optimizer's dynamics. This paper introduces DiSK, a novel framework designed to significantly enhance the performance of DP optimizers. DiSK employs Kalman filtering, a technique drawn from control and signal processing, to effectively denoise privatized gradients and generate progressively refined gradient estimations. To ensure practicality for large-scale training, we simplify the Kalman filtering process, minimizing its memory and computational demands. We establish theoretical privacy-utility trade-off guarantees for DiSK, and demonstrate provable improvements over standard DP optimizers like DPSGD in terms of iteration complexity upper-bound. Extensive experiments across diverse tasks, including vision tasks such as CIFAR-100 and ImageNet-1k and language fine-tuning tasks such as GLUE, E2E, and DART, validate the effectiveness of DiSK. The results showcase its ability to significantly improve the performance of DP optimizers, surpassing state-of-the-art results under the same privacy constraints on several benchmarks."
      },
      {
        "id": "oai:arXiv.org:2410.05452v3",
        "title": "WearableMil: An End-to-End Framework for Military Activity Recognition and Performance Monitoring",
        "link": "https://arxiv.org/abs/2410.05452",
        "author": "Barak Gahtan, Shany Funk, Einat Kodesh, Itay Ketko, Tsvi Kuflik, Alex M. Bronstein",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.05452v3 Announce Type: replace \nAbstract: Musculoskeletal injuries during military training significantly impact readiness, making prevention through activity monitoring crucial. While Human Activity Recognition (HAR) using wearable devices offers promising solutions, it faces challenges in processing continuous data streams and recognizing diverse activities without predefined sessions. This paper introduces an end-to-end framework for preprocessing, analyzing, and recognizing activities from wearable data in military training contexts. Using data from 135 soldiers wearing \\textit{Garmin--55} smartwatches over six months with over 15 million minutes. We develop a hierarchical deep learning approach that achieves 93.8% accuracy in temporal splits and 83.8% in cross-user evaluation. Our framework addresses missing data through physiologically-informed methods, reducing unknown sleep states from 40.38% to 3.66%. We demonstrate that while longer time windows (45-60 minutes) improve basic state classification, they present trade-offs in detecting fine-grained activities. Additionally, we introduce an intuitive visualization system that enables real-time comparison of individual performance against group metrics across multiple physiological indicators. This approach to activity recognition and performance monitoring provides military trainers with actionable insights for optimizing training programs and preventing injuries."
      },
      {
        "id": "oai:arXiv.org:2410.07103v2",
        "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context",
        "link": "https://arxiv.org/abs/2410.07103",
        "author": "Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.07103v2 Announce Type: replace \nAbstract: Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order, relative position, in which the supporting documents are presented. We refer to this as the misordered context problem. To address this issue, based on the theoretical approach, we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context. This ensures that certain contiguous reasoning segments within supporting documents are presented in the optimal order, effectively guiding the model's reasoning in the appropriate direction. Applying CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known \"lost-in-the-middle\" problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning."
      },
      {
        "id": "oai:arXiv.org:2410.10572v3",
        "title": "Regularized Robustly Reliable Learners and Instance Targeted Attacks",
        "link": "https://arxiv.org/abs/2410.10572",
        "author": "Avrim Blum, Donya Saless",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10572v3 Announce Type: replace \nAbstract: Instance-targeted data poisoning attacks, where an adversary corrupts a training set to induce errors on specific test points, have raised significant concerns. Balcan et al (2022) proposed an approach to addressing this challenge by defining a notion of robustly-reliable learners that provide per-instance guarantees of correctness under well-defined assumptions, even in the presence of data poisoning attacks. They then give a generic optimal (but computationally inefficient) robustly reliable learner as well as a computationally efficient algorithm for the case of linear separators over log-concave distributions.\n  In this work, we address two challenges left open by Balcan et al (2022). The first is that the definition of robustly-reliable learners in Balcan et al (2022) becomes vacuous for highly-flexible hypothesis classes: if there are two classifiers h_0, h_1 \\in H both with zero error on the training set such that h_0(x) \\neq h_1(x), then a robustly-reliable learner must abstain on x. We address this problem by defining a modified notion of regularized robustly-reliable learners that allows for nontrivial statements in this case. The second is that the generic algorithm of Balcan et al (2022) requires re-running an ERM oracle (essentially, retraining the classifier) on each test point x, which is generally impractical even if ERM can be implemented efficiently. To tackle this problem, we show that at least in certain interesting cases we can design algorithms that can produce their outputs in time sublinear in training time, by using techniques from dynamic algorithm design."
      },
      {
        "id": "oai:arXiv.org:2410.11259v2",
        "title": "Rethinking the Role of Infrastructure in Collaborative Perception",
        "link": "https://arxiv.org/abs/2410.11259",
        "author": "Hyunchul Bae, Minhee Kang, Minwoo Song, Heejin Ahn",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11259v2 Announce Type: replace \nAbstract: Collaborative Perception (CP) is a process in which an ego agent receives and fuses sensor information from surrounding vehicles and infrastructure to enhance its perception capability. To evaluate the need for infrastructure equipped with sensors, extensive and quantitative analysis of the role of infrastructure data in CP is crucial, yet remains underexplored. To address this gap, we first quantitatively assess the importance of infrastructure data in existing vehicle-centric CP, where the ego agent is a vehicle. Furthermore, we compare vehicle-centric CP with infra-centric CP, where the ego agent is now the infrastructure, to evaluate the effectiveness of each approach. Our results demonstrate that incorporating infrastructure data improves 3D detection accuracy by up to 10.30%, and infra-centric CP shows enhanced noise robustness and increases accuracy by up to 46.47% compared with vehicle-centric CP."
      },
      {
        "id": "oai:arXiv.org:2410.11838v3",
        "title": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion",
        "link": "https://arxiv.org/abs/2410.11838",
        "author": "Junhwa Hur, Charles Herrmann, Saurabh Saxena, Janne Kontkanen, Wei-Sheng Lai, Yichang Shih, Michael Rubinstein, David J. Fleet, Deqing Sun",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11838v3 Announce Type: replace \nAbstract: Despite the recent progress, existing frame interpolation methods still struggle with processing extremely high resolution input and handling challenging cases such as repetitive textures, thin objects, and large motion. To address these issues, we introduce a patch-based cascaded pixel diffusion model for high resolution frame interpolation, HiFI, that excels in these scenarios while achieving competitive performance on standard benchmarks. Cascades, which generate a series of images from low to high resolution, can help significantly with large or complex motion that require both global context for a coarse solution and detailed context for high resolution output. However, contrary to prior work on cascaded diffusion models which perform diffusion on increasingly large resolutions, we use a single model that always performs diffusion at the same resolution and upsamples by processing patches of the inputs and the prior solution. At inference time, this drastically reduces memory usage and allows a single model, solving both frame interpolation (base model's task) and spatial up-sampling, saving training cost as well. HiFI excels at high-resolution images and complex repeated textures that require global context, achieving comparable or state-of-the-art performance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We further introduce a new dataset, LaMoR, that focuses on particularly challenging cases, and HiFI significantly outperforms other baselines. Please visit our project page for video results: https://hifi-diffusion.github.io"
      },
      {
        "id": "oai:arXiv.org:2410.13675v2",
        "title": "Pose-Based Sign Language Appearance Transfer",
        "link": "https://arxiv.org/abs/2410.13675",
        "author": "Amit Moryossef, Gerard Sant, Zifan Jiang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.13675v2 Announce Type: replace \nAbstract: We introduce a method for transferring the signer's appearance in sign language skeletal poses while preserving the sign content. Using estimated poses, we transfer the appearance of one signer to another, maintaining natural movements and transitions. This approach improves pose-based rendering and sign stitching while obfuscating identity. Our experiments show that while the method reduces signer identification accuracy, it slightly harms sign recognition performance, highlighting a tradeoff between privacy and utility. Our code is available at https://github.com/sign-language-processing/pose-anonymization."
      },
      {
        "id": "oai:arXiv.org:2410.20084v4",
        "title": "UniVST: A Unified Framework for Training-free Localized Video Style Transfer",
        "link": "https://arxiv.org/abs/2410.20084",
        "author": "Quanjian Song, Mingbao Lin, Wengyi Zhan, Shuicheng Yan, Liujuan Cao, Rongrong Ji",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.20084v4 Announce Type: replace \nAbstract: This paper presents UniVST, a unified framework for localized video style transfer based on diffusion models. It operates without the need for training, offering a distinct advantage over existing diffusion methods that transfer style across entire videos. The endeavors of this paper comprise: (1) A point-matching mask propagation strategy that leverages the feature maps from the DDIM inversion. This streamlines the model's architecture by obviating the need for tracking models. (2) A training-free AdaIN-guided localized video stylization mechanism that operates at both the latent and attention levels. This balances content fidelity and style richness, mitigating the loss of localized details commonly associated with direct video stylization. (3) A sliding-window consistent smoothing scheme that harnesses optical flow within the pixel representation and refines predicted noise to update the latent space. This significantly enhances temporal consistency and diminishes artifacts in stylized video. Our proposed UniVST has been validated to be superior to existing methods in quantitative and qualitative metrics. It adeptly addresses the challenges of preserving the primary object's style while ensuring temporal consistency and detail preservation. Our code is available at https://github.com/QuanjianSong/UniVST."
      },
      {
        "id": "oai:arXiv.org:2410.21491v3",
        "title": "Trustworthiness of Stochastic Gradient Descent in Distributed Learning",
        "link": "https://arxiv.org/abs/2410.21491",
        "author": "Hongyang Li, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.21491v3 Announce Type: replace \nAbstract: Distributed learning (DL) uses multiple nodes to accelerate training, enabling efficient optimization of large-scale models. Stochastic Gradient Descent (SGD), a key optimization algorithm, plays a central role in this process. However, communication bottlenecks often limit scalability and efficiency, leading to increasing adoption of compressed SGD techniques to alleviate these challenges. Despite addressing communication overheads, compressed SGD introduces trustworthiness concerns, as gradient exchanges among nodes are vulnerable to attacks like gradient inversion (GradInv) and membership inference attacks (MIA). The trustworthiness of compressed SGD remains unexplored, leaving important questions about its reliability unanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus uncompressed SGD. Specifically, we conducted empirical studies using GradInv attacks, revealing that compressed SGD demonstrates significantly higher resistance to privacy leakage compared to uncompressed SGD. In addition, our findings suggest that MIA may not be a reliable metric for assessing privacy risks in distributed learning."
      },
      {
        "id": "oai:arXiv.org:2410.23463v3",
        "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following",
        "link": "https://arxiv.org/abs/2410.23463",
        "author": "Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, Arman Cohan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.23463v3 Announce Type: replace \nAbstract: Multi-document (MD) processing is crucial for LLMs to handle real-world tasks such as summarization and question-answering across large sets of documents. While LLMs have improved at processing long inputs, MD contexts still present unique difficulties, including management of inter-document dependencies, redundancy, and incoherent structures. To address this challenge, we introduce MDCure, a scalable and effective instruction data generation framework to enhance the MD capabilities of LLMs without the computational cost of pre-training or reliance on human-annotated data. MDCure generates high-quality synthetic MD instruction data over sets of articles via targeted prompts. We also introduce MDCureRM, a cost-effective, MD-specific reward model to score and filter generated data based on their training utility for MD settings. MDCure is compatible with open- and closed-source models in addition to policy optimization methods such as PPO, enabling even small open-source models to surpass proprietary LLMs as strong generators of high-quality MD instruction data without further data filtering. With MDCure, we fine-tune a wide variety of LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model families. Extensive evaluations on a wide range of MD and long-context benchmarks spanning various tasks and domains show MDCure consistently improves performance over pre-trained baselines and base models by up to 75.1%. Our code, datasets, and models are available at https://github.com/yale-nlp/MDCure."
      },
      {
        "id": "oai:arXiv.org:2410.24175v2",
        "title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models",
        "link": "https://arxiv.org/abs/2410.24175",
        "author": "Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.24175v2 Announce Type: replace \nAbstract: Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research."
      },
      {
        "id": "oai:arXiv.org:2411.01263v2",
        "title": "Confidence Aware Learning for Reliable Face Anti-spoofing",
        "link": "https://arxiv.org/abs/2411.01263",
        "author": "Xingming Long, Jie Zhang, Shiguang Shan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.01263v2 Announce Type: replace \nAbstract: Current Face Anti-spoofing (FAS) models tend to make overly confident predictions even when encountering unfamiliar scenarios or unknown presentation attacks, which leads to serious potential risks. To solve this problem, we propose a Confidence Aware Face Anti-spoofing (CA-FAS) model, which is aware of its capability boundary, thus achieving reliable liveness detection within this boundary. To enable the CA-FAS to \"know what it doesn't know\", we propose to estimate its confidence during the prediction of each sample. Specifically, we build Gaussian distributions for both the live faces and the known attacks. The prediction confidence for each sample is subsequently assessed using the Mahalanobis distance between the sample and the Gaussians for the \"known data\". We further introduce the Mahalanobis distance-based triplet mining to optimize the parameters of both the model and the constructed Gaussians as a whole. Extensive experiments show that the proposed CA-FAS can effectively recognize samples with low prediction confidence and thus achieve much more reliable performance than other FAS models by filtering out samples that are beyond its reliable range."
      },
      {
        "id": "oai:arXiv.org:2411.01411v3",
        "title": "Mapping Global Floods with 10 Years of Satellite Radar Data",
        "link": "https://arxiv.org/abs/2411.01411",
        "author": "Amit Misra, Kevin White, Simone Fobi Nsutezo, William Straka, Juan Lavista",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.01411v3 Announce Type: replace \nAbstract: Floods cause extensive global damage annually, making effective monitoring essential. While satellite observations have proven invaluable for flood detection and tracking, comprehensive global flood datasets spanning extended time periods remain scarce. In this study, we introduce a novel deep learning flood detection model that leverages the cloud-penetrating capabilities of Sentinel-1 Synthetic Aperture Radar (SAR) satellite imagery, enabling consistent flood extent mapping in through cloud cover and in both day and night conditions. By applying this model to 10 years of SAR data, we create a unique, longitudinal global flood extent dataset with predictions unaffected by cloud coverage, offering comprehensive and consistent insights into historically flood-prone areas over the past decade. We use our model predictions to identify historically flood-prone areas in Ethiopia and demonstrate real-time disaster response capabilities during the May 2024 floods in Kenya. Additionally, our longitudinal analysis reveals potential increasing trends in global flood extent over time, although further validation is required to explore links to climate change. To maximize impact, we provide public access to both our model predictions and a code repository, empowering researchers and practitioners worldwide to advance flood monitoring and enhance disaster response strategies."
      },
      {
        "id": "oai:arXiv.org:2411.07127v2",
        "title": "Benchmarking LLMs' Judgments with No Gold Standard",
        "link": "https://arxiv.org/abs/2411.07127",
        "author": "Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.07127v2 Announce Type: replace \nAbstract: We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset."
      },
      {
        "id": "oai:arXiv.org:2411.07729v3",
        "title": "Exploring the loss landscape of regularized neural networks via convex duality",
        "link": "https://arxiv.org/abs/2411.07729",
        "author": "Sungyoon Kim, Aaron Mishkin, Mert Pilanci",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.07729v3 Announce Type: replace \nAbstract: We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with nonincreasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent convex problem and considering its dual. Starting from two-layer neural networks with scalar output, we first characterize the solution set of the convex problem using its dual and further characterize all stationary points. With the characterization, we show that the topology of the global optima goes through a phase transition as the width of the network changes, and construct counterexamples where the problem may have a continuum of optimal solutions. Finally, we show that the solution set characterization and connectivity results can be extended to different architectures, including two-layer vector-valued neural networks and parallel three-layer neural networks."
      },
      {
        "id": "oai:arXiv.org:2411.08243v2",
        "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset",
        "link": "https://arxiv.org/abs/2411.08243",
        "author": "Khaoula Chehbouni, Jonathan Cola\\c{c}o Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08243v2 Announce Type: replace \nAbstract: In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset's content through both manual and automated evaluation; (2) experiments demonstrating the dataset's impact on models' safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs."
      },
      {
        "id": "oai:arXiv.org:2411.09413v2",
        "title": "Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement",
        "link": "https://arxiv.org/abs/2411.09413",
        "author": "Wenxing Liu, Yueran Pan, Dong Zhang, Hongzhu Deng, Xiaobing Zou, Ming Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.09413v2 Announce Type: replace \nAbstract: The early diagnosis of autism spectrum disorder (ASD) is critically dependent on systematic observation and analysis of children's social behaviors. While current methodologies predominantly utilize supervised learning approaches, their clinical adoption faces two principal limitations: insufficient ASD diagnostic samples and inadequate interpretability of the detection outcomes. This paper presents a novel zero-shot ASD detection framework based on script-centric behavioral understanding with emotional enhancement, which is designed to overcome the aforementioned clinical constraints. The proposed pipeline automatically converts audio-visual data into structured behavioral text scripts through computer vision techniques, subsequently capitalizing on the generalization capabilities of large language models (LLMs) for zero-shot/few-shot ASD detection. Three core technical contributions are introduced: (1) A multimodal script transcription module transforming behavioral cues into structured textual representations. (2) An emotion textualization module encoding emotional dynamics as the contextual features to augment behavioral understanding. (3) A domain-specific prompt engineering strategy enables the injection of clinical knowledge into LLMs. Our method achieves an F1-score of 95.24\\% in diagnosing ASD in children with an average age of two years while generating interpretable detection rationales. This work opens up new avenues for leveraging the power of LLMs in analyzing and understanding ASD-related human behavior, thereby enhancing the accuracy of assisted autism diagnosis."
      },
      {
        "id": "oai:arXiv.org:2411.09694v2",
        "title": "A Bayesian Optimization Approach to Machine Translation Reranking",
        "link": "https://arxiv.org/abs/2411.09694",
        "author": "Julius Cheng, Maike Z\\\"ufle, Vil\\'em Zouhar, Andreas Vlachos",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.09694v2 Announce Type: replace \nAbstract: Reranking a list of candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate remains a simple and effective method for improving the overall output quality. Translation scoring models continue to grow in size, with the best models being comparable to generation models. Thus, reranking can add substantial computational cost to the translation pipeline. In this work, we pose reranking as a Bayesian optimization (BayesOpt) problem. By strategically selecting candidates to score based on a balance of exploration and exploitation, we show that it is possible to find top-scoring candidates when scoring only a fraction of the candidate list. For instance, our method achieves the same CometKiwi score using only 70 scoring evaluations compared a baseline system using 180. We present a multi-fidelity setting for BayesOpt, where the candidates are first scored with a cheaper but noisier proxy scoring model, which further improves the cost-performance tradeoff when using smaller but well-trained distilled proxy scorers."
      },
      {
        "id": "oai:arXiv.org:2411.10013v2",
        "title": "Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses",
        "link": "https://arxiv.org/abs/2411.10013",
        "author": "Yongfan Liu, Hyoukjun Kwon",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.10013v2 Announce Type: replace \nAbstract: Stereo depth estimation is a fundamental component in augmented reality (AR), which requires low latency for real-time processing. However, preprocessing such as rectification and non-ML computations such as cost volume require significant amount of latency exceeding that of an ML model itself, which hinders the real-time processing required by AR. Therefore, we develop alternative approaches to the rectification and cost volume that consider ML acceleration (GPU and NPUs) in recent hardware. For pre-processing, we eliminate it by introducing homography matrix prediction network with a rectification positional encoding (RPE), which delivers both low latency and robustness to unrectified images. For cost volume, we replace it with a group-pointwise convolution-based operator and approximation of cosine similarity based on layernorm and dot product. Based on our approaches, we develop MultiHeadDepth (replacing cost volume) and HomoDepth (MultiHeadDepth + removing pre-processing) models. MultiHeadDepth provides 11.8-30.3% improvements in accuracy and 22.9-25.2% reduction in latency compared to a state-of-the-art depth estimation model for AR glasses from industry. HomoDepth, which can directly process unrectified images, reduces the end-to-end latency by 44.5%. We also introduce a multi-task learning method to handle misaligned stereo inputs on HomoDepth, which reduces the AbsRel error by 10.0-24.3%. The overall results demonstrate the efficacy of our approaches, which not only reduce the inference latency but also improve the model performance. Our code is available at https://github.com/UCI-ISA-Lab/MultiHeadDepth-HomoDepth"
      },
      {
        "id": "oai:arXiv.org:2411.12636v2",
        "title": "PyAWD: A Library for Generating Large Synthetic Datasets of Acoustic Wave Propagation",
        "link": "https://arxiv.org/abs/2411.12636",
        "author": "Pascal Tribel, Gianluca Bontempi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.12636v2 Announce Type: replace \nAbstract: Seismic data is often sparse and unevenly distributed due to the high costs and logistical challenges associated with deploying physical seismometers, limiting the application of Machine Learning (ML) in earthquake analysis. While simulation methods exist, no tool allows the generation of large datasets containing simulated measurements of the ground motion. To address this gap, we introduce PyAWD, a Python library designed to generate high-resolution synthetic datasets simulating spatio-temporal acoustic wave propagation in both two-dimensional and three-dimensional heterogeneous media. By allowing fine control over parameters such as the wave speed, external forces, spatial and temporal discretization, and media composition, PyAWD enables the creation of ML-scale datasets that capture the complexity of seismic wave behavior. We illustrate the library's potential with an epicenter retrieval task, showcasing its suitability for designing complex, accurate seismic problems that require advanced ML approaches in the absence or lack of dense real-world data. We also show the usefulness of our tool to tackle the problem of data budgeting in the framework of epicenter retrieval."
      },
      {
        "id": "oai:arXiv.org:2411.14280v4",
        "title": "EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild",
        "link": "https://arxiv.org/abs/2411.14280",
        "author": "Yumeng Liu, Xiaoxiao Long, Zemin Yang, Yuan Liu, Marc Habermann, Christian Theobalt, Yuexin Ma, Wenping Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.14280v4 Announce Type: replace \nAbstract: Our work aims to reconstruct hand-object interactions from a single-view image, which is a fundamental but ill-posed task. Unlike methods that reconstruct from videos, multi-view images, or predefined 3D templates, single-view reconstruction faces significant challenges due to inherent ambiguities and occlusions. These challenges are further amplified by the diverse nature of hand poses and the vast variety of object shapes and sizes. Our key insight is that current foundational models for segmentation, inpainting, and 3D reconstruction robustly generalize to in-the-wild images, which could provide strong visual and geometric priors for reconstructing hand-object interactions. Specifically, given a single image, we first design a novel pipeline to estimate the underlying hand pose and object shape using off-the-shelf large models. Furthermore, with the initial reconstruction, we employ a prior-guided optimization scheme, which optimizes hand pose to comply with 3D physical constraints and the 2D input image content. We perform experiments across several datasets and show that our method consistently outperforms baselines and faithfully reconstructs a diverse set of hand-object interactions. Here is the link of our project page: https://lym29.github.io/EasyHOI-page/"
      },
      {
        "id": "oai:arXiv.org:2411.14823v2",
        "title": "Omni-IML: Towards Unified Image Manipulation Localization",
        "link": "https://arxiv.org/abs/2411.14823",
        "author": "Chenfan Qu, Yiwu Zhong, Fengjun Guo, Lianwen Jin",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.14823v2 Announce Type: replace \nAbstract: Existing Image Manipulation Localization (IML) methods mostly rely heavily on task-specific designs, making them perform well only on the target IML task, while joint training on multiple IML tasks causes significant performance degradation, hindering real applications.\n  To this end, we propose Omni-IML, the first generalist model designed to unify IML across diverse tasks.\n  Specifically, Omni-IML achieves generalization through three key components: (1) a Modal Gate Encoder, which adaptively selects the optimal encoding modality per sample, (2) a Dynamic Weight Decoder, which dynamically adjusts decoder filters to the task at hand, and (3) an Anomaly Enhancement module that leverages box supervision to highlight the tampered regions and facilitate the learning of task-agnostic features.\n  Beyond localization, to support interpretation of the tampered images, we construct Omni-273k, a large high-quality dataset that includes natural language descriptions of tampered artifact. It is annotated through our automatic, chain-of-thoughts annotation technique.\n  We also design a simple-yet-effective interpretation module to better utilize these descriptive annotations.\n  Our extensive experiments show that our single Omni-IML model achieves state-of-the-art performance across all four major IML tasks, providing a valuable solution for practical deployment and a promising direction of generalist models in image forensics. Our code and dataset will be publicly available."
      },
      {
        "id": "oai:arXiv.org:2412.02467v2",
        "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators",
        "link": "https://arxiv.org/abs/2412.02467",
        "author": "Tejumade Afonja, Hui-Po Wang, Raouf Kerkouche, Mario Fritz",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.02467v2 Announce Type: replace \nAbstract: Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision signals. Recently, pre-trained Large Language Models (LLMs) -- even those at the scale of GPT-2 -- have demonstrated great potential in synthesizing tabular data. However, their applications under DP constraints remain largely unexplored. In this work, we address this gap by applying DP techniques to the generation of synthetic tabular data. Our findings shows that LLMs face difficulties in generating coherent text when fine-tuned with DP, as privacy budgets are inefficiently allocated to non-private elements like table structures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning framework for differentially private tabular data generation. The first stage involves non-private fine-tuning on a pseudo dataset, followed by DP fine-tuning on a private dataset. Our empirical results show that this approach improves performance across various settings and metrics compared to directly fine-tuned LLMs in DP contexts. We release our code and setup at https://github.com/tejuafonja/DP-2Stage."
      },
      {
        "id": "oai:arXiv.org:2412.04189v4",
        "title": "HANDI: Hand-Centric Text-and-Image Conditioned Video Generation",
        "link": "https://arxiv.org/abs/2412.04189",
        "author": "Yayuan Li, Zhi Cao, Jason J. Corso",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.04189v4 Announce Type: replace \nAbstract: Despite the recent strides in video generation, state-of-the-art methods still struggle with elements of visual detail. One particularly challenging case is the class of videos in which the intricate motion of the hand coupled with a mostly stable and otherwise distracting environment is necessary to convey the execution of some complex action and its effects. To address these challenges, we introduce a new method for video generation that focuses on hand-centric actions. Our diffusion-based method incorporates two distinct innovations. First, we propose an automatic method to generate the motion area -- the region in the video in which the detailed activities occur -- guided by both the visual context and the action text prompt, rather than assuming this region can be provided manually as is now commonplace. Second, we introduce a critical Hand Refinement Loss to guide the diffusion model to focus on smooth and consistent hand poses. We evaluate our method on challenging augmented datasets based on EpicKitchens and Ego4D, demonstrating significant improvements over state-of-the-art methods in terms of action clarity, especially of the hand motion in the target region, across diverse environments and actions. Video results can be found in https://zhicaoisexcited.github.io/project_page"
      },
      {
        "id": "oai:arXiv.org:2412.07062v3",
        "title": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise Learning",
        "link": "https://arxiv.org/abs/2412.07062",
        "author": "Weihang Chen, Cheng Yang, Jie Ren, Zhiqiang Li, Zheng Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07062v3 Announce Type: replace \nAbstract: Real-life deployment of federated Learning (FL) often faces non-IID data, which leads to poor accuracy and slow convergence. Personalized FL (pFL) tackles these issues by tailoring local models to individual data sources and using weighted aggregation methods for client-specific learning. However, existing pFL methods often fail to provide each local model with global knowledge on demand while maintaining low computational overhead. Additionally, local models tend to over-personalize their data during the training process, potentially dropping previously acquired global information. We propose FLAYER, a novel layer-wise learning method for pFL that optimizes local model personalization performance. FLAYER considers the different roles and learning abilities of neural network layers of individual local models. It incorporates global information for each local model as needed to initialize the local model cost-effectively. It then dynamically adjusts learning rates for each layer during local training, optimizing the personalized learning process for each local model while preserving global knowledge. Additionally, to enhance global representation in pFL, FLAYER selectively uploads parameters for global aggregation in a layer-wise manner. We evaluate FLAYER on four representative datasets in computer vision and natural language processing domains. Compared to six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on average, by 5.40\\% (up to 14.29\\%)."
      },
      {
        "id": "oai:arXiv.org:2412.07775v4",
        "title": "Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets",
        "link": "https://arxiv.org/abs/2412.07775",
        "author": "Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07775v4 Announce Type: replace \nAbstract: While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and/or slow convergence in finetuning. In response to this challenge, we take inspiration from recent successes in generative flow networks (GFlowNets) and propose a reinforcement learning method for diffusion model finetuning, dubbed Nabla-GFlowNet (abbreviated as $\\nabla$-GFlowNet), that leverages the rich signal in reward gradients for probabilistic diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions."
      },
      {
        "id": "oai:arXiv.org:2412.09009v3",
        "title": "A physics-informed transformer neural operator for learning generalized solutions of initial boundary value problems",
        "link": "https://arxiv.org/abs/2412.09009",
        "author": "Sumanth Kumar Boya, Deepak Subramani",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.09009v3 Announce Type: replace \nAbstract: Initial boundary value problems arise commonly in applications with engineering and natural systems governed by nonlinear partial differential equations (PDEs). Operator learning is an emerging field for solving these equations by using a neural network to learn a map between infinite dimensional input and output function spaces. These neural operators are trained using a combination of data (observations or simulations) and PDE-residuals (physics-loss). A major drawback of existing neural approaches is the requirement to retrain with new initial/boundary conditions, and the necessity for a large amount of simulation data for training. We develop a physics-informed transformer neural operator (named PINTO) that efficiently generalizes to unseen initial and boundary conditions, trained in a simulation-free setting using only physics loss. The main innovation lies in our new iterative kernel integral operator units, implemented using cross-attention, to transform the PDE solution's domain points into an initial/boundary condition-aware representation vector, enabling efficient learning of the solution function for new scenarios. The PINTO architecture is applied to simulate the solutions of important equations used in engineering applications: advection, Burgers, and steady and unsteady Navier-Stokes equations (three flow scenarios). For these five test cases, we show that the relative errors during testing under challenging conditions of unseen initial/boundary conditions are only one-fifth to one-third of other leading physics informed operator learning methods. Moreover, our PINTO model is able to accurately solve the advection and Burgers equations at time steps that are not included in the training collocation points. The code is available at https://github.com/quest-lab-iisc/PINTO"
      },
      {
        "id": "oai:arXiv.org:2412.18565v2",
        "title": "3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement",
        "link": "https://arxiv.org/abs/2412.18565",
        "author": "Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.18565v2 Announce Type: replace \nAbstract: Despite advances in neural rendering, due to the scarcity of high-quality 3D datasets and the inherent limitations of multi-view diffusion models, view synthesis and 3D model generation are restricted to low resolutions with suboptimal multi-view consistency. In this study, we present a novel 3D enhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent diffusion model to enhance coarse 3D inputs while preserving multi-view consistency. Our method includes a pose-aware encoder and a diffusion-based denoiser to refine low-quality multi-view images, along with data augmentation and a multi-view attention module with epipolar aggregation to maintain consistent, high-quality 3D outputs across views. Unlike existing video-based approaches, our model supports seamless multi-view enhancement with improved coherence across diverse viewing angles. Extensive evaluations show that 3DEnhancer significantly outperforms existing methods, boosting both multi-view enhancement and per-instance 3D optimization tasks."
      },
      {
        "id": "oai:arXiv.org:2501.01950v4",
        "title": "MADGEN: Mass-Spec attends to De Novo Molecular generation",
        "link": "https://arxiv.org/abs/2501.01950",
        "author": "Yinkai Wang, Xiaohui Chen, Liping Liu, Soha Hassoun",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.01950v4 Announce Type: replace \nAbstract: The annotation (assigning structural chemical identities) of MS/MS spectra remains a significant challenge due to the enormous molecular diversity in biological samples and the limited scope of reference databases. Currently, the vast majority of spectral measurements remain in the \"dark chemical space\" without structural annotations. To improve annotation, we propose MADGEN (Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method for de novo molecular structure generation guided by mass spectrometry data. MADGEN operates in two stages: scaffold retrieval and spectra-conditioned molecular generation starting with the scaffold. In the first stage, given an MS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ contrastive learning to align mass spectra with candidate molecular scaffolds. In the second stage, starting from the retrieved scaffold, we employ the MS/MS spectrum to guide an attention-based generative model to generate the final molecule. Our approach constrains the molecular generation search space, reducing its complexity and improving generation accuracy. We evaluate MADGEN on three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's performance with a predictive scaffold retriever and with an oracle retriever. We demonstrate the effectiveness of using attention to integrate spectral information throughout the generation process to achieve strong results with the oracle retriever."
      },
      {
        "id": "oai:arXiv.org:2501.02330v2",
        "title": "SR-Reward: Taking The Path More Traveled",
        "link": "https://arxiv.org/abs/2501.02330",
        "author": "Seyed Mahdi B. Azad, Zahra Padar, Gabriel Kalweit, Joschka Boedecker",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.02330v2 Announce Type: replace \nAbstract: In this paper, we propose a novel method for learning reward functions directly from offline demonstrations. Unlike traditional inverse reinforcement learning (IRL), our approach decouples the reward function from the learner's policy, eliminating the adversarial interaction typically required between the two. This results in a more stable and efficient training process. Our reward function, called \\textit{SR-Reward}, leverages successor representation (SR) to encode a state based on expected future states' visitation under the demonstration policy and transition dynamics. By utilizing the Bellman equation, SR-Reward can be learned concurrently with most reinforcement learning (RL) algorithms without altering the existing training pipeline. We also introduce a negative sampling strategy to mitigate overestimation errors by reducing rewards for out-of-distribution data, thereby enhancing robustness. This strategy inherently introduces a conservative bias into RL algorithms that employ the learned reward. We evaluate our method on the D4RL benchmark, achieving competitive results compared to offline RL algorithms with access to true rewards and imitation learning (IL) techniques like behavioral cloning. Moreover, our ablation studies on data size and quality reveal the advantages and limitations of SR-Reward as a proxy for true rewards."
      },
      {
        "id": "oai:arXiv.org:2501.09298v2",
        "title": "Physics-informed deep learning for infectious disease forecasting",
        "link": "https://arxiv.org/abs/2501.09298",
        "author": "Ying Qian, Kui Zhang, \\'Eric Marty, Avranil Basu, Eamon B. O'Dea, Xianqiao Wang, Spencer Fox, Pejman Rohani, John M. Drake, He Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.09298v2 Announce Type: replace \nAbstract: Accurate forecasting of contagious diseases is critical for public health policymaking and pandemic preparedness. We propose a new infectious disease forecasting model based on physics-informed neural networks (PINNs), an emerging scientific machine learning approach. By embedding a compartmental model into the loss function, our method integrates epidemiological theory with data, helping to prevent model overfitting. We further enhance the model with a sub-network that accounts for covariates such as mobility and cumulative vaccine doses, which influence the transmission rate. Using state-level COVID-19 data from California, we demonstrate that the PINN model accurately predicts cases, deaths, and hospitalizations, aligning well with existing benchmarks. Notably, the PINN model outperforms naive baseline forecasts and several sequence deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRUs), and Transformers. It also achieves performance comparable to a sophisticated Gaussian infection state forecasting model that combines compartmental dynamics, a data observation model, and parameter regression. However, the PINN model features a simpler structure and is easier to implement. In summary, we systematically evaluate the PINN model's ability to forecast infectious disease dynamics, demonstrating its potential as an efficient computational tool to strengthen forecasting capabilities."
      },
      {
        "id": "oai:arXiv.org:2501.09552v3",
        "title": "Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images",
        "link": "https://arxiv.org/abs/2501.09552",
        "author": "Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.09552v3 Announce Type: replace \nAbstract: De-identification of medical images is a critical step to ensure privacy during data sharing in research and clinical settings. The initial step in this process involves detecting Protected Health Information (PHI), which can be found in image metadata or imprinted within image pixels. Despite the importance of such systems, there has been limited evaluation of existing AI-based solutions, creating barriers to the development of reliable and robust tools. In this study, we present an AI-based pipeline for PHI detection, comprising three key components: text detection, text extraction, and text analysis. We benchmark three models, YOLOv11, EasyOCR, and GPT-4o, across different setups corresponding to these components, evaluating the performance based on precision, recall, F1 score, and accuracy. All setups demonstrate excellent PHI detection, with all metrics exceeding 0.9. The combination of YOLOv11 for text localization and GPT-4o for extraction and analysis yields the best results. However, this setup incurs higher costs due to GPT-4o's token generation. Conversely, an end-to-end pipeline that relies solely on GPT-4o shows lower performance but highlights the potential of multimodal models for complex tasks. We recommend fine-tuning a dedicated object detection model and utilizing built-in OCR tools to achieve optimal performance and cost-effectiveness. Additionally, leveraging language models such as GPT-4o can facilitate thorough and flexible analysis of text content."
      },
      {
        "id": "oai:arXiv.org:2501.10221v2",
        "title": "Synthesising Activity Participations and Scheduling with Deep Generative Machine Learning",
        "link": "https://arxiv.org/abs/2501.10221",
        "author": "Fred Shone, Tim Hillel",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.10221v2 Announce Type: replace \nAbstract: Using a deep generative machine learning approach, we synthesise human activity participations and scheduling (the choices of what activities to participate in and when). Activity schedules, which represent what people do and when, are a core component of many applied transport, energy, and epidemiology models. Our data-driven approach learns the distributions resulting from human preferences and scheduling logic without the need for complex interacting combinations of sub-models and custom rules, This makes our approach significantly faster and simpler to operate than existing approaches to synthesise or anonymise schedule data. We additionally contribute a novel schedule representation and a comprehensive evaluation framework. We evaluate a range of schedule encoding and deep model architecture combinations. The evaluation shows our approach can rapidly generate large, diverse, novel, and realistic synthetic samples of activity schedules."
      },
      {
        "id": "oai:arXiv.org:2501.11153v3",
        "title": "Efficient Frame Extraction: A Novel Approach Through Frame Similarity and Surgical Tool Tracking for Video Segmentation",
        "link": "https://arxiv.org/abs/2501.11153",
        "author": "Huu Phong Nguyen, Shekhar Madhav Khairnar, Sofia Garces Palacios, Amr Al-Abbas, Melissa E. Hogg, Amer H. Zureikat, Patricio M. Polanco, Herbert Zeh III, Ganesh Sankaranarayanan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.11153v3 Announce Type: replace \nAbstract: The interest in leveraging Artificial Intelligence (AI) for surgical procedures to automate analysis has witnessed a significant surge in recent years. One of the primary tools for recording surgical procedures and conducting subsequent analyses, such as performance assessment, is through videos. However, these operative videos tend to be notably lengthy compared to other fields, spanning from thirty minutes to several hours, which poses a challenge for AI models to effectively learn from them. Despite this challenge, the foreseeable increase in the volume of such videos in the near future necessitates the development and implementation of innovative techniques to tackle this issue effectively. In this article, we propose a novel technique called Kinematics Adaptive Frame Recognition (KAFR) that can efficiently eliminate redundant frames to reduce dataset size and computation time while retaining useful frames to improve accuracy. Specifically, we compute the similarity between consecutive frames by tracking the movement of surgical tools. Our approach follows these steps: $i)$ Tracking phase: a YOLOv8 model is utilized to detect tools presented in the scene, $ii)$ Similarity phase: Similarities between consecutive frames are computed by estimating variation in the spatial positions and velocities of the tools, $iii$) Classification phase: An X3D CNN is trained to classify segmentation. We evaluate the effectiveness of our approach by analyzing datasets obtained through retrospective reviews of cases at two referral centers. The newly annotated Gastrojejunostomy (GJ) dataset covers procedures performed between 2017 and 2021, while the previously annotated Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the same centers."
      },
      {
        "id": "oai:arXiv.org:2501.12352v2",
        "title": "Test-time regression: a unifying framework for designing sequence models with associative memory",
        "link": "https://arxiv.org/abs/2501.12352",
        "author": "Ke Alexander Wang, Jiaxin Shi, Emily B. Fox",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.12352v2 Announce Type: replace \nAbstract: Sequence models lie at the heart of modern deep learning. However, rapid advancements have produced a diversity of seemingly unrelated architectures, such as Transformers and recurrent alternatives. In this paper, we introduce a unifying framework to understand and derive these sequence models, inspired by the empirical importance of associative recall, the capability to retrieve contextually relevant tokens. We formalize associative recall as a two-step process, memorization and retrieval, casting memorization as a regression problem. Layers that combine these two steps perform associative recall via ``test-time regression'' over its input tokens. Prominent layers, including linear attention, state-space models, fast-weight programmers, online learners, and softmax attention, arise as special cases defined by three design choices: the regression weights, the regressor function class, and the test-time optimization algorithm. Our approach clarifies how linear attention fails to capture inter-token correlations and offers a mathematical justification for the empirical effectiveness of query-key normalization in softmax attention. Further, it illuminates unexplored regions within the design space, which we use to derive novel higher-order generalizations of softmax attention. Beyond unification, our work bridges sequence modeling with classic regression methods, a field with extensive literature, paving the way for developing more powerful and theoretically principled architectures."
      },
      {
        "id": "oai:arXiv.org:2501.12433v2",
        "title": "Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models",
        "link": "https://arxiv.org/abs/2501.12433",
        "author": "Tabinda Aman, Mohammad Nadeem, Shahab Saquib Sohail, Mohammad Anas, Erik Cambria",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.12433v2 Announce Type: replace \nAbstract: Animal stereotypes are deeply embedded in human culture and language. They often shape our perceptions and expectations of various species. Our study investigates how animal stereotypes manifest in vision-language models during the task of image generation. Through targeted prompts, we explore whether DALL-E perpetuates stereotypical representations of animals, such as \"owls as wise,\" \"foxes as unfaithful,\" etc. Our findings reveal significant stereotyped instances where the model consistently generates images aligned with cultural biases. The current work is the first of its kind to examine animal stereotyping in vision-language models systematically and to highlight a critical yet underexplored dimension of bias in AI-generated visual content."
      },
      {
        "id": "oai:arXiv.org:2501.19184v3",
        "title": "A Survey on Class-Agnostic Counting: Advancements from Reference-Based to Open-World Text-Guided Approaches",
        "link": "https://arxiv.org/abs/2501.19184",
        "author": "Luca Ciampi, Ali Azmoudeh, Elif Ecem Akbaba, Erdi Sar{\\i}ta\\c{s}, Ziya Ata Yaz{\\i}c{\\i}, Haz{\\i}m Kemal Ekenel, Giuseppe Amato, Fabrizio Falchi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.19184v3 Announce Type: replace \nAbstract: Visual object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories -- a crucial capability for flexible and generalizable counting systems. Unlike humans, who effortlessly identify and count objects from diverse categories without prior knowledge, most existing counting methods are restricted to enumerating instances of known classes, requiring extensive labeled datasets for training and struggling in open-vocabulary settings. In contrast, CAC aims to count objects belonging to classes never seen during training, operating in a few-shot setting. In this paper, we present the first comprehensive review of CAC methodologies. We propose a taxonomy to categorize CAC approaches into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches achieve state-of-the-art performance by relying on exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods use vision-language models, enabling object class descriptions via textual prompts, offering a flexible and promising solution. Based on this taxonomy, we provide an overview of the architectures of 29 CAC approaches and report their results on gold-standard benchmarks. We compare their performance and discuss their strengths and limitations. Specifically, we present results on the FSC-147 dataset, setting a leaderboard using gold-standard metrics, and on the CARPK dataset to assess generalization capabilities. Finally, we offer a critical discussion of persistent challenges, such as annotation dependency and generalization, alongside future directions. We believe this survey will be a valuable resource, showcasing CAC advancements and guiding future research."
      },
      {
        "id": "oai:arXiv.org:2502.00607v2",
        "title": "PAC Learning is just Bipartite Matching (Sort of)",
        "link": "https://arxiv.org/abs/2502.00607",
        "author": "Shaddin Dughmi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00607v2 Announce Type: replace \nAbstract: The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning."
      },
      {
        "id": "oai:arXiv.org:2502.01070v3",
        "title": "An Inquiry into Datacenter TCO for LLM Inference with FP8",
        "link": "https://arxiv.org/abs/2502.01070",
        "author": "Jiwoo Kim, Joonhyung Lee, Gunho Park, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee, Youngjoo Lee",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01070v3 Announce Type: replace \nAbstract: As large language models (LLMs) continue to scale, their inference demands present significant challenges, particularly due to the high power consumption of AI accelerators in datacenters. These facilities require specialized cooling and power management systems, substantially increasing the total cost of ownership (TCO) for cloud service providers (CSPs). In this work, we analyze the computational characteristics and constraints of LLM inference from a TCO perspective, focusing on two representative accelerators: the Gaudi 2 and NVIDIA H100. We present a generalizable framework that enables CSPs to compare and select AI accelerators according to diverse operational requirements. Using this model, we analyze the impact of FP8 precision and LLM inference workload characteristics as key factors influencing TCO. We investigate FP8 quantization, which is gaining adoption in LLM training, as a technique to improve inference throughput while maintaining cost efficiency. Furthermore, our analysis of LLM inference workloads reveals that performance on thin GEMMs, which dominate the decode phase, can have a greater impact than theoretical hardware peak performance. By studying the interaction between power consumption, quantization strategies, and hardware architecture, we offer insights that support informed deployment decisions and guide future accelerator designs to improve the TCO of LLM inference."
      },
      {
        "id": "oai:arXiv.org:2502.01563v2",
        "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding",
        "link": "https://arxiv.org/abs/2502.01563",
        "author": "Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01563v2 Announce Type: replace \nAbstract: Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM."
      },
      {
        "id": "oai:arXiv.org:2502.03370v4",
        "title": "Deep Learning-Based Approach for Identification of Potato Leaf Diseases Using Wrapper Feature Selection and Feature Concatenation",
        "link": "https://arxiv.org/abs/2502.03370",
        "author": "Muhammad Ahtsam Naeem, Muhammad Asim Saleem, Muhammad Imran Sharif, Shahzad Akber, Sajjad Saleem, Zahid Akhtar, Kamran Siddique",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03370v4 Announce Type: replace \nAbstract: The potato is a widely grown crop in many regions of the world. In recent decades, potato farming has gained incredible traction in the world. Potatoes are susceptible to several illnesses that stunt their development. This plant seems to have significant leaf disease. Early Blight and Late Blight are two prevalent leaf diseases that affect potato plants. The early detection of these diseases would be beneficial for enhancing the yield of this crop. The ideal solution is to use image processing to identify and analyze these disorders. Here, we present an autonomous method based on image processing and machine learning to detect late blight disease affecting potato leaves. The proposed method comprises four different phases: (1) Histogram Equalization is used to improve the quality of the input image; (2) feature extraction is performed using a Deep CNN model, then these extracted features are concatenated; (3) feature selection is performed using wrapper-based feature selection; (4) classification is performed using an SVM classifier and its variants. This proposed method achieves the highest accuracy of 99% using SVM by selecting 550 features."
      },
      {
        "id": "oai:arXiv.org:2502.03629v2",
        "title": "REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations",
        "link": "https://arxiv.org/abs/2502.03629",
        "author": "Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, Ranjay Krishna",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03629v2 Announce Type: replace \nAbstract: Existing image editing models struggle to meet real-world demands. Despite excelling in academic benchmarks, they have yet to be widely adopted for real user needs. Datasets that power these models use artificial edits, lacking the scale and ecological validity necessary to address the true diversity of user requests. We introduce REALEDIT, a large-scale image editing dataset with authentic user requests and human-made edits sourced from Reddit. REALEDIT includes a test set of 9300 examples to evaluate models on real user requests. Our results show that existing models fall short on these tasks, highlighting the need for realistic training data. To address this, we introduce 48K training examples and train our REALEDIT model, achieving substantial gains - outperforming competitors by up to 165 Elo points in human judgment and 92 percent relative improvement on the automated VIEScore metric. We deploy our model on Reddit, testing it on new requests, and receive positive feedback. Beyond image editing, we explore REALEDIT's potential in detecting edited images by partnering with a deepfake detection non-profit. Finetuning their model on REALEDIT data improves its F1-score by 14 percentage points, underscoring the dataset's value for broad applications."
      },
      {
        "id": "oai:arXiv.org:2502.05857v2",
        "title": "EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds",
        "link": "https://arxiv.org/abs/2502.05857",
        "author": "Lu Chen, Yizhou Wang, Shixiang Tang, Qianhong Ma, Tong He, Wanli Ouyang, Xiaowei Zhou, Hujun Bao, Sida Peng",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05857v2 Announce Type: replace \nAbstract: This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, which prevents them from learning from each other. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions within a single transformer. EgoAgent introduces two innovations to learn from the causal and temporally intertwined nature of these abilities: (1) Interleaved sequential modeling of states and actions with the causal attention mechanism, and (2) A joint embedding-action-prediction architecture featuring temporal asymmetric predictor-observer branches. Integrating these designs based on JEPA, EgoAgent unifies these capabilities in a cohesive learning framework. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility."
      },
      {
        "id": "oai:arXiv.org:2502.06820v2",
        "title": "LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning",
        "link": "https://arxiv.org/abs/2502.06820",
        "author": "Zhekai Du, Yinjie Min, Jingjing Li, Ke Lu, Changliang Zou, Liuhua Peng, Tingjin Chu, Mingming Gong",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06820v2 Announce Type: replace \nAbstract: Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain decomposition with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods."
      },
      {
        "id": "oai:arXiv.org:2502.07319v2",
        "title": "Learnable Residual-based Latent Denoising in Semantic Communication",
        "link": "https://arxiv.org/abs/2502.07319",
        "author": "Mingkai Xu, Yongpeng Wu, Yuxuan Shi, Xiang-Gen Xia, Wenjun Zhang, Ping Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07319v2 Announce Type: replace \nAbstract: A latent denoising semantic communication (SemCom) framework is proposed for robust image transmission over noisy channels. By incorporating a learnable latent denoiser into the receiver, the received signals are preprocessed to effectively remove the channel noise and recover the semantic information, thereby enhancing the quality of the decoded images. Specifically, a latent denoising mapping is established by an iterative residual learning approach to improve the denoising efficiency while ensuring stable performance. Moreover, channel signal-to-noise ratio (SNR) is utilized to estimate and predict the latent similarity score (SS) for conditional denoising, where the number of denoising steps is adapted based on the predicted SS sequence, further reducing the communication latency. Finally, simulations demonstrate that the proposed framework can effectively and efficiently remove the channel noise at various levels and reconstruct visual-appealing images."
      },
      {
        "id": "oai:arXiv.org:2502.07608v3",
        "title": "Time2Lang: Bridging Time-Series Foundation Models and Large Language Models for Health Sensing Beyond Prompting",
        "link": "https://arxiv.org/abs/2502.07608",
        "author": "Arvind Pillai, Dimitris Spathis, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07608v3 Announce Type: replace \nAbstract: Large language models (LLMs) show promise for health applications when combined with behavioral sensing data. Traditional approaches convert sensor data into text prompts, but this process is prone to errors, computationally expensive, and requires domain expertise. These challenges are particularly acute when processing extended time series data. While time series foundation models (TFMs) have recently emerged as powerful tools for learning representations from temporal data, bridging TFMs and LLMs remains challenging. Here, we present Time2Lang, a framework that directly maps TFM outputs to LLM representations without intermediate text conversion. Our approach first trains on synthetic data using periodicity prediction as a pretext task, followed by evaluation on mental health classification tasks. We validate Time2Lang on two longitudinal wearable and mobile sensing datasets: daily depression prediction using step count data (17,251 days from 256 participants) and flourishing classification based on conversation duration (46 participants over 10 weeks). Time2Lang maintains near constant inference times regardless of input length, unlike traditional prompting methods. The generated embeddings preserve essential time-series characteristics such as auto-correlation. Our results demonstrate that TFMs and LLMs can be effectively integrated while minimizing information loss and enabling performance transfer across these distinct modeling paradigms. To our knowledge, we are the first to integrate a TFM and an LLM for health, thus establishing a foundation for future research combining general-purpose large models for complex healthcare tasks."
      },
      {
        "id": "oai:arXiv.org:2502.08365v2",
        "title": "Towards Principled Multi-Agent Task Agnostic Exploration",
        "link": "https://arxiv.org/abs/2502.08365",
        "author": "Riccardo Zamboni, Mirco Mutti, Marcello Restelli",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.08365v2 Announce Type: replace \nAbstract: In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings."
      },
      {
        "id": "oai:arXiv.org:2502.10712v2",
        "title": "FuncGenFoil: Airfoil Generation and Editing Model in Function Space",
        "link": "https://arxiv.org/abs/2502.10712",
        "author": "Jinouwen Zhang, Junjie Ren, Aobo Yang, Yan Lu, Lu Chen, Hairun Xie, Jing Wang, Miao Zhang, Wanli Ouyang, Shixiang Tang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.10712v2 Announce Type: replace \nAbstract: Aircraft manufacturing is the jewel in the crown of industry, among which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. While existing deep-learning-based methods rely on predefined parametric function families, e.g., B\\'ezier curves and discrete point-based representations, they suffer from inherent trade-offs between expressiveness and resolution flexibility. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly learns functional airfoil geometries. Our method inherits both the advantages of arbitrary resolution sampling and the smoothness of parametric functions, as well as the strong expressiveness of discrete point-based functions. Empirical evaluations on the AFBench dataset demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation by achieving a relative -74.4 label error reduction and +23.2 diversity increase on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design. Our code will be released."
      },
      {
        "id": "oai:arXiv.org:2502.12836v2",
        "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation",
        "link": "https://arxiv.org/abs/2502.12836",
        "author": "Mohammad Feli, Iman Azimi, Pasi Liljeberg, Amir M. Rahmani",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12836v2 Announce Type: replace \nAbstract: Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub."
      },
      {
        "id": "oai:arXiv.org:2502.17163v3",
        "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
        "link": "https://arxiv.org/abs/2502.17163",
        "author": "Mar\\'ia Andrea Cruz Bland\\'on, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17163v3 Announce Type: replace \nAbstract: Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG"
      },
      {
        "id": "oai:arXiv.org:2502.18773v2",
        "title": "Research on Edge Computing and Cloud Collaborative Resource Scheduling Optimization Based on Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2502.18773",
        "author": "Yuqing Wang, Xiao Yang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.18773v2 Announce Type: replace \nAbstract: This study addresses the challenge of resource scheduling optimization in edge-cloud collaborative computing using deep reinforcement learning (DRL). The proposed DRL-based approach improves task processing efficiency, reduces overall processing time, enhances resource utilization, and effectively controls task migrations. Experimental results demonstrate the superiority of DRL over traditional scheduling algorithms, particularly in managing complex task allocation, dynamic workloads, and multiple resource constraints. Despite its advantages, further improvements are needed to enhance learning efficiency, reduce training time, and address convergence issues. Future research should focus on increasing the algorithm's fault tolerance to handle more complex and uncertain scheduling scenarios, thereby advancing the intelligence and efficiency of edge-cloud computing systems."
      },
      {
        "id": "oai:arXiv.org:2503.00063v4",
        "title": "NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary",
        "link": "https://arxiv.org/abs/2503.00063",
        "author": "Zezeng Li, Xiaoyu Du, Na Lei, Liming Chen, Weimin Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00063v4 Announce Type: replace \nAbstract: Adversarial attacks exploit the vulnerability of deep models against adversarial samples. Existing point cloud attackers are tailored to specific models, iteratively optimizing perturbations based on gradients in either a white-box or black-box setting. Despite their promising attack performance, they often struggle to produce transferable adversarial samples due to overfitting the specific parameters of surrogate models. To overcome this issue, we shift our focus to the data distribution itself and introduce a novel approach named NoPain, which employs optimal transport (OT) to identify the inherent singular boundaries of the data manifold for cross-network point cloud attacks. Specifically, we first calculate the OT mapping from noise to the target feature space, then identify singular boundaries by locating non-differentiable positions. Finally, we sample along singular boundaries to generate adversarial point clouds. Once the singular boundaries are determined, NoPain can efficiently produce adversarial samples without the need of iterative updates or guidance from the surrogate classifiers. Extensive experiments demonstrate that the proposed end-to-end method outperforms baseline approaches in terms of both transferability and efficiency, while also maintaining notable advantages even against defense strategies. Code and model are available at https://github.com/cognaclee/nopain"
      },
      {
        "id": "oai:arXiv.org:2503.02689v3",
        "title": "STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks",
        "link": "https://arxiv.org/abs/2503.02689",
        "author": "Tianqing Zhang, Kairong Yu, Xian Zhong, Hongwei Wang, Qi Xu, Qiang Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02689v3 Announce Type: replace \nAbstract: Spiking Neural Networks (SNNs) have gained significant attention due to their biological plausibility and energy efficiency, making them promising alternatives to Artificial Neural Networks (ANNs). However, the performance gap between SNNs and ANNs remains a substantial challenge hindering the widespread adoption of SNNs. In this paper, we propose a Spatial-Temporal Attention Aggregator SNN (STAA-SNN) framework, which dynamically focuses on and captures both spatial and temporal dependencies. First, we introduce a spike-driven self-attention mechanism specifically designed for SNNs. Additionally, we pioneeringly incorporate position encoding to integrate latent temporal relationships into the incoming features. For spatial-temporal information aggregation, we employ step attention to selectively amplify relevant features at different steps. Finally, we implement a time-step random dropout strategy to avoid local optima. As a result, STAA-SNN effectively captures both spatial and temporal dependencies, enabling the model to analyze complex patterns and make accurate predictions. The framework demonstrates exceptional performance across diverse datasets and exhibits strong generalization capabilities. Notably, STAA-SNN achieves state-of-the-art results on neuromorphic datasets CIFAR10-DVS, with remarkable performances of 97.14%, 82.05% and 70.40% on the static datasets CIFAR-10, CIFAR-100 and ImageNet, respectively. Furthermore, our model exhibits improved performance ranging from 0.33\\% to 2.80\\% with fewer time steps. The code for the model is available on GitHub."
      },
      {
        "id": "oai:arXiv.org:2503.03144v2",
        "title": "Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks",
        "link": "https://arxiv.org/abs/2503.03144",
        "author": "Kairong Yu, Chengting Yu, Tianqing Zhang, Xiaochen Zhao, Shu Yang, Hongwei Wang, Qiang Zhang, Qi Xu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.03144v2 Announce Type: replace \nAbstract: Spiking Neural Networks (SNNs), inspired by the human brain, offer significant computational efficiency through discrete spike-based information transfer. Despite their potential to reduce inference energy consumption, a performance gap persists between SNNs and Artificial Neural Networks (ANNs), primarily due to current training methods and inherent model limitations. While recent research has aimed to enhance SNN learning by employing knowledge distillation (KD) from ANN teacher networks, traditional distillation techniques often overlook the distinctive spatiotemporal properties of SNNs, thus failing to fully leverage their advantages. To overcome these challenge, we propose a novel logit distillation method characterized by temporal separation and entropy regularization. This approach improves existing SNN distillation techniques by performing distillation learning on logits across different time steps, rather than merely on aggregated output features. Furthermore, the integration of entropy regularization stabilizes model optimization and further boosts the performance. Extensive experimental results indicate that our method surpasses prior SNN distillation strategies, whether based on logit distillation, feature distillation, or a combination of both. The code will be available on GitHub."
      },
      {
        "id": "oai:arXiv.org:2503.04088v2",
        "title": "Cloud Computing Energy Consumption Prediction Based on Kernel Extreme Learning Machine Algorithm Improved by Vector Weighted Average Algorithm",
        "link": "https://arxiv.org/abs/2503.04088",
        "author": "Yuqing Wang, Xiao Yang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04088v2 Announce Type: replace \nAbstract: With the rapid expansion of cloud computing infrastructure, energy consumption has become a critical challenge, driving the need for accurate and efficient prediction models. This study proposes a novel Vector Weighted Average Kernel Extreme Learning Machine (VWAA-KELM) model to enhance energy consumption prediction in cloud computing environments. By integrating a vector weighted average algorithm (VWAA) with kernel extreme learning machine (KELM), the proposed model dynamically adjusts feature weights and optimizes kernel functions, significantly improving prediction accuracy and generalization. Experimental results demonstrate the superior performance of VWAA-KELM: 94.7% of test set prediction errors fall within [0, 50] units, with only three cases exceeding 100 units, indicating strong stability. The model achieves a coefficient of determination (R2) of 0.987 in the training set (RMSE = 28.108, RPD = 8.872) and maintains excellent generalization with R2 = 0.973 in the test set (RMSE = 43.227, RPD = 6.202). Visual analysis confirms that predicted values closely align with actual energy consumption trends, avoiding overfitting while capturing nonlinear dependencies. A key innovation of this study is the introduction of adaptive feature weighting, allowing the model to dynamically assign importance to different input parameters, thereby enhancing high-dimensional data processing. This advancement provides a scalable and efficient approach for optimizing cloud data center energy consumption. Beyond cloud computing, the proposed hybrid framework has broader applications in Internet of Things (IoT) and edge computing, supporting real-time energy management and intelligent resource allocation."
      },
      {
        "id": "oai:arXiv.org:2503.04231v2",
        "title": "One-Shot Clustering for Federated Learning",
        "link": "https://arxiv.org/abs/2503.04231",
        "author": "Maciej Krzysztof Zuziak, Roberto Pellungrini, Salvatore Rinzivillo",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04231v2 Announce Type: replace \nAbstract: Federated Learning (FL) is a widespread and well adopted paradigm of decentralized learning that allows training one model from multiple sources without the need to directly transfer data between participating clients. Since its inception in 2015, it has been divided into numerous sub-fields that deal with application-specific issues, be it data heterogeneity or resource allocation. One such sub-field, Clustered Federated Learning (CFL), is dealing with the problem of clustering the population of clients into separate cohorts to deliver personalized models. Although few remarkable works have been published in this domain, the problem is still largely unexplored, as its basic assumption and settings are slightly different from standard FL. In this work, we present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic algorithm that can automatically detect the earliest suitable moment for clustering. Our algorithm is based on the computation of cosine similarity between gradients of the clients and a temperature measure that detects when the federated model starts to converge. We empirically evaluate our methodology by testing various one-shot clustering algorithms for over thirty different tasks on three benchmark datasets. Our experiments showcase the good performance of our approach when used to perform CFL in an automated manner without the need to adjust hyperparameters."
      },
      {
        "id": "oai:arXiv.org:2503.04606v3",
        "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
        "link": "https://arxiv.org/abs/2503.04606",
        "author": "Aoxiong Yin, Kai Shen, Yichong Leng, Xu Tan, Xinyu Zhou, Juncheng Li, Siliang Tang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04606v3 Announce Type: replace \nAbstract: Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/."
      },
      {
        "id": "oai:arXiv.org:2503.04992v2",
        "title": "Wanda++: Pruning Large Language Models via Regional Gradients",
        "link": "https://arxiv.org/abs/2503.04992",
        "author": "Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric Huybrechts, Markus M\\\"uller, Jonas M. K\\\"ubler, Rupak Vignesh Swaminathan, Athanasios Mouchtaris, Sravan Babu Bodapati, Nathan Susanj, Zheng Zhang, Jack FitzGerald, Abhishek Kumar",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04992v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal performance impact. However, existing methods often suffer from performance loss without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Further experiments indicate our proposed method is orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda method. The proposed method is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single NVIDIA H100 GPU."
      },
      {
        "id": "oai:arXiv.org:2503.06698v2",
        "title": "What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization",
        "link": "https://arxiv.org/abs/2503.06698",
        "author": "Xavier Thomas, Deepti Ghadiyaram",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06698v2 Announce Type: replace \nAbstract: Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training."
      },
      {
        "id": "oai:arXiv.org:2503.08727v2",
        "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
        "link": "https://arxiv.org/abs/2503.08727",
        "author": "Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan Vuli\\'c, Alessandro Sordoni",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08727v2 Announce Type: replace \nAbstract: Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG."
      },
      {
        "id": "oai:arXiv.org:2503.15358v2",
        "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation",
        "link": "https://arxiv.org/abs/2503.15358",
        "author": "Thomas Pickard, Aline Villavicencio, Maggie Mi, Wei He, Dylan Phelps, Marco Idiart",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15358v2 Announce Type: replace \nAbstract: Idiomatic expressions present a unique challenge in NLP, as their meanings are often not directly inferable from their constituent words. Despite recent advancements in Large Language Models (LLMs), idiomaticity remains a significant obstacle to robust semantic representation. We present datasets and tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity Representation), which challenges the community to assess and improve models' ability to interpret idiomatic expressions in multimodal contexts and in multiple languages. Participants competed in two subtasks: ranking images based on their alignment with idiomatic or literal meanings, and predicting the next image in a sequence. The most effective methods achieved human-level performance by leveraging pretrained LLMs and vision-language models in mixture-of-experts settings, with multiple queries used to smooth over the weaknesses in these models' representations of idiomaticity."
      },
      {
        "id": "oai:arXiv.org:2503.19878v2",
        "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2503.19878",
        "author": "Nengbo Wang, Xiaotian Han, Jagdip Singh, Jing Ma, Vipin Chaudhary",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.19878v2 Announce Type: replace \nAbstract: Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks."
      },
      {
        "id": "oai:arXiv.org:2503.21971v2",
        "title": "RocketPPA: Ultra-Fast LLM-Based PPA Estimator at Code-Level Abstraction",
        "link": "https://arxiv.org/abs/2503.21971",
        "author": "Armin Abdollahi, Mehdi Kamal, Massoud Pedram",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21971v2 Announce Type: replace \nAbstract: Large language models have recently transformed hardware design, yet bridging the gap between code synthesis and PPA (power, performance, and area) estimation remains a challenge. In this work, we introduce a novel framework that leverages a 21k dataset of thoroughly cleaned and synthesizable Verilog modules, each annotated with detailed power, delay, and area metrics. By employing chain-of-thought techniques, we automatically debug and curate this dataset to ensure high fidelity in downstream applications. We then fine-tune CodeLlama using LoRA-based parameter-efficient methods, framing the task as a regression problem to accurately predict PPA metrics from Verilog code. Furthermore, we augment our approach with a mixture-of-experts architecture-integrating both LoRA and an additional MLP expert layer-to further refine predictions. Experimental results demonstrate significant improvements: power estimation accuracy is enhanced by 5.9% at a 20% error threshold and by 7.2% at a 10% threshold, delay estimation improves by 5.1% and 3.9%, and area estimation sees gains of 4% and 7.9% for the 20% and 10% thresholds, respectively. Notably, the incorporation of the mixture-of-experts module contributes an additional 3--4% improvement across these tasks. Our results establish a new benchmark for PPA-aware Verilog generation, highlighting the effectiveness of our integrated dataset and modeling strategies for next-generation EDA workflows."
      },
      {
        "id": "oai:arXiv.org:2503.22480v4",
        "title": "Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model",
        "link": "https://arxiv.org/abs/2503.22480",
        "author": "Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, Jun Zhao, Kang Liu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22480v4 Announce Type: replace \nAbstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical technique for training large language models. However, reward hacking-a phenomenon where models exploit flaws in the reward model-remains a significant barrier to achieving robust and scalable intelligence through long-term training. Existing studies have proposed the uncertain reward models to address reward hacking, however, they often lack systematic or theoretical foundations, failing to model the uncertainty intrinsically emerging from preference data, and thus cannot sufficiently mitigate reward hacking to sustain prolonged RLHF training and exploration. In this paper, we propose a Probabilistic Uncertain Reward Model (PURM), a natural generalization of the classical Bradley-Terry reward model, that can directly learn the reward distribution emerged from the preference data. We theoretically derived PURM's loss function and the reward distribution uncertainty calculation based on Bhattacharyya Coefficient. To mitigate reward hacking with PURM, we further introduce an uncertainty-aware penalty into Proximal Policy Optimization (PPO), which leverages the learned uncertainty to dynamically balance reward optimization and exploration. We propose a lightweight and easy-to-use implementation of PURM. Experiments demonstrate that PURM effectively models the rewards and uncertainties, and significantly delays the onset of reward hacking while improving final reward performance compared with existing methods."
      },
      {
        "id": "oai:arXiv.org:2503.22809v2",
        "title": "Data-Driven Worker Activity Recognition and Efficiency Estimation in Manual Fruit Harvesting",
        "link": "https://arxiv.org/abs/2503.22809",
        "author": "Uddhav Bhattarai, Rajkishan Arikapudi, Steven A. Fennimore, Frank N Martin, Stavros G. Vougioukas",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22809v2 Announce Type: replace \nAbstract: Manual fruit harvesting is common in agriculture, but the amount of time pickers spend on non-productive activities can make it very inefficient. Accurately identifying picking vs. non-picking activity is crucial for estimating picker efficiency and optimising labour management and harvest processes. In this study, a practical system was developed to calculate the efficiency of pickers in commercial strawberry harvesting. Instrumented picking carts were developed to record the harvested fruit weight, geolocation, and cart movement in real time. These carts were deployed during the commercial strawberry harvest season in Santa Maria, CA. The collected data was then used to train a CNN-LSTM-based deep neural network to classify a picker's activity into \"Pick\" and \"NoPick\" classes. Experimental evaluations showed that the CNN-LSTM model showed promising activity recognition performance with an F1 score accuracy of over 0.97. The recognition results were then used to compute picker efficiency and the time required to fill a tray. Analysis of the season-long harvest data showed that the average picker efficiency was 75.07% with an estimation accuracy of 95.22%. Furthermore, the average tray fill time was 6.79 minutes with an estimation accuracy of 96.43%. When integrated into commercial harvesting, the proposed technology can aid growers in monitoring automated worker activity and optimising harvests to reduce non-productive time and enhance overall harvest efficiency."
      },
      {
        "id": "oai:arXiv.org:2503.23236v2",
        "title": "UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows",
        "link": "https://arxiv.org/abs/2503.23236",
        "author": "Isma\\\"el Zighed, Nicolas Thome, Patrick Gallinari, Taraneh Sayadi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23236v2 Announce Type: replace \nAbstract: Reduced order models (ROMs) play a critical role in fluid mechanics by providing low-cost predictions, making them an attractive tool for engineering applications. However, for ROMs to be widely applicable, they must not only generalise well across different regimes, but also provide a measure of confidence in their predictions. While recent data-driven approaches have begun to address nonlinear reduction techniques to improve predictions in transient environments, challenges remain in terms of robustness and parametrisation. In this work, we present a nonlinear reduction strategy specifically designed for transient flows that incorporates parametrisation and uncertainty quantification. Our reduction strategy features a variational auto-encoder (VAE) that uses variational inference for confidence measurement. We use a latent space transformer that incorporates recent advances in attention mechanisms to predict dynamical systems. Attention's versatility in learning sequences and capturing their dependence on external parameters enhances generalisation across a wide range of dynamics. Prediction, coupled with confidence, enables more informed decision making and addresses the need for more robust models. In addition, this confidence is used to cost-effectively sample the parameter space, improving model performance a priori across the entire parameter space without requiring evaluation data for the entire domain."
      },
      {
        "id": "oai:arXiv.org:2503.24091v2",
        "title": "4D mmWave Radar for Sensing Enhancement in Adverse Environments: Advances and Challenges",
        "link": "https://arxiv.org/abs/2503.24091",
        "author": "Xiangyuan Peng, Miao Tang, Huawei Sun, Kay Bierzynski, Lorenzo Servadei, Robert Wille",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24091v2 Announce Type: replace \nAbstract: Intelligent transportation systems require accurate and reliable sensing. However, adverse environments, such as rain, snow, and fog, can significantly degrade the performance of LiDAR and cameras. In contrast, 4D mmWave radar not only provides 3D point clouds and velocity measurements but also maintains robustness in challenging conditions. Recently, research on 4D mmWave radar under adverse environments has been growing, but a comprehensive review is still lacking. To bridge this gap, this work reviews the current research on 4D mmWave radar under adverse environments. First, we present an overview of existing 4D mmWave radar datasets encompassing diverse weather and lighting scenarios. Subsequently, we analyze existing learning-based methods leveraging 4D mmWave radar to enhance performance according to different adverse conditions. Finally, the challenges and potential future directions are discussed for advancing 4D mmWave radar applications in harsh environments. To the best of our knowledge, this is the first review specifically concentrating on 4D mmWave radar in adverse environments. The related studies are listed at: https://github.com/XiangyPeng/4D-mmWave-Radar-in-Adverse-Environments."
      },
      {
        "id": "oai:arXiv.org:2504.00799v2",
        "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users",
        "link": "https://arxiv.org/abs/2504.00799",
        "author": "Xi Wang, Fanfei Meng, Shiyang Zhang, Lan Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00799v2 Announce Type: replace \nAbstract: Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries."
      },
      {
        "id": "oai:arXiv.org:2504.00816v2",
        "title": "Two-stage deep learning framework for the restoration of incomplete-ring PET images",
        "link": "https://arxiv.org/abs/2504.00816",
        "author": "Yeqi Fang, Rong Zhou",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00816v2 Announce Type: replace \nAbstract: Positron Emission Tomography (PET) is an important molecular imaging tool widely used in medicine. Traditional PET systems rely on complete detector rings for full angular coverage and reliable data collection. However, incomplete-ring PET scanners have emerged due to hardware failures, cost constraints, or specific clinical needs. Standard reconstruction algorithms often suffer from performance degradation with these systems because of reduced data completeness and geometric inconsistencies. We present a two-stage deep-learning framework that, without incorporating any time-of-flight (TOF) information, restores high-quality images from data with about 50% missing coincidences - double the loss levels previously addressed by CNN-based methods. The pipeline operates in two stages: a projection-domain Attention U-Net first predicts the missing sections of the sinogram by leveraging spatial context from neighbouring slices, after which the completed data are reconstructed with OSEM algorithm and passed to a U-Net-diffusion module that removes residual artefacts while reinstating high-frequency detail. Using 206 brain volumes from a public dataset, the result shows that our model successfully preserves most anatomical structures and tracer distribution features with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher inference speed, thus providing an effective solution for incomplete-ring PET imaging."
      },
      {
        "id": "oai:arXiv.org:2504.04907v2",
        "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
        "link": "https://arxiv.org/abs/2504.04907",
        "author": "Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, Yongxin Ni",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04907v2 Announce Type: replace \nAbstract: Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our framework's assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment."
      },
      {
        "id": "oai:arXiv.org:2504.05239v2",
        "title": "LLM-based Automated Grading with Human-in-the-Loop",
        "link": "https://arxiv.org/abs/2504.05239",
        "author": "Hang Li, Yucheng Chu, Kaiqi Yang, Yasemin Copur-Gencturk, Jiliang Tang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05239v2 Announce Type: replace \nAbstract: The rise of artificial intelligence (AI) technologies, particularly large language models (LLMs), has brought significant advancements to the field of education. Among various applications, automatic short answer grading (ASAG), which focuses on evaluating open-ended textual responses, has seen remarkable progress with the introduction of LLMs. These models not only enhance grading performance compared to traditional ASAG approaches but also move beyond simple comparisons with predefined \"golden\" answers, enabling more sophisticated grading scenarios, such as rubric-based evaluation. However, existing LLM-powered methods still face challenges in achieving human-level grading performance in rubric-based assessments due to their reliance on fully automated approaches. In this work, we explore the potential of LLMs in ASAG tasks by leveraging their interactive capabilities through a human-in-the-loop (HITL) approach. Our proposed framework, GradeHITL, utilizes the generative properties of LLMs to pose questions to human experts, incorporating their insights to refine grading rubrics dynamically. This adaptive process significantly improves grading accuracy, outperforming existing methods and bringing ASAG closer to human-level evaluation."
      },
      {
        "id": "oai:arXiv.org:2504.07072v2",
        "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation",
        "link": "https://arxiv.org/abs/2504.07072",
        "author": "Israfel Salazar, Manuel Fern\\'andez Burda, Shayekh Bin Islam, Arshia Soltani Moakhar, Shivalika Singh, Fabian Farestam, Angelika Romanou, Danylo Boiko, Dipika Khullar, Mike Zhang, Dominik Krzemi\\'nski, Jekaterina Novikova, Lu\\'isa Shimabucoro, Joseph Marvin Imperial, Rishabh Maheshwary, Sharad Duwal, Alfonso Amayuelas, Swati Rajwal, Jebish Purbey, Ahmed Ruby, Nicholas Popovi\\v{c}, Marek Suppa, Azmine Toushik Wasi, Ram Mohan Rao Kadiyala, Olga Tsymboi, Maksim Kostritsya, Bardia Soltani Moakhar, Gabriel da Costa Merlin, Ot\\'avio Ferracioli Coletti, Maral Jabbari Shiviari, MohammadAmin farahani fard, Silvia Fernandez, Mar\\'ia Grandury, Dmitry Abulkhanov, Drishti Sharma, Andre Guarnier De Mitri, Leticia Bossatto Marchezi, Setayesh Heydari, Johan Obando-Ceron, Nazar Kohut, Beyza Ermis, Desmond Elliott, Enzo Ferrante, Sara Hooker, Marzieh Fadaee",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07072v2 Announce Type: replace \nAbstract: The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks."
      },
      {
        "id": "oai:arXiv.org:2504.10143v3",
        "title": "Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning",
        "link": "https://arxiv.org/abs/2504.10143",
        "author": "Yichao Cai, Yuhang Liu, Erdun Gao, Tianjiao Jiang, Zhen Zhang, Anton van den Hengel, Javen Qinfeng Shi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10143v3 Announce Type: replace \nAbstract: Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize misalignment by introducing two specific mechanisms: selection bias, where some semantic variables are missing, and perturbation bias, where semantic variables are distorted -- both affecting latent variables shared across modalities. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings through extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of misalignment on multimodal representation learning."
      },
      {
        "id": "oai:arXiv.org:2504.12397v2",
        "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
        "link": "https://arxiv.org/abs/2504.12397",
        "author": "Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.12397v2 Announce Type: replace \nAbstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is highly inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), which modifies the LoRA framework to only adapt weights for the tokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \\emph{intrinsics}, i.e. highly specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We use aLoRA to train a set of intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits."
      },
      {
        "id": "oai:arXiv.org:2504.13120v2",
        "title": "Probing and Inducing Combinational Creativity in Vision-Language Models",
        "link": "https://arxiv.org/abs/2504.13120",
        "author": "Yongqian Peng, Yuxi Ma, Mengmeng Wang, Yuxuan Wang, Yizhou Wang, Chi Zhang, Yixin Zhu, Zilong Zheng",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13120v2 Announce Type: replace \nAbstract: The ability to combine existing concepts into novel ideas stands as a fundamental hallmark of human intelligence. Recent advances in Vision-Language Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their outputs reflect combinational creativity--defined by M. A. Boden (1998) as synthesizing novel ideas through combining existing concepts--or sophisticated pattern matching of training data. Drawing inspiration from cognitive science, we investigate the combinational creativity of VLMs from the lens of concept blending. We propose the Identification-Explanation-Implication (IEI) framework, which decomposes creative processes into three levels: identifying input spaces, extracting shared attributes, and deriving novel semantic implications. To validate this framework, we curate CreativeMashup, a high-quality dataset of 666 artist-generated visual mashups annotated according to the IEI framework. Through extensive experiments, we demonstrate that in comprehension tasks, best VLMs have surpassed average human performance while falling short of expert-level understanding; in generation tasks, incorporating our IEI framework into the generation pipeline significantly enhances the creative quality of VLMs' outputs. Our findings establish both a theoretical foundation for evaluating artificial creativity and practical guidelines for improving creative generation in VLMs."
      },
      {
        "id": "oai:arXiv.org:2504.13181v2",
        "title": "Perception Encoder: The best visual embeddings are not at the output of the network",
        "link": "https://arxiv.org/abs/2504.13181",
        "author": "Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Doll\\'ar, Christoph Feichtenhofer",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13181v2 Announce Type: replace \nAbstract: We introduce Perception Encoder (PE), a state-of-the-art vision encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves best-in-class results on a wide variety of tasks, including (1) zero-shot image and video classification and retrieval, simultaneously obtaining 86.6 average zero-shot ImageNet robustness and 76.9 zero-shot Kinetics-400 video classification; (2) document, image, and video Q&amp;A, enabling 94.6 DocVQA, 80.9 InfographicVQA, and 82.7 PerceptionTest with an 8B LLM; and (3) spatial tasks such as detection, tracking, and depth estimation, setting a new COCO state-of-the-art of 66.0 box mAP. To foster further research, we release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models"
      },
      {
        "id": "oai:arXiv.org:2504.13754v2",
        "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis",
        "link": "https://arxiv.org/abs/2504.13754",
        "author": "Zhu Zhu, Shuo Jiang, Jingyuan Zheng, Yawen Li, Yifei Chen, Manli Zhao, Weizhong Gu, Feiwei Qin, Jinhu Wang, Gang Yu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13754v2 Announce Type: replace \nAbstract: Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics clinicians' comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to seamlessly bridge patch-level predictions to whole slide image-level classifications. We validate CMSwinKAN on the PpNTs dataset, which was collaboratively established with our partner hospital and the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at https://github.com/JSLiam94/CMSwinKAN."
      },
      {
        "id": "oai:arXiv.org:2504.13914v3",
        "title": "Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.13914",
        "author": "ByteDance Seed,  :, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, Zhihao Bai, Yu Bao, Xingyan Bin, Jiangjie Chen, Feng Chen, Hongmin Chen, Riwei Chen, Liangqiang Chen, Zixin Chen, Jinsong Chen, Siyan Chen, Kaiyuan Chen, Zhi Chen, Jin Chen, Jiecao Chen, Jinxin Chi, Weinan Dai, Ning Dai, Jiahui Dai, Shihan Dou, Yantao Du, Zhengyin Du, Jianhui Duan, Chen Dun, Ting-Han Fan, Jiazhan Feng, Junda Feng, Ziyuan Feng, Yuwei Fu, Wenqi Fu, Hanjie Fu, Hao Ge, Hongyi Guo, Mingji Han, Li Han, Wenhao Hao, Xintong Hao, Qianyu He, Jerry He, Feng He, Wen Heng, Zehua Hong, Qi Hou, Liang Hu, Shengding Hu, Nan Hu, Kai Hua, Qi Huang, Ziyue Huang, Hongzhi Huang, Zihao Huang, Ting Huang, Wenhao Huang, Wei Jia, Bin Jia, Xiaoying Jia, Yuhua Jiang, Haobin Jiang, Ziheng Jiang, Kaihua Jiang, Chengquan Jiang, Jianpeng Jiao, Xiaoran Jin, Xing Jin, Xunhao Lai, Zheng Li, Xiang Li, Liyi Li, Hongkai Li, Zheng Li, Shengxian Wan, Ya Wang, Yunshui Li, Chenggang Li, Niuniu Li, Siyu Li, Xi Li, Xiao Li, Aoyan Li, Yuntao Li, Nianning Liang, Xinnian Liang, Haibin Lin, Weijian Lin, Ye Lin, Zhicheng Liu, Guanlin Liu, Guanlin Liu, Chenxiao Liu, Yan Liu, Gaohong Liu, Juncai Liu, Chundian Liu, Deyi Liu, Kaibo Liu, Siyao Liu, Qi Liu, Yongfei Liu, Kang Liu, Gan Liu, Boyi Liu, Rui Long, Weiqiang Lou, Chenwei Lou, Xiang Luo, Yao Luo, Caiping Lv, Heyang Lv, Bole Ma, Qianli Ma, Hongzhi Ma, Yiyuan Ma, Jin Ma, Wenchang Ma, Tingting Ma, Chen Mao, Qiyang Min, Zhe Nan, Guanghan Ning, Jinxiang Ou, Haojie Pan, Renming Pang, Yanghua Peng, Tao Peng, Lihua Qian, Lihua Qian, Mu Qiao, Meng Qu, Cheng Ren, Hongbin Ren, Yong Shan, Wei Shen, Ke Shen, Kai Shen, Guangming Sheng, Jinlong Shi, Wenlei Shi, Guang Shi, Shuai Shuai Cao, Yuxin Song, Zuquan Song, Jing Su, Yifan Sun, Tao Sun, Zewei Sun, Borui Wan, Zihan Wang, Xiaohui Wang, Xi Wang, Shuguang Wang, Jun Wang, Qinlong Wang, Chenyuan Wang, Shuai Wang, Zihan Wang, Changbao Wang, Jiaqiang Wang, Shihang Wang, Xuwu Wang, Zaiyuan Wang, Yuxuan Wang, Wenqi Wang, Taiqing Wang, Chengzhi Wei, Houmin Wei, Ziyun Wei, Shufa Wei, Zheng Wu, Yonghui Wu, Yangjun Wu, Bohong Wu, Shuang Wu, Jingqiao Wu, Ning Wu, Shuangzhi Wu, Jianmin Wu, Chenguang Xi, Fan Xia, Yuqiao Xian, Liang Xiang, Boren Xiang, Bowen Xiao, Zhen Xiao, Xia Xiao, Yongsheng Xiao, Chao Xin, Shulin Xin, Yuwen Xiong, Jingjing Xu, Ziwen Xu, Chenyin Xu, Jiayi Xu, Yifan Xu, Wei Xu, Yufei Xu, Shikun Xu, Shipeng Yan, Shen Yan, Qingping Yang, Xi Yang, Tianhao Yang, Yuehang Yang, Yuan Yang, Ximing Yang, Zeyu Yang, Guang Yang, Yifan Yang, Xuesong Yao, Bairen Yi, Fan Yin, Jianian Yin, Ziqiang Ying, Xiangyu Yu, Hongli Yu, Song Yu, Menghan Yu, Huan Yu, Siyu Yuan, Jun Yuan, Yutao Zeng, Tianyang Zhan, Zheng Zhang, Yun Zhang, Mofan Zhang, Wang Zhang, Ru Zhang, Zhi Zhang, Tianqi Zhang, Xinyi Zhang, Zhexi Zhang, Sijun Zhang, Wenqiang Zhang, Xiangxiang Zhang, Yongtao Zhang, Yuyu Zhang, Ge Zhang, He Zhang, Yue Zhang, Renjie Zheng, Ningxin Zheng, Zhuolin Zheng, Yaowei Zheng, Chen Zheng, Xiaoyun Zhi, Wanjun Zhong, Cheng Zhong, Zheng Zhong, Baoquan Zhong, Xun Zhou, Na Zhou, Huan Zhou, Hang Zhu, Defa Zhu, Wenjia Zhu, Lei Zuo",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13914v3 Announce Type: replace \nAbstract: We introduce Seed1.5-Thinking, capable of reasoning through thinking before responding, resulting in improved performance on a wide range of benchmarks. Seed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on non-reasoning tasks, indicating its broader applicability. Compared to other state-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts (MoE) model with a relatively small size, featuring 20B activated and 200B total parameters. As part of our effort to assess generalized reasoning, we develop two internal benchmarks, BeyondAIME and Codeforces, both of which will be publicly released to support future research. Model trial link: https://www.volcengine.com/experience/ark."
      },
      {
        "id": "oai:arXiv.org:2504.14268v2",
        "title": "Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning",
        "link": "https://arxiv.org/abs/2504.14268",
        "author": "Xinye Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.14268v2 Announce Type: replace \nAbstract: This paper presents a novel reinforcement learning (RL) framework for dynamically optimizing numerical precision in the preconditioned conjugate gradient (CG) method. By modeling precision selection as a Markov Decision Process (MDP), we employ Q-learning to adaptively assign precision levels to key operations, striking an optimal balance between computational efficiency and numerical accuracy, while ensuring stability through double-precision scalar computations and residual computing. In practice, the algorithm is trained on a set of data and subsequently performs inference for precision selection on out-of-sample data, without requiring re-analysis or retraining for new datasets. This enables the method to adapt seamlessly to new problem instances without the computational overhead of recalibration. Our results demonstrate the effectiveness of RL in enhancing solver's performance, marking the first application of RL to mixed-precision numerical methods. The findings highlight the approach's practical advantages, robustness, and scalability, providing valuable insights into its integration with iterative solvers and paving the way for AI-driven advancements in scientific computing."
      },
      {
        "id": "oai:arXiv.org:2504.14582v2",
        "title": "NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results",
        "link": "https://arxiv.org/abs/2504.14582",
        "author": "Zheng Chen, Kai Liu, Jue Gong, Jingkai Wang, Lei Sun, Zongwei Wu, Radu Timofte, Yulun Zhang, Xiangyu Kong, Xiaoxuan Yu, Hyunhee Park, Suejin Han, Hakjae Jeon, Dafeng Zhang, Hyung-Ju Chun, Donghun Ryou, Inju Ha, Bohyung Han, Lu Zhao, Yuyi Zhang, Pengyu Yan, Jiawei Hu, Pengwei Liu, Fengjun Guo, Hongyuan Yu, Pufan Xu, Zhijuan Huang, Shuyuan Cui, Peng Guo, Jiahui Liu, Dongkai Zhang, Heng Zhang, Huiyuan Fu, Huadong Ma, Yanhui Guo, Sisi Tian, Xin Liu, Jinwen Liang, Jie Liu, Jie Tang, Gangshan Wu, Zeyu Xiao, Zhuoyuan Li, Yinxiang Zhang, Wenxuan Cai, Vijayalaxmi Ashok Aralikatti, Nikhil Akalwadi, G Gyaneshwar Rao, Chaitra Desai, Ramesh Ashok Tabib, Uma Mudenagudi, Marcos V. Conde, Alejandro Merino, Bruno Longarela, Javier Abad, Weijun Yuan, Zhan Li, Zhanglu Chen, Boyang Yao, Aagam Jain, Milan Kumar Singh, Ankit Kumar, Shubh Kawa, Divyavardhan Singh, Anjali Sarvaiya, Kishor Upla, Raghavendra Ramachandra, Chia-Ming Lee, Yu-Fan Lin, Chih-Chung Hsu, Risheek V Hiremath, Yashaswini Palani, Yuxuan Jiang, Qiang Zhu, Siyue Teng, Fan Zhang, Shuyuan Zhu, Bing Zeng, David Bull, Jingwei Liao, Yuqing Yang, Wenda Shao, Junyi Zhao, Qisheng Xu, Kele Xu, Sunder Ali Khowaja, Ik Hyun Lee, Snehal Singh Tomar, Rajarshi Ray, Klaus Mueller, Sachin Chaudhary, Surya Vashisth, Akshay Dudhane, Praful Hambarde, Satya Naryan Tazi, Prashant Patil, Santosh Kumar Vipparthi, Subrahmanyam Murala, Bilel Benjdira, Anas M. Ali, Wadii Boulila, Zahra Moammeri, Ahmad Mahmoudi-Aznaveh, Ali Karbasi, Hossein Motamednia, Liangyan Li, Guanhua Zhao, Kevin Le, Yimo Ning, Haoxuan Huang, Jun Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.14582v2 Announce Type: replace \nAbstract: This paper presents the NTIRE 2025 image super-resolution ($\\times$4) challenge, one of the associated competitions of the 10th NTIRE Workshop at CVPR 2025. The challenge aims to recover high-resolution (HR) images from low-resolution (LR) counterparts generated through bicubic downsampling with a $\\times$4 scaling factor. The objective is to develop effective network designs or solutions that achieve state-of-the-art SR performance. To reflect the dual objectives of image SR research, the challenge includes two sub-tracks: (1) a restoration track, emphasizes pixel-wise accuracy and ranks submissions based on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by a perceptual score. A total of 286 participants registered for the competition, with 25 teams submitting valid entries. This report summarizes the challenge design, datasets, evaluation protocol, the main results, and methods of each team. The challenge serves as a benchmark to advance the state of the art and foster progress in image SR."
      },
      {
        "id": "oai:arXiv.org:2504.15642v2",
        "title": "Computational Typology",
        "link": "https://arxiv.org/abs/2504.15642",
        "author": "Gerhard J\\\"ager",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.15642v2 Announce Type: replace \nAbstract: Typology is a subfield of linguistics that focuses on the study and classification of languages based on their structural features. Unlike genealogical classification, which examines the historical relationships between languages, typology seeks to understand the diversity of human languages by identifying common properties and patterns, known as universals. In recent years, computational methods have played an increasingly important role in typological research, enabling the analysis of large-scale linguistic data and the testing of hypotheses about language structure and evolution. This article provides an illustration of the benefits of computational statistical modeling in typology."
      },
      {
        "id": "oai:arXiv.org:2504.15900v3",
        "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.15900",
        "author": "Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.15900v3 Announce Type: replace \nAbstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to \"think before answering.\" Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding."
      },
      {
        "id": "oai:arXiv.org:2504.17365v3",
        "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation",
        "link": "https://arxiv.org/abs/2504.17365",
        "author": "Ling You, Wenxuan Huang, Xinni Xie, Xiangyi Wei, Bangyan Li, Shaohui Lin, Yang Li, Changbo Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.17365v3 Announce Type: replace \nAbstract: Soccer is a globally popular sporting event, typically characterized by long matches and distinctive highlight moments. Recent advances in Multimodal Large Language Models (MLLMs) offer promising capabilities in temporal grounding and video understanding, soccer commentary generation often requires precise temporal localization and semantically rich descriptions over long-form video. However, existing soccer MLLMs often rely on the temporal a priori for caption generation, so they cannot process the soccer video end-to-end. While some traditional approaches follow a two-step paradigm that is complex and fails to capture the global context to achieve suboptimal performance. To solve the above issues, we present TimeSoccer, the first end-to-end soccer MLLM for Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos. TimeSoccer jointly predicts timestamps and generates captions in a single pass, enabling global context modeling across 45-minute matches. To support long video understanding of soccer matches, we introduce MoFA-Select, a training-free, motion-aware frame compression module that adaptively selects representative frames via a coarse-to-fine strategy, and incorporates complementary training paradigms to strengthen the model's ability to handle long temporal sequences. Extensive experiments demonstrate that our TimeSoccer achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end form, generating high-quality commentary with accurate temporal alignment and strong semantic relevance."
      },
      {
        "id": "oai:arXiv.org:2504.17857v2",
        "title": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures",
        "link": "https://arxiv.org/abs/2504.17857",
        "author": "AJ Miller, Fangzhou Yu, Michael Brauckmann, Farbod Farshidian",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.17857v2 Announce Type: replace \nAbstract: This work presents an overview of the technical details behind a high performance reinforcement learning policy deployment with the Spot RL Researcher Development Kit for low level motor access on Boston Dynamics Spot. This represents the first public demonstration of an end to end end reinforcement learning policy deployed on Spot hardware with training code publicly available through Nvidia IsaacLab and deployment code available through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean Discrepancy to quantify the distributional dissimilarity of data collected on hardware and in simulation to measure our sim2real gap. We use these measures as a scoring function for the Covariance Matrix Adaptation Evolution Strategy to optimize simulated parameters that are unknown or difficult to measure from Spot. Our procedure for modeling and training produces high quality reinforcement learning policies capable of multiple gaits, including a flight phase. We deploy policies capable of over 5.2ms locomotion, more than triple Spots default controller maximum speed, robustness to slippery surfaces, disturbance rejection, and overall agility previously unseen on Spot. We detail our method and release our code to support future work on Spot with the low level API."
      },
      {
        "id": "oai:arXiv.org:2504.18317v2",
        "title": "Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy",
        "link": "https://arxiv.org/abs/2504.18317",
        "author": "Zhengru Fang, Zhenghao Liu, Jingjing Wang, Senkang Hu, Yu Guo, Yiqin Deng, Yuguang Fang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18317v2 Announce Type: replace \nAbstract: To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available: github.com/fangzr/TOC-Edge-Aerial."
      },
      {
        "id": "oai:arXiv.org:2504.18406v2",
        "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?",
        "link": "https://arxiv.org/abs/2504.18406",
        "author": "Yusen Zhang, Wenliang Zheng, Aashrith Madasu, Peng Shi, Ryo Kamoi, Hao Zhou, Zhuoyang Zou, Shu Zhao, Sarkar Snigdha Sarathi Das, Vipul Gupta, Xiaoxin Lu, Nan Zhang, Ranran Haoran Zhang, Avitej Iyer, Renze Lou, Wenpeng Yin, Rui Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18406v2 Announce Type: replace \nAbstract: High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research."
      },
      {
        "id": "oai:arXiv.org:2504.18583v2",
        "title": "PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation",
        "link": "https://arxiv.org/abs/2504.18583",
        "author": "Zihao An, Huajun Bai, Ziqiong Liu, Dong Li, Emad Barsoum",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18583v2 Announce Type: replace \nAbstract: The autoregressive nature of large language models (LLMs) limits inference speed. Each forward pass generates only a single token and is often bottlenecked by memory bandwidth. Speculative decoding alleviates this issue using a draft-then-verify approach to accelerate token generation. However, the overhead introduced during the draft phase and the training cost of the draft model limit the efficiency and adaptability of speculative decoding. In this work, we introduce PARallel Draft (PARD), a novel speculative decoding method that enables low-cost adaptation of autoregressive draft models into parallel draft models. PARD enhances inference efficiency by predicting multiple future tokens in a single forward pass of the draft phase, and incorporates a conditional drop token method to accelerate training. Its target-independence property allows a single draft model to be applied to an entire family of different models, minimizing the adaptation cost. Our proposed conditional drop token method can improves draft model training efficiency by 3x. On our optimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x, achieving 311.5 tokens per second."
      },
      {
        "id": "oai:arXiv.org:2504.18589v2",
        "title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency",
        "link": "https://arxiv.org/abs/2504.18589",
        "author": "Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18589v2 Announce Type: replace \nAbstract: Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements."
      },
      {
        "id": "oai:arXiv.org:2504.19013v2",
        "title": "\\$PINN - a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks",
        "link": "https://arxiv.org/abs/2504.19013",
        "author": "J\\'ulia Vicens Figueres, Juliette Vanderhaeghen, Federica Bragone, Kateryna Morozovska, Khemraj Shukla",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19013v2 Announce Type: replace \nAbstract: Physics-Informed Neural Networks (PINNs) are a novel computational approach for solving partial differential equations (PDEs) with noisy and sparse initial and boundary data. Although, efficient quantification of epistemic and aleatoric uncertainties in big multi-scale problems remains challenging. We propose \\$PINN a novel method of computing global uncertainty in PDEs using a Bayesian framework, by combining local Bayesian Physics-Informed Neural Networks (BPINN) with domain decomposition. The solution continuity across subdomains is obtained by imposing the flux continuity across the interface of neighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct a series of computational experiments on PDEs in 1D and 2D spatial domains. Although we have adopted conservative PINNs (cPINNs), the method can be seamlessly extended to other domain decomposition techniques. The results infer that the proposed method recovers the global uncertainty by computing the local uncertainty exactly more efficiently as the uncertainty in each subdomain can be computed concurrently. The robustness of \\$PINN is verified by adding uncorrelated random noise to the training data up to 15% and testing for different domain sizes."
      },
      {
        "id": "oai:arXiv.org:2504.19165v2",
        "title": "IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos",
        "link": "https://arxiv.org/abs/2504.19165",
        "author": "Yuan Li, Ziqian Bai, Feitong Tan, Zhaopeng Cui, Sean Fanello, Yinda Zhang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19165v2 Announce Type: replace \nAbstract: We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data."
      },
      {
        "id": "oai:arXiv.org:2504.19333v2",
        "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing",
        "link": "https://arxiv.org/abs/2504.19333",
        "author": "James O' Neill, Santhosh Subramanian, Eric Lin, Vaikkunth Mugunthan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19333v2 Announce Type: replace \nAbstract: The trend towards large language models (LLMs) for guardrailing against undesired behaviors is increasing and has shown promise for censoring user inputs. However, increased latency, memory consumption, hosting expenses and non-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to fine-tuned classifiers that significantly outperform current state of the art (SoTA) while being orders of magnitude smaller. Secondly, we show that using a single model, \\texttt{MultiTaskGuard}, that is pretrained on a large synthetically generated dataset with unique task instructions further improves generalization. Thirdly, our most performant models, \\texttt{UniGuard}, are found using our proposed search-based model merging approach that finds an optimal set of parameters to combine single-policy models and multi-policy guardrail models. % On 7 public datasets and 4 guardrail benchmarks we created, our efficient guardrail classifiers improve over the best performing SoTA publicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting unsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92} points over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o}, respectively. Lastly, our guardrail synthetic data generation process that uses custom task-specific guardrail poli"
      },
      {
        "id": "oai:arXiv.org:2504.19406v2",
        "title": "Context Selection and Rewriting for Video-based Educational Question Generation",
        "link": "https://arxiv.org/abs/2504.19406",
        "author": "Mengxia Yu, Bang Nguyen, Olivia Zino, Meng Jiang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19406v2 Announce Type: replace \nAbstract: Educational question generation (EQG) is a crucial component of intelligent educational systems, significantly aiding self-assessment, active learning, and personalized education. While EQG systems have emerged, existing datasets typically rely on predefined, carefully edited texts, failing to represent real-world classroom content, including lecture speech with a set of complementary slides. To bridge this gap, we collect a dataset of educational questions based on lectures from real-world classrooms. On this realistic dataset, we find that current methods for EQG struggle with accurately generating questions from educational videos, particularly in aligning with specific timestamps and target answers. Common challenges include selecting informative contexts from extensive transcripts and ensuring generated questions meaningfully incorporate the target answer. To address the challenges, we introduce a novel framework utilizing large language models for dynamically selecting and rewriting contexts based on target timestamps and answers. First, our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity. Then, we integrate the contexts selected from both modalities and rewrite them into answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer. This approach significantly improves the quality and relevance of the generated questions. Our dataset and code are released in https://github.com/mengxiayu/COSER."
      },
      {
        "id": "oai:arXiv.org:2504.19452v2",
        "title": "Geometry-Informed Neural Operator Transformer",
        "link": "https://arxiv.org/abs/2504.19452",
        "author": "Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric, Philippe Geubelle",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19452v2 Announce Type: replace \nAbstract: Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions for arbitrary geometries. GINOT encodes the surface points cloud of a geometry using a sampling and grouping mechanism combined with an attention mechanism, ensuring invariance to point order and padding while maintaining robustness to variations in point density. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries."
      },
      {
        "id": "oai:arXiv.org:2504.19458v2",
        "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective",
        "link": "https://arxiv.org/abs/2504.19458",
        "author": "Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19458v2 Announce Type: replace \nAbstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.19595v2",
        "title": "WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution",
        "link": "https://arxiv.org/abs/2504.19595",
        "author": "Pietro Bongini, Sara Mandelli, Andrea Montibeller, Mirko Casu, Orazio Pontorno, Claudio Vittorio Ragaglia, Luca Zanchetta, Mattia Aquilina, Taiba Majid Wani, Luca Guarnera, Benedetta Tondi, Giulia Boato, Paolo Bestagini, Irene Amerini, Francesco De Natale, Sebastiano Battiato, Mauro Barni",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19595v2 Announce Type: replace \nAbstract: Synthetic image source attribution is an open challenge, with an increasing number of image generators being released yearly. The complexity and the sheer number of available generative techniques, as well as the scarcity of high-quality open source datasets of diverse nature for this task, make training and benchmarking synthetic image source attribution models very challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to provide a powerful training and benchmarking tool for synthetic image attribution models. The dataset is built out of a closed set of 10 popular commercial generators, which constitutes the training base of attribution models, and an open set of 10 additional generators, simulating a real-world in-the-wild scenario. Each generator is represented by 1,000 images, for a total of 10,000 images in the closed set and 10,000 images in the open set. Half of the images are post-processed with a wide range of operators. WILD allows benchmarking attribution models in a wide range of tasks, including closed and open set identification and verification, and robust attribution with respect to post-processing and adversarial attacks. Models trained on WILD are expected to benefit from the challenging scenario represented by the dataset itself. Moreover, an assessment of seven baseline methodologies on closed and open set attribution is presented, including robustness tests with respect to post-processing."
      },
      {
        "id": "oai:arXiv.org:2504.19774v2",
        "title": "If Concept Bottlenecks are the Question, are Foundation Models the Answer?",
        "link": "https://arxiv.org/abs/2504.19774",
        "author": "Nicola Debole, Pietro Barbiero, Francesco Giannini, Andrea Passerini, Stefano Teso, Emanuele Marconato",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19774v2 Announce Type: replace \nAbstract: Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high performance with ante-hoc interpretability. CBMs work by first mapping inputs (e.g., images) to high-level concepts (e.g., visible objects and their properties) and then use these to solve a downstream task (e.g., tagging or scoring an image) in an interpretable manner. Their performance and interpretability, however, hinge on the quality of the concepts they learn. The go-to strategy for ensuring good quality concepts is to leverage expert annotations, which are expensive to collect and seldom available in applications. Researchers have recently addressed this issue by introducing \"VLM-CBM\" architectures that replace manual annotations with weak supervision from foundation models. It is however unclear what is the impact of doing so on the quality of the learned concepts. To answer this question, we put state-of-the-art VLM-CBMs to the test, analyzing their learned concepts empirically using a selection of significant metrics. Our results show that, depending on the task, VLM supervision can sensibly differ from expert annotations, and that concept accuracy and quality are not strongly correlated. Our code is available at https://github.com/debryu/CQA."
      },
      {
        "id": "oai:arXiv.org:2504.19979v2",
        "title": "Transfer Learning Under High-Dimensional Network Convolutional Regression Model",
        "link": "https://arxiv.org/abs/2504.19979",
        "author": "Liyuan Wang, Jiachen Chen, Kathryn L. Lunetta, Danyang Huang, Huimin Cheng, Debarghya Mukherjee",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19979v2 Announce Type: replace \nAbstract: Transfer learning enhances model performance by utilizing knowledge from related domains, particularly when labeled data is scarce. While existing research addresses transfer learning under various distribution shifts in independent settings, handling dependencies in networked data remains challenging. To address this challenge, we propose a high-dimensional transfer learning framework based on network convolutional regression (NCR), inspired by the success of graph convolutional networks (GCNs). The NCR model incorporates random network structure by allowing each node's response to depend on its features and the aggregated features of its neighbors, capturing local dependencies effectively. Our methodology includes a two-step transfer learning algorithm that addresses domain shift between source and target networks, along with a source detection mechanism to identify informative domains. Theoretically, we analyze the lasso estimator in the context of a random graph based on the Erdos-Renyi model assumption, demonstrating that transfer learning improves convergence rates when informative sources are present. Empirical evaluations, including simulations and a real-world application using Sina Weibo data, demonstrate substantial improvements in prediction accuracy, particularly when labeled data in the target domain is limited."
      },
      {
        "id": "oai:arXiv.org:2504.20013v2",
        "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation",
        "link": "https://arxiv.org/abs/2504.20013",
        "author": "Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20013v2 Announce Type: replace \nAbstract: Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems."
      },
      {
        "id": "oai:arXiv.org:1910.04295v2",
        "title": "Linear-Quadratic Mean-Field Reinforcement Learning: Convergence of Policy Gradient Methods",
        "link": "https://arxiv.org/abs/1910.04295",
        "author": "Ren\\'e Carmona, Mathieu Lauri\\`ere, Zongjun Tan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:1910.04295v2 Announce Type: replace-cross \nAbstract: We investigate reinforcement learning in the setting of Markov decision processes for a large number of exchangeable agents interacting in a mean field manner. Applications include, for example, the control of a large number of robots communicating through a central unit dispatching the optimal policy computed by maximizing an aggregate reward. An approximate solution is obtained by learning the optimal policy of a generic agent interacting with the statistical distribution of the states and actions of the other agents. We first provide a full analysis this discrete-time mean field control problem. We then rigorously prove the convergence of exact and model-free policy gradient methods in a mean-field linear-quadratic setting and establish bounds on the rates of convergence. We also provide graphical evidence of the convergence based on implementations of our algorithms."
      },
      {
        "id": "oai:arXiv.org:2206.02702v2",
        "title": "Stochastic Variance-Reduced Newton: Accelerating Finite-Sum Minimization with Large Batches",
        "link": "https://arxiv.org/abs/2206.02702",
        "author": "Micha{\\l} Derezi\\'nski",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2206.02702v2 Announce Type: replace-cross \nAbstract: Stochastic variance reduction has proven effective at accelerating first-order algorithms for solving convex finite-sum optimization tasks such as empirical risk minimization. Incorporating second-order information has proven helpful in further improving the performance of these first-order methods. Yet, comparatively little is known about the benefits of using variance reduction to accelerate popular stochastic second-order methods such as Subsampled Newton. To address this, we propose Stochastic Variance-Reduced Newton (SVRN), a finite-sum minimization algorithm that provably accelerates existing stochastic Newton methods from $O(\\alpha\\log(1/\\epsilon))$ to $O\\big(\\frac{\\log(1/\\epsilon)}{\\log(n)}\\big)$ passes over the data, i.e., by a factor of $O(\\alpha\\log(n))$, where $n$ is the number of sum components and $\\alpha$ is the approximation factor in the Hessian estimate. Surprisingly, this acceleration gets more significant the larger the data size $n$, which is a unique property of SVRN. Our algorithm retains the key advantages of Newton-type methods, such as easily parallelizable large-batch operations and a simple unit step size. We use SVRN to accelerate Subsampled Newton and Iterative Hessian Sketch algorithms, and show that it compares favorably to popular first-order methods with variance~reduction."
      },
      {
        "id": "oai:arXiv.org:2304.09310v5",
        "title": "The Adaptive $\\tau$-Lasso: Robustness and Oracle Properties",
        "link": "https://arxiv.org/abs/2304.09310",
        "author": "Emadaldin Mozafari-Majd, Visa Koivunen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2304.09310v5 Announce Type: replace-cross \nAbstract: This paper introduces a new regularized version of the robust $\\tau$-regression estimator for analyzing high-dimensional datasets subject to gross contamination in the response variables and covariates. The resulting estimator, termed adaptive $\\tau$-Lasso, is robust to outliers and high-leverage points. It also incorporates an adaptive $\\ell_1$-norm penalty term, which enables the selection of relevant variables and reduces the bias associated with large true regression coefficients. More specifically, this adaptive $\\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\\tau$-Lasso has the oracle property, ensuring both variable-selection consistency and asymptotic normality. Asymptotic normality applies only to the entries of the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We characterize its robustness by establishing the finite-sample breakdown point and the influence function. We carry out extensive simulations and observe that the class of $\\tau$-Lasso estimators exhibits robustness and reliable performance in both contaminated and uncontaminated data settings. We also validate our theoretical findings on robustness properties through simulations. In the face of outliers and high-leverage points, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators achieve the best performance or match the best performances of competing regularized estimators, with minimal or no loss in terms of prediction and variable selection accuracy for almost all scenarios considered in this study. Therefore, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators provide attractive tools for a variety of sparse linear regression problems, particularly in high-dimensional settings and when the data is contaminated by outliers and high-leverage points."
      },
      {
        "id": "oai:arXiv.org:2305.00241v3",
        "title": "When Deep Learning Meets Polyhedral Theory: A Survey",
        "link": "https://arxiv.org/abs/2305.00241",
        "author": "Joey Huchette, Gonzalo Mu\\~noz, Thiago Serra, Calvin Tsay",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.00241v3 Announce Type: replace-cross \nAbstract: In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\\unicode{x2014}$such as the typical fully-connected feedforward neural network$\\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to applying linear optimization techniques to train, verify, and reduce the size of such networks."
      },
      {
        "id": "oai:arXiv.org:2403.15509v2",
        "title": "Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection",
        "link": "https://arxiv.org/abs/2403.15509",
        "author": "Phai Vu Dinh, Quang Uy Nguyen, Thai Hoang Dinh, Diep N. Nguyen, Bao Son Pham, Eryk Dutkiewicz",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.15509v2 Announce Type: replace-cross \nAbstract: Representation learning (RL) methods for cyberattack detection face the diversity and sophistication of attack data, leading to the issue of mixed representations of different classes, particularly as the number of classes increases. To address this, the paper proposes a novel deep learning architecture/model called the Twin Auto-Encoder (TAE). TAE first maps the input data into latent space and then deterministically shifts data samples of different classes further apart to create separable data representations, referred to as representation targets. TAE's decoder then projects the input data into these representation targets. After training, TAE's decoder extracts data representations. TAE's representation target serves as a novel dynamic codeword, which refers to the vector that represents a specific class. This vector is updated after each training epoch for every data sample, in contrast to the conventional fixed codeword that does not incorporate information from the input data. We conduct extensive experiments on diverse cybersecurity datasets, including seven IoT botnet datasets, two network IDS datasets, three malware datasets, one cloud DDoS dataset, and ten artificial datasets as the number of classes increases. TAE boosts accuracy and F-score in attack detection by around 2% compared to state-of-the-art models, achieving up to 96.1% average accuracy in IoT attack detection. Additionally, TAE is well-suited for cybersecurity applications and potentially for IoT systems, with a model size of approximately 1 MB and an average running time of around 2.6E-07 seconds for extracting a data sample."
      },
      {
        "id": "oai:arXiv.org:2405.00252v3",
        "title": "Q-Newton: Hybrid Quantum-Classical Scheduling for Accelerating Neural Network Training with Newton's Gradient Descent",
        "link": "https://arxiv.org/abs/2405.00252",
        "author": "Pingzhi Li, Junyu Liu, Hanrui Wang, Tianlong Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.00252v3 Announce Type: replace-cross \nAbstract: Optimization techniques in deep learning are predominantly led by first-order gradient methodologies, such as SGD. However, neural network training can greatly benefit from the rapid convergence characteristics of second-order optimization. Newton's GD stands out in this category, by rescaling the gradient using the inverse Hessian. Nevertheless, one of its major bottlenecks is matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak scalability.\n  Matrix inversion can be translated into solving a series of linear equations. Given that quantum linear solver algorithms (QLSAs), leveraging the principles of quantum superposition and entanglement, can operate within a $\\text{polylog}(N)$ time frame, they present a promising approach with exponential acceleration. Specifically, one of the most recent QLSAs demonstrates a complexity scaling of $O(d\\cdot\\kappa \\log(N\\cdot\\kappa/\\epsilon))$, depending on: {size~$N$, condition number~$\\kappa$, error tolerance~$\\epsilon$, quantum oracle sparsity~$d$} of the matrix. However, this also implies that their potential exponential advantage may be hindered by certain properties (i.e. $\\kappa$ and $d$).\n  We propose Q-Newton, a hybrid quantum-classical scheduler for accelerating neural network training with Newton's GD. Q-Newton utilizes a streamlined scheduling module that coordinates between quantum and classical linear solvers, by estimating & reducing $\\kappa$ and constructing $d$ for the quantum solver.\n  Our evaluation showcases the potential for Q-Newton to significantly reduce the total training time compared to commonly used optimizers like SGD. We hypothesize a future scenario where the gate time of quantum machines is reduced, possibly realized by attoseconds physics. Our evaluation establishes an ambitious and promising target for the evolution of quantum computing."
      },
      {
        "id": "oai:arXiv.org:2408.05854v3",
        "title": "On the Robustness of Kernel Goodness-of-Fit Tests",
        "link": "https://arxiv.org/abs/2408.05854",
        "author": "Xing Liu, Fran\\c{c}ois-Xavier Briol",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.05854v3 Announce Type: replace-cross \nAbstract: Goodness-of-fit testing is often criticized for its lack of practical relevance: since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected as sample size grows. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is \\emph{good enough} for the task at hand. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated from a distribution that is a mild perturbation of the model. In this paper, we show that existing kernel goodness-of-fit tests are not robust under common notions of robustness including both qualitative and quantitative robustness. We further show that robustification techniques using tilted kernels, while effective in the parameter estimation literature, are not sufficient to ensure both types of robustness in the testing setting. To address this, we propose the first robust kernel goodness-of-fit test, which resolves this open problem by using kernel Stein discrepancy (KSD) balls. This framework encompasses many well-known perturbation models, such as Huber's contamination and density-band models."
      },
      {
        "id": "oai:arXiv.org:2408.15253v2",
        "title": "A Deep Generative Model for Five-Class Sleep Staging with Arbitrary Sensor Input",
        "link": "https://arxiv.org/abs/2408.15253",
        "author": "Hans van Gorp, Merel M. van Gilst, Pedro Fonseca, Fokke B. van Meulen, Johannes P. van Dijk, Sebastiaan Overeem, Ruud J. G. van Sloun",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.15253v2 Announce Type: replace-cross \nAbstract: Gold-standard sleep scoring is based on epoch-based assignment of sleep stages based on a combination of EEG, EOG and EMG signals. However, a polysomnographic recording consists of many other signals that could be used for sleep staging, including cardio-respiratory modalities. Leveraging this signal variety would offer important advantages, for example increasing reliability, resilience to signal loss, and application to long-term non-obtrusive recordings. We developed a deep generative model for automatic sleep staging from a plurality of sensors and any -arbitrary- combination thereof. We trained a score-based diffusion model using a dataset of 1947 expert-labelled overnight recordings with 36 different signals, and achieved zero-shot inference on any sensor set by leveraging a novel Bayesian factorization of the score function across the sensors. On single-channel EEG, the model reaches the performance limit in terms of polysomnography inter-rater agreement (5- class accuracy 85.6%, Cohen's kappa 0.791). Moreover, the method offers full flexibility to use any sensor set, for example finger photoplethysmography, nasal flow and thoracic respiratory movements, (5-class accuracy 79.0%, Cohen's kappa of 0.697), or even derivations very unconventional for sleep staging, such as tibialis and sternocleidomastoid EMG (5-class accuracy 71.0%, kappa 0.575). Additionally, we propose a novel interpretability metric in terms of information gain per sensor and show this is linearly correlated with classification performance. Finally, our model allows for post- hoc addition of entirely new sensor modalities by merely training a score estimator on the novel input instead of having to retrain from scratch on all inputs."
      },
      {
        "id": "oai:arXiv.org:2408.15268v3",
        "title": "Anomaly Detection in Time Series of EDFA Pump Currents to Monitor Degeneration Processes using Fuzzy Clustering",
        "link": "https://arxiv.org/abs/2408.15268",
        "author": "Dominic Schneider, Lutz Rapp, Christoph Ament",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.15268v3 Announce Type: replace-cross \nAbstract: This article proposes a novel fuzzy clustering based anomaly detection method for pump current time series of EDFA systems. The proposed change detection framework (CDF) strategically combines the advantages of entropy analysis (EA) and principle component analysis (PCA) with fuzzy clustering procedures. In the framework, EA is applied for dynamic selection of features for reduction of the feature space and increase of computational performance. Furthermore, PCA is utilized to extract features from the raw feature space to enable generalization capability of the subsequent fuzzy clustering procedures. Three different fuzzy clustering methods, more precisely the fuzzy clustering algorithm, a probabilistic clustering algorithm and a possibilistic clustering algorithm are evaluated for performance and generalization. Hence, the proposed framework has the innovative feature to detect changes in pump current time series at an early stage for arbitrary points of operation, compared to state-of-the-art predefined alarms in commercially used EDFAs. Moreover, the approach is implemented and tested using experimental data. In addition, the proposed framework enables further approaches of applying decentralized predictive maintenance for optical fiber networks."
      },
      {
        "id": "oai:arXiv.org:2409.07956v2",
        "title": "Community detection in multi-layer networks by regularized debiased spectral clustering",
        "link": "https://arxiv.org/abs/2409.07956",
        "author": "Huan Qing",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.07956v2 Announce Type: replace-cross \nAbstract: Community detection is a crucial problem in the analysis of multi-layer networks. While regularized spectral clustering methods using the classical regularized Laplacian matrix have shown great potential in handling sparse single-layer networks, to our knowledge, their potential in multi-layer network community detection remains unexplored. To address this gap, in this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical one to multi layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. Additionally, we introduce a sum of squared adjacency matrices modularity (SoS-modularity) to measure the quality of community partitions in multi-layer networks and estimate the number of communities by maximizing this metric. Our methods offer promising applications for predicting gene functions, improving recommender systems, detecting medical insurance fraud, and facilitating link prediction. Experimental results demonstrate that our methods exhibit insensitivity to the selection of the regularizer, generally outperform state-of-the-art techniques, uncover the assortative property of real networks, and that our SoS-modularity provides a more accurate assessment of community quality compared to the average of the Newman-Girvan modularity across layers."
      },
      {
        "id": "oai:arXiv.org:2409.08295v2",
        "title": "Higher order definition of causality by optimally conditioned transfer entropy",
        "link": "https://arxiv.org/abs/2409.08295",
        "author": "Jakub Ko\\v{r}enek, Pavel Sanda, Jaroslav Hlinka",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.08295v2 Announce Type: replace-cross \nAbstract: The description of the dynamics of complex systems, in particular the capture of the interaction structure and causal relationships between elements of the system, is one of the central questions of interdisciplinary research. While the characterization of pairwise causal interactions is a relatively ripe field with established theoretical concepts and the current focus is on technical issues of their efficient estimation, it turns out that the standard concepts such as Granger causality or transfer entropy may not faithfully reflect possible synergies or interactions of higher orders, phenomena highly relevant for many real-world complex systems. In this paper, we propose a generalization and refinement of the information-theoretic approach to causal inference, enabling the description of truly multivariate, rather than multiple pairwise, causal interactions, and moving thus from causal networks to causal hypernetworks. In particular, while keeping the ability to control for mediating variables or common causes, in case of purely synergetic interactions such as the exclusive disjunction, it ascribes the causal role to the multivariate causal set but \\emph{not} to individual inputs, distinguishing it thus from the case of e.g. two additive univariate causes. We demonstrate this concept by application to illustrative theoretical examples as well as a biophysically realistic simulation of biological neuronal dynamics recently reported to employ synergetic computations."
      },
      {
        "id": "oai:arXiv.org:2410.14760v2",
        "title": "Advancing Physics Data Analysis through Machine Learning and Physics-Informed Neural Networks",
        "link": "https://arxiv.org/abs/2410.14760",
        "author": "Vasileios Vatellis",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.14760v2 Announce Type: replace-cross \nAbstract: In an era increasingly focused on green computing and explainable AI, revisiting traditional approaches in theoretical and phenomenological particle physics is paramount. This project evaluates various machine learning (ML) algorithms-including Nearest Neighbors, Decision Trees, Random Forest, AdaBoost, Naive Bayes, Quadratic Discriminant Analysis (QDA), and XGBoost-alongside standard neural networks and a novel Physics-Informed Neural Network (PINN) for physics data analysis. We apply these techniques to a binary classification task that distinguishes the experimental viability of simulated scenarios based on Higgs observables and essential parameters. Through this comprehensive analysis, we aim to showcase the capabilities and computational efficiency of each model in binary classification tasks, thereby contributing to the ongoing discourse on integrating ML and Deep Neural Networks (DNNs) into physics research. In this study, XGBoost emerged as the preferred choice among the evaluated machine learning algorithms for its speed and effectiveness, especially in the initial stages of computation with limited datasets. However, while standard Neural Networks and Physics-Informed Neural Networks (PINNs) demonstrated superior performance in terms of accuracy and adherence to physical laws, they require more computational time. These findings underscore the trade-offs between computational efficiency and model sophistication."
      },
      {
        "id": "oai:arXiv.org:2410.21081v2",
        "title": "Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: Generalized Baselines",
        "link": "https://arxiv.org/abs/2410.21081",
        "author": "Benjamin Schiffer, Lucas Janson",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.21081v2 Announce Type: replace-cross \nAbstract: Many practical applications of online reinforcement learning require the satisfaction of safety constraints while learning about the unknown environment. In this work, we establish theoretical foundations for reinforcement learning with safety constraints by studying the canonical problem of Linear Quadratic Regulator learning with unknown dynamics, but with the additional constraint that the position must stay within a safe region for the entire trajectory with high probability. Our primary contribution is a general framework for studying stronger baselines of nonlinear controllers that are better suited for constrained problems than linear controllers. Due to the difficulty of analyzing non-linear controllers in a constrained problem, we focus on 1-dimensional state- and action- spaces, however we also discuss how we expect the high-level takeaways can generalize to higher dimensions. Using our framework, we show that for \\emph{any} non-linear baseline satisfying natural assumptions, $\\tilde{O}_T(\\sqrt{T})$-regret is possible when the noise distribution has sufficiently large support, and $\\tilde{O}_T(T^{2/3})$-regret is possible for \\emph{any} subgaussian noise distribution. In proving these results, we introduce a new uncertainty estimation bound for nonlinear controls which shows that enforcing safety in the presence of sufficient noise can provide ``free exploration'' that compensates for the added cost of uncertainty in safety-constrained control."
      },
      {
        "id": "oai:arXiv.org:2411.00405v2",
        "title": "HAVER: Instance-Dependent Error Bounds for Maximum Mean Estimation and Applications to Q-Learning and Monte Carlo Tree Search",
        "link": "https://arxiv.org/abs/2411.00405",
        "author": "Tuan Ngo Nguyen, Jay Barrett, Kwang-Sung Jun",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00405v2 Announce Type: replace-cross \nAbstract: We study the problem of estimating the \\emph{value} of the largest mean among K distributions via samples from them (rather than estimating \\emph{which} distribution has the largest mean), which arises from various machine learning tasks including Q-learning and Monte Carlo Tree Search (MCTS). While there have been a few proposed algorithms, their performance analyses have been limited to their biases rather than a precise error metric. In this paper, we propose a novel algorithm called HAVER (Head AVERaging) and analyze its mean squared error. Our analysis reveals that HAVER has a compelling performance in two respects. First, HAVER estimates the maximum mean as well as the oracle who knows the identity of the best distribution and reports its sample mean. Second, perhaps surprisingly, HAVER exhibits even better rates than this oracle when there are many distributions near the best one. Both of these improvements are the first of their kind in the literature, and we also prove that the naive algorithm that reports the largest empirical mean does not achieve these bounds. Finally, we confirm our theoretical findings via numerical experiments where we implement HAVER in bandit, Q-learning, and MCTS algorithms. In these experiments, HAVER consistently outperforms the baseline methods, demonstrating its effectiveness across different applications."
      },
      {
        "id": "oai:arXiv.org:2411.02549v2",
        "title": "Distributionally Robust Optimization",
        "link": "https://arxiv.org/abs/2411.02549",
        "author": "Daniel Kuhn, Soroosh Shafiee, Wolfram Wiesemann",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.02549v2 Announce Type: replace-cross \nAbstract: Distributionally robust optimization (DRO) studies decision problems under uncertainty where the probability distribution governing the uncertain problem parameters is itself uncertain. A key component of any DRO model is its ambiguity set, that is, a family of probability distributions consistent with any available structural or statistical information. DRO seeks decisions that perform best under the worst distribution in the ambiguity set. This worst case criterion is supported by findings in psychology and neuroscience, which indicate that many decision-makers have a low tolerance for distributional ambiguity. DRO is rooted in statistics, operations research and control theory, and recent research has uncovered its deep connections to regularization techniques and adversarial training in machine learning. This survey presents the key findings of the field in a unified and self-contained manner."
      },
      {
        "id": "oai:arXiv.org:2411.08773v2",
        "title": "Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity",
        "link": "https://arxiv.org/abs/2411.08773",
        "author": "Shabarish Chenakkod, Micha{\\l} Derezi\\'nski, Xiaoyu Dong",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08773v2 Announce Type: replace-cross \nAbstract: An oblivious subspace embedding is a random $m\\times n$ matrix $\\Pi$ such that, for any $d$-dimensional subspace, with high probability $\\Pi$ preserves the norms of all vectors in that subspace within a $1\\pm\\epsilon$ factor. In this work, we give an oblivious subspace embedding with the optimal dimension $m=\\Theta(d/\\epsilon^2)$ that has a near-optimal sparsity of $\\tilde O(1/\\epsilon)$ non-zero entries per column of $\\Pi$. This is the first result to nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of the best sparsity attainable by an optimal oblivious subspace embedding, improving on a prior bound of $\\tilde O(1/\\epsilon^6)$ non-zeros per column [Chenakkod et al., STOC 2024]. We further extend our approach to the non-oblivious setting, proposing a new family of Leverage Score Sparsified embeddings with Independent Columns, which yield faster runtimes for matrix approximation and regression tasks.\n  In our analysis, we develop a new method which uses a decoupling argument together with the cumulant method for bounding the edge universality error of isotropic random matrices. To achieve near-optimal sparsity, we combine this general-purpose approach with new traces inequalities that leverage the specific structure of our subspace embedding construction."
      },
      {
        "id": "oai:arXiv.org:2411.17982v2",
        "title": "HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction",
        "link": "https://arxiv.org/abs/2411.17982",
        "author": "Wei Zhang, Qing Cheng, David Skuddis, Niclas Zeller, Daniel Cremers, Norbert Haala",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.17982v2 Announce Type: replace-cross \nAbstract: We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast and accurate monocular scene reconstruction using only RGB input. Existing Neural SLAM or 3DGS-based SLAM methods often trade off between rendering quality and geometry accuracy, our research demonstrates that both can be achieved simultaneously with RGB input alone. The key idea of our approach is to enhance the ability for geometry estimation by combining easy-to-obtain monocular priors with learning-based dense SLAM, and then using 3D Gaussian splatting as our core map representation to efficiently model the scene. Upon loop closure, our method ensures on-the-fly global consistency through efficient pose graph bundle adjustment and instant map updates by explicitly deforming the 3D Gaussian units based on anchored keyframe updates. Furthermore, we introduce a grid-based scale alignment strategy to maintain improved scale consistency in prior depths for finer depth details. Through extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate significant improvements over existing Neural SLAM methods and even surpass RGB-D-based methods in both reconstruction and rendering quality. The project page and source code will be made available at https://hi-slam2.github.io/."
      },
      {
        "id": "oai:arXiv.org:2412.01858v5",
        "title": "MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption",
        "link": "https://arxiv.org/abs/2412.01858",
        "author": "Siddhant Dutta, Nouhaila Innan, Sadok Ben Yahia, Muhammad Shafique, David Esteban Bernal Neira",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01858v5 Announce Type: replace-cross \nAbstract: The integration of fully homomorphic encryption (FHE) in federated learning (FL) has led to significant advances in data privacy. However, during the aggregation phase, it often results in performance degradation of the aggregated model, hindering the development of robust representational generalization. In this work, we propose a novel multimodal quantum federated learning framework that utilizes quantum computing to counteract the performance drop resulting from FHE. For the first time in FL, our framework combines a multimodal quantum mixture of experts (MQMoE) model with FHE, incorporating multimodal datasets for enriched representation and task-specific learning. Our MQMoE framework enhances performance on multimodal datasets and combined genomics and brain MRI scans, especially for underrepresented categories. Our results also demonstrate that the quantum-enhanced approach mitigates the performance degradation associated with FHE and improves classification accuracy across diverse datasets, validating the potential of quantum interventions in enhancing privacy in FL."
      },
      {
        "id": "oai:arXiv.org:2412.06832v2",
        "title": "SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering",
        "link": "https://arxiv.org/abs/2412.06832",
        "author": "Michael Iannelli, Sneha Kuchipudi, Vera Dvorak",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.06832v2 Announce Type: replace-cross \nAbstract: Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to generalize to new information by decoupling reasoning capabilities from static knowledge bases. Traditional RAG enhancements have explored vertical scaling-assigning subtasks to specialized modules-and horizontal scaling-replicating tasks across multiple agents-to improve performance. However, real-world applications impose diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements, involving trade-offs among objectives such as reducing cost, ensuring answer quality, and adhering to specific operational constraints.\n  In this work, we present a systems-oriented approach to multi-agent RAG tailored for real-world Question Answering (QA) applications. By integrating task-specific non-functional requirements-such as answer quality, cost, and latency-into the system, we enable dynamic reconfiguration to meet diverse SLAs. Our method maps these Service Level Objectives (SLOs) to system-level parameters, allowing the generation of optimal results within specified resource constraints.\n  We conduct a case study in the QA domain, demonstrating how dynamic re-orchestration of a multi-agent RAG system can effectively manage the trade-off between answer quality and cost. By adjusting the system based on query intent and operational conditions, we systematically balance performance and resource utilization. This approach allows the system to meet SLOs for various query types, showcasing its practicality for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2501.11280v3",
        "title": "Empirical Bayes Estimation for Lasso-Type Regularizers: Analysis of Automatic Relevance Determination",
        "link": "https://arxiv.org/abs/2501.11280",
        "author": "Tsukasa Yoshida, Kazuho Watanabe",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.11280v3 Announce Type: replace-cross \nAbstract: This paper focuses on linear regression models with non-conjugate sparsity-inducing regularizers such as lasso and group lasso. Although the empirical Bayes approach enables us to estimate the regularization parameter, little is known on the properties of the estimators. In particular, many aspects regarding the specific conditions under which the mechanism of automatic relevance determination (ARD) occurs remain unexplained. In this paper, we derive the empirical Bayes estimators for the group lasso regularized linear regression models with limited parameters. It is shown that the estimators diverge under a specific condition, giving rise to the ARD mechanism. We also prove that empirical Bayes methods can produce the ARD mechanism in general regularized linear regression models and clarify the conditions under which models such as ridge, lasso, and group lasso can do so."
      },
      {
        "id": "oai:arXiv.org:2501.14246v2",
        "title": "Adaptive Progressive Attention Graph Neural Network for EEG Emotion Recognition",
        "link": "https://arxiv.org/abs/2501.14246",
        "author": "Tianzhi Feng, Chennan Wu, Yi Niu, Fu Li, Yang Li, Boxun Fu, Zhifu Zhao, Xiaotian Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.14246v2 Announce Type: replace-cross \nAbstract: In recent years, numerous neuroscientific studies demonstrate that specific areas of the brain are connected to human emotional responses, with these regions exhibiting variability across individuals and emotional states. To fully leverage these neural patterns, we propose an Adaptive Progressive Attention Graph Neural Network (APAGNN), which dynamically captures the spatial relationships among brain regions during emotional processing. The APAGNN employs three specialized experts that progressively analyze brain topology. The first expert captures global brain patterns, the second focuses on region-specific features, and the third examines emotion-related channels. This hierarchical approach enables increasingly refined analysis of neural activity. Additionally, a weight generator integrates the outputs of all three experts, balancing their contributions to produce the final predictive label. Extensive experiments conducted on SEED, SEED-IV and MPED datasets indicate that our method enhances EEG emotion recognition performance, achieving superior results compared to baseline methods."
      },
      {
        "id": "oai:arXiv.org:2501.18045v2",
        "title": "From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors",
        "link": "https://arxiv.org/abs/2501.18045",
        "author": "Myra Cheng, Angela Y. Lee, Kristina Rapuano, Kate Niederhoffer, Alex Liebscher, Jeffrey Hancock",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.18045v2 Announce Type: replace-cross \nAbstract: How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p < 0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development."
      },
      {
        "id": "oai:arXiv.org:2501.18627v2",
        "title": "Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss",
        "link": "https://arxiv.org/abs/2501.18627",
        "author": "Ziyi Zhang, Nicolas Roussel, Thomas M\\\"uller, Tizian Zeltner, Merlin Nimier-David, Fabrice Rousselle, Wenzel Jakob",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.18627v2 Announce Type: replace-cross \nAbstract: We present a fast and simple technique to convert images into a radiance surface-based scene representation. Building on existing radiance volume reconstruction algorithms, we introduce a subtle yet impactful modification of the loss function requiring changes to only a few lines of code: instead of integrating the radiance field along rays and supervising the resulting images, we project the training images into the scene to directly supervise the spatio-directional radiance field.\n  The primary outcome of this change is the complete removal of alpha blending and ray marching from the image formation model, instead moving these steps into the loss computation. In addition to promoting convergence to surfaces, this formulation assigns explicit semantic meaning to 2D subsets of the radiance field, turning them into well-defined radiance surfaces. We finally extract a level set from this representation, which results in a high-quality radiance surface model.\n  Our method retains much of the speed and quality of the baseline algorithm. For instance, a suitably modified variant of Instant NGP maintains comparable computational efficiency, while achieving an average PSNR that is only 0.1 dB lower. Most importantly, our method generates explicit surfaces in place of an exponential volume, doing so with a level of simplicity not seen in prior work."
      },
      {
        "id": "oai:arXiv.org:2502.00114v2",
        "title": "Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach",
        "link": "https://arxiv.org/abs/2502.00114",
        "author": "Aaron Hao Tan, Angus Fung, Haitong Wang, Goldie Nejat",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00114v2 Announce Type: replace-cross \nAbstract: Hand-drawn maps can be used to convey navigation instructions between humans and robots in a natural and efficient manner. However, these maps can often contain inaccuracies such as scale distortions and missing landmarks which present challenges for mobile robot navigation. This paper introduces a novel Hand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained vision language models (VLMs) for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. HAM-Nav integrates a unique Selective Visual Association Prompting approach for topological map-based position estimation and navigation planning as well as a Predictive Navigation Plan Parser to infer missing landmarks. Extensive experiments were conducted in photorealistic simulated environments, using both wheeled and legged robots, demonstrating the effectiveness of HAM-Nav in terms of navigation success rates and Success weighted by Path Length. Furthermore, a user study in real-world environments highlighted the practical utility of hand-drawn maps for robot navigation as well as successful navigation outcomes compared against a non-hand-drawn map approach."
      },
      {
        "id": "oai:arXiv.org:2502.05074v2",
        "title": "Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in Linear Models",
        "link": "https://arxiv.org/abs/2502.05074",
        "author": "Alexander Atanasov, Blake Bordelon, Jacob A. Zavatone-Veth, Courtney Paquette, Cengiz Pehlevan",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05074v2 Announce Type: replace-cross \nAbstract: We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones."
      },
      {
        "id": "oai:arXiv.org:2502.12181v3",
        "title": "3D ReX: Causal Explanations in 3D Neuroimaging Classification",
        "link": "https://arxiv.org/abs/2502.12181",
        "author": "Melane Navaratnarajah, Sophie A. Martin, David A. Kelly, Nathan Blake, Hana Chockler",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12181v3 Announce Type: replace-cross \nAbstract: Explainability remains a significant problem for AI models in medical imaging, making it challenging for clinicians to trust AI-driven predictions. We introduce 3D ReX, the first causality-based post-hoc explainability tool for 3D models. 3D ReX uses the theory of actual causality to generate responsibility maps which highlight the regions most crucial to the model's decision. We test 3D ReX on a stroke detection model, providing insight into the spatial distribution of features relevant to stroke."
      },
      {
        "id": "oai:arXiv.org:2503.04821v2",
        "title": "RGB-Thermal Infrared Fusion for Robust Depth Estimation in Complex Environments",
        "link": "https://arxiv.org/abs/2503.04821",
        "author": "Zelin Meng, Takanori Fukao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04821v2 Announce Type: replace-cross \nAbstract: Depth estimation in complex real-world scenarios is a challenging task, especially when relying solely on a single modality such as visible light or thermal infrared (THR) imagery. This paper proposes a novel multimodal depth estimation model, RTFusion, which enhances depth estimation accuracy and robustness by integrating the complementary strengths of RGB and THR data. The RGB modality provides rich texture and color information, while the THR modality captures thermal patterns, ensuring stability under adverse lighting conditions such as extreme illumination. The model incorporates a unique fusion mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA) module for cross-modal feature alignment and the Edge Saliency Enhancement Module (ESEM) to improve edge detail preservation. Comprehensive experiments on the MS2 and ViViD++ datasets demonstrate that the proposed model consistently produces high-quality depth maps across various challenging environments, including nighttime, rainy, and high-glare conditions. The experimental results highlight the potential of the proposed method in applications requiring reliable depth estimation, such as autonomous driving, robotics, and augmented reality."
      },
      {
        "id": "oai:arXiv.org:2503.09089v2",
        "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
        "link": "https://arxiv.org/abs/2503.09089",
        "author": "Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.09089v2 Announce Type: replace-cross \nAbstract: Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent."
      },
      {
        "id": "oai:arXiv.org:2503.14453v3",
        "title": "Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud Offloading",
        "link": "https://arxiv.org/abs/2503.14453",
        "author": "Qiushuo Hou, Sangwoo Park, Matteo Zecchin, Yunlong Cai, Guanding Yu, Osvaldo Simeone",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.14453v3 Announce Type: replace-cross \nAbstract: Consider an edge computing setting in which a user submits queries for the solution of a linear system to an edge processor, which is subject to time-varying computing availability. The edge processor applies a probabilistic linear solver (PLS) so as to be able to respond to the user's query within the allotted time and computing budget. Feedback to the user is in the form of a set of plausible solutions. Due to model misspecification, the highest-probability-density (HPD) set obtained via a direct application of PLS does not come with coverage guarantees with respect to the true solution of the linear system. This work introduces a new method to calibrate the HPD sets produced by PLS with the aim of guaranteeing long-term coverage requirements. The proposed method, referred to as online conformal prediction-PLS (OCP-PLS), assumes sporadic feedback from cloud to edge. This enables the online calibration of uncertainty thresholds via online conformal prediction (OCP), an online optimization method previously studied in the context of prediction models. The validity of OCP-PLS is verified via experiments that bring insights into trade-offs between coverage, prediction set size, and cloud usage."
      },
      {
        "id": "oai:arXiv.org:2503.17427v2",
        "title": "3D variational autoencoder for fingerprinting microstructure volume elements",
        "link": "https://arxiv.org/abs/2503.17427",
        "author": "Michael D. White, Michael D. Atkinson, Adam J. Plowman, Pratheek Shanthraj",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17427v2 Announce Type: replace-cross \nAbstract: Microstructure quantification is an important step towards establishing structure-property relationships in materials. Machine learning-based image processing methods have been shown to outperform conventional image processing techniques and are increasingly applied to microstructure quantification tasks. In this work, we present a 3D variational autoencoder (VAE) for encoding microstructure volume elements (VEs) comprising voxelated crystallographic orientation data. Crystal symmetries in the orientation space are accounted for by mapping to the crystallographic fundamental zone as a preprocessing step, which allows for a continuous loss function to be used and improves the training convergence rate. The VAE is then used to encode a training set of VEs with an equiaxed polycrystalline microstructure with random texture. Accurate reconstructions are achieved with a relative average misorientation error of 9x10-3 on the test dataset, for a continuous latent space with dimension 256. We show that the model generalises well to microstructures with textures, grain sizes and aspect ratios outside the training distribution. Structure-property relationships are explored through using the training set of VEs as initial configurations in various crystal plasticity (CP) simulations. Microstructural fingerprints extracted from the VAE, which parameterise the VEs in a low-dimensional latent space, are stored alongside the volume-averaged stress response, at each strain increment, to uniaxial tensile deformation from CP simulations. This is then used to train a fully connected neural network mapping the input fingerprint to the resulting stress response, which acts as a surrogate model for the CP simulation. The fingerprint-based surrogate model is shown to accurately predict the microstructural dependence in the CP stress response, with a relative mean-squared error of 8.9x10-4 on unseen test data."
      },
      {
        "id": "oai:arXiv.org:2504.01790v2",
        "title": "SOLAQUA: SINTEF Ocean Large Aquaculture Robotics Dataset",
        "link": "https://arxiv.org/abs/2504.01790",
        "author": "Sveinung Johan Ohrem, Bent Haugal{\\o}kken, Eleni Kelasidi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01790v2 Announce Type: replace-cross \nAbstract: This paper presents a dataset gathered with an underwater robot in a sea-based aquaculture setting. Data was gathered from an operational fish farm and includes data from sensors such as the Waterlinked A50 DVL, the Nortek Nucleus 1000 DVL, Sonardyne Micro Ranger 2 USBL, Sonoptix Mulitbeam Sonar, mono and stereo cameras, and vehicle sensor data such as power usage, IMU, pressure, temperature, and more. Data acquisition is performed during both manual and autonomous traversal of the net pen structure. The collected vision data is of undamaged nets with some fish and marine growth presence, and it is expected that both the research community and the aquaculture industry will benefit greatly from the utilization of the proposed SOLAQUA dataset."
      },
      {
        "id": "oai:arXiv.org:2504.04421v3",
        "title": "Deliberate Planning of 3D Bin Packing on Packing Configuration Trees",
        "link": "https://arxiv.org/abs/2504.04421",
        "author": "Hang Zhao, Juzhan Xu, Kexiong Yu, Ruizhen Hu, Chenyang Zhu, Kai Xu",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04421v3 Announce Type: replace-cross \nAbstract: Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation. Existing methods usually solve the problem with limited resolution of spatial discretization, and/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation, packing configuration tree (PCT). PCT is a full-fledged description of the state and action space of bin packing which can support packing policy learning based on deep reinforcement learning (DRL). The size of the packing action space is proportional to the number of leaf nodes, making the DRL model easy to train and well-performing even with continuous solution space. We further discover the potential of PCT as tree-based planners in deliberately solving packing problems of industrial significance, including large-scale packing and different variations of BPP setting. A recursive packing method is proposed to decompose large-scale packing into smaller sub-trees while a spatial ensemble mechanism integrates local solutions into global. For different BPP variations with additional decision variables, such as lookahead, buffering, and offline packing, we propose a unified planning framework enabling out-of-the-box problem solving. Extensive evaluations demonstrate that our method outperforms existing online BPP baselines and is versatile in incorporating various practical constraints. The planning process excels across large-scale problems and diverse problem variations. We develop a real-world packing robot for industrial warehousing, with careful designs accounting for constrained placement and transportation stability. Our packing robot operates reliably and efficiently on unprotected pallets at 10 seconds per box. It achieves averagely 19 boxes per pallet with 57.4% space utilization for relatively large-size boxes."
      },
      {
        "id": "oai:arXiv.org:2504.10560v2",
        "title": "Molecular Learning Dynamics",
        "link": "https://arxiv.org/abs/2504.10560",
        "author": "Yaroslav Gusev, Vitaly Vanchurin",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10560v2 Announce Type: replace-cross \nAbstract: We apply the physics-learning duality to molecular systems by complementing the physical description of interacting particles with a dual learning description, where each particle is modeled as an agent minimizing a loss function. In the traditional physics framework, the equations of motion are derived from the Lagrangian function, while in the learning framework, the same equations emerge from learning dynamics driven by the agent loss function. The loss function depends on scalar quantities that describe invariant properties of all other agents or particles. To demonstrate this approach, we first infer the loss functions of oxygen and hydrogen directly from a dataset generated by the CP2K physics-based simulation of water molecules. We then employ the loss functions to develop a learning-based simulation of water molecules, which achieves comparable accuracy while being significantly more computationally efficient than standard physics-based simulations."
      },
      {
        "id": "oai:arXiv.org:2504.13962v2",
        "title": "A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data",
        "link": "https://arxiv.org/abs/2504.13962",
        "author": "Jose Manuel Aroca-Fernandez, Jose Francisco Diez-Pastor, Pedro Latorre-Carmona, Victor Elvira, Gustau Camps-Valls, Rodrigo Pascual, Cesar Garcia-Osorio",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13962v2 Announce Type: replace-cross \nAbstract: Soil organic carbon (SOC) is a key indicator of soil health, fertility, and carbon sequestration, making it essential for sustainable land management and climate change mitigation. However, large-scale SOC monitoring remains challenging due to spatial variability, temporal dynamics, and multiple influencing factors. We present WALGREEN, a platform that enhances SOC inference by overcoming limitations of current applications. Leveraging machine learning and diverse soil samples, WALGREEN generates predictive models using historical public and private data. Built on cloud-based technologies, it offers a user-friendly interface for researchers, policymakers, and land managers to access carbon data, analyze trends, and support evidence-based decision-making. Implemented in Python, Java, and JavaScript, WALGREEN integrates Google Earth Engine and Sentinel Copernicus via scripting, OpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims to advance soil science, promote sustainable agriculture, and drive critical ecosystem responses to climate change."
      },
      {
        "id": "oai:arXiv.org:2504.15388v2",
        "title": "Deep learning with missing data",
        "link": "https://arxiv.org/abs/2504.15388",
        "author": "Tianyi Ma, Tengyao Wang, Richard J. Samworth",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.15388v2 Announce Type: replace-cross \nAbstract: In the context of multivariate nonparametric regression with missing covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be applied in conjunction with any existing imputation technique. In addition to a neural network trained on the imputed data, PENNs pass the vectors of observation indicators through a second neural network to provide a compact representation. The outputs are then combined in a third neural network to produce final predictions. Our main theoretical result exploits an assumption that the observation patterns can be partitioned into cells on which the Bayes regression function behaves similarly, and belongs to a compositional H\\\"older class. It provides a finite-sample excess risk bound that holds for an arbitrary missingness mechanism, and in combination with a complementary minimax lower bound, demonstrates that our PENN estimator attains in typical cases the minimax rate of convergence as if the cells of the partition were known in advance, up to a poly-logarithmic factor in the sample size. Numerical experiments on simulated, semi-synthetic and real data confirm that the PENN estimator consistently improves, often dramatically, on standard neural networks without pattern embedding. Code to reproduce our experiments, as well as a tutorial on how to apply our method, is publicly available."
      },
      {
        "id": "oai:arXiv.org:2504.16062v2",
        "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration",
        "link": "https://arxiv.org/abs/2504.16062",
        "author": "Hardik Shah, Jiaxu Xing, Nico Messikommer, Boyang Sun, Marc Pollefeys, Davide Scaramuzza",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.16062v2 Announce Type: replace-cross \nAbstract: Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration."
      },
      {
        "id": "oai:arXiv.org:2504.16137v2",
        "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
        "link": "https://arxiv.org/abs/2504.16137",
        "author": "Jasper G\\\"otting, Pedro Medeiros, Jon G Sanders, Nathaniel Li, Long Phan, Karam Elabd, Lennart Justen, Dan Hendrycks, Seth Donoughe",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.16137v2 Announce Type: replace-cross \nAbstract: We present the Virology Capabilities Test (VCT), a large language model (LLM) benchmark that measures the capability to troubleshoot complex virology laboratory protocols. Constructed from the inputs of dozens of PhD-level expert virologists, VCT consists of $322$ multimodal questions covering fundamental, tacit, and visual knowledge that is essential for practical work in virology laboratories. VCT is difficult: expert virologists with access to the internet score an average of $22.1\\%$ on questions specifically in their sub-areas of expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$ accuracy, outperforming $94\\%$ of expert virologists even within their sub-areas of specialization. The ability to provide expert-level virology troubleshooting is inherently dual-use: it is useful for beneficial research, but it can also be misused. Therefore, the fact that publicly available models outperform virologists on VCT raises pressing governance considerations. We propose that the capability of LLMs to provide expert-level troubleshooting of dual-use virology work should be integrated into existing frameworks for handling dual-use technologies in the life sciences."
      },
      {
        "id": "oai:arXiv.org:2504.17146v2",
        "title": "Utilizing Dynamic Time Warping for Pandemic Surveillance: Understanding the Relationship between Google Trends Network Metrics and COVID-19 Incidences",
        "link": "https://arxiv.org/abs/2504.17146",
        "author": "Michael T. Lopez II, Cheska Elise Hung, Maria Regina Justina E. Estuar",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.17146v2 Announce Type: replace-cross \nAbstract: The premise of network statistics derived from Google Trends data to foresee COVID-19 disease progression is gaining momentum in infodemiology. This approach was applied in Metro Manila, National Capital Region, Philippines. Through dynamic time warping (DTW), the temporal alignment was quantified between network metrics and COVID-19 case trajectories, and systematically explored 320 parameter configurations including two network metrics (network density and clustering coefficient), two data preprocessing methods (Rescaling Daily Data and MSV), multiple thresholds, two correlation window sizes, and Sakoe-Chiba band constraints. Results from the Kruskal-Wallis tests revealed that five of the six parameters significantly influenced alignment quality, with the disease comparison type (active cases vs. confirmed cases) demonstrating the strongest effect. The optimal configuration, which is using the network density statistic with a Rescaling Daily Data transformation, a threshold of 0.8, a 15-day window, and a 50-day radius constraint, achieved a DTW score of 36.30. This indicated substantial temporal alignment with the COVID-19 confirmed cases data. The discoveries demonstrate that network metrics rooted from online search behavior can serve as complementary indicators for epidemic surveillance in urban locations like Metro Manila. This strategy leverages the Philippines' extensive online usage during the pandemic to provide potentially valuable early signals of disease spread, and offers a supplementary tool for public health monitoring in resource-limited situations."
      },
      {
        "id": "oai:arXiv.org:2504.18950v2",
        "title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness",
        "link": "https://arxiv.org/abs/2504.18950",
        "author": "Erfan Loweimi, Mengjie Qian, Kate Knill, Mark Gales",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18950v2 Announce Type: replace-cross \nAbstract: There is a growing abundance of publicly available or company-owned audio/video archives, highlighting the increasing importance of efficient access to desired content and information retrieval from these archives. This paper investigates the challenges, solutions, effectiveness, and robustness of speaker retrieval systems developed \"in the wild\" which involves addressing two primary challenges: extraction of task-relevant labels from limited metadata for system development and evaluation, as well as the unconstrained acoustic conditions encountered in the archive, ranging from quiet studios to adverse noisy environments. While we focus on the publicly-available BBC Rewind archive (spanning 1948 to 1979), our framework addresses the broader issue of speaker retrieval on extensive and possibly aged archives with no control over the content and acoustic conditions. Typically, these archives offer a brief and general file description, mostly inadequate for specific applications like speaker retrieval, and manual annotation of such large-scale archives is unfeasible. We explore various aspects of system development (e.g., speaker diarisation, embedding extraction, query selection) and analyse the challenges, possible solutions, and their functionality. To evaluate the performance, we conduct systematic experiments in both clean setup and against various distortions simulating real-world applications. Our findings demonstrate the effectiveness and robustness of the developed speaker retrieval systems, establishing the versatility and scalability of the proposed framework for a wide range of applications beyond the BBC Rewind corpus."
      },
      {
        "id": "oai:arXiv.org:2504.19062v2",
        "title": "Versatile Framework for Song Generation with Prompt-based Control",
        "link": "https://arxiv.org/abs/2504.19062",
        "author": "Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Ruiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang, Zhiqing Hong, Ziyue Jiang, Zhou Zhao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19062v2 Announce Type: replace-cross \nAbstract: Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://aaronz345.github.io/VersBandDemo."
      },
      {
        "id": "oai:arXiv.org:2504.19203v2",
        "title": "Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction",
        "link": "https://arxiv.org/abs/2504.19203",
        "author": "Ehsan Karami, Hamid Soltanian-Zadeh",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19203v2 Announce Type: replace-cross \nAbstract: Knee osteoarthritis (KOA) is a common joint disease that causes pain and mobility issues. While MRI-based deep learning models have demonstrated superior performance in predicting total knee replacement (TKR) and disease progression, their generalizability remains challenging, particularly when applied to imaging data from different sources. In this study, we have shown that replacing batch normalization with instance normalization, using data augmentation, and applying contrastive loss improves model generalization in a baseline deep learning model for knee osteoarthritis (KOA) prediction. We trained and evaluated our model using MRI data from the Osteoarthritis Initiative (OAI) database, considering sagittal fat-suppressed intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state (DESS) images as the target domain. The results demonstrate a statistically significant improvement in classification accuracy across both domains, with our approach outperforming the baseline model."
      },
      {
        "id": "oai:arXiv.org:2504.19323v2",
        "title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI",
        "link": "https://arxiv.org/abs/2504.19323",
        "author": "Hanchen Yang, Zishen Wan, Ritik Raj, Joongun Park, Ziwei Li, Ananda Samajdar, Arijit Raychowdhury, Tushar Krishna",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19323v2 Announce Type: replace-cross \nAbstract: Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural networks with symbolic reasoning to enhance the transparency, reasoning capabilities, and data efficiency of AI systems. Recent NSAI systems have gained traction due to their exceptional performance in reasoning tasks and human-AI collaborative scenarios. Despite these algorithmic advancements, executing NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains challenging, due to their heterogeneous computing kernels, high memory intensity, and unique memory access patterns. Moreover, current NSAI algorithms exhibit significant variation in operation types and scales, making them incompatible with existing ML accelerators. These challenges highlight the need for a versatile and flexible acceleration framework tailored to NSAI workloads. In this paper, we propose NSFlow, an FPGA-based acceleration framework designed to achieve high efficiency, scalability, and versatility across NSAI systems. NSFlow features a design architecture generator that identifies workload data dependencies and creates optimized dataflow architectures, as well as a reconfigurable array with flexible compute units, re-organizable memory, and mixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves 31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like systolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates enhanced scalability, with only 4x runtime increase when symbolic workloads scale by 150x. To the best of our knowledge, NSFlow is the first framework to enable real-time generalizable NSAI algorithms acceleration, demonstrating a promising solution for next-generation cognitive systems."
      },
      {
        "id": "oai:arXiv.org:2504.19489v2",
        "title": "How Cohesive Are Community Search Results on Online Social Networks?: An Experimental Evaluation",
        "link": "https://arxiv.org/abs/2504.19489",
        "author": "Yining Zhao, Sourav S Bhowmick, Nastassja L. Fischer, SH Annabel Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19489v2 Announce Type: replace-cross \nAbstract: Recently, numerous community search methods for large graphs have been proposed, at the core of which is defining and measuring cohesion. This paper experimentally evaluates the effectiveness of these community search algorithms w.r.t. cohesiveness in the context of online social networks. Social communities are formed and developed under the influence of group cohesion theory, which has been extensively studied in social psychology. However, current generic methods typically measure cohesiveness using structural or attribute-based approaches and overlook domain-specific concepts such as group cohesion. We introduce five novel psychology-informed cohesiveness measures, based on the concept of group cohesion from social psychology, and propose a novel framework called CHASE for evaluating eight representative CS algorithms w.r.t. these measures on online social networks. Our analysis reveals that there is no clear correlation between structural and psychological cohesiveness, and no algorithm effectively identifies psychologically cohesive communities in online social networks. This study provides new insights that could guide the development of future community search methods."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Wed, 30 Apr 2025 04:02:02 +0000",
      "published": "Wed, 30 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.20124v1",
        "title": "Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier",
        "link": "https://arxiv.org/abs/2504.20124",
        "author": "Abul Ehtesham, Saket Kumar, Aditi Singh, Tala Talaei Khoei",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20124v1 Announce Type: new \nAbstract: Early detection of asthma in children is crucial to prevent long-term respiratory complications and reduce emergency interventions. This work presents an AI-powered diagnostic pipeline that leverages Googles Health Acoustic Representations (HeAR) model to detect early signs of asthma from pediatric respiratory sounds. The SPRSound dataset, the first open-access collection of annotated respiratory sounds in children aged 1 month to 18 years, is used to extract 2-second audio segments labeled as wheeze, crackle, rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional representation using HeAR, a foundation model pretrained on 300 million health-related audio clips, including 100 million cough sounds. Multiple classifiers, including SVM, Random Forest, and MLP, are trained on these embeddings to distinguish between asthma-indicative and normal sounds. The system achieves over 91\\% accuracy, with strong performance on precision-recall metrics for positive cases. In addition to classification, learned embeddings are visualized using PCA, misclassifications are analyzed through waveform playback, and ROC and confusion matrix insights are provided. This method demonstrates that short, low-resource pediatric recordings, when powered by foundation audio models, can enable fast, noninvasive asthma screening. The approach is especially promising for digital diagnostics in remote or underserved healthcare settings."
      },
      {
        "id": "oai:arXiv.org:2504.20334v1",
        "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
        "link": "https://arxiv.org/abs/2504.20334",
        "author": "Yuzhe Liang, Wenzhe Liu, Chunyu Qiang, Zhikang Niu, Yushen Chen, Ziyang Ma, Wenxi Chen, Nan Li, Chen Zhang, Xie Chen",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20334v1 Announce Type: new \nAbstract: Flow matching has demonstrated strong generative capabilities and has become a core component in modern Text-to-Speech (TTS) systems. To ensure high-quality speech synthesis, Classifier-Free Guidance (CFG) is widely used during the inference of flow-matching-based TTS models. However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios. In this paper, we explore removing CFG from flow-matching-based TTS models to improve inference efficiency, while maintaining performance. Specifically, we reformulated the flow matching training target to directly approximate the CFG optimization trajectory. This training method eliminates the need for unconditional model evaluation and guided tuning during inference, effectively cutting the computational overhead in half. Furthermore, It can be seamlessly integrated with existing optimized sampling strategies. We validate our approach using the F5-TTS model on the LibriTTS dataset. Experimental results show that our method achieves a 9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving comparable speech quality. We will release the code and models to support reproducibility and foster further research in this area."
      },
      {
        "id": "oai:arXiv.org:2504.20447v1",
        "title": "APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech",
        "link": "https://arxiv.org/abs/2504.20447",
        "author": "Zhicheng Lian, Lizhi Wang, Hua Huang",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20447v1 Announce Type: new \nAbstract: Automatic speech quality assessment aims to quantify subjective human perception of speech through computational models to reduce the need for labor-consuming manual evaluations. While models based on deep learning have achieved progress in predicting mean opinion scores (MOS) to assess synthetic speech, the neglect of fundamental auditory perception mechanisms limits consistency with human judgments. To address this issue, we propose an auditory perception guided-MOS prediction model (APG-MOS) that synergistically integrates auditory modeling with semantic analysis to enhance consistency with human judgments. Specifically, we first design a perceptual module, grounded in biological auditory mechanisms, to simulate cochlear functions, which encodes acoustic signals into biologically aligned electrochemical representations. Secondly, we propose a residual vector quantization (RVQ)-based semantic distortion modeling method to quantify the degradation of speech quality at the semantic level. Finally, we design a residual cross-attention architecture, coupled with a progressive learning strategy, to enable multimodal fusion of encoded electrochemical signals and semantic representations. Experiments demonstrate that APG-MOS achieves superior performance on two primary benchmarks. Our code and checkpoint will be available on a public repository upon publication."
      },
      {
        "id": "oai:arXiv.org:2504.20625v1",
        "title": "DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models",
        "link": "https://arxiv.org/abs/2504.20625",
        "author": "Sagi Della Torre, Mirco Pezzoli, Fabio Antonacci, Sharon Gannot",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20625v1 Announce Type: new \nAbstract: Room Impulse Responses (RIRs) characterize acoustic environments and are crucial in multiple audio signal processing tasks. High-quality RIR estimates drive applications such as virtual microphones, sound source localization, augmented reality, and data augmentation. However, obtaining RIR measurements with high spatial resolution is resource-intensive, making it impractical for large spaces or when dense sampling is required. This research addresses the challenge of estimating RIRs at unmeasured locations within a room using Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the analogy between RIR matrices and image inpainting, transforming RIR data into a format suitable for diffusion-based reconstruction.\n  Using simulated RIR data based on the image method, we demonstrate our approach's effectiveness on microphone arrays of different curvatures, from linear to semi-circular. Our method successfully reconstructs missing RIRs, even in large gaps between microphones. Under these conditions, it achieves accurate reconstruction, significantly outperforming baseline Spline Cubic Interpolation in terms of Normalized Mean Square Error and Cosine Distance between actual and interpolated RIRs.\n  This research highlights the potential of using generative models for effective RIR interpolation, paving the way for generating additional data from limited real-world measurements."
      },
      {
        "id": "oai:arXiv.org:2504.20630v1",
        "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
        "link": "https://arxiv.org/abs/2504.20630",
        "author": "Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20630v1 Announce Type: new \nAbstract: Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo."
      },
      {
        "id": "oai:arXiv.org:2504.20776v1",
        "title": "ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe",
        "link": "https://arxiv.org/abs/2504.20776",
        "author": "David Funosas, Elodie Massol, Yves Bas, Svenja Schmidt, Dominik Arend, Alexander Gebhard, Luc Barbaro, Sebastian K\\\"onig, Rafael Carbonell Font, David Sannier, Fernand Deroussen, J\\'er\\^ome Sueur, Christian Roesti, Tomi Trilar, Wolfgang Forstmeier, Lucas Roger, Elo\\\"isa Matheu, Piotr Guzik, Julien Barataud, Laurent Pelozuelo, St\\'ephane Puissant, Sandra Mueller, Bj\\\"orn Schuller, Jose M. Montoya, Andreas Triantafyllopoulos, Maxime Cauchoix",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20776v1 Announce Type: new \nAbstract: Currently available tools for the automated acoustic recognition of European insects in natural soundscapes are limited in scope. Large and ecologically heterogeneous acoustic datasets are currently needed for these algorithms to cross-contextually recognize the subtle and complex acoustic signatures produced by each species, thus making the availability of such datasets a key requisite for their development. Here we present ECOSoundSet (European Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when including subspecies) present in North, Central, and temperate Western Europe (Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland, Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly through targeted fieldwork in South France and Catalonia and partly through contributions from various European entomologists. The dataset is composed of a combination of coarsely labeled recordings, for which we can only infer the presence, at some point, of their target species (weak labeling), and finely annotated recordings, for which we know the specific time and frequency range of each insect sound present in the recording (strong labeling). We also provide a train/validation/test split of the strongly labeled recordings, with respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate their incorporation in the training and evaluation of deep learning algorithms. This dataset could serve as a meaningful complement to recordings already available online for the training of deep learning algorithms for the acoustic classification of orthopterans and cicadas in North, Central, and temperate Western Europe."
      },
      {
        "id": "oai:arXiv.org:2504.20835v1",
        "title": "Enhancing Non-Core Language Instruction-Following in Speech LLMs via Semi-Implicit Cross-Lingual CoT Reasoning",
        "link": "https://arxiv.org/abs/2504.20835",
        "author": "Hongfei Xue, Yufeng Tang, Hexin Liu, Jun Zhang, Xuelong Geng, Lei Xie",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20835v1 Announce Type: new \nAbstract: Large language models have been extended to the speech domain, leading to the development of speech large language models (SLLMs). While existing SLLMs demonstrate strong performance in speech instruction-following for core languages (e.g., English), they often struggle with non-core languages due to the scarcity of paired speech-text data and limited multilingual semantic reasoning capabilities. To address this, we propose the semi-implicit Cross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates speech-to-text translation into the reasoning process of SLLMs. The XS-CoT generates four types of tokens: instruction and response tokens in both core and non-core languages, enabling cross-lingual transfer of reasoning capabilities. To mitigate inference latency in generating target non-core response tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which progressively compresses the first three types of intermediate reasoning tokens while retaining global reasoning logic during training. By leveraging the robust reasoning capabilities of the core language, XS-CoT improves responses for non-core languages by up to 45\\% in GPT-4 score when compared to direct supervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN. Moreover, the semi-implicit XS-CoT reduces token delay by more than 50\\% with a slight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount of high-quality training data for non-core languages by leveraging the reasoning capabilities of core languages. To support training, we also develop a data pipeline and open-source speech instruction-following datasets in Japanese, German, and French."
      },
      {
        "id": "oai:arXiv.org:2504.20923v1",
        "title": "End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation",
        "link": "https://arxiv.org/abs/2504.20923",
        "author": "Andrea Di Pierno (IMT School of Advanced Studies, Lucca, Italy, Department of Mathematics and Computer Science, University of Catania, Italy), Luca Guarnera (Department of Mathematics and Computer Science, University of Catania, Italy), Dario Allegra (Department of Mathematics and Computer Science, University of Catania, Italy), Sebastiano Battiato (Department of Mathematics and Computer Science, University of Catania, Italy)",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20923v1 Announce Type: new \nAbstract: Audio deepfakes represent a growing threat to digital security and trust, leveraging advanced generative models to produce synthetic speech that closely mimics real human voices. Detecting such manipulations is especially challenging under open-world conditions, where spoofing methods encountered during testing may differ from those seen during training. In this work, we propose an end-to-end deep learning framework for audio deepfake detection that operates directly on raw waveforms. Our model, RawNetLite, is a lightweight convolutional-recurrent architecture designed to capture both spectral and temporal features without handcrafted preprocessing. To enhance robustness, we introduce a training strategy that combines data from multiple domains and adopts Focal Loss to emphasize difficult or ambiguous samples. We further demonstrate that incorporating codec-based manipulations and applying waveform-level audio augmentations (e.g., pitch shifting, noise, and time stretching) leads to significant generalization improvements under realistic acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging out-of-distribution test set (AVSpoof2021 + CodecFake). These findings highlight the importance of diverse training data, tailored objective functions and audio augmentations in building resilient and generalizable audio forgery detectors. Code and pretrained models are available at https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/."
      },
      {
        "id": "oai:arXiv.org:2504.20532v1",
        "title": "TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution",
        "link": "https://arxiv.org/abs/2504.20532",
        "author": "Yue Li, Weizhi Liu, Dongdong Lin",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20532v1 Announce Type: cross \nAbstract: The emergence of diffusion models has facilitated the generation of speech with reinforced fidelity and naturalness. While deepfake detection technologies have manifested the ability to identify AI-generated content, their efficacy decreases as generative models become increasingly sophisticated. Furthermore, current research in the field has not adequately addressed the necessity for robust watermarking to safeguard the intellectual property rights associated with synthetic speech and generative models. To remedy this deficiency, we propose a \\textbf{ro}bust generative \\textbf{s}peech wat\\textbf{e}rmarking method (TriniMark) for authenticating the generated content and safeguarding the copyrights by enabling the traceability of the diffusion model. We first design a structure-lightweight watermark encoder that embeds watermarks into the time-domain features of speech and reconstructs the waveform directly. A temporal-aware gated convolutional network is meticulously designed in the watermark decoder for bit-wise watermark recovery. Subsequently, the waveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion model, which leverages the transferability of watermarks and enables the diffusion model to incorporate watermark knowledge effectively. When an attacker trains a surrogate model using the outputs of the target model, the embedded watermark can still be learned by the surrogate model and correctly extracted. Comparative experiments with state-of-the-art methods demonstrate the superior robustness of our method, particularly in countering compound attacks."
      },
      {
        "id": "oai:arXiv.org:2504.20678v1",
        "title": "Non-native Children's Automatic Speech Assessment Challenge (NOCASA)",
        "link": "https://arxiv.org/abs/2504.20678",
        "author": "Yaroslav Getman, Tam\\'as Gr\\'osz, Mikko Kurimo, Giampiero Salvi",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20678v1 Announce Type: cross \nAbstract: This paper presents the \"Non-native Children's Automatic Speech Assessment\" (NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA challenges participants to develop new systems that can assess single-word pronunciations of young second language (L2) learners as part of a gamified pronunciation training app. To achieve this, several issues must be addressed, most notably the limited nature of available training data and the highly unbalanced distribution among the pronunciation level categories. To expedite the development, we provide a pseudo-anonymized training data (TeflonNorL2), containing 10,334 recordings from 44 speakers attempting to pronounce 205 distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that should be given in the game). In addition to the data, two already trained systems are released as official baselines: an SVM classifier trained on the ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%."
      },
      {
        "id": "oai:arXiv.org:2504.20844v1",
        "title": "Effect of Avatar Head Movement on Communication Behaviour, Experience of Presence and Conversation Success in Triadic Conversations",
        "link": "https://arxiv.org/abs/2504.20844",
        "author": "Angelika Kothe, Volker Hohmann, Giso Grimm",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20844v1 Announce Type: cross \nAbstract: Interactive communication in virtual reality can be used in experimental paradigms to increase the ecological validity of hearing device evaluations. This requires the virtual environment to elicit natural communication behaviour in listeners. This study evaluates the effect of virtual animated characters' head movements on participants' communication behaviour and experience.\n  Triadic conversations were conducted between a test participant and two confederates. To facilitate the manipulation of head movements, the conversation was conducted in telepresence using a system that transmitted audio, head movement data and video with low delay. The confederates were represented by virtual animated characters (avatars) with different levels of animation: Static heads, automated head movement animations based on speech level onsets, and animated head movements based on the transmitted head movements of the interlocutors. A condition was also included in which the videos of the interlocutors' heads were embedded in the visual scene.\n  The results show significant effects of animation level on the participants' speech and head movement behaviour as recorded by physical sensors, as well as on the subjective sense of presence and the success of the conversation. The largest effects were found for the range of head orientation during speech and the perceived realism of avatars. Participants reported that they were spoken to in a more helpful way when the avatars showed head movements transmitted from the interlocutors than when the avatars' heads were static.\n  We therefore conclude that the representation of interlocutors must include sufficiently realistic head movements in order to elicit natural communication behaviour."
      },
      {
        "id": "oai:arXiv.org:2405.15863v3",
        "title": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation",
        "link": "https://arxiv.org/abs/2405.15863",
        "author": "Chang Li, Ruoyu Wang, Lijuan Liu, Jun Du, Yixuan Sun, Zilu Guo, Zhenrong Zhang, Yuan Jiang, Jianqing Gao, Feng Ma",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15863v3 Announce Type: replace \nAbstract: Text-to-music (TTM) generation, which converts textual descriptions into audio, opens up innovative avenues for multimedia creation. Achieving high quality and diversity in this process demands extensive, high-quality data, which are often scarce in available datasets. Most open-source datasets frequently suffer from issues like low-quality waveforms and low text-audio consistency, hindering the advancement of music generation models. To address these challenges, we propose a novel quality-aware training paradigm for generating high-quality, high-musicality music from large-scale, quality-imbalanced datasets. Additionally, by leveraging unique properties in the latent space of musical signals, we adapt and implement a masked diffusion transformer (MDT) model for the TTM task, showcasing its capacity for quality control and enhanced musicality. Furthermore, we introduce a three-stage caption refinement approach to address low-quality captions' issue. Experiments show state-of-the-art (SOTA) performance on benchmark datasets including MusicCaps and the Song-Describer Dataset with both objective and subjective metrics. Demo audio samples are available at https://qa-mdt.github.io/, code and pretrained checkpoints are open-sourced at https://github.com/ivcylc/OpenMusic."
      },
      {
        "id": "oai:arXiv.org:2408.03581v3",
        "title": "Design and Analysis of Binaural Signal Matching with Arbitrary Microphone Arrays and Listener Head Rotations",
        "link": "https://arxiv.org/abs/2408.03581",
        "author": "Lior Madmoni, Zamir Ben-Hur, Jacob Donley, Vladimir Tourbabin, Boaz Rafaely",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.03581v3 Announce Type: replace \nAbstract: Binaural reproduction is rapidly becoming a topic of great interest in the research community, especially with the surge of new and popular devices, such as virtual reality headsets, smart glasses, and head-tracked headphones. In order to immerse the listener in a virtual or remote environment with such devices, it is essential to generate realistic and accurate binaural signals. This is challenging, especially since the microphone arrays mounted on these devices are typically composed of an arbitrarily-arranged small number of microphones, which impedes the use of standard audio formats like Ambisonics, and provides limited spatial resolution. The binaural signal matching (BSM) method was developed recently to overcome these challenges. While it produced binaural signals with low error using relatively simple arrays, its performance degraded significantly when head rotation was introduced. This paper aims to develop the BSM method further and overcome its limitations. For this purpose, the method is first analyzed in detail, and a design framework that guarantees accurate binaural reproduction for relatively complex acoustic environments is presented. Next, it is shown that the BSM accuracy may significantly degrade at high frequencies, and thus, a perceptually motivated extension to the method is proposed, based on a magnitude least-squares (MagLS) formulation. These insights and developments are then analyzed with the help of an extensive simulation study of a simple six-microphone semi-circular array. It is further shown that the BSM-MagLS method can be very useful in compensating for head rotations with this array. Finally, a listening experiment is conducted with a four-microphone array on a pair of glasses in a reverberant speech environment and including head rotations, where it is shown that BSM-MagLS can indeed produce binaural signals with a high perceived quality."
      },
      {
        "id": "oai:arXiv.org:2504.08524v2",
        "title": "Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion",
        "link": "https://arxiv.org/abs/2504.08524",
        "author": "Na Li, Chuke Wang, Yu Gu, Zhifeng Li",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08524v2 Announce Type: replace \nAbstract: Voice conversion (VC) transforms source speech into a target voice by preserving the content. However, timbre information from the source speaker is inherently embedded in the content representations, causing significant timbre leakage and reducing similarity to the target speaker. To address this, we introduce a residual block to a content extractor. The residual block consists of two weighted branches: 1) universal semantic dictionary based Content Feature Re-expression (CFR) module, supplying timbre-free content representation. 2) skip connection to the original content layer, providing complementary fine-grained information. In the CFR module, each dictionary entry in the universal semantic dictionary represents a phoneme class, computed statistically using speech from multiple speakers, creating a stable, speaker-independent semantic set. We introduce a CFR method to obtain timbre-free content representations by expressing each content frame as a weighted linear combination of dictionary entries using corresponding phoneme posteriors as weights. Extensive experiments across various VC frameworks demonstrate that our approach effectively mitigates timbre leakage and significantly improves similarity to the target speaker."
      },
      {
        "id": "oai:arXiv.org:2504.18950v2",
        "title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness",
        "link": "https://arxiv.org/abs/2504.18950",
        "author": "Erfan Loweimi, Mengjie Qian, Kate Knill, Mark Gales",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18950v2 Announce Type: replace \nAbstract: There is a growing abundance of publicly available or company-owned audio/video archives, highlighting the increasing importance of efficient access to desired content and information retrieval from these archives. This paper investigates the challenges, solutions, effectiveness, and robustness of speaker retrieval systems developed \"in the wild\" which involves addressing two primary challenges: extraction of task-relevant labels from limited metadata for system development and evaluation, as well as the unconstrained acoustic conditions encountered in the archive, ranging from quiet studios to adverse noisy environments. While we focus on the publicly-available BBC Rewind archive (spanning 1948 to 1979), our framework addresses the broader issue of speaker retrieval on extensive and possibly aged archives with no control over the content and acoustic conditions. Typically, these archives offer a brief and general file description, mostly inadequate for specific applications like speaker retrieval, and manual annotation of such large-scale archives is unfeasible. We explore various aspects of system development (e.g., speaker diarisation, embedding extraction, query selection) and analyse the challenges, possible solutions, and their functionality. To evaluate the performance, we conduct systematic experiments in both clean setup and against various distortions simulating real-world applications. Our findings demonstrate the effectiveness and robustness of the developed speaker retrieval systems, establishing the versatility and scalability of the proposed framework for a wide range of applications beyond the BBC Rewind corpus."
      },
      {
        "id": "oai:arXiv.org:2504.19062v2",
        "title": "Versatile Framework for Song Generation with Prompt-based Control",
        "link": "https://arxiv.org/abs/2504.19062",
        "author": "Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Ruiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang, Zhiqing Hong, Ziyue Jiang, Zhou Zhao",
        "published": "Wed, 30 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19062v2 Announce Type: replace \nAbstract: Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://aaronz345.github.io/VersBandDemo."
      }
    ]
  }
}