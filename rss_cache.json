{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Mon, 07 Apr 2025 04:09:53 +0000",
      "published": "Mon, 07 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.02853v1",
        "title": "Mapping Technological Futures: Anticipatory Discourse Through Text Mining",
        "link": "https://arxiv.org/abs/2504.02853",
        "author": "Maciej Skorski, Alina Landowska, Krzysztof Rajda",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02853v1 Announce Type: new \nAbstract: The volatility and unpredictability of emerging technologies, such as artificial intelligence (AI), generate significant uncertainty, which is widely discussed on social media. This study examines anticipatory discourse surrounding technological futures by analysing 1.5 million posts from 400 key opinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using advanced text mining techniques, including BERTopic modelling, sentiment, emotion, and attitude analyses, the research identifies 100 distinct topics reflecting anticipated tech-driven futures. Our findings emphasize the dual role of KOLs in framing \\textit{present futures} -- optimistic visions of transformative technologies like AI and IoT -- and influencing \\textit{future presents}, where these projections shape contemporary societal and geopolitical debates. Positive emotions such as Hope dominate, outweighing Anxiety, particularly in topics like ``Machine Learning, Data Science, and Deep Learning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and Trump People'' elicit \\textit{Anxiety}. By framing technologies as solutions to societal challenges, KOLs act as mediators of societal narratives, bridging imagined futures and current realities. These insights underscore their pivotal role in directing public attention with emerging technologies during periods of heightened uncertainty, advancing our understanding of anticipatory discourse in technology-mediated contexts."
      },
      {
        "id": "oai:arXiv.org:2504.02858v1",
        "title": "Optimizing Humor Generation in Large Language Models: Temperature Configurations and Architectural Trade-offs",
        "link": "https://arxiv.org/abs/2504.02858",
        "author": "Evgenii Evstafev",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02858v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate increasing capabilities in creative text generation, yet systematic evaluations of their humor production remain underexplored. This study presents a comprehensive analysis of 13 state-of-the-art LLMs across five architectural families, evaluating their performance in generating technically relevant humor for software developers. Through a full factorial design testing 715 unique configurations of temperature settings and prompt variations, we assess model outputs using five weighted criteria: humor quality, domain relevance, concept originality, tone precision, and delivery efficiency. Our methodology employs rigorous statistical analysis including ANOVA, correlation studies, and quadratic regression to identify optimal configurations and architectural influences. Results reveal significant performance variations across models, with certain architectures achieving 21.8% superiority over baseline systems. Temperature sensitivity analysis demonstrates that 73% of models achieve peak performance at lower stochasticity settings (<= 0.5), though optimal ranges vary substantially by architecture. We identify distinct model clusters: compact high-performers maintaining efficiency-quality balance versus verbose specialists requiring longer outputs for marginal gains. Statistical validation confirms model architecture explains 38.7% of performance variance, with significant correlations between humor quality and concept originality. The study establishes practical guidelines for model selection and configuration, demonstrating how temperature adjustments and architectural considerations impact humor generation effectiveness. These findings advance understanding of LLM capabilities in creative technical writing and provide empirically validated configuration strategies for developers implementing humor-generation systems."
      },
      {
        "id": "oai:arXiv.org:2504.02860v1",
        "title": "Computer Vision and Deep Learning for 4D Augmented Reality",
        "link": "https://arxiv.org/abs/2504.02860",
        "author": "Karthik Shivashankar",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02860v1 Announce Type: new \nAbstract: The prospect of 4D video in Extended Reality (XR) platform is huge and exciting, it opens a whole new way of human computer interaction and the way we perceive the reality and consume multimedia. In this thesis, we have shown that feasibility of rendering 4D video in Microsoft mixed reality platform. This enables us to port any 3D performance capture from CVSSP into XR product like the HoloLens device with relative ease. However, if the 3D model is too complex and is made up of millions of vertices, the data bandwidth required to port the model is a severe limitation with the current hardware and communication system. Therefore, in this project we have also developed a compact representation of both shape and appearance of the 4d video sequence using deep learning models to effectively learn the compact representation of 4D video sequence and reconstruct it without affecting the shape and appearance of the video sequence."
      },
      {
        "id": "oai:arXiv.org:2504.02862v1",
        "title": "Towards Understanding How Knowledge Evolves in Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2504.02862",
        "author": "Sudong Wang, Yunjian Zhang, Yao Zhu, Jianing Li, Zizhe Wang, Yanwei Liu, Xiangyang Ji",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02862v1 Announce Type: new \nAbstract: Large Vision-Language Models (LVLMs) are gradually becoming the foundation for many artificial intelligence applications. However, understanding their internal working mechanisms has continued to puzzle researchers, which in turn limits the further enhancement of their capabilities. In this paper, we seek to investigate how multimodal knowledge evolves and eventually induces natural languages in LVLMs. We design a series of novel strategies for analyzing internal knowledge within LVLMs, and delve into the evolution of multimodal knowledge from three levels, including single token probabilities, token probability distributions, and feature encodings. In this process, we identify two key nodes in knowledge evolution: the critical layers and the mutation layers, dividing the evolution process into three stages: rapid evolution, stabilization, and mutation. Our research is the first to reveal the trajectory of knowledge evolution in LVLMs, providing a fresh perspective for understanding their underlying mechanisms. Our codes are available at https://github.com/XIAO4579/Vlm-interpretability."
      },
      {
        "id": "oai:arXiv.org:2504.02863v1",
        "title": "GS_DravidianLangTech@2025: Women Targeted Abusive Texts Detection on Social Media",
        "link": "https://arxiv.org/abs/2504.02863",
        "author": "Girma Yohannis Bade, Zahra Ahani, Olga Kolesnikova, Jos\\'e Luis Oropeza, Grigori Sidorov",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02863v1 Announce Type: new \nAbstract: The increasing misuse of social media has become a concern; however, technological solutions are being developed to moderate its content effectively. This paper focuses on detecting abusive texts targeting women on social media platforms. Abusive speech refers to communication intended to harm or incite hatred against vulnerable individuals or groups. Specifically, this study aims to identify abusive language directed toward women. To achieve this, we utilized logistic regression and BERT as base models to train datasets sourced from DravidianLangTech@2025 for Tamil and Malayalam languages. The models were evaluated on test datasets, resulting in a 0.729 macro F1 score for BERT and 0.6279 for logistic regression in Tamil and Malayalam, respectively."
      },
      {
        "id": "oai:arXiv.org:2504.02864v1",
        "title": "The Material Contracts Corpus",
        "link": "https://arxiv.org/abs/2504.02864",
        "author": "Peter Adelson, Julian Nyarko",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02864v1 Announce Type: new \nAbstract: This paper introduces the Material Contracts Corpus (MCC), a publicly available dataset comprising over one million contracts filed by public companies with the U.S. Securities and Exchange Commission (SEC) between 2000 and 2023. The MCC facilitates empirical research on contract design and legal language, and supports the development of AI-based legal tools. Contracts in the corpus are categorized by agreement type and linked to specific parties using machine learning and natural language processing techniques, including a fine-tuned LLaMA-2 model for contract classification. The MCC further provides metadata such as filing form, document format, and amendment status. We document trends in contractual language, length, and complexity over time, and highlight the dominance of employment and security agreements in SEC filings. This resource is available for bulk download and online access at https://mcc.law.stanford.edu."
      },
      {
        "id": "oai:arXiv.org:2504.02865v1",
        "title": "The Illusionist's Prompt: Exposing the Factual Vulnerabilities of Large Language Models with Linguistic Nuances",
        "link": "https://arxiv.org/abs/2504.02865",
        "author": "Yining Wang, Yuquan Wang, Xi Li, Mi Zhang, Geng Hong, Min Yang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02865v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) continue to advance, they are increasingly relied upon as real-time sources of information by non-expert users. To ensure the factuality of the information they provide, much research has focused on mitigating hallucinations in LLM responses, but only in the context of formal user queries, rather than maliciously crafted ones. In this study, we introduce The Illusionist's Prompt, a novel hallucination attack that incorporates linguistic nuances into adversarial queries, challenging the factual accuracy of LLMs against five types of fact-enhancing strategies. Our attack automatically generates highly transferrable illusory prompts to induce internal factual errors, all while preserving user intent and semantics. Extensive experiments confirm the effectiveness of our attack in compromising black-box LLMs, including commercial APIs like GPT-4o and Gemini-2.0, even with various defensive mechanisms."
      },
      {
        "id": "oai:arXiv.org:2504.02866v1",
        "title": "OpenFACADES: An Open Framework for Architectural Caption and Attribute Data Enrichment via Street View Imagery",
        "link": "https://arxiv.org/abs/2504.02866",
        "author": "Xiucheng Liang, Jinheng Xie, Tianhong Zhao, Rudi Stouffs, Filip Biljecki",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02866v1 Announce Type: new \nAbstract: Building properties, such as height, usage, and material composition, play a crucial role in spatial data infrastructures, supporting applications such as energy simulation, risk assessment, and environmental modeling. Despite their importance, comprehensive and high-quality building attribute data remain scarce in many urban areas. Recent advances have enabled the extraction and tagging of objective building attributes using remote sensing and street-level imagery. However, establishing a method and pipeline that integrates diverse open datasets, acquires holistic building imagery at scale, and infers comprehensive building attributes remains a significant challenge. Among the first, this study bridges the gaps by introducing OpenFACADES, an open framework that leverages multimodal crowdsourced data to enrich building profiles with both objective attributes and semantic descriptors through multimodal large language models. Our methodology proceeds in three major steps. First, we integrate street-level image metadata from Mapillary with OpenStreetMap geometries via isovist analysis, effectively identifying images that provide suitable vantage points for observing target buildings. Second, we automate the detection of building facades in panoramic imagery and tailor a reprojection approach to convert objects into holistic perspective views that approximate real-world observation. Third, we introduce an innovative approach that harnesses and systematically investigates the capabilities of open-source large vision-language models (VLMs) for multi-attribute prediction and open-vocabulary captioning in building-level analytics, leveraging a globally sourced dataset of 30,180 labeled images from seven cities. Evaluation shows that fine-tuned VLM excel in multi-attribute inference, outperforming single-attribute computer vision models and zero-shot ChatGPT-4o."
      },
      {
        "id": "oai:arXiv.org:2504.02867v1",
        "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications",
        "link": "https://arxiv.org/abs/2504.02867",
        "author": "Hongliu Cao, Ilias Driouich, Robin Singh, Eoin Thomas",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02867v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated impressive performance across diverse domains, yet they still encounter challenges such as insufficient domain-specific knowledge, biases, and hallucinations. This underscores the need for robust evaluation methodologies to accurately assess LLM-based applications. Traditional evaluation methods, which rely on word overlap or text embeddings, are inadequate for capturing the nuanced semantic information necessary to evaluate dynamic, open-ended text generation. Recent research has explored leveraging LLMs to mimic human reasoning and decision-making processes for evaluation purposes known as LLM-as-a-judge framework. However, these existing frameworks have two significant limitations. First, they lack the flexibility to adapt to different text styles, including various answer and ground truth styles, thereby reducing their generalization performance. Second, the evaluation scores produced by these frameworks are often skewed and hard to interpret, showing a low correlation with human judgment. To address these challenges, we propose a novel dynamic multi-agent system that automatically designs personalized LLM judges for various natural language generation applications. This system iteratively refines evaluation prompts and balances the trade-off between the adaptive requirements of downstream tasks and the alignment with human perception. Our experimental results show that the proposed multi-agent LLM Judge framework not only enhances evaluation accuracy compared to existing methods but also produces evaluation scores that better align with human perception."
      },
      {
        "id": "oai:arXiv.org:2504.02869v1",
        "title": "A Dataset of the Representatives Elected in France During the Fifth Republic",
        "link": "https://arxiv.org/abs/2504.02869",
        "author": "No\\'emie F\\'evrat (FR 3621, JPEG), Vincent Labatut (LIA), \\'Emilie Volpi (FR 3621), Guillaume Marrel (JPEG)",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02869v1 Announce Type: new \nAbstract: The electoral system is a cornerstone of democracy, shaping the structure of political competition, representation, and accountability. In the case of France, it is difficult to access data describing elected representatives, though, as they are scattered across a number of sources, including public institutions, but also academic and individual efforts. This article presents a unified relational database that aims at tackling this issue by gathering information regarding representatives elected in France over the whole Fifth Republic (1958-present). This database constitutes an unprecedented resource for analyzing the evolution of political representation in France, exploring trends in party system dynamics, gender equality, and the professionalization of politics. By providing a longitudinal view of French elected representatives, the database facilitates research on the institutional stability of the Fifth Republic, offering insights into the factors of political change."
      },
      {
        "id": "oai:arXiv.org:2504.02870v1",
        "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening",
        "link": "https://arxiv.org/abs/2504.02870",
        "author": "Frank P. -W. Lo, Jianing Qiu, Zeyu Wang, Haibao Yu, Yeming Chen, Gao Zhang, Benny Lo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02870v1 Announce Type: new \nAbstract: Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows."
      },
      {
        "id": "oai:arXiv.org:2504.02871v1",
        "title": "Synthesized Annotation Guidelines are Knowledge-Lite Boosters for Clinical Information Extraction",
        "link": "https://arxiv.org/abs/2504.02871",
        "author": "Enshuo Hsu, Martin Ugbala, Krishna Kumar Kookal, Zouaidi Kawtar, Nicholas L. Rider, Muhammad F. Walji, Kirk Roberts",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02871v1 Announce Type: new \nAbstract: Generative information extraction using large language models, particularly through few-shot learning, has become a popular method. Recent studies indicate that providing a detailed, human-readable guideline-similar to the annotation guidelines traditionally used for training human annotators can significantly improve performance. However, constructing these guidelines is both labor- and knowledge-intensive. Additionally, the definitions are often tailored to meet specific needs, making them highly task-specific and often non-reusable. Handling these subtle differences requires considerable effort and attention to detail. In this study, we propose a self-improving method that harvests the knowledge summarization and text generation capacity of LLMs to synthesize annotation guidelines while requiring virtually no human input. Our zero-shot experiments on the clinical named entity recognition benchmarks, 2012 i2b2 EVENT, 2012 i2b2 TIMEX, 2014 i2b2, and 2018 n2c2 showed 25.86%, 4.36%, 0.20%, and 7.75% improvements in strict F1 scores from the no-guideline baseline. The LLM-synthesized guidelines showed equivalent or better performance compared to human-written guidelines by 1.15% to 4.14% in most tasks. In conclusion, this study proposes a novel LLM self-improving method that requires minimal knowledge and human input and is applicable to multiple biomedical domains."
      },
      {
        "id": "oai:arXiv.org:2504.02872v1",
        "title": "Scraping the Shadows: Deep Learning Breakthroughs in Dark Web Intelligence",
        "link": "https://arxiv.org/abs/2504.02872",
        "author": "Ingmar Bakermans, Daniel De Pascale, Gon\\c{c}alo Marcelino, Giuseppe Cascavilla, Zeno Geradts",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02872v1 Announce Type: new \nAbstract: Darknet markets (DNMs) facilitate the trade of illegal goods on a global scale. Gathering data on DNMs is critical to ensuring law enforcement agencies can effectively combat crime. Manually extracting data from DNMs is an error-prone and time-consuming task. Aiming to automate this process we develop a framework for extracting data from DNMs and evaluate the application of three state-of-the-art Named Entity Recognition (NER) models, ELMo-BiLSTM \\citep{ShahEtAl2022}, UniversalNER \\citep{ZhouEtAl2024}, and GLiNER \\citep{ZaratianaEtAl2023}, at the task of extracting complex entities from DNM product listing pages. We propose a new annotated dataset, which we use to train, fine-tune, and evaluate the models. Our findings show that state-of-the-art NER models perform well in information extraction from DNMs, achieving 91% Precision, 96% Recall, and an F1 score of 94%. In addition, fine-tuning enhances model performance, with UniversalNER achieving the best performance."
      },
      {
        "id": "oai:arXiv.org:2504.02873v1",
        "title": "Short-PHD: Detecting Short LLM-generated Text with Topological Data Analysis After Off-topic Content Insertion",
        "link": "https://arxiv.org/abs/2504.02873",
        "author": "Dongjun Wei, Minjia Mao, Xiao Fang, Michael Chau",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02873v1 Announce Type: new \nAbstract: The malicious usage of large language models (LLMs) has motivated the detection of LLM-generated texts. Previous work in topological data analysis shows that the persistent homology dimension (PHD) of text embeddings can serve as a more robust and promising score than other zero-shot methods. However, effectively detecting short LLM-generated texts remains a challenge. This paper presents Short-PHD, a zero-shot LLM-generated text detection method tailored for short texts. Short-PHD stabilizes the estimation of the previous PHD method for short texts by inserting off-topic content before the given input text and identifies LLM-generated text based on an established detection threshold. Experimental results on both public and generated datasets demonstrate that Short-PHD outperforms existing zero-shot methods in short LLM-generated text detection. Implementation codes are available online."
      },
      {
        "id": "oai:arXiv.org:2504.02874v1",
        "title": "TheBlueScrubs-v1, a comprehensive curated medical dataset derived from the internet",
        "link": "https://arxiv.org/abs/2504.02874",
        "author": "Luis Felipe, Carlos Garcia, Issam El Naqa, Monique Shotande, Aakash Tripathi, Vivek Rudrapatna, Ghulam Rasool, Danielle Bitterman, Gilmer Valdes",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02874v1 Announce Type: new \nAbstract: The need for robust and diverse data sets to train clinical large language models (cLLMs) is critical given that currently available public repositories often prove too limited in size or scope for comprehensive medical use. While resources like PubMed provide foundational medical literature, they capture only a narrow range of formal publications and omit the broader medical discourse on the internet. To address these deficits, we introduce TheBlueScrubs-v1, a curated dataset of over 25 billion medical tokens - nearly three times larger than PubMed - drawn from a broad-scale internet corpus. Our two-stage filtering pipeline employs a Logistic Regression model for document screening (achieving an AUC of approximately 0.95 on external validation), followed by verification via a 70B-parameter Llama 3.1 instruct model. Each text is assigned three LLM-based quality scores encompassing medical relevance, precision and factual detail, and safety and ethical standards. Clinician reviews confirm high concordance with these automated evaluations, and a specialized cancer classifier further labels approximately 11 billion oncology tokens. Two demonstration tasks highlight the dataset's practical value: first, we distill the safety evaluations to a smaller BERT-style model that reaches an AUC near 0.96 on unseen data; second, we fine-tune a compact LLM on a filtered subset, showing measurable improvements over standard baselines in medical benchmarks as well as private ones. This Data Descriptor details the dataset's creation and validation, underscoring its potential utility for medical AI research."
      },
      {
        "id": "oai:arXiv.org:2504.02876v1",
        "title": "Multimodal Reference Visual Grounding",
        "link": "https://arxiv.org/abs/2504.02876",
        "author": "Yangxiao Lu, Ruosen Li, Liqiang Jing, Jikai Wang, Xinya Du, Yunhui Guo, Nicholas Ruozzi, Yu Xiang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02876v1 Announce Type: new \nAbstract: Visual grounding focuses on detecting objects from images based on language expressions. Recent Large Vision-Language Models (LVLMs) have significantly advanced visual grounding performance by training large models with large-scale datasets. However, the problem remains challenging, especially when similar objects appear in the input image. For example, an LVLM may not be able to differentiate Diet Coke and regular Coke in an image. In this case, if additional reference images of Diet Coke and regular Coke are available, it can help the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual Grounding (MRVG). In this task, a model has access to a set of reference images of objects in a database. Based on these reference images and a language expression, the model is required to detect a target object from a query image. We first introduce a new dataset to study the MRVG problem. Then we introduce a novel method, named MRVG-Net, to solve this visual grounding problem. We show that by efficiently using reference images with few-shot object detection and using Large Language Models (LLMs) for object matching, our method achieves superior visual grounding performance compared to the state-of-the-art LVLMs such as Qwen2.5-VL-7B. Our approach bridges the gap between few-shot detection and visual grounding, unlocking new capabilities for visual understanding. Project page with our code and dataset: https://irvlutd.github.io/MultiGrounding"
      },
      {
        "id": "oai:arXiv.org:2504.02877v1",
        "title": "Revisiting Funnel Transformers for Modern LLM Architectures with Comprehensive Ablations in Training and Inference Configurations",
        "link": "https://arxiv.org/abs/2504.02877",
        "author": "DongHyun Choi, Lucas Spangher, Chris Hidey, Peter Grabowski, Ramy Eskander",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02877v1 Announce Type: new \nAbstract: Transformer-based Large Language Models, which suffer from high computational costs, advance so quickly that techniques proposed to streamline earlier iterations are not guaranteed to benefit more modern models. Building upon the Funnel Transformer proposed by Dai and Le (2020), which progressively compresses intermediate representations, we investigate the impact of funneling in contemporary Gemma2 Transformer architectures. We systematically evaluate various funnel configurations and recovery methods, comparing: (1) standard pretraining to funnel-aware pretraining strategies, (2) the impact of funnel-aware fine-tuning, and (3) the type of sequence recovery operation. Our results demonstrate that funneling creates information bottlenecks that propagate through deeper network layers, particularly in larger models (e.g., Gemma 7B), leading to at times unmanageable performance lost. However, carefully selecting the funneling layer and employing effective recovery strategies, can substantially mitigate performance losses, achieving up to a 44\\% reduction in latency. Our findings highlight key trade-offs between computational efficiency and model accuracy, providing practical guidance for deploying funnel-based approaches in large-scale natural language applications."
      },
      {
        "id": "oai:arXiv.org:2504.02878v1",
        "title": "Exploring the Capabilities of LLMs for IMU-based Fine-grained Human Activity Understanding",
        "link": "https://arxiv.org/abs/2504.02878",
        "author": "Lilin Xu, Kaiyuan Hou, Xiaofan Jiang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02878v1 Announce Type: new \nAbstract: Human activity recognition (HAR) using inertial measurement units (IMUs) increasingly leverages large language models (LLMs), yet existing approaches focus on coarse activities like walking or running. Our preliminary study indicates that pretrained LLMs fail catastrophically on fine-grained HAR tasks such as air-written letter recognition, achieving only near-random guessing accuracy. In this work, we first bridge this gap for flat-surface writing scenarios: by fine-tuning LLMs with a self-collected dataset and few-shot learning, we achieved up to a 129x improvement on 2D data. To extend this to 3D scenarios, we designed an encoder-based pipeline that maps 3D data into 2D equivalents, preserving the spatiotemporal information for robust letter prediction. Our end-to-end pipeline achieves 78% accuracy on word recognition with up to 5 letters in mid-air writing scenarios, establishing LLMs as viable tools for fine-grained HAR."
      },
      {
        "id": "oai:arXiv.org:2504.02881v1",
        "title": "Better Bill GPT: Comparing Large Language Models against Legal Invoice Reviewers",
        "link": "https://arxiv.org/abs/2504.02881",
        "author": "Nick Whitehouse, Nicole Lincoln, Stephanie Yiu, Lizzie Catterson, Rivindu Perera",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02881v1 Announce Type: new \nAbstract: Legal invoice review is a costly, inconsistent, and time-consuming process, traditionally performed by Legal Operations, Lawyers or Billing Specialists who scrutinise billing compliance line by line. This study presents the first empirical comparison of Large Language Models (LLMs) against human invoice reviewers - Early-Career Lawyers, Experienced Lawyers, and Legal Operations Professionals-assessing their accuracy, speed, and cost-effectiveness. Benchmarking state-of-the-art LLMs against a ground truth set by expert legal professionals, our empirically substantiated findings reveal that LLMs decisively outperform humans across every metric. In invoice approval decisions, LLMs achieve up to 92% accuracy, surpassing the 72% ceiling set by experienced lawyers. On a granular level, LLMs dominate line-item classification, with top models reaching F-scores of 81%, compared to just 43% for the best-performing human group. Speed comparisons are even more striking - while lawyers take 194 to 316 seconds per invoice, LLMs are capable of completing reviews in as fast as 3.6 seconds. And cost? AI slashes review expenses by 99.97%, reducing invoice processing costs from an average of $4.27 per invoice for human invoice reviewers to mere cents. These results highlight the evolving role of AI in legal spend management. As law firms and corporate legal departments struggle with inefficiencies, this study signals a seismic shift: The era of LLM-powered legal spend management is not on the horizon, it has arrived. The challenge ahead is not whether AI can perform as well as human reviewers, but how legal teams will strategically incorporate it, balancing automation with human discretion."
      },
      {
        "id": "oai:arXiv.org:2504.02882v1",
        "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models",
        "link": "https://arxiv.org/abs/2504.02882",
        "author": "Sunghee Jung, Donghun Lee, Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Junrae Cho, Kihyun Kim, Eunggyun Kim, Myeongcheol Shin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02882v1 Announce Type: new \nAbstract: Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling."
      },
      {
        "id": "oai:arXiv.org:2504.02883v1",
        "title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models",
        "link": "https://arxiv.org/abs/2504.02883",
        "author": "Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02883v1 Announce Type: new \nAbstract: We introduce SemEval-2025 Task 4: unlearning sensitive content from Large Language Models (LLMs). The task features 3 subtasks for LLM unlearning spanning different use cases: (1) unlearn long form synthetic creative documents spanning different genres; (2) unlearn short form synthetic biographies containing personally identifiable information (PII), including fake names, phone number, SSN, email and home addresses, and (3) unlearn real documents sampled from the target model's training dataset. We received over 100 submissions from over 30 institutions and we summarize the key techniques and lessons in this paper."
      },
      {
        "id": "oai:arXiv.org:2504.02884v1",
        "title": "Enhancing Traffic Sign Recognition On The Performance Based On Yolov8",
        "link": "https://arxiv.org/abs/2504.02884",
        "author": "Baba Ibrahim (Hubei University of Automotive Technology,Hubei University of Automotive Technology), Zhou Kui (Hubei University of Automotive Technology,Hubei University of Automotive Technology)",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02884v1 Announce Type: new \nAbstract: This paper Traffic sign recognition plays a crucial role in the development of autonomous vehicles and advanced driver-assistance systems (ADAS). Despite significant advances in deep learning and object detection, accurately detecting and classifying traffic signs remains challenging due to their small sizes, variable environmental conditions, occlusion, and class imbalance. This thesis presents an enhanced YOLOv8-based detection system that integrates advanced data augmentation techniques, novel architectural enhancements including Coordinate Attention (CA), Bidirectional Feature Pyramid Network (BiFPN), and dynamic modules such as ODConv and LSKA, along with refined loss functions (EIoU and WIoU combined with Focal Loss). Extensive experiments conducted on datasets including GTSRB, TT100K, and GTSDB demonstrate marked improvements in detection accuracy, robustness under adverse conditions, and real-time inference on edge devices. The findings contribute actionable insights for deploying reliable traffic sign recognition systems in real-world autonomous driving scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.02885v1",
        "title": "LVMed-R2: Perception and Reflection-driven Complex Reasoning for Medical Report Generation",
        "link": "https://arxiv.org/abs/2504.02885",
        "author": "Hao Wang, Shuchang Ye, Jinghao Lin, Usman Naseem, Jinman Kim",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02885v1 Announce Type: new \nAbstract: Large vision-language models (LVMs) hold a great promise for automating medical report generation, potentially reducing the burden of manual reporting. State-of-the-art (SOTA) research fine-tunes general LVMs with medical data to align radiology images to corresponding medical reports. However, there are two key factors that limit these LVM's performance. Firstly, LVMs lack complex reasoning capability that leads to logical inconsistencies and potential diagnostic errors in generated reports. Secondly, LVMs lack reflection mechanism that leads to an inability to discover errors in the thinking process. To address these gaps, we propose LVMed-R2, a new fine-tuning strategy that introduces complex reasoning and reflection mechanisms for LVMs to enhance medical report generation. To the best of our knowledge, this is the first work to introduce complex reasoning to the medical report generation (MRG) task. Our proposed complex reasoning contains medical knowledge injection and perception-enhancing modules which improve the accuracy of LVMs diagnosis, coupled with a perception tree to provide guidance to limit the perception range. Further, the reflection mechanism forces self-verification for outputs to correct for potential errors. We experimented by fine-tuning LVMs with our proposed LVMed-R2 strategy, using IU-Xray and MIMIC-CXR datasets. Our results, measured on natural language generation (NLG) metrics and clinical efficacy (CE) metrics, demonstrate that LVMs fine-tuned with the proposed reflection mechanism possess the ability to correct outputs and complex reasoning effectively and improve LVMs performance for MRG."
      },
      {
        "id": "oai:arXiv.org:2504.02887v1",
        "title": "Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding of Online Discourse Datasets",
        "link": "https://arxiv.org/abs/2504.02887",
        "author": "John Chen, Alexandros Lotsos, Grace Wang, Lexie Zhao, Bruce Sherin, Uri Wilensky, Michael Horn",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02887v1 Announce Type: new \nAbstract: Open coding, a key inductive step in qualitative research, discovers and constructs concepts from human datasets. However, capturing extensive and nuanced aspects or \"coding moments\" can be challenging, especially with large discourse datasets. While some studies explore machine learning (ML)/Generative AI (GAI)'s potential for open coding, few evaluation studies exist. We compare open coding results by five recently published ML/GAI approaches and four human coders, using a dataset of online chat messages around a mobile learning software. Our systematic analysis reveals ML/GAI approaches' strengths and weaknesses, uncovering the complementary potential between humans and AI. Line-by-line AI approaches effectively identify content-based codes, while humans excel in interpreting conversational dynamics. We discussed how embedded analytical processes could shape the results of ML/GAI approaches. Instead of replacing humans in open coding, researchers should integrate AI with and according to their analytical processes, e.g., as parallel co-coders."
      },
      {
        "id": "oai:arXiv.org:2504.02888v1",
        "title": "A Status Quo Investigation of Large Language Models towards Cost-Effective CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek",
        "link": "https://arxiv.org/abs/2504.02888",
        "author": "Wenkang Wang, Ran Xu, Jingsen Feng, Qingfu Zhang, Xu Chu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02888v1 Announce Type: new \nAbstract: We evaluated the performance of OpenFOAMGPT incorporating multiple large-language models. Some of the present models efficiently manage different CFD tasks such as adjusting boundary conditions, turbulence models, and solver configurations, although their token cost and stability vary. Locally deployed smaller models like QwQ-32B struggled with generating valid solver files for complex processes. Zero-shot prompting commonly failed in simulations with intricate settings, even for large models. Challenges with boundary conditions and solver keywords stress the requirement for expert supervision, indicating that further development is needed to fully automate specialized CFD simulations."
      },
      {
        "id": "oai:arXiv.org:2504.02889v1",
        "title": "Embedding Method for Knowledge Graph with Densely Defined Ontology",
        "link": "https://arxiv.org/abs/2504.02889",
        "author": "Takanori Ugai",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02889v1 Announce Type: new \nAbstract: Knowledge graph embedding (KGE) is a technique that enhances knowledge graphs by addressing incompleteness and improving knowledge retrieval. A limitation of the existing KGE models is their underutilization of ontologies, specifically the relationships between properties. This study proposes a KGE model, TransU, designed for knowledge graphs with well-defined ontologies that incorporate relationships between properties. The model treats properties as a subset of entities, enabling a unified representation. We present experimental results using a standard dataset and a practical dataset."
      },
      {
        "id": "oai:arXiv.org:2504.02890v1",
        "title": "Scaling Test-time Compute for Low-resource Languages: Multilingual Reasoning in LLMs",
        "link": "https://arxiv.org/abs/2504.02890",
        "author": "Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02890v1 Announce Type: new \nAbstract: Recent advances in test-time compute scaling have enabled Large Language Models (LLMs) to tackle deep reasoning tasks by generating a chain-of-thought (CoT) that includes trial and error, backtracking, and intermediate reasoning steps before producing the final answer. However, these techniques have been applied predominantly to popular languages, such as English, leaving reasoning in low-resource languages underexplored and misaligned. In this work, we investigate the multilingual mechanism by which LLMs internally operate in a latent space biased toward their inherently dominant language. To leverage this phenomenon for low-resource languages, we train models to generate the CoT in English while outputting the final response in the target language, given input in the low-resource language. Our experiments demonstrate that this approach, named English-Pivoted CoT Training, outperforms other baselines, including training to generate both the CoT and the final response solely in the target language, with up to 28.33% improvement. Further analysis provides novel insights into the relationships between reasoning and multilinguality of LLMs, prompting for better approaches in developing multilingual large reasoning models"
      },
      {
        "id": "oai:arXiv.org:2504.02891v1",
        "title": "Automated Survey Collection with LLM-based Conversational Agents",
        "link": "https://arxiv.org/abs/2504.02891",
        "author": "Kurmanbek Kaiyrbekov, Nicholas J Dobbins, Sean D Mooney",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02891v1 Announce Type: new \nAbstract: Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems."
      },
      {
        "id": "oai:arXiv.org:2504.02894v1",
        "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System",
        "link": "https://arxiv.org/abs/2504.02894",
        "author": "Ahsan Bilal, Beiyu Lin, Mehdi Zaeifi",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02894v1 Announce Type: new \nAbstract: Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment."
      },
      {
        "id": "oai:arXiv.org:2504.02895v1",
        "title": "UAC: Uncertainty-Aware Calibration of Neural Networks for Gesture Detection",
        "link": "https://arxiv.org/abs/2504.02895",
        "author": "Farida Al Haddad, Yuxin Wang, Malcolm Mielle",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02895v1 Announce Type: new \nAbstract: Artificial intelligence has the potential to impact safety and efficiency in safety-critical domains such as construction, manufacturing, and healthcare. For example, using sensor data from wearable devices, such as inertial measurement units (IMUs), human gestures can be detected while maintaining privacy, thereby ensuring that safety protocols are followed. However, strict safety requirements in these domains have limited the adoption of AI, since accurate calibration of predicted probabilities and robustness against out-of-distribution (OOD) data is necessary.\n  This paper proposes UAC (Uncertainty-Aware Calibration), a novel two-step method to address these challenges in IMU-based gesture recognition. First, we present an uncertainty-aware gesture network architecture that predicts both gesture probabilities and their associated uncertainties from IMU data. This uncertainty is then used to calibrate the probabilities of each potential gesture. Second, an entropy-weighted expectation of predictions over multiple IMU data windows is used to improve accuracy while maintaining correct calibration.\n  Our method is evaluated using three publicly available IMU datasets for gesture detection and is compared to three state-of-the-art calibration methods for neural networks: temperature scaling, entropy maximization, and Laplace approximation. UAC outperforms existing methods, achieving improved accuracy and calibration in both OOD and in-distribution scenarios. Moreover, we find that, unlike our method, none of the state-of-the-art methods significantly improve the calibration of IMU-based gesture recognition models. In conclusion, our work highlights the advantages of uncertainty-aware calibration of neural networks, demonstrating improvements in both calibration and accuracy for gesture detection using IMU data."
      },
      {
        "id": "oai:arXiv.org:2504.02898v1",
        "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
        "link": "https://arxiv.org/abs/2504.02898",
        "author": "Lele Cao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02898v1 Announce Type: new \nAbstract: Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media."
      },
      {
        "id": "oai:arXiv.org:2504.02900v1",
        "title": "Comparative Analysis of Deepfake Detection Models: New Approaches and Perspectives",
        "link": "https://arxiv.org/abs/2504.02900",
        "author": "Matheus Martins Batista",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02900v1 Announce Type: new \nAbstract: The growing threat posed by deepfake videos, capable of manipulating realities and disseminating misinformation, drives the urgent need for effective detection methods. This work investigates and compares different approaches for identifying deepfakes, focusing on the GenConViT model and its performance relative to other architectures present in the DeepfakeBenchmark. To contextualize the research, the social and legal impacts of deepfakes are addressed, as well as the technical fundamentals of their creation and detection, including digital image processing, machine learning, and artificial neural networks, with emphasis on Convolutional Neural Networks (CNNs), Generative Adversarial Networks (GANs), and Transformers. The performance evaluation of the models was conducted using relevant metrics and new datasets established in the literature, such as WildDeep-fake and DeepSpeak, aiming to identify the most effective tools in the battle against misinformation and media manipulation. The obtained results indicated that GenConViT, after fine-tuning, exhibited superior performance in terms of accuracy (93.82%) and generalization capacity, surpassing other architectures in the DeepfakeBenchmark on the DeepSpeak dataset. This study contributes to the advancement of deepfake detection techniques, offering contributions to the development of more robust and effective solutions against the dissemination of false information."
      },
      {
        "id": "oai:arXiv.org:2504.02901v1",
        "title": "Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLM-Powered Assistance",
        "link": "https://arxiv.org/abs/2504.02901",
        "author": "Bo Yuan, Yulin Chen, Yin Zhang, Wei Jiang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02901v1 Announce Type: new \nAbstract: Learning from noisy labels (LNL) is a challenge that arises in many real-world scenarios where collected training data can contain incorrect or corrupted labels. Most existing solutions identify noisy labels and adopt active learning to query human experts on them for denoising. In the era of large language models (LLMs), although we can reduce the human effort to improve these methods, their performances are still subject to accurately separating the clean and noisy samples from noisy data. In this paper, we propose an innovative collaborative learning framework NoiseAL based on active learning to combine LLMs and small models (SMs) for learning from noisy labels. During collaborative training, we first adopt two SMs to form a co-prediction network and propose a dynamic-enhanced threshold strategy to divide the noisy data into different subsets, then select the clean and noisy samples from these subsets to feed the active annotator LLMs to rectify noisy samples. Finally, we employ different optimization objectives to conquer subsets with different degrees of label noises. Extensive experiments on synthetic and real-world noise datasets further demonstrate the superiority of our framework over state-of-the-art baselines."
      },
      {
        "id": "oai:arXiv.org:2504.02902v1",
        "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
        "link": "https://arxiv.org/abs/2504.02902",
        "author": "Liangjie Huang, Dawei Li, Huan Liu, Lu Cheng",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02902v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability."
      },
      {
        "id": "oai:arXiv.org:2504.02904v1",
        "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence",
        "link": "https://arxiv.org/abs/2504.02904",
        "author": "Hongzhe Du, Weikai Li, Min Cai, Karim Saraipour, Zimin Zhang, Himabindu Lakkaraju, Yizhou Sun, Shichang Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02904v1 Announce Type: new \nAbstract: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by linear vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training."
      },
      {
        "id": "oai:arXiv.org:2504.02906v1",
        "title": "Enhancing Chart-to-Code Generation in Multimodal Large Language Models via Iterative Dual Preference Learning",
        "link": "https://arxiv.org/abs/2504.02906",
        "author": "Zhihan Zhang, Yixin Cao, Lizi Liao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02906v1 Announce Type: new \nAbstract: Chart-to-code generation, the process of converting chart images into executable plotting scripts, provides a lossless representation of chart information, requiring models to accurately capture and summarize all visual and structural elements. However, this remains a significant challenge for multimodal large language models (MLLMs), which are not inherently well-aligned with code generation tasks. To bridge this gap, we introduce Chart2Code, a novel iterative dual preference learning framework designed to enhance MLLMs' chart-to-code generation capabilities through structured code variant generation and fine-grained dual reward signals. We validate Chart2Code across three MLLMs and find that iterative preference learning consistently improves out-of-distribution chart-to-code generation quality. Throughout this process, our dual scoring method, which evaluates both the textual code structure and its visual representation, leads to greater performance improvements, even with a reduced preference dataset size. Further analysis explores the key components of our framework and highlights the interplay between chart-to-code generation and broader chart reasoning, paving the way for future advancements in chart comprehension."
      },
      {
        "id": "oai:arXiv.org:2504.02911v1",
        "title": "Noiser: Bounded Input Perturbations for Attributing Large Language Models",
        "link": "https://arxiv.org/abs/2504.02911",
        "author": "Mohammad Reza Ghasemi Madani, Aryo Pradipta Gema, Gabriele Sarti, Yu Zhao, Pasquale Minervini, Andrea Passerini",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02911v1 Announce Type: new \nAbstract: Feature attribution (FA) methods are common post-hoc approaches that explain how Large Language Models (LLMs) make predictions. Accordingly, generating faithful attributions that reflect the actual inner behavior of the model is crucial. In this paper, we introduce Noiser, a perturbation-based FA method that imposes bounded noise on each input embedding and measures the robustness of the model against partially noised input to obtain the input attributions. Additionally, we propose an answerability metric that employs an instructed judge model to assess the extent to which highly scored tokens suffice to recover the predicted output. Through a comprehensive evaluation across six LLMs and three tasks, we demonstrate that Noiser consistently outperforms existing gradient-based, attention-based, and perturbation-based FA methods in terms of both faithfulness and answerability, making it a robust and effective approach for explaining language model predictions."
      },
      {
        "id": "oai:arXiv.org:2504.02912v1",
        "title": "Haphazard Inputs as Images in Online Learning",
        "link": "https://arxiv.org/abs/2504.02912",
        "author": "Rohit Agarwal, Aryan Dessai, Arif Ahmed Sekh, Krishna Agarwal, Alexander Horsch, Dilip K. Prasad",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02912v1 Announce Type: new \nAbstract: The field of varying feature space in online learning settings, also known as haphazard inputs, is very prominent nowadays due to its applicability in various fields. However, the current solutions to haphazard inputs are model-dependent and cannot benefit from the existing advanced deep-learning methods, which necessitate inputs of fixed dimensions. Therefore, we propose to transform the varying feature space in an online learning setting to a fixed-dimension image representation on the fly. This simple yet novel approach is model-agnostic, allowing any vision-based models to be applicable for haphazard inputs, as demonstrated using ResNet and ViT. The image representation handles the inconsistent input data seamlessly, making our proposed approach scalable and robust. We show the efficacy of our method on four publicly available datasets. The code is available at https://github.com/Rohit102497/HaphazardInputsAsImages."
      },
      {
        "id": "oai:arXiv.org:2504.02917v1",
        "title": "Bias in Large Language Models Across Clinical Applications: A Systematic Review",
        "link": "https://arxiv.org/abs/2504.02917",
        "author": "Thanathip Suenghataiphorn, Narisara Tribuddharat, Pojsakorn Danpanichkul, Narathorn Kulthamrongsri",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02917v1 Announce Type: new \nAbstract: Background: Large language models (LLMs) are rapidly being integrated into healthcare, promising to enhance various clinical tasks. However, concerns exist regarding their potential for bias, which could compromise patient care and exacerbate health inequities. This systematic review investigates the prevalence, sources, manifestations, and clinical implications of bias in LLMs. Methods: We conducted a systematic search of PubMed, OVID, and EMBASE from database inception through 2025, for studies evaluating bias in LLMs applied to clinical tasks. We extracted data on LLM type, bias source, bias manifestation, affected attributes, clinical task, evaluation methods, and outcomes. Risk of bias was assessed using a modified ROBINS-I tool. Results: Thirty-eight studies met inclusion criteria, revealing pervasive bias across various LLMs and clinical applications. Both data-related bias (from biased training data) and model-related bias (from model training) were significant contributors. Biases manifested as: allocative harm (e.g., differential treatment recommendations); representational harm (e.g., stereotypical associations, biased image generation); and performance disparities (e.g., variable output quality). These biases affected multiple attributes, most frequently race/ethnicity and gender, but also age, disability, and language. Conclusions: Bias in clinical LLMs is a pervasive and systemic issue, with a potential to lead to misdiagnosis and inappropriate treatment, particularly for marginalized patient populations. Rigorous evaluation of the model is crucial. Furthermore, the development and implementation of effective mitigation strategies, coupled with continuous monitoring in real-world clinical settings, are essential to ensure the safe, equitable, and trustworthy deployment of LLMs in healthcare."
      },
      {
        "id": "oai:arXiv.org:2504.02918v1",
        "title": "Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments",
        "link": "https://arxiv.org/abs/2504.02918",
        "author": "Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck W. E. Prinzhorn, Mark Bodracska, Nicu Sebe, Efstratios Gavves",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02918v1 Announce Type: new \nAbstract: Recent advances in image and video generation raise hopes that these models possess world modeling capabilities, the ability to generate realistic, physically plausible videos. This could revolutionize applications in robotics, autonomous driving, and scientific simulation. However, before treating these models as world models, we must ask: Do they adhere to physical conservation laws? To answer this, we introduce Morpheus, a benchmark for evaluating video generation models on physical reasoning. It features 80 real-world videos capturing physical phenomena, guided by conservation laws. Since artificial generations lack ground truth, we assess physical plausibility using physics-informed metrics evaluated with respect to infallible conservation laws known per physical setting, leveraging advances in physics-informed neural networks and vision-language foundation models. Our findings reveal that even with advanced prompting and video conditioning, current models struggle to encode physical principles despite generating aesthetically pleasing videos. All data, leaderboard, and code are open-sourced at our project page."
      },
      {
        "id": "oai:arXiv.org:2504.02920v1",
        "title": "LiDAR-based Object Detection with Real-time Voice Specifications",
        "link": "https://arxiv.org/abs/2504.02920",
        "author": "Anurag Kulkarni",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02920v1 Announce Type: new \nAbstract: This paper presents a LiDAR-based object detection system with real-time voice specifications, integrating KITTI's 3D point clouds and RGB images through a multi-modal PointNet framework. It achieves 87.0% validation accuracy on a 3000-sample subset, surpassing a 200-sample baseline of 67.5% by combining spatial and visual data, addressing class imbalance with weighted loss, and refining training via adaptive techniques. A Tkinter prototype provides natural Indian male voice output using Edge TTS (en-IN-PrabhatNeural), alongside 3D visualizations and real-time feedback, enhancing accessibility and safety in autonomous navigation, assistive technology, and beyond. The study offers a detailed methodology, comprehensive experimental analysis, and a broad review of applications and challenges, establishing this work as a scalable advancement in human-computer interaction and environmental perception, aligned with current research trends."
      },
      {
        "id": "oai:arXiv.org:2504.02921v1",
        "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse",
        "link": "https://arxiv.org/abs/2504.02921",
        "author": "Yuwei An, Yihua Cheng, Seo Jin Park, Junchen Jiang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02921v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the performance of large language models (LLMs) by integrating external knowledge into the generation process. A key component of RAG pipelines is the reranker, which selects the most relevant documents from a pool of retrieved candidates and significantly improves the quality of the generated responses. While rerankers refine the selection of retrieved documents in RAG pipelines, they introduce computational challenges that hinder high throughput and low latency. To address this problem, we propose HyperRAG, a system that optimizes the trade-off between quality and efficiency in RAG pipelines by leveraging KV-cache reuse for efficient reranker inference. By reusing document-side KV-cache, HyperRAG achieves both high-quality generation and system-level efficiency. To fully realize the benefits of KV-cache reuse, HyperRAG incorporates a range of system-level optimizations designed to enhance efficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3 throughput improvement with decoder-only rerankers while also delivering higher downstream performance compared with traditional RAG service."
      },
      {
        "id": "oai:arXiv.org:2504.02922v1",
        "title": "Robustly identifying concepts introduced during chat fine-tuning using crosscoders",
        "link": "https://arxiv.org/abs/2504.02922",
        "author": "Julian Minder, Clement Dumas, Caden Juang, Bilal Chugtai, Neel Nanda",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02922v1 Announce Type: new \nAbstract: Model diffing is the study of how fine-tuning changes a model's representations and internal algorithms. Many behaviours of interest are introduced during fine-tuning, and model diffing offers a promising lens to interpret such behaviors. Crosscoders are a recent model diffing method that learns a shared dictionary of interpretable concepts represented as latent directions in both the base and fine-tuned models, allowing us to track how concepts shift or emerge during fine-tuning. Notably, prior work has observed concepts with no direction in the base model, and it was hypothesized that these model-specific latents were concepts introduced during fine-tuning. However, we identify two issues which stem from the crosscoders L1 training loss that can misattribute concepts as unique to the fine-tuned model, when they really exist in both models. We develop Latent Scaling to flag these issues by more accurately measuring each latent's presence across models. In experiments comparing Gemma 2 2B base and chat models, we observe that the standard crosscoder suffers heavily from these issues. Building on these insights, we train a crosscoder with BatchTopK loss and show that it substantially mitigates these issues, finding more genuinely chat-specific and highly interpretable concepts. We recommend practitioners adopt similar techniques. Using the BatchTopK crosscoder, we successfully identify a set of genuinely chat-specific latents that are both interpretable and causally effective, representing concepts such as $\\textit{false information}$ and $\\textit{personal question}$, along with multiple refusal-related latents that show nuanced preferences for different refusal triggers. Overall, our work advances best practices for the crosscoder-based methodology for model diffing and demonstrates that it can provide concrete insights into how chat tuning modifies language model behavior."
      },
      {
        "id": "oai:arXiv.org:2504.02938v1",
        "title": "Graph Attention for Heterogeneous Graphs with Positional Encoding",
        "link": "https://arxiv.org/abs/2504.02938",
        "author": "Nikhil Shivakumar Nayak",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02938v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have emerged as the de facto standard for modeling graph data, with attention mechanisms and transformers significantly enhancing their performance on graph-based tasks. Despite these advancements, the performance of GNNs on heterogeneous graphs often remains complex, with networks generally underperforming compared to their homogeneous counterparts. This work benchmarks various GNN architectures to identify the most effective methods for heterogeneous graphs, with a particular focus on node classification and link prediction. Our findings reveal that graph attention networks excel in these tasks. As a main contribution, we explore enhancements to these attention networks by integrating positional encodings for node embeddings. This involves utilizing the full Laplacian spectrum to accurately capture both the relative and absolute positions of each node within the graph, further enhancing performance on downstream tasks such as node classification and link prediction."
      },
      {
        "id": "oai:arXiv.org:2504.02949v1",
        "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.02949",
        "author": "Xianwei Zhuang, Yuxin Xie, Yufan Deng, Dongchao Yang, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02949v1 Announce Type: new \nAbstract: In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1."
      },
      {
        "id": "oai:arXiv.org:2504.02953v1",
        "title": "Cultural Learning-Based Culture Adaptation of Language Models",
        "link": "https://arxiv.org/abs/2504.02953",
        "author": "Chen Cecilia Liu, Anna Korhonen, Iryna Gurevych",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02953v1 Announce Type: new \nAbstract: Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially causing harm to others. In this paper, we present CLCA, a novel framework for enhancing LLM alignment with cultural values based on cultural learning. The framework leverages simulated social interactions to generate conversations in which LLMs engage in role-playing within culturally adapted social scenarios, capturing implicit cultural norms for model fine-tuning. CLCA improves cultural value alignment across various model architectures measured using World Value Survey data, demonstrating the effectiveness of our proposed approach. Our results provide early evidence that understanding intent and social interactions can enhance cultural value adaptation in LLMs, highlighting the promise of training approaches based on cultural learning."
      },
      {
        "id": "oai:arXiv.org:2504.02956v1",
        "title": "Understanding Aha Moments: from External Observations to Internal Mechanisms",
        "link": "https://arxiv.org/abs/2504.02956",
        "author": "Shu Yang, Junchao Wu, Xin Chen, Yunze Xiao, Xinyi Yang, Derek F. Wong, Di Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02956v1 Announce Type: new \nAbstract: Large Reasoning Models (LRMs), capable of reasoning through complex problems, have become crucial for tasks like programming, mathematics, and commonsense reasoning. However, a key challenge lies in understanding how these models acquire reasoning capabilities and exhibit \"aha moments\" when they reorganize their methods to allocate more thinking time to problems. In this work, we systematically study \"aha moments\" in LRMs, from linguistic patterns, description of uncertainty, \"Reasoning Collapse\" to analysis in latent space. We demonstrate that the \"aha moment\" is externally manifested in a more frequent use of anthropomorphic tones for self-reflection and an adaptive adjustment of uncertainty based on problem difficulty. This process helps the model complete reasoning without succumbing to \"Reasoning Collapse\". Internally, it corresponds to a separation between anthropomorphic characteristics and pure reasoning, with an increased anthropomorphic tone for more difficult problems. Furthermore, we find that the \"aha moment\" helps models solve complex problems by altering their perception of problem difficulty. As the layer of the model increases, simpler problems tend to be perceived as more complex, while more difficult problems appear simpler."
      },
      {
        "id": "oai:arXiv.org:2504.02957v1",
        "title": "Randomized Pairwise Learning with Adaptive Sampling: A PAC-Bayes Analysis",
        "link": "https://arxiv.org/abs/2504.02957",
        "author": "Sijia Zhou (School of Computer Science, University of Birmingham), Yunwen Lei (Department of Mathematics, The University of Hong Kong), Ata Kab\\'an (School of Computer Science, University of Birmingham)",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02957v1 Announce Type: new \nAbstract: We study stochastic optimization with data-adaptive sampling schemes to train pairwise learning models. Pairwise learning is ubiquitous, and it covers several popular learning tasks such as ranking, metric learning and AUC maximization. A notable difference of pairwise learning from pointwise learning is the statistical dependencies among input pairs, for which existing analyses have not been able to handle in the general setting considered in this paper. To this end, we extend recent results that blend together two algorithm-dependent frameworks of analysis -- algorithmic stability and PAC-Bayes -- which allow us to deal with any data-adaptive sampling scheme in the optimizer. We instantiate this framework to analyze (1) pairwise stochastic gradient descent, which is a default workhorse in many machine learning problems, and (2) pairwise stochastic gradient descent ascent, which is a method used in adversarial training. All of these algorithms make use of a stochastic sampling from a discrete distribution (sample indices) before each update. Non-uniform sampling of these indices has been already suggested in the recent literature, to which our work provides generalization guarantees in both smooth and non-smooth convex problems."
      },
      {
        "id": "oai:arXiv.org:2504.02965v1",
        "title": "CoLa -- Learning to Interactively Collaborate with Large LMs",
        "link": "https://arxiv.org/abs/2504.02965",
        "author": "Abhishek Sharma, Dan Goldwasser",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02965v1 Announce Type: new \nAbstract: LLMs' remarkable ability to tackle a wide range of language tasks opened new opportunities for collaborative human-AI problem solving. LLMs can amplify human capabilities by applying their intuitions and reasoning strategies at scale. We explore whether human guides can be simulated, by generalizing from human demonstrations of guiding an AI system to solve complex language problems. We introduce CoLa, a novel self-guided learning paradigm for training automated $\\textit{guides}$ and evaluate it on two QA datasets, a puzzle-solving task, and a constrained text generation task. Our empirical results show that CoLa consistently outperforms competitive approaches across all domains. Moreover, a small-sized trained guide outperforms a strong model like GPT-4 when acting as a guide. We compare the strategies employed by humans and automated guides by conducting a human study on a QA dataset. We show that automated guides outperform humans by adapting their strategies to reasoners' capabilities and conduct qualitative analyses highlighting distinct differences in guiding strategies."
      },
      {
        "id": "oai:arXiv.org:2504.02968v1",
        "title": "Global-Order GFlowNets",
        "link": "https://arxiv.org/abs/2504.02968",
        "author": "Llu\\'is Pastor-P\\'erez, Javier Alonso-Garcia, Lukas Mauch",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02968v1 Announce Type: new \nAbstract: Order-Preserving (OP) GFlowNets have demonstrated remarkable success in tackling complex multi-objective (MOO) black-box optimization problems using stochastic optimization techniques. Specifically, they can be trained online to efficiently sample diverse candidates near the Pareto front. A key advantage of OP GFlowNets is their ability to impose a local order on training samples based on Pareto dominance, eliminating the need for scalarization - a common requirement in other approaches like Preference-Conditional GFlowNets. However, we identify an important limitation of OP GFlowNets: imposing a local order on training samples can lead to conflicting optimization objectives. To address this issue, we introduce Global-Order GFlowNets, which transform the local order into a global one, thereby resolving these conflicts. Our experimental evaluations on various benchmarks demonstrate the efficacy and promise of our proposed method."
      },
      {
        "id": "oai:arXiv.org:2504.02971v1",
        "title": "QID: Efficient Query-Informed ViTs in Data-Scarce Regimes for OCR-free Visual Document Understanding",
        "link": "https://arxiv.org/abs/2504.02971",
        "author": "Binh M. Le, Shaoyuan Xu, Jinmiao Fu, Zhishen Huang, Moyan Li, Yanhui Guo, Hongdong Li, Sameera Ramasinghe, Bryan Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02971v1 Announce Type: new \nAbstract: In Visual Document Understanding (VDU) tasks, fine-tuning a pre-trained Vision-Language Model (VLM) with new datasets often falls short in optimizing the vision encoder to identify query-specific regions in text-rich document images. Existing methods that directly inject queries into model layers by modifying the network architecture often struggle to adapt to new datasets with limited annotations. To address this, we introduce QID, a novel, streamlined, architecture-preserving approach that integrates query embeddings into the vision encoder, leading to notable performance gains, particularly in data-scarce fine-tuning scenarios. Specifically, our approach introduces a dual-module framework: a query-aware module that generates a unique query vector to precisely guide the model's focus, as well as a query-agnostic module that captures the positional relationships among tokens, ensuring robust spatial understanding. Notably, both modules operate independently of the vision attention blocks, facilitating targeted learning of query embeddings and enhancing visual semantic identification. Experiments with OCR-free VLMs across multiple datasets demonstrate significant performance improvements using our method, especially in handling text-rich documents in data-scarce environments."
      },
      {
        "id": "oai:arXiv.org:2504.02973v1",
        "title": "A Bayesian account of pronoun and neopronoun acquisition",
        "link": "https://arxiv.org/abs/2504.02973",
        "author": "Cassandra L. Jacobs, Morgan Grobol",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02973v1 Announce Type: new \nAbstract: A major challenge to equity among members of queer communities is the use of one's chosen forms of reference, such as personal names or pronouns. Speakers often dismiss their misuses of pronouns as \"unintentional\", and claim that their errors reflect many decades of fossilized mainstream language use, as well as attitudes or expectations about the relationship between one's appearance and acceptable forms of reference. We argue for explicitly modeling individual differences in pronoun selection and present a probabilistic graphical modeling approach based on the nested Chinese Restaurant Franchise Process (nCRFP) (Ahmed et al., 2013) to account for flexible pronominal reference such as chosen names and neopronouns while moving beyond form-to-meaning mappings and without lexical co-occurrence statistics to learn referring expressions, as in contemporary language models. We show that such a model can account for variability in how quickly pronouns or names are integrated into symbolic knowledge and can empower computational systems to be both flexible and respectful of queer people with diverse gender expression."
      },
      {
        "id": "oai:arXiv.org:2504.02976v1",
        "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept Mechanistic Interpretability Study via Activation Patching",
        "link": "https://arxiv.org/abs/2504.02976",
        "author": "Nooshin Bahador",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02976v1 Announce Type: new \nAbstract: This study investigates the localization of knowledge representation in fine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching (CLAP), a method that identifies critical neural layers responsible for correct answer generation. The model was fine-tuned on 9,958 PubMed abstracts (epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions) using two configurations with validation loss monitoring for early stopping. CLAP involved (1) caching clean (correct answer) and corrupted (incorrect answer) activations, (2) computing logit difference to quantify model preference, and (3) patching corrupted activations with clean ones to assess recovery. Results revealed three findings: First, patching the first feedforward layer recovered 56% of correct preference, demonstrating that associative knowledge is distributed across multiple layers. Second, patching the final output layer completely restored accuracy (100% recovery), indicating that definitional knowledge is localised. The stronger clean logit difference for definitional questions further supports this localized representation. Third, minimal recovery from convolutional layer patching (13.6%) suggests low-level features contribute marginally to high-level reasoning. Statistical analysis confirmed significant layer-specific effects (p<0.01). These findings demonstrate that factual knowledge is more localized and associative knowledge depends on distributed representations. We also showed that editing efficacy depends on task type. Our findings not only reconcile conflicting observations about localization in model editing but also emphasize on using task-adaptive techniques for reliable, interpretable updates."
      },
      {
        "id": "oai:arXiv.org:2504.02983v1",
        "title": "Hummus: A Dataset of Humorous Multimodal Metaphor Use",
        "link": "https://arxiv.org/abs/2504.02983",
        "author": "Xiaoyu Tong, Zhi Zhang, Martha Lewis, Ekaterina Shutova",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02983v1 Announce Type: new \nAbstract: Metaphor and humor share a lot of common ground, and metaphor is one of the most common humorous mechanisms. This study focuses on the humorous capacity of multimodal metaphors, which has not received due attention in the community. We take inspiration from the Incongruity Theory of humor, the Conceptual Metaphor Theory, and the annotation scheme behind the VU Amsterdam Metaphor Corpus, and developed a novel annotation scheme for humorous multimodal metaphor use in image-caption pairs. We create the Hummus Dataset of Humorous Multimodal Metaphor Use, providing expert annotation on 1k image-caption pairs sampled from the New Yorker Caption Contest corpus. Using the dataset, we test state-of-the-art multimodal large language models (MLLMs) on their ability to detect and understand humorous multimodal metaphor use. Our experiments show that current MLLMs still struggle with processing humorous multimodal metaphors, particularly with regard to integrating visual and textual information. We release our dataset and code at github.com/xiaoyuisrain/humorous-multimodal-metaphor-use."
      },
      {
        "id": "oai:arXiv.org:2504.02994v1",
        "title": "Improving log-based anomaly detection through learned adaptive filter",
        "link": "https://arxiv.org/abs/2504.02994",
        "author": "Yiyuan Xiong, Shaofeng Cai",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02994v1 Announce Type: new \nAbstract: Log messages record important system runtime information and are useful for detecting anomalous behaviors and managing modern software systems. Many supervised and unsupervised learning methods have been proposed recently for log-based anomaly detection. State-of-the-art unsupervised methods predict the next log event given a log sequence and apply fixed configurations that use the same filter condition (i.e. k, the top k predicted log events will be regarded as normal next events) which leads to inferior performance in the detection stage because it sets one fixed k for all log sequences, which ignores the dynamic nature and variance in different log sequences. Recently, deep reinforcement learning (DRL) are widely applied to make intelligent decisions in a dynamic environment. In this work, we contend that it is necessary to apply adaptive filters for different log sequences. To achieve this, we propose a novel approach based on DRL to construct a learned adaptive filter and apply different normal/abnormal filter thresholds for different log sequences. We define the Markov Decision Process (MDP) and formulate the learned adaptive filter as a problem that can be solved by DRL. We evaluate the learned adaptive filter on two state-of-the-art log-based anomaly detection unsupervised approaches DeepLog and LogAnomaly in two datasets HDFS and BGL. Extensive experiments show that our approach outperforms the fixed configurations and achieves significantly better performance in log-based anomaly detection."
      },
      {
        "id": "oai:arXiv.org:2504.02996v1",
        "title": "Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization",
        "link": "https://arxiv.org/abs/2504.02996",
        "author": "Siqi Wang, Aoming Liu, Bryan A. Plummer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02996v1 Announce Type: new \nAbstract: Multi-source Domain Generalization (DG) aims to improve model robustness to new distributions. However, DG methods often overlook the effect of label noise, which can confuse a model during training, reducing performance. Limited prior work has analyzed DG method's noise-robustness, typically focused on an analysis of existing methods rather than new solutions. In this paper, we investigate this underexplored space, where models are evaluated under both distribution shifts and label noise, which we refer to as Noise-Aware Generalization (NAG). A natural solution to address label noise would be to combine a Learning with Noisy Labels (LNL) method with those from DG. Many LNL methods aim to detect distribution shifts in a class's samples, i.e., they assume that distribution shifts often correspond to label noise. However, in NAG distribution shifts can be due to label noise or domain shifts, breaking the assumptions used by LNL methods. A naive solution is to make a similar assumption made by many DG methods, where we presume to have domain labels during training, enabling us to isolate the two types of shifts. However, this ignores valuable cross-domain information. Specifically, our proposed DL4ND approach improves noise detection by taking advantage of the observation that noisy samples that may appear indistinguishable within a single domain often show greater variation when compared across domains. Experiments show that DL4ND significantly improves performance across four diverse datasets, offering a promising direction for tackling NAG."
      },
      {
        "id": "oai:arXiv.org:2504.02999v1",
        "title": "Anomaly Detection in Time Series Data Using Reinforcement Learning, Variational Autoencoder, and Active Learning",
        "link": "https://arxiv.org/abs/2504.02999",
        "author": "Bahareh Golchin, Banafsheh Rekabdar",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02999v1 Announce Type: new \nAbstract: A novel approach to detecting anomalies in time series data is presented in this paper. This approach is pivotal in domains such as data centers, sensor networks, and finance. Traditional methods often struggle with manual parameter tuning and cannot adapt to new anomaly types. Our method overcomes these limitations by integrating Deep Reinforcement Learning (DRL) with a Variational Autoencoder (VAE) and Active Learning. By incorporating a Long Short-Term Memory (LSTM) network, our approach models sequential data and its dependencies effectively, allowing for the detection of new anomaly classes with minimal labeled data. Our innovative DRL- VAE and Active Learning combination significantly improves existing methods, as shown by our evaluations on real-world datasets, enhancing anomaly detection techniques and advancing time series analysis."
      },
      {
        "id": "oai:arXiv.org:2504.03006v1",
        "title": "DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed Human Mesh Recovery",
        "link": "https://arxiv.org/abs/2504.03006",
        "author": "Jing Gao, Ce Zheng, Laszlo A. Jeni, Zackory Erickson",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03006v1 Announce Type: new \nAbstract: In-bed human mesh recovery can be crucial and enabling for several healthcare applications, including sleep pattern monitoring, rehabilitation support, and pressure ulcer prevention. However, it is difficult to collect large real-world visual datasets in this domain, in part due to privacy and expense constraints, which in turn presents significant challenges for training and deploying deep learning models. Existing in-bed human mesh estimation methods often rely heavily on real-world data, limiting their ability to generalize across different in-bed scenarios, such as varying coverings and environmental settings. To address this, we propose a Sim-to-Real Transfer Framework for in-bed human mesh recovery from overhead depth images, which leverages large-scale synthetic data alongside limited or no real-world samples. We introduce a diffusion model that bridges the gap between synthetic data and real data to support generalization in real-world in-bed pose and body inference scenarios. Extensive experiments and ablation studies validate the effectiveness of our framework, demonstrating significant improvements in robustness and adaptability across diverse healthcare scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.03010v1",
        "title": "Emotion Recognition Using Convolutional Neural Networks",
        "link": "https://arxiv.org/abs/2504.03010",
        "author": "Shaoyuan Xu, Yang Cheng, Qian Lin, Jan P. Allebach",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03010v1 Announce Type: new \nAbstract: Emotion has an important role in daily life, as it helps people better communicate with and understand each other more efficiently. Facial expressions can be classified into 7 categories: angry, disgust, fear, happy, neutral, sad and surprise. How to detect and recognize these seven emotions has become a popular topic in the past decade. In this paper, we develop an emotion recognition system that can apply emotion recognition on both still images and real-time videos by using deep learning.\n  We build our own emotion recognition classification and regression system from scratch, which includes dataset collection, data preprocessing , model training and testing. Given a certain image or a real-time video, our system is able to show the classification and regression results for all of the 7 emotions. The proposed system is tested on 2 different datasets, and achieved an accuracy of over 80\\%. Moreover, the result obtained from real-time testing proves the feasibility of implementing convolutional neural networks in real time to detect emotions accurately and efficiently."
      },
      {
        "id": "oai:arXiv.org:2504.03011v1",
        "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization",
        "link": "https://arxiv.org/abs/2504.03011",
        "author": "Junying Wang, Jingyuan Liu, Xin Sun, Krishna Kumar Singh, Zhixin Shu, He Zhang, Jimei Yang, Nanxuan Zhao, Tuanfeng Y. Wang, Simon S. Chen, Ulrich Neumann, Jae Shin Yoon",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03011v1 Announce Type: new \nAbstract: This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods."
      },
      {
        "id": "oai:arXiv.org:2504.03020v1",
        "title": "Page Classification for Print Imaging Pipeline",
        "link": "https://arxiv.org/abs/2504.03020",
        "author": "Shaoyuan Xu, Cheng Lu, Mark Shaw, Peter Bauer, Jan P. Allebach",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03020v1 Announce Type: new \nAbstract: Digital copiers and printers are widely used nowadays. One of the most important things people care about is copying or printing quality. In order to improve it, we previously came up with an SVM-based classification method to classify images with only text, only pictures or a mixture of both based on the fact that modern copiers and printers are equipped with processing pipelines designed specifically for different kinds of images. However, in some other applications, we need to distinguish more than three classes. In this paper, we develop a more advanced SVM-based classification method using four more new features to classify 5 types of images which are text, picture, mixed, receipt and highlight."
      },
      {
        "id": "oai:arXiv.org:2504.03022v1",
        "title": "The Dual-Route Model of Induction",
        "link": "https://arxiv.org/abs/2504.03022",
        "author": "Sheridan Feucht, Eric Todd, Byron Wallace, David Bau",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03022v1 Announce Type: new \nAbstract: Prior work on in-context copying has shown the existence of induction heads, which attend to and promote individual tokens during copying. In this work we introduce a new type of induction head: concept-level induction heads, which copy entire lexical units instead of individual tokens. Concept induction heads learn to attend to the ends of multi-token words throughout training, working in parallel with token-level induction heads to copy meaningful text. We show that these heads are responsible for semantic tasks like word-level translation, whereas token induction heads are vital for tasks that can only be done verbatim, like copying nonsense tokens. These two \"routes\" operate independently: in fact, we show that ablation of token induction heads causes models to paraphrase where they would otherwise copy verbatim. In light of these findings, we argue that although token induction heads are vital for specific tasks, concept induction heads may be more broadly relevant for in-context learning."
      },
      {
        "id": "oai:arXiv.org:2504.03024v1",
        "title": "Deep Reinforcement Learning via Object-Centric Attention",
        "link": "https://arxiv.org/abs/2504.03024",
        "author": "Jannis Bl\\\"uml, Cedric Derstroff, Bjarne Gregori, Elisabeth Dillies, Quentin Delfosse, Kristian Kersting",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03024v1 Announce Type: new \nAbstract: Deep reinforcement learning agents, trained on raw pixel inputs, often fail to generalize beyond their training environments, relying on spurious correlations and irrelevant background details. To address this issue, object-centric agents have recently emerged. However, they require different representations tailored to the task specifications. Contrary to deep agents, no single object-centric architecture can be applied to any environment. Inspired by principles of cognitive science and Occam's Razor, we introduce Object-Centric Attention via Masking (OCCAM), which selectively preserves task-relevant entities while filtering out irrelevant visual information. Specifically, OCCAM takes advantage of the object-centric inductive bias. Empirical evaluations on Atari benchmarks demonstrate that OCCAM significantly improves robustness to novel perturbations and reduces sample complexity while showing similar or improved performance compared to conventional pixel-based RL. These results suggest that structured abstraction can enhance generalization without requiring explicit symbolic representations or domain-specific object extraction pipelines."
      },
      {
        "id": "oai:arXiv.org:2504.03026v1",
        "title": "HALO: Human-Aligned End-to-end Image Retargeting with Layered Transformations",
        "link": "https://arxiv.org/abs/2504.03026",
        "author": "Yiran Xu, Siqi Xie, Zhuofang Li, Harris Shadmany, Yinxiao Li, Luciano Sbaiz, Miaosen Wang, Junjie Ke, Jose Lezama, Hang Qi, Han Zhang, Jesse Berent, Ming-Hsuan Yang, Irfan Essa, Jia-Bin Huang, Feng Yang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03026v1 Announce Type: new \nAbstract: Image retargeting aims to change the aspect-ratio of an image while maintaining its content and structure with less visual artifacts. Existing methods still generate many artifacts or fail to maintain original content or structure. To address this, we introduce HALO, an end-to-end trainable solution for image retargeting. Since humans are more sensitive to distortions in salient areas than non-salient areas of an image, HALO decomposes the input image into salient/non-salient layers and applies different wrapping fields to different layers. To further minimize the structure distortion in the output images, we propose perceptual structure similarity loss which measures the structure similarity between input and output images and aligns with human perception. Both quantitative results and a user study on the RetargetMe dataset show that HALO achieves SOTA. Especially, our method achieves an 18.4% higher user preference compared to the baselines on average."
      },
      {
        "id": "oai:arXiv.org:2504.03036v1",
        "title": "IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling",
        "link": "https://arxiv.org/abs/2504.03036",
        "author": "Z\\'ebulon Goriely, Paula Buttery",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03036v1 Announce Type: new \nAbstract: In this paper, we introduce two resources: (i) G2P+, a tool for converting orthographic datasets to a consistent phonemic representation; and (ii) IPA CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior tools for grapheme-to-phoneme conversion result in phonemic vocabularies that are inconsistent with established phonemic inventories, an issue which G2P+ addresses by leveraging the inventories in the Phoible database. Using this tool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES. This new resource fills several gaps in existing phonemic datasets, which often lack multilingual coverage, spontaneous speech, and a focus on child-directed language. We demonstrate the utility of this dataset for phonological research by training phoneme language models on 11 languages and probing them for distinctive features, finding that the distributional properties of phonemes are sufficient to learn major class and place features cross-lingually."
      },
      {
        "id": "oai:arXiv.org:2504.03040v1",
        "title": "Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards",
        "link": "https://arxiv.org/abs/2504.03040",
        "author": "Hanping Zhang, Yuhong Guo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03040v1 Announce Type: new \nAbstract: Safe Reinforcement Learning (Safe RL) aims to train an RL agent to maximize its performance in real-world environments while adhering to safety constraints, as exceeding safety violation limits can result in severe consequences. In this paper, we propose a novel safe RL approach called Safety Modulated Policy Optimization (SMPO), which enables safe policy function learning within the standard policy optimization framework through safety modulated rewards. In particular, we consider safety violation costs as feedback from the RL environments that are parallel to the standard awards, and introduce a Q-cost function as safety critic to estimate expected future cumulative costs. Then we propose to modulate the rewards using a cost-aware weighting function, which is carefully designed to ensure the safety limits based on the estimation of the safety critic, while maximizing the expected rewards. The policy function and the safety critic are simultaneously learned through gradient descent during online interactions with the environment. We conduct experiments using multiple RL environments and the experimental results demonstrate that our method outperforms several classic and state-of-the-art comparison methods in terms of overall safe RL performance."
      },
      {
        "id": "oai:arXiv.org:2504.03041v1",
        "title": "VIP: Video Inpainting Pipeline for Real World Human Removal",
        "link": "https://arxiv.org/abs/2504.03041",
        "author": "Huiming Sun, Yikang Li, Kangning Yang, Ruineng Li, Daitao Xing, Yangbo Xie, Lan Fu, Kaiyu Zhang, Ming Chen, Jiaming Ding, Jiang Geng, Jie Cai, Zibo Meng, Chiuman Ho",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03041v1 Announce Type: new \nAbstract: Inpainting for real-world human and pedestrian removal in high-resolution video clips presents significant challenges, particularly in achieving high-quality outcomes, ensuring temporal consistency, and managing complex object interactions that involve humans, their belongings, and their shadows. In this paper, we introduce VIP (Video Inpainting Pipeline), a novel promptless video inpainting framework for real-world human removal applications. VIP enhances a state-of-the-art text-to-video model with a motion module and employs a Variational Autoencoder (VAE) for progressive denoising in the latent space. Additionally, we implement an efficient human-and-belongings segmentation for precise mask generation. Sufficient experimental results demonstrate that VIP achieves superior temporal consistency and visual fidelity across diverse real-world scenarios, surpassing state-of-the-art methods on challenging datasets. Our key contributions include the development of the VIP pipeline, a reference frame integration technique, and the Dual-Fusion Latent Segment Refinement method, all of which address the complexities of inpainting in long, high-resolution video sequences."
      },
      {
        "id": "oai:arXiv.org:2504.03043v1",
        "title": "Sliced Wasserstein Discrepancy in Disentangling Representation and Adaptation Networks for Unsupervised Domain Adaptation",
        "link": "https://arxiv.org/abs/2504.03043",
        "author": "Joel Sol, Shadi Alijani, Homayoun Najjaran",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03043v1 Announce Type: new \nAbstract: This paper introduces DRANet-SWD, an extension of existing work that disentangles content and style representations of images for unsupervised domain adaptation (UDA). The approach builds upon DRANet by incorporating the sliced Wasserstein discrepancy (SWD) as a style loss instead of the traditional Gram matrix loss. The potential advantages of SWD over the Gram matrix loss for capturing style variations in domain adaptation are investigated. Experiments using digit classification datasets and driving scenario segmentation validate the method, demonstrating that DRANet-SWD enhances performance. Results indicate that SWD provides a more robust statistical comparison of feature distributions, leading to better style adaptation. These findings highlight the effectiveness of SWD in refining feature alignment and improving domain adaptation tasks across these benchmarks. Our code can be found here."
      },
      {
        "id": "oai:arXiv.org:2504.03045v1",
        "title": "Extending CREAMT: Leveraging Large Language Models for Literary Translation Post-Editing",
        "link": "https://arxiv.org/abs/2504.03045",
        "author": "Antonio Castaldo, Sheila Castilho, Joss Moorkens, Johanna Monti",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03045v1 Announce Type: new \nAbstract: Post-editing machine translation (MT) for creative texts, such as literature, requires balancing efficiency with the preservation of creativity and style. While neural MT systems struggle with these challenges, large language models (LLMs) offer improved capabilities for context-aware and creative translation. This study evaluates the feasibility of post-editing literary translations generated by LLMs. Using a custom research tool, we collaborated with professional literary translators to analyze editing time, quality, and creativity. Our results indicate that post-editing LLM-generated translations significantly reduces editing time compared to human translation while maintaining a similar level of creativity. The minimal difference in creativity between PE and MT, combined with substantial productivity gains, suggests that LLMs may effectively support literary translators working with high-resource languages."
      },
      {
        "id": "oai:arXiv.org:2504.03047v1",
        "title": "Attention-Aware Multi-View Pedestrian Tracking",
        "link": "https://arxiv.org/abs/2504.03047",
        "author": "Reef Alturki, Adrian Hilton, Jean-Yves Guillemaut",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03047v1 Announce Type: new \nAbstract: In spite of the recent advancements in multi-object tracking, occlusion poses a significant challenge. Multi-camera setups have been used to address this challenge by providing a comprehensive coverage of the scene. Recent multi-view pedestrian detection models have highlighted the potential of an early-fusion strategy, projecting feature maps of all views to a common ground plane or the Bird's Eye View (BEV), and then performing detection. This strategy has been shown to improve both detection and tracking performance. However, the perspective transformation results in significant distortion on the ground plane, affecting the robustness of the appearance features of the pedestrians. To tackle this limitation, we propose a novel model that incorporates attention mechanisms in a multi-view pedestrian tracking scenario. Our model utilizes an early-fusion strategy for detection, and a cross-attention mechanism to establish robust associations between pedestrians in different frames, while efficiently propagating pedestrian features across frames, resulting in a more robust feature representation for each pedestrian. Extensive experiments demonstrate that our model outperforms state-of-the-art models, with an IDF1 score of $96.1\\%$ on Wildtrack dataset, and $85.7\\%$ on MultiviewX dataset."
      },
      {
        "id": "oai:arXiv.org:2504.03048v1",
        "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
        "link": "https://arxiv.org/abs/2504.03048",
        "author": "Ian Berlot-Attwell, Frank Rudzicz, Xujie Si",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03048v1 Announce Type: new \nAbstract: Recent advancements in the coding, reasoning, and tool-using abilities of LLMs have spurred interest in library learning (i.e., online learning through the creation, storage, and retrieval of reusable and composable functions, knowledge, checklists, or lemmas). Such systems often promise improved task performance through the automatic creation of broadly applicable tools, as well as superior computational performance through the caching of reasoning (i.e., the storage of generated tools). However, we find strong reason to be skeptical. We perform a deep dive into one such system, LEGO-Prover, which purports to learn reusable lemmas for mathematical reasoning. We find no evidence of the direct reuse of learned lemmas, and find evidence against the soft reuse of learned lemmas (i.e., reuse by modifying relevant examples). Crucially, we find that LEGO-Prover does not in fact improve over the simple baseline of prompting the model - the improvements in task accuracy vanish once computational cost is accounted for. Our findings suggest that serious misconceptions exist as to the effectiveness of these techniques, that a serious re-examination of the state of LLM-based library learning is required, and that we require much stronger standards for evaluation including behavioural analysis and ensuring that an equal computational budget is used for baselines."
      },
      {
        "id": "oai:arXiv.org:2504.03051v1",
        "title": "Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language Models",
        "link": "https://arxiv.org/abs/2504.03051",
        "author": "Chengyang He, Wenlong Zhang, Violet Xinying Chen, Yue Ning, Ping Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03051v1 Announce Type: new \nAbstract: Accurate medical symptom coding from unstructured clinical text, such as vaccine safety reports, is a critical task with applications in pharmacovigilance and safety monitoring. Symptom coding, as tailored in this study, involves identifying and linking nuanced symptom mentions to standardized vocabularies like MedDRA, differentiating it from broader medical coding tasks. Traditional approaches to this task, which treat symptom extraction and linking as independent workflows, often fail to handle the variability and complexity of clinical narratives, especially for rare cases. Recent advancements in Large Language Models (LLMs) offer new opportunities but face challenges in achieving consistent performance. To address these issues, we propose Task as Context (TACO) Prompting, a novel framework that unifies extraction and linking tasks by embedding task-specific context into LLM prompts. Our study also introduces SYMPCODER, a human-annotated dataset derived from Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage evaluation framework to comprehensively assess both symptom linking and mention fidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat, Jackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's effectiveness in improving flexibility and accuracy for tailored tasks like symptom coding, paving the way for more specific coding tasks and advancing clinical text processing methodologies."
      },
      {
        "id": "oai:arXiv.org:2504.03052v1",
        "title": "Cooperative Inference for Real-Time 3D Human Pose Estimation in Multi-Device Edge Networks",
        "link": "https://arxiv.org/abs/2504.03052",
        "author": "Hyun-Ho Choi, Kangsoo Kim, Ki-Ho Lee, Kisong Lee",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03052v1 Announce Type: new \nAbstract: Accurate and real-time three-dimensional (3D) pose estimation is challenging in resource-constrained and dynamic environments owing to its high computational complexity. To address this issue, this study proposes a novel cooperative inference method for real-time 3D human pose estimation in mobile edge computing (MEC) networks. In the proposed method, multiple end devices equipped with lightweight inference models employ dual confidence thresholds to filter ambiguous images. Only the filtered images are offloaded to an edge server with a more powerful inference model for re-evaluation, thereby improving the estimation accuracy under computational and communication constraints. We numerically analyze the performance of the proposed inference method in terms of the inference accuracy and end-to-end delay and formulate a joint optimization problem to derive the optimal confidence thresholds and transmission time for each device, with the objective of minimizing the mean per-joint position error (MPJPE) while satisfying the required end-to-end delay constraint. To solve this problem, we demonstrate that minimizing the MPJPE is equivalent to maximizing the sum of the inference accuracies for all devices, decompose the problem into manageable subproblems, and present a low-complexity optimization algorithm to obtain a near-optimal solution. The experimental results show that a trade-off exists between the MPJPE and end-to-end delay depending on the confidence thresholds. Furthermore, the results confirm that the proposed cooperative inference method achieves a significant reduction in the MPJPE through the optimal selection of confidence thresholds and transmission times, while consistently satisfying the end-to-end delay requirement in various MEC environments."
      },
      {
        "id": "oai:arXiv.org:2504.03059v1",
        "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization",
        "link": "https://arxiv.org/abs/2504.03059",
        "author": "Haishan Wang, Mohammad Hassan Vali, Arno Solin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03059v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1~GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications."
      },
      {
        "id": "oai:arXiv.org:2504.03064v1",
        "title": "Context-Aware Self-Adaptation for Domain Generalization",
        "link": "https://arxiv.org/abs/2504.03064",
        "author": "Hao Yan, Yuhong Guo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03064v1 Announce Type: new \nAbstract: Domain generalization aims at developing suitable learning algorithms in source training domains such that the model learned can generalize well on a different unseen testing domain. We present a novel two-stage approach called Context-Aware Self-Adaptation (CASA) for domain generalization. CASA simulates an approximate meta-generalization scenario and incorporates a self-adaptation module to adjust pre-trained meta source models to the meta-target domains while maintaining their predictive capability on the meta-source domains. The core concept of self-adaptation involves leveraging contextual information, such as the mean of mini-batch features, as domain knowledge to automatically adapt a model trained in the first stage to new contexts in the second stage. Lastly, we utilize an ensemble of multiple meta-source models to perform inference on the testing domain. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on standard benchmarks."
      },
      {
        "id": "oai:arXiv.org:2504.03071v1",
        "title": "AD-GPT: Large Language Models in Alzheimer's Disease",
        "link": "https://arxiv.org/abs/2504.03071",
        "author": "Ziyu Liu, Lintao Tang, Zeliang Sun, Zhengliang Liu, Yanjun Lyu, Wei Ruan, Yangshuang Xu, Liang Shan, Jiyoon Shin, Xiaohe Chen, Dajiang Zhu, Tianming Liu, Rongjie Liu, Chao Huang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03071v1 Announce Type: new \nAbstract: Large language models (LLMs) have emerged as powerful tools for medical information retrieval, yet their accuracy and depth remain limited in specialized domains such as Alzheimer's disease (AD), a growing global health challenge. To address this gap, we introduce AD-GPT, a domain-specific generative pre-trained transformer designed to enhance the retrieval and analysis of AD-related genetic and neurobiological information. AD-GPT integrates diverse biomedical data sources, including potential AD-associated genes, molecular genetic information, and key gene variants linked to brain regions. We develop a stacked LLM architecture combining Llama3 and BERT, optimized for four critical tasks in AD research: (1) genetic information retrieval, (2) gene-brain region relationship assessment, (3) gene-AD relationship analysis, and (4) brain region-AD relationship mapping. Comparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's superior precision and reliability across these tasks, underscoring its potential as a robust and specialized AI tool for advancing AD research and biomarker discovery."
      },
      {
        "id": "oai:arXiv.org:2504.03072v1",
        "title": "How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models",
        "link": "https://arxiv.org/abs/2504.03072",
        "author": "Pascal Chang, Jingwei Tang, Markus Gross, Vinicius C. Azevedo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03072v1 Announce Type: new \nAbstract: Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the results. This either produces high-frequency flickering, or texture-sticking artifacts that are not amenable to post-processing. With this in mind, we propose a novel method for preserving temporal correlations in a sequence of noise samples. This approach is materialized by a novel noise representation, dubbed $\\int$-noise (integral noise), that reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses $\\int$-noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed $\\int$-noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation. See https://warpyournoise.github.io/ for video results."
      },
      {
        "id": "oai:arXiv.org:2504.03089v1",
        "title": "SLACK: Attacking LiDAR-based SLAM with Adversarial Point Injections",
        "link": "https://arxiv.org/abs/2504.03089",
        "author": "Prashant Kumar, Dheeraj Vattikonda, Kshitij Madhav Bhat, Kunal Dargan, Prem Kalra",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03089v1 Announce Type: new \nAbstract: The widespread adoption of learning-based methods for the LiDAR makes autonomous vehicles vulnerable to adversarial attacks through adversarial \\textit{point injections (PiJ)}. It poses serious security challenges for navigation and map generation. Despite its critical nature, no major work exists that studies learning-based attacks on LiDAR-based SLAM. Our work proposes SLACK, an end-to-end deep generative adversarial model to attack LiDAR scans with several point injections without deteriorating LiDAR quality. To facilitate SLACK, we design a novel yet simple autoencoder that augments contrastive learning with segmentation-based attention for precise reconstructions. SLACK demonstrates superior performance on the task of \\textit{point injections (PiJ)} compared to the best baselines on KITTI and CARLA-64 dataset while maintaining accurate scan quality. We qualitatively and quantitatively demonstrate PiJ attacks using a fraction of LiDAR points. It severely degrades navigation and map quality without deteriorating the LiDAR scan quality."
      },
      {
        "id": "oai:arXiv.org:2504.03092v1",
        "title": "Machine Learning-Based Detection and Analysis of Suspicious Activities in Bitcoin Wallet Transactions in the USA",
        "link": "https://arxiv.org/abs/2504.03092",
        "author": "Md Zahidul Islam, Md Shahidul Islam, Biswajit Chandra das, Syed Ali Reza, Proshanta Kumar Bhowmik, Kanchon Kumar Bishnu, Md Shafiqur Rahman, Redoyan Chowdhury, Laxmi Pant",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03092v1 Announce Type: new \nAbstract: The dramatic adoption of Bitcoin and other cryptocurrencies in the USA has revolutionized the financial landscape and provided unprecedented investment and transaction efficiency opportunities. The prime objective of this research project is to develop machine learning algorithms capable of effectively identifying and tracking suspicious activity in Bitcoin wallet transactions. With high-tech analysis, the study aims to create a model with a feature for identifying trends and outliers that can expose illicit activity. The current study specifically focuses on Bitcoin transaction information in America, with a strong emphasis placed on the importance of knowing about the immediate environment in and through which such transactions pass through. The dataset is composed of in-depth Bitcoin wallet transactional information, including important factors such as transaction values, timestamps, network flows, and addresses for wallets. All entries in the dataset expose information about financial transactions between wallets, including received and sent transactions, and such information is significant for analysis and trends that can represent suspicious activity. This study deployed three accredited algorithms, most notably, Logistic Regression, Random Forest, and Support Vector Machines. In retrospect, Random Forest emerged as the best model with the highest F1 Score, showcasing its ability to handle non-linear relationships in the data. Insights revealed significant patterns in wallet activity, such as the correlation between unredeemed transactions and final balances. The application of machine algorithms in tracking cryptocurrencies is a tool for creating transparent and secure U.S. markets."
      },
      {
        "id": "oai:arXiv.org:2504.03093v1",
        "title": "Post-processing for Fair Regression via Explainable SVD",
        "link": "https://arxiv.org/abs/2504.03093",
        "author": "Zhiqun Zuo, Ding Zhu, Mohammad Mahdi Khalili",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03093v1 Announce Type: new \nAbstract: This paper presents a post-processing algorithm for training fair neural network regression models that satisfy statistical parity, utilizing an explainable singular value decomposition (SVD) of the weight matrix. We propose a linear transformation of the weight matrix, whereby the singular values derived from the SVD of the transformed matrix directly correspond to the differences in the first and second moments of the output distributions across two groups. Consequently, we can convert the fairness constraints into constraints on the singular values. We analytically solve the problem of finding the optimal weights under these constraints. Experimental validation on various datasets demonstrates that our method achieves a similar or superior fairness-accuracy trade-off compared to the baselines without using the sensitive attribute at the inference time."
      },
      {
        "id": "oai:arXiv.org:2504.03096v1",
        "title": "Scaling Open-Vocabulary Action Detection",
        "link": "https://arxiv.org/abs/2504.03096",
        "author": "Zhen Hao Sia, Yogesh Singh Rawat",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03096v1 Announce Type: new \nAbstract: In this work, we focus on scaling open-vocabulary action detection. Existing approaches for action detection are predominantly limited to closed-set scenarios and rely on complex, parameter-heavy architectures. Extending these models to the open-vocabulary setting poses two key challenges: (1) the lack of large-scale datasets with many action classes for robust training, and (2) parameter-heavy adaptations to a pretrained vision-language contrastive model to convert it for detection, risking overfitting the additional non-pretrained parameters to base action classes. Firstly, we introduce an encoder-only multimodal model for video action detection, reducing the reliance on parameter-heavy additions for video action detection. Secondly, we introduce a simple weakly supervised training strategy to exploit an existing closed-set action detection dataset for pretraining. Finally, we depart from the ill-posed base-to-novel benchmark used by prior works in open-vocabulary action detection and devise a new benchmark to evaluate on existing closed-set action detection datasets without ever using them for training, showing novel results to serve as baselines for future work."
      },
      {
        "id": "oai:arXiv.org:2504.03101v1",
        "title": "Single-Pass Document Scanning for Question Answering",
        "link": "https://arxiv.org/abs/2504.03101",
        "author": "Weili Cao, Jianyou Wang, Youze Zheng, Longtian Bao, Qirui Zheng, Taylor Berg-Kirkpatrick, Ramamohan Paturi, Leon Bergen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03101v1 Announce Type: new \nAbstract: Handling extremely large documents for question answering is challenging: chunk-based embedding methods often lose track of important global context, while full-context transformers can be prohibitively expensive for hundreds of thousands of tokens. We propose a single-pass document scanning approach that processes the entire text in linear time, preserving global coherence while deciding which sentences are most relevant to the query. On 41 QA benchmarks, our single-pass scanner consistently outperforms chunk-based embedding methods and competes with large language models at a fraction of the computational cost. By conditioning on the entire preceding context without chunk breaks, the method preserves global coherence, which is especially important for long documents. Overall, single-pass document scanning offers a simple solution for question answering over massive text. All code, datasets, and model checkpoints are available at https://github.com/MambaRetriever/MambaRetriever"
      },
      {
        "id": "oai:arXiv.org:2504.03108v1",
        "title": "Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation",
        "link": "https://arxiv.org/abs/2504.03108",
        "author": "Xuanyu Liu, Huiyun Yao, Jinggui Gao, Zhongyi Guo, Xue Zhang, Yulin Dong",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03108v1 Announce Type: new \nAbstract: Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT) are the main techniques used in Medical image segmentation. However, CNN is limited to local contextual information, and ViT's quadratic complexity results in significant computational costs. At the same time, equipping the model to distinguish lesion boundaries with varying degrees of severity is also a challenge encountered in skin lesion segmentation. Purpose:This research aims to optimize the balance between computational costs and long-range dependency modelling and achieve excellent generalization across lesions with different degrees of severity. Methods:we propose a lightweight U-shape network that utilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the advantages of Fastformer's additive attention mechanism, combining element-wise product and matrix product for comprehensive feature extraction and channel reduction to save computational costs. In order to accurately identify the lesion boundaries with varying degrees of severity, we designed Fusion Mechanism including Multi-Granularity Fusion and Channel Fusion, which can process the feature maps in the granularity and channel levels to obtain different contextual information. Results:Comprehensive experiments on the ISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms existing state-of-the-art models regarding parameter numbers, computational complexity and segmentation performance. In short, compared to MISSFormer, our model achieves superior segmentation performance while reducing parameter and computation costs by 101x and 15x, respectively. Conclusions:Both quantitative and qualitative analyses show that VFFM-UNet sets a new benchmark by reaching an ideal balance between parameter numbers, computational complexity, and segmentation performance compared to existing state-of-the-art models."
      },
      {
        "id": "oai:arXiv.org:2504.03118v1",
        "title": "NuWa: Deriving Lightweight Task-Specific Vision Transformers for Edge Devices",
        "link": "https://arxiv.org/abs/2504.03118",
        "author": "Ziteng Wei, Qiang He, Bing Li, Feifei Chen, Yun Yang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03118v1 Announce Type: new \nAbstract: Vision Transformers (ViTs) excel in computer vision tasks but lack flexibility for edge devices' diverse needs. A vital issue is that ViTs pre-trained to cover a broad range of tasks are \\textit{over-qualified} for edge devices that usually demand only part of a ViT's knowledge for specific tasks. Their task-specific accuracy on these edge devices is suboptimal. We discovered that small ViTs that focus on device-specific tasks can improve model accuracy and in the meantime, accelerate model inference. This paper presents NuWa, an approach that derives small ViTs from the base ViT for edge devices with specific task requirements. NuWa can transfer task-specific knowledge extracted from the base ViT into small ViTs that fully leverage constrained resources on edge devices to maximize model accuracy with inference latency assurance. Experiments with three base ViTs on three public datasets demonstrate that compared with state-of-the-art solutions, NuWa improves model accuracy by up to $\\text{11.83}\\%$ and accelerates model inference by 1.29$\\times$ - 2.79$\\times$. Code for reproduction is available at https://anonymous.4open.science/r/Task_Specific-3A5E."
      },
      {
        "id": "oai:arXiv.org:2504.03119v1",
        "title": "Graph Network Modeling Techniques for Visualizing Human Mobility Patterns",
        "link": "https://arxiv.org/abs/2504.03119",
        "author": "Sinjini Mitra, Anuj Srivastava, Avipsa Roy, Pavan Turaga",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03119v1 Announce Type: new \nAbstract: Human mobility analysis at urban-scale requires models to represent the complex nature of human movements, which in turn are affected by accessibility to nearby points of interest, underlying socioeconomic factors of a place, and local transport choices for people living in a geographic region. In this work, we represent human mobility and the associated flow of movements as a grapyh. Graph-based approaches for mobility analysis are still in their early stages of adoption and are actively being researched. The challenges of graph-based mobility analysis are multifaceted - the lack of sufficiently high-quality data to represent flows at high spatial and teporal resolution whereas, limited computational resources to translate large voluments of mobility data into a network structure, and scaling issues inherent in graph models etc. The current study develops a methodology by embedding graphs into a continuous space, which alleviates issues related to fast graph matching, graph time-series modeling, and visualization of mobility dynamics. Through experiments, we demonstrate how mobility data collected from taxicab trajectories could be transformed into network structures and patterns of mobility flow changes, and can be used for downstream tasks reporting approx 40% decrease in error on average in matched graphs vs unmatched ones."
      },
      {
        "id": "oai:arXiv.org:2504.03122v1",
        "title": "From Observation to Orientation: an Adaptive Integer Programming Approach to Intervention Design",
        "link": "https://arxiv.org/abs/2504.03122",
        "author": "Abdelmonem Elrefaey, Rong Pan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03122v1 Announce Type: new \nAbstract: Using both observational and experimental data, a causal discovery process can identify the causal relationships between variables. A unique adaptive intervention design paradigm is presented in this work, where causal directed acyclic graphs (DAGs) are for effectively recovered with practical budgetary considerations. In order to choose treatments that optimize information gain under these considerations, an iterative integer programming (IP) approach is proposed, which drastically reduces the number of experiments required. Simulations over a broad range of graph sizes and edge densities are used to assess the effectiveness of the suggested approach. Results show that the proposed adaptive IP approach achieves full causal graph recovery with fewer intervention iterations and variable manipulations than random intervention baselines, and it is also flexible enough to accommodate a variety of practical constraints."
      },
      {
        "id": "oai:arXiv.org:2504.03128v1",
        "title": "FontGuard: A Robust Font Watermarking Approach Leveraging Deep Font Knowledge",
        "link": "https://arxiv.org/abs/2504.03128",
        "author": "Kahim Wong, Jicheng Zhou, Kemou Li, Yain-Whar Si, Xiaowei Wu, Jiantao Zhou",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03128v1 Announce Type: new \nAbstract: The proliferation of AI-generated content brings significant concerns on the forensic and security issues such as source tracing, copyright protection, etc, highlighting the need for effective watermarking technologies. Font-based text watermarking has emerged as an effective solution to embed information, which could ensure copyright, traceability, and compliance of the generated text content. Existing font watermarking methods usually neglect essential font knowledge, which leads to watermarked fonts of low quality and limited embedding capacity. These methods are also vulnerable to real-world distortions, low-resolution fonts, and inaccurate character segmentation. In this paper, we introduce FontGuard, a novel font watermarking model that harnesses the capabilities of font models and language-guided contrastive learning. Unlike previous methods that focus solely on the pixel-level alteration, FontGuard modifies fonts by altering hidden style features, resulting in better font quality upon watermark embedding. We also leverage the font manifold to increase the embedding capacity of our proposed method by generating substantial font variants closely resembling the original font. Furthermore, in the decoder, we employ an image-text contrastive learning to reconstruct the embedded bits, which can achieve desirable robustness against various real-world transmission distortions. FontGuard outperforms state-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under synthetic, cross-media, and online social network distortions, respectively, while improving the visual quality by 52.7% in terms of LPIPS. Moreover, FontGuard uniquely allows the generation of watermarked fonts for unseen fonts without re-training the network. The code and dataset are available at https://github.com/KAHIMWONG/FontGuard."
      },
      {
        "id": "oai:arXiv.org:2504.03133v1",
        "title": "Joint Retrieval of Cloud properties using Attention-based Deep Learning Models",
        "link": "https://arxiv.org/abs/2504.03133",
        "author": "Zahid Hassan Tushar, Adeleke Ademakinwa, Jianwu Wang, Zhibo Zhang, Sanjay Purushotham",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03133v1 Announce Type: new \nAbstract: Accurate cloud property retrieval is vital for understanding cloud behavior and its impact on climate, including applications in weather forecasting, climate modeling, and estimating Earth's radiation balance. The Independent Pixel Approximation (IPA), a widely used physics-based approach, simplifies radiative transfer calculations by assuming each pixel is independent of its neighbors. While computationally efficient, IPA has significant limitations, such as inaccuracies from 3D radiative effects, errors at cloud edges, and ineffectiveness for overlapping or heterogeneous cloud fields. Recent AI/ML-based deep learning models have improved retrieval accuracy by leveraging spatial relationships across pixels. However, these models are often memory-intensive, retrieve only a single cloud property, or struggle with joint property retrievals. To overcome these challenges, we introduce CloudUNet with Attention Module (CAM), a compact UNet-based model that employs attention mechanisms to reduce errors in thick, overlapping cloud regions and a specialized loss function for joint retrieval of Cloud Optical Thickness (COT) and Cloud Effective Radius (CER). Experiments on a Large Eddy Simulation (LES) dataset show that our CAM model outperforms state-of-the-art deep learning methods, reducing mean absolute errors (MAE) by 34% for COT and 42% for CER, and achieving 76% and 86% lower MAE for COT and CER retrievals compared to the IPA method."
      },
      {
        "id": "oai:arXiv.org:2504.03135v1",
        "title": "Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion",
        "link": "https://arxiv.org/abs/2504.03135",
        "author": "Junkai Zhang, Bin Li, Shoujun Zhou, Yue Du",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03135v1 Announce Type: new \nAbstract: Medical Visual Question Answering (Med-VQA) answers clinical questions using medical images, aiding diagnosis. Designing the MedVQA system holds profound importance in assisting clinical diagnosis and enhancing diagnostic accuracy. Building upon this foundation, Hierarchical Medical VQA extends Medical VQA by organizing medical questions into a hierarchical structure and making level-specific predictions to handle fine-grained distinctions. Recently, many studies have proposed hierarchical MedVQA tasks and established datasets, However, several issues still remain: (1) imperfect hierarchical modeling leads to poor differentiation between question levels causing semantic fragmentation across hierarchies. (2) Excessive reliance on implicit learning in Transformer-based cross-modal self-attention fusion methods, which obscures crucial local semantic correlations in medical scenarios. To address these issues, this study proposes a HiCA-VQA method, including two modules: Hierarchical Prompting for fine-grained medical questions and Hierarchical Answer Decoders. The hierarchical prompting module pre-aligns hierarchical text prompts with image features to guide the model in focusing on specific image regions according to question types, while the hierarchical decoder performs separate predictions for questions at different levels to improve accuracy across granularities. The framework also incorporates a cross-attention fusion module where images serve as queries and text as key-value pairs. Experiments on the Rad-Restruct benchmark demonstrate that the HiCA-VQA framework better outperforms existing state-of-the-art methods in answering hierarchical fine-grained questions. This study provides an effective pathway for hierarchical visual question answering systems, advancing medical image understanding."
      },
      {
        "id": "oai:arXiv.org:2504.03136v1",
        "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
        "link": "https://arxiv.org/abs/2504.03136",
        "author": "Xin Jin, Simon Niklaus, Zhoutong Zhang, Zhihao Xia, Chunle Guo, Yuting Yang, Jiawen Chen, Chongyi Li",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03136v1 Announce Type: new \nAbstract: Denoising is a crucial step in many video processing pipelines such as in interactive editing, where high quality, speed, and user control are essential. While recent approaches achieve significant improvements in denoising quality by leveraging deep learning, they are prone to unexpected failures due to discrepancies between training data distributions and the wide variety of noise patterns found in real-world videos. These methods also tend to be slow and lack user control. In contrast, traditional denoising methods perform reliably on in-the-wild videos and run relatively quickly on modern hardware. However, they require manually tuning parameters for each input video, which is not only tedious but also requires skill. We bridge the gap between these two paradigms by proposing a differentiable denoising pipeline based on traditional methods. A neural network is then trained to predict the optimal denoising parameters for each specific input, resulting in a robust and efficient approach that also supports user control."
      },
      {
        "id": "oai:arXiv.org:2504.03140v1",
        "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video Diffusion Models",
        "link": "https://arxiv.org/abs/2504.03140",
        "author": "Xuran Ma, Yexin Liu, Yaofu Liu, Xianfeng Wu, Mingzhe Zheng, Zihao Wang, Ser-Nam Lim, Harry Yang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03140v1 Announce Type: new \nAbstract: Recent advances in diffusion models have demonstrated remarkable capabilities in video generation. However, the computational intensity remains a significant challenge for practical applications. While feature caching has been proposed to reduce the computational burden of diffusion models, existing methods typically overlook the heterogeneous significance of individual blocks, resulting in suboptimal reuse and degraded output quality. To this end, we address this gap by introducing ProfilingDiT, a novel adaptive caching strategy that explicitly disentangles foreground and background-focused blocks. Through a systematic analysis of attention distributions in diffusion models, we reveal a key observation: 1) Most layers exhibit a consistent preference for either foreground or background regions. 2) Predicted noise shows low inter-step similarity initially, which stabilizes as denoising progresses. This finding inspires us to formulate a selective caching strategy that preserves full computation for dynamic foreground elements while efficiently caching static background features. Our approach substantially reduces computational overhead while preserving visual fidelity. Extensive experiments demonstrate that our framework achieves significant acceleration (e.g., 2.01 times speedup for Wan2.1) while maintaining visual fidelity across comprehensive quality metrics, establishing a viable method for efficient video generation."
      },
      {
        "id": "oai:arXiv.org:2504.03151v1",
        "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
        "link": "https://arxiv.org/abs/2504.03151",
        "author": "Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, Jiarui Wu, Shu Yang, Daoan Zhang, Chen Chen, Lianggong Bruce Wen, Zhang Liu, Jiebo Luo, Chenliang Xu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03151v1 Announce Type: new \nAbstract: Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research."
      },
      {
        "id": "oai:arXiv.org:2504.03152v1",
        "title": "Safe Screening Rules for Group OWL Models",
        "link": "https://arxiv.org/abs/2504.03152",
        "author": "Runxue Bao, Quanchao Lu, Yanfu Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03152v1 Announce Type: new \nAbstract: Group Ordered Weighted $L_{1}$-Norm (Group OWL) regularized models have emerged as a useful procedure for high-dimensional sparse multi-task learning with correlated features. Proximal gradient methods are used as standard approaches to solving Group OWL models. However, Group OWL models usually suffer huge computational costs and memory usage when the feature size is large in the high-dimensional scenario. To address this challenge, in this paper, we are the first to propose the safe screening rule for Group OWL models by effectively tackling the structured non-separable penalty, which can quickly identify the inactive features that have zero coefficients across all the tasks. Thus, by removing the inactive features during the training process, we may achieve substantial computational gain and memory savings. More importantly, the proposed screening rule can be directly integrated with the existing solvers both in the batch and stochastic settings. Theoretically, we prove our screening rule is safe and also can be safely applied to the existing iterative optimization algorithms. Our experimental results demonstrate that our screening rule can effectively identify the inactive features and leads to a significant computational speedup without any loss of accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.03153v1",
        "title": "MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories",
        "link": "https://arxiv.org/abs/2504.03153",
        "author": "Natalie Tirabassi, Sathish A. P. Kumar, Sumit Jha, Arvind Ramanathan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03153v1 Announce Type: new \nAbstract: We propose MORAL (a multimodal reinforcement learning framework for decision making in autonomous laboratories) that enhances sequential decision-making in autonomous robotic laboratories through the integration of visual and textual inputs. Using the BridgeData V2 dataset, we generate fine-tuned image captions with a pretrained BLIP-2 vision-language model and combine them with visual features through an early fusion strategy. The fused representations are processed using Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) agents. Experimental results demonstrate that multimodal agents achieve a 20% improvement in task completion rates and significantly outperform visual-only and textual-only baselines after sufficient training. Compared to transformer-based and recurrent multimodal RL models, our approach achieves superior performance in cumulative reward and caption quality metrics (BLEU, METEOR, ROUGE-L). These results highlight the impact of semantically aligned language cues in enhancing agent learning efficiency and generalization. The proposed framework contributes to the advancement of multimodal reinforcement learning and embodied AI systems in dynamic, real-world environments."
      },
      {
        "id": "oai:arXiv.org:2504.03154v1",
        "title": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference",
        "link": "https://arxiv.org/abs/2504.03154",
        "author": "Junshan Hu, Jialiang Mao, Zhikang Liu, Zhongpu Xia, Peng Jia, Xianpeng Lang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03154v1 Announce Type: new \nAbstract: Conventional Vision-Language Models(VLMs) typically utilize a fixed number of vision tokens, regardless of task complexity. This one-size-fits-all strategy introduces notable inefficiencies: using excessive tokens leads to unnecessary computational overhead in simpler tasks, whereas insufficient tokens compromise fine-grained visual comprehension in more complex contexts. To overcome these limitations, we present TokenFLEX, an innovative and adaptable vision-language framework that encodes images into a variable number of tokens for efficient integration with a Large Language Model (LLM). Our approach is underpinned by two pivotal innovations. Firstly, we present a novel training paradigm that enhances performance across varying numbers of vision tokens by stochastically modulating token counts during training. Secondly, we design a lightweight vision token projector incorporating an adaptive pooling layer and SwiGLU, allowing for flexible downsampling of vision tokens and adaptive selection of features tailored to specific token counts. Comprehensive experiments reveal that TokenFLEX consistently outperforms its fixed-token counterparts, achieving notable performance gains across various token counts enhancements of 1.6%, 1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight vision-language benchmarks. These results underscore TokenFLEX's remarkable flexibility while maintaining high-performance vision-language understanding."
      },
      {
        "id": "oai:arXiv.org:2504.03159v1",
        "title": "Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction",
        "link": "https://arxiv.org/abs/2504.03159",
        "author": "Junlang Qian, Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Zepeng Zhai, Kezhi Mao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03159v1 Announce Type: new \nAbstract: Zero-shot text classification typically relies on prompt engineering, but the inherent prompt brittleness of large language models undermines its reliability. Minor changes in prompt can cause significant discrepancies in model performance. We attribute this prompt brittleness largely to the narrow focus on nexttoken probabilities in existing methods. To address this, we propose Placeholding Parallel Prediction (P3), a novel approach that predicts token probabilities across multiple positions and simulates comprehensive sampling of generation paths in a single run of a language model. Experiments show improved accuracy and up to 98% reduction in the standard deviation across prompts, boosting robustness. Even without a prompt, P3 maintains comparable performance, reducing the need for prompt engineering."
      },
      {
        "id": "oai:arXiv.org:2504.03162v1",
        "title": "Beyond Progress Measures: Theoretical Insights into the Mechanism of Grokking",
        "link": "https://arxiv.org/abs/2504.03162",
        "author": "Zihan Gu, Ruoyu Chen, Hua Zhang, Yue Hu, Xiaochun Cao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03162v1 Announce Type: new \nAbstract: Grokking, referring to the abrupt improvement in test accuracy after extended overfitting, offers valuable insights into the mechanisms of model generalization. Existing researches based on progress measures imply that grokking relies on understanding the optimization dynamics when the loss function is dominated solely by the weight decay term. However, we find that this optimization merely leads to token uniformity, which is not a sufficient condition for grokking. In this work, we investigate the grokking mechanism underlying the Transformer in the task of prime number operations. Based on theoretical analysis and experimental validation, we present the following insights: (i) The weight decay term encourages uniformity across all tokens in the embedding space when it is minimized. (ii) The occurrence of grokking is jointly determined by the uniformity of the embedding space and the distribution of the training dataset. Building on these insights, we provide a unified perspective for understanding various previously proposed progress measures and introduce a novel, concise, and effective progress measure that could trace the changes in test loss more accurately. Finally, to demonstrate the versatility of our theoretical framework, we design a dedicated dataset to validate our theory on ResNet-18, successfully showcasing the occurrence of grokking."
      },
      {
        "id": "oai:arXiv.org:2504.03163v1",
        "title": "Enhanced Penalty-based Bidirectional Reinforcement Learning Algorithms",
        "link": "https://arxiv.org/abs/2504.03163",
        "author": "Sai Gana Sandeep Pula, Sathish A. P. Kumar, Sumit Jha, Arvind Ramanathan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03163v1 Announce Type: new \nAbstract: This research focuses on enhancing reinforcement learning (RL) algorithms by integrating penalty functions to guide agents in avoiding unwanted actions while optimizing rewards. The goal is to improve the learning process by ensuring that agents learn not only suitable actions but also which actions to avoid. Additionally, we reintroduce a bidirectional learning approach that enables agents to learn from both initial and terminal states, thereby improving speed and robustness in complex environments. Our proposed Penalty-Based Bidirectional methodology is tested against Mani skill benchmark environments, demonstrating an optimality improvement of success rate of approximately 4% compared to existing RL implementations. The findings indicate that this integrated strategy enhances policy learning, adaptability, and overall performance in challenging scenarios"
      },
      {
        "id": "oai:arXiv.org:2504.03164v1",
        "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving",
        "link": "https://arxiv.org/abs/2504.03164",
        "author": "Kexin Tian, Jingrui Mao, Yunlong Zhang, Jiwan Jiang, Yang Zhou, Zhengzhong Tu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03164v1 Announce Type: new \nAbstract: Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning-key capabilities for autonomous driving-still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMs' performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning."
      },
      {
        "id": "oai:arXiv.org:2504.03165v1",
        "title": "Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation",
        "link": "https://arxiv.org/abs/2504.03165",
        "author": "Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03165v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge integration during large language model (LLM) inference in recent years. However, current RAG implementations face challenges in effectively addressing noise, repetition and redundancy in retrieved content, primarily due to their limited ability to exploit fine-grained inter-document relationships. To address these limitations, we propose an \\textbf{E}fficient \\textbf{D}ynamic \\textbf{C}lustering-based document \\textbf{C}ompression framework (\\textbf{EDC\\textsuperscript{2}-RAG}) that effectively utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The results show that this method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets can be found at https://github.com/Tsinghua-dhy/EDC-2-RAG."
      },
      {
        "id": "oai:arXiv.org:2504.03166v1",
        "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation",
        "link": "https://arxiv.org/abs/2504.03166",
        "author": "Hanbo Bi, Yingchao Feng, Boyuan Tong, Mengyu Wang, Haichen Yu, Yongqiang Mao, Hao Chang, Wenhui Diao, Peijin Wang, Yue Yu, Hanyang Peng, Yehong Zhang, Kun Fu, Xian Sun",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03166v1 Announce Type: new \nAbstract: The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning."
      },
      {
        "id": "oai:arXiv.org:2504.03168v1",
        "title": "Finding the Reflection Point: Unpadding Images to Remove Data Augmentation Artifacts in Large Open Source Image Datasets for Machine Learning",
        "link": "https://arxiv.org/abs/2504.03168",
        "author": "Lucas Choi, Ross Greer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03168v1 Announce Type: new \nAbstract: In this paper, we address a novel image restoration problem relevant to machine learning dataset curation: the detection and removal of noisy mirrored padding artifacts. While data augmentation techniques like padding are necessary for standardizing image dimensions, they can introduce artifacts that degrade model evaluation when datasets are repurposed across domains. We propose a systematic algorithm to precisely delineate the reflection boundary through a minimum mean squared error approach with thresholding and remove reflective padding. Our method effectively identifies the transition between authentic content and its mirrored counterpart, even in the presence of compression or interpolation noise. We demonstrate our algorithm's efficacy on the SHEL5k dataset, showing significant performance improvements in zero-shot object detection tasks using OWLv2, with average precision increasing from 0.47 to 0.61 for hard hat detection and from 0.68 to 0.73 for person detection. By addressing annotation inconsistencies and distorted objects in padded regions, our approach enhances dataset integrity, enabling more reliable model evaluation across computer vision tasks."
      },
      {
        "id": "oai:arXiv.org:2504.03169v1",
        "title": "REJEPA: A Novel Joint-Embedding Predictive Architecture for Efficient Remote Sensing Image Retrieval",
        "link": "https://arxiv.org/abs/2504.03169",
        "author": "Shabnam Choudhury, Yash Salunkhe, Sarthak Mehrotra, Biplab Banerjee",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03169v1 Announce Type: new \nAbstract: The rapid expansion of remote sensing image archives demands the development of strong and efficient techniques for content-based image retrieval (RS-CBIR). This paper presents REJEPA (Retrieval with Joint-Embedding Predictive Architecture), an innovative self-supervised framework designed for unimodal RS-CBIR. REJEPA utilises spatially distributed context token encoding to forecast abstract representations of target tokens, effectively capturing high-level semantic features and eliminating unnecessary pixel-level details. In contrast to generative methods that focus on pixel reconstruction or contrastive techniques that depend on negative pairs, REJEPA functions within feature space, achieving a reduction in computational complexity of 40-60% when compared to pixel-reconstruction baselines like Masked Autoencoders (MAE). To guarantee strong and varied representations, REJEPA incorporates Variance-Invariance-Covariance Regularisation (VICReg), which prevents encoder collapse by promoting feature diversity and reducing redundancy. The method demonstrates an estimated enhancement in retrieval accuracy of 5.1% on BEN-14K (S1), 7.4% on BEN-14K (S2), 6.0% on FMoW-RGB, and 10.1% on FMoW-Sentinel compared to prominent SSL techniques, including CSMAE-SESD, Mask-VLM, SatMAE, ScaleMAE, and SatMAE++, on extensive RS benchmarks BEN-14K (multispectral and SAR data), FMoW-RGB and FMoW-Sentinel. Through effective generalisation across sensor modalities, REJEPA establishes itself as a sensor-agnostic benchmark for efficient, scalable, and precise RS-CBIR, addressing challenges like varying resolutions, high object density, and complex backgrounds with computational efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.03170v1",
        "title": "Water Mapping and Change Detection Using Time Series Derived from the Continuous Monitoring of Land Disturbance Algorithm",
        "link": "https://arxiv.org/abs/2504.03170",
        "author": "Huong Pham, Samuel Cheng, Tao Hu, Chengbin Deng",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03170v1 Announce Type: new \nAbstract: Given the growing environmental challenges, accurate monitoring and prediction of changes in water bodies are essential for sustainable management and conservation. The Continuous Monitoring of Land Disturbance (COLD) algorithm provides a valuable tool for real-time analysis of land changes, such as deforestation, urban expansion, agricultural activities, and natural disasters. This capability enables timely interventions and more informed decision-making. This paper assesses the effectiveness of the algorithm to estimate water bodies and track pixel-level water trends over time. Our findings indicate that COLD-derived data can reliably estimate estimate water frequency during stable periods and delineate water bodies. Furthermore, it enables the evaluation of trends in water areas after disturbances, allowing for the determination of whether water frequency increases, decreases, or remains constant."
      },
      {
        "id": "oai:arXiv.org:2504.03171v1",
        "title": "Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion",
        "link": "https://arxiv.org/abs/2504.03171",
        "author": "Zeyang Zheng, Arman Hosseini, Dong Chen, Omid Shoghli, Arsalan Heydarian",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03171v1 Announce Type: new \nAbstract: The increasing adoption of electric scooters (e-scooters) in urban areas has coincided with a rise in traffic accidents and injuries, largely due to their small wheels, lack of suspension, and sensitivity to uneven surfaces. While deep learning-based object detection has been widely used to improve automobile safety, its application for e-scooter obstacle detection remains unexplored. This study introduces a novel ground obstacle detection system for e-scooters, integrating an RGB camera, and a depth camera to enhance real-time road hazard detection. Additionally, the Inertial Measurement Unit (IMU) measures linear vertical acceleration to identify surface vibrations, guiding the selection of six obstacle categories: tree branches, manhole covers, potholes, pine cones, non-directional cracks, and truncated domes. All sensors, including the RGB camera, depth camera, and IMU, are integrated within the Intel RealSense Camera D435i. A deep learning model powered by YOLO detects road hazards and utilizes depth data to estimate obstacle proximity. Evaluated on the seven hours of naturalistic riding dataset, the system achieves a high mean average precision (mAP) of 0.827 and demonstrates excellent real-time performance. This approach provides an effective solution to enhance e-scooter safety through advanced computer vision and data fusion. The dataset is accessible at https://zenodo.org/records/14583718, and the project code is hosted on https://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters."
      },
      {
        "id": "oai:arXiv.org:2504.03174v1",
        "title": "Multi-lingual Multi-turn Automated Red Teaming for LLMs",
        "link": "https://arxiv.org/abs/2504.03174",
        "author": "Abhishek Singhania, Christophe Dupuy, Shivam Mangale, Amani Namboori",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03174v1 Announce Type: new \nAbstract: Language Model Models (LLMs) have improved dramatically in the past few years, increasing their adoption and the scope of their capabilities over time. A significant amount of work is dedicated to ``model alignment'', i.e., preventing LLMs to generate unsafe responses when deployed into customer-facing applications. One popular method to evaluate safety risks is \\textit{red-teaming}, where agents attempt to bypass alignment by crafting elaborate prompts that trigger unsafe responses from a model. Standard human-driven red-teaming is costly, time-consuming and rarely covers all the recent features (e.g., multi-lingual, multi-modal aspects), while proposed automation methods only cover a small subset of LLMs capabilities (i.e., English or single-turn). We present Multi-lingual Multi-turn Automated Red Teaming (\\textbf{MM-ART}), a method to fully automate conversational, multi-lingual red-teaming operations and quickly identify prompts leading to unsafe responses. Through extensive experiments on different languages, we show the studied LLMs are on average 71\\% more vulnerable after a 5-turn conversation in English than after the initial turn. For conversations in non-English languages, models display up to 195\\% more safety vulnerabilities than the standard single-turn English approach, confirming the need for automated red-teaming methods matching LLMs capabilities."
      },
      {
        "id": "oai:arXiv.org:2504.03177v1",
        "title": "Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image",
        "link": "https://arxiv.org/abs/2504.03177",
        "author": "Yuki Kawana, Tatsuya Harada",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03177v1 Announce Type: new \nAbstract: We propose an end-to-end trainable, cross-category method for reconstructing multiple man-made articulated objects from a single RGBD image, focusing on part-level shape reconstruction and pose and kinematics estimation. We depart from previous works that rely on learning instance-level latent space, focusing on man-made articulated objects with predefined part counts. Instead, we propose a novel alternative approach that employs part-level representation, representing instances as combinations of detected parts. While our detect-then-group approach effectively handles instances with diverse part structures and various part counts, it faces issues of false positives, varying part sizes and scales, and an increasing model size due to end-to-end training. To address these challenges, we propose 1) test-time kinematics-aware part fusion to improve detection performance while suppressing false positives, 2) anisotropic scale normalization for part shape learning to accommodate various part sizes and scales, and 3) a balancing strategy for cross-refinement between feature space and output space to improve part detection while maintaining model size. Evaluation on both synthetic and real data demonstrates that our method successfully reconstructs variously structured multiple instances that previous works cannot handle, and outperforms prior works in shape reconstruction and kinematics estimation."
      },
      {
        "id": "oai:arXiv.org:2504.03181v1",
        "title": "MIMRS: A Survey on Masked Image Modeling in Remote Sensing",
        "link": "https://arxiv.org/abs/2504.03181",
        "author": "Shabnam Choudhury, Akhil Vasim, Michael Schmitt, Biplab Banerjee",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03181v1 Announce Type: new \nAbstract: Masked Image Modeling (MIM) is a self-supervised learning technique that involves masking portions of an image, such as pixels, patches, or latent representations, and training models to predict the missing information using the visible context. This approach has emerged as a cornerstone in self-supervised learning, unlocking new possibilities in visual understanding by leveraging unannotated data for pre-training. In remote sensing, MIM addresses challenges such as incomplete data caused by cloud cover, occlusions, and sensor limitations, enabling applications like cloud removal, multi-modal data fusion, and super-resolution. By synthesizing and critically analyzing recent advancements, this survey (MIMRS) is a pioneering effort to chart the landscape of mask image modeling in remote sensing. We highlight state-of-the-art methodologies, applications, and future research directions, providing a foundational review to guide innovation in this rapidly evolving field."
      },
      {
        "id": "oai:arXiv.org:2504.03185v1",
        "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
        "link": "https://arxiv.org/abs/2504.03185",
        "author": "Jaymari Chua, Chen Wang, Lina Yao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03185v1 Announce Type: new \nAbstract: Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings."
      },
      {
        "id": "oai:arXiv.org:2504.03187v1",
        "title": "On the Connection Between Diffusion Models and Molecular Dynamics",
        "link": "https://arxiv.org/abs/2504.03187",
        "author": "Liam Harcombe, Timothy T. Duignan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03187v1 Announce Type: new \nAbstract: Neural Network Potentials (NNPs) have emerged as a powerful tool for modelling atomic interactions with high accuracy and computational efficiency. Recently, denoising diffusion models have shown promise in NNPs by training networks to remove noise added to stable configurations, eliminating the need for force data during training. In this work, we explore the connection between noise and forces by providing a new, simplified mathematical derivation of their relationship. We also demonstrate how a denoising model can be implemented using a conventional MD software package interfaced with a standard NNP architecture. We demonstrate the approach by training a diffusion-based NNP to simulate a coarse-grained lithium chloride solution and employ data duplication to enhance model performance."
      },
      {
        "id": "oai:arXiv.org:2504.03188v1",
        "title": "Simultaneous Learning of Optimal Transports for Training All-to-All Flow-Based Condition Transfer Model",
        "link": "https://arxiv.org/abs/2504.03188",
        "author": "Kotaro Ikeda, Masanori Koyama, Jinzhe Zhang, Kohei Hayashi, Kenji Fukumizu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03188v1 Announce Type: new \nAbstract: In this paper, we propose a flow-based method for learning all-to-all transfer maps among conditional distributions, approximating pairwise optimal transport. The proposed method addresses the challenge of handling continuous conditions, which often involve a large set of conditions with sparse empirical observations per condition. We introduce a novel cost function that enables simultaneous learning of optimal transports for all pairs of conditional distributions. Our method is supported by a theoretical guarantee that, in the limit, it converges to pairwise optimal transports among infinite pairs of conditional distributions. The learned transport maps are subsequently used to couple data points in conditional flow matching. We demonstrate the effectiveness of this method on synthetic and benchmark datasets, as well as on chemical datasets where continuous physical properties are defined as conditions."
      },
      {
        "id": "oai:arXiv.org:2504.03191v1",
        "title": "Three Forensic Cues for JPEG AI Images",
        "link": "https://arxiv.org/abs/2504.03191",
        "author": "Sandra Bergmann, Fabian Brand, Christian Riess",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03191v1 Announce Type: new \nAbstract: The JPEG standard was vastly successful. Currently, the first AI-based compression method ``JPEG AI'' will be standardized. JPEG AI brings remarkable benefits. JPEG AI images exhibit impressive image quality at bitrates that are an order of magnitude lower than images compressed with traditional JPEG. However, forensic analysis of JPEG AI has to be completely re-thought: forensic tools for traditional JPEG do not transfer to JPEG AI, and artifacts from JPEG AI are easily confused with artifacts from artificially generated images (``DeepFakes''). This creates a need for novel forensic approaches to detection and distinction of JPEG AI images. In this work, we make a first step towards a forensic JPEG AI toolset. We propose three cues for forensic algorithms for JPEG AI. These algorithms address three forensic questions: first, we show that the JPEG AI preprocessing introduces correlations in the color channels that do not occur in uncompressed images. Second, we show that repeated compression of JPEG AI images leads to diminishing distortion differences. This can be used to detect recompression, in a spirit similar to some classic JPEG forensics methods. Third, we show that the quantization of JPEG AI images in the latent space can be used to distinguish real images with JPEG AI compression from synthetically generated images. The proposed methods are interpretable for a forensic analyst, and we hope that they inspire further research in the forensics of AI-compressed images."
      },
      {
        "id": "oai:arXiv.org:2504.03193v1",
        "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation",
        "link": "https://arxiv.org/abs/2504.03193",
        "author": "Xin Zhang, Robby T. Tan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03193v1 Announce Type: new \nAbstract: Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser."
      },
      {
        "id": "oai:arXiv.org:2504.03197v1",
        "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation",
        "link": "https://arxiv.org/abs/2504.03197",
        "author": "Jaewoo Park, Jungyang Park, Dongju Jang, Jiwan Chung, Byungwoo Yoo, Jaewoo Shin, Seonjoon Park, Taehyeong Kim, Youngjae Yu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03197v1 Announce Type: new \nAbstract: With the rapid advancement of mathematical reasoning capabilities in large language models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: visual explanation. In real-world instructional contexts, human tutors routinely employ visual aids-such as diagrams, markings, and highlights-to enhance conceptual clarity. To bridge this gap, we introduce a novel task of visual solution explanation, which requires not only solving problems but also generating explanations that incorporate newly introduced visual elements essential for understanding (e.g., auxiliary lines, annotations, or geometric constructions). To evaluate model performance on this task, we propose MathExplain, a multimodal benchmark consisting of 997 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that while some closed-source models demonstrate promising capabilities on visual solution-explaining, current open-source general-purpose models perform inconsistently, particularly in identifying relevant visual components and producing coherent keypoint-based explanations. We expect that visual solution-explaining and the MathExplain dataset will catalyze further research on multimodal LLMs in education and advance their deployment as effective, explanation-oriented AI tutors. Code and data will be released publicly."
      },
      {
        "id": "oai:arXiv.org:2504.03198v1",
        "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video",
        "link": "https://arxiv.org/abs/2504.03198",
        "author": "Jiaxin Guo, Wenzhen Dong, Tianyu Huang, Hao Ding, Ziyi Wang, Haomin Kuang, Qi Dou, Yun-Hui Liu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03198v1 Announce Type: new \nAbstract: Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's perception and therefore plays a vital role in various computer-assisted surgery tasks. However, achieving scale-consistent reconstruction remains an open challenge due to inherent issues in endoscopic videos, such as dynamic deformations and textureless surfaces. Despite recent advances, current methods either rely on calibration or instrument priors to estimate scale, or employ SfM-like multi-stage pipelines, leading to error accumulation and requiring offline optimization. In this paper, we present Endo3R, a unified 3D foundation model for online scale-consistent reconstruction from monocular surgical video, without any priors or extra optimization. Our model unifies the tasks by predicting globally aligned pointmaps, scale-consistent video depths, and camera parameters without any offline optimization. The core contribution of our method is expanding the capability of the recent pairwise reconstruction model to long-term incremental dynamic reconstruction by an uncertainty-aware dual memory mechanism. The mechanism maintains history tokens of both short-term dynamics and long-term spatial consistency. Notably, to tackle the highly dynamic nature of surgical scenes, we measure the uncertainty of tokens via Sampson distance and filter out tokens with high uncertainty. Regarding the scarcity of endoscopic datasets with ground-truth depth and camera poses, we further devise a self-supervised mechanism with a novel dynamics-aware flow loss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our superior performance in zero-shot surgical video depth prediction and camera pose estimation with online efficiency. Project page: https://wrld.github.io/Endo3R/."
      },
      {
        "id": "oai:arXiv.org:2504.03205v1",
        "title": "BondMatcher: H-Bond Stability Analysis in Molecular Systems",
        "link": "https://arxiv.org/abs/2504.03205",
        "author": "Thomas Daniel, Malgorzata Olejniczak, Julien Tierny",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03205v1 Announce Type: new \nAbstract: This application paper investigates the stability of hydrogen bonds (H-bonds), as characterized by the Quantum Theory of Atoms in Molecules (QTAIM). First, we contribute a database of 4544 electron densities associated to four isomers of water hexamers (the so-called Ring, Book, Cage and Prism), generated by distorting their equilibrium geometry under various structural perturbations, modeling the natural dynamic behavior of molecular systems. Second, we present a new stability measure, called bond occurrence rate, associating each bond path present at equilibrium with its rate of occurrence within the input ensemble. We also provide an algorithm, called BondMatcher, for its automatic computation, based on a tailored, geometry-aware partial isomorphism estimation between the extremum graphs of the considered electron densities. Our new stability measure allows for the automatic identification of densities lacking H-bond paths, enabling further visual inspections. Specifically, the topological analysis enabled by our framework corroborates experimental observations and provides refined geometrical criteria for characterizing the disappearance of H-bond paths. Our electron density database and our C++ implementation are available at this address: https://github.com/thom-dani/BondMatcher."
      },
      {
        "id": "oai:arXiv.org:2504.03206v1",
        "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
        "link": "https://arxiv.org/abs/2504.03206",
        "author": "Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03206v1 Announce Type: new \nAbstract: Effective conversational agents must be able to personalize their behavior to suit a user's preferences, personality, and attributes, whether they are assisting with writing tasks or operating in domains like education or healthcare. Current training methods like Reinforcement Learning from Human Feedback (RLHF) prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized interactions. Traditional approaches to personalization often rely on extensive user history, limiting their effectiveness for new or context-limited users. To overcome these limitations, we propose to incorporate an intrinsic motivation to improve the conversational agents's model of the user as an additional reward alongside multi-turn RLHF. This reward mechanism encourages the agent to actively elicit user traits by optimizing conversations to increase the accuracy of its user model. Consequently, the policy agent can deliver more personalized interactions through obtaining more information about the user. We applied our method both education and fitness settings, where LLMs teach concepts or recommend personalized strategies based on users' hidden learning style or lifestyle attributes. Using LLM-simulated users, our approach outperformed a multi-turn RLHF baseline in revealing information about the users' preferences, and adapting to them."
      },
      {
        "id": "oai:arXiv.org:2504.03209v1",
        "title": "PIONM: A Generalized Approach to Solving Density-Constrained Mean-Field Games Equilibrium under Modified Boundary Conditions",
        "link": "https://arxiv.org/abs/2504.03209",
        "author": "Jinwei Liu, Wang Yao, Xiao Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03209v1 Announce Type: new \nAbstract: Neural network-based methods are effective for solving equilibria in Mean-Field Games (MFGs), particularly in high-dimensional settings. However, solving the coupled partial differential equations (PDEs) in MFGs limits their applicability since solving coupled PDEs is computationally expensive. Additionally, modifying boundary conditions, such as the initial state distribution or terminal value function, necessitates extensive retraining, reducing scalability. To address these challenges, we propose a generalized framework, PIONM (Physics-Informed Neural Operator NF-MKV Net), which leverages physics-informed neural operators to solve MFGs equations. PIONM utilizes neural operators to compute MFGs equilibria for arbitrary boundary conditions. The method encodes boundary conditions as input features and trains the model to align them with density evolution, modeled using discrete-time normalizing flows. Once trained, the algorithm efficiently computes the density distribution at any time step for modified boundary condition, ensuring efficient adaptation to different boundary conditions in MFGs equilibria. Unlike traditional MFGs methods constrained by fixed coefficients, PIONM efficiently computes equilibria under varying boundary conditions, including obstacles, diffusion coefficients, initial densities, and terminal functions. PIONM can adapt to modified conditions while preserving density distribution constraints, demonstrating superior scalability and generalization capabilities compared to existing methods."
      },
      {
        "id": "oai:arXiv.org:2504.03211v1",
        "title": "Persuasive Calibration",
        "link": "https://arxiv.org/abs/2504.03211",
        "author": "Yiding Feng, Wei Tang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03211v1 Announce Type: new \nAbstract: We introduce and study the persuasive calibration problem, where a principal aims to provide trustworthy predictions about underlying events to a downstream agent to make desired decisions. We adopt the standard calibration framework that regulates predictions to be unbiased conditional on their own value, and thus, they can reliably be interpreted at the face value by the agent. Allowing a small calibration error budget, we aim to answer the following question: what is and how to compute the optimal predictor under this calibration error budget, especially when there exists incentive misalignment between the principal and the agent? We focus on standard Lt-norm Expected Calibration Error (ECE) metric.\n  We develop a general framework by viewing predictors as post-processed versions of perfectly calibrated predictors. Using this framework, we first characterize the structure of the optimal predictor. Specifically, when the principal's utility is event-independent and for L1-norm ECE, we show: (1) the optimal predictor is over-(resp. under-) confident for high (resp. low) true expected outcomes, while remaining perfectly calibrated in the middle; (2) the miscalibrated predictions exhibit a collinearity structure with the principal's utility function. On the algorithmic side, we provide a FPTAS for computing approximately optimal predictor for general principal utility and general Lt-norm ECE. Moreover, for the L1- and L-Infinity-norm ECE, we provide polynomial-time algorithms that compute the exact optimal predictor."
      },
      {
        "id": "oai:arXiv.org:2504.03214v1",
        "title": "Structured Knowledge Accumulation: The Principle of Entropic Least Action in Forward-Only Neural Learning",
        "link": "https://arxiv.org/abs/2504.03214",
        "author": "Bouarfa Mahi Quantiota",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03214v1 Announce Type: new \nAbstract: This paper aims to extend the Structured Knowledge Accumulation (SKA) framework recently proposed by \\cite{mahi2025ska}. We introduce two core concepts: the Tensor Net function and the characteristic time property of neural learning. First, we reinterpret the learning rate as a time step in a continuous system. This transforms neural learning from discrete optimization into continuous-time evolution. We show that learning dynamics remain consistent when the product of learning rate and iteration steps stays constant. This reveals a time-invariant behavior and identifies an intrinsic timescale of the network. Second, we define the Tensor Net function as a measure that captures the relationship between decision probabilities, entropy gradients, and knowledge change. Additionally, we define its zero-crossing as the equilibrium state between decision probabilities and entropy gradients. We show that the convergence of entropy and knowledge flow provides a natural stopping condition, replacing arbitrary thresholds with an information-theoretic criterion. We also establish that SKA dynamics satisfy a variational principle based on the Euler-Lagrange equation. These findings extend SKA into a continuous and self-organizing learning model. The framework links computational learning with physical systems that evolve by natural laws. By understanding learning as a time-based process, we open new directions for building efficient, robust, and biologically-inspired AI systems."
      },
      {
        "id": "oai:arXiv.org:2504.03219v1",
        "title": "From ChatGPT to DeepSeek AI: A Comprehensive Analysis of Evolution, Deviation, and Future Implications in AI-Language Models",
        "link": "https://arxiv.org/abs/2504.03219",
        "author": "Simrandeep Singh, Shreya Bansal, Abdulmotaleb El Saddik, Mukesh Saini",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03219v1 Announce Type: new \nAbstract: The rapid advancement of artificial intelligence (AI) has reshaped the field of natural language processing (NLP), with models like OpenAI ChatGPT and DeepSeek AI. Although ChatGPT established a strong foundation for conversational AI, DeepSeek AI introduces significant improvements in architecture, performance, and ethical considerations. This paper presents a detailed analysis of the evolution from ChatGPT to DeepSeek AI, highlighting their technical differences, practical applications, and broader implications for AI development. To assess their capabilities, we conducted a case study using a predefined set of multiple choice questions in various domains, evaluating the strengths and limitations of each model. By examining these aspects, we provide valuable insight into the future trajectory of AI, its potential to transform industries, and key research directions for improving AI-driven language models."
      },
      {
        "id": "oai:arXiv.org:2504.03220v1",
        "title": "Learning Lie Group Generators from Trajectories",
        "link": "https://arxiv.org/abs/2504.03220",
        "author": "Lifan Hu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03220v1 Announce Type: new \nAbstract: This work investigates the inverse problem of generator recovery in matrix Lie groups from discretized trajectories. Let $G$ be a real matrix Lie group and $\\mathfrak{g} = \\text{Lie}(G)$ its corresponding Lie algebra. A smooth trajectory $\\gamma($t$)$ generated by a fixed Lie algebra element $\\xi \\in \\mathfrak{g}$ follows the exponential flow $\\gamma($t$) = g_0 \\cdot \\exp(t \\xi)$. The central task addressed in this work is the reconstruction of such a latent generator $\\xi$ from a discretized sequence of poses $ \\{g_0, g_1, \\dots, g_T\\} \\subset G$, sampled at uniform time intervals. This problem is formulated as a data-driven regression from normalized sequences of discrete Lie algebra increments $\\log\\left(g_{t}^{-1} g_{t+1}\\right)$ to the constant generator $\\xi \\in \\mathfrak{g}$. A feedforward neural network is trained to learn this mapping across several groups, including $\\text{SE(2)}, \\text{SE(3)}, \\text{SO(3)}, and \\text{SL(2,$\\mathbb{R})$}$. It demonstrates strong empirical accuracy under both clean and noisy conditions, which validates the viability of data-driven recovery of Lie group generators using shallow neural architectures. This is Lie-RL GitHub Repo https://github.com/Anormalm/LieRL-on-Trajectories. Feel free to make suggestions and collaborations!"
      },
      {
        "id": "oai:arXiv.org:2504.03221v1",
        "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics",
        "link": "https://arxiv.org/abs/2504.03221",
        "author": "Jungpil Shin, Abu Saleh Musa Miah, Sota Konnai, Shu Hoshitaka, Pankoo Kim",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03221v1 Announce Type: new \nAbstract: Hand gesture recognition using multichannel surface electromyography (sEMG) is challenging due to unstable predictions and inefficient time-varying feature enhancement. To overcome the lack of signal based time-varying feature problems, we propose a lightweight squeeze-excitation deep learning-based multi stream spatial temporal dynamics time-varying feature extraction approach to build an effective sEMG-based hand gesture recognition system. Each branch of the proposed model was designed to extract hierarchical features, capturing both global and detailed spatial-temporal relationships to ensure feature effectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN), focuses on capturing long-term temporal dependencies by modelling past and future temporal contexts, providing a holistic view of gesture dynamics. The second branch, incorporating a 1D Convolutional layer, separable CNN, and Squeeze-and-Excitation (SE) block, efficiently extracts spatial-temporal features while emphasizing critical feature channels, enhancing feature relevance. The third branch, combining a Temporal Convolutional Network (TCN) and Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships and time-varying patterns. Outputs from all branches are fused using concatenation to capture subtle variations in the data and then refined with a channel attention module, selectively focusing on the most informative features while improving computational efficiency. The proposed model was tested on the Ninapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%, and 93.34%, respectively. These results demonstrate the capability of the system to handle complex sEMG dynamics, offering advancements in prosthetic limb control and human-machine interface technologies with significant implications for assistive technologies."
      },
      {
        "id": "oai:arXiv.org:2504.03230v1",
        "title": "Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection",
        "link": "https://arxiv.org/abs/2504.03230",
        "author": "Yasmine Mustafa, Mohamed Elmahallawy, Tie Luo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03230v1 Announce Type: new \nAbstract: Alzheimer's disease (AD) leads to progressive cognitive decline, making early detection crucial for effective intervention. While deep learning models have shown high accuracy in AD diagnosis, their lack of interpretability limits clinical trust and adoption. This paper introduces a novel pre-model approach leveraging Jacobian Maps (JMs) within a multi-modal framework to enhance explainability and trustworthiness in AD detection. By capturing localized brain volume changes, JMs establish meaningful correlations between model predictions and well-known neuroanatomical biomarkers of AD. We validate JMs through experiments comparing a 3D CNN trained on JMs versus on traditional preprocessed data, which demonstrates superior accuracy. We also employ 3D Grad-CAM analysis to provide both visual and quantitative insights, further showcasing improved interpretability and diagnostic reliability."
      },
      {
        "id": "oai:arXiv.org:2504.03234v1",
        "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning",
        "link": "https://arxiv.org/abs/2504.03234",
        "author": "Junjie Yang, Ke Lin, Xing Yu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03234v1 Announce Type: new \nAbstract: Chain of Thought (CoT) reasoning enhances language models' performance but often leads to inefficient \"overthinking\" on simple problems. We identify that existing approaches directly penalizing reasoning length fail to account for varying problem complexity. Our approach constructs rewards through length and quality comparisons, guided by theoretical assumptions that jointly enhance solution correctness with conciseness. Moreover, we further demonstrate our method to fuzzy tasks where ground truth is unavailable. Experiments across multiple reasoning benchmarks demonstrate that our method maintains accuracy while generating significantly more concise explanations, effectively teaching models to \"think when needed.\""
      },
      {
        "id": "oai:arXiv.org:2504.03235v1",
        "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage",
        "link": "https://arxiv.org/abs/2504.03235",
        "author": "Ibne Farabi Shihab, Anuj Sharma",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03235v1 Announce Type: new \nAbstract: Traffic crash detection in long-form surveillance videos is critical for emergency response and infrastructure planning but remains difficult due to the brief and rare nature of crash events. We introduce HybridMamba, a novel architecture that combines visual transformers with state-space temporal modeling to achieve accurate crash time localization. Our method uses multi-level token compression and hierarchical temporal processing to remain computationally efficient without sacrificing temporal resolution. Evaluated on a large-scale dataset from the Iowa Department of Transportation, HybridMamba achieves a mean absolute error of 1.50 seconds, with 65.2 percent of predictions within one second of the ground truth. It outperforms recent video-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds, while using significantly fewer parameters. Our results demonstrate strong generalization across videos ranging from 2 to 40 minutes in diverse conditions. HybridMamba offers a robust and efficient solution for fine-grained temporal localization in traffic surveillance. The code will be released upon publication."
      },
      {
        "id": "oai:arXiv.org:2504.03241v1",
        "title": "Rotation Invariance in Floor Plan Digitization using Zernike Moments",
        "link": "https://arxiv.org/abs/2504.03241",
        "author": "Marius Graumann (German Aerospace Center), Jan Marius St\\\"urmer (German Aerospace Center), Tobias Koch (German Aerospace Center)",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03241v1 Announce Type: new \nAbstract: Nowadays, a lot of old floor plans exist in printed form or are stored as scanned raster images. Slight rotations or shifts may occur during scanning. Bringing floor plans of this form into a machine readable form to enable further use, still poses a problem. Therefore, we propose an end-to-end pipeline that pre-processes the image and leverages a novel approach to create a region adjacency graph (RAG) from the pre-processed image and predict its nodes. By incorporating normalization steps into the RAG feature extraction, we significantly improved the rotation invariance of the RAG feature calculation. Moreover, applying our method leads to an improved F1 score and IoU on rotated data. Furthermore, we proposed a wall splitting algorithm for partitioning walls into segments associated with the corresponding rooms."
      },
      {
        "id": "oai:arXiv.org:2504.03249v1",
        "title": "Robot Localization Using a Learned Keypoint Detector and Descriptor with a Floor Camera and a Feature Rich Industrial Floor",
        "link": "https://arxiv.org/abs/2504.03249",
        "author": "Piet Br\\\"ommel, Dominik Br\\\"amer, Oliver Urbann, Diana Kleingarn",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03249v1 Announce Type: new \nAbstract: The localization of moving robots depends on the availability of good features from the environment. Sensor systems like Lidar are popular, but unique features can also be extracted from images of the ground. This work presents the Keypoint Localization Framework (KOALA), which utilizes deep neural networks that extract sufficient features from an industrial floor for accurate localization without having readable markers. For this purpose, we use a floor covering that can be produced as cheaply as common industrial floors. Although we do not use any filtering, prior, or temporal information, we can estimate our position in 75.7 % of all images with a mean position error of 2 cm and a rotation error of 2.4 %. Thus, the robot kidnapping problem can be solved with high precision in every frame, even while the robot is moving. Furthermore, we show that our framework with our detector and descriptor combination is able to outperform comparable approaches."
      },
      {
        "id": "oai:arXiv.org:2504.03254v1",
        "title": "SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding",
        "link": "https://arxiv.org/abs/2504.03254",
        "author": "Yimin Wei, Aoran Xiao, Yexian Ren, Yuting Zhu, Hongruixuan Chen, Junshi Xia, Naoto Yokoya",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03254v1 Announce Type: new \nAbstract: Synthetic Aperture Radar (SAR) is a crucial remote sensing technology, enabling all-weather, day-and-night observation with strong surface penetration for precise and continuous environmental monitoring and analysis. However, SAR image interpretation remains challenging due to its complex physical imaging mechanisms and significant visual disparities from human perception. Recently, Vision-Language Models (VLMs) have demonstrated remarkable success in RGB image understanding, offering powerful open-vocabulary interpretation and flexible language interaction. However, their application to SAR images is severely constrained by the absence of SAR-specific knowledge in their training distributions, leading to suboptimal performance. To address this limitation, we introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR image understanding, with a primary focus on integrating SAR with textual modality. SARLANG-1M comprises more than 1 million high-quality SAR image-text pairs collected from over 59 cities worldwide. It features hierarchical resolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions (including both concise and detailed captions), diverse remote sensing categories (1,696 object types and 16 land cover classes), and multi-task question-answering pairs spanning seven applications and 1,012 question types. Extensive experiments on mainstream VLMs demonstrate that fine-tuning with SARLANG-1M significantly enhances their performance in SAR image interpretation, reaching performance comparable to human experts. The dataset and code will be made publicly available at https://github.com/Jimmyxichen/SARLANG-1M."
      },
      {
        "id": "oai:arXiv.org:2504.03258v1",
        "title": "TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking",
        "link": "https://arxiv.org/abs/2504.03258",
        "author": "Shuxiao Ding, Yutong Yang, Julian Wiederer, Markus Braun, Peizheng Li, Juergen Gall, Bin Yang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03258v1 Announce Type: new \nAbstract: Query denoising has become a standard training strategy for DETR-based detectors by addressing the slow convergence issue. Besides that, query denoising can be used to increase the diversity of training samples for modeling complex scenarios which is critical for Multi-Object Tracking (MOT), showing its potential in MOT application. Existing approaches integrate query denoising within the tracking-by-attention paradigm. However, as the denoising process only happens within the single frame, it cannot benefit the tracker to learn temporal-related information. In addition, the attention mask in query denoising prevents information exchange between denoising and object queries, limiting its potential in improving association using self-attention. To address these issues, we propose TQD-Track, which introduces Temporal Query Denoising (TQD) tailored for MOT, enabling denoising queries to carry temporal information and instance-specific feature representation. We introduce diverse noise types onto denoising queries that simulate real-world challenges in MOT. We analyze our proposed TQD for different tracking paradigms, and find out the paradigm with explicit learned data association module, e.g. tracking-by-detection or alternating detection and association, benefit from TQD by a larger margin. For these paradigms, we further design an association mask in the association module to ensure the consistent interaction between track and detection queries as during inference. Extensive experiments on the nuScenes dataset demonstrate that our approach consistently enhances different tracking methods by only changing the training process, especially the paradigms with explicit association module."
      },
      {
        "id": "oai:arXiv.org:2504.03292v1",
        "title": "FaR: Enhancing Multi-Concept Text-to-Image Diffusion via Concept Fusion and Localized Refinement",
        "link": "https://arxiv.org/abs/2504.03292",
        "author": "Gia-Nghia Tran, Quang-Huy Che, Trong-Tai Dam Vu, Bich-Nga Pham, Vinh-Tiep Nguyen, Trung-Nghia Le, Minh-Triet Tran",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03292v1 Announce Type: new \nAbstract: Generating multiple new concepts remains a challenging problem in the text-to-image task. Current methods often overfit when trained on a small number of samples and struggle with attribute leakage, particularly for class-similar subjects (e.g., two specific dogs). In this paper, we introduce Fuse-and-Refine (FaR), a novel approach that tackles these challenges through two key contributions: Concept Fusion technique and Localized Refinement loss function. Concept Fusion systematically augments the training data by separating reference subjects from backgrounds and recombining them into composite images to increase diversity. This augmentation technique tackles the overfitting problem by mitigating the narrow distribution of the limited training samples. In addition, Localized Refinement loss function is introduced to preserve subject representative attributes by aligning each concept's attention map to its correct region. This approach effectively prevents attribute leakage by ensuring that the diffusion model distinguishes similar subjects without mixing their attention maps during the denoising process. By fine-tuning specific modules at the same time, FaR balances the learning of new concepts with the retention of previously learned knowledge. Empirical results show that FaR not only prevents overfitting and attribute leakage while maintaining photorealism, but also outperforms other state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.03295v1",
        "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task",
        "link": "https://arxiv.org/abs/2504.03295",
        "author": "Bingqian Wang, Quan Fang, Jiachen Sun, Xiaoxiao Ma",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03295v1 Announce Type: new \nAbstract: Formulating statements that support diverse or controversial stances on specific topics is vital for platforms that enable user expression, reshape political discourse, and drive social critique and information dissemination. With the rise of Large Language Models (LLMs), controllable text generation towards specific stances has become a promising research area with applications in shaping public opinion and commercial marketing. However, current datasets often focus solely on pure texts, lacking multimodal content and effective context, particularly in the context of stance detection. In this paper, we formally define and study the new problem of stance-driven controllable content generation for tweets with text and images, where given a multimodal post (text and image/video), a model generates a stance-controlled response. To this end, we create the Multimodal Stance Generation Dataset (StanceGen2024), the first resource explicitly designed for multimodal stance-controllable text generation in political discourse. It includes posts and user comments from the 2024 U.S. presidential election, featuring text, images, videos, and stance annotations to explore how multimodal political content shapes stance expression. Furthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework that integrates weighted fusion of multimodal features and stance guidance to improve semantic consistency and stance control. We release the dataset and code (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and further research."
      },
      {
        "id": "oai:arXiv.org:2504.03302v1",
        "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models",
        "link": "https://arxiv.org/abs/2504.03302",
        "author": "Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03302v1 Announce Type: new \nAbstract: Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&amp;B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility."
      },
      {
        "id": "oai:arXiv.org:2504.03306v1",
        "title": "Multi-Flow: Multi-View-Enriched Normalizing Flows for Industrial Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.03306",
        "author": "Mathis Kruse, Bodo Rosenhahn",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03306v1 Announce Type: new \nAbstract: With more well-performing anomaly detection methods proposed, many of the single-view tasks have been solved to a relatively good degree. However, real-world production scenarios often involve complex industrial products, whose properties may not be fully captured by one single image. While normalizing flow based approaches already work well in single-camera scenarios, they currently do not make use of the priors in multi-view data. We aim to bridge this gap by using these flow-based models as a strong foundation and propose Multi-Flow, a novel multi-view anomaly detection method. Multi-Flow makes use of a novel multi-view architecture, whose exact likelihood estimation is enhanced by fusing information across different views. For this, we propose a new cross-view message-passing scheme, letting information flow between neighboring views. We empirically validate it on the real-world multi-view data set Real-IAD and reach a new state-of-the-art, surpassing current baselines in both image-wise and sample-wise anomaly detection tasks."
      },
      {
        "id": "oai:arXiv.org:2504.03312v1",
        "title": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices",
        "link": "https://arxiv.org/abs/2504.03312",
        "author": "Lu\\'is Couto Seller, \\'I\\~nigo Sanz Torres, Adri\\'an Vogel-Fern\\'andez, Carlos Gonz\\'alez Carballo, Pedro Miguel S\\'anchez S\\'anchez, Adri\\'an Carruana Mart\\'in, Enrique de Miguel Ambite",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03312v1 Announce Type: new \nAbstract: Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance"
      },
      {
        "id": "oai:arXiv.org:2504.03313v1",
        "title": "Steerable Anatomical Shape Synthesis with Implicit Neural Representations",
        "link": "https://arxiv.org/abs/2504.03313",
        "author": "Bram de Wilde, Max T. Rietberg, Guillaume Lajoinie, Jelmer M. Wolterink",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03313v1 Announce Type: new \nAbstract: Generative modeling of anatomical structures plays a crucial role in virtual imaging trials, which allow researchers to perform studies without the costs and constraints inherent to in vivo and phantom studies. For clinical relevance, generative models should allow targeted control to simulate specific patient populations rather than relying on purely random sampling. In this work, we propose a steerable generative model based on implicit neural representations. Implicit neural representations naturally support topology changes, making them well-suited for anatomical structures with varying topology, such as the thyroid. Our model learns a disentangled latent representation, enabling fine-grained control over shape variations. Evaluation includes reconstruction accuracy and anatomical plausibility. Our results demonstrate that the proposed model achieves high-quality shape generation while enabling targeted anatomical modifications."
      },
      {
        "id": "oai:arXiv.org:2504.03327v1",
        "title": "Optimal Embedding Guided Negative Sample Generation for Knowledge Graph Link Prediction",
        "link": "https://arxiv.org/abs/2504.03327",
        "author": "Makoto Takamoto, Daniel O\\~noro-Rubio, Wiem Ben Rim, Takashi Maruyama, Bhushan Kotnis",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03327v1 Announce Type: new \nAbstract: Knowledge graph embedding (KGE) models encode the structural information of knowledge graphs to predicting new links. Effective training of these models requires distinguishing between positive and negative samples with high precision. Although prior research has shown that improving the quality of negative samples can significantly enhance model accuracy, identifying high-quality negative samples remains a challenging problem. This paper theoretically investigates the condition under which negative samples lead to optimal KG embedding and identifies a sufficient condition for an effective negative sample distribution. Based on this theoretical foundation, we propose \\textbf{E}mbedding \\textbf{MU}tation (\\textsc{EMU}), a novel framework that \\emph{generates} negative samples satisfying this condition, in contrast to conventional methods that focus on \\emph{identifying} challenging negative samples within the training data. Importantly, the simplicity of \\textsc{EMU} ensures seamless integration with existing KGE models and negative sampling methods. To evaluate its efficacy, we conducted comprehensive experiments across multiple datasets. The results consistently demonstrate significant improvements in link prediction performance across various KGE models and negative sampling methods. Notably, \\textsc{EMU} enables performance improvements comparable to those achieved by models with embedding dimension five times larger. An implementation of the method and experiments are available at https://github.com/nec-research/EMU-KG."
      },
      {
        "id": "oai:arXiv.org:2504.03334v1",
        "title": "Data Augmentation of Time-Series Data in Human Movement Biomechanics: A Scoping Review",
        "link": "https://arxiv.org/abs/2504.03334",
        "author": "Christina Halmich, Lucas H\\\"oschler, Christoph Schranz, Christian Borgelt",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03334v1 Announce Type: new \nAbstract: The integration of machine learning and deep learning has transformed data analytics in biomechanics, enabled by extensive wearable sensor data. However, the field faces challenges such as limited large-scale datasets and high data acquisition costs, which hinder the development of robust algorithms. Data augmentation techniques show promise in addressing these issues, but their application to biomechanical time-series data requires comprehensive evaluation.\n  This scoping review investigates data augmentation methods for time-series data in the biomechanics domain. It analyzes current approaches for augmenting and generating time-series datasets, evaluates their effectiveness, and offers recommendations for applying these techniques in biomechanics.\n  Four databases, PubMed, IEEE Xplore, Scopus, and Web of Science, were searched for studies published between 2013 and 2024. Following PRISMA-ScR guidelines, a two-stage screening identified 21 relevant publications.\n  Results show that there is no universally preferred method for augmenting biomechanical time-series data; instead, methods vary based on study objectives. A major issue identified is the absence of soft tissue artifacts in synthetic data, leading to discrepancies referred to as the synthetic gap. Moreover, many studies lack proper evaluation of augmentation methods, making it difficult to assess their effects on model performance and data quality.\n  This review highlights the critical role of data augmentation in addressing limited dataset availability and improving model generalization in biomechanics. Tailoring augmentation strategies to the characteristics of biomechanical data is essential for advancing predictive modeling. A better understanding of how different augmentation methods impact data quality and downstream tasks will be key to developing more effective and realistic techniques."
      },
      {
        "id": "oai:arXiv.org:2504.03337v1",
        "title": "QIRL: Boosting Visual Question Answering via Optimized Question-Image Relation Learning",
        "link": "https://arxiv.org/abs/2504.03337",
        "author": "Quanxing Xu, Ling Zhou, Xian Zhong, Feifei Zhang, Rubing Huang, Chia-Wen Lin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03337v1 Announce Type: new \nAbstract: Existing debiasing approaches in Visual Question Answering (VQA) primarily focus on enhancing visual learning, integrating auxiliary models, or employing data augmentation strategies. However, these methods exhibit two major drawbacks. First, current debiasing techniques fail to capture the superior relation between images and texts because prevalent learning frameworks do not enable models to extract deeper correlations from highly contrasting samples. Second, they do not assess the relevance between the input question and image during inference, as no prior work has examined the degree of input relevance in debiasing studies. Motivated by these limitations, we propose a novel framework, Optimized Question-Image Relation Learning (QIRL), which employs a generation-based self-supervised learning strategy. Specifically, two modules are introduced to address the aforementioned issues. The Negative Image Generation (NIG) module automatically produces highly irrelevant question-image pairs during training to enhance correlation learning, while the Irrelevant Sample Identification (ISI) module improves model robustness by detecting and filtering irrelevant inputs, thereby reducing prediction errors. Furthermore, to validate our concept of reducing output errors through filtering unrelated question-image inputs, we propose a specialized metric to evaluate the performance of the ISI module. Notably, our approach is model-agnostic and can be integrated with various VQA models. Extensive experiments on VQA-CPv2 and VQA-v2 demonstrate the effectiveness and generalization ability of our method. Among data augmentation strategies, our approach achieves state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2504.03338v1",
        "title": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task",
        "link": "https://arxiv.org/abs/2504.03338",
        "author": "Z\\'ebulon Goriely",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03338v1 Announce Type: new \nAbstract: Language models provide a key framework for studying linguistic theories based on prediction, but phonological analysis using large language models (LLMs) is difficult; there are few phonological benchmarks beyond English and the standard input representation used in LLMs (subwords of graphemes) is not suitable for analyzing the representation of phonemes. In this work, we demonstrate how word segmentation can be used as a phonological probing task, allowing us to study the representations learned by phoneme-based language models trained on child-directed speech across 31 languages. Following computational models of word segmentation, we present unsupervised methods for extracting word boundaries from a trained model using the observation that prediction-error peaks at the start of words. We also use linear probes to identify that these models implicitly track word boundaries, even when they do not appear in training. This cross-lingual work corroborates statistical learning theories of acquisition and empirically motivates new methods for training subword tokenizers."
      },
      {
        "id": "oai:arXiv.org:2504.03342v1",
        "title": "EOOD: Entropy-based Out-of-distribution Detection",
        "link": "https://arxiv.org/abs/2504.03342",
        "author": "Guide Yang, Chao Hou, Weilong Peng, Xiang Fang, Yongwei Nie, Peican Zhu, Keke Tang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03342v1 Announce Type: new \nAbstract: Deep neural networks (DNNs) often exhibit overconfidence when encountering out-of-distribution (OOD) samples, posing significant challenges for deployment. Since DNNs are trained on in-distribution (ID) datasets, the information flow of ID samples through DNNs inevitably differs from that of OOD samples. In this paper, we propose an Entropy-based Out-Of-distribution Detection (EOOD) framework. EOOD first identifies specific block where the information flow differences between ID and OOD samples are more pronounced, using both ID and pseudo-OOD samples. It then calculates the conditional entropy on the selected block as the OOD confidence score. Comprehensive experiments conducted across various ID and OOD settings demonstrate the effectiveness of EOOD in OOD detection and its superiority over state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2504.03349v1",
        "title": "Meta-DAN: towards an efficient prediction strategy for page-level handwritten text recognition",
        "link": "https://arxiv.org/abs/2504.03349",
        "author": "Denis Coquenet",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03349v1 Announce Type: new \nAbstract: Recent advances in text recognition led to a paradigm shift for page-level recognition, from multi-step segmentation-based approaches to end-to-end attention-based ones. However, the na\\\"ive character-level autoregressive decoding process results in long prediction times: it requires several seconds to process a single page image on a modern GPU. We propose the Meta Document Attention Network (Meta-DAN) as a novel decoding strategy to reduce the prediction time while enabling a better context modeling. It relies on two main components: windowed queries, to process several transformer queries altogether, enlarging the context modeling with near future; and multi-token predictions, whose goal is to predict several tokens per query instead of only the next one. We evaluate the proposed approach on 10 full-page handwritten datasets and demonstrate state-of-the-art results on average in terms of character error rate. Source code and weights of trained models are available at https://github.com/FactoDeepLearning/meta_dan."
      },
      {
        "id": "oai:arXiv.org:2504.03352v1",
        "title": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings",
        "link": "https://arxiv.org/abs/2504.03352",
        "author": "Kaustubh Shivshankar Shejole, Pushpak Bhattacharyya",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03352v1 Announce Type: new \nAbstract: Stereotypes are known to be highly pernicious, making their detection critically important. However, current research predominantly focuses on detecting and evaluating stereotypical biases in LLMs, leaving the study of stereotypes in its early stages. Many studies have failed to clearly distinguish between stereotypes and stereotypical biases, which has significantly slowed progress in advancing research in this area. Stereotype and anti-stereotype detection is a problem that requires knowledge of society; hence, it is one of the most difficult areas in Responsible AI. This work investigates this task, where we propose a four-tuple definition and provide precise terminology distinguishing stereotype, anti-stereotype, stereotypical bias, and bias, offering valuable insights into their various aspects. In this paper, we propose StereoDetect, a high-quality benchmarking dataset curated for this task by optimally utilizing current datasets such as StereoSet and WinoQueer, involving a manual verification process and the transfer of semantic information. We demonstrate that language models for reasoning with fewer than 10B parameters often get confused when detecting anti-stereotypes. We also demonstrate the critical importance of well-curated datasets by comparing our model with other current models for stereotype detection. The dataset and code is available at https://github.com/KaustubhShejole/StereoDetect."
      },
      {
        "id": "oai:arXiv.org:2504.03359v1",
        "title": "A metrological framework for uncertainty evaluation in machine learning classification models",
        "link": "https://arxiv.org/abs/2504.03359",
        "author": "Samuel Bilson, Maurice Cox, Anna Pustogvar, Andrew Thompson",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03359v1 Announce Type: new \nAbstract: Machine learning (ML) classification models are increasingly being used in a wide range of applications where it is important that predictions are accompanied by uncertainties, including in climate and earth observation, medical diagnosis and bioaerosol monitoring. The output of an ML classification model is a type of categorical variable known as a nominal property in the International Vocabulary of Metrology (VIM). However, concepts related to uncertainty evaluation for nominal properties are not defined in the VIM, nor is such evaluation addressed by the Guide to the Expression of Uncertainty in Measurement (GUM). In this paper we propose a metrological conceptual uncertainty evaluation framework for ML classification, and illustrate its use in the context of two applications that exemplify the issues and have significant societal impact, namely, climate and earth observation and medical diagnosis. Our framework would enable an extension of the VIM and GUM to uncertainty for nominal properties, which would make both applicable to ML classification models."
      },
      {
        "id": "oai:arXiv.org:2504.03376v1",
        "title": "FLAIRBrainSeg: Fine-grained brain segmentation using FLAIR MRI only",
        "link": "https://arxiv.org/abs/2504.03376",
        "author": "Edern Le Bot, R\\'emi Giraud, Boris Mansencal, Thomas Tourdias, Jos\\`e V. Manjon, Pierrick Coup\\'e",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03376v1 Announce Type: new \nAbstract: This paper introduces a novel method for brain segmentation using only FLAIR MRIs, specifically targeting cases where access to other imaging modalities is limited. By leveraging existing automatic segmentation methods, we train a network to approximate segmentations, typically obtained from T1-weighted MRIs. Our method, called FLAIRBrainSeg, produces segmentations of 132 structures and is robust to multiple sclerosis lesions. Experiments on both in-domain and out-of-domain datasets demonstrate that our method outperforms modality-agnostic approaches based on image synthesis, the only currently available alternative for performing brain parcellation using FLAIR MRI alone. This technique holds promise for scenarios where T1-weighted MRIs are unavailable and offers a valuable alternative for clinicians and researchers in need of reliable anatomical segmentation."
      },
      {
        "id": "oai:arXiv.org:2504.03380v1",
        "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.03380",
        "author": "Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03380v1 Announce Type: new \nAbstract: Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks show that balanced online filtering yields an additional 10% in AIME and 4% improvements in average over plain GRPO. Moreover, further analysis shows the gains in sample efficiency and training time efficiency, exceeding the maximum reward of plain GRPO within 60% training time and the volume of the training set."
      },
      {
        "id": "oai:arXiv.org:2504.03387v1",
        "title": "BitHEP -- The Limits of Low-Precision ML in HEP",
        "link": "https://arxiv.org/abs/2504.03387",
        "author": "Claudius Krause, Daohan Wang, Ramon Winterhalder",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03387v1 Announce Type: new \nAbstract: The increasing complexity of modern neural network architectures demands fast and memory-efficient implementations to mitigate computational bottlenecks. In this work, we evaluate the recently proposed BitNet architecture in HEP applications, assessing its performance in classification, regression, and generative modeling tasks. Specifically, we investigate its suitability for quark-gluon discrimination, SMEFT parameter estimation, and detector simulation, comparing its efficiency and accuracy to state-of-the-art methods. Our results show that while BitNet consistently performs competitively in classification tasks, its performance in regression and generation varies with the size and type of the network, highlighting key limitations and potential areas for improvement."
      },
      {
        "id": "oai:arXiv.org:2504.03420v1",
        "title": "Autonomous state-space segmentation for Deep-RL sparse reward scenarios",
        "link": "https://arxiv.org/abs/2504.03420",
        "author": "Gianluca Maselli, Vieri Giuliano Santucci",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03420v1 Announce Type: new \nAbstract: Dealing with environments with sparse rewards has always been crucial for systems developed to operate in autonomous open-ended learning settings. Intrinsic Motivations could be an effective way to help Deep Reinforcement Learning algorithms learn in such scenarios. In fact, intrinsic reward signals, such as novelty or curiosity, are generally adopted to improve exploration when extrinsic rewards are delayed or absent. Building on previous works, we tackle the problem of learning policies in the presence of sparse rewards by proposing a two-level architecture that alternates an ''intrinsically driven'' phase of exploration and autonomous sub-goal generation, to a phase of sparse reward, goal-directed policy learning. The idea is to build several small networks, each one specialized on a particular sub-path, and use them as starting points for future exploration without the need to further explore from scratch previously learnt paths. Two versions of the system have been trained and tested in the Gym SuperMarioBros environment without considering any additional extrinsic reward. The results show the validity of our approach and the importance of autonomously segment the environment to generate an efficient path towards the final goal."
      },
      {
        "id": "oai:arXiv.org:2504.03423v1",
        "title": "DML-RAM: Deep Multimodal Learning Framework for Robotic Arm Manipulation using Pre-trained Models",
        "link": "https://arxiv.org/abs/2504.03423",
        "author": "Sathish Kumar, Swaroop Damodaran, Naveen Kumar Kuruba, Sumit Jha, Arvind Ramanathan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03423v1 Announce Type: new \nAbstract: This paper presents a novel deep learning framework for robotic arm manipulation that integrates multimodal inputs using a late-fusion strategy. Unlike traditional end-to-end or reinforcement learning approaches, our method processes image sequences with pre-trained models and robot state data with machine learning algorithms, fusing their outputs to predict continuous action values for control. Evaluated on BridgeData V2 and Kuka datasets, the best configuration (VGG16 + Random Forest) achieved MSEs of 0.0021 and 0.0028, respectively, demonstrating strong predictive performance and robustness. The framework supports modularity, interpretability, and real-time decision-making, aligning with the goals of adaptive, human-in-the-loop cyber-physical systems."
      },
      {
        "id": "oai:arXiv.org:2504.03429v1",
        "title": "Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning and Graph Neural Networks",
        "link": "https://arxiv.org/abs/2504.03429",
        "author": "Alexander Mattick, Maniraman Periyasamy, Christian Ufrecht, Abhishek Y. Dubey, Christopher Mutschler, Axel Plinge, Daniel D. Scherer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03429v1 Announce Type: new \nAbstract: Quantum computing is currently strongly limited by the impact of noise, in particular introduced by the application of two-qubit gates. For this reason, reducing the number of two-qubit gates is of paramount importance on noisy intermediate-scale quantum hardware. To advance towards more reliable quantum computing, we introduce a framework based on ZX calculus, graph-neural networks and reinforcement learning for quantum circuit optimization. By combining reinforcement learning and tree search, our method addresses the challenge of selecting optimal sequences of ZX calculus rewrite rules. Instead of relying on existing heuristic rules for minimizing circuits, our method trains a novel reinforcement learning policy that directly operates on ZX-graphs, therefore allowing us to search through the space of all possible circuit transformations to find a circuit significantly minimizing the number of CNOT gates. This way we can scale beyond hard-coded rules towards discovering arbitrary optimization rules. We demonstrate our method's competetiveness with state-of-the-art circuit optimizers and generalization capabilities on large sets of diverse random circuits."
      },
      {
        "id": "oai:arXiv.org:2504.03434v1",
        "title": "Locations of Characters in Narratives: Andersen and Persuasion Datasets",
        "link": "https://arxiv.org/abs/2504.03434",
        "author": "Batuhan Ozyurt, Roya Arkhmammadova, Deniz Yuret",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03434v1 Announce Type: new \nAbstract: The ability of machines to grasp spatial understanding within narrative contexts is an intriguing aspect of reading comprehension that continues to be studied. Motivated by the goal to test the AI's competence in understanding the relationship between characters and their respective locations in narratives, we introduce two new datasets: Andersen and Persuasion. For the Andersen dataset, we selected fifteen children's stories from \"Andersen's Fairy Tales\" by Hans Christian Andersen and manually annotated the characters and their respective locations throughout each story. Similarly, for the Persuasion dataset, characters and their locations in the novel \"Persuasion\" by Jane Austen were also manually annotated. We used these datasets to prompt Large Language Models (LLMs). The prompts are created by extracting excerpts from the stories or the novel and combining them with a question asking the location of a character mentioned in that excerpt. Out of the five LLMs we tested, the best-performing one for the Andersen dataset accurately identified the location in 61.85% of the examples, while for the Persuasion dataset, the best-performing one did so in 56.06% of the cases."
      },
      {
        "id": "oai:arXiv.org:2504.03438v1",
        "title": "ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object Perception in Autonomous Driving",
        "link": "https://arxiv.org/abs/2504.03438",
        "author": "Sheng Yang, Tong Zhan, Shichen Qiao, Jicheng Gong, Qing Yang, Yanfeng Lu, Jian Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03438v1 Announce Type: new \nAbstract: Reliable 3D object perception is essential in autonomous driving. Owing to its sensing capabilities in all weather conditions, 4D radar has recently received much attention. However, compared to LiDAR, 4D radar provides much sparser point cloud. In this paper, we propose a 3D object detection method, termed ZFusion, which fuses 4D radar and vision modality. As the core of ZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross Attention) fuser complements the (sparse) radar information and (dense) vision information, effectively. Specifically, with a feature-pyramid structure, the FP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal features at different scales, thus enhancing perception accuracy. In addition, we utilize the Depth-Context-Split view transformation module due to the physical properties of 4D radar. Considering that 4D radar has a much lower cost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods. In typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments show that with reasonable inference speed, ZFusion achieved the state-of-the-art mAP (mean average precision) in the region of interest, while having competitive mAP in the entire area compared to the baseline methods, which demonstrates performance close to LiDAR and greatly outperforms those camera-only methods."
      },
      {
        "id": "oai:arXiv.org:2504.03440v1",
        "title": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models",
        "link": "https://arxiv.org/abs/2504.03440",
        "author": "Mirko Borszukovszki, Ivo Pascal de Jong, Matias Valdenegro-Toro",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03440v1 Announce Type: new \nAbstract: To leverage the full potential of Large Language Models (LLMs) it is crucial to have some information on their answers' uncertainty. This means that the model has to be able to quantify how certain it is in the correctness of a given response. Bad uncertainty estimates can lead to overconfident wrong answers undermining trust in these models. Quite a lot of research has been done on language models that work with text inputs and provide text outputs. Still, since the visual capabilities have been added to these models recently, there has not been much progress on the uncertainty of Visual Language Models (VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found that the severity of the corruption negatively impacted the models' ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments."
      },
      {
        "id": "oai:arXiv.org:2504.03442v1",
        "title": "Pyramid-based Mamba Multi-class Unsupervised Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.03442",
        "author": "Nasar Iqbal, Niki Martinel",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03442v1 Announce Type: new \nAbstract: Recent advances in convolutional neural networks (CNNs) and transformer-based methods have improved anomaly detection and localization, but challenges persist in precisely localizing small anomalies. While CNNs face limitations in capturing long-range dependencies, transformer architectures often suffer from substantial computational overheads. We introduce a state space model (SSM)-based Pyramidal Scanning Strategy (PSS) for multi-class anomaly detection and localization--a novel approach designed to address the challenge of small anomaly localization. Our method captures fine-grained details at multiple scales by integrating the PSS with a pre-trained encoder for multi-scale feature extraction and a feature-level synthetic anomaly generator. An improvement of $+1\\%$ AP for multi-class anomaly localization and a +$1\\%$ increase in AU-PRO on MVTec benchmark demonstrate our method's superiority in precise anomaly localization across diverse industrial scenarios. The code is available at https://github.com/iqbalmlpuniud/Pyramid Mamba."
      },
      {
        "id": "oai:arXiv.org:2504.03450v1",
        "title": "Optimizing Specific and Shared Parameters for Efficient Parameter Tuning",
        "link": "https://arxiv.org/abs/2504.03450",
        "author": "Van-Anh Nguyen, Thanh-Toan Do, Mehrtash Harandi, Dinh Phung, Trung Le",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03450v1 Announce Type: new \nAbstract: Foundation models, with a vast number of parameters and pretraining on massive datasets, achieve state-of-the-art performance across various applications. However, efficiently adapting them to downstream tasks with minimal computational overhead remains a challenge. Parameter-Efficient Transfer Learning (PETL) addresses this by fine-tuning only a small subset of parameters while preserving pre-trained knowledge. In this paper, we propose SaS, a novel PETL method that effectively mitigates distributional shifts during fine-tuning. SaS integrates (1) a shared module that captures common statistical characteristics across layers using low-rank projections and (2) a layer-specific module that employs hypernetworks to generate tailored parameters for each layer. This dual design ensures an optimal balance between performance and parameter efficiency while introducing less than 0.05% additional parameters, making it significantly more compact than existing methods. Extensive experiments on diverse downstream tasks, few-shot settings and domain generalization demonstrate that SaS significantly enhances performance while maintaining superior parameter efficiency compared to existing methods, highlighting the importance of capturing both shared and layer-specific information in transfer learning. Code and data are available at https://anonymous.4open.science/r/SaS-PETL-3565."
      },
      {
        "id": "oai:arXiv.org:2504.03454v1",
        "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing",
        "link": "https://arxiv.org/abs/2504.03454",
        "author": "William Fleshman, Benjamin Van Durme",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03454v1 Announce Type: new \nAbstract: Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains."
      },
      {
        "id": "oai:arXiv.org:2504.03468v1",
        "title": "D-Garment: Physics-Conditioned Latent Diffusion for Dynamic Garment Deformations",
        "link": "https://arxiv.org/abs/2504.03468",
        "author": "Antoine Dumoulin, Adnane Boukhayma, Laurence Boissieux, Bharath Bhushan Damodaran, Pierre Hellier, Stefanie Wuhrer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03468v1 Announce Type: new \nAbstract: Adjusting and deforming 3D garments to body shapes, body motion, and cloth material is an important problem in virtual and augmented reality. Applications are numerous, ranging from virtual change rooms to the entertainment and gaming industry. This problem is challenging as garment dynamics influence geometric details such as wrinkling patterns, which depend on physical input including the wearer's body shape and motion, as well as cloth material features. Existing work studies learning-based modeling techniques to generate garment deformations from example data, and physics-inspired simulators to generate realistic garment dynamics. We propose here a learning-based approach trained on data generated with a physics-based simulator. Compared to prior work, our 3D generative model learns garment deformations for loose cloth geometry, especially for large deformations and dynamic wrinkles driven by body motion and cloth material. Furthermore, the model can be efficiently fitted to observations captured using vision sensors. We propose to leverage the capability of diffusion models to learn fine-scale detail: we model the 3D garment in a 2D parameter space, and learn a latent diffusion model using this representation independent from the mesh resolution. This allows to condition global and local geometric information with body and material information. We quantitatively and qualitatively evaluate our method on both simulated data and data captured with a multi-view acquisition platform. Compared to strong baselines, our method is more accurate in terms of Chamfer distance."
      },
      {
        "id": "oai:arXiv.org:2504.03471v1",
        "title": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis",
        "link": "https://arxiv.org/abs/2504.03471",
        "author": "Xi Wang, Ziqi He, Yang Zhou",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03471v1 Announce Type: new \nAbstract: Traditional diffusion models typically employ a U-Net architecture. Previous studies have unveiled the roles of attention blocks in the U-Net. However, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. In this study, we first theoretically proved that, re-weighting the outputs of the Transformer blocks within the U-Net is a \"free lunch\" for improving the signal-to-noise ratio during the sampling process. Next, we proposed Importance Probe to uncover and quantify the dynamic shifts in importance of the Transformer blocks throughout the denoising process. Finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. Experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. Our method can be seamlessly integrated into any U-Net-based architecture. Code: https://github.com/Hytidel/UNetReweighting"
      },
      {
        "id": "oai:arXiv.org:2504.03474v1",
        "title": "Multi-encoder nnU-Net outperforms Transformer models with self-supervised pretraining",
        "link": "https://arxiv.org/abs/2504.03474",
        "author": "Seyedeh Sahar Taheri Otaghsara, Reza Rahmanzadeh",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03474v1 Announce Type: new \nAbstract: This study addresses the essential task of medical image segmentation, which involves the automatic identification and delineation of anatomical structures and pathological regions in medical images. Accurate segmentation is crucial in radiology, as it aids in the precise localization of abnormalities such as tumors, thereby enabling effective diagnosis, treatment planning, and monitoring of disease progression. Specifically, the size, shape, and location of tumors can significantly influence clinical decision-making and therapeutic strategies, making accurate segmentation a key component of radiological workflows. However, challenges posed by variations in MRI modalities, image artifacts, and the scarcity of labeled data complicate the segmentation task and impact the performance of traditional models. To overcome these limitations, we propose a novel self-supervised learning Multi-encoder nnU-Net architecture designed to process multiple MRI modalities independently through separate encoders. This approach allows the model to capture modality-specific features before fusing them for the final segmentation, thus improving accuracy. Our Multi-encoder nnU-Net demonstrates exceptional performance, achieving a Dice Similarity Coefficient (DSC) of 93.72%, which surpasses that of other models such as vanilla nnU-Net, SegResNet, and Swin UNETR. By leveraging the unique information provided by each modality, the model enhances segmentation tasks, particularly in scenarios with limited annotated data. Evaluations highlight the effectiveness of this architecture in improving tumor segmentation outcomes."
      },
      {
        "id": "oai:arXiv.org:2504.03476v1",
        "title": "ATM-Net: Anatomy-Aware Text-Guided Multi-Modal Fusion for Fine-Grained Lumbar Spine Segmentation",
        "link": "https://arxiv.org/abs/2504.03476",
        "author": "Sheng Lian, Dengfeng Pan, Jianlong Cai, Guang-Yong Chen, Zhun Zhong, Zhiming Luo, Shen Zhao, Shuo Li",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03476v1 Announce Type: new \nAbstract: Accurate lumbar spine segmentation is crucial for diagnosing spinal disorders. Existing methods typically use coarse-grained segmentation strategies that lack the fine detail needed for precise diagnosis. Additionally, their reliance on visual-only models hinders the capture of anatomical semantics, leading to misclassified categories and poor segmentation details. To address these limitations, we present ATM-Net, an innovative framework that employs an anatomy-aware, text-guided, multi-modal fusion mechanism for fine-grained segmentation of lumbar substructures, i.e., vertebrae (VBs), intervertebral discs (IDs), and spinal canal (SC). ATM-Net adopts the Anatomy-aware Text Prompt Generator (ATPG) to adaptively convert image annotations into anatomy-aware prompts in different views. These insights are further integrated with image features via the Holistic Anatomy-aware Semantic Fusion (HASF) module, building a comprehensive anatomical context. The Channel-wise Contrastive Anatomy-Aware Enhancement (CCAE) module further enhances class discrimination and refines segmentation through class-wise channel-level multi-modal contrastive learning. Extensive experiments on the MRSpineSeg and SPIDER datasets demonstrate that ATM-Net significantly outperforms state-of-the-art methods, with consistent improvements regarding class discrimination and segmentation details. For example, ATM-Net achieves Dice of 79.39% and HD95 of 9.91 pixels on SPIDER, outperforming the competitive SpineParseNet by 8.31% and 4.14 pixels, respectively."
      },
      {
        "id": "oai:arXiv.org:2504.03478v1",
        "title": "Probabilistic Machine Learning for Noisy Labels in Earth Observation",
        "link": "https://arxiv.org/abs/2504.03478",
        "author": "Spyros Kondylatos, Nikolaos Ioannis Bountos, Ioannis Prapas, Angelos Zavras, Gustau Camps-Valls, Ioannis Papoutsis",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03478v1 Announce Type: new \nAbstract: Label noise poses a significant challenge in Earth Observation (EO), often degrading the performance and reliability of supervised Machine Learning (ML) models. Yet, given the critical nature of several EO applications, developing robust and trustworthy ML solutions is essential. In this study, we take a step in this direction by leveraging probabilistic ML to model input-dependent label noise and quantify data uncertainty in EO tasks, accounting for the unique noise sources inherent in the domain. We train uncertainty-aware probabilistic models across a broad range of high-impact EO applications-spanning diverse noise sources, input modalities, and ML configurations-and introduce a dedicated pipeline to assess their accuracy and reliability. Our experimental results show that the uncertainty-aware models consistently outperform the standard deterministic approaches across most datasets and evaluation metrics. Moreover, through rigorous uncertainty evaluation, we validate the reliability of the predicted uncertainty estimates, enhancing the interpretability of model predictions. Our findings emphasize the importance of modeling label noise and incorporating uncertainty quantification in EO, paving the way for more accurate, reliable, and trustworthy ML solutions in the field."
      },
      {
        "id": "oai:arXiv.org:2504.03483v1",
        "title": "Online Traffic Density Estimation using Physics-Informed Neural Networks",
        "link": "https://arxiv.org/abs/2504.03483",
        "author": "Dennis Wilkman, Kateryna Morozovska, Karl Henrik Johansson, Matthieu Barreau",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03483v1 Announce Type: new \nAbstract: Recent works on the application of Physics-Informed Neural Networks to traffic density estimation have shown to be promising for future developments due to their robustness to model errors and noisy data. In this paper, we introduce a methodology for online approximation of the traffic density using measurements from probe vehicles in two settings: one using the Greenshield model and the other considering a high-fidelity traffic simulation. The proposed method continuously estimates the real-time traffic density in space and performs model identification with each new set of measurements. The density estimate is updated in almost real-time using gradient descent and adaptive weights. In the case of full model knowledge, the resulting algorithm has similar performance to the classical open-loop one. However, in the case of model mismatch, the iterative solution behaves as a closed-loop observer and outperforms the baseline method. Similarly, in the high-fidelity setting, the proposed algorithm correctly reproduces the traffic characteristics."
      },
      {
        "id": "oai:arXiv.org:2504.03484v1",
        "title": "Discovering Partially Known Ordinary Differential Equations: a Case Study on the Chemical Kinetics of Cellulose Degradation",
        "link": "https://arxiv.org/abs/2504.03484",
        "author": "Federica Bragone, Kateryna Morozovska, Tor Laneryd, Khemraj Shukla, Stefano Markidis",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03484v1 Announce Type: new \nAbstract: The degree of polymerization (DP) is one of the methods for estimating the aging of the polymer based insulation systems, such as cellulose insulation in power components. The main degradation mechanisms in polymers are hydrolysis, pyrolysis, and oxidation. These mechanisms combined cause a reduction of the DP. However, the data availability for these types of problems is usually scarce. This study analyzes insulation aging using cellulose degradation data from power transformers. The aging problem for the cellulose immersed in mineral oil inside power transformers is modeled with ordinary differential equations (ODEs). We recover the governing equations of the degradation system using Physics-Informed Neural Networks (PINNs) and symbolic regression. We apply PINNs to discover the Arrhenius equation's unknown parameters in the Ekenstam ODE describing cellulose contamination content and the material aging process related to temperature for synthetic data and real DP values. A modification of the Ekenstam ODE is given by Emsley's system of ODEs, where the rate constant expressed by the Arrhenius equation decreases in time with the new formulation. We use PINNs and symbolic regression to recover the functional form of one of the ODEs of the system and to identify an unknown parameter."
      },
      {
        "id": "oai:arXiv.org:2504.03485v1",
        "title": "Gaussian Process Tilted Nonparametric Density Estimation using Fisher Divergence Score Matching",
        "link": "https://arxiv.org/abs/2504.03485",
        "author": "John Paisley, Wei Zhang, Brian Barr",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03485v1 Announce Type: new \nAbstract: We present three Fisher divergence (FD) minimization algorithms for learning Gaussian process (GP) based score models for lower dimensional density estimation problems. The density is formed by multiplying a base multivariate normal distribution with an exponentiated GP refinement, and so we refer to it as a GP-tilted nonparametric density. By representing the GP part of the score as a linear function using the random Fourier feature (RFF) approximation, we show that all learning problems can be solved in closed form. This includes the basic and noise conditional versions of the Fisher divergence, as well as a novel alternative to noise conditional FD models based on variational inference (VI). Here, we propose using an ELBO-like optimization of the approximate posterior with which we derive a Fisher variational predictive distribution. The RFF representation of the GP, which is functionally equivalent to a single layer neural network score model with cosine activation, provides a unique linear form for which all expectations are in closed form. The Gaussian base also helps with tractability of the VI approximation. We demonstrate our three learning algorithms, as well as a MAP baseline algorithm, on several low dimensional density estimation problems. The closed-form nature of the learning problem removes the reliance on iterative algorithms, making this technique particularly well-suited to large data sets."
      },
      {
        "id": "oai:arXiv.org:2504.03486v1",
        "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej",
        "link": "https://arxiv.org/abs/2504.03486",
        "author": "Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03486v1 Announce Type: new \nAbstract: Automating legal document drafting can significantly enhance efficiency, reduce manual effort, and streamline legal workflows. While prior research has explored tasks such as judgment prediction and case summarization, the structured generation of private legal documents in the Indian legal domain remains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej, a novel, anonymized dataset of private legal documents, and develop NyayaShilp, a fine-tuned legal document generation model specifically adapted to Indian legal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework that first generates structured section titles and then iteratively produces content while leveraging retrieval-based mechanisms to ensure coherence and factual accuracy. We benchmark multiple open-source LLMs, including instruction-tuned and domain-adapted versions, alongside proprietary models for comparison. Our findings indicate that while direct fine-tuning on small datasets does not always yield improvements, our structured wrapper significantly enhances coherence, factual adherence, and overall document quality while mitigating hallucinations. To ensure real-world applicability, we developed a Human-in-the-Loop (HITL) Document Generation System, an interactive user interface that enables users to specify document types, refine section details, and generate structured legal drafts. This tool allows legal professionals and researchers to generate, validate, and refine AI-generated legal documents efficiently. Extensive evaluations, including expert assessments, confirm that our framework achieves high reliability in structured legal drafting. This research establishes a scalable and adaptable foundation for AI-assisted legal drafting in India, offering an effective approach to structured legal document generation."
      },
      {
        "id": "oai:arXiv.org:2504.03490v1",
        "title": "BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution",
        "link": "https://arxiv.org/abs/2504.03490",
        "author": "Zihao He, Shengchuan Zhang, Runze Hu, Yunhang Shen, Yan Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03490v1 Announce Type: new \nAbstract: Super-resolution (SR) techniques are critical for enhancing image quality, particularly in scenarios where high-resolution imagery is essential yet limited by hardware constraints. Existing diffusion models for SR have relied predominantly on Gaussian models for noise generation, which often fall short when dealing with the complex and variable texture inherent in natural scenes. To address these deficiencies, we introduce the Bayesian Uncertainty Guided Diffusion Probabilistic Model (BUFF). BUFF distinguishes itself by incorporating a Bayesian network to generate high-resolution uncertainty masks. These masks guide the diffusion process, allowing for the adjustment of noise intensity in a manner that is both context-aware and adaptive. This novel approach not only enhances the fidelity of super-resolved images to their original high-resolution counterparts but also significantly mitigates artifacts and blurring in areas characterized by complex textures and fine details. The model demonstrates exceptional robustness against complex noise patterns and showcases superior adaptability in handling textures and edges within images. Empirical evidence, supported by visual results, illustrates the model's robustness, especially in challenging scenarios, and its effectiveness in addressing common SR issues such as blurring. Experimental evaluations conducted on the DIV2K dataset reveal that BUFF achieves a notable improvement, with a +0.61 increase compared to baseline in SSIM on BSD100, surpassing traditional diffusion approaches by an average additional +0.20dB PSNR gain. These findings underscore the potential of Bayesian methods in enhancing diffusion processes for SR, paving the way for future advancements in the field."
      },
      {
        "id": "oai:arXiv.org:2504.03491v1",
        "title": "Diffusion Active Learning: Towards Data-Driven Experimental Design in Computed Tomography",
        "link": "https://arxiv.org/abs/2504.03491",
        "author": "Luis Barba, Johannes Kirschner, Tomas Aidukas, Manuel Guizar-Sicairos, Benjam\\'in B\\'ejar",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03491v1 Announce Type: new \nAbstract: We introduce Diffusion Active Learning, a novel approach that combines generative diffusion modeling with data-driven sequential experimental design to adaptively acquire data for inverse problems. Although broadly applicable, we focus on scientific computed tomography (CT) for experimental validation, where structured prior datasets are available, and reducing data requirements directly translates to shorter measurement times and lower X-ray doses. We first pre-train an unconditional diffusion model on domain-specific CT reconstructions. The diffusion model acts as a learned prior that is data-dependent and captures the structure of the underlying data distribution, which is then used in two ways: It drives the active learning process and also improves the quality of the reconstructions. During the active learning loop, we employ a variant of diffusion posterior sampling to generate conditional data samples from the posterior distribution, ensuring consistency with the current measurements. Using these samples, we quantify the uncertainty in the current estimate to select the most informative next measurement. Our results show substantial reductions in data acquisition requirements, corresponding to lower X-ray doses, while simultaneously improving image reconstruction quality across multiple real-world tomography datasets."
      },
      {
        "id": "oai:arXiv.org:2504.03494v1",
        "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
        "link": "https://arxiv.org/abs/2504.03494",
        "author": "Alexander Windmann, Henrik Steude, Daniel Boschmann, Oliver Niggemann",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03494v1 Announce Type: new \nAbstract: Cyber-Physical Systems (CPS) in domains such as manufacturing and energy distribution generate complex time series data crucial for Prognostics and Health Management (PHM). While Deep Learning (DL) methods have demonstrated strong forecasting capabilities, their adoption in industrial CPS remains limited due insufficient robustness. Existing robustness evaluations primarily focus on formal verification or adversarial perturbations, inadequately representing the complexities encountered in real-world CPS scenarios. To address this, we introduce a practical robustness definition grounded in distributional robustness, explicitly tailored to industrial CPS, and propose a systematic framework for robustness evaluation. Our framework simulates realistic disturbances, such as sensor drift, noise and irregular sampling, enabling thorough robustness analyses of forecasting models on real-world CPS datasets. The robustness definition provides a standardized score to quantify and compare model performance across diverse datasets, assisting in informed model selection and architecture design. Through extensive empirical studies evaluating prominent DL architectures (including recurrent, convolutional, attention-based, modular, and structured state-space models) we demonstrate the applicability and effectiveness of our approach. We publicly release our robustness benchmark to encourage further research and reproducibility."
      },
      {
        "id": "oai:arXiv.org:2504.03497v1",
        "title": "Hybrid Real- and Complex-valued Neural Network Architecture",
        "link": "https://arxiv.org/abs/2504.03497",
        "author": "Alex Young, Luan Vin\\'icius Fiorio, Bo Yang, Boris Karanov, Wim van Houtum, Ronald M. Aarts",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03497v1 Announce Type: new \nAbstract: We propose a \\emph{hybrid} real- and complex-valued \\emph{neural network} (HNN) architecture, designed to combine the computational efficiency of real-valued processing with the ability to effectively handle complex-valued data. We illustrate the limitations of using real-valued neural networks (RVNNs) for inherently complex-valued problems by showing how it learnt to perform complex-valued convolution, but with notable inefficiencies stemming from its real-valued constraints. To create the HNN, we propose to use building blocks containing both real- and complex-valued paths, where information between domains is exchanged through domain conversion functions. We also introduce novel complex-valued activation functions, with higher generalisation and parameterisation efficiency. HNN-specific architecture search techniques are described to navigate the larger solution space. Experiments with the AudioMNIST dataset demonstrate that the HNN reduces cross-entropy loss and consumes less parameters compared to an RVNN for all considered cases. Such results highlight the potential for the use of partially complex-valued processing in neural networks and applications for HNNs in many signal processing domains."
      },
      {
        "id": "oai:arXiv.org:2504.03501v1",
        "title": "LV-MAE: Learning Long Video Representations through Masked-Embedding Autoencoders",
        "link": "https://arxiv.org/abs/2504.03501",
        "author": "Ilan Naiman, Emanuel Ben-Baruch, Oron Anschel, Alon Shoshan, Igor Kviatkovsky, Manoj Aggarwal, Gerard Medioni",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03501v1 Announce Type: new \nAbstract: In this work, we introduce long-video masked-embedding autoencoders (LV-MAE), a self-supervised learning framework for long video representation. Our approach treats short- and long-span dependencies as two separate tasks. Such decoupling allows for a more intuitive video processing where short-span spatiotemporal primitives are first encoded and are then used to capture long-range dependencies across consecutive video segments. To achieve this, we leverage advanced off-the-shelf multimodal encoders to extract representations from short segments within the long video, followed by pre-training a masked-embedding autoencoder capturing high-level interactions across segments. LV-MAE is highly efficient to train and enables the processing of much longer videos by alleviating the constraint on the number of input frames. Furthermore, unlike existing methods that typically pre-train on short-video datasets, our approach offers self-supervised pre-training using long video samples (e.g., 20+ minutes video clips) at scale. Using LV-MAE representations, we achieve state-of-the-art results on three long-video benchmarks -- LVU, COIN, and Breakfast -- employing only a simple classification head for either attentive or linear probing. Finally, to assess LV-MAE pre-training and visualize its reconstruction quality, we leverage the video-language aligned space of short video representations to monitor LV-MAE through video-text retrieval."
      },
      {
        "id": "oai:arXiv.org:2504.03505v1",
        "title": "Hierarchical Knowledge Structuring for Effective Federated Learning in Heterogeneous Environments",
        "link": "https://arxiv.org/abs/2504.03505",
        "author": "Wai Fong Tam, Qilei Li, Ahmed M. Abdelmonie",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03505v1 Announce Type: new \nAbstract: Federated learning enables collaborative model training across distributed entities while maintaining individual data privacy. A key challenge in federated learning is balancing the personalization of models for local clients with generalization for the global model. Recent efforts leverage logit-based knowledge aggregation and distillation to overcome these issues. However, due to the non-IID nature of data across diverse clients and the imbalance in the client's data distribution, directly aggregating the logits often produces biased knowledge that fails to apply to individual clients and obstructs the convergence of local training. To solve this issue, we propose a Hierarchical Knowledge Structuring (HKS) framework that formulates sample logits into a multi-granularity codebook to represent logits from personalized per-sample insights to globalized per-class knowledge. The unsupervised bottom-up clustering method is leveraged to enable the global server to provide multi-granularity responses to local clients. These responses allow local training to integrate supervised learning objectives with global generalization constraints, which results in more robust representations and improved knowledge sharing in subsequent training rounds. The proposed framework's effectiveness is validated across various benchmarks and model architectures."
      },
      {
        "id": "oai:arXiv.org:2504.03510v1",
        "title": "FADConv: A Frequency-Aware Dynamic Convolution for Farmland Non-agriculturalization Identification and Segmentation",
        "link": "https://arxiv.org/abs/2504.03510",
        "author": "Tan Shu, Li Shen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03510v1 Announce Type: new \nAbstract: Cropland non-agriculturalization refers to the conversion of arable land into non-agricultural uses such as forests, residential areas, and construction sites. This phenomenon not only directly leads to the loss of cropland resources but also poses systemic threats to food security and agricultural sustainability. Accurate identification of cropland and non-cropland areas is crucial for detecting and addressing this issue. Traditional CNNs employ static convolution layers, while dynamic convolution studies demonstrate that adaptively weighting multiple convolutional kernels through attention mechanisms can enhance accuracy. However, existing dynamic convolution methods relying on Global Average Pooling (GAP) for attention weight allocation suffer from information loss, limiting segmentation precision. This paper proposes Frequency-Aware Dynamic Convolution (FADConv) and a Frequency Attention (FAT) module to address these limitations. Building upon the foundational structure of dynamic convolution, we designed FADConv by integrating 2D Discrete Cosine Transform (2D DCT) to capture frequency domain features and fuse them. FAT module generates high-quality attention weights that replace the traditional GAP method,making the combination between dynamic convolution kernels more reasonable.Experiments on the GID and Hi-CNA datasets demonstrate that FADConv significantly improves segmentation accuracy with minimal computational overhead. For instance, ResNet18 with FADConv achieves 1.9% and 2.7% increases in F1-score and IoU for cropland segmentation on GID, with only 58.87M additional MAdds. Compared to other dynamic convolution approaches, FADConv exhibits superior performance in cropland segmentation tasks."
      },
      {
        "id": "oai:arXiv.org:2504.03520v1",
        "title": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles",
        "link": "https://arxiv.org/abs/2504.03520",
        "author": "Chen Wei Kuo, Kevin Chu, Nouar AlDahoul, Hazem Ibrahim, Talal Rahwan, Yasir Zaki",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03520v1 Announce Type: new \nAbstract: Bias in news reporting significantly impacts public perception, particularly regarding crime, politics, and societal issues. Traditional bias detection methods, predominantly reliant on human moderation, suffer from subjective interpretations and scalability constraints. Here, we introduce an AI-driven framework leveraging advanced large language models (LLMs), specifically GPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to systematically identify and mitigate biases in news articles. To this end, we collect an extensive dataset consisting of over 30,000 crime-related articles from five politically diverse news sources spanning a decade (2013-2023). Our approach employs a two-stage methodology: (1) bias detection, where each LLM scores and justifies biased content at the paragraph level, validated through human evaluation for ground truth establishment, and (2) iterative debiasing using GPT-4o Mini, verified by both automated reassessment and human reviewers. Empirical results indicate GPT-4o Mini's superior accuracy in bias detection and effectiveness in debiasing. Furthermore, our analysis reveals temporal and geographical variations in media bias correlating with socio-political dynamics and real-world events. This study contributes to scalable computational methodologies for bias mitigation, promoting fairness and accountability in news reporting."
      },
      {
        "id": "oai:arXiv.org:2504.03524v1",
        "title": "RANa: Retrieval-Augmented Navigation",
        "link": "https://arxiv.org/abs/2504.03524",
        "author": "Gianluca Monaci, Rafael S. Rezende, Romain Deffayet, Gabriela Csurka, Guillaume Bono, Herv\\'e D\\'ejean, St\\'ephane Clinchant, Christian Wolf",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03524v1 Announce Type: new \nAbstract: Methods for navigation based on large-scale learning typically treat each episode as a new problem, where the agent is spawned with a clean memory in an unknown environment. While these generalization capabilities to an unknown environment are extremely important, we claim that, in a realistic setting, an agent should have the capacity of exploiting information collected during earlier robot operations. We address this by introducing a new retrieval-augmented agent, trained with RL, capable of querying a database collected from previous episodes in the same environment and learning how to integrate this additional context information. We introduce a unique agent architecture for the general navigation task, evaluated on ObjectNav, ImageNav and Instance-ImageNav. Our retrieval and context encoding methods are data-driven and heavily employ vision foundation models (FM) for both semantic and geometric understanding. We propose new benchmarks for these settings and we show that retrieval allows zero-shot transfer across tasks and environments while significantly improving performance."
      },
      {
        "id": "oai:arXiv.org:2504.03531v1",
        "title": "Dense Neural Network Based Arrhythmia Classification on Low-cost and Low-compute Micro-controller",
        "link": "https://arxiv.org/abs/2504.03531",
        "author": "Md Abu Obaida Zishan, H M Shihab, Sabik Sadman Islam, Maliha Alam Riya, Gazi Mashrur Rahman, Jannatun Noor",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03531v1 Announce Type: new \nAbstract: The electrocardiogram (ECG) monitoring device is an expensive albeit essential device for the treatment and diagnosis of cardiovascular diseases (CVD). The cost of this device typically ranges from $2000 to $10000. Several studies have implemented ECG monitoring systems in micro-controller units (MCU) to reduce industrial development costs by up to 20 times. However, to match industry-grade systems and display heartbeats effectively, it is essential to develop an efficient algorithm for detecting arrhythmia (irregular heartbeat). Hence in this study, a dense neural network is developed to detect arrhythmia on the Arduino Nano. The Nano consists of the ATMega328 microcontroller with a 16MHz clock, 2KB of SRAM, and 32KB of program memory. Additionally, the AD8232 SparkFun Single-Lead Heart Rate Monitor is used as the ECG sensor. The implemented neural network model consists of two layers (excluding the input) with 10 and four neurons respectively with sigmoid activation function. However, four approaches are explored to choose the appropriate activation functions. The model has a size of 1.267 KB, achieves an F1 score (macro-average) of 78.3\\% for classifying four types of arrhythmia, an accuracy rate of 96.38%, and requires 0.001314 MOps of floating-point operations (FLOPs)."
      },
      {
        "id": "oai:arXiv.org:2504.03536v1",
        "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration",
        "link": "https://arxiv.org/abs/2504.03536",
        "author": "Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, Xingang Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03536v1 Announce Type: new \nAbstract: Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce \\textbf{HumanDreamer-X}, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, \\textbf{HumanFixer} is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models."
      },
      {
        "id": "oai:arXiv.org:2504.03541v1",
        "title": "Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing",
        "link": "https://arxiv.org/abs/2504.03541",
        "author": "Mayank Kothyari, Sunita Sarawagi, Soumen Chakrabarti, Gaurav Arora, Srujana Merugu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03541v1 Announce Type: new \nAbstract: LLMs are increasingly used as seq2seq translators from natural language utterances to structured programs, a process called semantic interpretation. Unlike atomic labels or token sequences, programs are naturally represented as abstract syntax trees (ASTs). Such structured representation raises novel issues related to the design and selection of in-context examples (ICEs) presented to the LLM. We focus on decomposing the pool of available ICE trees into fragments, some of which may be better suited to solving the test instance. Next, we propose how to use (additional invocations of) an LLM with prompted syntax constraints to automatically map the fragments to corresponding utterances. Finally, we adapt and extend a recent method for diverse ICE selection to work with whole and fragmented ICE instances. We evaluate our system, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing visible accuracy gains from our proposed decomposed diverse demonstration method. Benefits are particularly notable for smaller LLMs, ICE pools having larger labeled trees, and programs in lower resource languages."
      },
      {
        "id": "oai:arXiv.org:2504.03546v1",
        "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation",
        "link": "https://arxiv.org/abs/2504.03546",
        "author": "Khai Le-Duc, Tuyen Tran, Bach Phan Tat, Nguyen Kim Hai Bui, Quan Dang, Hung-Phong Tran, Thanh-Thuy Nguyen, Ly Nguyen, Tuan-Minh Phan, Thi Thu Phuong Tran, Chris Ngo, Nguyen X. Khanh, Thanh Nguyen-Tang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03546v1 Announce Type: new \nAbstract: Multilingual speech translation (ST) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, Traditional Chinese and Simplified Chinese, together with the models. With 290,000 samples, our dataset is the largest medical machine translation (MT) dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most extensive analysis study in ST research to date, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence (seq2seq) comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https://github.com/leduckhai/MultiMed-ST."
      },
      {
        "id": "oai:arXiv.org:2504.03553v1",
        "title": "Agentic Knowledgeable Self-awareness",
        "link": "https://arxiv.org/abs/2504.03553",
        "author": "Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03553v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a \"flood irrigation\" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf."
      },
      {
        "id": "oai:arXiv.org:2504.03561v1",
        "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
        "link": "https://arxiv.org/abs/2504.03561",
        "author": "Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03561v1 Announce Type: new \nAbstract: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld."
      },
      {
        "id": "oai:arXiv.org:2504.03563v1",
        "title": "PF3Det: A Prompted Foundation Feature Assisted Visual LiDAR 3D Detector",
        "link": "https://arxiv.org/abs/2504.03563",
        "author": "Kaidong Li, Tianxiao Zhang, Kuan-Chuan Peng, Guanghui Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03563v1 Announce Type: new \nAbstract: 3D object detection is crucial for autonomous driving, leveraging both LiDAR point clouds for precise depth information and camera images for rich semantic information. Therefore, the multi-modal methods that combine both modalities offer more robust detection results. However, efficiently fusing LiDAR points and images remains challenging due to the domain gaps. In addition, the performance of many models is limited by the amount of high quality labeled data, which is expensive to create. The recent advances in foundation models, which use large-scale pre-training on different modalities, enable better multi-modal fusion. Combining the prompt engineering techniques for efficient training, we propose the Prompted Foundational 3D Detector (PF3Det), which integrates foundation model encoders and soft prompts to enhance LiDAR-camera feature fusion. PF3Det achieves the state-of-the-art results under limited training data, improving NDS by 1.19% and mAP by 2.42% on the nuScenes dataset, demonstrating its efficiency in 3D detection."
      },
      {
        "id": "oai:arXiv.org:2504.03579v1",
        "title": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of Semantic Entropy",
        "link": "https://arxiv.org/abs/2504.03579",
        "author": "Kamil Ciosek, Nicol\\`o Felicioni, Sina Ghiassian",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03579v1 Announce Type: new \nAbstract: Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 59% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM."
      },
      {
        "id": "oai:arXiv.org:2504.03583v1",
        "title": "Scalable Hypergraph Structure Learning with Diverse Smoothness Priors",
        "link": "https://arxiv.org/abs/2504.03583",
        "author": "Benjamin T. Brown, Haoxiang Zhang, Daniel L. Lau, Gonzalo R. Arce",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03583v1 Announce Type: new \nAbstract: In graph signal processing, learning the weighted connections between nodes from a set of sample signals is a fundamental task when the underlying relationships are not known a priori. This task is typically addressed by finding a graph Laplacian on which the observed signals are smooth. With the extension of graphs to hypergraphs - where edges can connect more than two nodes - graph learning methods have similarly been generalized to hypergraphs. However, the absence of a unified framework for calculating total variation has led to divergent definitions of smoothness and, consequently, differing approaches to hyperedge recovery. We confront this challenge through generalization of several previously proposed hypergraph total variations, subsequently allowing ease of substitution into a vector based optimization. To this end, we propose a novel hypergraph learning method that recovers a hypergraph topology from time-series signals based on a smoothness prior. Our approach addresses key limitations in prior works, such as hyperedge selection and convergence issues, by formulating the problem as a convex optimization solved via a forward-backward-forward algorithm, ensuring guaranteed convergence. Additionally, we introduce a process that simultaneously limits the span of the hyperedge search and maintains a valid hyperedge selection set. In doing so, our method becomes scalable in increasingly complex network structures. The experimental results demonstrate improved performance, in terms of accuracy, over other state-of-the-art hypergraph inference methods; furthermore, we empirically show our method to be robust to total variation terms, biased towards global smoothness, and scalable to larger hypergraphs."
      },
      {
        "id": "oai:arXiv.org:2504.03587v1",
        "title": "AutoSSVH: Exploring Automated Frame Sampling for Efficient Self-Supervised Video Hashing",
        "link": "https://arxiv.org/abs/2504.03587",
        "author": "Niu Lian, Jun Li, Jinpeng Wang, Ruisheng Luo, Yaowei Wang, Shu-Tao Xia, Bin Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03587v1 Announce Type: new \nAbstract: Self-Supervised Video Hashing (SSVH) compresses videos into hash codes for efficient indexing and retrieval using unlabeled training videos. Existing approaches rely on random frame sampling to learn video features and treat all frames equally. This results in suboptimal hash codes, as it ignores frame-specific information density and reconstruction difficulty. To address this limitation, we propose a new framework, termed AutoSSVH, that employs adversarial frame sampling with hash-based contrastive learning. Our adversarial sampling strategy automatically identifies and selects challenging frames with richer information for reconstruction, enhancing encoding capability. Additionally, we introduce a hash component voting strategy and a point-to-set (P2Set) hash-based contrastive objective, which help capture complex inter-video semantic relationships in the Hamming space and improve the discriminability of learned hash codes. Extensive experiments demonstrate that AutoSSVH achieves superior retrieval efficacy and efficiency compared to state-of-the-art approaches. Code is available at https://github.com/EliSpectre/CVPR25-AutoSSVH."
      },
      {
        "id": "oai:arXiv.org:2504.03595v1",
        "title": "Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers",
        "link": "https://arxiv.org/abs/2504.03595",
        "author": "Fabio Lilliu (University of Cagliari), Amir Laadhar (PANTOPIX GmbH & Co. KG), Christian Thomsen (Aalborg University), Diego Reforgiato Recupero (University of Cagliari), Torben Bach Pedersen (Aalborg University)",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03595v1 Announce Type: new \nAbstract: A key element to support the increased amounts of renewable energy in the energy system is flexibility, i.e., the possibility of changing energy loads in time and amount. Many flexibility models have been designed; however, exact models fail to scale for long time horizons or many devices. Because of this, the FlexOffer (FOs) model has been designed, to provide device-independent approximations of flexibility with good accuracy, and much better scaling for long time horizons and many devices. An important aspect of the real-life implementation of energy flexibility is enabling flexible data exchange with many types of smart energy appliances and market systems, e.g., in smart buildings. For this, ontologies standardizing data formats are required. However, the current industry standard ontology for integrating smart devices for energy purposes, SAREF for Energy Flexibility (SAREF4ENER) only has limited support for flexibility and thus cannot support important use cases. In this paper we propose an extension of SAREF4ENER that integrates full support for the complete FlexOffer model, including advanced use cases, while maintaining backward compatibility. This novel ontology module can accurately describe flexibility for advanced devices such as electric vehicles, batteries, and heat pumps. It can also capture the inherent uncertainty associated with many flexible load types."
      },
      {
        "id": "oai:arXiv.org:2504.03598v1",
        "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline",
        "link": "https://arxiv.org/abs/2504.03598",
        "author": "Peter Baile Chen, Tomer Wolfson, Michael Cafarella, Dan Roth",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03598v1 Announce Type: new \nAbstract: Existing information retrieval systems excel in cases where the language of target documents closely matches that of the user query. However, real-world retrieval systems are often required to implicitly reason whether a document is relevant. For example, when retrieving technical texts or tables, their relevance to the user query may be implied through a particular jargon or structure, rather than explicitly expressed in their content. Large language models (LLMs) hold great potential in identifying such implied relevance by leveraging their reasoning skills. Nevertheless, current LLM-augmented retrieval is hindered by high latency and computation cost, as the LLM typically computes the query-document relevance online, for every query anew. To tackle this issue we introduce EnrichIndex, a retrieval approach which instead uses the LLM offline to build semantically-enriched retrieval indices, by performing a single pass over all documents in the retrieval corpus once during ingestion time. Furthermore, the semantically-enriched indices can complement existing online retrieval approaches, boosting the performance of LLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving passages and tables, and found that it outperforms strong online LLM-based retrieval systems, with an average improvement of 11.7 points in recall @ 10 and 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online calls to the LLM, it processes 293.3 times fewer tokens which greatly reduces the online latency and cost. Overall, EnrichIndex is an effective way to build better retrieval indices offline by leveraging the strong reasoning skills of LLMs."
      },
      {
        "id": "oai:arXiv.org:2504.03601v1",
        "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
        "link": "https://arxiv.org/abs/2504.03601",
        "author": "Akshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Shiyu Wang, Zhiwei Liu, Tulika Awalgaonkar, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03601v1 Announce Type: new \nAbstract: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io"
      },
      {
        "id": "oai:arXiv.org:2504.03602v1",
        "title": "Robust Human Registration with Body Part Segmentation on Noisy Point Clouds",
        "link": "https://arxiv.org/abs/2504.03602",
        "author": "Kai Lascheit, Daniel Barath, Marc Pollefeys, Leonidas Guibas, Francis Engelmann",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03602v1 Announce Type: new \nAbstract: Registering human meshes to 3D point clouds is essential for applications such as augmented reality and human-robot interaction but often yields imprecise results due to noise and background clutter in real-world data. We introduce a hybrid approach that incorporates body-part segmentation into the mesh fitting process, enhancing both human pose estimation and segmentation accuracy. Our method first assigns body part labels to individual points, which then guide a two-step SMPL-X fitting: initial pose and orientation estimation using body part centroids, followed by global refinement of the point cloud alignment. Additionally, we demonstrate that the fitted human mesh can refine body part labels, leading to improved segmentation. Evaluations on the cluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that our approach significantly outperforms prior methods in both pose estimation and segmentation accuracy. Code and results are available on our project website: https://segfit.github.io"
      },
      {
        "id": "oai:arXiv.org:2504.03607v1",
        "title": "Multimodal Diffusion Bridge with Attention-Based SAR Fusion for Satellite Image Cloud Removal",
        "link": "https://arxiv.org/abs/2504.03607",
        "author": "Yuyang Hu, Suhas Lohit, Ulugbek S. Kamilov, Tim K. Marks",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03607v1 Announce Type: new \nAbstract: Deep learning has achieved some success in addressing the challenge of cloud removal in optical satellite images, by fusing with synthetic aperture radar (SAR) images. Recently, diffusion models have emerged as powerful tools for cloud removal, delivering higher-quality estimation by sampling from cloud-free distributions, compared to earlier methods. However, diffusion models initiate sampling from pure Gaussian noise, which complicates the sampling trajectory and results in suboptimal performance. Also, current methods fall short in effectively fusing SAR and optical data. To address these limitations, we propose Diffusion Bridges for Cloud Removal, DB-CR, which directly bridges between the cloudy and cloud-free image distributions. In addition, we propose a novel multimodal diffusion bridge architecture with a two-branch backbone for multimodal image restoration, incorporating an efficient backbone and dedicated cross-modality fusion blocks to effectively extract and fuse features from synthetic aperture radar (SAR) and optical images. By formulating cloud removal as a diffusion-bridge problem and leveraging this tailored architecture, DB-CR achieves high-fidelity results while being computationally efficient. We evaluated DB-CR on the SEN12MS-CR cloud-removal dataset, demonstrating that it achieves state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2504.03612v1",
        "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset",
        "link": "https://arxiv.org/abs/2504.03612",
        "author": "Bingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen Hong, Longtao Huang, Hui Xue, Ganqu Cui, Wanxiang Che, Zhiyuan Liu, Maosong Sun",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03612v1 Announce Type: new \nAbstract: Preference learning is critical for aligning large language models (LLMs) with human values, yet its success hinges on high-quality datasets comprising three core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions, and \\textbf{R}esponse Pairs. Current approaches conflate these components, obscuring their individual impacts and hindering systematic optimization. In this work, we propose \\textbf{AIR}, a component-wise analysis framework that systematically isolates and optimizes each component while evaluating their synergistic effects. Through rigorous experimentation, AIR reveals actionable principles: annotation simplicity (point-wise generative scoring), instruction inference stability (variance-based filtering across LLMs), and response pair quality (moderate margins + high absolute scores). When combined, these principles yield +5.3 average gains over baseline method, even with only 14k high-quality pairs. Our work shifts preference dataset design from ad hoc scaling to component-aware optimization, offering a blueprint for efficient, reproducible alignment."
      },
      {
        "id": "oai:arXiv.org:2504.03615v1",
        "title": "Autonomous and Self-Adapting System for Synthetic Media Detection and Attribution",
        "link": "https://arxiv.org/abs/2504.03615",
        "author": "Aref Azizpour, Tai D. Nguyen, Matthew C. Stamm",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03615v1 Announce Type: new \nAbstract: Rapid advances in generative AI have enabled the creation of highly realistic synthetic images, which, while beneficial in many domains, also pose serious risks in terms of disinformation, fraud, and other malicious applications. Current synthetic image identification systems are typically static, relying on feature representations learned from known generators; as new generative models emerge, these systems suffer from severe performance degradation. In this paper, we introduce the concept of an autonomous self-adaptive synthetic media identification system -- one that not only detects synthetic images and attributes them to known sources but also autonomously identifies and incorporates novel generators without human intervention. Our approach leverages an open-set identification strategy with an evolvable embedding space that distinguishes between known and unknown sources. By employing an unsupervised clustering method to aggregate unknown samples into high-confidence clusters and continuously refining its decision boundaries, our system maintains robust detection and attribution performance even as the generative landscape evolves. Extensive experiments demonstrate that our method significantly outperforms existing approaches, marking a crucial step toward universal, adaptable forensic systems in the era of rapidly advancing generative models."
      },
      {
        "id": "oai:arXiv.org:2504.03616v1",
        "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task",
        "link": "https://arxiv.org/abs/2504.03616",
        "author": "Leonardo Ranaldi, Barry Haddow, Alexandra Birch",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03616v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) has become a cornerstone of contemporary NLP, enhancing large language models (LLMs) by allowing them to access richer factual contexts through in-context retrieval. While effective in monolingual settings, especially in English, its use in multilingual tasks remains unexplored. This paper investigates the effectiveness of RAG across multiple languages by proposing novel approaches for multilingual open-domain question-answering. We evaluate the performance of various multilingual RAG strategies, including question-translation (tRAG), which translates questions into English before retrieval, and Multilingual RAG (MultiRAG), where retrieval occurs directly across multiple languages. Our findings reveal that tRAG, while useful, suffers from limited coverage. In contrast, MultiRAG improves efficiency by enabling multilingual retrieval but introduces inconsistencies due to cross-lingual variations in the retrieved content. To address these issues, we propose Crosslingual RAG (CrossRAG), a method that translates retrieved documents into a common language (e.g., English) before generating the response. Our experiments show that CrossRAG significantly enhances performance on knowledge-intensive tasks, benefiting both high-resource and low-resource languages."
      },
      {
        "id": "oai:arXiv.org:2504.03621v1",
        "title": "VISTA-OCR: Towards generative and interactive end to end OCR models",
        "link": "https://arxiv.org/abs/2504.03621",
        "author": "Laziz Hamdi, Amine Tamasna, Pascal Boisson, Thierry Paquet",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03621v1 Announce Type: new \nAbstract: We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis OCR), a lightweight architecture that unifies text detection and recognition within a single generative model. Unlike conventional methods that require separate branches with dedicated parameters for text recognition and detection, our approach leverages a Transformer decoder to sequentially generate text transcriptions and their spatial coordinates in a unified branch. Built on an encoder-decoder architecture, VISTA-OCR is progressively trained, starting with the visual feature extraction phase, followed by multitask learning with multimodal token generation. To address the increasing demand for versatile OCR systems capable of advanced tasks, such as content-based text localization \\ref{content_based_localization}, we introduce new prompt-controllable OCR tasks during pre-training.To enhance the model's capabilities, we built a new dataset composed of real-world examples enriched with bounding box annotations and synthetic samples. Although recent Vision Large Language Models (VLLMs) can efficiently perform these tasks, their high computational cost remains a barrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$ variant processes both handwritten and printed documents with only 150M parameters, interactively, by prompting. Extensive experiments on multiple datasets demonstrate that VISTA-OCR achieves better performance compared to state-of-the-art specialized models on standard OCR tasks while showing strong potential for more sophisticated OCR applications, addressing the growing need for interactive OCR systems. All code and annotations for VISTA-OCR will be made publicly available upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2504.03622v1",
        "title": "Align to Structure: Aligning Large Language Models with Structural Information",
        "link": "https://arxiv.org/abs/2504.03622",
        "author": "Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03622v1 Announce Type: new \nAbstract: Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at https://github.com/minnesotanlp/struct_align."
      },
      {
        "id": "oai:arXiv.org:2504.03623v1",
        "title": "Quantifying the uncertainty of model-based synthetic image quality metrics",
        "link": "https://arxiv.org/abs/2504.03623",
        "author": "Ciaran Bench, Spencer A. Thomas",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03623v1 Announce Type: new \nAbstract: The quality of synthetically generated images (e.g. those produced by diffusion models) are often evaluated using information about image contents encoded by pretrained auxiliary models. For example, the Fr\\'{e}chet Inception Distance (FID) uses embeddings from an InceptionV3 model pretrained to classify ImageNet. The effectiveness of this feature embedding model has considerable impact on the trustworthiness of the calculated metric (affecting its suitability in several domains, including medical imaging). Here, uncertainty quantification (UQ) is used to provide a heuristic measure of the trustworthiness of the feature embedding model and an FID-like metric called the Fr\\'{e}chet Autoencoder Distance (FAED). We apply Monte Carlo dropout to a feature embedding model (convolutional autoencoder) to model the uncertainty in its embeddings. The distribution of embeddings for each input are then used to compute a distribution of FAED values. We express uncertainty as the predictive variance of the embeddings as well as the standard deviation of the computed FAED values. We find that their magnitude correlates with the extent to which the inputs are out-of-distribution to the model's training data, providing some validation of its ability to assess the trustworthiness of the FAED."
      },
      {
        "id": "oai:arXiv.org:2504.03624v1",
        "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
        "link": "https://arxiv.org/abs/2504.03624",
        "author": "NVIDIA,  :, Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl, Marta Stepniewska-Dziubinska, Matthieu Le, Matvei Novikov, Mehrzad Samadi, Michael Andersch, Michael Evans, Miguel Martinez, Mike Chrzanowski, Mike Ranzinger, Mikolaj Blaz, Misha Smelyanskiy, Mohamed Fawzy, Mohammad Shoeybi, Mostofa Patwary, Nayeon Lee, Nima Tajbakhsh, Ning Xu, Oleg Rybakov, Oleksii Kuchaiev, Olivier Delalleau, Osvald Nitski, Parth Chadha, Pasha Shamis, Paulius Micikevicius, Pavlo Molchanov, Peter Dykas, Philipp Fischer, Pierre-Yves Aquilanti, Piotr Bialecki, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi, Rahul Kandu, Ran El-Yaniv, Raviraj Joshi, Roger Waleffe, Ruoxi Zhang, Sabrina Kavanaugh, Sahil Jain, Samuel Kriman, Sangkug Lym, Sanjeev Satheesh, Saurav Muralidharan, Sean Narenthiran, Selvaraj Anandaraj, Seonmyeong Bak, Sergey Kashirsky, Seungju Han, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Sharon Clay, Shelby Thomas, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shyamala Prayaga, Siddhartha Jain, Sirshak Das, Slawek Kierat, Somshubra Majumdar, Song Han, Soumye Singhal, Sriharsha Niverty, Stefania Alborghetti, Suseella Panguluri, Swetha Bhendigeri, Syeda Nahida Akter, Szymon Migacz, Tal Shiri, Terry Kong, Timo Roman, Tomer Ronen, Trisha Saar, Tugrul Konuk, Tuomas Rintamaki, Tyler Poon, Ushnish De, Vahid Noroozi, Varun Singh, Vijay Korthikanti, Vitaly Kurin, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenliang Dai, Wonmin Byeon, Xiaowei Ren, Yao Xu, Yejin Choi, Yian Zhang, Ying Lin, Yoshi Suhara, Zhiding Yu, Zhiqi Li, Zhiyu Li, Zhongbo Zhu, Zhuolin Yang, Zijia Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03624v1 Announce Type: new \nAbstract: As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. All Nemotron-H models will be released, with support in Hugging Face, NeMo, and Megatron-LM."
      },
      {
        "id": "oai:arXiv.org:2504.03625v1",
        "title": "Reciprocity-Aware Convolutional Neural Networks for Map-Based Path Loss Prediction",
        "link": "https://arxiv.org/abs/2504.03625",
        "author": "Ryan G. Dempsey, Jonathan Ethier, Halim Yanikomeroglu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03625v1 Announce Type: new \nAbstract: Path loss modeling is a widely used technique for estimating point-to-point losses along a communications link from transmitter (Tx) to receiver (Rx). Accurate path loss predictions can optimize use of the radio frequency spectrum and minimize unwanted interference. Modern path loss modeling often leverages data-driven approaches, using machine learning to train models on drive test measurement datasets. Drive tests primarily represent downlink scenarios, where the Tx is located on a building and the Rx is located on a moving vehicle. Consequently, trained models are frequently reserved for downlink coverage estimation, lacking representation of uplink scenarios. In this paper, we demonstrate that data augmentation can be used to train a path loss model that is generalized to uplink, downlink, and backhaul scenarios, training using only downlink drive test measurements. By adding a small number of synthetic samples representing uplink scenarios to the training set, root mean squared error is reduced by >8 dB on uplink examples in the test set."
      },
      {
        "id": "oai:arXiv.org:2504.03637v1",
        "title": "An Algebraic Geometry Approach to Viewing Graph Solvability",
        "link": "https://arxiv.org/abs/2504.03637",
        "author": "Federica Arrigoni, Kathl\\'en Kohn, Andrea Fusiello, Tomas Pajdla",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03637v1 Announce Type: new \nAbstract: The concept of viewing graph solvability has gained significant interest in the context of structure-from-motion. A viewing graph is a mathematical structure where nodes are associated to cameras and edges represent the epipolar geometry connecting overlapping views. Solvability studies under which conditions the cameras are uniquely determined by the graph. In this paper we propose a novel framework for analyzing solvability problems based on Algebraic Geometry, demonstrating its potential in understanding structure-from-motion graphs and proving a conjecture that was previously proposed."
      },
      {
        "id": "oai:arXiv.org:2504.03639v1",
        "title": "Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions",
        "link": "https://arxiv.org/abs/2504.03639",
        "author": "Ting-Hsuan Liao, Yi Zhou, Yu Shen, Chun-Hao Paul Huang, Saayan Mitra, Jia-Bin Huang, Uttaran Bhattacharya",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03639v1 Announce Type: new \nAbstract: We explore how body shapes influence human motion synthesis, an aspect often overlooked in existing text-to-motion generation methods due to the ease of learning a homogenized, canonical body shape. However, this homogenization can distort the natural correlations between different body shapes and their motion dynamics. Our method addresses this gap by generating body-shape-aware human motions from natural language prompts. We utilize a finite scalar quantization-based variational autoencoder (FSQ-VAE) to quantize motion into discrete tokens and then leverage continuous body shape information to de-quantize these tokens back into continuous, detailed motion. Additionally, we harness the capabilities of a pretrained language model to predict both continuous shape parameters and motion tokens, facilitating the synthesis of text-aligned motions and decoding them into shape-aware motions. We evaluate our method quantitatively and qualitatively, and also conduct a comprehensive perceptual study to demonstrate its efficacy in generating shape-aware motions."
      },
      {
        "id": "oai:arXiv.org:2504.03640v1",
        "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning",
        "link": "https://arxiv.org/abs/2504.03640",
        "author": "Kate Sanders, Benjamin Van Durme",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03640v1 Announce Type: new \nAbstract: To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces."
      },
      {
        "id": "oai:arXiv.org:2504.03641v1",
        "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models",
        "link": "https://arxiv.org/abs/2504.03641",
        "author": "Wulin Xie, Yi-Fan Zhang, Chaoyou Fu, Yang Shi, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, Tieniu Tan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03641v1 Announce Type: new \nAbstract: Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/."
      },
      {
        "id": "oai:arXiv.org:2504.02830v1",
        "title": "DualMS: Implicit Dual-Channel Minimal Surface Optimization for Heat Exchanger Design",
        "link": "https://arxiv.org/abs/2504.02830",
        "author": "Weizheng Zhang (Shandong University), Hao Pan (Tsinghua University), Lin Lu (Shandong University), Xiaowei Duan (Shandong University), Xin Yan (Shandong University), Ruonan Wang (Institute of Engineering Thermophysics, Chinese Academy of Sciences), Qiang Du (Institute of Engineering Thermophysics, Chinese Academy of Sciences)",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02830v1 Announce Type: cross \nAbstract: Heat exchangers are critical components in a wide range of engineering applications, from energy systems to chemical processing, where efficient thermal management is essential. The design objectives for heat exchangers include maximizing the heat exchange rate while minimizing the pressure drop, requiring both a large interface area and a smooth internal structure. State-of-the-art designs, such as triply periodic minimal surfaces (TPMS), have proven effective in optimizing heat exchange efficiency. However, TPMS designs are constrained by predefined mathematical equations, limiting their adaptability to freeform boundary shapes. Additionally, TPMS structures do not inherently control flow directions, which can lead to flow stagnation and undesirable pressure drops.\n  This paper presents DualMS, a novel computational framework for optimizing dual-channel minimal surfaces specifically for heat exchanger designs in freeform shapes. To the best of our knowledge, this is the first attempt to directly optimize minimal surfaces for two-fluid heat exchangers, rather than relying on TPMS. Our approach formulates the heat exchange maximization problem as a constrained connected maximum cut problem on a graph, with flow constraints guiding the optimization process. To address undesirable pressure drops, we model the minimal surface as a classification boundary separating the two fluids, incorporating an additional regularization term for area minimization. We employ a neural network that maps spatial points to binary flow types, enabling it to classify flow skeletons and automatically determine the surface boundary. DualMS demonstrates greater flexibility in surface topology compared to TPMS and achieves superior thermal performance, with lower pressure drops while maintaining a similar heat exchange rate under the same material cost."
      },
      {
        "id": "oai:arXiv.org:2504.02833v1",
        "title": "Scalable Min-Max Optimization via Primal-Dual Exact Pareto Optimization",
        "link": "https://arxiv.org/abs/2504.02833",
        "author": "Sangwoo Park, Stefan Vlaski, Lajos Hanzo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02833v1 Announce Type: cross \nAbstract: In multi-objective optimization, minimizing the worst objective can be preferable to minimizing the average objective, as this ensures improved fairness across objectives. Due to the non-smooth nature of the resultant min-max optimization problem, classical subgradient-based approaches typically exhibit slow convergence. Motivated by primal-dual consensus techniques in multi-agent optimization and learning, we formulate a smooth variant of the min-max problem based on the augmented Lagrangian. The resultant Exact Pareto Optimization via Augmented Lagrangian (EPO-AL) algorithm scales better with the number of objectives than subgradient-based strategies, while exhibiting lower per-iteration complexity than recent smoothing-based counterparts. We establish that every fixed-point of the proposed algorithm is both Pareto and min-max optimal under mild assumptions and demonstrate its effectiveness in numerical simulations."
      },
      {
        "id": "oai:arXiv.org:2504.02834v1",
        "title": "Explainable Dual-Attention Tabular Transformer for Soil Electrical Resistivity Prediction: A Decision Support Framework for High-Voltage Substation Construction",
        "link": "https://arxiv.org/abs/2504.02834",
        "author": "Warat Kongkitkul, Sompote Youwai, Warut Sakulpojworachai",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02834v1 Announce Type: cross \nAbstract: This research introduces a novel dual-attention transformer architecture for predicting soil electrical resistivity, a critical parameter for high-voltage substation construction. Our model employs attention mechanisms operating across both features and data batches, enhanced by feature embedding layers that project inputs into higher-dimensional spaces. We implements Particle Swarm Optimization for hyperparameter tuning, systematically optimizing embedding dimensions, attention heads, and neural network architecture. The proposed architecture achieves superior predictive performance (Mean Absolute Percentage Error: 0.63%) compared to recent state of the art models for tabular data. Crucially, our model maintains explainability through SHapley Additive exPlanations value analysis, revealing that fine particle content and dry density are the most influential parameters affecting soil resistivity. We developes a web-based application implementing this model to provide engineers with an accessible decision support framework that bridges geotechnical and electrical engineering requirements for the Electricity Generating Authority of Thailand. This integrated approach satisfies both structural stability and electrical safety standards, improving construction efficiency and safety compliance in high-voltage infrastructure implementation."
      },
      {
        "id": "oai:arXiv.org:2504.02839v1",
        "title": "PETIMOT: A Novel Framework for Inferring Protein Motions from Sparse Data Using SE(3)-Equivariant Graph Neural Networks",
        "link": "https://arxiv.org/abs/2504.02839",
        "author": "Valentin Lombard, Sergei Grudinin, Elodie Laine",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02839v1 Announce Type: cross \nAbstract: Proteins move and deform to ensure their biological functions. Despite significant progress in protein structure prediction, approximating conformational ensembles at physiological conditions remains a fundamental open problem. This paper presents a novel perspective on the problem by directly targeting continuous compact representations of protein motions inferred from sparse experimental observations. We develop a task-specific loss function enforcing data symmetries, including scaling and permutation operations. Our method PETIMOT (Protein sEquence and sTructure-based Inference of MOTions) leverages transfer learning from pre-trained protein language models through an SE(3)-equivariant graph neural network. When trained and evaluated on the Protein Data Bank, PETIMOT shows superior performance in time and accuracy, capturing protein dynamics, particularly large/slow conformational changes, compared to state-of-the-art flow-matching approaches and traditional physics-based models."
      },
      {
        "id": "oai:arXiv.org:2504.02842v1",
        "title": "Enhanced ECG Arrhythmia Detection Accuracy by Optimizing Divergence-Based Data Fusion",
        "link": "https://arxiv.org/abs/2504.02842",
        "author": "Baozhuo Su, Qingli Dou, Kang Liu, Zhengxian Qu, Jerry Deng, Ting Tan, Yanan Gu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02842v1 Announce Type: cross \nAbstract: AI computation in healthcare faces significant challenges when clinical datasets are limited and heterogeneous. Integrating datasets from multiple sources and different equipments is critical for effective AI computation but is complicated by their diversity, complexity, and lack of representativeness, so we often need to join multiple datasets for analysis. The currently used method is fusion after normalization. But when using this method, it can introduce redundant information, decreasing the signal-to-noise ratio and reducing classification accuracy. To tackle this issue, we propose a feature-based fusion algorithm utilizing Kernel Density Estimation (KDE) and Kullback-Leibler (KL) divergence. Our approach involves initially preprocessing and continuous estimation on the extracted features, followed by employing the gradient descent method to identify the optimal linear parameters that minimize the KL divergence between the feature distributions. Using our in-house datasets consisting of ECG signals collected from 2000 healthy and 2000 diseased individuals by different equipments and verifying our method by using the publicly available PTB-XL dataset which contains 21,837 ECG recordings from 18,885 patients. We employ a Light Gradient Boosting Machine (LGBM) model to do the binary classification. The results demonstrate that the proposed fusion method significantly enhances feature-based classification accuracy for abnormal ECG cases in the merged datasets, compared to the normalization method. This data fusion strategy provides a new approach to process heterogeneous datasets for the optimal AI computation results."
      },
      {
        "id": "oai:arXiv.org:2504.02848v1",
        "title": "Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation",
        "link": "https://arxiv.org/abs/2504.02848",
        "author": "Yuta Yahagi, Kiichi Obuchi, Fumihiko Kosaka, Kota Matsui",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02848v1 Announce Type: cross \nAbstract: Simulation-to-Real (Sim2Real) transfer learning, the machine learning technique that efficiently solves a real-world task by leveraging knowledge from computational data, has received increasing attention in materials science as a promising solution to the scarcity of experimental data. We proposed an efficient transfer learning scheme from first-principles calculations to experiments based on the chemistry-informed domain transformation, that integrates the heterogeneous source and target domains by harnessing the underlying physics and chemistry. The proposed method maps the computational data from the simulation space (source domain) into the space of experimental data (target domain). During this process, these qualitatively different domains are efficiently bridged by prior knowledge of chemistry, the statistical ensemble and the relationship between source and target quantities. As a proof-of-concept, we predict the catalyst activity for the reverse water-gas shift reaction by using the abundant first-principles data in addition to the experimental data. Through the demonstration, we confirmed that the transfer learning model exhibits positive transfer in accuracy and data efficiency. In particular, a significantly high accuracy was achieved despite using a few (less than ten) target data in domain transformation, whose accuracy is one order of magnitude smaller than that of a full scratch model trained with over 100 target data. This result indicates that the proposed method leverages the high prediction performance with few target data, which helps to save the number of trials in real laboratories."
      },
      {
        "id": "oai:arXiv.org:2504.02854v1",
        "title": "Efficient First-Order Optimization on the Pareto Set for Multi-Objective Learning under Preference Guidance",
        "link": "https://arxiv.org/abs/2504.02854",
        "author": "Lisha Chen, Quan Xiao, Ellen Hidemi Fukuda, Xinyi Chen, Kun Yuan, Tianyi Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02854v1 Announce Type: cross \nAbstract: Multi-objective learning under user-specified preference is common in real-world problems such as multi-lingual speech recognition under fairness. In this work, we frame such a problem as a semivectorial bilevel optimization problem, whose goal is to optimize a pre-defined preference function, subject to the constraint that the model parameters are weakly Pareto optimal. To solve this problem, we convert the multi-objective constraints to a single-objective constraint through a merit function with an easy-to-evaluate gradient, and then, we use a penalty-based reformulation of the bilevel optimization problem. We theoretically establish the properties of the merit function, and the relations of solutions for the penalty reformulation and the constrained formulation. Then we propose algorithms to solve the reformulated single-level problem, and establish its convergence guarantees. We test the method on various synthetic and real-world problems. The results demonstrate the effectiveness of the proposed method in finding preference-guided optimal solutions to the multi-objective problem."
      },
      {
        "id": "oai:arXiv.org:2504.02856v1",
        "title": "The epistemic dimension of algorithmic fairness: assessing its impact in innovation diffusion and fair policy making",
        "link": "https://arxiv.org/abs/2504.02856",
        "author": "Eugenia Villa, Camilla Quaresmini, Valentina Breschi, Viola Schiaffonati, Mara Tanelli",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02856v1 Announce Type: cross \nAbstract: Algorithmic fairness is an expanding field that addresses a range of discrimination issues associated with algorithmic processes. However, most works in the literature focus on analyzing it only from an ethical perspective, focusing on moral principles and values that should be considered in the design and evaluation of algorithms, while disregarding the epistemic dimension related to knowledge transmission and validation. However, this aspect of algorithmic fairness should also be included in the debate, as it is crucial to introduce a specific type of harm: an individual may be systematically excluded from the dissemination of knowledge due to the attribution of a credibility deficit/excess. In this work, we specifically focus on characterizing and analyzing the impact of this credibility deficit or excess on the diffusion of innovations on a societal scale, a phenomenon driven by individual attitudes and social interactions, and also by the strength of mutual connections. Indeed, discrimination might shape the latter, ultimately modifying how innovations spread within the network. In this light, to incorporate, also from a formal point of view, the epistemic dimension in innovation diffusion models becomes paramount, especially if these models are intended to support fair policy design. For these reasons, we formalize the epistemic properties of a social environment, by extending the well-established Linear Threshold Model (LTM) in an epistemic direction to show the impact of epistemic biases in innovation diffusion. Focusing on the impact of epistemic bias in both open-loop and closed-loop scenarios featuring optimal fostering policies, our results shed light on the pivotal role the epistemic dimension might have in the debate of algorithmic fairness in decision-making."
      },
      {
        "id": "oai:arXiv.org:2504.02868v1",
        "title": "Machine Learning Prediction of Cardiovascular Risk in Type 1 Diabetes Mellitus Using Radiomics Features from Multimodal Retinal Images",
        "link": "https://arxiv.org/abs/2504.02868",
        "author": "Ariadna Toh\\`a-Dalmau (Department of Computer Science, Universitat Polit\\`ecnica de Catalunya), Josep Rosin\\'es-Fonoll (Institut Cl\\'inic d'Oftalmolog\\'ia, Hospital Cl\\'inic de Barcelona), Enrique Romero (Department of Computer Science, Universitat Polit\\`ecnica de Catalunya, Intelligent Data Science and Artificial Intelligence Research Center), Ferran Mazzanti (Department of Physics, Universitat Polit\\`ecnica de Catalunya), Ruben Martin-Pinardel (August Pi i Sunyer Biomedical Research Institute), Sonia Marias-Perez (Institut Cl\\'inic d'Oftalmolog\\'ia, Hospital Cl\\'inic de Barcelona), Carolina Bernal-Morales (Institut Cl\\'inic d'Oftalmolog\\'ia, Hospital Cl\\'inic de Barcelona, Diabetes Unit, Hospital Cl\\'inic de Barcelona), Rafael Castro-Dominguez (Institut Cl\\'inic d'Oftalmolog\\'ia, Hospital Cl\\'inic de Barcelona), Andrea Mendez (Institut Cl\\'inic d'Oftalmolog\\'ia, Hospital Cl\\'inic de Barcelona), Emilio Ortega (August Pi i Sunyer Biomedical Research Institute, School of Medicine, Universitat de Barcelona), Irene Vinagre (August Pi i Sunyer Biomedical Research Institute, School of Medicine, Universitat de Barcelona), Marga Gimenez (August Pi i Sunyer Biomedical Research Institute, School of Medicine, Universitat de Barcelona), Alfredo Vellido (Department of Computer Science, Universitat Polit\\`ecnica de Catalunya, Intelligent Data Science and Artificial Intelligence Research Center), Javier Zarranz-Ventura (Institut Cl\\'inic d'Oftalmolog\\'ia, Hospital Cl\\'inic de Barcelona, School of Medicine, Universitat de Barcelona)",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02868v1 Announce Type: cross \nAbstract: This study aimed to develop a machine learning (ML) algorithm capable of determining cardiovascular risk in multimodal retinal images from patients with type 1 diabetes mellitus, distinguishing between moderate, high, and very high-risk levels. Radiomic features were extracted from fundus retinography, optical coherence tomography (OCT), and OCT angiography (OCTA) images. ML models were trained using these features either individually or combined with clinical data. A dataset of 597 eyes (359 individuals) was analyzed, and models trained only with radiomic features achieved AUC values of (0.79 $\\pm$ 0.03) for identifying moderate risk cases from high and very high-risk cases, and (0.73 $\\pm$ 0.07) for distinguishing between high and very high-risk cases. The addition of clinical variables improved all AUC values, reaching (0.99 $\\pm$ 0.01) for identifying moderate risk cases and (0.95 $\\pm$ 0.02) for differentiating between high and very high-risk cases. For very high CV risk, radiomics combined with OCT+OCTA metrics and ocular data achieved an AUC of (0.89 $\\pm$ 0.02) without systemic data input. These results demonstrate that radiomic features obtained from multimodal retinal images are useful for discriminating and classifying CV risk labels, highlighting the potential of this oculomics approach for CV risk assessment."
      },
      {
        "id": "oai:arXiv.org:2504.02880v1",
        "title": "Global Rice Multi-Class Segmentation Dataset (RiceSEG): A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms",
        "link": "https://arxiv.org/abs/2504.02880",
        "author": "Junchi Zhou, Haozhou Wang, Yoichiro Kato, Tejasri Nampally, P. Rajalakshmi, M. Balram, Keisuke Katsura, Hao Lu, Yue Mu, Wanneng Yang, Yangmingrui Gao, Feng Xiao, Hongtao Chen, Yuhao Chen, Wenjuan Li, Jingwen Wang, Fenghua Yu, Jian Zhou, Wensheng Wang, Xiaochun Hu, Yuanzhu Yang, Yanfeng Ding, Wei Guo, Shouyang Liu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02880v1 Announce Type: cross \nAbstract: Developing computer vision-based rice phenotyping techniques is crucial for precision field management and accelerating breeding, thereby continuously advancing rice production. Among phenotyping tasks, distinguishing image components is a key prerequisite for characterizing plant growth and development at the organ scale, enabling deeper insights into eco-physiological processes. However, due to the fine structure of rice organs and complex illumination within the canopy, this task remains highly challenging, underscoring the need for a high-quality training dataset. Such datasets are scarce, both due to a lack of large, representative collections of rice field images and the time-intensive nature of annotation. To address this gap, we established the first comprehensive multi-class rice semantic segmentation dataset, RiceSEG. We gathered nearly 50,000 high-resolution, ground-based images from five major rice-growing countries (China, Japan, India, the Philippines, and Tanzania), encompassing over 6,000 genotypes across all growth stages. From these original images, 3,078 representative samples were selected and annotated with six classes (background, green vegetation, senescent vegetation, panicle, weeds, and duckweed) to form the RiceSEG dataset. Notably, the sub-dataset from China spans all major genotypes and rice-growing environments from the northeast to the south. Both state-of-the-art convolutional neural networks and transformer-based semantic segmentation models were used as baselines. While these models perform reasonably well in segmenting background and green vegetation, they face difficulties during the reproductive stage, when canopy structures are more complex and multiple classes are involved. These findings highlight the importance of our dataset for developing specialized segmentation models for rice and other crops."
      },
      {
        "id": "oai:arXiv.org:2504.02905v1",
        "title": "Scenario Discovery for Urban Planning: The Case of Green Urbanism and the Impact on Stress",
        "link": "https://arxiv.org/abs/2504.02905",
        "author": "Lorena Torres Lahoz, Carlos Lima Azevedo, Leonardo Ancora, Paulo Morgado, Zenia Kotval, Bruno Miranda, Francisco Camara Pereira",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02905v1 Announce Type: cross \nAbstract: Urban environments significantly influence mental health outcomes, yet the role of an effective framework for decision-making under deep uncertainty (DMDU) for optimizing urban policies for stress reduction remains underexplored. While existing research has demonstrated the effects of urban design on mental health, there is a lack of systematic scenario-based analysis to guide urban planning decisions. This study addresses this gap by applying Scenario Discovery (SD) in urban planning to evaluate the effectiveness of urban vegetation interventions in stress reduction across different urban environments using a predictive model based on emotional responses collected from a neuroscience-based outdoor experiment in Lisbon. Combining these insights with detailed urban data from Copenhagen, we identify key intervention thresholds where vegetation-based solutions succeed or fail in mitigating stress responses. Our findings reveal that while increased vegetation generally correlates with lower stress levels, high-density urban environments, crowding, and individual psychological traits (e.g., extraversion) can reduce its effectiveness. This work showcases our Scenario Discovery framework as a systematic approach for identifying robust policy pathways in urban planning, opening the door for its exploration in other urban decision-making contexts where uncertainty and design resiliency are critical."
      },
      {
        "id": "oai:arXiv.org:2504.02909v1",
        "title": "Enhancing Air Quality Monitoring: A Brief Review of Federated Learning Advances",
        "link": "https://arxiv.org/abs/2504.02909",
        "author": "Sara Yarham, Mehran Behjati",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02909v1 Announce Type: cross \nAbstract: Monitoring air quality and environmental conditions is crucial for public health and effective urban planning. Current environmental monitoring approaches often rely on centralized data collection and processing, which pose significant privacy, security, and scalability challenges. Federated Learning (FL) offers a promising solution to these limitations by enabling collaborative model training across multiple devices without sharing raw data. This decentralized approach addresses privacy concerns while still leveraging distributed data sources. This paper provides a comprehensive review of FL applications in air quality and environmental monitoring, emphasizing its effectiveness in predicting pollutants and managing environmental data. However, the paper also identifies key limitations of FL when applied in this domain, including challenges such as communication overhead, infrastructure demands, generalizability issues, computational complexity, and security vulnerabilities. For instance, communication overhead, caused by the frequent exchange of model updates between local devices and central servers, is a notable challenge. To address this, future research should focus on optimizing communication protocols and reducing the frequency of updates to lessen the burden on network resources. Additionally, the paper suggests further research directions to refine FL frameworks and enhance their applicability in real-world environmental monitoring scenarios. By synthesizing findings from existing studies, this paper highlights the potential of FL to improve air quality management while maintaining data privacy and security, and it provides valuable insights for future developments in the field."
      },
      {
        "id": "oai:arXiv.org:2504.02913v1",
        "title": "On Word-of-Mouth and Private-Prior Sequential Social Learning",
        "link": "https://arxiv.org/abs/2504.02913",
        "author": "Andrea Da Col, Cristian R. Rojas, Vikram Krishnamurthy",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02913v1 Announce Type: cross \nAbstract: Social learning provides a fundamental framework in economics and social sciences for studying interactions among rational agents who observe each other's actions but lack direct access to individual beliefs. This paper investigates a specific social learning paradigm known as Word-of-Mouth (WoM), where a series of agents seeks to estimate the state of a dynamical system. The first agent receives noisy measurements of the state, while each subsequent agent relies solely on a degraded version of her predecessor's estimate. A defining feature of WoM is that the final agent's belief is publicly broadcast and adopted by all agents, in place of their own. We analyze this setting both theoretically and through numerical simulations, showing that some agents benefit from using the public belief broadcast by the last agent, while others suffer from performance deterioration."
      },
      {
        "id": "oai:arXiv.org:2504.02916v1",
        "title": "Feature Engineering on LMS Data to Optimize Student Performance Prediction",
        "link": "https://arxiv.org/abs/2504.02916",
        "author": "Keith Hubbard, Sheilla Amponsah",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02916v1 Announce Type: cross \nAbstract: Nearly every educational institution uses a learning management system (LMS), often producing terabytes of data generated by thousands of people. We examine LMS grade and login data from a regional comprehensive university, specifically documenting key considerations for engineering features from these data when trying to predict student performance. We specifically document changes to LMS data patterns since Covid-19, which are critical for data scientists to account for when using historic data. We compare numerous engineered features and approaches to utilizing those features for machine learning. We finish with a summary of the implications of including these features into more comprehensive student performance models."
      },
      {
        "id": "oai:arXiv.org:2504.02919v1",
        "title": "ConfEviSurrogate: A Conformalized Evidential Surrogate Model for Uncertainty Quantification",
        "link": "https://arxiv.org/abs/2504.02919",
        "author": "Yuhan Duan, Xin Zhao, Neng Shi, Han-Wei Shen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02919v1 Announce Type: cross \nAbstract: Surrogate models, crucial for approximating complex simulation data across sciences, inherently carry uncertainties that range from simulation noise to model prediction errors. Without rigorous uncertainty quantification, predictions become unreliable and hence hinder analysis. While methods like Monte Carlo dropout and ensemble models exist, they are often costly, fail to isolate uncertainty types, and lack guaranteed coverage in prediction intervals. To address this, we introduce ConfEviSurrogate, a novel Conformalized Evidential Surrogate Model that can efficiently learn high-order evidential distributions, directly predict simulation outcomes, separate uncertainty sources, and provide prediction intervals. A conformal prediction-based calibration step further enhances interval reliability to ensure coverage and improve efficiency. Our ConfEviSurrogate demonstrates accurate predictions and robust uncertainty estimates in diverse simulations, including cosmology, ocean dynamics, and fluid dynamics."
      },
      {
        "id": "oai:arXiv.org:2504.02984v1",
        "title": "Language Models Guidance with Multi-Aspect-Cueing: A Case Study for Competitor Analysis",
        "link": "https://arxiv.org/abs/2504.02984",
        "author": "Amir Hadifar, Christopher Ochs, Arjan Van Ewijk",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02984v1 Announce Type: cross \nAbstract: Competitor analysis is essential in modern business due to the influence of industry rivals on strategic planning. It involves assessing multiple aspects and balancing trade-offs to make informed decisions. Recent Large Language Models (LLMs) have demonstrated impressive capabilities to reason about such trade-offs but grapple with inherent limitations such as a lack of knowledge about contemporary or future realities and an incomplete understanding of a market's competitive landscape. In this paper, we address this gap by incorporating business aspects into LLMs to enhance their understanding of a competitive market. Through quantitative and qualitative experiments, we illustrate how integrating such aspects consistently improves model performance, thereby enhancing analytical efficacy in competitor analysis."
      },
      {
        "id": "oai:arXiv.org:2504.02993v1",
        "title": "Route Recommendations for Traffic Management Under Learned Partial Driver Compliance",
        "link": "https://arxiv.org/abs/2504.02993",
        "author": "Heeseung Bang, Jung-Hoon Cho, Cathy Wu, Andreas A. Malikopoulos",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02993v1 Announce Type: cross \nAbstract: In this paper, we aim to mitigate congestion in traffic management systems by guiding travelers along system-optimal (SO) routes. However, we recognize that most theoretical approaches assume perfect driver compliance, which often does not reflect reality, as drivers tend to deviate from recommendations to fulfill their personal objectives. Therefore, we propose a route recommendation framework that explicitly learns partial driver compliance and optimizes traffic flow under realistic adherence. We first compute an SO edge flow through flow optimization techniques. Next, we train a compliance model based on historical driver decisions to capture individual responses to our recommendations. Finally, we formulate a stochastic optimization problem that minimizes the gap between the target SO flow and the realized flow under conditions of imperfect adherence. Our simulations conducted on a grid network reveal that our approach significantly reduces travel time compared to baseline strategies, demonstrating the practical advantage of incorporating learned compliance into traffic management."
      },
      {
        "id": "oai:arXiv.org:2504.03029v1",
        "title": "Ontologies in Design: How Imagining a Tree Reveals Possibilites and Assumptions in Large Language Models",
        "link": "https://arxiv.org/abs/2504.03029",
        "author": "Nava Haghighi, Sunny Yu, James Landay, Daniela Rosner",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03029v1 Announce Type: cross \nAbstract: Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies -- concerning what we allow ourselves to think or talk about -- is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems."
      },
      {
        "id": "oai:arXiv.org:2504.03035v1",
        "title": "High-dimensional ridge regression with random features for non-identically distributed data with a variance profile",
        "link": "https://arxiv.org/abs/2504.03035",
        "author": "Issa-Mbenard Dabo, J\\'er\\'emie Bigot",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03035v1 Announce Type: cross \nAbstract: The behavior of the random feature model in the high-dimensional regression framework has become a popular issue of interest in the machine learning literature}. This model is generally considered for feature vectors $x_i = \\Sigma^{1/2} x_i'$, where $x_i'$ is a random vector made of independent and identically distributed (iid) entries, and $\\Sigma$ is a positive definite matrix representing the covariance of the features.\n  In this paper, we move beyond {\\CB this standard assumption by studying the performances of the random features model in the setting of non-iid feature vectors}. Our approach is related to the analysis of the spectrum of large random matrices through random matrix theory (RMT) {\\CB and free probability} results. We turn to the analysis of non-iid data by using the notion of variance profile {\\CB which} is {\\CB well studied in RMT.} Our main contribution is then the study of the limits of the training and {\\CB prediction} risks associated to the ridge estimator in the random features model when its dimensions grow. We provide asymptotic equivalents of these risks that capture the behavior of ridge regression with random features in a {\\CB high-dimensional} framework. These asymptotic equivalents, {\\CB which prove to be sharp in numerical experiments}, are retrieved by adapting, to our setting, established results from operator-valued free probability theory. Moreover, {\\CB for various classes of random feature vectors that have not been considered so far in the literature}, our approach allows to show the appearance of the double descent phenomenon when the ridge regularization parameter is small enough."
      },
      {
        "id": "oai:arXiv.org:2504.03097v1",
        "title": "A computational transition for detecting multivariate shuffled linear regression by low-degree polynomials",
        "link": "https://arxiv.org/abs/2504.03097",
        "author": "Zhangsong Li",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03097v1 Announce Type: cross \nAbstract: In this paper, we study the problem of multivariate shuffled linear regression, where the correspondence between predictors and responses in a linear model is obfuscated by a latent permutation. Specifically, we investigate the model $Y=\\tfrac{1}{\\sqrt{1+\\sigma^2}}(\\Pi_* X Q_* + \\sigma Z)$, where $X$ is an $n*d$ standard Gaussian design matrix, $Z$ is an $n*m$ Gaussian noise matrix, $\\Pi_*$ is an unknown $n*n$ permutation matrix, and $Q_*$ is an unknown $d*m$ on the Grassmanian manifold satisfying $Q_*^{\\top} Q_* = \\mathbb I_m$.\n  Consider the hypothesis testing problem of distinguishing this model from the case where $X$ and $Y$ are independent Gaussian random matrices of sizes $n*d$ and $n*m$, respectively. Our results reveal a phase transition phenomenon in the performance of low-degree polynomial algorithms for this task. (1) When $m=o(d)$, we show that all degree-$D$ polynomials fail to distinguish these two models even when $\\sigma=0$, provided with $D^4=o\\big( \\tfrac{d}{m} \\big)$. (2) When $m=d$ and $\\sigma=\\omega(1)$, we show that all degree-$D$ polynomials fail to distinguish these two models provided with $D=o(\\sigma)$. (3) When $m=d$ and $\\sigma=o(1)$, we show that there exists a constant-degree polynomial that strongly distinguish these two models. These results establish a smooth transition in the effectiveness of low-degree polynomial algorithms for this problem, highlighting the interplay between the dimensions $m$ and $d$, the noise level $\\sigma$, and the computational complexity of the testing task."
      },
      {
        "id": "oai:arXiv.org:2504.03129v1",
        "title": "GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction",
        "link": "https://arxiv.org/abs/2504.03129",
        "author": "Haozhan Tang, Tianyi Zhang, Oliver Kroemer, Matthew Johnson-Roberson, Weiming Zhi",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03129v1 Announce Type: cross \nAbstract: Robots operating in unstructured environments often require accurate and consistent object-level representations. This typically requires segmenting individual objects from the robot's surroundings. While recent large models such as Segment Anything (SAM) offer strong performance in 2D image segmentation. These advances do not translate directly to performance in the physical 3D world, where they often over-segment objects and fail to produce consistent mask correspondences across views. In this paper, we present GraphSeg, a framework for generating consistent 3D object segmentations from a sparse set of 2D images of the environment without any depth information. GraphSeg adds edges to graphs and constructs dual correspondence graphs: one from 2D pixel-level similarities and one from inferred 3D structure. We formulate segmentation as a problem of edge addition, then subsequent graph contraction, which merges multiple 2D masks into unified object-level segmentations. We can then leverage \\emph{3D foundation models} to produce segmented 3D representations. GraphSeg achieves robust segmentation with significantly fewer images and greater accuracy than prior methods. We demonstrate state-of-the-art performance on tabletop scenes and show that GraphSeg enables improved performance on downstream robotic manipulation tasks. Code available at https://github.com/tomtang502/graphseg.git."
      },
      {
        "id": "oai:arXiv.org:2504.03137v1",
        "title": "LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph",
        "link": "https://arxiv.org/abs/2504.03137",
        "author": "Tu Ao, Yanhua Yu, Yuling Wang, Yang Deng, Zirui Guo, Liang Pang, Pinghui Wang, Tat-Seng Chua, Xiao Zhang, Zhen Cai",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03137v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have impressive capabilities in text understanding and zero-shot reasoning. However, delays in knowledge updates may cause them to reason incorrectly or produce harmful results. Knowledge Graphs (KGs) provide rich and reliable contextual information for the reasoning process of LLMs by structurally organizing and connecting a wide range of entities and relations. Existing KG-based LLM reasoning methods only inject KGs' knowledge into prompts in a textual form, ignoring its structural information. Moreover, they mostly rely on close-source models or open-source models with large parameters, which poses challenges to high resource consumption. To address this, we propose a novel Lightweight and efficient Prompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the full potential of LLMs to tackle complex reasoning tasks in a parameter-efficient manner. Specifically, LightPROF follows a \"Retrieve-Embed-Reason process\", first accurately, and stably retrieving the corresponding reasoning graph from the KG through retrieval module. Next, through a Transformer-based Knowledge Adapter, it finely extracts and integrates factual and structural information from the KG, then maps this information to the LLM's token embedding space, creating an LLM-friendly prompt to be used by the LLM for the final reasoning. Additionally, LightPROF only requires training Knowledge Adapter and can be compatible with any open-source LLM. Extensive experiments on two public KGQA benchmarks demonstrate that LightPROF achieves superior performance with small-scale LLMs. Furthermore, LightPROF shows significant advantages in terms of input token count and reasoning time."
      },
      {
        "id": "oai:arXiv.org:2504.03146v1",
        "title": "Comparative Analysis of Unsupervised and Supervised Autoencoders for Nuclei Classification in Clear Cell Renal Cell Carcinoma Images",
        "link": "https://arxiv.org/abs/2504.03146",
        "author": "Fatemeh Javadian, Zahra Aminparast, Johannes Stegmaier, Abin Jose",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03146v1 Announce Type: cross \nAbstract: This study explores the application of supervised and unsupervised autoencoders (AEs) to automate nuclei classification in clear cell renal cell carcinoma (ccRCC) images, a diagnostic task traditionally reliant on subjective visual grading by pathologists. We evaluate various AE architectures, including standard AEs, contractive AEs (CAEs), and discriminative AEs (DAEs), as well as a classifier-based discriminative AE (CDAE), optimized using the hyperparameter tuning tool Optuna. Bhattacharyya distance is selected from several metrics to assess class separability in the latent space, revealing challenges in distinguishing adjacent grades using unsupervised models. CDAE, integrating a supervised classifier branch, demonstrated superior performance in both latent space separation and classification accuracy. Given that CDAE-CNN achieved notable improvements in classification metrics, affirming the value of supervised learning for class-specific feature extraction, F1 score was incorporated into the tuning process to optimize classification performance. Results show significant improvements in identifying aggressive ccRCC grades by leveraging the classification capability of AE through latent clustering followed by fine-grained classification. Our model outperforms the current state of the art, CHR-Network, across all evaluated metrics. These findings suggest that integrating a classifier branch in AEs, combined with neural architecture search and contrastive learning, enhances grading automation in ccRCC pathology, particularly in detecting aggressive tumor grades, and may improve diagnostic accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.03158v1",
        "title": "Accelerating Particle-based Energetic Variational Inference",
        "link": "https://arxiv.org/abs/2504.03158",
        "author": "Xuelian Bao, Lulu Kang, Chun Liu, Yiwei Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03158v1 Announce Type: cross \nAbstract: In this work, we propose a novel particle-based variational inference (ParVI) method that accelerates the EVI-Im. Inspired by energy quadratization (EQ) and operator splitting techniques for gradient flows, our approach efficiently drives particles towards the target distribution. Unlike EVI-Im, which employs the implicit Euler method to solve variational-preserving particle dynamics for minimizing the KL divergence, derived using a \"discretize-then-variational\" approach, the proposed algorithm avoids repeated evaluation of inter-particle interaction terms, significantly reducing computational cost. The framework is also extensible to other gradient-based sampling techniques. Through several numerical experiments, we demonstrate that our method outperforms existing ParVI approaches in efficiency, robustness, and accuracy."
      },
      {
        "id": "oai:arXiv.org:2504.03160v1",
        "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
        "link": "https://arxiv.org/abs/2504.03160",
        "author": "Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03160v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
      },
      {
        "id": "oai:arXiv.org:2504.03172v1",
        "title": "Bayesian Optimization of Robustness Measures Using Randomized GP-UCB-based Algorithms under Input Uncertainty",
        "link": "https://arxiv.org/abs/2504.03172",
        "author": "Yu Inatsu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03172v1 Announce Type: cross \nAbstract: Bayesian optimization based on Gaussian process upper confidence bound (GP-UCB) has a theoretical guarantee for optimizing black-box functions. Black-box functions often have input uncertainty, but even in this case, GP-UCB can be extended to optimize evaluation measures called robustness measures. However, GP-UCB-based methods for robustness measures include a trade-off parameter $\\beta$, which must be excessively large to achieve theoretical validity, just like the original GP-UCB. In this study, we propose a new method called randomized robustness measure GP-UCB (RRGP-UCB), which samples the trade-off parameter $\\beta$ from a probability distribution based on a chi-squared distribution and avoids explicitly specifying $\\beta$. The expected value of $\\beta$ is not excessively large. Furthermore, we show that RRGP-UCB provides tight bounds on the expected value of regret based on the optimal solution and estimated solutions. Finally, we demonstrate the usefulness of the proposed method through numerical experiments."
      },
      {
        "id": "oai:arXiv.org:2504.03175v1",
        "title": "Mathematical Modeling of Option Pricing with an Extended Black-Scholes Framework",
        "link": "https://arxiv.org/abs/2504.03175",
        "author": "Nikhil Shivakumar Nayak, Michael P. Brenner",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03175v1 Announce Type: cross \nAbstract: This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine learning-based LSTM model are developed and evaluated for pricing Google stock options. Both models were backtested using historical market data. While the LSTM model exhibited higher predictive accuracy, the finite difference method demonstrated superior computational efficiency. This work provides insights into model performance under varying market conditions and emphasizes the potential of hybrid approaches for robust financial modeling."
      },
      {
        "id": "oai:arXiv.org:2504.03190v1",
        "title": "The Ground Cost for Optimal Transport of Angular Velocity",
        "link": "https://arxiv.org/abs/2504.03190",
        "author": "Karthik Elamvazhuthi, Abhishek Halder",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03190v1 Announce Type: cross \nAbstract: We revisit the optimal transport problem over angular velocity dynamics given by the controlled Euler equation. The solution of this problem enables stochastic guidance of spin states of a rigid body (e.g., spacecraft) over hard deadline constraint by transferring a given initial state statistics to a desired terminal state statistics. This is an instance of generalized optimal transport over a nonlinear dynamical system. While prior work has reported existence-uniqueness and numerical solution of this dynamical optimal transport problem, here we present structural results about the equivalent Kantorovich a.k.a. optimal coupling formulation. Specifically, we focus on deriving the ground cost for the associated Kantorovich optimal coupling formulation. The ground cost equals to the cost of transporting unit amount of mass from a specific realization of the initial or source joint probability measure to a realization of the terminal or target joint probability measure, and determines the Kantorovich formulation. Finding the ground cost leads to solving a structured deterministic nonlinear optimal control problem, which is shown to be amenable to an analysis technique pioneered by Athans et. al. We show that such techniques have broader applicability in determining the ground cost (thus Kantorovich formulation) for a class of generalized optimal mass transport problems involving nonlinear dynamics with translated norm-invariant drift."
      },
      {
        "id": "oai:arXiv.org:2504.03238v1",
        "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs",
        "link": "https://arxiv.org/abs/2504.03238",
        "author": "Akis Nousias, Efklidis Katsaros, Evangelos Syrmos, Panagiotis Radoglou-Grammatikis, Thomas Lagkas, Vasileios Argyriou, Ioannis Moscholios, Evangelos Markakis, Sotirios Goudos, Panagiotis Sarigiannidis",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03238v1 Announce Type: cross \nAbstract: Malware detection is increasingly challenged by evolving techniques like obfuscation and polymorphism, limiting the effectiveness of traditional methods. Meanwhile, the widespread adoption of software containers has introduced new security challenges, including the growing threat of malicious software injection, where a container, once compromised, can serve as entry point for further cyberattacks. In this work, we address these security issues by introducing a method to identify compromised containers through machine learning analysis of their file systems. We cast the entire software containers into large RGB images via their tarball representations, and propose to use established Convolutional Neural Network architectures on a streaming, patch-based manner. To support our experiments, we release the COSOCO dataset--the first of its kind--containing 3364 large-scale RGB images of benign and compromised software containers at https://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method detects more malware and achieves higher F1 and Recall scores than all individual and ensembles of VirusTotal engines, demonstrating its effectiveness and setting a new standard for identifying malware-compromised software containers."
      },
      {
        "id": "oai:arXiv.org:2504.03255v1",
        "title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective",
        "link": "https://arxiv.org/abs/2504.03255",
        "author": "Garry A. Gabison, R. Patrick Xian",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03255v1 Announce Type: cross \nAbstract: Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention over effective governance policies, monitoring and control protocols. Based on emerging landscapes of the agentic market, we analyze the potential liability issues stemming from delegated use of LLM agents and their extended systems from a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing and monitoring approaches to enhancing transparency and accountability."
      },
      {
        "id": "oai:arXiv.org:2504.03272v1",
        "title": "Verification of Autonomous Neural Car Control with KeYmaera X",
        "link": "https://arxiv.org/abs/2504.03272",
        "author": "Enguerrand Prebet, Samuel Teuber, Andr\\'e Platzer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03272v1 Announce Type: cross \nAbstract: This article presents a formal model and formal safety proofs for the ABZ'25 case study in differential dynamic logic (dL). The case study considers an autonomous car driving on a highway avoiding collisions with neighbouring cars. Using KeYmaera X's dL implementation, we prove absence of collision on an infinite time horizon which ensures that safety is preserved independently of trip length. The safety guarantees hold for time-varying reaction time and brake force. Our dL model considers the single lane scenario with cars ahead or behind. We demonstrate that dL with its tools is a rigorous foundation for runtime monitoring, shielding, and neural network verification. Doing so sheds light on inconsistencies between the provided specification and simulation environment highway-env of the ABZ'25 study. We attempt to fix these inconsistencies and uncover numerous counterexamples which also indicate issues in the provided reinforcement learning environment."
      },
      {
        "id": "oai:arXiv.org:2504.03278v1",
        "title": "JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention",
        "link": "https://arxiv.org/abs/2504.03278",
        "author": "Guido Barducci, Ivan Rossi, Francesco Codic\\`e, Cesare Rollo, Valeria Repetto, Corrado Pancotti, Virginia Iannibelli, Tiziana Sanavia, Piero Fariselli",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03278v1 Announce Type: cross \nAbstract: Understanding how residue variations affect protein stability is crucial for designing functional proteins and deciphering the molecular mechanisms underlying disease-related mutations. Recent advances in protein language models (PLMs) have revolutionized computational protein analysis, enabling, among other things, more accurate predictions of mutational effects. In this work, we introduce JanusDDG, a deep learning framework that leverages PLM-derived embeddings and a bidirectional cross-attention transformer architecture to predict $\\Delta \\Delta G$ of single and multiple-residue mutations while simultaneously being constrained to respect fundamental thermodynamic properties, such as antisymmetry and transitivity. Unlike conventional self-attention, JanusDDG computes queries (Q) and values (V) as the difference between wild-type and mutant embeddings, while keys (K) alternate between the two. This cross-interleaved attention mechanism enables the model to capture mutation-induced perturbations while preserving essential contextual information. Experimental results show that JanusDDG achieves state-of-the-art performance in predicting $\\Delta \\Delta G$ from sequence alone, matching or exceeding the accuracy of structure-based methods for both single and multiple mutations."
      },
      {
        "id": "oai:arXiv.org:2504.03289v1",
        "title": "RWKVTTS: Yet another TTS based on RWKV-7",
        "link": "https://arxiv.org/abs/2504.03289",
        "author": "Lin yueyu, Liu Xiao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03289v1 Announce Type: cross \nAbstract: Human-AI interaction thrives on intuitive and efficient interfaces, among which voice stands out as a particularly natural and accessible modality. Recent advancements in transformer-based text-to-speech (TTS) systems, such as Fish-Speech, CosyVoice, and MegaTTS 3, have delivered remarkable improvements in quality and realism, driving a significant evolution in the TTS domain. In this paper, we introduce RWKV-7 \\cite{peng2025rwkv}, a cutting-edge RNN-based architecture tailored for TTS applications. Unlike traditional transformer models, RWKV-7 leverages the strengths of recurrent neural networks to achieve greater computational efficiency and scalability, while maintaining high-quality output. Our comprehensive benchmarks demonstrate that RWKV-7 outperforms transformer-based models across multiple key metrics, including synthesis speed, naturalness of speech, and resource efficiency. Furthermore, we explore its adaptability to diverse linguistic contexts and low-resource environments, showcasing its potential to democratize TTS technology. These findings position RWKV-7 as a powerful and innovative alternative, paving the way for more accessible and versatile voice synthesis solutions in real-world applications.Our code and weights are https://github.com/yynil/RWKVTTS, https://huggingface.co/spaces/RWKV-Red-Team"
      },
      {
        "id": "oai:arXiv.org:2504.03299v1",
        "title": "Universal Collection of Euclidean Invariants between Pairs of Position-Orientations",
        "link": "https://arxiv.org/abs/2504.03299",
        "author": "Gijs Bellaard, Bart M. N. Smets, Remco Duits",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03299v1 Announce Type: cross \nAbstract: Euclidean E(3) equivariant neural networks that employ scalar fields on position-orientation space M(3) have been effectively applied to tasks such as predicting molecular dynamics and properties. To perform equivariant convolutional-like operations in these architectures one needs Euclidean invariant kernels on M(3) x M(3). In practice, a handcrafted collection of invariants is selected, and this collection is then fed into multilayer perceptrons to parametrize the kernels. We rigorously describe an optimal collection of 4 smooth scalar invariants on the whole of M(3) x M(3). With optimal we mean that the collection is independent and universal, meaning that all invariants are pertinent, and any invariant kernel is a function of them. We evaluate two collections of invariants, one universal and one not, using the PONITA neural network architecture. Our experiments show that using a collection of invariants that is universal positively impacts the accuracy of PONITA significantly."
      },
      {
        "id": "oai:arXiv.org:2504.03309v1",
        "title": "Roto-Translation Invariant Metrics on Position-Orientation Space",
        "link": "https://arxiv.org/abs/2504.03309",
        "author": "Gijs Bellaard, Bart M. N. Smets",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03309v1 Announce Type: cross \nAbstract: Riemannian metrics on the position-orientation space M(3) that are roto-translation group SE(3) invariant play a key role in image analysis tasks like enhancement, denoising, and segmentation. These metrics enable roto-translation equivariant algorithms, with the associated Riemannian distance often used in implementation.\n  However, computing the Riemannian distance is costly, which makes it unsuitable in situations where constant recomputation is needed. We propose the mav (minimal angular velocity) distance, defined as the Riemannian length of a geometrically meaningful curve, as a practical alternative.\n  We see an application of the mav distance in geometric deep learning. Namely, neural networks architectures such as PONITA, relies on geometric invariants to create their roto-translation equivariant model. The mav distance offers a trainable invariant, with the parameters that determine the Riemannian metric acting as learnable weights.\n  In this paper we: 1) classify and parametrize all SE(3) invariant metrics on M(3), 2) describes how to efficiently calculate the mav distance, and 3) investigate if including the mav distance within PONITA can positively impact its accuracy in predicting molecular properties."
      },
      {
        "id": "oai:arXiv.org:2504.03315v1",
        "title": "Detecting underdetermination in parameterized quantum circuits",
        "link": "https://arxiv.org/abs/2504.03315",
        "author": "Marie Kempkes, Jakob Spiegelberg, Evert van Nieuwenburg, Vedran Dunjko",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03315v1 Announce Type: cross \nAbstract: A central question in machine learning is how reliable the predictions of a trained model are. Reliability includes the identification of instances for which a model is likely not to be trusted based on an analysis of the learning system itself. Such unreliability for an input may arise from the model family providing a variety of hypotheses consistent with the training data, which can vastly disagree in their predictions on that particular input point. This is called the underdetermination problem, and it is important to develop methods to detect it. With the emergence of quantum machine learning (QML) as a prospective alternative to classical methods for certain learning problems, the question arises to what extent they are subject to underdetermination and whether similar techniques as those developed for classical models can be employed for its detection. In this work, we first provide an overview of concepts from Safe AI and reliability, which in particular received little attention in QML. We then explore the use of a method based on local second-order information for the detection of underdetermination in parameterized quantum circuits through numerical experiments. We further demonstrate that the approach is robust to certain levels of shot noise. Our work contributes to the body of literature on Safe Quantum AI, which is an emerging field of growing importance."
      },
      {
        "id": "oai:arXiv.org:2504.03318v1",
        "title": "Adaptive Classification of Interval-Valued Time Series",
        "link": "https://arxiv.org/abs/2504.03318",
        "author": "Wan Tian, Zhongfeng Qin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03318v1 Announce Type: cross \nAbstract: In recent years, the modeling and analysis of interval-valued time series have garnered significant attention in the fields of econometrics and statistics. However, the existing literature primarily focuses on regression tasks while neglecting classification aspects. In this paper, we propose an adaptive approach for interval-valued time series classification. Specifically, we represent interval-valued time series using convex combinations of upper and lower bounds of intervals and transform these representations into images based on point-valued time series imaging methods. We utilize a fine-grained image classification neural network to classify these images, to achieve the goal of classifying the original interval-valued time series. This proposed method is applicable to both univariate and multivariate interval-valued time series. On the optimization front, we treat the convex combination coefficients as learnable parameters similar to the parameters of the neural network and provide an efficient estimation method based on the alternating direction method of multipliers (ADMM). On the theoretical front, under specific conditions, we establish a margin-based multiclass generalization bound for generic CNNs composed of basic blocks involving convolution, pooling, and fully connected layers. Through simulation studies and real data applications, we validate the effectiveness of the proposed method and compare its performance against a wide range of point-valued time series classification methods."
      },
      {
        "id": "oai:arXiv.org:2504.03322v1",
        "title": "Block Toeplitz Sparse Precision Matrix Estimation for Large-Scale Interval-Valued Time Series Forecasting",
        "link": "https://arxiv.org/abs/2504.03322",
        "author": "Wan Tian, Zhongfeng Qin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03322v1 Announce Type: cross \nAbstract: Modeling and forecasting interval-valued time series (ITS) have attracted considerable attention due to their growing presence in various contexts. To the best of our knowledge, there have been no efforts to model large-scale ITS. In this paper, we propose a feature extraction procedure for large-scale ITS, which involves key steps such as auto-segmentation and clustering, and feature transfer learning. This procedure can be seamlessly integrated with any suitable prediction models for forecasting purposes. Specifically, we transform the automatic segmentation and clustering of ITS into the estimation of Toeplitz sparse precision matrices and assignment set. The majorization-minimization algorithm is employed to convert this highly non-convex optimization problem into two subproblems. We derive efficient dynamic programming and alternating direction method to solve these two subproblems alternately and establish their convergence properties. By employing the Joint Recurrence Plot (JRP) to image subsequence and assigning a class label to each cluster, an image dataset is constructed. Then, an appropriate neural network is chosen to train on this image dataset and used to extract features for the next step of forecasting. Real data applications demonstrate that the proposed method can effectively obtain invariant representations of the raw data and enhance forecasting performance."
      },
      {
        "id": "oai:arXiv.org:2504.03328v1",
        "title": "Policy Optimization Algorithms in a Unified Framework",
        "link": "https://arxiv.org/abs/2504.03328",
        "author": "Shuang Wu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03328v1 Announce Type: cross \nAbstract: Policy optimization algorithms are crucial in many fields but challenging to grasp and implement, often due to complex calculations related to Markov decision processes and varying use of discount and average reward setups. This paper presents a unified framework that applies generalized ergodicity theory and perturbation analysis to clarify and enhance the application of these algorithms. Generalized ergodicity theory sheds light on the steady-state behavior of stochastic processes, aiding understanding of both discounted and average rewards. Perturbation analysis provides in-depth insights into the fundamental principles of policy optimization algorithms. We use this framework to identify common implementation errors and demonstrate the correct approaches. Through a case study on Linear Quadratic Regulator problems, we illustrate how slight variations in algorithm design affect implementation outcomes. We aim to make policy optimization algorithms more accessible and reduce their misuse in practice."
      },
      {
        "id": "oai:arXiv.org:2504.03350v1",
        "title": "Bayesian LSTM for indoor temperature modeling",
        "link": "https://arxiv.org/abs/2504.03350",
        "author": "Emma Hannula, Arttu H\\\"akkinen, Antti Solonen, Felibe Uribe, Jana de Wiljes, Lassi Roininen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03350v1 Announce Type: cross \nAbstract: Improving energy efficiency of building heating systems is essential for reducing global energy consumption and greenhouse gas emissions. Traditional control methods in buildings rely on static heating curves based solely on outdoor temperature measurements, neglecting system state and free heat sources like solar gain. Model predictive control (MPC) not only addresses these limitations but further optimizes heating control by incorporating weather forecasts and system state predictions. However, current industrial MPC solutions often use simplified physics-inspired models, which compromise accuracy for interpretability. While purely data-driven models offer better predictive performance, they face challenges like overfitting and lack of transparency.\n  To bridge this gap, we propose a Bayesian Long Short-Term Memory (LSTM) architecture for indoor temperature modeling. Our experiments across 100 real-world buildings demonstrate that the Bayesian LSTM outperforms an industrial physics-based model in predictive accuracy, enabling potential for improved energy efficiency and thermal comfort if deployed in heating MPC solutions. Over deterministic black-box approaches, the Bayesian framework provides additional advantages by improving generalization ability and allowing interpretation of predictions via uncertainty quantification. This work advances data-driven heating control by balancing predictive performance with the transparency and reliability required for real-world heating MPC applications."
      },
      {
        "id": "oai:arXiv.org:2504.03360v1",
        "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency",
        "link": "https://arxiv.org/abs/2504.03360",
        "author": "Erik Johannes Husom, Arda Goknil, Merve Astekin, Lwin Khin Shar, Andre K{\\aa}sen, Sagar Sen, Benedikt Andreas Mithassel, Ahmet Soylu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03360v1 Announce Type: cross \nAbstract: Deploying Large Language Models (LLMs) on edge devices presents significant challenges due to computational constraints, memory limitations, inference speed, and energy consumption. Model quantization has emerged as a key technique to enable efficient LLM inference by reducing model size and computational overhead. In this study, we conduct a comprehensive analysis of 28 quantized LLMs from the Ollama library, which applies by default Post-Training Quantization (PTQ) and weight-only quantization techniques, deployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy efficiency, inference performance, and output accuracy across multiple quantization levels and task types. Models are benchmarked on five standardized datasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and we employ a high-resolution, hardware-based energy measurement tool to capture real-world power consumption. Our findings reveal the trade-offs between energy efficiency, inference speed, and accuracy in different quantization settings, highlighting configurations that optimize LLM deployment for resource-constrained environments. By integrating hardware-level energy profiling with LLM benchmarking, this study provides actionable insights for sustainable AI, bridging a critical gap in existing research on energy-aware LLM deployment."
      },
      {
        "id": "oai:arXiv.org:2504.03369v1",
        "title": "Point Cloud-based Grasping for Soft Hand Exoskeleton",
        "link": "https://arxiv.org/abs/2504.03369",
        "author": "Chen Hu, Enrica Tricomi, Eojin Rho, Daekyum Kim, Lorenzo Masia, Shan Luo, Letizia Gionfrida",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03369v1 Announce Type: cross \nAbstract: Grasping is a fundamental skill for interacting with and manipulating objects in the environment. However, this ability can be challenging for individuals with hand impairments. Soft hand exoskeletons designed to assist grasping can enhance or restore essential hand functions, yet controlling these soft exoskeletons to support users effectively remains difficult due to the complexity of understanding the environment. This study presents a vision-based predictive control framework that leverages contextual awareness from depth perception to predict the grasping target and determine the next control state for activation. Unlike data-driven approaches that require extensive labelled datasets and struggle with generalizability, our method is grounded in geometric modelling, enabling robust adaptation across diverse grasping scenarios. The Grasping Ability Score (GAS) was used to evaluate performance, with our system achieving a state-of-the-art GAS of 91% across 15 objects and healthy participants, demonstrating its effectiveness across different object types. The proposed approach maintained reconstruction success for unseen objects, underscoring its enhanced generalizability compared to learning-based models."
      },
      {
        "id": "oai:arXiv.org:2504.03415v1",
        "title": "NeRFlex: Resource-aware Real-time High-quality Rendering of Complex Scenes on Mobile Devices",
        "link": "https://arxiv.org/abs/2504.03415",
        "author": "Zhe Wang, Yifei Zhu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03415v1 Announce Type: cross \nAbstract: Neural Radiance Fields (NeRF) is a cutting-edge neural network-based technique for novel view synthesis in 3D reconstruction. However, its significant computational demands pose challenges for deployment on mobile devices. While mesh-based NeRF solutions have shown potential in achieving real-time rendering on mobile platforms, they often fail to deliver high-quality reconstructions when rendering practical complex scenes. Additionally, the non-negligible memory overhead caused by pre-computed intermediate results complicates their practical application. To overcome these challenges, we present NeRFlex, a resource-aware, high-resolution, real-time rendering framework for complex scenes on mobile devices. NeRFlex integrates mobile NeRF rendering with multi-NeRF representations that decompose a scene into multiple sub-scenes, each represented by an individual NeRF network. Crucially, NeRFlex considers both memory and computation constraints as first-class citizens and redesigns the reconstruction process accordingly. NeRFlex first designs a detail-oriented segmentation module to identify sub-scenes with high-frequency details. For each NeRF network, a lightweight profiler, built on domain knowledge, is used to accurately map configurations to visual quality and memory usage. Based on these insights and the resource constraints on mobile devices, NeRFlex presents a dynamic programming algorithm to efficiently determine configurations for all NeRF representations, despite the NP-hardness of the original decision problem. Extensive experiments on real-world datasets and mobile devices demonstrate that NeRFlex achieves real-time, high-quality rendering on commercial mobile devices."
      },
      {
        "id": "oai:arXiv.org:2504.03432v1",
        "title": "A Polynomial-Time Algorithm for Variational Inequalities under the Minty Condition",
        "link": "https://arxiv.org/abs/2504.03432",
        "author": "Ioannis Anagnostides, Gabriele Farina, Tuomas Sandholm, Brian Hu Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03432v1 Announce Type: cross \nAbstract: Solving (Stampacchia) variational inequalities (SVIs) is a foundational problem at the heart of optimization, with a host of critical applications ranging from engineering to economics. However, this expressivity comes at the cost of computational hardness. As a result, most research has focused on carving out specific subclasses that elude those intractability barriers. A classical property that goes back to the 1960s is the Minty condition, which postulates that the Minty VI (MVI) problem -- the weak dual of the SVI problem -- admits a solution.\n  In this paper, we establish the first polynomial-time algorithm -- that is, with complexity growing polynomially in the dimension $d$ and $\\log(1/\\epsilon)$ -- for solving $\\epsilon$-SVIs for Lipschitz continuous mappings under the Minty condition. Prior approaches either incurred an exponentially worse dependence on $1/\\epsilon$ (and other natural parameters of the problem) or made overly restrictive assumptions -- such as strong monotonicity. To do so, we introduce a new variant of the ellipsoid algorithm wherein separating hyperplanes are obtained after taking a gradient descent step from the center of the ellipsoid. It succeeds even though the set of SVIs can be nonconvex and not fully dimensional. Moreover, when our algorithm is applied to an instance with no MVI solution and fails to identify an SVI solution, it produces a succinct certificate of MVI infeasibility. We also show that deciding whether the Minty condition holds is $\\mathsf{coNP}$-complete.\n  We provide several extensions and new applications of our main results. Specifically, we obtain the first polynomial-time algorithms for i) solving monotone VIs, ii) globally minimizing a (potentially nonsmooth) quasar-convex function, and iii) computing Nash equilibria in multi-player harmonic games."
      },
      {
        "id": "oai:arXiv.org:2504.03439v1",
        "title": "Early detection of diabetes through transfer learning-based eye (vision) screening and improvement of machine learning model performance and advanced parameter setting algorithms",
        "link": "https://arxiv.org/abs/2504.03439",
        "author": "Mohammad Reza Yousefi, Ali Bakrani, Amin Dehghani",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03439v1 Announce Type: cross \nAbstract: Diabetic Retinopathy (DR) is a serious and common complication of diabetes, caused by prolonged high blood sugar levels that damage the small retinal blood vessels. If left untreated, DR can progress to retinal vein occlusion and stimulate abnormal blood vessel growth, significantly increasing the risk of blindness. Traditional diabetes diagnosis methods often utilize convolutional neural networks (CNNs) to extract visual features from retinal images, followed by classification algorithms such as decision trees and k-nearest neighbors (KNN) for disease detection. However, these approaches face several challenges, including low accuracy and sensitivity, lengthy machine learning (ML) model training due to high data complexity and volume, and the use of limited datasets for testing and evaluation. This study investigates the application of transfer learning (TL) to enhance ML model performance in DR detection. Key improvements include dimensionality reduction, optimized learning rate adjustments, and advanced parameter tuning algorithms, aimed at increasing efficiency and diagnostic accuracy. The proposed model achieved an overall accuracy of 84% on the testing dataset, outperforming prior studies. The highest class-specific accuracy reached 89%, with a maximum sensitivity of 97% and an F1-score of 92%, demonstrating strong performance in identifying DR cases. These findings suggest that TL-based DR screening is a promising approach for early diagnosis, enabling timely interventions to prevent vision loss and improve patient outcomes."
      },
      {
        "id": "oai:arXiv.org:2504.03461v1",
        "title": "Conditioning Diffusions Using Malliavin Calculus",
        "link": "https://arxiv.org/abs/2504.03461",
        "author": "Jakiw Pidstrigach, Elizabeth Baker, Carles Domingo-Enrich, George Deligiannidis, Nikolas N\\\"usken",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03461v1 Announce Type: cross \nAbstract: In stochastic optimal control and conditional generative modelling, a central computational task is to modify a reference diffusion process to maximise a given terminal-time reward. Most existing methods require this reward to be differentiable, using gradients to steer the diffusion towards favourable outcomes. However, in many practical settings, like diffusion bridges, the reward is singular, taking an infinite value if the target is hit and zero otherwise. We introduce a novel framework, based on Malliavin calculus and path-space integration by parts, that enables the development of methods robust to such singular rewards. This allows our approach to handle a broad range of applications, including classification, diffusion bridges, and conditioning without the need for artificial observational noise. We demonstrate that our approach offers stable and reliable training, outperforming existing techniques."
      },
      {
        "id": "oai:arXiv.org:2504.03463v1",
        "title": "Generating ensembles of spatially-coherent in-situ forecasts using flow matching",
        "link": "https://arxiv.org/abs/2504.03463",
        "author": "David Landry, Claire Monteleoni, Anastase Charantonis",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03463v1 Announce Type: cross \nAbstract: We propose a machine-learning-based methodology for in-situ weather forecast postprocessing that is both spatially coherent and multivariate. Compared to previous work, our Flow MAtching Postprocessing (FMAP) better represents the correlation structures of the observations distribution, while also improving marginal performance at the stations. FMAP generates forecasts that are not bound to what is already modeled by the underlying gridded prediction and can infer new correlation structures from data. The resulting model can generate an arbitrary number of forecasts from a limited number of numerical simulations, allowing for low-cost forecasting systems. A single training is sufficient to perform postprocessing at multiple lead times, in contrast with other methods which use multiple trained networks at generation time. This work details our methodology, including a spatial attention transformer backbone trained within a flow matching generative modeling framework. FMAP shows promising performance in experiments on the EUPPBench dataset, forecasting surface temperature and wind gust values at station locations in western Europe up to five-day lead times."
      },
      {
        "id": "oai:arXiv.org:2504.03499v1",
        "title": "Optimistic Learning for Communication Networks",
        "link": "https://arxiv.org/abs/2504.03499",
        "author": "George Iosifidis, Naram Mhaisen, Douglas J. Leith",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03499v1 Announce Type: cross \nAbstract: AI/ML-based tools are at the forefront of resource management solutions for communication networks. Deep learning, in particular, is highly effective in facilitating fast and high-performing decision-making whenever representative training data is available to build offline accurate models. Conversely, online learning solutions do not require training and enable adaptive decisions based on runtime observations, alas are often overly conservative. This extensive tutorial proposes the use of optimistic learning (OpL) as a decision engine for resource management frameworks in modern communication systems. When properly designed, such solutions can achieve fast and high-performing decisions -- comparable to offline-trained models -- while preserving the robustness and performance guarantees of the respective online learning approaches. We introduce the fundamental concepts, algorithms and results of OpL, discuss the roots of this theory and present different approaches to defining and achieving optimism. We proceed to showcase how OpL can enhance resource management in communication networks for several key problems such as caching, edge computing, network slicing, and workload assignment in decentralized O-RAN platforms. Finally, we discuss the open challenges that must be addressed to unlock the full potential of this new resource management approach."
      },
      {
        "id": "oai:arXiv.org:2504.03500v1",
        "title": "Learning Dual-Arm Coordination for Grasping Large Flat Objects",
        "link": "https://arxiv.org/abs/2504.03500",
        "author": "Yongliang Wang, Hamidreza Kasaei",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03500v1 Announce Type: cross \nAbstract: Grasping large flat objects, such as books or keyboards lying horizontally, presents significant challenges for single-arm robotic systems, often requiring extra actions like pushing objects against walls or moving them to the edge of a surface to facilitate grasping. In contrast, dual-arm manipulation, inspired by human dexterity, offers a more refined solution by directly coordinating both arms to lift and grasp the object without the need for complex repositioning. In this paper, we propose a model-free deep reinforcement learning (DRL) framework to enable dual-arm coordination for grasping large flat objects. We utilize a large-scale grasp pose detection model as a backbone to extract high-dimensional features from input images, which are then used as the state representation in a reinforcement learning (RL) model. A CNN-based Proximal Policy Optimization (PPO) algorithm with shared Actor-Critic layers is employed to learn coordinated dual-arm grasp actions. The system is trained and tested in Isaac Gym and deployed to real robots. Experimental results demonstrate that our policy can effectively grasp large flat objects without requiring additional maneuvers. Furthermore, the policy exhibits strong generalization capabilities, successfully handling unseen objects. Importantly, it can be directly transferred to real robots without fine-tuning, consistently outperforming baseline methods."
      },
      {
        "id": "oai:arXiv.org:2504.03503v1",
        "title": "Operator Learning: A Statistical Perspective",
        "link": "https://arxiv.org/abs/2504.03503",
        "author": "Unique Subedi, Ambuj Tewari",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03503v1 Announce Type: cross \nAbstract: Operator learning has emerged as a powerful tool in scientific computing for approximating mappings between infinite-dimensional function spaces. A primary application of operator learning is the development of surrogate models for the solution operators of partial differential equations (PDEs). These methods can also be used to develop black-box simulators to model system behavior from experimental data, even without a known mathematical model. In this article, we begin by formalizing operator learning as a function-to-function regression problem and review some recent developments in the field. We also discuss PDE-specific operator learning, outlining strategies for incorporating physical and mathematical constraints into architecture design and training processes. Finally, we end by highlighting key future directions such as active data collection and the development of rigorous uncertainty quantification frameworks."
      },
      {
        "id": "oai:arXiv.org:2504.03515v1",
        "title": "Dexterous Manipulation through Imitation Learning: A Survey",
        "link": "https://arxiv.org/abs/2504.03515",
        "author": "Shan An, Ziyu Meng, Chao Tang, Yuning Zhou, Tengyu Liu, Fangqiang Ding, Shufang Zhang, Yao Mu, Ran Song, Wei Zhang, Zeng-Guang Hou, Hong Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03515v1 Announce Type: cross \nAbstract: Dexterous manipulation, which refers to the ability of a robotic hand or multi-fingered end-effector to skillfully control, reorient, and manipulate objects through precise, coordinated finger movements and adaptive force modulation, enables complex interactions similar to human hand dexterity. With recent advances in robotics and machine learning, there is a growing demand for these systems to operate in complex and unstructured environments. Traditional model-based approaches struggle to generalize across tasks and object variations due to the high-dimensionality and complex contact dynamics of dexterous manipulation. Although model-free methods such as reinforcement learning (RL) show promise, they require extensive training, large-scale interaction data, and carefully designed rewards for stability and effectiveness. Imitation learning (IL) offers an alternative by allowing robots to acquire dexterous manipulation skills directly from expert demonstrations, capturing fine-grained coordination and contact dynamics while bypassing the need for explicit modeling and large-scale trial-and-error. This survey provides an overview of dexterous manipulation methods based on imitation learning (IL), details recent advances, and addresses key challenges in the field. Additionally, it explores potential research directions to enhance IL-driven dexterous manipulation. Our goal is to offer researchers and practitioners a comprehensive introduction to this rapidly evolving domain."
      },
      {
        "id": "oai:arXiv.org:2504.03560v1",
        "title": "Stochastic Optimization with Optimal Importance Sampling",
        "link": "https://arxiv.org/abs/2504.03560",
        "author": "Liviu Aolaritei, Bart P. G. Van Parys, Henry Lam, Michael I. Jordan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03560v1 Announce Type: cross \nAbstract: Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its power, the performance of IS is often highly sensitive to the choice of the proposal distribution and frequently requires stochastic calibration techniques. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a unique challenge: the decision and the IS distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both the analysis of convergence for decision iterates and the efficiency of the IS scheme. In this paper, we propose an iterative gradient-based algorithm that jointly updates the decision variable and the IS distribution without requiring time-scale separation between the two. Our method achieves the lowest possible asymptotic variance and guarantees global convergence under convexity of the objective and mild assumptions on the IS distribution family. Furthermore, we show that these properties are preserved under linear constraints by incorporating a recent variant of Nesterov's dual averaging method."
      },
      {
        "id": "oai:arXiv.org:2504.03589v1",
        "title": "AdaViT: Adaptive Vision Transformer for Flexible Pretrain and Finetune with Variable 3D Medical Image Modalities",
        "link": "https://arxiv.org/abs/2504.03589",
        "author": "Badhan Kumar Das, Gengyan Zhao, Han Liu, Thomas J. Re, Dorin Comaniciu, Eli Gibson, Andreas Maier",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03589v1 Announce Type: cross \nAbstract: Pretrain techniques, whether supervised or self-supervised, are widely used in deep learning to enhance model performance. In real-world clinical scenarios, different sets of magnetic resonance (MR) contrasts are often acquired for different subjects/cases, creating challenges for deep learning models assuming consistent input modalities among all the cases and between pretrain and finetune. Existing methods struggle to maintain performance when there is an input modality/contrast set mismatch with the pretrained model, often resulting in degraded accuracy. We propose an adaptive Vision Transformer (AdaViT) framework capable of handling variable set of input modalities for each case. We utilize a dynamic tokenizer to encode different input image modalities to tokens and take advantage of the characteristics of the transformer to build attention mechanism across variable length of tokens. Through extensive experiments, we demonstrate that this architecture effectively transfers supervised pretrained models to new datasets with different input modality/contrast sets, resulting in superior performance on zero-shot testing, few-shot finetuning, and backward transferring in brain infarct and brain tumor segmentation tasks. Additionally, for self-supervised pretrain, the proposed method is able to maximize the pretrain data and facilitate transferring to diverse downstream tasks with variable sets of input modalities."
      },
      {
        "id": "oai:arXiv.org:2504.03592v1",
        "title": "Optimistic Online Learning in Symmetric Cone Games",
        "link": "https://arxiv.org/abs/2504.03592",
        "author": "Anas Barakat, Wayne Lin, John Lazarsfeld, Antonios Varvitsiotis",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03592v1 Announce Type: cross \nAbstract: Optimistic online learning algorithms have led to significant advances in equilibrium computation, particularly for two-player zero-sum games, achieving an iteration complexity of $\\mathcal{O}(1/\\epsilon)$ to reach an $\\epsilon$-saddle point. These advances have been established in normal-form games, where strategies are simplex vectors, and quantum games, where strategies are trace-one positive semidefinite matrices. We extend optimistic learning to symmetric cone games (SCGs), a class of two-player zero-sum games where strategy spaces are generalized simplices (trace-one slices of symmetric cones). A symmetric cone is the cone of squares of a Euclidean Jordan Algebra; canonical examples include the nonnegative orthant, the second-order cone, the cone of positive semidefinite matrices, and their products, all fundamental to convex optimization. SCGs unify normal-form and quantum games and, as we show, offer significantly greater modeling flexibility, allowing us to model applications such as distance metric learning problems and the Fermat-Weber problem. To compute approximate saddle points in SCGs, we introduce the Optimistic Symmetric Cone Multiplicative Weights Update algorithm and establish an iteration complexity of $\\mathcal{O}(1/\\epsilon)$ to reach an $\\epsilon$-saddle point. Our analysis builds on the Optimistic Follow-the-Regularized-Leader framework, with a key technical contribution being a new proof of the strong convexity of the symmetric cone negative entropy with respect to the trace-one norm, a result that may be of independent interest."
      },
      {
        "id": "oai:arXiv.org:2504.03600v1",
        "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
        "link": "https://arxiv.org/abs/2504.03600",
        "author": "Jun Ma, Zongxin Yang, Sumin Kim, Bihui Chen, Mohammed Baharoon, Adibvafa Fallahpour, Reza Asakereh, Hongwei Lyu, Bo Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03600v1 Announce Type: cross \nAbstract: Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments."
      },
      {
        "id": "oai:arXiv.org:2504.03603v1",
        "title": "Towards deployment-centric multimodal AI beyond vision and language",
        "link": "https://arxiv.org/abs/2504.03603",
        "author": "Xianyuan Liu, Jiayang Zhang, Shuo Zhou, Thijs L. van der Plas, Avish Vijayaraghavan, Anastasiia Grishina, Mengdie Zhuang, Daniel Schofield, Christopher Tomlinson, Yuhan Wang, Ruizhe Li, Louisa van Zeeland, Sina Tabakhi, Cyndie Demeocq, Xiang Li, Arunav Das, Orlando Timmerman, Thomas Baldwin-McDonald, Jinge Wu, Peizhen Bai, Zahraa Al Sahili, Omnia Alwazzan, Thao N. Do, Mohammod N. I. Suvon, Angeline Wang, Lucia Cipolina-Kun, Luigi A. Moretti, Lucas Farndale, Nitisha Jain, Natalia Efremova, Yan Ge, Marta Varela, Hak-Keung Lam, Oya Celiktutan, Ben R. Evans, Alejandro Coca-Castro, Honghan Wu, Zahraa S. Abdallah, Chen Chen, Valentin Danchev, Nataliya Tkachenko, Lei Lu, Tingting Zhu, Gregory G. Slabaugh, Roger K. Moore, William K. Cheung, Peter H. Charlton, Haiping Lu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03603v1 Announce Type: cross \nAbstract: Multimodal artificial intelligence (AI) integrates diverse types of data via machine learning to improve understanding, prediction, and decision-making across disciplines such as healthcare, science, and engineering. However, most multimodal AI advances focus on models for vision and language data, while their deployability remains a key challenge. We advocate a deployment-centric workflow that incorporates deployment constraints early to reduce the likelihood of undeployable solutions, complementing data-centric and model-centric approaches. We also emphasise deeper integration across multiple levels of multimodality and multidisciplinary collaboration to significantly broaden the research scope beyond vision and language. To facilitate this approach, we identify common multimodal-AI-specific challenges shared across disciplines and examine three real-world use cases: pandemic response, self-driving car design, and climate change adaptation, drawing expertise from healthcare, social science, engineering, science, sustainability, and finance. By fostering multidisciplinary dialogue and open research practices, our community can accelerate deployment-centric development for broad societal impact."
      },
      {
        "id": "oai:arXiv.org:2504.03618v1",
        "title": "Trading off Relevance and Revenue in the Jobs Marketplace: Estimation, Optimization and Auction Design",
        "link": "https://arxiv.org/abs/2504.03618",
        "author": "Farzad Pourbabaee, Sophie Yanying Sheng, Peter McCrory, Luke Simon, Di Mo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03618v1 Announce Type: cross \nAbstract: We study the problem of position allocation in job marketplaces, where the platform determines the ranking of the jobs for each seeker. The design of ranking mechanisms is critical to marketplace efficiency, as it influences both short-term revenue from promoted job placements and long-term health through sustained seeker engagement. Our analysis focuses on the tradeoff between revenue and relevance, as well as the innovations in job auction design. We demonstrated two ways to improve relevance with minimal impact on revenue: incorporating the seekers preferences and applying position-aware auctions."
      },
      {
        "id": "oai:arXiv.org:2504.03626v1",
        "title": "Quantum Speedups for Markov Chain Monte Carlo Methods with Application to Optimization",
        "link": "https://arxiv.org/abs/2504.03626",
        "author": "Guneykan Ozgul, Xiantao Li, Mehrdad Mahdavi, Chunhao Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03626v1 Announce Type: cross \nAbstract: We propose quantum algorithms that provide provable speedups for Markov Chain Monte Carlo (MCMC) methods commonly used for sampling from probability distributions of the form $\\pi \\propto e^{-f}$, where $f$ is a potential function. Our first approach considers Gibbs sampling for finite-sum potentials in the stochastic setting, employing an oracle that provides gradients of individual functions. In the second setting, we consider access only to a stochastic evaluation oracle, allowing simultaneous queries at two points of the potential function under the same stochastic parameter. By introducing novel techniques for stochastic gradient estimation, our algorithms improve the gradient and evaluation complexities of classical samplers, such as Hamiltonian Monte Carlo (HMC) and Langevin Monte Carlo (LMC) in terms of dimension, precision, and other problem-dependent parameters. Furthermore, we achieve quantum speedups in optimization, particularly for minimizing non-smooth and approximately convex functions that commonly appear in empirical risk minimization problems."
      },
      {
        "id": "oai:arXiv.org:2504.03635v1",
        "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning",
        "link": "https://arxiv.org/abs/2504.03635",
        "author": "Xinyi Wang, Shawn Tan, Mingyu Jin, William Yang Wang, Rameswar Panda, Yikang Shen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03635v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. In this paper, we introduce a synthetic multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. Our reasoning task involves completing missing edges in the graph, which requires advanced multi-hop reasoning and mimics real-world reasoning scenarios. To evaluate this, we pretrain language models (LMs) from scratch solely on triples from the incomplete graph and assess their ability to infer the missing edges. Interestingly, we observe that overparameterization can impair reasoning performance due to excessive memorization. We investigate different factors that affect this U-shaped loss curve, including graph structure, model size, and training steps. To predict the optimal model size for a specific knowledge graph, we find an empirical scaling that linearly maps the knowledge graph search entropy to the optimal model size. This work provides new insights into the relationship between scaling and reasoning in LLMs, shedding light on possible ways to optimize their performance for reasoning tasks."
      },
      {
        "id": "oai:arXiv.org:2202.03613v5",
        "title": "Conformal Prediction Under Feedback Covariate Shift for Biomolecular Design",
        "link": "https://arxiv.org/abs/2202.03613",
        "author": "Clara Fannjiang, Stephen Bates, Anastasios N. Angelopoulos, Jennifer Listgarten, Michael I. Jordan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2202.03613v5 Announce Type: replace \nAbstract: Many applications of machine learning methods involve an iterative protocol in which data are collected, a model is trained, and then outputs of that model are used to choose what data to consider next. For example, one data-driven approach for designing proteins is to train a regression model to predict the fitness of protein sequences, then use it to propose new sequences believed to exhibit greater fitness than observed in the training data. Since validating designed sequences in the wet lab is typically costly, it is important to quantify the uncertainty in the model's predictions. This is challenging because of a characteristic type of distribution shift between the training and test data in the design setting -- one in which the training and test data are statistically dependent, as the latter is chosen based on the former. Consequently, the model's error on the test data -- that is, the designed sequences -- has an unknown and possibly complex relationship with its error on the training data. We introduce a method to quantify predictive uncertainty in such settings. We do so by constructing confidence sets for predictions that account for the dependence between the training and test data. The confidence sets we construct have finite-sample guarantees that hold for any prediction algorithm, even when a trained model chooses the test-time input distribution. As a motivating use case, we demonstrate with several real data sets how our method quantifies uncertainty for the predicted fitness of designed proteins, and can therefore be used to select design algorithms that achieve acceptable trade-offs between high predicted fitness and low predictive uncertainty."
      },
      {
        "id": "oai:arXiv.org:2208.10598v2",
        "title": "Generalizing Hate Speech Detection Using Multi-Task Learning: A Case Study of Political Public Figures",
        "link": "https://arxiv.org/abs/2208.10598",
        "author": "Lanqin Yuan, Marian-Andrei Rizoiu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2208.10598v2 Announce Type: replace \nAbstract: Automatic identification of hateful and abusive content is vital in combating the spread of harmful online content and its damaging effects. Most existing works evaluate models by examining the generalization error on train-test splits on hate speech datasets. These datasets often differ in their definitions and labeling criteria, leading to poor generalization performance when predicting across new domains and datasets. This work proposes a new Multi-task Learning (MTL) pipeline that trains simultaneously across multiple hate speech datasets to construct a more encompassing classification model. Using a dataset-level leave-one-out evaluation (designating a dataset for testing and jointly training on all others), we trial the MTL detection on new, previously unseen datasets. Our results consistently outperform a large sample of existing work. We show strong results when examining the generalization error in train-test splits and substantial improvements when predicting on previously unseen datasets. Furthermore, we assemble a novel dataset, dubbed PubFigs, focusing on the problematic speech of American Public Political Figures. We crowdsource-label using Amazon MTurk more than $20,000$ tweets and machine-label problematic speech in all the $305,235$ tweets in PubFigs. We find that the abusive and hate tweeting mainly originates from right-leaning figures and relates to six topics, including Islam, women, ethnicity, and immigrants. We show that MTL builds embeddings that can simultaneously separate abusive from hate speech, and identify its topics."
      },
      {
        "id": "oai:arXiv.org:2302.10963v3",
        "title": "Can Learning Be Explained By Local Optimality In Robust Low-rank Matrix Recovery?",
        "link": "https://arxiv.org/abs/2302.10963",
        "author": "Jianhao Ma, Salar Fattahi",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2302.10963v3 Announce Type: replace \nAbstract: We explore the local landscape of low-rank matrix recovery, focusing on reconstructing a $d_1\\times d_2$ matrix $X^\\star$ with rank $r$ from $m$ linear measurements, some potentially noisy. When the noise is distributed according to an outlier model, minimizing a nonsmooth $\\ell_1$-loss with a simple sub-gradient method can often perfectly recover the ground truth matrix $X^\\star$. Given this, a natural question is what optimization property (if any) enables such learning behavior. The most plausible answer is that the ground truth $X^\\star$ manifests as a local optimum of the loss function. In this paper, we provide a strong negative answer to this question, showing that, under moderate assumptions, the true solutions corresponding to $X^\\star$ do not emerge as local optima, but rather as strict saddle points -- critical points with strictly negative curvature in at least one direction. Our findings challenge the conventional belief that all strict saddle points are undesirable and should be avoided."
      },
      {
        "id": "oai:arXiv.org:2304.00571v3",
        "title": "DropMAE: Learning Representations via Masked Autoencoders with Spatial-Attention Dropout for Temporal Matching Tasks",
        "link": "https://arxiv.org/abs/2304.00571",
        "author": "Qiangqiang Wu, Tianyu Yang, Ziquan Liu, Wei Lin, Baoyuan Wu, Antoni B. Chan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2304.00571v3 Announce Type: replace \nAbstract: This paper studies masked autoencoder (MAE) video pre-training for various temporal matching-based downstream tasks, i.e., object-level tracking tasks including video object tracking (VOT) and video object segmentation (VOS), self-supervised visual correspondence learning, dense tracking tasks including optical flow estimation and long-term point tracking, and 3D point cloud tracking. Specifically, our work explores to provide a general representation to boost the temporal matching ability in various downstream tracking tasks. To achieve this, we firstly find that a simple extension of MAE, which randomly masks out frame patches in videos and reconstruct the frame pixels, heavily relies on spatial cues while ignoring temporal relations for frame reconstruction, thus leading to sub-optimal temporal matching representations. To alleviate this, we propose DropMAE, which adaptively performs spatial-attention dropout in the frame reconstruction to facilitate temporal correspondence learning in videos. We obtain several important findings with DropMAE: 1) DropMAE is a strong and efficient temporal matching learner, which achieves better fine-tuning results on matching-based tasks than the ImageNet-based MAE with 2x faster pre-training speed. 2) DropMAE is effective for different tracking tasks, i.e., object-level matching tasks including VOT and VOS, dense tracking tasks including optical flow estimation and tracking any point (TAP), and even 3D tracking in the different modality of point cloud data. Since none exists, we build ViT-based trackers for different downstream tracking tasks, and our pre-trained DropMAE model can be directly loaded in these ViT-based trackers for fine-tuning without further modifications. Experiments on 6 downstream tracking tasks demonstrate the effectiveness of DropMAE as a general pre-trained representation for diverse tracking tasks."
      },
      {
        "id": "oai:arXiv.org:2305.09907v2",
        "title": "Incremental Outlier Detection Modelling Using Streaming Analytics in Finance & Health Care",
        "link": "https://arxiv.org/abs/2305.09907",
        "author": "Vivek Yelleti, Ch Priyanka",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.09907v2 Announce Type: replace \nAbstract: In the era of real-time data, traditional methods often struggle to keep pace with the dynamic nature of streaming environments. In this paper, we proposed a hybrid framework where in (i) stage-I follows a traditional approach where the model is built once and evaluated in a real-time environment, and (ii) stage-II employs an incremental learning approach where the model is continuously retrained as new data arrives, enabling it to adapt and stay up to date. To implement these frameworks, we employed 8 distinct state-of-the-art outlier detection models, including one-class support vector machine (OCSVM), isolation forest adaptive sliding window approach (IForest ASD), exact storm (ES), angle-based outlier detection (ABOD), local outlier factor (LOF), Kitsunes online algorithm (KitNet), and K-nearest neighbour conformal density and distance based (KNN CAD). We evaluated the performance of these models across seven financial and healthcare prediction tasks, including credit card fraud detection, churn prediction, Ethereum fraud detection, heart stroke prediction, and diabetes prediction. The results indicate that our proposed incremental learning framework significantly improves performance, particularly on highly imbalanced datasets. Among all models, the IForest ASD model consistently ranked among the top three best-performing models, demonstrating superior effectiveness across various datasets."
      },
      {
        "id": "oai:arXiv.org:2307.02073v2",
        "title": "Performance Modeling of Data Storage Systems using Generative Models",
        "link": "https://arxiv.org/abs/2307.02073",
        "author": "Abdalaziz Rashid Al-Maeeni, Aziz Temirkhanov, Artem Ryzhikov, Mikhail Hushchyn",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2307.02073v2 Announce Type: replace \nAbstract: High-precision modeling of systems is one of the main areas of industrial data analysis. Models of systems, their digital twins, are used to predict their behavior under various conditions. We have developed several models of a storage system using machine learning-based generative models. The system consists of several components: hard disk drive (HDD) and solid-state drive (SSD) storage pools with different RAID schemes and cache. Each storage component is represented by a probabilistic model that describes the probability distribution of the component performance in terms of IOPS and latency, depending on their configuration and external data load parameters. The results of the experiments demonstrate the errors of 4-10 % for IOPS and 3-16 % for latency predictions depending on the components and models of the system. The predictions show up to 0.99 Pearson correlation with Little's law, which can be used for unsupervised reliability checks of the models. In addition, we present novel data sets that can be used for benchmarking regression algorithms, conditional generative models, and uncertainty estimation methods in machine learning."
      },
      {
        "id": "oai:arXiv.org:2309.09454v3",
        "title": "Asymptotically efficient adaptive identification under saturated output observation",
        "link": "https://arxiv.org/abs/2309.09454",
        "author": "Lantian Zhang, Lei Guo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2309.09454v3 Announce Type: replace \nAbstract: As saturated output observations are ubiquitous in practice, identifying stochastic systems with such nonlinear observations is a fundamental problem across various fields. This paper investigates the asymptotically efficient identification problem for stochastic dynamical systems with saturated output observations. In contrast to most of the existing results, our results do not need the commonly used but stringent conditions such as periodic or independent assumptions on the system signals, and thus do not exclude applications to stochastic feedback systems. To be specific, we introduce a new adaptive Newton-type algorithm on the negative log-likelihood of the partially observed samples using a two-step design technique. Under some general excitation data conditions, we show that the parameter estimate is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Furthermore, we show that the mean square error of the estimates can achieve the Cramer-Rao bound asymptotically without resorting to i.i.d data assumptions. This indicates that the performance of the proposed algorithm is the best possible that one can expect in general. A numerical example is provided to illustrate the superiority of our new adaptive algorithm over the existing related ones in the literature."
      },
      {
        "id": "oai:arXiv.org:2310.06906v3",
        "title": "Distillation Improves Visual Place Recognition for Low Quality Images",
        "link": "https://arxiv.org/abs/2310.06906",
        "author": "Anbang Yang, Ge Jin, Junjie Huang, Yao Wang, John-Ross Rizzo, Chen Feng",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2310.06906v3 Announce Type: replace \nAbstract: Real-time visual localization often utilizes online computing, for which query images or videos are transmitted to remote servers for visual place recognition (VPR). However, limited network bandwidth necessitates image-quality reduction and thus the degradation of global image descriptors, reducing VPR accuracy. We address this issue at the descriptor extraction level with a knowledge-distillation methodology that learns feature representations from high-quality images to extract more discriminative descriptors from low-quality images. Our approach includes the Inter-channel Correlation Knowledge Distillation (ICKD) loss, Mean Squared Error (MSE) loss, and Triplet loss. We validate the proposed losses on multiple VPR methods and datasets subjected to JPEG compression, resolution reduction, and video quantization. We obtain significant improvements in VPR recall rates under all three tested modalities of lowered image quality. Furthermore, we fill a gap in VPR literature on video-based data and its influence on VPR performance. This work contributes to more reliable place recognition in resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2311.02757v2",
        "title": "Certified Defense on the Fairness of Graph Neural Networks",
        "link": "https://arxiv.org/abs/2311.02757",
        "author": "Yushun Dong, Binchi Zhang, Hanghang Tong, Jundong Li",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.02757v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically proved that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes any GNNs as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not have any assumption over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs, where ELEGANT is also demonstrated to be beneficial for GNN debiasing. Open-source code can be found at https://github.com/yushundong/ELEGANT."
      },
      {
        "id": "oai:arXiv.org:2311.08870v3",
        "title": "One-Shot Heterogeneous Federated Learning with Local Model-Guided Diffusion Models",
        "link": "https://arxiv.org/abs/2311.08870",
        "author": "Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.08870v3 Announce Type: replace \nAbstract: In recent years, One-shot Federated Learning methods based on Diffusion Models have garnered increasing attention due to their remarkable performance. However, most of these methods require the deployment of foundation models on client devices, which significantly raises the computational requirements and reduces their adaptability to heterogeneous client models compared to traditional FL methods. In this paper, we propose FedLMG, a heterogeneous one-shot Federated learning method with Local Model-Guided diffusion models. Briefly speaking, in FedLMG, clients do not need access to any foundation models but only train and upload their local models, which is consistent with traditional FL methods. On the clients, we employ classification loss and BN loss to capture the broad category features and detailed contextual features of the client distributions. On the server, based on the uploaded client models, we utilize backpropagation to guide the server's DM in generating synthetic datasets that comply with the client distributions, which are then used to train the aggregated model. By using the locally trained client models as a medium to transfer client knowledge, our method significantly reduces the computational requirements on client devices and effectively adapts to scenarios with heterogeneous clients. Extensive quantitation and visualization experiments on three large-scale real-world datasets, along with theoretical analysis, demonstrate that the synthetic datasets generated by FedLMG exhibit comparable quality and diversity to the client datasets, which leads to an aggregated model that outperforms all compared methods and even the performance ceiling, further elucidating the significant potential of utilizing DMs in FL."
      },
      {
        "id": "oai:arXiv.org:2311.18773v3",
        "title": "Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding in Novel Domains",
        "link": "https://arxiv.org/abs/2311.18773",
        "author": "Zitian Tang, Rohan Myer Krishnan, Zhiqiu Yu, Chen Sun",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.18773v3 Announce Type: replace \nAbstract: Learning from (procedural) videos has increasingly served as a pathway for embodied agents to acquire skills from human demonstrations. To do this, video understanding models must be able to obtain structured understandings, such as the temporal segmentation of a demonstration into sequences of actions and skills, and to generalize the understandings to novel environments, tasks, and problem domains. In pursuit of this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1) step recognition and (2) video question answering, over a dataset of temporally segmented and labeled tasks in International Space Station spacewalk recordings. In tandem, the two tasks quantify a model's ability to: (1) generalize to novel domains; (2) utilize long temporal context and multimodal (e.g. visual and speech) information. Our extensive experimental analysis highlights the challenges of Spacewalk-18, but also suggests best practices for domain generalization and long-form understanding. Notably, we discover a promising adaptation via summarization technique that leads to significant performance improvement without model fine-tuning. The Spacewalk-18 benchmark is released at https://brown-palm.github.io/Spacewalk-18/."
      },
      {
        "id": "oai:arXiv.org:2312.15063v3",
        "title": "A universal approximation theorem for nonlinear resistive networks",
        "link": "https://arxiv.org/abs/2312.15063",
        "author": "Benjamin Scellier, Siddhartha Mishra",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.15063v3 Announce Type: replace \nAbstract: Resistor networks have recently been studied as analog computing platforms for machine learning, particularly due to their compatibility with the Equilibrium Propagation training framework. In this work, we explore the computational capabilities of these networks. We prove that electrical networks consisting of voltage sources, linear resistors, diodes, and voltage-controlled voltage sources (VCVSs) can approximate any continuous function to arbitrary precision. Central to our proof is a method for translating a neural network with rectified linear units into an approximately equivalent electrical network comprising these four elements. Our proof relies on two assumptions: (a) that circuit elements are ideal, and (b) that variable resistor conductances and VCVS amplification factors can take any value (arbitrarily small or large). Our findings provide insights that could guide the development of universal self-learning electrical networks."
      },
      {
        "id": "oai:arXiv.org:2402.12309v2",
        "title": "TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs",
        "link": "https://arxiv.org/abs/2402.12309",
        "author": "Siheng Xiong, Yuan Yang, Faramarz Fekri, James Clayton Kerce",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.12309v2 Announce Type: replace \nAbstract: Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results. In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different. In all these cases, TILP works much better than the state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2403.17834v3",
        "title": "Developing Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography",
        "link": "https://arxiv.org/abs/2403.17834",
        "author": "Ibrahim Ethem Hamamci, Sezgin Er, Chenyu Wang, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Doga, Omer Faruk Durugol, Weicheng Dai, Murong Xu, Muhammed Furkan Dasdelen, Bastian Wittmann, Tamaz Amiranashvili, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Christian Bluethgen, Kayhan Batmanghelich, Mehmet Kemal Ozdemir, Bjoern Menze",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.17834v3 Announce Type: replace \nAbstract: While computer vision has achieved tremendous success with multimodal encoding and direct textual interaction with images via chat-based large language models, similar advancements in medical imaging AI, particularly in 3D imaging, have been limited due to the scarcity of comprehensive datasets. To address this critical gap, we introduce CT-RATE, the first dataset that pairs 3D medical images with corresponding textual reports. CT-RATE comprises 25,692 non-contrast 3D chest CT scans from 21,304 unique patients. Through various reconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3 million 2D slices. Each scan is accompanied by its corresponding radiology report. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive language-image pretraining framework designed for broad applications without the need for task-specific training. We demonstrate how CT-CLIP can be used in two tasks: multi-abnormality detection and case retrieval. Remarkably, in multi-abnormality detection, CT-CLIP outperforms state-of-the-art fully supervised models across all key metrics, effectively eliminating the need for manual annotation. In case retrieval, it efficiently retrieves relevant cases using either image or textual queries, thereby enhancing knowledge dissemination. By combining CT-CLIP's vision encoder with a pretrained large language model, we create CT-CHAT, a vision-language foundational chat model for 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs derived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI assistants, underscoring the necessity for specialized methods in 3D medical imaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT not only addresses critical challenges in 3D medical imaging, but also lays the groundwork for future innovations in medical AI and improved patient care."
      },
      {
        "id": "oai:arXiv.org:2404.00916v3",
        "title": "Gyro-based Neural Single Image Deblurring",
        "link": "https://arxiv.org/abs/2404.00916",
        "author": "Heemin Yang, Jaesung Rim, Seungyong Lee, Seung-Hwan Baek, Sunghyun Cho",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.00916v3 Announce Type: replace \nAbstract: In this paper, we present GyroDeblurNet, a novel single-image deblurring method that utilizes a gyro sensor to resolve the ill-posedness of image deblurring. The gyro sensor provides valuable information about camera motion that can improve deblurring quality. However, exploiting real-world gyro data is challenging due to errors from various sources. To handle these errors, GyroDeblurNet is equipped with two novel neural network blocks: a gyro refinement block and a gyro deblurring block. The gyro refinement block refines the erroneous gyro data using the blur information from the input image. The gyro deblurring block removes blur from the input image using the refined gyro data and further compensates for gyro error by leveraging the blur information from the input image. For training a neural network with erroneous gyro data, we propose a training strategy based on the curriculum learning. We also introduce a novel gyro data embedding scheme to represent real-world intricate camera shakes. Finally, we present both synthetic and real-world datasets for training and evaluating gyro-based single image deblurring. Our experiments demonstrate that our approach achieves state-of-the-art deblurring quality by effectively utilizing erroneous gyro data."
      },
      {
        "id": "oai:arXiv.org:2404.05659v3",
        "title": "VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain",
        "link": "https://arxiv.org/abs/2404.05659",
        "author": "Khai Le-Duc",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.05659v3 Announce Type: replace \nAbstract: Due to privacy restrictions, there's a shortage of publicly available speech recognition datasets in the medical domain. In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech. To our best knowledge, VietMed is by far the world's largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents. VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration. Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country. Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR. Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%). All code, data and models are made publicly available: https://github.com/leduckhai/MultiMed/tree/master/VietMed."
      },
      {
        "id": "oai:arXiv.org:2404.10620v2",
        "title": "PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction",
        "link": "https://arxiv.org/abs/2404.10620",
        "author": "Sinisa Stekovic, Arslan Artykov, Stefan Ainetter, Mattia D'Urso, Friedrich Fraundorfer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.10620v2 Announce Type: replace \nAbstract: We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D objects and their parameters from images using interpretable shape programs. Unlike traditional CAD model retrieval, shape programs allow reasoning about semantic parameters, editing, and a low memory footprint. Despite their potential, shape programs for 3D scene understanding have been largely overlooked. Our key contribution is enabling gradient-based optimization by parsing shape programs, or more precisely procedural models designed in Blender, into efficient PyTorch code. While there are many possible applications of our PyTochGeoNodes, we show that a combination of PyTorchGeoNodes with genetic algorithm is a method of choice to optimize both discrete and continuous shape program parameters for 3D reconstruction and understanding of 3D object parameters. Our modular framework can be further integrated with other reconstruction algorithms, and we demonstrate one such integration to enable procedural Gaussian splatting. Our experiments on the ScanNet dataset show that our method achieves accurate reconstructions while enabling, until now, unseen level of 3D scene understanding."
      },
      {
        "id": "oai:arXiv.org:2405.16444v3",
        "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion",
        "link": "https://arxiv.org/abs/2405.16444",
        "author": "Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.16444v3 Announce Type: replace \nAbstract: Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text's cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache."
      },
      {
        "id": "oai:arXiv.org:2405.17743v5",
        "title": "ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling",
        "link": "https://arxiv.org/abs/2405.17743",
        "author": "Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, Zizhuo Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.17743v5 Announce Type: replace \nAbstract: Optimization modeling plays a critical role in the application of Operations Research (OR) tools to address real-world problems, yet they pose challenges and require extensive expertise from OR experts. With the advent of large language models (LLMs), new opportunities have emerged to streamline and automate such task. However, current research predominantly relies on closed-source LLMs such as GPT-4, along with extensive prompt engineering techniques. This reliance stems from the scarcity of high-quality training datasets for optimization modeling, resulting in elevated costs, prolonged processing times, and privacy concerns. To address these challenges, our work is the first to propose a viable path for training open-source LLMs that are capable of optimization modeling and developing solver codes, eventually leading to a superior ability for automating optimization modeling and solving. Particularly, we design the {\\sc OR-Instruct}, a semi-automated data synthesis framework for optimization modeling that enables customizable enhancements for specific scenarios or model types. This work also introduces IndustryOR, the first industrial benchmark for evaluating LLMs in solving practical OR problems. We train several 7B-scale open-source LLMs using synthesized data (dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit significantly enhanced optimization modeling capabilities, achieving competitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks. Additionally, our experiments highlight the potential of scaling law and reinforcement learning to further enhance the performance of ORLMs. The workflows and human-machine interaction paradigms of ORLMs in practical industrial applications are also discussed in the paper."
      },
      {
        "id": "oai:arXiv.org:2406.02424v3",
        "title": "Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints",
        "link": "https://arxiv.org/abs/2406.02424",
        "author": "Zifeng Zhao, Feiyu Jiang, Yi Yu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.02424v3 Announce Type: replace \nAbstract: We study contextual dynamic pricing problems where a firm sells products to $T$ sequentially-arriving consumers, behaving according to an unknown demand model. The firm aims to minimize its regret over a clairvoyant that knows the model in advance. The demand follows a generalized linear model (GLM), allowing for stochastic feature vectors in $\\mathbb R^d$ encoding product and consumer information. We first show the optimal regret is of order $\\sqrt{dT}$, up to logarithmic factors, improving existing upper bounds by a $\\sqrt{d}$ factor. This optimal rate is materialized by two algorithms: a confidence bound-type algorithm and an explore-then-commit (ETC) algorithm. A key insight is an intrinsic connection between dynamic pricing and contextual multi-armed bandit problems with many arms with a careful discretization. We further study contextual dynamic pricing under local differential privacy (LDP) constraints. We propose a stochastic gradient descent-based ETC algorithm achieving regret upper bounds of order $d\\sqrt{T}/\\epsilon$, up to logarithmic factors, where $\\epsilon>0$ is the privacy parameter. The upper bounds with and without LDP constraints are matched by newly constructed minimax lower bounds, characterizing costs of privacy. Moreover, we extend our study to dynamic pricing under mixed privacy constraints, improving the privacy-utility tradeoff by leveraging public data. This is the first time such setting is studied in the dynamic pricing literature and our theoretical results seamlessly bridge dynamic pricing with and without LDP. Extensive numerical experiments and real data applications are conducted to illustrate the efficiency and practical value of our algorithms."
      },
      {
        "id": "oai:arXiv.org:2406.15523v2",
        "title": "Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark",
        "link": "https://arxiv.org/abs/2406.15523",
        "author": "Yili Wang, Yixin Liu, Xu Shen, Chenyu Li, Kaize Ding, Rui Miao, Ying Wang, Shirui Pan, Xin Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.15523v2 Announce Type: replace \nAbstract: To build safe and reliable graph machine learning systems, unsupervised graph-level anomaly detection (GLAD) and unsupervised graph-level out-of-distribution (OOD) detection (GLOD) have received significant attention in recent years. Though those two lines of research indeed share the same objective, they have been studied independently in the community due to distinct evaluation setups, creating a gap that hinders the application and evaluation of methods from one to the other. To bridge the gap, in this work, we present a \\underline{\\textbf{U}}nified \\underline{\\textbf{B}}enchmark for unsupervised \\underline{\\textbf{G}}raph-level \\underline{\\textbf{O}}OD and anoma\\underline{\\textbf{L}}y \\underline{\\textbf{D}}etection (\\ourmethod), a comprehensive evaluation framework that unifies GLAD and GLOD under the concept of generalized graph-level OOD detection. Our benchmark encompasses 35 datasets spanning four practical anomaly and OOD detection scenarios, facilitating the comparison of 18 representative GLAD/GLOD methods. We conduct multi-dimensional analyses to explore the effectiveness, OOD sensitivity spectrum, robustness, and efficiency of existing methods, shedding light on their strengths and limitations. Furthermore, we provide an open-source codebase (https://github.com/UB-GOLD/UB-GOLD) of \\ourmethod to foster reproducible research and outline potential directions for future investigations based on our insights."
      },
      {
        "id": "oai:arXiv.org:2406.15888v2",
        "title": "Real-time Speech Summarization for Medical Conversations",
        "link": "https://arxiv.org/abs/2406.15888",
        "author": "Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.15888v2 Announce Type: replace \nAbstract: In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum"
      },
      {
        "id": "oai:arXiv.org:2406.18038v5",
        "title": "MT2ST: Adaptive Multi-Task to Single-Task Learning",
        "link": "https://arxiv.org/abs/2406.18038",
        "author": "Dong Liu, Yanxuan Yu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.18038v5 Announce Type: replace \nAbstract: Efficient machine learning (ML) has become increasingly important as models grow larger and data volumes expand. In this work, we address the trade-off between generalization in multi-task learning (MTL) and precision in single-task learning (STL) by introducing the Multi-Task to Single-Task (MT2ST) framework. MT2ST is designed to enhance training efficiency and accuracy in multi-modal tasks, showcasing its value as a practical application of efficient ML."
      },
      {
        "id": "oai:arXiv.org:2406.20052v3",
        "title": "Understanding and Mitigating Language Confusion in LLMs",
        "link": "https://arxiv.org/abs/2406.20052",
        "author": "Kelly Marchisio, Wei-Yin Ko, Alexandre B\\'erard, Th\\'eo Dehaze, Sebastian Ruder",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.20052v3 Announce Type: replace \nAbstract: We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user's desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation at https://github.com/for-ai/language-confusion."
      },
      {
        "id": "oai:arXiv.org:2407.04620v3",
        "title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States",
        "link": "https://arxiv.org/abs/2407.04620",
        "author": "Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.04620v3 Announce Type: replace \nAbstract: Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research."
      },
      {
        "id": "oai:arXiv.org:2407.21121v3",
        "title": "Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks",
        "link": "https://arxiv.org/abs/2407.21121",
        "author": "Tiago Novello, Diana Aldana, Andre Araujo, Luiz Velho",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.21121v3 Announce Type: replace \nAbstract: Sinusoidal neural networks have been shown effective as implicit neural representations (INRs) of low-dimensional signals, due to their smoothness and high representation capacity. However, initializing and training them remain empirical tasks which lack on deeper understanding to guide the learning process. To fill this gap, our work introduces a theoretical framework that explains the capacity property of sinusoidal networks and offers robust control mechanisms for initialization and training. Our analysis is based on a novel amplitude-phase expansion of the sinusoidal multilayer perceptron, showing how its layer compositions produce a large number of new frequencies expressed as integer combinations of the input frequencies. This relationship can be directly used to initialize the input neurons, as a form of spectral sampling, and to bound the network's spectrum while training. Our method, referred to as TUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and convergence of sinusoidal INR training, leading to detailed reconstructions, while preventing overfitting."
      },
      {
        "id": "oai:arXiv.org:2408.03745v2",
        "title": "Intuitionistic Fuzzy Cognitive Maps for Interpretable Image Classification",
        "link": "https://arxiv.org/abs/2408.03745",
        "author": "Georgia Sovatzidi, Michael D. Vasilakakis, Dimitris K. Iakovidis",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.03745v2 Announce Type: replace \nAbstract: Several deep learning (DL) approaches have been proposed to deal with image classification tasks. However, despite their effectiveness, they lack interpretability, as they are unable to explain or justify their results. To address the challenge of interpretable image classification, this paper introduces a novel framework, named Interpretable Intuitionistic Fuzzy Cognitive Maps (I2FCMs).Intuitionistic FCMs (iFCMs) have been proposed as an extension of FCMs offering a natural mechanism to assess the quality of their output through the estimation of hesitancy, a concept resembling human hesitation in decision making. In the context of image classification, hesitancy is considered as a degree of unconfidence with which an image is categorized to a class. To the best of our knowledge this is the first time iFCMs are applied for image classification. Further novel contributions of the introduced framework include the following: a) a feature extraction process focusing on the most informative image regions; b) a learning algorithm for automatic data-driven determination of the intuitionistic fuzzy interconnections of the iFCM, thereby reducing human intervention in the definition of the graph structure; c) an inherently interpretable classification approach based on image contents, providing understandable explanations of its predictions, using linguistic terms. Furthermore, the proposed I2FCM framework can be applied to DL models, including Convolutional Neural Network (CNN), rendering them interpretable. The effectiveness of I2FCM is evaluated on publicly available datasets, and the results confirm that it can provide enhanced classification performance, while providing interpretable inferences."
      },
      {
        "id": "oai:arXiv.org:2408.11546v3",
        "title": "Memorization in In-Context Learning",
        "link": "https://arxiv.org/abs/2408.11546",
        "author": "Shahriar Golchin, Mihai Surdeanu, Steven Bethard, Eduardo Blanco, Ellen Riloff",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.11546v3 Announce Type: replace \nAbstract: In-context learning (ICL) has proven to be an effective strategy for improving the performance of large language models (LLMs) with no additional training. However, the exact mechanism behind this performance improvement remains unclear. This study is the first to show how ICL surfaces memorized training data and to explore the correlation between this memorization and performance on downstream tasks across various ICL regimes: zero-shot, few-shot, and many-shot. Our most notable findings include: (1) ICL significantly surfaces memorization compared to zero-shot learning in most cases; (2) demonstrations, without their labels, are the most effective element in surfacing memorization; (3) ICL improves performance when the surfaced memorization in few-shot regimes reaches a high level (about 40%); and (4) there is a very strong correlation between performance and memorization in ICL when it outperforms zero-shot learning. Overall, our study uncovers memorization as a new factor impacting ICL, raising an important question: to what extent do LLMs truly generalize from demonstrations in ICL, and how much of their success is due to memorization?"
      },
      {
        "id": "oai:arXiv.org:2409.00159v3",
        "title": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities",
        "link": "https://arxiv.org/abs/2409.00159",
        "author": "Gurvan Richardeau, Samy Chali, Erwan Le Merrer, Camilla Penzo, Gilles Tredan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.00159v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) are nowadays prompted for a wide variety of tasks. In this article, we investigate their ability in reciting and generating graphs. We first study the ability of LLMs to regurgitate well known graphs from the literature (e.g. Karate club or the graph atlas)4. Secondly, we question the generative capabilities of LLMs by asking for Erdos-Renyi random graphs. As opposed to the possibility that they could memorize some Erdos-Renyi graphs included in their scraped training set, this second investigation aims at studying a possible emergent property of LLMs. For both tasks, we propose a metric to assess their errors with the lens of hallucination (i.e. incorrect information returned as facts). We most notably find that the amplitude of graph hallucinations can characterize the superiority of some LLMs. Indeed, for the recitation task, we observe that graph hallucinations correlate with the Hallucination Leaderboard, a hallucination rank that leverages 10, 000 times more prompts to obtain its ranking. For the generation task, we find surprisingly good and reproducible results in most of LLMs. We believe this to constitute a starting point for more in-depth studies of this emergent capability and a challenging benchmark for their improvements. Altogether, these two aspects of LLMs capabilities bridge a gap between the network science and machine learning communities."
      },
      {
        "id": "oai:arXiv.org:2409.01354v3",
        "title": "Explanation Space: A New Perspective into Time Series Interpretability",
        "link": "https://arxiv.org/abs/2409.01354",
        "author": "Shahbaz Rezaei, Xin Liu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.01354v3 Announce Type: replace \nAbstract: Human understandable explanation of deep learning models is essential for various critical and sensitive applications. Unlike image or tabular data where the importance of each input feature (for the classifier's decision) can be directly projected into the input, time series distinguishable features (e.g. dominant frequency) are often hard to manifest in time domain for a user to easily understand. Additionally, most explanation methods require a baseline value as an indication of the absence of any feature. However, the notion of lack of feature, which is often defined as black pixels for vision tasks or zero/mean values for tabular data, is not well-defined in time series. Despite the adoption of explainable AI methods (XAI) from tabular and vision domain into time series domain, these differences limit the application of these XAI methods in practice. In this paper, we propose a simple yet effective method that allows a model originally trained on the time domain to be interpreted in other explanation spaces using existing methods. We suggest five explanation spaces, each of which can potentially alleviate these issues in certain types of time series. Our method can be easily integrated into existing platforms without any changes to trained models or XAI methods. The code will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2409.02634v3",
        "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency",
        "link": "https://arxiv.org/abs/2409.02634",
        "author": "Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.02634v3 Announce Type: replace \nAbstract: With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios."
      },
      {
        "id": "oai:arXiv.org:2409.02838v2",
        "title": "iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation",
        "link": "https://arxiv.org/abs/2409.02838",
        "author": "Hayeon Jo, Hyesong Choi, Minhee Cho, Dongbo Min",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.02838v2 Announce Type: replace \nAbstract: Transfer learning based on full fine-tuning (FFT) of the pre-trained encoder and task-specific decoder becomes increasingly complex as deep models grow exponentially. Parameter efficient fine-tuning (PEFT) approaches using adapters consisting of small learnable layers have emerged as an alternative to FFT, achieving comparable performance while maintaining high training efficiency. However, the inflexibility of the adapter with respect to input instances limits its capability of learning task-specific information in diverse downstream tasks. In this paper, we propose a novel PEFT approach, input-Conditioned transFormer, termed iConFormer, that leverages a dynamic adapter conditioned on the input instances. To secure flexible learning ability on input instances in various downstream tasks, we introduce an input-Conditioned Network (iCoN) in the dynamic adapter that enables instance-level feature transformation. To be specific, iCoN generates channel-wise convolutional kernels for each feature and transform it using adaptive convolution process to effectively capture task-specific and fine-grained details tailor to downstream tasks. Experimental results demonstrate that by tuning just 1.6% to 2.8% of the Transformer backbone parameters, iConFormer achieves performance comparable to FFT in monocular depth estimation and semantic segmentation, while outperforming it in image classification and instance segmentation. Also, the proposed method consistently outperforms recent PEFT methods for all the tasks mentioned above."
      },
      {
        "id": "oai:arXiv.org:2409.13521v2",
        "title": "A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges",
        "link": "https://arxiv.org/abs/2409.13521",
        "author": "Lorenzo Zangari, Candida M. Greco, Davide Picca, Andrea Tagarelli",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.13521v2 Announce Type: replace \nAbstract: Moral values have deep roots in early civilizations, codified within norms and laws that regulated societal order and the common good. They play a crucial role in understanding the psychological basis of human behavior and cultural orientation. The Moral Foundation Theory (MFT) is a well-established framework that identifies the core moral foundations underlying the manner in which different cultures shape individual and social lives. Recent advancements in natural language processing, particularly Pre-trained Language Models (PLMs), have enabled the extraction and analysis of moral dimensions from textual data. This survey presents a comprehensive review of MFT-informed PLMs, providing an analysis of moral tendencies in PLMs and their application in the context of the MFT. We also review relevant datasets and lexicons and discuss trends, limitations, and future directions. By providing a structured overview of the intersection between PLMs and MFT, this work bridges moral psychology insights within the realm of PLMs, paving the way for further research and development in creating morally aware AI systems."
      },
      {
        "id": "oai:arXiv.org:2409.18932v2",
        "title": "ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse Weather Conditions",
        "link": "https://arxiv.org/abs/2409.18932",
        "author": "Wenfeng Huang, Guoan Xu, Wenjing Jia, Stuart Perry, Guangwei Gao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.18932v2 Announce Type: replace \nAbstract: Images captured in challenging environments--such as nighttime, smoke, rainy weather, and underwater--often suffer from significant degradation, resulting in a substantial loss of visual quality. The effective restoration of these degraded images is critical for the subsequent vision tasks. While many existing approaches have successfully incorporated specific priors for individual tasks, these tailored solutions limit their applicability to other degradations. In this work, we propose a universal network architecture, dubbed ``ReviveDiff'', which can address various degradations and bring images back to life by enhancing and restoring their quality. Our approach is inspired by the observation that, unlike degradation caused by movement or electronic issues, quality degradation under adverse conditions primarily stems from natural media (such as fog, water, and low luminance), which generally preserves the original structures of objects. To restore the quality of such images, we leveraged the latest advancements in diffusion models and developed ReviveDiff to restore image quality from both macro and micro levels across some key factors determining image quality, such as sharpness, distortion, noise level, dynamic range, and color accuracy. We rigorously evaluated ReviveDiff on seven benchmark datasets covering five types of degrading conditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Our experimental results demonstrate that ReviveDiff outperforms the state-of-the-art methods both quantitatively and visually."
      },
      {
        "id": "oai:arXiv.org:2410.01618v2",
        "title": "SGBA: Semantic Gaussian Mixture Model-Based LiDAR Bundle Adjustment",
        "link": "https://arxiv.org/abs/2410.01618",
        "author": "Xingyu Ji, Shenghai Yuan, Jianping Li, Pengyu Yin, Haozhi Cao, Lihua Xie",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.01618v2 Announce Type: replace \nAbstract: LiDAR bundle adjustment (BA) is an effective approach to reduce the drifts in pose estimation from the front-end. Existing works on LiDAR BA usually rely on predefined geometric features for landmark representation. This reliance restricts generalizability, as the system will inevitably deteriorate in environments where these specific features are absent. To address this issue, we propose SGBA, a LiDAR BA scheme that models the environment as a semantic Gaussian mixture model (GMM) without predefined feature types. This approach encodes both geometric and semantic information, offering a comprehensive and general representation adaptable to various environments. Additionally, to limit computational complexity while ensuring generalizability, we propose an adaptive semantic selection framework that selects the most informative semantic clusters for optimization by evaluating the condition number of the cost function. Lastly, we introduce a probabilistic feature association scheme that considers the entire probability density of assignments, which can manage uncertainties in measurement and initial pose estimation. We have conducted various experiments and the results demonstrate that SGBA can achieve accurate and robust pose refinement even in challenging scenarios with low-quality initial pose estimation and limited geometric features. We plan to open-source the work for the benefit of the community https://github.com/Ji1Xinyu/SGBA."
      },
      {
        "id": "oai:arXiv.org:2410.02558v3",
        "title": "Improving Unsupervised Constituency Parsing via Maximizing Semantic Information",
        "link": "https://arxiv.org/abs/2410.02558",
        "author": "Junjie Chen, Xiangheng He, Yusuke Miyao, Danushka Bollegala",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02558v3 Announce Type: replace \nAbstract: Unsupervised constituency parsers organize phrases within a sentence into a tree-shaped syntactic constituent structure that reflects the organization of sentence semantics. However, the traditional objective of maximizing sentence log-likelihood (LL) does not explicitly account for the close relationship between the constituent structure and the semantics, resulting in a weak correlation between LL values and parsing accuracy. In this paper, we introduce a novel objective that trains parsers by maximizing SemInfo, the semantic information encoded in constituent structures. We introduce a bag-of-substrings model to represent the semantics and estimate the SemInfo value using the probability-weighted information metric. We apply the SemInfo maximization objective to training Probabilistic Context-Free Grammar (PCFG) parsers and develop a Tree Conditional Random Field (TreeCRF)-based model to facilitate the training. Experiments show that SemInfo correlates more strongly with parsing accuracy than LL, establishing SemInfo as a better unsupervised parsing objective. As a result, our algorithm significantly improves parsing accuracy by an average of 7.85 sentence-F1 scores across five PCFG variants and in four languages, achieving state-of-the-art level results in three of the four languages."
      },
      {
        "id": "oai:arXiv.org:2410.07838v3",
        "title": "Minority-Focused Text-to-Image Generation via Prompt Optimization",
        "link": "https://arxiv.org/abs/2410.07838",
        "author": "Soobin Um, Jong Chul Ye",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.07838v3 Announce Type: replace \nAbstract: We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of text-conditional data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for high-quality generation. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that encourages emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes generation of minority features by incorporating a carefully-crafted likelihood objective. Extensive experiments conducted across various types of T2I models demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers. Code is available at https://github.com/soobin-um/MinorityPrompt."
      },
      {
        "id": "oai:arXiv.org:2410.09893v2",
        "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment",
        "link": "https://arxiv.org/abs/2410.09893",
        "author": "Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.09893v2 Announce Type: replace \nAbstract: Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization. We demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs. Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. Our evaluation code and datasets are available at https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark."
      },
      {
        "id": "oai:arXiv.org:2410.10516v3",
        "title": "UniGEM: A Unified Approach to Generation and Property Prediction for Molecules",
        "link": "https://arxiv.org/abs/2410.10516",
        "author": "Shikun Feng, Yuyan Ni, Yan Lu, Zhi-Ming Ma, Wei-Ying Ma, Yanyan Lan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10516v3 Announce Type: replace \nAbstract: Molecular generation and molecular property prediction are both crucial for drug discovery, but they are often developed independently. Inspired by recent studies, which demonstrate that diffusion model, a prominent generative approach, can learn meaningful data representations that enhance predictive tasks, we explore the potential for developing a unified generative model in the molecular domain that effectively addresses both molecular generation and property prediction tasks. However, the integration of these tasks is challenging due to inherent inconsistencies, making simple multi-task learning ineffective. To address this, we propose UniGEM, the first unified model to successfully integrate molecular generation and property prediction, delivering superior performance in both tasks. Our key innovation lies in a novel two-phase generative process, where predictive tasks are activated in the later stages, after the molecular scaffold is formed. We further enhance task balance through innovative training strategies. Rigorous theoretical analysis and comprehensive experiments demonstrate our significant improvements in both tasks. The principles behind UniGEM hold promise for broader applications, including natural language processing and computer vision."
      },
      {
        "id": "oai:arXiv.org:2410.15316v3",
        "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
        "link": "https://arxiv.org/abs/2410.15316",
        "author": "Alan Dao (Gia Tuan Dao), Dinh Bach Vu, Huy Hoang Ha",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15316v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models."
      },
      {
        "id": "oai:arXiv.org:2410.17477v4",
        "title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination",
        "link": "https://arxiv.org/abs/2410.17477",
        "author": "Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Boxing Chen, Sarath Chandar",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.17477v4 Announce Type: replace \nAbstract: The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to hallucinate false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations."
      },
      {
        "id": "oai:arXiv.org:2410.23132v3",
        "title": "Revisiting MAE pre-training for 3D medical image segmentation",
        "link": "https://arxiv.org/abs/2410.23132",
        "author": "Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Maximilian Miller, Leander Maerkisch, Paul F. J\\\"ager, Klaus Maier-Hein",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.23132v3 Announce Type: replace \nAbstract: Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the potential of vast, untapped clinical datasets, for various downstream applications that suffer from the scarcity of labeled data. While SSL has revolutionized fields like natural language processing and computer vision, its adoption in 3D medical image computing has been limited by three key pitfalls: Small pre-training dataset sizes, architectures inadequate for 3D medical image analysis, and insufficient evaluation practices. In this paper, we address these issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes and ii) using a Residual Encoder U-Net architecture within the state-of-the-art nnU-Net framework. iii) A robust development framework, incorporating 5 development and 8 testing brain MRI segmentation datasets, allowed performance-driven design decisions to optimize the simple concept of Masked Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses previous SSL methods but also outperforms the strong nnU-Net baseline by an average of approximately 3 Dice points setting a new state-of-the-art. Our code and models are made available here."
      },
      {
        "id": "oai:arXiv.org:2411.00623v2",
        "title": "Dual Low-Rank Adaptation for Continual Learning with Pre-Trained Models",
        "link": "https://arxiv.org/abs/2411.00623",
        "author": "Huancheng Chen, Jingtao Li, Nidham Gazagnadou, Weiming Zhuang, Chen Chen, Lingjuan Lyu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00623v2 Announce Type: replace \nAbstract: In the era of foundation models, we revisit continual learning~(CL), which aims to enable vision transformers (ViTs) to learn new tasks over time. However, as the scale of these models increases, catastrophic forgetting remains a persistent challenge, particularly in the presence of significant domain shifts across tasks. Recent studies highlight a crossover between CL techniques and parameter-efficient fine-tuning (PEFT), which focuses on fine-tuning only a small set of trainable parameters to adapt to downstream tasks, such as low-rank adaptation (LoRA). While LoRA achieves faster convergence and requires fewer trainable parameters, it has seldom been explored in the context of continual learning. To address this gap, we propose a novel PEFT-CL method called Dual Low-Rank Adaptation (DualLoRA), which introduces both an orthogonal LoRA adapter and a residual LoRA adapter parallel to pre-trained weights in each layer. These components are orchestrated by a dynamic memory mechanism to strike a balance between stability and plasticity. The orthogonal LoRA adapter's parameters are updated in an orthogonal subspace of previous tasks to mitigate catastrophic forgetting, while the residual LoRA adapter's parameters are updated in the residual subspace spanned by task-specific bases without interaction across tasks, offering complementary capabilities for fine-tuning new tasks. On ViT-based models, we demonstrate that DualLoRA offers significant advantages in accuracy, inference speed, and memory efficiency over existing CL methods across multiple benchmarks."
      },
      {
        "id": "oai:arXiv.org:2411.04607v2",
        "title": "Cross- and Intra-image Prototypical Learning for Multi-label Disease Diagnosis and Interpretation",
        "link": "https://arxiv.org/abs/2411.04607",
        "author": "Chong Wang, Fengbei Liu, Yuanhong Chen, Helen Frazer, Gustavo Carneiro",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.04607v2 Announce Type: replace \nAbstract: Recent advances in prototypical learning have shown remarkable potential to provide useful decision interpretations associating activation maps and predictions with class-specific training prototypes. Such prototypical learning has been well-studied for various single-label diseases, but for quite relevant and more challenging multi-label diagnosis, where multiple diseases are often concurrent within an image, existing prototypical learning models struggle to obtain meaningful activation maps and effective class prototypes due to the entanglement of the multiple diseases. In this paper, we present a novel Cross- and Intra-image Prototypical Learning (CIPL) framework, for accurate multi-label disease diagnosis and interpretation from medical images. CIPL takes advantage of common cross-image semantics to disentangle the multiple diseases when learning the prototypes, allowing a comprehensive understanding of complicated pathological lesions. Furthermore, we propose a new two-level alignment-based regularisation strategy that effectively leverages consistent intra-image information to enhance interpretation robustness and predictive performance. Extensive experiments show that our CIPL attains the state-of-the-art (SOTA) classification accuracy in two public multi-label benchmarks of disease diagnosis: thoracic radiography and fundus images. Quantitative interpretability results show that CIPL also has superiority in weakly-supervised thoracic disease localisation over other leading saliency- and prototype-based explanation methods."
      },
      {
        "id": "oai:arXiv.org:2411.06965v2",
        "title": "Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration",
        "link": "https://arxiv.org/abs/2411.06965",
        "author": "Xingrui Yu, Zhenglin Wan, David Mark Bossens, Yueming Lyu, Qing Guo, Ivor W. Tsang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.06965v2 Announce Type: replace \nAbstract: Learning diverse and high-performance behaviors from a limited set of demonstrations is a grand challenge. Traditional imitation learning methods usually fail in this task because most of them are designed to learn one specific behavior even with multiple demonstrations. Therefore, novel techniques for \\textit{quality diversity imitation learning}, which bridges the quality diversity optimization and imitation learning methods, are needed to solve the above challenge. This work introduces Wasserstein Quality Diversity Imitation Learning (WQDIL), which 1) improves the stability of imitation learning in the quality diversity setting with latent adversarial training based on a Wasserstein Auto-Encoder (WAE), and 2) mitigates a behavior-overfitting issue using a measure-conditioned reward function with a single-step archive exploration bonus. Empirically, our method significantly outperforms state-of-the-art IL methods, achieving near-expert or beyond-expert QD performance on the challenging continuous control tasks derived from MuJoCo environments."
      },
      {
        "id": "oai:arXiv.org:2411.07462v3",
        "title": "MureObjectStitch: Multi-reference Image Composition",
        "link": "https://arxiv.org/abs/2411.07462",
        "author": "Jiaxuan Chen, Bo Zhang, Qingdong He, Jinlong Peng, Li Niu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.07462v3 Announce Type: replace \nAbstract: Generative image composition aims to regenerate the given foreground object in the background image to produce a realistic composite image. The existing methods are struggling to preserve the foreground details and adjust the foreground pose/viewpoint at the same time. In this work, we propose an effective finetuning strategy for generative image composition model, in which we finetune a pretrained model using one or more images containing the same foreground object. Moreover, we propose a multi-reference strategy, which allows the model to take in multiple reference images of the foreground object. The experiments on MureCOM dataset verify the effectiveness of our method. The code and model have been released at https://github.com/bcmi/MureObjectStitch-Image-Composition."
      },
      {
        "id": "oai:arXiv.org:2411.07660v3",
        "title": "HMIL: Hierarchical Multi-Instance Learning for Fine-Grained Whole Slide Image Classification",
        "link": "https://arxiv.org/abs/2411.07660",
        "author": "Cheng Jin, Luyang Luo, Huangjing Lin, Jun Hou, Hao Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.07660v3 Announce Type: replace \nAbstract: Fine-grained classification of whole slide images (WSIs) is essential in precision oncology, enabling precise cancer diagnosis and personalized treatment strategies. The core of this task involves distinguishing subtle morphological variations within the same broad category of gigapixel-resolution images, which presents a significant challenge. While the multi-instance learning (MIL) paradigm alleviates the computational burden of WSIs, existing MIL methods often overlook hierarchical label correlations, treating fine-grained classification as a flat multi-class classification task. To overcome these limitations, we introduce a novel hierarchical multi-instance learning (HMIL) framework. By facilitating on the hierarchical alignment of inherent relationships between different hierarchy of labels at instance and bag level, our approach provides a more structured and informative learning process. Specifically, HMIL incorporates a class-wise attention mechanism that aligns hierarchical information at both the instance and bag levels. Furthermore, we introduce supervised contrastive learning to enhance the discriminative capability for fine-grained classification and a curriculum-based dynamic weighting module to adaptively balance the hierarchical feature during training. Extensive experiments on our large-scale cytology cervical cancer (CCC) dataset and two public histology datasets, BRACS and PANDA, demonstrate the state-of-the-art class-wise and overall performance of our HMIL framework. Our source code is available at https://github.com/ChengJin-git/HMIL."
      },
      {
        "id": "oai:arXiv.org:2411.09921v2",
        "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level",
        "link": "https://arxiv.org/abs/2411.09921",
        "author": "Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, Chen Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.09921v2 Announce Type: replace \nAbstract: In this paper, we introduce Motion-Grounded Video Reasoning, a new motion understanding task that requires generating visual answers (video segmentation masks) according to the input question, and hence needs implicit spatiotemporal reasoning and grounding. This task extends existing spatiotemporal grounding work focusing on explicit action/motion grounding, to a more general format by enabling implicit reasoning via questions. To facilitate the development of the new task, we collect a large-scale dataset called GROUNDMORE, which comprises 1,715 video clips, 249K object masks that are deliberately designed with 4 question types (Causal, Sequential, Counterfactual, and Descriptive) for benchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE uniquely requires models to generate visual answers, providing a more concrete and visually interpretable response than plain texts. It evaluates models on both spatiotemporal grounding and reasoning, fostering to address complex challenges in motion-related video reasoning, temporal perception, and pixel-level understanding. Furthermore, we introduce a novel baseline model named Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the multimodal reasoning ability from the Multimodal LLM, the pixel-level perception capability from the grounding model (SAM), and the temporal perception ability from a lightweight localization head. MORA achieves respectable performance on GROUNDMORE outperforming the best existing visual grounding baseline model by an average of 21.5% relatively. We hope this novel and challenging task will pave the way for future advancements in robust and general motion understanding via video reasoning segmentation"
      },
      {
        "id": "oai:arXiv.org:2411.12593v3",
        "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
        "link": "https://arxiv.org/abs/2411.12593",
        "author": "Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao Yin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.12593v3 Announce Type: replace \nAbstract: The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM$^2$, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%."
      },
      {
        "id": "oai:arXiv.org:2411.16198v4",
        "title": "Interpreting Object-level Foundation Models via Visual Precision Search",
        "link": "https://arxiv.org/abs/2411.16198",
        "author": "Ruoyu Chen, Siyuan Liang, Jingzhi Li, Shiming Liu, Maosen Li, Zhen Huang, Hua Zhang, Xiaochun Cao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16198v4 Announce Type: replace \nAbstract: Advances in multimodal pre-training have propelled object-level foundation models, such as Grounding DINO and Florence-2, in tasks like visual grounding and object detection. However, interpreting these models' decisions has grown increasingly challenging. Existing interpretable attribution methods for object-level task interpretation have notable limitations: (1) gradient-based methods lack precise localization due to visual-textual fusion in foundation models, and (2) perturbation-based methods produce noisy saliency maps, limiting fine-grained interpretability. To address these, we propose a Visual Precision Search method that generates accurate attribution maps with fewer regions. Our method bypasses internal model parameters to overcome attribution issues from multimodal fusion, dividing inputs into sparse sub-regions and using consistency and collaboration scores to accurately identify critical decision-making regions. We also conducted a theoretical analysis of the boundary guarantees and scope of applicability of our method. Experiments on RefCOCO, MS COCO, and LVIS show our approach enhances object-level task interpretability over SOTA for Grounding DINO and Florence-2 across various evaluation metrics, with faithfulness gains of 23.7%, 31.6%, and 20.1% on MS COCO, LVIS, and RefCOCO for Grounding DINO, and 102.9% and 66.9% on MS COCO and RefCOCO for Florence-2. Additionally, our method can interpret failures in visual grounding and object detection tasks, surpassing existing methods across multiple evaluation metrics. The code will be released at https://github.com/RuoyuChen10/VPS."
      },
      {
        "id": "oai:arXiv.org:2411.17735v5",
        "title": "3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning",
        "link": "https://arxiv.org/abs/2411.17735",
        "author": "Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, Chuang Gan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.17735v5 Announce Type: replace \nAbstract: Constructing compact and informative 3D scene representations is essential for effective embodied exploration and reasoning, especially in complex environments over extended periods. Existing representations, such as object-centric 3D scene graphs, oversimplify spatial relationships by modeling scenes as isolated objects with restrictive textual relationships, making it difficult to address queries requiring nuanced spatial understanding. Moreover, these representations lack natural mechanisms for active exploration and memory management, hindering their application to lifelong autonomy. In this work, we propose 3D-Mem, a novel 3D scene memory framework for embodied agents. 3D-Mem employs informative multi-view images, termed Memory Snapshots, to represent the scene and capture rich visual information of explored regions. It further integrates frontier-based exploration by introducing Frontier Snapshots-glimpses of unexplored areas-enabling agents to make informed decisions by considering both known and potential new information. To support lifelong memory in active exploration settings, we present an incremental construction pipeline for 3D-Mem, as well as a memory retrieval technique for memory management. Experimental results on three benchmarks demonstrate that 3D-Mem significantly enhances agents' exploration and reasoning capabilities in 3D environments, highlighting its potential for advancing applications in embodied AI."
      },
      {
        "id": "oai:arXiv.org:2411.18145v2",
        "title": "CHOICE: Benchmarking the Remote Sensing Capabilities of Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2411.18145",
        "author": "Xiao An, Jiaxing Sun, Zihan Gui, Wei He",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.18145v2 Announce Type: replace \nAbstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive benchmark designed to objectively evaluate the hierarchical remote sensing capabilities of VLMs. Focusing on 2 primary capability dimensions essential to remote sensing: perception and reasoning, we further categorize 6 secondary dimensions and 23 leaf tasks to ensure a well-rounded assessment coverage. CHOICE guarantees the quality of all 10,507 problems through a rigorous process of data collection from 50 globally distributed cities, question construction and quality control. The newly curated data and the format of multiple-choice questions with definitive answers allow for an objective and straightforward performance assessment. Our evaluation of 3 proprietary and 21 open-source VLMs highlights their critical limitations within this specialized context. We hope that CHOICE will serve as a valuable resource and offer deeper insights into the challenges and potential of VLMs in the field of remote sensing. We will release CHOICE at https://github.com/ShawnAn-WHU/CHOICE."
      },
      {
        "id": "oai:arXiv.org:2412.00081v3",
        "title": "Task Singular Vectors: Reducing Task Interference in Model Merging",
        "link": "https://arxiv.org/abs/2412.00081",
        "author": "Antonio Andrea Gargiulo, Donato Crisostomi, Maria Sofia Bucarelli, Simone Scardapane, Fabrizio Silvestri, Emanuele Rodol\\`a",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.00081v3 Announce Type: replace \nAbstract: Task Arithmetic has emerged as a simple yet effective method to merge models without additional training. However, by treating entire networks as flat parameter vectors, it overlooks key structural information and is susceptible to task interference. In this paper, we study task vectors at the layer level, focusing on task layer matrices and their singular value decomposition. In particular, we concentrate on the resulting singular vectors, which we refer to as Task Singular Vectors (TSV). Recognizing that layer task matrices are often low-rank, we propose TSV-Compress (TSV-C), a simple procedure that compresses them to 10% of their original size while retaining 99% of accuracy. We further leverage this low-rank space to define a new measure of task interference based on the interaction of singular vectors from different tasks. Building on these findings, we introduce TSV-Merge (TSV-M), a novel model merging approach that combines compression with interference reduction, significantly outperforming existing methods."
      },
      {
        "id": "oai:arXiv.org:2412.04377v3",
        "title": "A Hitchhiker's Guide to Understanding Performances of Two-Class Classifiers",
        "link": "https://arxiv.org/abs/2412.04377",
        "author": "Ana\\\"is Halin, S\\'ebastien Pi\\'erard, Anthony Cioppa, Marc Van Droogenbroeck",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.04377v3 Announce Type: replace \nAbstract: Properly understanding the performances of classifiers is essential in various scenarios. However, the literature often relies only on one or two standard scores to compare classifiers, which fails to capture the nuances of application-specific requirements. The Tile is a recently introduced visualization tool organizing an infinity of ranking scores into a 2D map. Thanks to the Tile, it is now possible to compare classifiers efficiently, displaying all possible application-specific preferences instead of having to rely on a pair of scores. This hitchhiker's guide to understanding the performances of two-class classifiers presents four scenarios showcasing different user profiles: a theoretical analyst, a method designer, a benchmarker, and an application developer. We introduce several interpretative flavors adapted to the user's needs by mapping different values on the Tile. We illustrate this guide by ranking and analyzing the performances of 74 state-of-the-art semantic segmentation models through the perspective of the four scenarios. Through these user profiles, we demonstrate that the Tile effectively captures the behavior of classifiers in a single visualization, while accommodating an infinite number of ranking scores. Code for mapping the different Tile flavors is available in supplementary material."
      },
      {
        "id": "oai:arXiv.org:2412.06613v2",
        "title": "3D Spatial Understanding in MLLMs: Disambiguation and Evaluation",
        "link": "https://arxiv.org/abs/2412.06613",
        "author": "Chun-Peng Chang, Alain Pagani, Didier Stricker",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.06613v2 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) have made significant progress in tasks such as image captioning and question answering. However, while these models can generate realistic captions, they often struggle with providing precise instructions, particularly when it comes to localizing and disambiguating objects in complex 3D environments. This capability is critical as MLLMs become more integrated with collaborative robotic systems. In scenarios where a target object is surrounded by similar objects (distractors), robots must deliver clear, spatially-aware instructions to guide humans effectively. We refer to this challenge as contextual object localization and disambiguation, which imposes stricter constraints than conventional 3D dense captioning, especially regarding ensuring target exclusivity. In response, we propose simple yet effective techniques to enhance the model's ability to localize and disambiguate target objects. Our approach not only achieves state-of-the-art performance on conventional metrics that evaluate sentence similarity, but also demonstrates improved 3D spatial understanding through 3D visual grounding model."
      },
      {
        "id": "oai:arXiv.org:2412.06786v3",
        "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis",
        "link": "https://arxiv.org/abs/2412.06786",
        "author": "M. Hamza Mughal, Rishabh Dabral, Merel C. J. Scholman, Vera Demberg, Christian Theobalt",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.06786v3 Announce Type: replace \nAbstract: Non-verbal communication often comprises of semantically rich gestures that help convey the meaning of an utterance. Producing such semantic co-speech gestures has been a major challenge for the existing neural systems that can generate rhythmic beat gestures, but struggle to produce semantically meaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based gesture generation approach that leverages Retrieval Augmented Generation (RAG) to produce natural-looking and semantically rich gestures. Our neuro-explicit gesture generation approach is designed to produce semantic gestures grounded in interpretable linguistic knowledge. We achieve this by using explicit domain knowledge to retrieve exemplar motions from a database of co-speech gestures. Once retrieved, we then inject these semantic exemplar gestures into our diffusion-based gesture generation pipeline using DDIM inversion and retrieval guidance at the inference time without any need of training. Further, we propose a control paradigm for guidance, that allows the users to modulate the amount of influence each retrieval insertion has over the generated sequence. Our comparative evaluations demonstrate the validity of our approach against recent gesture generation approaches. The reader is urged to explore the results on our project page."
      },
      {
        "id": "oai:arXiv.org:2412.06978v2",
        "title": "Edge-SD-SR: Low Latency and Parameter Efficient On-device Super-Resolution with Stable Diffusion via Bidirectional Conditioning",
        "link": "https://arxiv.org/abs/2412.06978",
        "author": "Mehdi Noroozi, Isma Hadji, Victor Escorcia, Anestis Zaganidis, Brais Martinez, Georgios Tzimiropoulos",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.06978v2 Announce Type: replace \nAbstract: There has been immense progress recently in the visual quality of Stable Diffusion-based Super Resolution (SD-SR). However, deploying large diffusion models on computationally restricted devices such as mobile phones remains impractical due to the large model size and high latency. This is compounded for SR as it often operates at high res (e.g. 4Kx3K). In this work, we introduce Edge-SD-SR, the first parameter efficient and low latency diffusion model for image super-resolution. Edge-SD-SR consists of ~169M parameters, including UNet, encoder and decoder, and has a complexity of only ~142 GFLOPs. To maintain a high visual quality on such low compute budget, we introduce a number of training strategies: (i) A novel conditioning mechanism on the low resolution input, coined bidirectional conditioning, which tailors the SD model for the SR task. (ii) Joint training of the UNet and encoder, while decoupling the encodings of the HR and LR images and using a dedicated schedule. (iii) Finetuning the decoder using the UNet's output to directly tailor the decoder to the latents obtained at inference time. Edge-SD-SR runs efficiently on device, e.g. it can upscale a 128x128 patch to 512x512 in 38 msec while running on a Samsung S24 DSP, and of a 512x512 to 2048x2048 (requiring 25 model evaluations) in just ~1.1 sec. Furthermore, we show that Edge-SD-SR matches or even outperforms state-of-the-art SR approaches on the most established SR benchmarks."
      },
      {
        "id": "oai:arXiv.org:2412.07030v4",
        "title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering",
        "link": "https://arxiv.org/abs/2412.07030",
        "author": "Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07030v4 Announce Type: replace \nAbstract: Multimodal multihop question answering (MMQA) requires reasoning over images and text from multiple sources. Despite advances in visual question answering, this multihop setting remains underexplored due to a lack of quality datasets. Existing methods focus on single-hop, single-modality, or short texts, limiting real-world applications like interpreting educational documents with long, multimodal content. To fill this gap, we introduce FM2DS, the first framework for creating a high-quality dataset for MMQA. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure data quality. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks: MultimodalQA and WebQA. Our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) score on average. Additionally, we introduce M2QA-Bench with 1k samples, the first benchmark for MMQA on long documents, generated using FM2DS and refined by human annotators. We believe our data synthesis method will serve as a strong foundation for training and evaluating MMQA models."
      },
      {
        "id": "oai:arXiv.org:2412.07494v2",
        "title": "ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery",
        "link": "https://arxiv.org/abs/2412.07494",
        "author": "Yanzhe Lyu, Kai Cheng, Xin Kang, Xuejin Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07494v2 Announce Type: replace \nAbstract: Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view synthesis, achieving high fidelity and efficiency. However, it often struggles to capture rich details and complete geometry. Our analysis reveals that the 3D-GS densification operation lacks adaptiveness and faces a dilemma between geometry coverage and detail recovery. To address this, we introduce a novel densification operation, residual split, which adds a downscaled Gaussian as a residual. Our approach is capable of adaptively retrieving details and complementing missing geometry. To further support this method, we propose a pipeline named ResGS. Specifically, we integrate a Gaussian image pyramid for progressive supervision and implement a selection scheme that prioritizes the densification of coarse Gaussians over time. Extensive experiments demonstrate that our method achieves SOTA rendering quality. Consistent performance improvements can be achieved by applying our residual split on various 3D-GS variants, underscoring its versatility and potential for broader application in 3D-GS-based applications."
      },
      {
        "id": "oai:arXiv.org:2412.10153v3",
        "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
        "link": "https://arxiv.org/abs/2412.10153",
        "author": "Weixiang Zhang, Shuzhao Xie, Chengwei Ren, Siyi Xie, Chen Tang, Shijia Ge, Mingzi Wang, Zhi Wang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10153v3 Announce Type: replace \nAbstract: We propose EVOlutionary Selector (EVOS), an efficient training paradigm for accelerating Implicit Neural Representation (INR). Unlike conventional INR training that feeds all samples through the neural network in each iteration, our approach restricts training to strategically selected points, reducing computational overhead by eliminating redundant forward passes. Specifically, we treat each sample as an individual in an evolutionary process, where only those fittest ones survive and merit inclusion in training, adaptively evolving with the neural network dynamics. While this is conceptually similar to Evolutionary Algorithms, their distinct objectives (selection for acceleration vs. iterative solution optimization) require a fundamental redefinition of evolutionary mechanisms for our context. In response, we design sparse fitness evaluation, frequency-guided crossover, and augmented unbiased mutation to comprise EVOS. These components respectively guide sample selection with reduced computational cost, enhance performance through frequency-domain balance, and mitigate selection bias from cached evaluation. Extensive experiments demonstrate that our method achieves approximately 48%-66% reduction in training time while ensuring superior convergence without additional cost, establishing state-of-the-art acceleration among recent sampling-based strategies."
      },
      {
        "id": "oai:arXiv.org:2412.11785v3",
        "title": "InterDyn: Controllable Interactive Dynamics with Video Diffusion Models",
        "link": "https://arxiv.org/abs/2412.11785",
        "author": "Rick Akkerman, Haiwen Feng, Michael J. Black, Dimitrios Tzionas, Victoria Fern\\'andez Abrevaya",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11785v3 Announce Type: replace \nAbstract: Predicting the dynamics of interacting objects is essential for both humans and intelligent systems. However, existing approaches are limited to simplified, toy settings and lack generalizability to complex, real-world environments. Recent advances in generative models have enabled the prediction of state transitions based on interventions, but focus on generating a single future state which neglects the continuous dynamics resulting from the interaction. To address this gap, we propose InterDyn, a novel framework that generates videos of interactive dynamics given an initial frame and a control signal encoding the motion of a driving object or actor. Our key insight is that large video generation models can act as both neural renderers and implicit physics ``simulators'', having learned interactive dynamics from large-scale video data. To effectively harness this capability, we introduce an interactive control mechanism that conditions the video generation process on the motion of the driving entity. Qualitative results demonstrate that InterDyn generates plausible, temporally consistent videos of complex object interactions while generalizing to unseen objects. Quantitative evaluations show that InterDyn outperforms baselines that focus on static state transitions. This work highlights the potential of leveraging video generative models as implicit physics engines. Project page: https://interdyn.is.tue.mpg.de/"
      },
      {
        "id": "oai:arXiv.org:2412.12480v4",
        "title": "Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?",
        "link": "https://arxiv.org/abs/2412.12480",
        "author": "Alex Mallen, Charlie Griffin, Misha Wagner, Alessandro Abate, Buck Shlegeris",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.12480v4 Announce Type: replace \nAbstract: An AI control protocol is a plan for usefully deploying AI systems that aims to prevent an AI from intentionally causing some unacceptable outcome. This paper investigates how well AI systems can generate and act on their own strategies for subverting control protocols whilst operating statelessly (without shared memory between contexts). To do this, an AI system may need to reliably generate optimal plans in each context, take actions with well-calibrated probabilities, and coordinate plans with other instances of itself without communicating. We develop Subversion Strategy Eval, a suite of eight environments, covering a range of protocols and strategic capabilities, and six sets of affordances that help isolate individual capabilities. We implement the evaluation in Inspect-AI and release it open-source. We evaluate Claude 3.5 models, including helpful-only versions, as well as OpenAI reasoning models. None of the models demonstrate substantial capability in strategizing to subvert control protocols statelessly. However, providing models with additional affordances, such as the ability to share a plan between contexts, can substantially improve performance. We hope our evaluations can act as a leading indicator for when models are capable of subverting control protocols and also relax the worst-case assumption of perfect strategic ability in AI control evaluations."
      },
      {
        "id": "oai:arXiv.org:2412.12997v3",
        "title": "Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO",
        "link": "https://arxiv.org/abs/2412.12997",
        "author": "Umer Butt, Stalin Varanasi, G\\\"unter Neumann",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.12997v3 Announce Type: replace \nAbstract: As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. This paper introduces the first large-scale Urdu IR dataset, created by translating the MS MARCO dataset through machine translation. We establish baseline results through zero-shot learning for IR in Urdu and subsequently apply the mMARCO multilingual IR methodology to this newly translated dataset. Our findings demonstrate that the fine-tuned model (Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a Recall@10 of 0.439, representing significant improvements over zero-shot results and showing the potential for expanding IR access for Urdu speakers. By bridging access gaps for speakers of low-resource languages, this work not only advances multilingual IR research but also emphasizes the ethical and societal importance of inclusive IR technologies. This work provides valuable insights into the challenges and solutions for improving language representation and lays the groundwork for future research, especially in South Asian languages, which can benefit from the adaptable methods used in this study."
      },
      {
        "id": "oai:arXiv.org:2412.15291v3",
        "title": "A Large-Scale Simulation on Large Language Models for Decision-Making in Political Science",
        "link": "https://arxiv.org/abs/2412.15291",
        "author": "Chenxiao Yu, Jinyi Ye, Yuangang Li, Zhaotian Weng, Zheng Li, Emilio Ferrara, Xiyang Hu, Yue Zhao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15291v3 Announce Type: replace \nAbstract: While LLMs have demonstrated remarkable capabilities in text generation and reasoning, their ability to simulate human decision-making -- particularly in political contexts -- remains an open question. However, modeling voter behavior presents unique challenges due to limited voter-level data, evolving political landscapes, and the complexity of human reasoning. In this study, we develop a theory-driven, multi-step reasoning framework that integrates demographic, temporal and ideological factors to simulate voter decision-making at scale. Using synthetic personas calibrated to real-world voter data, we conduct large-scale simulations of recent U.S. presidential elections. Our method significantly improves simulation accuracy while mitigating model biases. We examine its robustness by comparing performance across different LLMs. We further investigate the challenges and constraints that arise from LLM-based political simulations. Our work provides both a scalable framework for modeling political decision-making behavior and insights into the promise and limitations of using LLMs in political science research."
      },
      {
        "id": "oai:arXiv.org:2412.15396v2",
        "title": "Learning Visual Composition through Improved Semantic Guidance",
        "link": "https://arxiv.org/abs/2412.15396",
        "author": "Austin Stone, Hagen Soltau, Robert Geirhos, Xi Yi, Ye Xia, Bingyi Cao, Kaifeng Chen, Abhijit Ogale, Jonathon Shlens",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15396v2 Announce Type: replace \nAbstract: Visual imagery does not consist of solitary objects, but instead reflects the composition of a multitude of fluid concepts. While there have been great advances in visual representation learning, such advances have focused on building better representations for a small number of discrete objects bereft of an understanding of how these objects are interacting. One can observe this limitation in representations learned through captions or contrastive learning -- where the learned model treats an image essentially as a bag of words. Several works have attempted to address this limitation through the development of bespoke learned architectures to directly address the shortcomings in compositional learning. In this work, we focus on simple, and scalable approaches. In particular, we demonstrate that by substantially improving weakly labeled data, i.e. captions, we can vastly improve the performance of standard contrastive learning approaches. Previous CLIP models achieved near chance rate on challenging tasks probing compositional learning. However, our simple approach boosts performance of CLIP substantially and surpasses all bespoke architectures. Furthermore, we showcase our results on a relatively new captioning benchmark derived from DOCCI. We demonstrate through a series of ablations that a standard CLIP model trained with enhanced data may demonstrate impressive performance on image retrieval tasks."
      },
      {
        "id": "oai:arXiv.org:2412.15678v2",
        "title": "Multi-Pair Temporal Sentence Grounding via Multi-Thread Knowledge Transfer Network",
        "link": "https://arxiv.org/abs/2412.15678",
        "author": "Xiang Fang, Wanlong Fang, Changshuo Wang, Daizong Liu, Keke Tang, Jianfeng Dong, Pan Zhou, Beibei Li",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15678v2 Announce Type: replace \nAbstract: Given some video-query pairs with untrimmed videos and sentence queries, temporal sentence grounding (TSG) aims to locate query-relevant segments in these videos. Although previous respectable TSG methods have achieved remarkable success, they train each video-query pair separately and ignore the relationship between different pairs. We observe that the similar video/query content not only helps the TSG model better understand and generalize the cross-modal representation but also assists the model in locating some complex video-query pairs. Previous methods follow a single-thread framework that cannot co-train different pairs and usually spends much time re-obtaining redundant knowledge, limiting their real-world applications. To this end, in this paper, we pose a brand-new setting: Multi-Pair TSG, which aims to co-train these pairs. In particular, we propose a novel video-query co-training approach, Multi-Thread Knowledge Transfer Network, to locate a variety of video-query pairs effectively and efficiently. Firstly, we mine the spatial and temporal semantics across different queries to cooperate with each other. To learn intra- and inter-modal representations simultaneously, we design a cross-modal contrast module to explore the semantic consistency by a self-supervised strategy. To fully align visual and textual representations between different pairs, we design a prototype alignment strategy to 1) match object prototypes and phrase prototypes for spatial alignment, and 2) align activity prototypes and sentence prototypes for temporal alignment. Finally, we develop an adaptive negative selection module to adaptively generate a threshold for cross-modal matching. Extensive experiments show the effectiveness and efficiency of our proposed method."
      },
      {
        "id": "oai:arXiv.org:2412.16915v2",
        "title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation",
        "link": "https://arxiv.org/abs/2412.16915",
        "author": "Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.16915v2 Announce Type: replace \nAbstract: Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage http://fadavatar.github.io."
      },
      {
        "id": "oai:arXiv.org:2412.19331v2",
        "title": "CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2412.19331",
        "author": "Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.19331v2 Announce Type: replace \nAbstract: Recent advances in Large Vision-Language Models (LVLMs) have enabled general-purpose vision tasks through visual instruction tuning. While existing LVLMs can generate segmentation masks from text prompts for single images, they struggle with segmentation-grounded reasoning across images, especially at finer granularities such as object parts. In this paper, we introduce the new task of part-focused semantic co-segmentation, which involves identifying and segmenting common objects, as well as common and unique object parts across images. To address this task, we present CALICO, the first LVLM designed for multi-image part-level reasoning segmentation. CALICO features two key components, a novel Correspondence Extraction Module that identifies semantic part-level correspondences, and Correspondence Adaptation Modules that embed this information into the LVLM to facilitate multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MixedParts, a large-scale multi-image segmentation dataset containing $\\sim$2.4M samples across $\\sim$44K images spanning diverse object and part categories. Experimental results demonstrate that CALICO, with just 0.3% of its parameters finetuned, achieves strong performance on this challenging task."
      },
      {
        "id": "oai:arXiv.org:2412.20596v2",
        "title": "Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond)",
        "link": "https://arxiv.org/abs/2412.20596",
        "author": "Tomer Garber, Tom Tirer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.20596v2 Announce Type: replace \nAbstract: In recent years, it has become popular to tackle image restoration tasks with a single pretrained diffusion model (DM) and data-fidelity guidance, instead of training a dedicated deep neural network per task. However, such \"zero-shot\" restoration schemes currently require many Neural Function Evaluations (NFEs) for performing well, which may be attributed to the many NFEs needed in the original generative functionality of the DMs. Recently, faster variants of DMs have been explored for image generation. These include Consistency Models (CMs), which can generate samples via a couple of NFEs. However, existing works that use guided CMs for restoration still require tens of NFEs or fine-tuning of the model per task that leads to performance drop if the assumptions during the fine-tuning are not accurate. In this paper, we propose a zero-shot restoration scheme that uses CMs and operates well with as little as 4 NFEs. It is based on a wise combination of several ingredients: better initialization, back-projection guidance, and above all a novel noise injection mechanism. We demonstrate the advantages of our approach for image super-resolution, deblurring and inpainting. Interestingly, we show that the usefulness of our noise injection technique goes beyond CMs: it can also mitigate the performance degradation of existing guided DM methods when reducing their NFE count."
      },
      {
        "id": "oai:arXiv.org:2501.03544v2",
        "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models",
        "link": "https://arxiv.org/abs/2501.03544",
        "author": "Lingzhi Yuan, Xiaojun Jia, Yihao Huang, Wei Dong, Yang Liu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.03544v2 Announce Type: replace \nAbstract: Text-to-image (T2I) models have been shown to be vulnerable to misuse, particularly in generating not-safe-for-work (NSFW) content, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. Extensive experiments across three datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 7.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%."
      },
      {
        "id": "oai:arXiv.org:2501.08083v3",
        "title": "Benchmarking Vision Foundation Models for Input Monitoring in Autonomous Driving",
        "link": "https://arxiv.org/abs/2501.08083",
        "author": "Mert Keser, Halil Ibrahim Orhan, Niki Amini-Naieni, Gesina Schwalbe, Alois Knoll, Matthias Rottmann",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.08083v3 Announce Type: replace \nAbstract: Deep neural networks (DNNs) remain challenged by distribution shifts in complex open-world domains like automated driving (AD): Robustness against yet unknown novel objects (semantic shift) or styles like lighting conditions (covariate shift) cannot be guaranteed. Hence, reliable operation-time monitors for identification of out-of-training-data-distribution (OOD) scenarios are imperative. Current approaches for OOD classification are untested for complex domains like AD, are limited in the kinds of shifts they detect, or even require supervision with OOD samples. To prepare for unanticipated shifts, we instead establish a framework around a principled, unsupervised and model-agnostic method that unifies detection of semantic and covariate shifts: Find a full model of the training data's feature distribution, to then use its density at new points as in-distribution (ID) score. To implement this, we propose to combine Vision Foundation Models (VFMs) as feature extractors with density modeling techniques. Through a comprehensive benchmark of 4 VFMs with different backbone architectures and 5 density-modeling techniques against established baselines, we provide the first systematic evaluation of OOD classification capabilities of VFMs across diverse conditions. A comparison with state-of-the-art binary OOD classification methods reveals that VFM embeddings with density estimation outperform existing approaches in identifying OOD inputs. Additionally, we show that our method detects high-risk inputs likely to cause errors in downstream tasks, thereby improving overall performance. Overall, VFMs, when coupled with robust density modeling techniques, are promising to realize model-agnostic, unsupervised, reliable safety monitors in complex vision tasks"
      },
      {
        "id": "oai:arXiv.org:2501.09898v4",
        "title": "FoundationStereo: Zero-Shot Stereo Matching",
        "link": "https://arxiv.org/abs/2501.09898",
        "author": "Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.09898v4 Announce Type: replace \nAbstract: Tremendous progress has been made in deep stereo matching to excel on benchmark datasets through per-domain fine-tuning. However, achieving strong zero-shot generalization - a hallmark of foundation models in other computer vision tasks - remains challenging for stereo matching. We introduce FoundationStereo, a foundation model for stereo depth estimation designed to achieve strong zero-shot generalization. To this end, we first construct a large-scale (1M stereo pairs) synthetic training dataset featuring large diversity and high photorealism, followed by an automatic self-curation pipeline to remove ambiguous samples. We then design a number of network architecture components to enhance scalability, including a side-tuning feature backbone that adapts rich monocular priors from vision foundation models to mitigate the sim-to-real gap, and long-range context reasoning for effective cost volume filtering. Together, these components lead to strong robustness and accuracy across domains, establishing a new standard in zero-shot stereo depth estimation. Project page: https://nvlabs.github.io/FoundationStereo/"
      },
      {
        "id": "oai:arXiv.org:2501.10598v2",
        "title": "Solving Finite-Horizon MDPs via Low-Rank Tensors",
        "link": "https://arxiv.org/abs/2501.10598",
        "author": "Sergio Rozada, Jose Luis Orejuela, Antonio G. Marques",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.10598v2 Announce Type: replace \nAbstract: We study the problem of learning optimal policies in finite-horizon Markov Decision Processes (MDPs) using low-rank reinforcement learning (RL) methods. In finite-horizon MDPs, the policies, and therefore the value functions (VFs) are not stationary. This aggravates the challenges of high-dimensional MDPs, as they suffer from the curse of dimensionality and high sample complexity. To address these issues, we propose modeling the VFs of finite-horizon MDPs as low-rank tensors, enabling a scalable representation that renders the problem of learning optimal policies tractable. We introduce an optimization-based framework for solving the Bellman equations with low-rank constraints, along with block-coordinate descent (BCD) and block-coordinate gradient descent (BCGD) algorithms, both with theoretical convergence guarantees. For scenarios where the system dynamics are unknown, we adapt the proposed BCGD method to estimate the VFs using sampled trajectories. Numerical experiments further demonstrate that the proposed framework reduces computational demands in controlled synthetic scenarios and more realistic resource allocation problems."
      },
      {
        "id": "oai:arXiv.org:2501.14524v2",
        "title": "Training-Free Style and Content Transfer by Leveraging U-Net Skip Connections in Stable Diffusion",
        "link": "https://arxiv.org/abs/2501.14524",
        "author": "Ludovica Schaerf, Andrea Alfarano, Fabrizio Silvestri, Leonardo Impett",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.14524v2 Announce Type: replace \nAbstract: Recent advances in diffusion models for image generation have led to detailed examinations of several components within the U-Net architecture for image editing. While previous studies have focused on the bottleneck layer (h-space), cross-attention, self-attention, and decoding layers, the overall role of the skip connections of the U-Net itself has not been specifically addressed. We conduct thorough analyses on the role of the skip connections and find that the residual connections passed by the third encoder block carry most of the spatial information of the reconstructed image, splitting the content from the style, passed by the remaining stream in the opposed decoding layer. We show that injecting the representations from this block can be used for text-based editing, precise modifications, and style transfer. We compare our method, SkipInject, to state-of-the-art style transfer and image editing methods and demonstrate that our method obtains the best content alignment and optimal structural preservation tradeoff."
      },
      {
        "id": "oai:arXiv.org:2502.03771v2",
        "title": "Adaptive Semantic Prompt Caching with VectorQ",
        "link": "https://arxiv.org/abs/2502.03771",
        "author": "Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03771v2 Announce Type: replace \nAbstract: Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different embeddings. We propose VectorQ, an online framework with a threshold convergence guarantee to learn embedding-specific threshold regions that adapt to the uncertainty of an embedding. Through evaluations on a combination of three diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 26x increases in cache hit rate and error rate reductions up to 74%."
      },
      {
        "id": "oai:arXiv.org:2502.09137v2",
        "title": "Trust Me, I Know the Way: Predictive Uncertainty in the Presence of Shortcut Learning",
        "link": "https://arxiv.org/abs/2502.09137",
        "author": "Lisa Wimmer, Bernd Bischl, Ludwig Bothmann",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09137v2 Announce Type: replace \nAbstract: The correct way to quantify predictive uncertainty in neural networks remains a topic of active discussion. In particular, it is unclear whether the state-of-the art entropy decomposition leads to a meaningful representation of model, or epistemic, uncertainty (EU) in the light of a debate that pits ignorance against disagreement perspectives. We aim to reconcile the conflicting viewpoints by arguing that both are valid but arise from different learning situations. Notably, we show that the presence of shortcuts is decisive for EU manifesting as disagreement."
      },
      {
        "id": "oai:arXiv.org:2502.09563v2",
        "title": "Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction",
        "link": "https://arxiv.org/abs/2502.09563",
        "author": "Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09563v2 Announce Type: replace \nAbstract: In this paper, we present a self-calibrating framework that jointly optimizes camera parameters, lens distortion and 3D Gaussian representations, enabling accurate and efficient scene reconstruction. In particular, our technique enables high-quality scene reconstruction from Large field-of-view (FOV) imagery taken with wide-angle lenses, allowing the scene to be modeled from a smaller number of images. Our approach introduces a novel method for modeling complex lens distortions using a hybrid network that combines invertible residual networks with explicit grids. This design effectively regularizes the optimization process, achieving greater accuracy than conventional camera models. Additionally, we propose a cubemap-based resampling strategy to support large FOV images without sacrificing resolution or introducing distortion artifacts. Our method is compatible with the fast rasterization of Gaussian Splatting, adaptable to a wide variety of camera lens distortion, and demonstrates state-of-the-art performance on both synthetic and real-world datasets."
      },
      {
        "id": "oai:arXiv.org:2502.11071v5",
        "title": "Generalization of the Gibbs algorithm with high probability at low temperatures",
        "link": "https://arxiv.org/abs/2502.11071",
        "author": "Andreas Maurer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11071v5 Announce Type: replace \nAbstract: The paper gives a bound on the generalization error of the Gibbs algorithm, which recovers known data-independent bounds for the high temperature range and extends to the low-temperature range, where generalization depends critically on the data-dependent loss-landscape. It is shown, that with high probability the generalization error of a single hypothesis drawn from the Gibbs posterior decreases with the total prior volume of all hypotheses with similar or smaller empirical error. This gives theoretical support to the belief in the benefit of flat minima. The zero temperature limit is discussed and the bound is extended to a class of similar stochastic algorithms."
      },
      {
        "id": "oai:arXiv.org:2502.14671v3",
        "title": "Explanations of Large Language Models Explain Language Representations in the Brain",
        "link": "https://arxiv.org/abs/2502.14671",
        "author": "Maryam Rahimi, Yadollah Yaghoobzadeh, Mohammad Reza Daliri",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14671v3 Announce Type: replace \nAbstract: Large language models (LLMs) not only exhibit human-like performance but also share computational principles with the brain's language processing mechanisms. While prior research has focused on mapping LLMs' internal representations to neural activity, we propose a novel approach using explainable AI (XAI) to strengthen this link. Applying attribution methods, we quantify the influence of preceding words on LLMs' next-word predictions and use these explanations to predict fMRI data from participants listening to narratives. We find that attribution methods robustly predict brain activity across the language network, revealing a hierarchical pattern: explanations from early layers align with the brain's initial language processing stages, while later layers correspond to more advanced stages. Additionally, layers with greater influence on next-word prediction$\\unicode{x2014}$reflected in higher attribution scores$\\unicode{x2014}$demonstrate stronger brain alignment. These results underscore XAI's potential for exploring the neural basis of language and suggest brain alignment for assessing the biological plausibility of explanation methods."
      },
      {
        "id": "oai:arXiv.org:2502.15004v2",
        "title": "Digital implementations of deep feature extractors are intrinsically informative",
        "link": "https://arxiv.org/abs/2502.15004",
        "author": "Max Getter",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15004v2 Announce Type: replace \nAbstract: Rapid information (energy) propagation in deep feature extractors is crucial to balance computational complexity versus expressiveness as a representation of the input. We prove an upper bound for the speed of energy propagation in a unified framework that covers different neural network models, both over Euclidean and non-Euclidean domains. Additional structural information about the signal domain can be used to explicitly determine or improve the rate of decay. To illustrate this, we show global exponential energy decay for a range of 1) feature extractors with discrete-domain input signals, and 2) convolutional neural networks (CNNs) via scattering over locally compact abelian (LCA) groups."
      },
      {
        "id": "oai:arXiv.org:2502.15429v3",
        "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations",
        "link": "https://arxiv.org/abs/2502.15429",
        "author": "Lihu Chen, Shuojie Fu, Gabriel Freedman, Cemre Zor, Guy Martin, James Kinross, Uddhav Vaghela, Ovidiu Serban, Francesca Toni",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15429v3 Announce Type: replace \nAbstract: A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool."
      },
      {
        "id": "oai:arXiv.org:2502.17796v2",
        "title": "LAM: Large Avatar Model for One-shot Animatable Gaussian Head",
        "link": "https://arxiv.org/abs/2502.17796",
        "author": "Yisheng He, Xiaodong Gu, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17796v2 Announce Type: replace \nAbstract: We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. Our experimental results demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks. Our code and video are available at https://aigc3d.github.io/projects/LAM/"
      },
      {
        "id": "oai:arXiv.org:2502.18661v3",
        "title": "TikTok StitchGraph: Characterizing communication patterns on TikTok through a collection of interaction networks",
        "link": "https://arxiv.org/abs/2502.18661",
        "author": "Mads H{\\o}genhaug, Marcus Friis, Morten Pedersen, Luca Rossi",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.18661v3 Announce Type: replace \nAbstract: We present TikTok StitchGraph: a collection of 36 graphs based on TikTok stitches. With its rapid growth and widespread popularity, TikTok presents a compelling platform for study, yet given its video-first nature the network structure of the conversations that it hosts remains largely unexplored. Leveraging its recently released APIs, in combination with web scraping, we construct graphs detailing stitch relations from both a video- and user-centric perspective. Specifically, we focus on user multi-digraphs, with vertices representing users and edges representing directed stitch relations. From the user graphs, we characterize common communication patterns of the stitch using frequent subgraph mining, finding a preference for stars and star-like structures, an aversion towards cyclic structures, and directional disposition favoring in- and out-stars over mixed-direction structures. These structures are augmented with sentiment labels in the form of edge attributes. We then use these subgraphs for graph-level embeddings together with Graph2Vec, we show no clear distinction between topologies for different hashtag topic categories. Lastly, we compare our StitchGraphs to Twitter reply networks and show that a remakable similarity between the conversation networks on the two platforms."
      },
      {
        "id": "oai:arXiv.org:2502.19781v2",
        "title": "RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings",
        "link": "https://arxiv.org/abs/2502.19781",
        "author": "Aayush Dhakal, Srikumar Sastry, Subash Khanal, Adeel Ahmad, Eric Xing, Nathan Jacobs",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.19781v2 Announce Type: replace \nAbstract: The choice of representation for geographic location significantly impacts the accuracy of models for a broad range of geospatial tasks, including fine-grained species classification, population density estimation, and biome classification. Recent works like SatCLIP and GeoCLIP learn such representations by contrastively aligning geolocation with co-located images. While these methods work exceptionally well, in this paper, we posit that the current training strategies fail to fully capture the important visual features. We provide an information-theoretic perspective on why the resulting embeddings from these methods discard crucial visual information that is important for many downstream tasks. To solve this problem, we propose a novel retrieval-augmented strategy called RANGE. We build our method on the intuition that the visual features of a location can be estimated by combining the visual features from multiple similar-looking locations. We evaluate our method across a wide variety of tasks. Our results show that RANGE outperforms the existing state-of-the-art models with significant margins in most tasks. We show gains of up to 13.1% on classification tasks and 0.145 $R^2$ on regression tasks. All our code and models will be made available at: https://github.com/mvrl/RANGE."
      },
      {
        "id": "oai:arXiv.org:2502.20837v2",
        "title": "Tuning-Free Structured Sparse PCA via Deep Unfolding Networks",
        "link": "https://arxiv.org/abs/2502.20837",
        "author": "Long Chen, Xianchao Xiu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20837v2 Announce Type: replace \nAbstract: Sparse principal component analysis (PCA) is a well-established dimensionality reduction technique that is often used for unsupervised feature selection (UFS). However, determining the regularization parameters is rather challenging, and conventional approaches, including grid search and Bayesian optimization, not only bring great computational costs but also exhibit high sensitivity. To address these limitations, we first establish a structured sparse PCA formulation by integrating $\\ell_1$-norm and $\\ell_{2,1}$-norm to capture the local and global structures, respectively. Building upon the off-the-shelf alternating direction method of multipliers (ADMM) optimization framework, we then design an interpretable deep unfolding network that translates iterative optimization steps into trainable neural architectures. This innovation enables automatic learning of the regularization parameters, effectively bypassing the empirical tuning requirements of conventional methods. Numerical experiments on benchmark datasets validate the advantages of our proposed method over the existing state-of-the-art methods. Our code will be accessible at https://github.com/xianchaoxiu/SPCA-Net."
      },
      {
        "id": "oai:arXiv.org:2503.00808v3",
        "title": "Predictive Data Selection: The Data That Predicts Is the Data That Teaches",
        "link": "https://arxiv.org/abs/2503.00808",
        "author": "Kashun Shum, Yuzhen Huang, Hongjian Zou, Qi Ding, Yixuan Liao, Xiaoxin Chen, Qian Liu, Junxian He",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00808v3 Announce Type: replace \nAbstract: Language model pretraining involves training on extensive corpora, where data quality plays a pivotal role. In this work, we aim to directly estimate the contribution of data during pretraining and select pretraining data in an efficient manner. Specifically, we draw inspiration from recent findings showing that compression efficiency (i.e., the normalized loss) of diverse models on certain text correlates strongly with their downstream performance, when the text domain aligns with the downstream benchmarks(Huang et al., 2024). Building on this observation, we hypothesize that data on which model losses are predictive of downstream abilities also contribute effectively to learning. To leverage this insight, we introduce predictive data selection (PreSelect), a lightweight and efficient data selection method that requires training and deploying only a fastText-based scorer. Through comprehensive experiments with 1B and 3B parameter models, we demonstrate that models trained on 30B tokens selected with PreSelect surpass the performance of the vanilla baseline trained on 300B tokens, achieving a 10x reduction in compute requirements. Furthermore, PreSelect significantly outperforms other competitive data selection baselines, such as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our trained data selection scorer along with the curated datasets at https://github.com/hkust-nlp/PreSelect."
      },
      {
        "id": "oai:arXiv.org:2503.04718v2",
        "title": "Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation",
        "link": "https://arxiv.org/abs/2503.04718",
        "author": "David T. Hoffmann, Syed Haseeb Raza, Hanqiu Jiang, Denis Tananaev, Steffen Klingenhoefer, Martin Meinke",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04718v2 Announce Type: replace \nAbstract: Scene flow estimation is a foundational task for many robotic applications, including robust dynamic object detection, automatic labeling, and sensor synchronization. Two types of approaches to the problem have evolved: 1) Supervised and 2) optimization-based methods. Supervised methods are fast during inference and achieve high-quality results, however, they are limited by the need for large amounts of labeled training data and are susceptible to domain gaps. In contrast, unsupervised test-time optimization methods do not face the problem of domain gaps but usually suffer from substantial runtime, exhibit artifacts, or fail to converge to the right solution. In this work, we mitigate several limitations of existing optimization-based methods. To this end, we 1) introduce a simple voxel grid-based model that improves over the standard MLP-based formulation in multiple dimensions and 2) introduce a new multiframe loss formulation. 3) We combine both contributions in our new method, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only by EulerFlow among unsupervised methods while achieving comparable performance at a fraction of the computational cost. Floxels achieves a massive speedup of more than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10 minutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels achieves a speedup of ~14x."
      },
      {
        "id": "oai:arXiv.org:2503.10488v2",
        "title": "Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion",
        "link": "https://arxiv.org/abs/2503.10488",
        "author": "Evgeniia Vu, Andrei Boiarov, Dmitry Vetrov",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10488v2 Announce Type: replace \nAbstract: Generating co-speech gestures in real time requires both temporal coherence and efficient sampling. We introduce Accelerated Rolling Diffusion, a novel framework for streaming gesture generation that extends rolling diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that restructures the noise schedule into a stepwise ladder, allowing multiple frames to be denoised simultaneously. This significantly improves sampling efficiency while maintaining motion consistency, achieving up to a 2x speedup with high visual fidelity and temporal coherence. We evaluate our approach on ZEGGS and BEAT, strong benchmarks for real-world applicability. Our framework is universally applicable to any diffusion-based gesture generation model, transforming it into a streaming approach. Applied to three state-of-the-art methods, it consistently outperforms them, demonstrating its effectiveness as a generalizable and efficient solution for real-time, high-fidelity co-speech gesture synthesis."
      },
      {
        "id": "oai:arXiv.org:2503.10679v2",
        "title": "End-to-end Learning of Sparse Interventions on Activations to Steer Generation",
        "link": "https://arxiv.org/abs/2503.10679",
        "author": "Pau Rodriguez, Michal Klein, Eleonora Gualdoni, Arno Blaas, Luca Zappella, Marco Cuturi, Xavier Suau",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.10679v2 Announce Type: replace \nAbstract: The growing use of generative models in daily life calls for efficient mechanisms to control their generation, to e.g., produce safe content or provide users with tools to explore style changes. Ideally, such mechanisms should be cheap, both at train and inference time, while preserving output quality. Recent research has shown that such mechanisms can be obtained by intervening exclusively on model activations, with the goal of correcting distributional differences between activations seen when using prompts from a source vs. a target set (e.g., toxic and non-toxic sentences). While cheap, these fast methods are inherently crude: their maps are tuned locally, not accounting for their impact on downstream layers, resulting in interventions that cause unintended shifts when used out-of-sample. We propose in this work linear end-to-end activation steering (LinEAS), an approach trained with a global loss that accounts simultaneously for all layerwise distributional shifts. In addition to being more robust, the loss used to train LinEAS can be regularized with sparsifying norms, which can automatically carry out neuron and layer selection. Empirically, LinEAS only requires a handful of samples to be effective, and beats similar baselines on toxicity mitigation, while performing on par with far more involved finetuning approaches. We show that LinEAS interventions can be composed, study the impact of sparsity on their performance, and showcase applications in text-to-image diffusions."
      },
      {
        "id": "oai:arXiv.org:2503.12001v3",
        "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction",
        "link": "https://arxiv.org/abs/2503.12001",
        "author": "Peizhen Zheng, Longfei Wei, Dongjing Jiang, Jianfei Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12001v3 Announce Type: replace \nAbstract: The accurate reconstruction of dynamic street scenes is critical for applications in autonomous driving, augmented reality, and virtual reality. Traditional methods relying on dense point clouds and triangular meshes struggle with moving objects, occlusions, and real-time processing constraints, limiting their effectiveness in complex urban environments. While multi-view stereo and neural radiance fields have advanced 3D reconstruction, they face challenges in computational efficiency and handling scene dynamics. This paper proposes a novel 3D Gaussian point distribution method for dynamic street scene reconstruction. Our approach introduces an adaptive transparency mechanism that eliminates moving objects while preserving high-fidelity static scene details. Additionally, iterative refinement of Gaussian point distribution enhances geometric accuracy and texture representation. We integrate directional encoding with spatial position optimization to optimize storage and rendering efficiency, reducing redundancy while maintaining scene integrity. Experimental results demonstrate that our method achieves high reconstruction quality, improved rendering performance, and adaptability in large-scale dynamic environments. These contributions establish a robust framework for real-time, high-precision 3D reconstruction, advancing the practicality of dynamic scene modeling across multiple applications."
      },
      {
        "id": "oai:arXiv.org:2503.12507v2",
        "title": "Segment Any-Quality Images with Generative Latent Space Enhancement",
        "link": "https://arxiv.org/abs/2503.12507",
        "author": "Guangqian Guo, Yong Guo, Xuehui Yu, Wenbo Li, Yaoxing Wang, Shan Gao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12507v2 Announce Type: replace \nAbstract: Despite their success, Segment Anything Models (SAMs) experience significant performance drops on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Specifically, we adapt the concept of latent diffusion to SAM-based segmentation frameworks and perform the generative diffusion process in the latent space of SAM to reconstruct high-quality representation, thereby improving segmentation. Additionally, we introduce two techniques to improve compatibility between the pre-trained diffusion model and the segmentation framework. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. We also construct the LQSeg dataset with a greater diversity of degradation types and levels for training and evaluating the model. Extensive experiments demonstrate that GleSAM significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM also performs well on unseen degradations, underscoring the versatility of our approach and dataset."
      },
      {
        "id": "oai:arXiv.org:2503.15683v2",
        "title": "The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generation",
        "link": "https://arxiv.org/abs/2503.15683",
        "author": "Yanis Benidir, Nicolas Gonthier, Clement Mallet",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15683v2 Announce Type: replace \nAbstract: Bi-temporal change detection at scale based on Very High Resolution (VHR) images is crucial for Earth monitoring. This remains poorly addressed so far: methods either require large volumes of annotated data (semantic case), or are limited to restricted datasets (binary set-ups). Most approaches do not exhibit the versatility required for temporal and spatial adaptation: simplicity in architecture design and pretraining on realistic and comprehensive datasets. Synthetic datasets are the key solution but still fail to handle complex and diverse scenes. In this paper, we present HySCDG a generative pipeline for creating a large hybrid semantic change detection dataset that contains both real VHR images and inpainted ones, along with land cover semantic map at both dates and the change map. Being semantically and spatially guided, HySCDG generates realistic images, leading to a comprehensive and hybrid transfer-proof dataset FSC-180k. We evaluate FSC-180k on five change detection cases (binary and semantic), from zero-shot to mixed and sequential training, and also under low data regime training. Experiments demonstrate that pretraining on our hybrid dataset leads to a significant performance boost, outperforming SyntheWorld, a fully synthetic dataset, in every configuration. All codes, models, and data are available here: https://yb23.github.io/projects/cywd/"
      },
      {
        "id": "oai:arXiv.org:2503.18871v2",
        "title": "Bootstrapped Model Predictive Control",
        "link": "https://arxiv.org/abs/2503.18871",
        "author": "Yuhang Wang, Hanwei Guo, Sizhe Wang, Long Qian, Xuguang Lan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18871v2 Announce Type: replace \nAbstract: Model Predictive Control (MPC) has been demonstrated to be effective in continuous control tasks. When a world model and a value function are available, planning a sequence of actions ahead of time leads to a better policy. Existing methods typically obtain the value function and the corresponding policy in a model-free manner. However, we find that such an approach struggles with complex tasks, resulting in poor policy learning and inaccurate value estimation. To address this problem, we leverage the strengths of MPC itself. In this work, we introduce Bootstrapped Model Predictive Control (BMPC), a novel algorithm that performs policy learning in a bootstrapped manner. BMPC learns a network policy by imitating an MPC expert, and in turn, uses this policy to guide the MPC process. Combined with model-based TD-learning, our policy learning yields better value estimation and further boosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism, which enables computationally efficient imitation learning. Our method achieves superior performance over prior works on diverse continuous control tasks. In particular, on challenging high-dimensional locomotion tasks, BMPC significantly improves data efficiency while also enhancing asymptotic performance and training stability, with comparable training time and smaller network sizes. Code is available at https://github.com/wertyuilife2/bmpc."
      },
      {
        "id": "oai:arXiv.org:2503.19207v2",
        "title": "FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images",
        "link": "https://arxiv.org/abs/2503.19207",
        "author": "Rong Wang, Fabian Prada, Ziyan Wang, Zhongshi Jiang, Chengxiang Yin, Junxuan Li, Shunsuke Saito, Igor Santesteban, Javier Romero, Rohan Joshi, Hongdong Li, Jason Saragih, Yaser Sheikh",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.19207v2 Announce Type: replace \nAbstract: We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA."
      },
      {
        "id": "oai:arXiv.org:2503.20279v2",
        "title": "sudo rm -rf agentic_security",
        "link": "https://arxiv.org/abs/2503.20279",
        "author": "Sejin Lee, Jian Kim, Haon Park, Ashkan Yousefpour, Sangyoon Yu, Min Song",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20279v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs Our code is available at: https://github.com/AIM-Intelligence/SUDO.git"
      },
      {
        "id": "oai:arXiv.org:2503.20880v2",
        "title": "BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology",
        "link": "https://arxiv.org/abs/2503.20880",
        "author": "Amaya Gallagher-Syed, Henry Senior, Omnia Alwazzan, Elena Pontarini, Michele Bombardieri, Costantino Pitzalis, Myles J. Lewis, Michael R. Barnes, Luca Rossi, Gregory Slabaugh",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20880v2 Announce Type: replace \nAbstract: The development of biologically interpretable and explainable models remains a key challenge in computational pathology, particularly for multistain immunohistochemistry (IHC) analysis. We present BioX-CPath, an explainable graph neural network architecture for whole slide image (WSI) classification that leverages both spatial and semantic features across multiple stains. At its core, BioX-CPath introduces a novel Stain-Aware Attention Pooling (SAAP) module that generates biologically meaningful, stain-aware patient embeddings. Our approach achieves state-of-the-art performance on both Rheumatoid Arthritis and Sjogren's Disease multistain datasets. Beyond performance metrics, BioX-CPath provides interpretable insights through stain attention scores, entropy measures, and stain interaction scores, that permit measuring model alignment with known pathological mechanisms. This biological grounding, combined with strong classification performance, makes BioX-CPath particularly suitable for clinical applications where interpretability is key. Source code and documentation can be found at: https://github.com/AmayaGS/BioX-CPath."
      },
      {
        "id": "oai:arXiv.org:2503.21489v2",
        "title": "Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric Mapping to Deep Learning",
        "link": "https://arxiv.org/abs/2503.21489",
        "author": "Edwin Tay, Nazli T\\\"umer, Amir A. Zadpoor",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21489v2 Announce Type: replace \nAbstract: Living biological tissue is a complex system, constantly growing and changing in response to external and internal stimuli. These processes lead to remarkable and intricate changes in shape. Modeling and understanding both natural and pathological (or abnormal) changes in the shape of anatomical structures is highly relevant, with applications in diagnostic, prognostic, and therapeutic healthcare. Nevertheless, modeling the longitudinal shape change of biological tissue is a non-trivial task due to its inherent nonlinear nature. In this review, we highlight several existing methodologies and tools for modeling longitudinal shape change (i.e., spatiotemporal shape modeling). These methods range from diffeomorphic metric mapping to deep-learning based approaches (e.g., autoencoders, generative networks, recurrent neural networks, etc.). We discuss the synergistic combinations of existing technologies and potential directions for future research, underscoring key deficiencies in the current research landscape."
      },
      {
        "id": "oai:arXiv.org:2503.21530v2",
        "title": "Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models",
        "link": "https://arxiv.org/abs/2503.21530",
        "author": "Umer Butt, Stalin Veranasi, G\\\"unter Neumann",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21530v2 Announce Type: replace \nAbstract: As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. Transliteration between Urdu and its Romanized form, Roman Urdu, remains underexplored despite the widespread use of both scripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset showed promising results but suffered from poor domain adaptability and limited evaluation. We propose a transformer-based approach using the m2m100 multilingual translation model, enhanced with masked language modeling (MLM) pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse Dakshina dataset. To address previous evaluation flaws, we introduce rigorous dataset splits and assess performance using BLEU, character-level BLEU, and CHRF. Our model achieves strong transliteration performance, with Char-BLEU scores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These results outperform both RNN baselines and GPT-4o Mini and demonstrate the effectiveness of multilingual transfer learning for low-resource transliteration tasks."
      },
      {
        "id": "oai:arXiv.org:2503.23130v3",
        "title": "Can DeepSeek Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Robotic-Assisted Surgery",
        "link": "https://arxiv.org/abs/2503.23130",
        "author": "Boyi Ma, Yanguang Zhao, Jie Wang, Guankun Wang, Kun Yuan, Tong Chen, Long Bai, Hongliang Ren",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23130v3 Announce Type: replace \nAbstract: The DeepSeek models have shown exceptional performance in general scene understanding, question-answering (QA), and text generation tasks, owing to their efficient training paradigm and strong reasoning capabilities. In this study, we investigate the dialogue capabilities of the DeepSeek model in robotic surgery scenarios, focusing on tasks such as Single Phrase QA, Visual QA, and Detailed Description. The Single Phrase QA tasks further include sub-tasks such as surgical instrument recognition, action understanding, and spatial position analysis. We conduct extensive evaluations using publicly available datasets, including EndoVis18 and CholecT50, along with their corresponding dialogue data. Our empirical study shows that, compared to existing general-purpose multimodal large language models, DeepSeek-VL2 performs better on complex understanding tasks in surgical scenes. Additionally, although DeepSeek-V3 is purely a language model, we find that when image tokens are directly inputted, the model demonstrates better performance on single-sentence QA tasks. However, overall, the DeepSeek models still fall short of meeting the clinical requirements for understanding surgical scenes. Under general prompts, DeepSeek models lack the ability to effectively analyze global surgical concepts and fail to provide detailed insights into surgical scenarios. Based on our observations, we argue that the DeepSeek models are not ready for vision-language tasks in surgical contexts without fine-tuning on surgery-specific datasets."
      },
      {
        "id": "oai:arXiv.org:2503.23185v2",
        "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training",
        "link": "https://arxiv.org/abs/2503.23185",
        "author": "Shota Hirose, Kazuki Kotoyori, Kasidis Arunruangsirilert, Fangzheng Lin, Heming Sun, Jiro Katto",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23185v2 Announce Type: replace \nAbstract: Transmission latency significantly affects users' quality of experience in real-time interaction and actuation. As latency is principally inevitable, video prediction can be utilized to mitigate the latency and ultimately enable zero-latency transmission. However, most of the existing video prediction methods are computationally expensive and impractical for real-time applications. In this work, we therefore propose real-time video prediction towards the zero-latency interaction over networks, called IFRVP (Intermediate Feature Refinement Video Prediction). Firstly, we propose three training methods for video prediction that extend frame interpolation models, where we utilize a simple convolution-only frame interpolation network based on IFRNet. Secondly, we introduce ELAN-based residual blocks into the prediction models to improve both inference speed and accuracy. Our evaluations show that our proposed models perform efficiently and achieve the best trade-off between prediction accuracy and computational speed among the existing video prediction methods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The code will be released at https://github.com/FykAikawa/IFRVP."
      },
      {
        "id": "oai:arXiv.org:2503.23368v3",
        "title": "VLIPP: Towards Physically Plausible Video Generation with Vision and Language Informed Physical Prior",
        "link": "https://arxiv.org/abs/2503.23368",
        "author": "Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, Xu Jia",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23368v3 Announce Type: replace \nAbstract: Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics with vision and language informed physical prior. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation."
      },
      {
        "id": "oai:arXiv.org:2503.23705v2",
        "title": "Steering Large Agent Populations using Mean-Field Schrodinger Bridges with Gaussian Mixture Models",
        "link": "https://arxiv.org/abs/2503.23705",
        "author": "George Rapakoulias, Ali Reza Pedram, Panagiotis Tsiotras",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23705v2 Announce Type: replace \nAbstract: The Mean-Field Schrodinger Bridge (MFSB) problem is an optimization problem aiming to find the minimum effort control policy to drive a McKean-Vlassov stochastic differential equation from one probability measure to another. In the context of multiagent control, the objective is to control the configuration of a swarm of identical, interacting cooperative agents, as captured by the time-varying probability measure of their state. Available methods for solving this problem for distributions with continuous support rely either on spatial discretizations of the problem's domain or on approximating optimal solutions using neural networks trained through stochastic optimization schemes. For agents following Linear Time-Varying dynamics, and for Gaussian Mixture Model boundary distributions, we propose a highly efficient parameterization to approximate the solutions of the corresponding MFSB in closed form, without any learning steps. Our proposed approach consists of a mixture of elementary policies, each solving a Gaussian-to-Gaussian Covariance Steering problem from the components of the initial to the components of the terminal mixture. Leveraging the semidefinite formulation of the Covariance Steering problem, our proposed solver can handle probabilistic hard constraints on the system's state, while maintaining numerical tractability. We illustrate our approach on a variety of numerical examples."
      },
      {
        "id": "oai:arXiv.org:2504.00396v2",
        "title": "SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning",
        "link": "https://arxiv.org/abs/2504.00396",
        "author": "Xiaole Xian, Zhichao Liao, Qingyu Li, Wenyu Qin, Pengfei Wan, Weicheng Xie, Long Zeng, Linlin Shen, Pingfa Feng",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00396v2 Announce Type: replace \nAbstract: Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, a pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose a dual-path pipeline that introduces the original model as a reference for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce a novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose a novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-the-art performance. Project webpage: https://spf-portrait.github.io/SPF-Portrait/"
      },
      {
        "id": "oai:arXiv.org:2504.00589v2",
        "title": "Efficient Annotator Reliability Assessment with EffiARA",
        "link": "https://arxiv.org/abs/2504.00589",
        "author": "Owen Cook, Jake Vasilakes, Ian Roberts, Xingyi Song",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00589v2 Announce Type: replace \nAbstract: Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework's efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk."
      },
      {
        "id": "oai:arXiv.org:2504.01222v2",
        "title": "AutoML Benchmark with shorter time constraints and early stopping",
        "link": "https://arxiv.org/abs/2504.01222",
        "author": "Israel Campero Jurado, Pieter Gijsbers, Joaquin Vanschoren",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01222v2 Announce Type: replace \nAbstract: Automated Machine Learning (AutoML) automatically builds machine learning (ML) models on data. The de facto standard for evaluating new AutoML frameworks for tabular data is the AutoML Benchmark (AMLB). AMLB proposed to evaluate AutoML frameworks using 1- and 4-hour time budgets across 104 tasks. We argue that shorter time constraints should be considered for the benchmark because of their practical value, such as when models need to be retrained with high frequency, and to make AMLB more accessible. This work considers two ways in which to reduce the overall computation used in the benchmark: smaller time constraints and the use of early stopping. We conduct evaluations of 11 AutoML frameworks on 104 tasks with different time constraints and find the relative ranking of AutoML frameworks is fairly consistent across time constraints, but that using early-stopping leads to a greater variety in model performance."
      },
      {
        "id": "oai:arXiv.org:2504.01739v2",
        "title": "Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers",
        "link": "https://arxiv.org/abs/2504.01739",
        "author": "Lukas Boehm, Jonas Leo Mueller, Christoffer Loeffler, Leo Schwinn, Bjoern Eskofier, Dario Zanca",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01739v2 Announce Type: replace \nAbstract: Understanding the perceptual invariances of artificial neural networks is essential for improving explainability and aligning models with human vision. Metamers - stimuli that are physically distinct yet produce identical neural activations - serve as a valuable tool for investigating these invariances. We introduce a novel approach to metamer generation by leveraging ensembles of artificial neural networks, capturing shared representational subspaces across diverse architectures, including convolutional neural networks and vision transformers. To characterize the properties of the generated metamers, we employ a suite of image-based metrics that assess factors such as semantic fidelity and naturalness. Our findings show that convolutional neural networks generate more recognizable and human-like metamers, while vision transformers produce realistic but less transferable metamers, highlighting the impact of architectural biases on representational invariances."
      },
      {
        "id": "oai:arXiv.org:2504.02456v2",
        "title": "The Amenability Framework: Rethinking Causal Ordering Without Estimating Causal Effects",
        "link": "https://arxiv.org/abs/2504.02456",
        "author": "Carlos Fern\\'andez-Lor\\'ia, Jorge Lor\\'ia",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02456v2 Announce Type: replace \nAbstract: Who should we prioritize for intervention when we cannot estimate intervention effects? In many applied domains (e.g., advertising, customer retention, and behavioral nudging) prioritization is guided by predictive models that estimate outcome probabilities rather than causal effects. This paper investigates when these predictions (scores) can effectively rank individuals by their intervention effects, particularly when direct effect estimation is infeasible or unreliable. We propose a conceptual framework based on amenability: an individual's latent proclivity to be influenced by an intervention. We then formalize conditions under which predictive scores serve as effective proxies for amenability. These conditions justify using non-causal scores for intervention prioritization, even when the scores do not directly estimate effects. We further show that, under plausible assumptions, predictive models can outperform causal effect estimators in ranking individuals by intervention effects. Empirical evidence from an advertising context supports our theoretical findings, demonstrating that predictive modeling can offer a more robust approach to targeting than effect estimation. Our framework suggests a shift in focus, from estimating effects to inferring who is amenable, as a practical and theoretically grounded strategy for prioritizing interventions in resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2504.02542v2",
        "title": "Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation",
        "link": "https://arxiv.org/abs/2504.02542",
        "author": "Fa-Ting Hong, Zunnan Xu, Zixiang Zhou, Jun Zhou, Xiu Li, Qin Lin, Qinglin Lu, Dan Xu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02542v2 Announce Type: replace \nAbstract: Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce \\textbf{ACTalker}, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict. The project website can be found at \\href{https://harlanhong.github.io/publications/actalker/index.html}{HERE}."
      },
      {
        "id": "oai:arXiv.org:2504.02587v2",
        "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme",
        "link": "https://arxiv.org/abs/2504.02587",
        "author": "Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, Pengfei Liu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02587v2 Announce Type: replace \nAbstract: Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research."
      },
      {
        "id": "oai:arXiv.org:2504.02692v2",
        "title": "GPTQv2: Efficient Finetuning-Free Quantization for Asymmetric Calibration",
        "link": "https://arxiv.org/abs/2504.02692",
        "author": "Yuhang Li, Ruokai Yin, Donghyun Lee, Shiting Xiao, Priyadarshini Panda",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02692v2 Announce Type: replace \nAbstract: We introduce GPTQv2, a novel finetuning-free quantization method for compressing large-scale transformer architectures. Unlike the previous GPTQ method, which independently calibrates each layer, we always match the quantized layer's output to the exact output in the full-precision model, resulting in a scheme that we call asymmetric calibration. Such a scheme can effectively reduce the quantization error accumulated in previous layers. We analyze this problem using optimal brain compression to derive a close-formed solution. The new solution explicitly minimizes the quantization error as well as the accumulated asymmetry error. Furthermore, we utilize various techniques to parallelize the solution calculation, including channel parallelization, neuron decomposition, and Cholesky reformulation for matrix fusion. As a result, GPTQv2 is easy to implement, simply using 20 more lines of code than GPTQ but improving its performance under low-bit quantization. Remarkably, on a single GPU, we quantize a 405B language transformer as well as EVA-02 the rank first vision transformer that achieves 90% pretraining Imagenet accuracy. Code is available at github.com/Intelligent-Computing-Lab-Yale/GPTQv2."
      },
      {
        "id": "oai:arXiv.org:2504.02732v2",
        "title": "Why do LLMs attend to the first token?",
        "link": "https://arxiv.org/abs/2504.02732",
        "author": "Federico Barbero, \\'Alvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veli\\v{c}kovi\\'c, Razvan Pascanu",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02732v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training."
      },
      {
        "id": "oai:arXiv.org:2504.02800v2",
        "title": "A Survey of Large Language Models in Mental Health Disorder Detection on Social Media",
        "link": "https://arxiv.org/abs/2504.02800",
        "author": "Zhuohan Ge, Nicole Hu, Darian Li, Yubo Wang, Shihao Qi, Yuming Xu, Han Shi, Jason Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02800v2 Announce Type: replace \nAbstract: The detection and intervention of mental health issues represent a critical global research focus, and social media data has been recognized as an important resource for mental health research. However, how to utilize Large Language Models (LLMs) for mental health problem detection on social media poses significant challenges. Hence, this paper aims to explore the potential of LLM applications in social media data analysis, focusing not only on the most common psychological disorders such as depression and anxiety but also incorporating psychotic disorders and externalizing disorders, summarizing the application methods of LLM from different dimensions, such as text data analysis and detection of mental disorders, and revealing the major challenges and shortcomings of current research. In addition, the paper provides an overview of popular datasets, and evaluation metrics. The survey in this paper provides a comprehensive frame of reference for researchers in the field of mental health, while demonstrating the great potential of LLMs in mental health detection to facilitate the further application of LLMs in future mental health interventions."
      },
      {
        "id": "oai:arXiv.org:2210.02672v4",
        "title": "Orthogonal Nonnegative Matrix Factorization with Sparsity Constraints",
        "link": "https://arxiv.org/abs/2210.02672",
        "author": "Salar Basiri, Alisina Bayati, Srinivasa Salapaka",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2210.02672v4 Announce Type: replace-cross \nAbstract: This article presents a novel approach to solving the sparsity-constrained Orthogonal Nonnegative Matrix Factorization (SCONMF) problem, which requires decomposing a non-negative data matrix into the product of two lower-rank non-negative matrices, X=WH, where the mixing matrix H has orthogonal rows HH^T=I, while also satisfying an upper bound on the number of nonzero elements in each row. By reformulating SCONMF as a capacity-constrained facility-location problem (CCFLP), the proposed method naturally integrates non-negativity, orthogonality, and sparsity constraints. Specifically, our approach integrates control-barrier function (CBF) based framework used for dynamic optimal control design problems with maximum-entropy-principle-based framework used for facility location problems to enforce these constraints while ensuring robust factorization. Additionally, this work introduces a quantitative approach for determining the ``true\" rank of W or H, equivalent to the number of ``true\" features - a critical aspect in ONMF applications where the number of features is unknown. Simulations on various datasets demonstrate significantly improved factorizations with low reconstruction errors (as small as by 150 times) while strictly satisfying all constraints, outperforming existing methods that struggle with balancing accuracy and constraint adherence."
      },
      {
        "id": "oai:arXiv.org:2401.02520v2",
        "title": "Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel",
        "link": "https://arxiv.org/abs/2401.02520",
        "author": "Jinhang Chai, Jianqing Fan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.02520v2 Announce Type: replace-cross \nAbstract: The problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. This paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. We propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. To attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries; in other words, it cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. We then showcase the applications of our framework to several important statistical machine learning problems. In the problem of estimating a structured Markov transition kernel, the proposed method achieves the minimax optimality and the result can be extended to estimating the conditional mean operator, a crucial component in reinforcement learning. The applications to multitask regression and structured covariance estimation are also presented. We propose an alternating minimization algorithm to approximately solve the potentially hard optimization problem. Numerical results corroborate the effectiveness of our method which typically converges in a few steps."
      },
      {
        "id": "oai:arXiv.org:2403.10070v2",
        "title": "A Structure-Preserving Kernel Method for Learning Hamiltonian Systems",
        "link": "https://arxiv.org/abs/2403.10070",
        "author": "Jianyu Hu, Juan-Pablo Ortega, Daiying Yin",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.10070v2 Announce Type: replace-cross \nAbstract: A structure-preserving kernel ridge regression method is presented that allows the recovery of nonlinear Hamiltonian functions out of datasets made of noisy observations of Hamiltonian vector fields. The method proposes a closed-form solution that yields excellent numerical performances that surpass other techniques proposed in the literature in this setup. From the methodological point of view, the paper extends kernel regression methods to problems in which loss functions involving linear functions of gradients are required and, in particular, a differential reproducing property and a Representer Theorem are proved in this context. The relation between the structure-preserving kernel estimator and the Gaussian posterior mean estimator is analyzed. A full error analysis is conducted that provides convergence rates using fixed and adaptive regularization parameters. The good performance of the proposed estimator together with the convergence rate is illustrated with various numerical experiments."
      },
      {
        "id": "oai:arXiv.org:2404.09151v3",
        "title": "Productively Deploying Emerging Models on Emerging Platforms: A Top-Down Approach for Testing and Debugging",
        "link": "https://arxiv.org/abs/2404.09151",
        "author": "Siyuan Feng, Jiawei Liu, Ruihang Lai, Charlie F. Ruan, Yong Yu, Lingming Zhang, Tianqi Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.09151v3 Announce Type: replace-cross \nAbstract: While existing machine learning (ML) frameworks focus on established platforms, like running CUDA on server-grade GPUs, there have been growing demands to enable emerging AI applications in a broader set of scenarios, such as running Large Language Models (LLMs) within browsers and mobile phones. However, deploying emerging models on new platforms (such as Metal and WebGPU) presents significant software engineering challenges due to rapid model evolution and limited tooling and practices for these platforms.\n  Previous practice for ML model deployment often follows a bottom-up fashion, where engineers first implement individual required operators and then put them together. However, this traditional development approach fails to meet the productivity requirements when deploying emerging ML applications, with the testing and debugging part as a bottleneck. To this end, we introduce \\textsc{TapML}, a top-down approach designed to streamline model deployment on diverse platforms. While the traditional bottom-up approach requires crafting manual tests, \\textsc{TapML} automatically creates high-quality, realistic test data through operator-wise test carving. Furthermore, \\textsc{TapML} uses a migration-based strategy to gradually offload model implementation from the mature source platform to the target platform, minimizing the debugging scope of compound errors.\n  \\textsc{TapML} has been used as the default development method in the MLC-LLM project to deploy emerging ML models. Within 2 years, \\textsc{TapML} has accelerated the deployment of 105 emerging models in 27 model architectures across 5 emerging platforms. We show that \\textsc{TapML} effectively boosts developer productivity while ensuring the quality of deployed models. Furthermore, we summarize comprehensive case studies from our real-world development, offering best practices for developing emerging ML systems."
      },
      {
        "id": "oai:arXiv.org:2407.05625v3",
        "title": "New User Event Prediction Through the Lens of Causal Inference",
        "link": "https://arxiv.org/abs/2407.05625",
        "author": "Henry Shaowu Yuchi, Shixiang Zhu, Li Dong, Yigit M. Arisoy, Matthew C. Spencer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.05625v3 Announce Type: replace-cross \nAbstract: Modeling and analysis for event series generated by users of heterogeneous behavioral patterns are closely involved in our daily lives, including credit card fraud detection, online platform user recommendation, and social network analysis. The most commonly adopted approach to this task is to assign users to behavior-based categories and analyze each of them separately. However, this requires extensive data to fully understand the user behavior, presenting challenges in modeling newcomers without significant historical knowledge. In this work, we propose a novel discrete event prediction framework for new users with limited history, without needing to know the user's category. We treat the user event history as the \"treatment\" for future events and the user category as the key confounder. Thus, the prediction problem can be framed as counterfactual outcome estimation, where each event is re-weighted by its inverse propensity score. We demonstrate the improved performance of the proposed framework with a numerical simulation study and two real-world applications, including Netflix rating prediction and seller contact prediction for customer support at Amazon."
      },
      {
        "id": "oai:arXiv.org:2407.07368v3",
        "title": "Semi-Supervised Model-Free Bayesian State Estimation from Compressed Measurements",
        "link": "https://arxiv.org/abs/2407.07368",
        "author": "Anubhab Ghosh, Yonina C. Eldar, Saikat Chatterjee",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.07368v3 Announce Type: replace-cross \nAbstract: We consider data-driven Bayesian state estimation from compressed measurements (BSCM) of a model-free process. The dimension of the temporal measurement vector is lower than that of the temporal state vector to be estimated, leading to an under-determined inverse problem. The underlying dynamical model of the state's evolution is unknown for a 'model-free process.' Hence, it is difficult to use traditional model-driven methods, for example, Kalman and particle filters. Instead, we consider data-driven methods. We experimentally show that two existing unsupervised learning-based data-driven methods fail to address the BSCM problem in a model-free process. The methods are -- data-driven nonlinear state estimation (DANSE) and deep Markov model (DMM). While DANSE provides good predictive/forecasting performance to model the temporal measurement data as a time series, its unsupervised learning lacks suitable regularization for tackling the BSCM task. We then propose a semi-supervised learning approach and develop a semi-supervised learning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a large amount of unlabelled data along with a limited amount of labelled data, i.e., pairwise measurement-and-state data, which provides the desired regularization. Using three benchmark dynamical systems, we show that the data-driven SemiDANSE provides competitive state estimation performance for BSCM against a hybrid method called KalmanNet and two model-driven methods (extended Kalman filter and unscented Kalman filter) that know the dynamical models exactly."
      },
      {
        "id": "oai:arXiv.org:2408.06277v4",
        "title": "Multi-marginal Schr\\\"odinger Bridges with Iterative Reference Refinement",
        "link": "https://arxiv.org/abs/2408.06277",
        "author": "Yunyi Shen, Renato Berlinghieri, Tamara Broderick",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.06277v4 Announce Type: replace-cross \nAbstract: Practitioners often aim to infer an unobserved population trajectory using sample snapshots at multiple time points. E.g., given single-cell sequencing data, scientists would like to learn how gene expression changes over a cell's life cycle. But sequencing any cell destroys that cell. So we can access data for any particular cell only at a single time point, but we have data across many cells. The deep learning community has recently explored using Schr\\\"odinger bridges (SBs) and their extensions in similar settings. However, existing methods either (1) interpolate between just two time points or (2) require a single fixed reference dynamic (often set to Brownian motion within SBs). But learning piecewise from adjacent time points can fail to capture long-term dependencies. And practitioners are typically able to specify a model family for the reference dynamic but not the exact values of the parameters within it. So we propose a new method that (1) learns the unobserved trajectories from sample snapshots across multiple time points and (2) requires specification only of a family of reference dynamics, not a single fixed one. We demonstrate the advantages of our method on simulated and real data."
      },
      {
        "id": "oai:arXiv.org:2409.05925v2",
        "title": "Assessing SPARQL capabilities of Large Language Models",
        "link": "https://arxiv.org/abs/2409.05925",
        "author": "Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.05925v2 Announce Type: replace-cross \nAbstract: The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases."
      },
      {
        "id": "oai:arXiv.org:2409.06142v5",
        "title": "Variational Search Distributions",
        "link": "https://arxiv.org/abs/2409.06142",
        "author": "Daniel M. Steinberg, Rafael Oliveira, Cheng Soon Ong, Edwin V. Bonilla",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.06142v5 Announce Type: replace-cross \nAbstract: We develop VSD, a method for conditioning a generative model of discrete, combinatorial designs on a rare desired class by efficiently evaluating a black-box (e.g. experiment, simulation) in a batch sequential manner. We call this task active generation; we formalize active generation's requirements and desiderata, and formulate a solution via variational inference. VSD uses off-the-shelf gradient based optimization routines, can learn powerful generative models for desirable designs, and can take advantage of scalable predictive models. We derive asymptotic convergence rates for learning the true conditional generative distribution of designs with certain configurations of our method. After illustrating the generative model on images, we empirically demonstrate that VSD can outperform existing baseline methods on a set of real sequence-design problems in various protein and DNA/RNA engineering tasks."
      },
      {
        "id": "oai:arXiv.org:2409.09794v2",
        "title": "Federated Learning in Adversarial Environments: Testbed Design and Poisoning Resilience in Cybersecurity",
        "link": "https://arxiv.org/abs/2409.09794",
        "author": "Hao Jian Huang, Hakan T. Otal, M. Abdullah Canbaz",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09794v2 Announce Type: replace-cross \nAbstract: This paper presents the design and implementation of a Federated Learning (FL) testbed, focusing on its application in cybersecurity and evaluating its resilience against poisoning attacks. Federated Learning allows multiple clients to collaboratively train a global model while keeping their data decentralized, addressing critical needs for data privacy and security, particularly in sensitive fields like cybersecurity. Our testbed, built using Raspberry Pi and Nvidia Jetson hardware by running the Flower framework, facilitates experimentation with various FL frameworks, assessing their performance, scalability, and ease of integration. Through a case study on federated intrusion detection systems, the testbed's capabilities are shown in detecting anomalies and securing critical infrastructure without exposing sensitive network data. Comprehensive poisoning tests, targeting both model and data integrity, evaluate the system's robustness under adversarial conditions. The results show that while federated learning enhances data privacy and distributed learning, it remains vulnerable to poisoning attacks, which must be mitigated to ensure its reliability in real-world applications."
      },
      {
        "id": "oai:arXiv.org:2409.12799v3",
        "title": "The Central Role of the Loss Function in Reinforcement Learning",
        "link": "https://arxiv.org/abs/2409.12799",
        "author": "Kaiwen Wang, Nathan Kallus, Wen Sun",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.12799v3 Announce Type: replace-cross \nAbstract: This paper illustrates the central role of loss functions in data-driven decision making, providing a comprehensive survey on their influence in cost-sensitive classification (CSC) and reinforcement learning (RL). We demonstrate how different regression loss functions affect the sample efficiency and adaptivity of value-based decision making algorithms. Across multiple settings, we prove that algorithms using the binary cross-entropy loss achieve first-order bounds scaling with the optimal policy's cost and are much more efficient than the commonly used squared loss. Moreover, we prove that distributional algorithms using the maximum likelihood loss achieve second-order bounds scaling with the policy variance and are even sharper than first-order bounds. This in particular proves the benefits of distributional RL. We hope that this paper serves as a guide analyzing decision making algorithms with varying loss functions, and can inspire the reader to seek out better loss functions to improve any decision making algorithm."
      },
      {
        "id": "oai:arXiv.org:2409.17858v2",
        "title": "How Feature Learning Can Improve Neural Scaling Laws",
        "link": "https://arxiv.org/abs/2409.17858",
        "author": "Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.17858v2 Announce Type: replace-cross \nAbstract: We develop a solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model shows how performance scales with model size, training time, and the total amount of available data. We identify three scaling regimes corresponding to varying task difficulties: hard, easy, and super easy tasks. For easy and super-easy target functions, which lie in the reproducing kernel Hilbert space (RKHS) defined by the initial infinite-width Neural Tangent Kernel (NTK), the scaling exponents remain unchanged between feature learning and kernel regime models. For hard tasks, defined as those outside the RKHS of the initial NTK, we demonstrate both analytically and empirically that feature learning can improve scaling with training time and compute, nearly doubling the exponent for hard tasks. This leads to a different compute optimal strategy to scale parameters and training time in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks but not for easy and super-easy tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks."
      },
      {
        "id": "oai:arXiv.org:2410.02099v2",
        "title": "A Watermark for Black-Box Language Models",
        "link": "https://arxiv.org/abs/2410.02099",
        "author": "Dara Bahri, John Wieting",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02099v2 Announce Type: replace-cross \nAbstract: Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require white-box access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. black-box access), boasts a distortion-free property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments."
      },
      {
        "id": "oai:arXiv.org:2410.16290v4",
        "title": "A Unified Model for Compressed Sensing MRI Across Undersampling Patterns",
        "link": "https://arxiv.org/abs/2410.16290",
        "author": "Armeet Singh Jatyani, Jiayun Wang, Aditi Chandrashekar, Zihui Wu, Miguel Liu-Schiaffini, Bahareh Tolooshams, Anima Anandkumar",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.16290v4 Announce Type: replace-cross \nAbstract: Compressed Sensing MRI reconstructs images of the body's internal anatomy from undersampled measurements, thereby reducing scan time. Recently, deep learning has shown great potential for reconstructing high-fidelity images from highly undersampled measurements. However, one needs to train multiple models for different undersampling patterns and desired output image resolutions, since most networks operate on a fixed discretization. Such approaches are highly impractical in clinical settings, where undersampling patterns and image resolutions are frequently changed to accommodate different real-time imaging and diagnostic requirements.\n  We propose a unified MRI reconstruction model robust to various measurement undersampling patterns and image resolutions. Our approach uses neural operators, a discretization-agnostic architecture applied in both image and measurement spaces, to capture local and global features. Empirically, our model improves SSIM by 11% and PSNR by 4 dB over a state-of-the-art CNN (End-to-End VarNet), with 600$\\times$ faster inference than diffusion methods. The resolution-agnostic design also enables zero-shot super-resolution and extended field-of-view reconstruction, offering a versatile and efficient solution for clinical MR imaging. Our unified model offers a versatile solution for MRI, adapting seamlessly to various measurement undersampling and imaging resolutions, making it highly effective for flexible and reliable clinical imaging. Our code is available at https://armeet.ca/nomri."
      },
      {
        "id": "oai:arXiv.org:2411.03321v3",
        "title": "Towards More Accurate US Presidential Election via Multi-step Reasoning with Large Language Models",
        "link": "https://arxiv.org/abs/2411.03321",
        "author": "Chenxiao Yu, Zhaotian Weng, Yuangang Li, Zheng Li, Xiyang Hu, Yue Zhao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.03321v3 Announce Type: replace-cross \nAbstract: Can Large Language Models (LLMs) accurately predict election outcomes? While LLMs have demonstrated impressive performance in various domains, including healthcare, legal analysis, and creative tasks, their ability to forecast elections remains unknown. Election prediction poses unique challenges, such as limited voter-level data, rapidly changing political landscapes, and the need to model complex human behavior. To address these challenges, we introduce a multi-step reasoning framework designed for political analysis. Our approach is validated on real-world data from the American National Election Studies (ANES) 2016 and 2020, as well as synthetic personas generated by the leading machine learning framework, offering scalable datasets for voter behavior modeling. To capture temporal dynamics, we incorporate candidates' policy positions and biographical details, ensuring that the model adapts to evolving political contexts. Drawing on Chain of Thought prompting, our multi-step reasoning pipeline systematically integrates demographic, ideological, and time-dependent factors, enhancing the model's predictive power."
      },
      {
        "id": "oai:arXiv.org:2411.07483v2",
        "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
        "link": "https://arxiv.org/abs/2411.07483",
        "author": "Pasan Dissanayake, Faisal Hamman, Barproda Halder, Ilia Sucholutsky, Qiuyi Zhang, Sanghamitra Dutta",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.07483v2 Announce Type: replace-cross \nAbstract: Knowledge distillation deploys complex machine learning models in resource-constrained environments by training a smaller student model to emulate internal representations of a complex teacher model. However, the teacher's representations can also encode nuisance or additional information not relevant to the downstream task. Distilling such irrelevant information can actually impede the performance of a capacity-limited student model. This observation motivates our primary question: What are the information-theoretic limits of knowledge distillation? To this end, we leverage Partial Information Decomposition to quantify and explain the transferred knowledge and knowledge left to distill for a downstream task. We theoretically demonstrate that the task-relevant transferred knowledge is succinctly captured by the measure of redundant information about the task between the teacher and student. We propose a novel multi-level optimization to incorporate redundant information as a regularizer, leading to our framework of Redundant Information Distillation (RID). RID leads to more resilient and effective distillation under nuisance teachers as it succinctly quantifies task-relevant knowledge rather than simply aligning student and teacher representations."
      },
      {
        "id": "oai:arXiv.org:2411.11896v3",
        "title": "HeartBERT: A Self-Supervised ECG Embedding Model for Efficient and Effective Medical Signal Analysis",
        "link": "https://arxiv.org/abs/2411.11896",
        "author": "Saedeh Tahery, Fatemeh Hamid Akhlaghi, Termeh Amirsoleimani",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.11896v3 Announce Type: replace-cross \nAbstract: The HeartBert model is introduced with three primary objectives: reducing the need for labeled data, minimizing computational resources, and simultaneously improving performance in machine learning systems that analyze Electrocardiogram (ECG) signals. Inspired by Bidirectional Encoder Representations from Transformers (BERT) in natural language processing and enhanced with a self-supervised learning approach, the HeartBert model-built on the RoBERTa architecture-generates sophisticated embeddings tailored for ECG-based projects in the medical domain. To demonstrate the versatility, generalizability, and efficiency of the proposed model, two key downstream tasks have been selected: sleep stage detection and heartbeat classification. HeartBERT-based systems, utilizing bidirectional LSTM heads, are designed to address complex challenges. A series of practical experiments have been conducted to demonstrate the superiority and advancements of HeartBERT, particularly in terms of its ability to perform well with smaller training datasets, reduced learning parameters, and effective performance compared to rival models. The code and data are publicly available at https://github.com/ecgResearch/HeartBert."
      },
      {
        "id": "oai:arXiv.org:2412.06436v2",
        "title": "An Adaptively Inexact Method for Bilevel Learning Using Primal-Dual Style Differentiation",
        "link": "https://arxiv.org/abs/2412.06436",
        "author": "Lea Bogensperger, Matthias J. Ehrhardt, Thomas Pock, Mohammad Sadegh Salehi, Hok Shing Wong",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.06436v2 Announce Type: replace-cross \nAbstract: We consider a bilevel learning framework for learning linear operators. In this framework, the learnable parameters are optimized via a loss function that also depends on the minimizer of a convex optimization problem (denoted lower-level problem). We utilize an iterative algorithm called `piggyback' to compute the gradient of the loss and minimizer of the lower-level problem. Given that the lower-level problem is solved numerically, the loss function and thus its gradient can only be computed inexactly. To estimate the accuracy of the computed hypergradient, we derive an a-posteriori error bound, which provides guides for setting the tolerance for the lower-level problem, as well as the piggyback algorithm. To efficiently solve the upper-level optimization, we also propose an adaptive method for choosing a suitable step-size. To illustrate the proposed method, we consider a few learned regularizer problems, such as training an input-convex neural network."
      },
      {
        "id": "oai:arXiv.org:2412.18773v2",
        "title": "Learning Broken Symmetries with Approximate Invariance",
        "link": "https://arxiv.org/abs/2412.18773",
        "author": "Seth Nabat, Aishik Ghosh, Edmund Witkowski, Gregor Kasieczka, Daniel Whiteson",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.18773v2 Announce Type: replace-cross \nAbstract: Recognizing symmetries in data allows for significant boosts in neural network training, which is especially important where training data are limited. In many cases, however, the exact underlying symmetry is present only in an idealized dataset, and is broken in actual data, due to asymmetries in the detector, or varying response resolution as a function of particle momentum. Standard approaches, such as data augmentation or equivariant networks fail to represent the nature of the full, broken symmetry, effectively overconstraining the response of the neural network. We propose a learning model which balances the generality and asymptotic performance of unconstrained networks with the rapid learning of constrained networks. This is achieved through a dual-subnet structure, where one network is constrained by the symmetry and the other is not, along with a learned symmetry factor. In a simplified toy example that demonstrates violation of Lorentz invariance, our model learns as rapidly as symmetry-constrained networks but escapes its performance limitations."
      },
      {
        "id": "oai:arXiv.org:2502.13107v2",
        "title": "MatterChat: A Multi-Modal LLM for Material Science",
        "link": "https://arxiv.org/abs/2502.13107",
        "author": "Yingheng Tang, Wenbin Xu, Jie Cao, Jianzhu Ma, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W. Mahoney, Andy Nonaka, Zhi Yao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.13107v2 Announce Type: replace-cross \nAbstract: Understanding and predicting the properties of inorganic materials is crucial for accelerating advancements in materials science and driving applications in energy, electronics, and beyond. Integrating material structure data with language-based information through multi-modal large language models (LLMs) offers great potential to support these efforts by enhancing human-AI interaction. However, a key challenge lies in integrating atomic structures at full resolution into LLMs. In this work, we introduce MatterChat, a versatile structure-aware multi-modal LLM that unifies material structural data and textual inputs into a single cohesive model. MatterChat employs a bridging module to effectively align a pretrained machine learning interatomic potential with a pretrained LLM, reducing training costs and enhancing flexibility. Our results demonstrate that MatterChat significantly improves performance in material property prediction and human-AI interaction, surpassing general-purpose LLMs such as GPT-4. We also demonstrate its usefulness in applications such as more advanced scientific reasoning and step-by-step material synthesis."
      },
      {
        "id": "oai:arXiv.org:2502.14202v2",
        "title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions",
        "link": "https://arxiv.org/abs/2502.14202",
        "author": "Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, Preetha Chatterjee",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14202v2 Announce Type: replace-cross \nAbstract: The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses."
      },
      {
        "id": "oai:arXiv.org:2502.16936v2",
        "title": "Supervised contrastive learning from weakly-labeled audio segments for musical version matching",
        "link": "https://arxiv.org/abs/2502.16936",
        "author": "Joan Serr\\`a, R. Oguz Araz, Dmitry Bogdanov, Yuki Mitsufuji",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16936v2 Announce Type: replace-cross \nAbstract: Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Because of the ground truth nature, existing approaches match musical versions at the track level (e.g., whole song). However, most applications require to match them at the segment level (e.g., 20s chunks). In addition, existing approaches resort to classification and triplet losses, disregarding more recent losses that could bring meaningful improvements. In this paper, we propose a method to learn from weakly annotated segments, together with a contrastive loss variant that outperforms well-studied alternatives. The former is based on pairwise segment distance reductions, while the latter modifies an existing loss following decoupling, hyper-parameter, and geometric considerations. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching."
      },
      {
        "id": "oai:arXiv.org:2503.01036v2",
        "title": "Data-Efficient Kernel Methods for Learning Differential Equations and Their Solution Operators: Algorithms and Error Analysis",
        "link": "https://arxiv.org/abs/2503.01036",
        "author": "Yasamin Jalalian, Juan Felipe Osorio Ramirez, Alexander Hsu, Bamdad Hosseini, Houman Owhadi",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.01036v2 Announce Type: replace-cross \nAbstract: We introduce a novel kernel-based framework for learning differential equations and their solution maps that is efficient in data requirements, in terms of solution examples and amount of measurements from each example, and computational cost, in terms of training procedures. Our approach is mathematically interpretable and backed by rigorous theoretical guarantees in the form of quantitative worst-case error bounds for the learned equation. Numerical benchmarks demonstrate significant improvements in computational complexity and robustness while achieving one to two orders of magnitude improvements in terms of accuracy compared to state-of-the-art algorithms."
      },
      {
        "id": "oai:arXiv.org:2503.11305v2",
        "title": "Lightweight Learning for Grant-Free Activity Detection in Cell-Free Massive MIMO Networks",
        "link": "https://arxiv.org/abs/2503.11305",
        "author": "Ali Elkeshawy, Haifa Fares, Amor Nafkha",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.11305v2 Announce Type: replace-cross \nAbstract: Grant-free random access (GF-RA) is a promising access technique for massive machine-type communications (mMTC) in future wireless networks, particularly in the context of 5G and beyond (6G) systems. Within the context of GF-RA, this study investigates the efficiency of employing supervised machine learning techniques to tackle the challenges on the device activity detection (AD). GF-RA addresses scalability by employing non-orthogonal pilot sequences, which provides an efficient alternative comparing to conventional grant-based random access (GB-RA) technique that are constrained by the scarcity of orthogonal preamble resources. In this paper, we propose a novel lightweight data-driven algorithmic framework specifically designed for activity detection in GF-RA for mMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks. We propose two distinct framework deployment strategies, centralized and decentralized, both tailored to streamline the proposed approach implementation across network infrastructures. Moreover, we introduce optimized post-detection methodologies complemented by a clustering stage to enhance overall detection performances. Our 3GPP-compliant simulations have validated that the proposed algorithm achieves state-of-the-art model-based activity detection accuracy while significantly reducing complexity. Achieving 99% accuracy, it demonstrates real-world viability and effectiveness."
      },
      {
        "id": "oai:arXiv.org:2503.13558v4",
        "title": "Survival Analysis with Machine Learning for Predicting Li-ion Battery Remaining Useful Life",
        "link": "https://arxiv.org/abs/2503.13558",
        "author": "Jingyuan Xue, Longfei Wei, Fang Sheng, Jianfei Zhang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13558v4 Announce Type: replace-cross \nAbstract: Battery degradation significantly impacts the reliability and efficiency of energy storage systems, particularly in electric vehicles (EVs) and industrial applications. Predicting the remaining useful life (RUL) of lithium-ion (Li-ion) batteries is crucial for optimizing maintenance schedules, reducing costs, and improving safety. Traditional RUL prediction methods often struggle with nonlinear degradation patterns and uncertainty quantification. To address these challenges, we propose a hybrid survival analysis framework integrating both statistical and machine-learning-based models for RUL estimation. Our approach transforms time-series battery data into time-to-failure data using path signatures, enabling effective survival modeling. We apply five models, including Cox-based survival models and machine-learning-based methods such as DeepHit and MTLR, to estimate failure-free probabilities over time. Experiments conducted on 362 Toyota battery datasets demonstrate the effectiveness of our approach, achieving high time-dependent AUC and concordance index while maintaining a low integrated Brier score. The proposed methodology provides actionable insights for battery manufacturers and engineers, supporting dynamic maintenance strategies and optimized lifecycle management."
      },
      {
        "id": "oai:arXiv.org:2503.22943v2",
        "title": "Towards Mobile Sensing with Event Cameras on High-agility Resource-constrained Devices: A Survey",
        "link": "https://arxiv.org/abs/2503.22943",
        "author": "Haoyang Wang, Ruishan Guo, Pengtao Ma, Ciyu Ruan, Xinyu Luo, Wenhua Ding, Tianyang Zhong, Jingao Xu, Yunhao Liu, Xinlei Chen",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22943v2 Announce Type: replace-cross \nAbstract: With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in terms of achieving high accuracy and low latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution, low latency, and energy efficiency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, the lack of inherent semantic information, and the large data volume pose significant challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature over the period 2014-2024, provides a comprehensive overview of event-based mobile sensing systems, covering fundamental principles, event abstraction methods, algorithmic advancements, hardware and software acceleration strategies. We also discuss key applications of event cameras in mobile sensing, including visual odometry, object tracking, optical flow estimation, and 3D reconstruction, while highlighting the challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving event camera hardware with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms to enhance perception. To support ongoing research, we provide an open-source \\textit{Online Sheet} with curated resources and recent developments. We hope this survey serves as a valuable reference, facilitating the adoption of event-based vision across diverse applications."
      },
      {
        "id": "oai:arXiv.org:2504.01043v2",
        "title": "Are clinicians ethically obligated to disclose their use of medical machine learning systems to patients?",
        "link": "https://arxiv.org/abs/2504.01043",
        "author": "Joshua Hatherley",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01043v2 Announce Type: replace-cross \nAbstract: It is commonly accepted that clinicians are ethically obligated to disclose their use of medical machine learning systems to patients, and that failure to do so would amount to a moral fault for which clinicians ought to be held accountable. Call this \"the disclosure thesis.\" Four main arguments have been, or could be, given to support the disclosure thesis in the ethics literature: the risk-based argument, the rights-based argument, the materiality argument, and the autonomy argument. In this article, I argue that each of these four arguments are unconvincing, and therefore, that the disclosure thesis ought to be rejected. I suggest that mandating disclosure may also even risk harming patients by providing stakeholders with a way to avoid accountability for harm that results from improper applications or uses of these systems."
      },
      {
        "id": "oai:arXiv.org:2504.01848v2",
        "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
        "link": "https://arxiv.org/abs/2504.01848",
        "author": "Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, Tejal Patwardhan",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01848v2 Announce Type: replace-cross \nAbstract: We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We \\href{https://github.com/openai/preparedness}{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents."
      },
      {
        "id": "oai:arXiv.org:2504.01964v2",
        "title": "What Can 240,000 New Credit Transactions Tell Us About the Impact of NGEU Funds?",
        "link": "https://arxiv.org/abs/2504.01964",
        "author": "Alvaro Ortiz, Tomasa Rodrigo, David Sarasa, Sirenia Vazquez",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01964v2 Announce Type: replace-cross \nAbstract: Using a panel data local projections model and controlling for firm characteristics, procurement bid attributes, and macroeconomic conditions, the study estimates the dynamic effects of procurement awards on new lending, a more precise measure than the change in the stock of credit. The analysis further examines heterogeneity in credit responses based on firm size, industry, credit maturity, and value chain position of the firms. The empirical evidence confirms that public procurement awards significantly increase new lending, with NGEU-funded contracts generating stronger credit expansion than traditional procurement during the recent period. The results show that the impact of NGEU procurement programs aligns closely with historical procurement impacts, with differences driven mainly by lower utilization rates. Moreover, integrating high-frequency financial data with procurement records highlights the potential of Big Data in refining public policy design."
      },
      {
        "id": "oai:arXiv.org:2504.02191v2",
        "title": "A User-Tunable Machine Learning Framework for Step-Wise Synthesis Planning",
        "link": "https://arxiv.org/abs/2504.02191",
        "author": "Shivesh Prakash, Hans-Arno Jacobsen, Viki Kumar Prasad",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02191v2 Announce Type: replace-cross \nAbstract: We introduce MHNpath, a machine learning-driven retrosynthetic tool designed for computer-aided synthesis planning. Leveraging modern Hopfield networks and novel comparative metrics, MHNpath efficiently prioritizes reaction templates, improving the scalability and accuracy of retrosynthetic predictions. The tool incorporates a tunable scoring system that allows users to prioritize pathways based on cost, reaction temperature, and toxicity, thereby facilitating the design of greener and cost-effective reaction routes. We demonstrate its effectiveness through case studies involving complex molecules from ChemByDesign, showcasing its ability to predict novel synthetic and enzymatic pathways. Furthermore, we benchmark MHNpath against existing frameworks, replicating experimentally validated \"gold-standard\" pathways from PaRoutes. Our case studies reveal that the tool can generate shorter, cheaper, moderate-temperature routes employing green solvents, as exemplified by compounds such as dronabinol, arformoterol, and lupinine."
      },
      {
        "id": "oai:arXiv.org:2504.02288v2",
        "title": "FEASE: Shallow AutoEncoding Recommender with Cold Start Handling via Side Features",
        "link": "https://arxiv.org/abs/2504.02288",
        "author": "Edward DongBo Cui, Lu Zhang, William Ping-hsun Lee",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02288v2 Announce Type: replace-cross \nAbstract: User and item cold starts present significant challenges in industrial applications of recommendation systems. Supplementing user-item interaction data with metadata is a common solution-but often at the cost of introducing additional biases. In this work, we introduce an augmented EASE model, i.e. FEASE, that seamlessly integrates both user and item side information to address these cold start issues. Our straightforward, autoencoder-based method produces a closed-form solution that leverages rich content signals for cold items while refining user representations in data-sparse environments. Importantly, our method strikes a balance by effectively recommending cold start items and handling cold start users without incurring extra bias, and it maintains strong performance in warm settings. Experimental results demonstrate improved recommendation accuracy and robustness compared to previous collaborative filtering approaches. Moreover, our model serves as a strong baseline for future comparative studies."
      },
      {
        "id": "oai:arXiv.org:2504.02361v2",
        "title": "MG-Gen: Single Image to Motion Graphics Generation with Layer Decomposition",
        "link": "https://arxiv.org/abs/2504.02361",
        "author": "Takahiro Shirakawa, Tomoyuki Suzuki, Daichi Haraguchi",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02361v2 Announce Type: replace-cross \nAbstract: General image-to-video generation methods often produce suboptimal animations that do not meet the requirements of animated graphics, as they lack active text motion and exhibit object distortion. Also, code-based animation generation methods typically require layer-structured vector data which are often not readily available for motion graphic generation. To address these challenges, we propose a novel framework named MG-Gen that reconstructs data in vector format from a single raster image to extend the capabilities of code-based methods to enable motion graphics generation from a raster image in the framework of general image-to-video generation. MG-Gen first decomposes the input image into layer-wise elements, reconstructs them as HTML format data and then generates executable JavaScript code for the reconstructed HTML data. We experimentally confirm that MG-Gen generates motion graphics while preserving text readability and input consistency. These successful results indicate that combining layer decomposition and animation code generation is an effective strategy for motion graphics generation."
      },
      {
        "id": "oai:arXiv.org:2504.02648v2",
        "title": "Controlled Social Learning: Altruism vs. Bias",
        "link": "https://arxiv.org/abs/2504.02648",
        "author": "Raghu Arghal, Kevin He, Shirin Saeedi Bidokhti, Saswati Sarkar",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02648v2 Announce Type: replace-cross \nAbstract: We introduce a model of controlled sequential social learning in which a planner may pay a cost to adjust the private information structure of agents. The planner may seek to induce correct actions that are consistent with an unknown true state of the world (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates. This sheds light on practical policy questions, such as how the socially optimal level of ad personalization changes according to current beliefs or how a political campaign may selectively illuminate or obfuscate the winning potential of its candidate among voters. We then prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from the choices they induce in the agents. Even for a planner who has equivalent knowledge to an individual, cannot lie or cherry-pick information, and is fully observable, we demonstrate that it is possible to dramatically influence social welfare in both positive and negative directions."
      },
      {
        "id": "oai:arXiv.org:2504.02737v2",
        "title": "RBT4DNN: Requirements-based Testing of Neural Networks",
        "link": "https://arxiv.org/abs/2504.02737",
        "author": "Nusrat Jahan Mozumder, Felipe Toledo, Swaroopa Dola, Matthew B. Dwyer",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02737v2 Announce Type: replace-cross \nAbstract: Deep neural network (DNN) testing is crucial for the reliability and safety of critical systems, where failures can have severe consequences. Although various techniques have been developed to create robustness test suites, requirements-based testing for DNNs remains largely unexplored - yet such tests are recognized as an essential component of software validation of critical systems. In this work, we propose a requirements-based test suite generation method that uses structured natural language requirements formulated in a semantic feature space to create test suites by prompting text-conditional latent diffusion models with the requirement precondition and then using the associated postcondition to define a test oracle to judge outputs of the DNN under test. We investigate the approach using fine-tuned variants of pre-trained generative models. Our experiments on the MNIST, CelebA-HQ, ImageNet, and autonomous car driving datasets demonstrate that the generated test suites are realistic, diverse, consistent with preconditions, and capable of revealing faults."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Mon, 07 Apr 2025 04:02:02 +0000",
      "published": "Mon, 07 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.02988v1",
        "title": "Generating Diverse Audio-Visual 360 Soundscapes for Sound Event Localization and Detection",
        "link": "https://arxiv.org/abs/2504.02988",
        "author": "Adrian S. Roman, Aiden Chang, Gerardo Meza, Iran R. Roman",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02988v1 Announce Type: new \nAbstract: We present SELDVisualSynth, a tool for generating synthetic videos for audio-visual sound event localization and detection (SELD). Our approach incorporates real-world background images to improve realism in synthetic audio-visual SELD data while also ensuring audio-visual spatial alignment. The tool creates 360 synthetic videos where objects move matching synthetic SELD audio data and its annotations. Experimental results demonstrate that a model trained with this data attains performance gains across multiple metrics, achieving superior localization recall (56.4 LR) and competitive localization error (21.9deg LE). We open-source our data generation tool for maximal use by members of the SELD research community."
      },
      {
        "id": "oai:arXiv.org:2504.03289v1",
        "title": "RWKVTTS: Yet another TTS based on RWKV-7",
        "link": "https://arxiv.org/abs/2504.03289",
        "author": "Lin yueyu, Liu Xiao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03289v1 Announce Type: new \nAbstract: Human-AI interaction thrives on intuitive and efficient interfaces, among which voice stands out as a particularly natural and accessible modality. Recent advancements in transformer-based text-to-speech (TTS) systems, such as Fish-Speech, CosyVoice, and MegaTTS 3, have delivered remarkable improvements in quality and realism, driving a significant evolution in the TTS domain. In this paper, we introduce RWKV-7 \\cite{peng2025rwkv}, a cutting-edge RNN-based architecture tailored for TTS applications. Unlike traditional transformer models, RWKV-7 leverages the strengths of recurrent neural networks to achieve greater computational efficiency and scalability, while maintaining high-quality output. Our comprehensive benchmarks demonstrate that RWKV-7 outperforms transformer-based models across multiple key metrics, including synthesis speed, naturalness of speech, and resource efficiency. Furthermore, we explore its adaptability to diverse linguistic contexts and low-resource environments, showcasing its potential to democratize TTS technology. These findings position RWKV-7 as a powerful and innovative alternative, paving the way for more accessible and versatile voice synthesis solutions in real-world applications.Our code and weights are https://github.com/yynil/RWKVTTS, https://huggingface.co/spaces/RWKV-Red-Team"
      },
      {
        "id": "oai:arXiv.org:2504.03329v1",
        "title": "Mind the Prompt: Prompting Strategies in Audio Generations for Improving Sound Classification",
        "link": "https://arxiv.org/abs/2504.03329",
        "author": "Francesca Ronchini, Ho-Hsiang Wu, Wei-Cheng Lin, Fabio Antonacci",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03329v1 Announce Type: new \nAbstract: This paper investigates the design of effective prompt strategies for generating realistic datasets using Text-To-Audio (TTA) models. We also analyze different techniques for efficiently combining these datasets to enhance their utility in sound classification tasks. By evaluating two sound classification datasets with two TTA models, we apply a range of prompt strategies. Our findings reveal that task-specific prompt strategies significantly outperform basic prompt approaches in data generation. Furthermore, merging datasets generated using different TTA models proves to enhance classification performance more effectively than merely increasing the training dataset size. Overall, our results underscore the advantages of these methods as effective data augmentation techniques using synthetic data."
      },
      {
        "id": "oai:arXiv.org:2504.03373v1",
        "title": "An Efficient GPU-based Implementation for Noise Robust Sound Source Localization",
        "link": "https://arxiv.org/abs/2504.03373",
        "author": "Zirui Lin, Masayuki Takigahira, Naoya Terakado, Haris Gulzar, Monikka Roslianna Busto, Takeharu Eda, Katsutoshi Itoyama, Kazuhiro Nakadai, Hideharu Amano",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03373v1 Announce Type: new \nAbstract: Robot audition, encompassing Sound Source Localization (SSL), Sound Source Separation (SSS), and Automatic Speech Recognition (ASR), enables robots and smart devices to acquire auditory capabilities similar to human hearing. Despite their wide applicability, processing multi-channel audio signals from microphone arrays in SSL involves computationally intensive matrix operations, which can hinder efficient deployment on Central Processing Units (CPUs), particularly in embedded systems with limited CPU resources. This paper introduces a GPU-based implementation of SSL for robot audition, utilizing the Generalized Singular Value Decomposition-based Multiple Signal Classification (GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an open-source software suite. For a 60-channel microphone array, the proposed implementation achieves significant performance improvements. On the Jetson AGX Orin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2 64-bit CPUs, we observe speedups of 4645.1x for GSVD calculations and 8.8x for the SSL module, while speedups of 2223.4x for GSVD calculation and 8.95x for the entire SSL module on a server configured with an NVIDIA A100 GPU and AMD EPYC 7352 CPUs, making real-time processing feasible for large-scale microphone arrays and providing ample capacity for real-time processing of potential subsequent machine learning or deep learning tasks."
      },
      {
        "id": "oai:arXiv.org:2504.03546v1",
        "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation",
        "link": "https://arxiv.org/abs/2504.03546",
        "author": "Khai Le-Duc, Tuyen Tran, Bach Phan Tat, Nguyen Kim Hai Bui, Quan Dang, Hung-Phong Tran, Thanh-Thuy Nguyen, Ly Nguyen, Tuan-Minh Phan, Thi Thu Phuong Tran, Chris Ngo, Nguyen X. Khanh, Thanh Nguyen-Tang",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03546v1 Announce Type: cross \nAbstract: Multilingual speech translation (ST) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, Traditional Chinese and Simplified Chinese, together with the models. With 290,000 samples, our dataset is the largest medical machine translation (MT) dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most extensive analysis study in ST research to date, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence (seq2seq) comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https://github.com/leduckhai/MultiMed-ST."
      },
      {
        "id": "oai:arXiv.org:2502.16936v2",
        "title": "Supervised contrastive learning from weakly-labeled audio segments for musical version matching",
        "link": "https://arxiv.org/abs/2502.16936",
        "author": "Joan Serr\\`a, R. Oguz Araz, Dmitry Bogdanov, Yuki Mitsufuji",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16936v2 Announce Type: replace \nAbstract: Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Because of the ground truth nature, existing approaches match musical versions at the track level (e.g., whole song). However, most applications require to match them at the segment level (e.g., 20s chunks). In addition, existing approaches resort to classification and triplet losses, disregarding more recent losses that could bring meaningful improvements. In this paper, we propose a method to learn from weakly annotated segments, together with a contrastive loss variant that outperforms well-studied alternatives. The former is based on pairwise segment distance reductions, while the latter modifies an existing loss following decoupling, hyper-parameter, and geometric considerations. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching."
      },
      {
        "id": "oai:arXiv.org:2404.05659v3",
        "title": "VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain",
        "link": "https://arxiv.org/abs/2404.05659",
        "author": "Khai Le-Duc",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.05659v3 Announce Type: replace-cross \nAbstract: Due to privacy restrictions, there's a shortage of publicly available speech recognition datasets in the medical domain. In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech. To our best knowledge, VietMed is by far the world's largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents. VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration. Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country. Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR. Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%). All code, data and models are made publicly available: https://github.com/leduckhai/MultiMed/tree/master/VietMed."
      },
      {
        "id": "oai:arXiv.org:2406.15888v2",
        "title": "Real-time Speech Summarization for Medical Conversations",
        "link": "https://arxiv.org/abs/2406.15888",
        "author": "Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.15888v2 Announce Type: replace-cross \nAbstract: In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum"
      },
      {
        "id": "oai:arXiv.org:2410.15316v3",
        "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
        "link": "https://arxiv.org/abs/2410.15316",
        "author": "Alan Dao (Gia Tuan Dao), Dinh Bach Vu, Huy Hoang Ha",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15316v3 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models."
      },
      {
        "id": "oai:arXiv.org:2412.16915v2",
        "title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation",
        "link": "https://arxiv.org/abs/2412.16915",
        "author": "Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao",
        "published": "Mon, 07 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.16915v2 Announce Type: replace-cross \nAbstract: Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage http://fadavatar.github.io."
      }
    ]
  }
}