{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Tue, 06 May 2025 04:10:15 +0000",
      "published": "Tue, 06 May 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2505.01427v1",
        "title": "Perturbation Analysis of Singular Values in Concatenated Matrices",
        "link": "https://arxiv.org/abs/2505.01427",
        "author": "Maksym Shamrai",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01427v1 Announce Type: new \nAbstract: Concatenating matrices is a common technique for uncovering shared structures in data through singular value decomposition (SVD) and low-rank approximations. However, a fundamental question arises: how does the singular value spectrum of the concatenated matrix relate to the spectra of its individual components? In this work, we develop a perturbation framework that extends classical results such as Weyl's inequality to concatenated matrices. We establish analytical bounds that quantify the stability of singular values under small perturbations in the submatrices. Our results show that if the matrices being concatenated are close in norm, the dominant singular values of the concatenated matrix remain stable, enabling controlled trade-offs between accuracy and compression. These insights provide a theoretical foundation for improved matrix clustering and compression strategies, with applications in numerical linear algebra, signal processing, and data-driven modeling."
      },
      {
        "id": "oai:arXiv.org:2505.01428v1",
        "title": "Multi-party Collaborative Attention Control for Image Customization",
        "link": "https://arxiv.org/abs/2505.01428",
        "author": "Han Yang, Chuanguang Yang, Qiuli Wang, Zhulin An, Weilun Feng, Libo Huang, Yongjun Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01428v1 Announce Type: new \nAbstract: The rapid advancement of diffusion models has increased the need for customized image generation. However, current customization methods face several limitations: 1) typically accept either image or text conditions alone; 2) customization in complex visual scenarios often leads to subject leakage or confusion; 3) image-conditioned outputs tend to suffer from inconsistent backgrounds; and 4) high computational costs. To address these issues, this paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free method that enables high-quality image customization using both text and complex visual conditions. Specifically, MCA-Ctrl leverages two key operations within the self-attention layer to coordinate multiple parallel diffusion processes and guide the target image generation. This approach allows MCA-Ctrl to capture the content and appearance of specific subjects while maintaining semantic consistency with the conditional input. Additionally, to mitigate subject leakage and confusion issues common in complex visual scenarios, we introduce a Subject Localization Module that extracts precise subject and editable image layers based on user instructions. Extensive quantitative and human evaluation experiments show that MCA-Ctrl outperforms existing methods in zero-shot image customization, effectively resolving the mentioned issues."
      },
      {
        "id": "oai:arXiv.org:2505.01429v1",
        "title": "Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis",
        "link": "https://arxiv.org/abs/2505.01429",
        "author": "Md. Zahid Hossain, Md. Rakibul Islam, Most. Sharmin Sultana Samu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01429v1 Announce Type: new \nAbstract: Since mpox can spread from person to person, it is a zoonotic viral illness that poses a significant public health concern. It is difficult to make an early clinical diagnosis because of how closely its symptoms match those of measles and chickenpox. Medical imaging combined with deep learning (DL) techniques has shown promise in improving disease detection by analyzing affected skin areas. Our study explore the feasibility to train deep learning and vision transformer-based models from scratch with publicly available skin lesion image dataset. Our experimental results show dataset limitation as a major drawback to build better classifier models trained from scratch. We used transfer learning with the help of pre-trained models to get a better classifier. The MobileNet-v2 outperformed other state of the art pre-trained models with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and ResNet-50 also achieved satisfactory performance compared to already available studies with accuracy 92.12% and 86.21% respectively. To further validate the performance of the models, we applied explainable AI techniques."
      },
      {
        "id": "oai:arXiv.org:2505.01430v1",
        "title": "Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models",
        "link": "https://arxiv.org/abs/2505.01430",
        "author": "Muna Numan Said, Aarib Zaidi, Rabia Usman, Sonia Okon, Praneeth Medepalli, Kevin Zhu, Vasu Sharma, Sean O'Brien",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01430v1 Announce Type: new \nAbstract: The transformative potential of text-to-image (T2I) models hinges on their ability to synthesize culturally diverse, photorealistic images from textual prompts. However, these models often perpetuate cultural biases embedded within their training data, leading to systemic misrepresentations. This paper benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate the fidelity of image generation across cultural contexts. Through extensive analysis involving 2,400 images, we quantify biases in terms of compositional fragility and contextual misalignment, revealing significant performance gaps between Western and non-Western cultural prompts. Our findings underscore the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, we provide insights into architectural and data-centric interventions for enhancing cultural inclusivity in AI-generated imagery. This work advances the field by offering a comprehensive tool for diagnosing and mitigating biases in T2I generation, advocating for more equitable AI systems."
      },
      {
        "id": "oai:arXiv.org:2505.01431v1",
        "title": "ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation",
        "link": "https://arxiv.org/abs/2505.01431",
        "author": "Wenqi Guo, Shan Du",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01431v1 Announce Type: new \nAbstract: Camouflaged object segmentation presents unique challenges compared to traditional segmentation tasks, primarily due to the high similarity in patterns and colors between camouflaged objects and their backgrounds. Effective solutions to this problem have significant implications in critical areas such as pest control, defect detection, and lesion segmentation in medical imaging. Prior research has predominantly emphasized supervised or unsupervised pre-training methods, leaving zero-shot approaches significantly underdeveloped. Existing zero-shot techniques commonly utilize the Segment Anything Model (SAM) in automatic mode or rely on vision-language models to generate cues for segmentation; however, their performances remain unsatisfactory, likely due to the similarity of the camouflaged object and the background. Optical flow, commonly utilized for detecting moving objects, has demonstrated effectiveness even with camouflaged entities. Our method integrates optical flow, a vision-language model, and SAM 2 into a sequential pipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding performance improvements, significantly outperforming existing zero-shot methods by raising the F-measure ($F_\\beta^w$) from 0.296 to 0.628. Remarkably, our approach also surpasses supervised methods, increasing the F-measure from 0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset demonstrates an increase in the success rate from 0.628 to 0.697 when compared with FlowSAM, a supervised transfer method. A thorough ablation study further validates the individual contributions of each component. More details can be found on https://github.com/weathon/vcos."
      },
      {
        "id": "oai:arXiv.org:2505.01437v1",
        "title": "Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets",
        "link": "https://arxiv.org/abs/2505.01437",
        "author": "Hassan Wasswa, Timothy Lynar, Hussein Abbass",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01437v1 Announce Type: new \nAbstract: The Internet of Things (IoT) technology has rapidly gained popularity with applications widespread across a variety of industries. However, IoT devices have been recently serving as a porous layer for many malicious attacks to both personal and enterprise information systems with the most famous attacks being botnet-related attacks. The work in this study leveraged Variational Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet effective, models for IoT-botnet detection. The aim is to enhance the detection of minority class attack traffic instances which are often missed by machine learning models. The proposed approach is evaluated on a multi-class problem setting for the detection of traffic categories on highly imbalanced datasets. The performance of two deep learning models including the standard feed forward deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and both recorded commendable results in terms of accuracy, precision, recall and F1-score for all traffic classes."
      },
      {
        "id": "oai:arXiv.org:2505.01438v1",
        "title": "Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials",
        "link": "https://arxiv.org/abs/2505.01438",
        "author": "Tengfei Xing, Xiaodan Ren, Jie Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01438v1 Announce Type: new \nAbstract: Material stress analysis is a critical aspect of material design and performance optimization. Under dynamic loading, the global stress evolution in materials exhibits complex spatiotemporal characteristics, especially in two-phase random materials (TRMs). Such kind of material failure is often associated with stress concentration, and the phase boundaries are key locations where stress concentration occurs. In practical engineering applications, the spatiotemporal resolution of acquired microstructural data and its dynamic stress evolution is often limited. This poses challenges for deep learning methods in generating high-resolution spatiotemporal stress fields, particularly for accurately capturing stress concentration regions. In this study, we propose a framework for global stress generation and spatiotemporal super-resolution in TRMs under dynamic loading. First, we introduce a diffusion model-based approach, named as Spatiotemporal Stress Diffusion (STS-diffusion), for generating global spatiotemporal stress data. This framework incorporates Space-Time U-Net (STU-net), and we systematically investigate the impact of different attention positions on model accuracy. Next, we develop a physics-informed network for spatiotemporal super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning method. The influence of data-driven and physics-informed loss function weights on model accuracy is explored in detail. Benefiting from physics-based constraints, ST-SRPINN requires only low-resolution stress field data during training and can upscale the spatiotemporal resolution of stress fields to arbitrary magnifications."
      },
      {
        "id": "oai:arXiv.org:2505.01440v1",
        "title": "Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving",
        "link": "https://arxiv.org/abs/2505.01440",
        "author": "Alkis Sygkounas, Ioannis Athanasiadis, Andreas Persson, Michael Felsberg, Amy Loutfi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01440v1 Announce Type: new \nAbstract: Integrating human expertise with machine learning is crucial for applications demanding high accuracy and safety, such as autonomous driving. This study introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop (HITL) approach that enhances Reinforcement Learning (RL) by merging human insights directly into the RL training process, improving model performance. Our proposed iDDQN method modifies the Q-value update equation to integrate human and agent actions, establishing a collaborative approach for policy development. Additionally, we present an offline evaluative framework that simulates the agent's trajectory as if no human intervention had occurred, to assess the effectiveness of human interventions. Empirical results in simulated autonomous driving scenarios demonstrate that iDDQN outperforms established approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for improving performance and adaptability."
      },
      {
        "id": "oai:arXiv.org:2505.01445v1",
        "title": "Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding",
        "link": "https://arxiv.org/abs/2505.01445",
        "author": "Muhammad Muaz, Sameed Sajid, Tobias Schulze, Chang Liu, Nils Klasen, Benny Drescher",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01445v1 Announce Type: new \nAbstract: If a product deviates from its desired properties in the injection moulding process, its root cause analysis can be aided by models that relate the input machine settings with the output quality characteristics. The machine learning models tested in the quality prediction are mostly black boxes; therefore, no direct explanation of their prognosis is given, which restricts their applicability in the quality control. The previously attempted explainability methods are either restricted to tree-based algorithms only or do not emphasize on the fact that some explainability methods can lead to wrong root cause identification of a product's deviation from its desired properties. This study first shows that the interactions among the multiple input machine settings do exist in real experimental data collected as per a central composite design. Then, the model-agnostic explainable AI methods are compared for the first time to show that different explainability methods indeed lead to different feature impact analysis in injection moulding. Moreover, it is shown that the better feature attribution translates to the correct cause identification and actionable insights for the injection moulding process. Being model agnostic, explanations on both random forest and multilayer perceptron are performed for the cause analysis, as both models have the mean absolute percentage error of less than 0.05% on the experimental dataset."
      },
      {
        "id": "oai:arXiv.org:2505.01448v1",
        "title": "OpenAVS: Training-Free Open-Vocabulary Audio Visual Segmentation with Foundational Models",
        "link": "https://arxiv.org/abs/2505.01448",
        "author": "Shengkai Chen, Yifang Yin, Jinming Cao, Shili Xiang, Zhenguang Liu, Roger Zimmermann",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01448v1 Announce Type: new \nAbstract: Audio-visual segmentation aims to separate sounding objects from videos by predicting pixel-level masks based on audio signals. Existing methods primarily concentrate on closed-set scenarios and direct audio-visual alignment and fusion, which limits their capability to generalize to new, unseen situations. In this paper, we propose OpenAVS, a novel training-free language-based approach that, for the first time, effectively aligns audio and visual modalities using text as a proxy for open-vocabulary Audio-Visual Segmentation (AVS). Equipped with multimedia foundation models, OpenAVS directly infers masks through 1) audio-to-text prompt generation, 2) LLM-guided prompt translation, and 3) text-to-visual sounding object segmentation. The objective of OpenAVS is to establish a simple yet flexible architecture that relies on the most appropriate foundation models by fully leveraging their capabilities to enable more effective knowledge transfer to the downstream AVS task. Moreover, we present a model-agnostic framework OpenAVS-ST that enables the integration of OpenAVS with any advanced supervised AVS model via pseudo-label based self-training. This approach enhances performance by effectively utilizing large-scale unlabeled data when available. Comprehensive experiments on three benchmark datasets demonstrate the superior performance of OpenAVS. It surpasses existing unsupervised, zero-shot, and few-shot AVS methods by a significant margin, achieving absolute performance gains of approximately 9.4% and 10.9% in mIoU and F-score, respectively, in challenging scenarios."
      },
      {
        "id": "oai:arXiv.org:2505.01449v1",
        "title": "COSMOS: Predictable and Cost-Effective Adaptation of LLMs",
        "link": "https://arxiv.org/abs/2505.01449",
        "author": "Jiayu Wang, Aws Albarghouthi, Frederic Sala",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01449v1 Announce Type: new \nAbstract: Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards."
      },
      {
        "id": "oai:arXiv.org:2505.01450v1",
        "title": "Towards Film-Making Production Dialogue, Narration, Monologue Adaptive Moving Dubbing Benchmarks",
        "link": "https://arxiv.org/abs/2505.01450",
        "author": "Chaoyi Wang, Junjie Zheng, Zihao Chen, Shiyu Xia, Chaofan Ding, Xiaohao Zhang, Xi Tao, Xiaoming He, Xinhan Di",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01450v1 Announce Type: new \nAbstract: Movie dubbing has advanced significantly, yet assessing the real-world effectiveness of these models remains challenging. A comprehensive evaluation benchmark is crucial for two key reasons: 1) Existing metrics fail to fully capture the complexities of dialogue, narration, monologue, and actor adaptability in movie dubbing. 2) A practical evaluation system should offer valuable insights to improve movie dubbing quality and advancement in film production. To this end, we introduce Talking Adaptive Dubbing Benchmarks (TA-Dubbing), designed to improve film production by adapting to dialogue, narration, monologue, and actors in movie dubbing. TA-Dubbing offers several key advantages: 1) Comprehensive Dimensions: TA-Dubbing covers a variety of dimensions of movie dubbing, incorporating metric evaluations for both movie understanding and speech generation. 2) Versatile Benchmarking: TA-Dubbing is designed to evaluate state-of-the-art movie dubbing models and advanced multi-modal large language models. 3) Full Open-Sourcing: We fully open-source TA-Dubbing at https://github.com/woka- 0a/DeepDubber- V1 including all video suits, evaluation methods, annotations. We also continuously integrate new movie dubbing models into the TA-Dubbing leaderboard at https://github.com/woka- 0a/DeepDubber-V1 to drive forward the field of movie dubbing."
      },
      {
        "id": "oai:arXiv.org:2505.01456v1",
        "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation",
        "link": "https://arxiv.org/abs/2505.01456",
        "author": "Vaidehi Patil, Yi-Lin Sung, Peter Hase, Jie Peng, Tianlong Chen, Mohit Bansal",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01456v1 Announce Type: new \nAbstract: LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs."
      },
      {
        "id": "oai:arXiv.org:2505.01459v1",
        "title": "MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling",
        "link": "https://arxiv.org/abs/2505.01459",
        "author": "Abdoul Majid O. Thiombiano, Brahim Hnich, Ali Ben Mrad, Mohamed Wiem Mkaouer",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01459v1 Announce Type: new \nAbstract: This paper introduces MoxE, a novel architecture that synergistically combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of Experts (MoE) framework to address critical scalability and efficiency challenges in large language models (LLMs). The proposed method effectively leverages xLSTM's innovative memory structures while strategically introducing sparsity through MoE to substantially reduce computational overhead. At the heart of our approach is a novel entropy-based routing mechanism, designed to dynamically route tokens to specialized experts, thereby ensuring efficient and balanced resource utilization. This entropy awareness enables the architecture to effectively manage both rare and common tokens, with mLSTM blocks being favored to handle rare tokens. To further enhance generalization, we introduce a suite of auxiliary losses, including entropy-based and group-wise balancing losses, ensuring robust performance and efficient training. Theoretical analysis and empirical evaluations rigorously demonstrate that MoxE achieves significant efficiency gains and enhanced effectiveness compared to existing approaches, marking a notable advancement in scalable LLM architectures."
      },
      {
        "id": "oai:arXiv.org:2505.01479v1",
        "title": "SymPlanner: Deliberate Planning in Language Models with Symbolic Representation",
        "link": "https://arxiv.org/abs/2505.01479",
        "author": "Siheng Xiong, Jieyu Zhou, Zhangding Liu, Yusen Su",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01479v1 Announce Type: new \nAbstract: Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines."
      },
      {
        "id": "oai:arXiv.org:2505.01481v1",
        "title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos",
        "link": "https://arxiv.org/abs/2505.01481",
        "author": "Zongxia Li, Xiyang Wu, Yubin Qin, Guangyao Shi, Hongyang Du, Dinesh Manocha, Tianyi Zhou, Jordan Lee Boyd-Graber",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01481v1 Announce Type: new \nAbstract: Synthetic video generation with foundation models has gained attention for its realism and wide applications. While these models produce high-quality frames, they often fail to respect common sense and physical laws, resulting in abnormal content. Existing metrics like VideoScore emphasize general quality but ignore such violations and lack interpretability. A more insightful approach is using multi-modal large language models (MLLMs) as interpretable evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities in synthetic videos remains underexplored. To address this, we introduce VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora, and Kling, paired with expert-designed QA tasks solvable via human-level reasoning across various categories. We assess several SoTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat, these models still hallucinate on basic commonsense and physics tasks in synthetic settings, underscoring the challenge of hallucination. We further fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real and synthetic commonsense/physics data. Results show notable accuracy gains, especially with counterexample integration, advancing MLLMs' reasoning capabilities. Our data is available at https://github.com/zli12321/VideoHallu."
      },
      {
        "id": "oai:arXiv.org:2505.01488v1",
        "title": "Explainable Machine Learning for Cyberattack Identification from Traffic Flows",
        "link": "https://arxiv.org/abs/2505.01488",
        "author": "Yujing Zhou, Marc L. Jacquet, Robel Dawit, Skyler Fabre, Dev Sarawat, Faheem Khan, Madison Newell, Yongxin Liu, Dahai Liu, Hongyun Chen, Jian Wang, Huihui Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01488v1 Announce Type: new \nAbstract: The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems."
      },
      {
        "id": "oai:arXiv.org:2505.01489v1",
        "title": "Machine Learning for Cyber-Attack Identification from Traffic Flows",
        "link": "https://arxiv.org/abs/2505.01489",
        "author": "Yujing Zhou, Marc L. Jacquet, Robel Dawit, Skyler Fabre, Dev Sarawat, Faheem Khan, Madison Newell, Yongxin Liu, Dahai Liu, Hongyun Chen, Jian Wang, Huihui Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01489v1 Announce Type: new \nAbstract: This paper presents our simulation of cyber-attacks and detection strategies on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual machines and the OPNSense firewall, along with traffic dynamics from SUMO and exploitation via the Metasploit framework. We try to answer the research questions: are we able to identify cyber attacks by only analyzing traffic flow patterns. In this research, the cyber attacks are focused particularly when lights are randomly turned all green or red at busy intersections by adversarial attackers. Despite challenges stemming from imbalanced data and overlapping traffic patterns, our best model shows 85\\% accuracy when detecting intrusions purely using traffic flow statistics. Key indicators for successful detection included occupancy, jam length, and halting durations."
      },
      {
        "id": "oai:arXiv.org:2505.01490v1",
        "title": "WorldGenBench: A World-Knowledge-Integrated Benchmark for Reasoning-Driven Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2505.01490",
        "author": "Daoan Zhang, Che Jiang, Ruoshi Xu, Biaoxiang Chen, Zijian Jin, Yutian Lu, Jianguo Zhang, Liang Yong, Jiebo Luo, Shengda Luo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01490v1 Announce Type: new \nAbstract: Recent advances in text-to-image (T2I) generation have achieved impressive results, yet existing models still struggle with prompts that require rich world knowledge and implicit reasoning: both of which are critical for producing semantically accurate, coherent, and contextually appropriate images in real-world scenarios. To address this gap, we introduce \\textbf{WorldGenBench}, a benchmark designed to systematically evaluate T2I models' world knowledge grounding and implicit inferential capabilities, covering both the humanities and nature domains. We propose the \\textbf{Knowledge Checklist Score}, a structured metric that measures how well generated images satisfy key semantic expectations. Experiments across 21 state-of-the-art models reveal that while diffusion models lead among open-source methods, proprietary auto-regressive models like GPT-4o exhibit significantly stronger reasoning and knowledge integration. Our findings highlight the need for deeper understanding and inference capabilities in next-generation T2I systems. Project Page: \\href{https://dwanzhang-ai.github.io/WorldGenBench/}{https://dwanzhang-ai.github.io/WorldGenBench/}"
      },
      {
        "id": "oai:arXiv.org:2505.01523v1",
        "title": "Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation",
        "link": "https://arxiv.org/abs/2505.01523",
        "author": "Madhav Kotecha, Vijendra Kumar Vaishya, Smita Gautam, Suraj Racha",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01523v1 Announce Type: new \nAbstract: We propose a refined approach to efficiently fine-tune large language models (LLMs) on specific domains like the mathematical domain by employing a budgeted subset selection method. Our approach combines utility and diversity metrics to select the most informative and representative training examples. The final goal is to achieve near-full dataset performance with meticulously selected data points from the entire dataset while significantly reducing computational cost and training time and achieving competitive performance as the full dataset. The utility metric incorporates both perplexity and Chain-of-Thought (CoT) loss to identify challenging examples that contribute most to model learning, while the diversity metric ensures broad coverage across mathematical subdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing against several baseline approaches, including random selection, diversity-based sampling, and existing state-of-the-art subset selection techniques."
      },
      {
        "id": "oai:arXiv.org:2505.01530v1",
        "title": "Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer",
        "link": "https://arxiv.org/abs/2505.01530",
        "author": "Muhammad Tayyab Khan, Zane Yong, Lequn Chen, Jun Ming Tan, Wenhe Feng, Seung Ki Moon",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01530v1 Announce Type: new \nAbstract: Accurate extraction of key information from 2D engineering drawings is crucial for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional Optical Character Recognition (OCR) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. To address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (OBB) detection model with a transformer-based document parsing model (Donut). An in-house annotated dataset is used to train YOLOv11 for detecting nine key categories: Geometric Dimensioning and Tolerancing (GD&amp;T), General Tolerances, Measures, Materials, Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are cropped into images and labeled to fine-tune Donut for structured JSON output. Fine-tuning strategies include a single model trained across all categories and category-specific models. Results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for GD&amp;T), recall (100% for most), and F1 score (97.3%), while reducing hallucination (5.23%). The proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries."
      },
      {
        "id": "oai:arXiv.org:2505.01548v1",
        "title": "Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation",
        "link": "https://arxiv.org/abs/2505.01548",
        "author": "Zhen Yao, Xiaowen Ying, Mooi Choo Chuah",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01548v1 Announce Type: new \nAbstract: Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet."
      },
      {
        "id": "oai:arXiv.org:2505.01557v1",
        "title": "Contextures: Representations from Contexts",
        "link": "https://arxiv.org/abs/2505.01557",
        "author": "Runtian Zhai, Kai Yang, Che-Ping Tsai, Burak Varici, Zico Kolter, Pradeep Ravikumar",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01557v1 Announce Type: new \nAbstract: Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms -- supervised, self-supervised, and manifold learning -- can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets."
      },
      {
        "id": "oai:arXiv.org:2505.01558v1",
        "title": "A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning",
        "link": "https://arxiv.org/abs/2505.01558",
        "author": "Anan Yaghmour, Melba M. Crawford, Saurabh Prasad",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01558v1 Announce Type: new \nAbstract: Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation."
      },
      {
        "id": "oai:arXiv.org:2505.01559v1",
        "title": "On the effectiveness of Large Language Models in the mechanical design domain",
        "link": "https://arxiv.org/abs/2505.01559",
        "author": "Daniele Grandi, Fabian Riquelme",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01559v1 Announce Type: new \nAbstract: In this work, we seek to understand the performance of large language models in the mechanical engineering domain. We leverage the semantic data found in the ABC dataset, specifically the assembly names that designers assigned to the overall assemblies, and the individual semantic part names that were assigned to each part. After pre-processing the data we developed two unsupervised tasks to evaluate how different model architectures perform on domain-specific data: a binary sentence-pair classification task and a zero-shot classification task. We achieved a 0.62 accuracy for the binary sentence-pair classification task with a fine-tuned model that focuses on fighting over-fitting: 1) modifying learning rates, 2) dropout values, 3) Sequence Length, and 4) adding a multi-head attention layer. Our model on the zero-shot classification task outperforms the baselines by a wide margin, and achieves a top-1 classification accuracy of 0.386. The results shed some light on the specific failure modes that arise when learning from language in this domain."
      },
      {
        "id": "oai:arXiv.org:2505.01560v1",
        "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains",
        "link": "https://arxiv.org/abs/2505.01560",
        "author": "Vicent Briva Iglesias, Gokhan Dogru",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01560v1 Announce Type: new \nAbstract: Large language models (LLMs) and multi-agent orchestration are touted as the next leap in machine translation (MT), but their benefits relative to conventional neural MT (NMT) remain unclear. This paper offers an empirical reality check. We benchmark five paradigms, Google Translate (strong NMT baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM), and two GPT-4o-powered agentic workflows (sequential three-stage and iterative refinement), on test data drawn from a legal contract and news prose in three English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with expert ratings of adequacy and fluency; efficiency with total input-plus-output token counts mapped to April 2025 pricing.\n  Automatic scores still favour the mature NMT system, which ranks first in seven of twelve metric-language combinations; o1-preview ties or places second in most remaining cases, while both multi-agent workflows trail. Human evaluation reverses part of this narrative: o1-preview produces the most adequate and fluent output in five of six comparisons, and the iterative agent edges ahead once, indicating that reasoning layers capture semantic nuance undervalued by surface metrics. Yet these qualitative gains carry steep costs. The sequential agent consumes roughly five times, and the iterative agent fifteen times, the tokens used by NMT or single-pass LLMs.\n  We advocate multidimensional, cost-aware evaluation protocols and highlight research directions that could tip the balance: leaner coordination strategies, selective agent activation, and hybrid pipelines combining single-pass LLMs with targeted agent intervention."
      },
      {
        "id": "oai:arXiv.org:2505.01571v1",
        "title": "PainFormer: a Vision Foundation Model for Automatic Pain Assessment",
        "link": "https://arxiv.org/abs/2505.01571",
        "author": "Stefanos Gkikas, Raul Fernandez Rojas, Manolis Tsiknakis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01571v1 Announce Type: new \nAbstract: Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities-including RGB, synthetic thermal, and estimated depth videos-and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 73 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment."
      },
      {
        "id": "oai:arXiv.org:2505.01578v1",
        "title": "Grounding Task Assistance with Multimodal Cues from a Single Demonstration",
        "link": "https://arxiv.org/abs/2505.01578",
        "author": "Gabriel Sarch, Balasaravanan Thoravi Kumaravel, Sahithya Ravi, Vibhav Vineet, Andrew D. Wilson",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01578v1 Announce Type: new \nAbstract: A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance."
      },
      {
        "id": "oai:arXiv.org:2505.01583v1",
        "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action",
        "link": "https://arxiv.org/abs/2505.01583",
        "author": "Jen-Hao Cheng, Vivian Wang, Huayu Wang, Huapeng Zhou, Yi-Hao Peng, Hou-I Liu, Hsiang-Wei Huang, Kuang-Ming Chen, Cheng-Yen Yang, Wenhao Chai, Yi-Ling Chen, Vibhav Vineet, Qin Cai, Jenq-Neng Hwang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01583v1 Announce Type: new \nAbstract: Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding."
      },
      {
        "id": "oai:arXiv.org:2505.01584v1",
        "title": "Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation",
        "link": "https://arxiv.org/abs/2505.01584",
        "author": "Zhiqiang He, Zhi Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01584v1 Announce Type: new \nAbstract: Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions."
      },
      {
        "id": "oai:arXiv.org:2505.01591v1",
        "title": "Machine Learning Fairness in House Price Prediction: A Case Study of America's Expanding Metropolises",
        "link": "https://arxiv.org/abs/2505.01591",
        "author": "Abdalwahab Almajed, Maryam Tabar, Peyman Najafirad",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01591v1 Announce Type: new \nAbstract: As a basic human need, housing plays a key role in enhancing health, well-being, and educational outcome in society, and the housing market is a major factor for promoting quality of life and ensuring social equity. To improve the housing conditions, there has been extensive research on building Machine Learning (ML)-driven house price prediction solutions to accurately forecast the future conditions, and help inform actions and policies in the field. In spite of their success in developing high-accuracy models, there is a gap in our understanding of the extent to which various ML-driven house price prediction approaches show ethnic and/or racial bias, which in turn is essential for the responsible use of ML, and ensuring that the ML-driven solutions do not exacerbate inequity. To fill this gap, this paper develops several ML models from a combination of structural and neighborhood-level attributes, and conducts comprehensive assessments on the fairness of ML models under various definitions of privileged groups. As a result, it finds that the ML-driven house price prediction models show various levels of bias towards protected attributes (i.e., race and ethnicity in this study). Then, it investigates the performance of different bias mitigation solutions, and the experimental results show their various levels of effectiveness on different ML-driven methods. However, in general, the in-processing bias mitigation approach tends to be more effective than the pre-processing one in this problem domain. Our code is available at https://github.com/wahab1412/housing_fairness."
      },
      {
        "id": "oai:arXiv.org:2505.01592v1",
        "title": "PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents",
        "link": "https://arxiv.org/abs/2505.01592",
        "author": "Takyoung Kim, Janvijay Singh, Shuhaib Mehri, Emre Can Acikgoz, Sagnik Mukherjee, Nimet Beyza Bozdag, Sumuk Shashidhar, Gokhan Tur, Dilek Hakkani-T\\\"ur",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01592v1 Announce Type: new \nAbstract: The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose PIPA, a unified evaluation protocol that conceptualizes the behavioral process of interactive task planning agents within a partially observable Markov Decision Process (POMDP) paradigm. The proposed protocol offers a comprehensive assessment of agent performance through a set of atomic evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent's decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning."
      },
      {
        "id": "oai:arXiv.org:2505.01595v1",
        "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation",
        "link": "https://arxiv.org/abs/2505.01595",
        "author": "Liaoyaqi Wang, Zhengping Jiang, Anqi Liu, Benjamin Van Durme",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01595v1 Announce Type: new \nAbstract: We present a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context. Recent advances in large language models (LLMs) have significantly enhanced their reasoning capabilities, particularly on well-defined tasks with complete information. However, LLMs continue to struggle with making accurate and well-calibrated probabilistic predictions under uncertainty or partial information. While incorporating uncertainty into model predictions often boosts performance, obtaining reliable estimates of that uncertainty remains understudied. In particular, LLM probability estimates tend to be coarse and biased towards more frequent numbers. Through a combination of human and synthetic data creation and assessment, scaling to larger models, and better supervision, we propose a set of strong and precise probability estimation models. We conduct systematic evaluations across tasks that rely on conditional probability estimation and show that our approach consistently outperforms existing fine-tuned and prompting-based methods by a large margin."
      },
      {
        "id": "oai:arXiv.org:2505.01615v1",
        "title": "Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation",
        "link": "https://arxiv.org/abs/2505.01615",
        "author": "Dimitrios Dagdilelis, Panagiotis Grigoriadis, Roberto Galeazzi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01615v1 Announce Type: new \nAbstract: We propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. The model deeply fuses multiview RGB and long wave infrared images with sparse LiDAR point clouds. Training also integrates X band radar and electronic chart data to inform predictions. The resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. Real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings."
      },
      {
        "id": "oai:arXiv.org:2505.01618v1",
        "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers",
        "link": "https://arxiv.org/abs/2505.01618",
        "author": "Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, Joel Hestness",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01618v1 Announce Type: new \nAbstract: We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the unique parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34\\% compute efficiency improvements over the prior state-of-the-art."
      },
      {
        "id": "oai:arXiv.org:2505.01619v1",
        "title": "Skill-based Safe Reinforcement Learning with Risk Planning",
        "link": "https://arxiv.org/abs/2505.01619",
        "author": "Hanping Zhang, Yuhong Guo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01619v1 Announce Type: new \nAbstract: Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods."
      },
      {
        "id": "oai:arXiv.org:2505.01627v1",
        "title": "A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components",
        "link": "https://arxiv.org/abs/2505.01627",
        "author": "Fatemeh Elhambakhsh, Daniele Grandi, Hyunwoong Ko",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01627v1 Announce Type: new \nAbstract: The conceptual design phase represents a critical early stage in the product development process, where designers generate potential solutions that meet predefined design specifications based on functional requirements. Functional modeling, a foundational aspect of this phase, enables designers to reason about product functions before specific structural details are determined. A widely adopted approach to functional modeling is the Function-Behavior-Structure (FBS) framework, which supports the transformation of functional intent into behavioral and structural descriptions. However, the effectiveness of function-based design is often hindered by the lack of well-structured and comprehensive functional data. This scarcity can negatively impact early design decision-making and hinder the development of accurate behavioral models. Recent advances in Large Language Models (LLMs), such as those based on GPT architectures, offer a promising avenue to address this gap. LLMs have demonstrated significant capabilities in language understanding and natural language processing (NLP), making them suitable for automated classification tasks. This study proposes a novel LLM-based domain adaptation (DA) framework using fine-tuning for the automated classification of mechanical assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the traditionally manual and subjective process of function annotation can be improved in both accuracy and consistency. A case study demonstrates fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the domain-adapted LLM can generate high-quality functional data, enhancing the semantic representation of mechanical parts and supporting more effective design exploration in early-phase engineering."
      },
      {
        "id": "oai:arXiv.org:2505.01650v1",
        "title": "Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability",
        "link": "https://arxiv.org/abs/2505.01650",
        "author": "Wenxuan Zhang, Peng Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01650v1 Announce Type: new \nAbstract: The rapid expansion of advanced low-Earth orbit (LEO) satellites in large constellations is positioning space assets as key to the future, enabling global internet access and relay systems for deep space missions. A solution to the challenge is effective space object detection (SOD) for collision assessment and avoidance. In SOD, an LEO satellite must detect other satellites and objects with high precision and minimal delay. This paper investigates the feasibility and effectiveness of employing vision sensors for SOD tasks based on deep learning (DL) models. It introduces models based on the Squeeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the Generalized Efficient Layer Aggregation Network (GELAN) and evaluates their performance under SOD scenarios. Experimental results show that the proposed models achieve mean average precision at intersection over union threshold 0.5 (mAP50) scores of up to 0.751 and mean average precision averaged over intersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to 0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model increases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from 0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to 5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\\%."
      },
      {
        "id": "oai:arXiv.org:2505.01652v1",
        "title": "Causally Fair Node Classification on Non-IID Graph Data",
        "link": "https://arxiv.org/abs/2505.01652",
        "author": "Yucong Dai, Lu Zhang, Yaowei Hu, Susan Gauch, Yongkai Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01652v1 Announce Type: new \nAbstract: Fair machine learning seeks to identify and mitigate biases in predictions against unfavorable populations characterized by demographic attributes, such as race and gender. Recently, a few works have extended fairness to graph data, such as social networks, but most of them neglect the causal relationships among data instances. This paper addresses the prevalent challenge in fairness-aware ML algorithms, which typically assume Independent and Identically Distributed (IID) data. We tackle the overlooked domain of non-IID, graph-based settings where data instances are interconnected, influencing the outcomes of fairness interventions. We base our research on the Network Structural Causal Model (NSCM) framework and posit two main assumptions: Decomposability and Graph Independence, which enable the computation of interventional distributions in non-IID settings using the $do$-calculus. Based on that, we develop the Message Passing Variational Autoencoder for Causal Inference (MPVA) to compute interventional distributions and facilitate causally fair node classification through estimated interventional distributions. Empirical evaluations on semi-synthetic and real-world datasets demonstrate that MPVA outperforms conventional methods by effectively approximating interventional distributions and mitigating bias. The implications of our findings underscore the potential of causality-based fairness in complex ML applications, setting the stage for further research into relaxing the initial assumptions to enhance model fairness."
      },
      {
        "id": "oai:arXiv.org:2505.01656v1",
        "title": "A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory",
        "link": "https://arxiv.org/abs/2505.01656",
        "author": "Chenyang Fan, Xujie Zhu, Taige Luo, Sheng Xu, Zhulin Chen, Hongxin Yang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01656v1 Announce Type: new \nAbstract: The pattern analysis of tree structure holds significant scientific value for genetic breeding and forestry management. The current trunk and branch extraction technologies are mainly LiDAR-based or UAV-based. The former approaches obtain high-precision 3D data, but its equipment cost is high and the three-dimensional (3D) data processing is complex. The latter approaches efficiently capture canopy information, but they miss the 3-D structure of trees. In order to deal with the branch information extraction from the complex background interference and occlusion, this work proposes a novel WaveInst instance segmentation framework, involving a discrete wavelet transform, to enhance multi-scale edge information for accurately improving tree structure extraction. Experimental results of the proposed model show superior performance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset. Moreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated to extract tree structure and pattern analysis from artificial forest. The proposed method achieves a mean average precision of 49.6 and 24.3 for the structure extraction of mature and juvenile trees, respectively, surpassing the existing state-of-the-art method by 9.9. Furthermore, by in tegrating the segmentation model within the regression model, we accurately achieve significant tree grown parameters, such as the location of trees, the diameter-at-breast-height of individual trees, and the plant height, from 2D images directly. This study provides a scientific and plenty of data for tree structure analysis in related to the phenotype research, offering a platform for the significant applications in precision forestry, ecological monitoring, and intelligent breeding."
      },
      {
        "id": "oai:arXiv.org:2505.01658v1",
        "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency",
        "link": "https://arxiv.org/abs/2505.01658",
        "author": "Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01658v1 Announce Type: new \nAbstract: Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine"
      },
      {
        "id": "oai:arXiv.org:2505.01660v1",
        "title": "Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification",
        "link": "https://arxiv.org/abs/2505.01660",
        "author": "Sicong Li, Qianqian Xu, Zhiyong Yang, Zitai Wang, Linchao Zhang, Xiaochun Cao, Qingming Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01660v1 Announce Type: new \nAbstract: Real-world datasets often follow a long-tailed distribution, making generalization to tail classes difficult. Recent methods resorted to long-tail variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to improve generalization by flattening the loss landscape. However, these attempts face a trade-off between computational efficiency and control over the loss landscape. On the one hand, ImbSAM is efficient but offers only coarse control as it excludes head classes from the SAM process. On the other hand, CC-SAM provides fine-grained control through class-dependent perturbations but at the cost of efficiency due to multiple backpropagations. Seeing this dilemma, we introduce Focal-SAM, which assigns different penalties to class-wise sharpness, achieving fine-grained control without extra backpropagations, thus maintaining efficiency. Furthermore, we theoretically analyze Focal-SAM's generalization ability and derive a sharper generalization bound. Extensive experiments on both traditional and foundation models validate the effectiveness of Focal-SAM."
      },
      {
        "id": "oai:arXiv.org:2505.01664v1",
        "title": "Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation",
        "link": "https://arxiv.org/abs/2505.01664",
        "author": "Yi-Ming Zhai, Chuan-Xian Ren, Hong Yan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01664v1 Announce Type: new \nAbstract: Visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. Partial domain adaptation (PDA) is a general and practical scenario in which the target label space is a subset of the source one. The challenges of PDA exist due to not only domain shift but also the non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual Optimal Transport (SSOT) method is proposed to deal with the PDA problem. Specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. A soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. To deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized Kantorovich problem is employed since it can be optimized by gradient-based algorithms. Further, a neural network is exploited to approximate the Kantorovich potential due to its strong fitting ability. This network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. The SSOT model is built upon neural networks, which can be optimized alternately in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of SSOT."
      },
      {
        "id": "oai:arXiv.org:2505.01665v1",
        "title": "Adaptively Point-weighting Curriculum Learning",
        "link": "https://arxiv.org/abs/2505.01665",
        "author": "Wensheng Li, Hao Wang, Ruifeng Zhou, Hanting Guan, Chao Zhang, Dacheng Tao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01665v1 Announce Type: new \nAbstract: Curriculum learning (CL) is referred to as a training strategy that makes easy samples learned first and then fits hard samples. It imitates the process of humans learning knowledge, and has become a potential manner of effectively training deep networks. In this study, we develop the adaptively point-weighting (APW) curriculum learning algorithm, which adaptively assigns the weight to every training sample not only based on its training error but also considering the current training state of the network. Specifically, in the early training phase, it increases the weights of easy samples to make the network rapidly capture the overall characteristics of the dataset; and in the later training phase, the weights of hard points rise to improve the fitting performance on the discrete local regions. Moreover, we also present the theoretical analysis on the properties of APW including training effectiveness, training feasibility, training stability, and generalization performance. The numerical experiments support the superiority of APW and demonstrate the validity of our theoretical findings."
      },
      {
        "id": "oai:arXiv.org:2505.01680v1",
        "title": "Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study",
        "link": "https://arxiv.org/abs/2505.01680",
        "author": "Tamim Ahmed, Thanassis Rikakis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01680v1 Announce Type: new \nAbstract: Manual scoring of the Action Research Arm Test (ARAT) for upper extremity assessment in stroke rehabilitation is time-intensive and variable. We propose an automated ARAT scoring system integrating multimodal video analysis with SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object locations. Our approach employs multi-view data (ipsilateral, contralateral, and top perspectives), applying early and late fusion to combine features across views and models. Hierarchical Bayesian Models (HBMs) infer movement quality components, enhancing interpretability. A clinician dashboard displays task scores, execution times, and quality assessments. We conducted a study with five clinicians who reviewed 500 video ratings generated by our system, providing feedback on its accuracy and usability. Evaluated on a stroke rehabilitation dataset, our framework achieves 89.0% validation accuracy with late fusion, with HBMs aligning closely with manual assessments. This work advances automated rehabilitation by offering a scalable, interpretable solution with clinical validation."
      },
      {
        "id": "oai:arXiv.org:2505.01693v1",
        "title": "High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers",
        "link": "https://arxiv.org/abs/2505.01693",
        "author": "Brian Wong, Kaito Tanaka",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01693v1 Announce Type: new \nAbstract: Automated labeling of chest X-ray reports is essential for enabling downstream tasks such as training image-based diagnostic models, population health studies, and clinical decision support. However, the high variability, complexity, and prevalence of negation and uncertainty in these free-text reports pose significant challenges for traditional Natural Language Processing methods. While large language models (LLMs) demonstrate strong text understanding, their direct application for large-scale, efficient labeling is limited by computational cost and speed. This paper introduces DeBERTa-RAD, a novel two-stage framework that combines the power of state-of-the-art LLM pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast chest X-ray report labeling. We leverage an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming established rule-based systems, fine-tuned transformer models, and direct LLM inference, while maintaining a practical inference speed suitable for high-throughput applications. Our analysis shows particular strength in handling uncertain findings. This work demonstrates a promising path to overcome data annotation bottlenecks and achieve high-performance medical text processing through the strategic combination of LLM capabilities and efficient student models trained via distillation."
      },
      {
        "id": "oai:arXiv.org:2505.01694v1",
        "title": "Topology-Aware CLIP Few-Shot Learning",
        "link": "https://arxiv.org/abs/2505.01694",
        "author": "Dazhi Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01694v1 Announce Type: new \nAbstract: Efficiently adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. Existing methods often overlook valuable structural information within the VLM's latent space. We introduce a topology-aware tuning approach integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework. By explicitly aligning the topological structures of visual and text representations using a combined RTD and Cross-Entropy loss, while freezing base VLM encoders, our method enhances few-shot performance. We optimize only lightweight Task Residual parameters, effectively leveraging topological information. Across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\\% over relevant baseline methods in few-shot settings. This work presents an effective strategy to boost VLM few-shot capabilities by incorporating topological alignment."
      },
      {
        "id": "oai:arXiv.org:2505.01698v1",
        "title": "Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs",
        "link": "https://arxiv.org/abs/2505.01698",
        "author": "Yuying Zhao, Yu Wang, Xueqi Cheng, Anne Marie Tumlin, Yunchao Liu, Damin Xia, Meng Jiang, Tyler Derr",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01698v1 Announce Type: new \nAbstract: The remarkable advancements in Large Language Models (LLMs) have revolutionized the content generation process in social media, offering significant convenience in writing tasks. However, existing applications, such as sentence completion and fluency enhancement, do not fully address the complex challenges in real-world social media contexts. A prevalent goal among social media users is to increase the visibility and influence of their posts. This paper, therefore, delves into the compelling question: Can LLMs generate personalized influential content to amplify a user's presence on social media? We begin by examining prevalent techniques in content generation to assess their impact on post influence. Acknowledging the critical impact of underlying network structures in social media, which are instrumental in initiating content cascades and highly related to the influence/popularity of a post, we then inject network information into prompt for content generation to boost the post's influence. We design multiple content-centric and structure-aware prompts. The empirical experiments across LLMs validate their ability in improving the influence and draw insights on which strategies are more effective. Our code is available at https://github.com/YuyingZhao/LLM-influence-amplifier."
      },
      {
        "id": "oai:arXiv.org:2505.01699v1",
        "title": "Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning",
        "link": "https://arxiv.org/abs/2505.01699",
        "author": "Yifan Liu, Ruichen Yao, Yaokun Liu, Ruohan Zong, Zelin Li, Yang Zhang, Dong Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01699v1 Announce Type: new \nAbstract: The widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. While previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. In this paper, we focus on face component fairness, a fairness notion defined by biological face features. To our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. In this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. To address these issues, we propose \\textbf{B}ayesian \\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which incorporates a Bayesian Network calibrator to guide an adaptive meta-learning-based sample reweighting process. During the training process of our approach, the Bayesian Network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. To demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. Our results show that BNMR is able to consistently outperform recent face bias mitigation baselines. Moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \\textit{gender}). Our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. The code for our work is publicly available~\\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}."
      },
      {
        "id": "oai:arXiv.org:2505.01700v1",
        "title": "PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking",
        "link": "https://arxiv.org/abs/2505.01700",
        "author": "Yize Jiang, Xinze Li, Yuanyuan Zhang, Jin Han, Youjun Xu, Ayush Pandit, Zaixi Zhang, Mengdi Wang, Mengyang Wang, Chong Liu, Guang Yang, Yejin Choi, Wu-Jun Li, Tianfan Fu, Fang Wu, Junhong Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01700v1 Announce Type: new \nAbstract: Recently, significant progress has been made in protein-ligand docking, especially in modern deep learning methods, and some benchmarks were proposed, e.g., PoseBench, Plinder. However, these benchmarks suffer from less practical evaluation setups (e.g., blind docking, self docking), or heavy framework that involves training, raising challenges to assess docking methods efficiently. To fill this gap, we proposed PoseX, an open-source benchmark focusing on self-docking and cross-docking, to evaluate the algorithmic advances practically and comprehensively. Specifically, first, we curate a new evaluation dataset with 718 entries for self docking and 1,312 for cross docking; second, we incorporate 22 docking methods across three methodological categories, including (1) traditional physics-based methods (e.g., Schr\\\"odinger Glide), (2) AI docking methods (e.g., DiffDock), (3) AI co-folding methods (e.g., AlphaFold3); third, we design a relaxation method as post-processing to minimize conformation energy and refine binding pose; fourth, we released a leaderboard to rank submitted models in real time. We draw some key insights via extensive experiments: (1) AI-based approaches have already surpassed traditional physics-based approaches in overall docking accuracy (RMSD). The longstanding generalization issues that have plagued AI molecular docking have been significantly alleviated in the latest models. (2) The stereochemical deficiencies of AI-based approaches can be greatly alleviated with post-processing relaxation. Combining AI docking methods with the enhanced relaxation method achieves the best performance to date. (3) AI co-folding methods commonly face ligand chirality issues, which cannot be resolved by relaxation. The code, curated dataset and leaderboard are released at https://github.com/CataAI/PoseX."
      },
      {
        "id": "oai:arXiv.org:2505.01711v1",
        "title": "Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings",
        "link": "https://arxiv.org/abs/2505.01711",
        "author": "Alexander Davis, Rafael Souza, Jia-Hao Lim",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01711v1 Announce Type: new \nAbstract: Automated interpretation of chest X-rays (CXR) is a critical task with the potential to significantly improve clinical workflow and patient care. While recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (LLMs) for this visual task remains an underexplored area. This paper introduces CXR-TextInter, a novel framework that repurposes powerful text-centric LLMs for CXR interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. We augment this LLM-centric approach with an integrated medical knowledge module to enhance clinical reasoning. To facilitate training and evaluation, we developed the MediInstruct-CXR dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the CXR-ClinEval benchmark for comprehensive assessment across various interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that CXR-TextInter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. Ablation studies confirm the critical contribution of the knowledge integration module. Furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by CXR-TextInter. Our work validates an alternative paradigm for medical image AI, showcasing the potential of harnessing advanced LLM capabilities when visual information is effectively structured and domain knowledge is integrated."
      },
      {
        "id": "oai:arXiv.org:2505.01713v1",
        "title": "Vision and Intention Boost Large Language Model in Long-Term Action Anticipation",
        "link": "https://arxiv.org/abs/2505.01713",
        "author": "Congqi Cao, Lanshu Hu, Yating Yu, Yanning Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01713v1 Announce Type: new \nAbstract: Long-term action anticipation (LTA) aims to predict future actions over an extended period. Previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. Recent researches leverage large language models (LLMs) by utilizing text-based inputs which suffer severe information loss. To tackle these limitations single-modality methods face, we propose a novel Intention-Conditioned Vision-Language (ICVL) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of LLMs. Considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (VLM) to infer behavioral intentions as comprehensive textual features directly from video inputs. The inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. These enhanced visual representations, along with textual prompts, are fed into LLM for future action anticipation. Furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. Extensive experiments with state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets fully demonstrate the effectiveness and superiority of the proposed method."
      },
      {
        "id": "oai:arXiv.org:2505.01726v1",
        "title": "Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes",
        "link": "https://arxiv.org/abs/2505.01726",
        "author": "Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01726v1 Announce Type: new \nAbstract: Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations."
      },
      {
        "id": "oai:arXiv.org:2505.01729v1",
        "title": "PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth",
        "link": "https://arxiv.org/abs/2505.01729",
        "author": "Bu Jin, Weize Li, Baihan Yang, Zhenxin Zhu, Junpeng Jiang, Huan-ang Gao, Haiyang Sun, Kun Zhan, Hengtong Hu, Xueyang Zhang, Peng Jia, Hao Zhao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01729v1 Announce Type: new \nAbstract: Recent advancements in autonomous driving (AD) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. However, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. Drawing inspiration from self-supervised depth estimation, PosePilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. Specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. These outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. To further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. Extensive experiments on autonomous driving and general-domain video datasets demonstrate that PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. By steering camera pose with self-supervised depth, PosePilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models."
      },
      {
        "id": "oai:arXiv.org:2505.01731v1",
        "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
        "link": "https://arxiv.org/abs/2505.01731",
        "author": "Chuan Sun, Han Yu, Lizhen Cui",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01731v1 Announce Type: new \nAbstract: Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the \\underline{S}hapley \\underline{V}alue-based \\underline{N}on-\\underline{U}niform \\underline{P}runing (\\methodname{}) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, \\methodname{} achieves a reduction in perplexity (PPL) of 18.01\\% and 19.55\\% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70\\% sparsity."
      },
      {
        "id": "oai:arXiv.org:2505.01736v1",
        "title": "PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems",
        "link": "https://arxiv.org/abs/2505.01736",
        "author": "Han Wan, Rui Zhang, Qi Wang, Yang Liu, Hao Sun",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01736v1 Announce Type: new \nAbstract: Accurately modeling and forecasting complex systems governed by partial differential equations (PDEs) is crucial in various scientific and engineering domains. However, traditional numerical methods struggle in real-world scenarios due to incomplete or unknown physical laws. Meanwhile, machine learning approaches often fail to generalize effectively when faced with scarce observational data and the challenge of capturing local and global features. To this end, we propose the Physics-encoded Spectral Attention Network (PeSANet), which integrates local and global information to forecast complex systems with limited data and incomplete physical priors. The model consists of two key components: a physics-encoded block that uses hard constraints to approximate local differential operators from limited data, and a spectral-enhanced block that captures long-range global dependencies in the frequency domain. Specifically, we introduce a novel spectral attention mechanism to model inter-spectrum relationships and learn long-range spatial features. Experimental results demonstrate that PeSANet outperforms existing methods across all metrics, particularly in long-term forecasting accuracy, providing a promising solution for simulating complex systems with limited data and incomplete physics."
      },
      {
        "id": "oai:arXiv.org:2505.01737v1",
        "title": "Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes",
        "link": "https://arxiv.org/abs/2505.01737",
        "author": "Seong Hyeon Park, Jinwoo Shin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01737v1 Announce Type: new \nAbstract: In monocular videos that capture dynamic scenes, estimating the 3D geometry of video contents has been a fundamental challenge in computer vision. Specifically, the task is significantly challenged by the object motion, where existing models are limited to predict only partial attributes of the dynamic scenes, such as depth or pointmaps spanning only over a pair of frames. Since these attributes are inherently noisy under multiple frames, test-time global optimizations are often employed to fully recover the geometry, which is liable to failure and incurs heavy inference costs. To address the challenge, we present a new model, coined MMP, to estimate the geometry in a feed-forward manner, which produces a dynamic pointmap representation that evolves over multiple frames. Specifically, based on the recent Siamese architecture, we introduce a new trajectory encoding module to project point-wise dynamics on the representation for each frame, which can provide significantly improved expressiveness for dynamic scenes. In our experiments, we find MMP can achieve state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1% enhancement in the regression error."
      },
      {
        "id": "oai:arXiv.org:2505.01743v1",
        "title": "An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding",
        "link": "https://arxiv.org/abs/2505.01743",
        "author": "Siyang Jiang, Bufang Yang, Lilin Xu, Mu Yuan, Yeerzhati Abudunuer, Kaiwei Liu, Liekang Zeng, Hongkai Chen, Zhenyu Yan, Xiaofan Jiang, Guoliang Xing",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01743v1 Announce Type: new \nAbstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (HBU) in low-resolution vision systems, such as depth, thermal, and infrared. However, existing large vision language model (LVLM) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as RGB images. A quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. In this paper, we propose a novel, labor-saving system, Llambda, designed to support low-resolution HBU. The core idea is to leverage limited labeled data and a large amount of unlabeled data to guide LLMs in generating informative captions, which can be combined with raw data to effectively fine-tune LVLM models for understanding low-resolution videos in HBU. First, we propose a Contrastive-Oriented Data Labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. Second, we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. Therefore, it can improve LLMs' understanding of sequential data and then generate high-quality video captions. Finally, to ensure on-device deployability, we employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data. We evaluate Llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that Llambda outperforms several state-of-the-art LVLM systems up to $40.03\\%$ on average Bert-Score."
      },
      {
        "id": "oai:arXiv.org:2505.01744v1",
        "title": "Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients",
        "link": "https://arxiv.org/abs/2505.01744",
        "author": "Yezhen Wang, Zhouhao Yang, Brian K Chen, Fanyi Pu, Bo Li, Tianyu Gao, Kenji Kawaguchi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01744v1 Announce Type: new \nAbstract: Building upon the success of low-rank adapter (LoRA), low-rank gradient projection (LoRP) has emerged as a promising solution for memory-efficient fine-tuning. However, existing LoRP methods typically treat each row of the gradient matrix as the default projection unit, leaving the role of projection granularity underexplored. In this work, we propose a novel framework, VLoRP, that extends low-rank gradient projection by introducing an additional degree of freedom for controlling the trade-off between memory efficiency and performance, beyond the rank hyper-parameter. Through this framework, we systematically explore the impact of projection granularity, demonstrating that finer-grained projections lead to enhanced stability and efficiency even under a fixed memory budget. Regarding the optimization for VLoRP, we present ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces memory requirement while ensuring competitive performance, even in the presence of gradient accumulation. Additionally, we provide a theoretical analysis of VLoRP, demonstrating the descent and convergence of its optimization trajectory under both SGD and ProjFactor. Extensive experiments are conducted to validate our findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K."
      },
      {
        "id": "oai:arXiv.org:2505.01746v1",
        "title": "Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion",
        "link": "https://arxiv.org/abs/2505.01746",
        "author": "Xingqun Qi, Yatian Wang, Hengyuan Zhang, Jiahao Pan, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01746v1 Announce Type: new \nAbstract: Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter. Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. Considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. Specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a Temporal Interaction Module (TIM). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. The dataset and source code are publicly available at \\href{https://mattie-e.github.io/Co3/}{\\textit{https://mattie-e.github.io/Co3/}}."
      },
      {
        "id": "oai:arXiv.org:2505.01761v1",
        "title": "Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models",
        "link": "https://arxiv.org/abs/2505.01761",
        "author": "Tobias Domhan, Dawei Zhu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01761v1 Announce Type: new \nAbstract: Accurately evaluating machine-translated text remains a long-standing challenge, particularly for long documents. Recent work has shown that large language models (LLMs) can serve as reliable and interpretable sentence-level translation evaluators via MQM error span annotations. With modern LLMs supporting larger context windows, a natural question arises: can we feed entire document translations into an LLM for quality assessment? Ideally, evaluation should be invariant to text length, producing consistent error spans regardless of input granularity. However, our analysis shows that text length significantly impacts evaluation: longer texts lead to fewer error spans and reduced system ranking accuracy. To address this limitation, we evaluate several strategies, including granularity-aligned prompting, Focus Sentence Prompting (FSP), and a fine-tuning approach to better align LLMs with the evaluation task. The latter two methods largely mitigate this length bias, making LLMs more reliable for long-form translation evaluation."
      },
      {
        "id": "oai:arXiv.org:2505.01766v1",
        "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement",
        "link": "https://arxiv.org/abs/2505.01766",
        "author": "Long Bai, Boyi Ma, Ruohan Wang, Guankun Wang, Beilei Cui, Zhongliang Jiang, Mobarakol Islam, Zhe Min, Jiewen Lai, Nassir Navab, Hongliang Ren",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01766v1 Announce Type: new \nAbstract: Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures."
      },
      {
        "id": "oai:arXiv.org:2505.01783v1",
        "title": "Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition",
        "link": "https://arxiv.org/abs/2505.01783",
        "author": "Amirmohammad Farzaneh, Osvaldo Simeone",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01783v1 Announce Type: new \nAbstract: Online anomaly detection is essential in fields such as cybersecurity, healthcare, and industrial monitoring, where promptly identifying deviations from expected behavior can avert critical failures or security breaches. While numerous anomaly scoring methods based on supervised or unsupervised learning have been proposed, current approaches typically rely on a continuous stream of real-world calibration data to provide assumption-free guarantees on the false discovery rate (FDR). To address the inherent challenges posed by limited real calibration data, we introduce context-aware prediction-powered conformal online anomaly detection (C-PP-COAD). Our framework strategically leverages synthetic calibration data to mitigate data scarcity, while adaptively integrating real data based on contextual cues. C-PP-COAD utilizes conformal p-values, active p-value statistics, and online FDR control mechanisms to maintain rigorous and reliable anomaly detection performance over time. Experiments conducted on both synthetic and real-world datasets demonstrate that C-PP-COAD significantly reduces dependency on real calibration data without compromising guaranteed FDR control."
      },
      {
        "id": "oai:arXiv.org:2505.01788v1",
        "title": "Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning",
        "link": "https://arxiv.org/abs/2505.01788",
        "author": "Md. Tanzib Hosain, Asif Zaman, Md. Shahriar Sajid, Shadman Sakeeb Khan, Shanjida Akter",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01788v1 Announce Type: new \nAbstract: The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies."
      },
      {
        "id": "oai:arXiv.org:2505.01790v1",
        "title": "Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos",
        "link": "https://arxiv.org/abs/2505.01790",
        "author": "Markos Stamatakis, Joshua Berger, Christian Wartena, Ralph Ewerth, Anett Hoppe",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01790v1 Announce Type: new \nAbstract: Web-based educational videos offer flexible learning opportunities and are becoming increasingly popular. However, improving user engagement and knowledge retention remains a challenge. Automatically generated questions can activate learners and support their knowledge acquisition. Further, they can help teachers and learners assess their understanding. While large language and vision-language models have been employed in various tasks, their application to question generation for educational videos remains underexplored. In this paper, we investigate the capabilities of current vision-language models for generating learning-oriented questions for educational video content. We assess (1) out-of-the-box models' performance; (2) fine-tuning effects on content-specific question generation; (3) the impact of different video modalities on question quality; and (4) in a qualitative study, question relevance, answerability, and difficulty levels of generated questions. Our findings delineate the capabilities of current vision-language models, highlighting the need for fine-tuning and addressing challenges in question diversity and relevance. We identify requirements for future multimodal datasets and outline promising research directions."
      },
      {
        "id": "oai:arXiv.org:2505.01794v1",
        "title": "A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments",
        "link": "https://arxiv.org/abs/2505.01794",
        "author": "Jared D. T. Guerrero-Sosa, Francisco P. Romero, V\\'ictor Hugo Men\\'endez-Dom\\'inguez, Jesus Serrano-Guerrero, Andres Montoro-Montarroso, Jose A. Olivas",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01794v1 Announce Type: new \nAbstract: In the rapidly evolving educational landscape, the unbiased assessment of soft skills is a significant challenge, particularly in higher education. This paper presents a fuzzy logic approach that employs a Granular Linguistic Model of Phenomena integrated with multimodal analysis to evaluate soft skills in undergraduate students. By leveraging computational perceptions, this approach enables a structured breakdown of complex soft skill expressions, capturing nuanced behaviours with high granularity and addressing their inherent uncertainties, thereby enhancing interpretability and reliability. Experiments were conducted with undergraduate students using a developed tool that assesses soft skills such as decision-making, communication, and creativity. This tool identifies and quantifies subtle aspects of human interaction, such as facial expressions and gesture recognition. The findings reveal that the framework effectively consolidates multiple data inputs to produce meaningful and consistent assessments of soft skills, showing that integrating multiple modalities into the evaluation process significantly improves the quality of soft skills scores, making the assessment work transparent and understandable to educational stakeholders."
      },
      {
        "id": "oai:arXiv.org:2505.01799v1",
        "title": "AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting",
        "link": "https://arxiv.org/abs/2505.01799",
        "author": "Junhao Shi, Jisheng Xu, Jianping He, Zhiliang Lin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01799v1 Announce Type: new \nAbstract: Underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3D models from images captured by underwater platforms. However, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of Structure-from-Motion (SfM) pose estimation, leading to subsequent reconstruction failures. Additionally, SfM methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. In this paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model based on the SeaThru algorithm, which facilitates rapid and accurate separation of scene details and medium features. Our approach initializes Gaussians by integrating state-of-the-art multi-view stereo (MVS) technology, employs implicit Neural Radiance Fields (NeRF) for rendering translucent media and utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. Experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms."
      },
      {
        "id": "oai:arXiv.org:2505.01800v1",
        "title": "Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis",
        "link": "https://arxiv.org/abs/2505.01800",
        "author": "Chidimma Opara",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01800v1 Announce Type: new \nAbstract: The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI."
      },
      {
        "id": "oai:arXiv.org:2505.01802v1",
        "title": "Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows",
        "link": "https://arxiv.org/abs/2505.01802",
        "author": "Georgios Fotios Angelis, Savas Ozkan, Sinan Mutlu, Paul Wisbey, Anastasios Drosou, Mete Ozay",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01802v1 Announce Type: new \nAbstract: To have a seamless user experience on immersive AR/VR applications, the importance of efficient and effective Neural Network (NN) models is undeniable, since missing body parts that cannot be captured by limited sensors should be generated using these models for a complete 3D full-body reconstruction in virtual environment. However, the state-of-the-art NN-models are typically computational expensive and they leverage longer sequences of sparse tracking inputs to generate full-body movements by capturing temporal context. Inevitably, longer sequences increase the computation overhead and introduce noise in longer temporal dependencies that adversely affect the generation performance. In this paper, we propose a novel Multi-Layer Perceptron (MLP)-based method that enhances the overall performance while balancing the computational cost and memory overhead for efficient 3D full-body generation. Precisely, we introduce a NN-mechanism that divides the longer sequence of inputs into smaller temporal windows. Later, the current motion is merged with the information from these windows through latent representations to utilize the past context for the generation. Our experiments demonstrate that generation accuracy of our method with this NN-mechanism is significantly improved compared to the state-of-the-art methods while greatly reducing computational costs and memory overhead, making our method suitable for resource-constrained devices."
      },
      {
        "id": "oai:arXiv.org:2505.01805v1",
        "title": "Not Every Tree Is a Forest: Benchmarking Forest Types from Satellite Remote Sensing",
        "link": "https://arxiv.org/abs/2505.01805",
        "author": "Yuchang Jiang, Maxim Neumann",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01805v1 Announce Type: new \nAbstract: Developing accurate and reliable models for forest types mapping is critical to support efforts for halting deforestation and for biodiversity conservation (such as European Union Deforestation Regulation (EUDR)). This work introduces ForTy, a benchmark for global-scale FORest TYpes mapping using multi-temporal satellite data1. The benchmark comprises 200,000 time series of image patches, each consisting of Sentinel-2, Sentinel-1, climate, and elevation data. Each time series captures variations at monthly or seasonal cadence. Per-pixel annotations, including forest types and other land use classes, support image segmentation tasks. Unlike most existing land use products that often categorize all forest areas into a single class, our benchmark differentiates between three forest types classes: natural forest, planted forest, and tree crops. By leveraging multiple public data sources, we achieve global coverage with this benchmark. We evaluate the forest types dataset using several baseline models, including convolution neural networks and transformer-based models. Additionally, we propose a novel transformer-based model specifically designed to handle multi-modal, multi-temporal satellite data for forest types mapping. Our experimental results demonstrate that the proposed model surpasses the baseline models in performance."
      },
      {
        "id": "oai:arXiv.org:2505.01809v1",
        "title": "3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment",
        "link": "https://arxiv.org/abs/2505.01809",
        "author": "Xiaoqi Li, Jiaming Liu, Nuowei Han, Liang Heng, Yandong Guo, Hao Dong, Yang Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01809v1 Announce Type: new \nAbstract: The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef."
      },
      {
        "id": "oai:arXiv.org:2505.01810v1",
        "title": "Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees",
        "link": "https://arxiv.org/abs/2505.01810",
        "author": "Zhiyi Zhou, Hexin Peng, Hongyu Long",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01810v1 Announce Type: new \nAbstract: With the advancement of Internet of Things (IoT) technologies, high-precision indoor positioning has become essential for Location-Based Services (LBS) in complex indoor environments. Fingerprint-based localization is popular, but traditional algorithms and deep learning-based methods face challenges such as poor generalization, overfitting, and lack of interpretability. This paper applies conformal prediction (CP) to deep learning-based indoor positioning. CP transforms the uncertainty of the model into a non-conformity score, constructs prediction sets to ensure correctness coverage, and provides statistical guarantees. We also introduce conformal risk control for path navigation tasks to manage the false discovery rate (FDR) and the false negative rate (FNR).The model achieved an accuracy of approximately 100% on the training dataset and 85% on the testing dataset, effectively demonstrating its performance and generalization capability. Furthermore, we also develop a conformal p-value framework to control the proportion of position-error points. Experiments on the UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19, MobileNetV2, ResNet50, and EfficientNet show that the conformal prediction technique can effectively approximate the target coverage, and different models have different performance in terms of prediction set size and uncertainty quantification."
      },
      {
        "id": "oai:arXiv.org:2505.01812v1",
        "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge",
        "link": "https://arxiv.org/abs/2505.01812",
        "author": "Core Francisco Park, Zechen Zhang, Hidenori Tanaka",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01812v1 Announce Type: new \nAbstract: Humans and intelligent animals can effortlessly internalize new information (\"news\") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\\textit{contexual shadowing effect}$, where training with the news $\\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT."
      },
      {
        "id": "oai:arXiv.org:2505.01819v1",
        "title": "An LSTM-PINN Hybrid Method to the specific problem of population forecasting",
        "link": "https://arxiv.org/abs/2505.01819",
        "author": "Ze Tao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01819v1 Announce Type: new \nAbstract: Deep learning has emerged as a powerful tool in scientific modeling, particularly for complex dynamical systems; however, accurately capturing age-structured population dynamics under policy-driven fertility changes remains a significant challenge due to the lack of effective integration between domain knowledge and long-term temporal dependencies. To address this issue, we propose two physics-informed deep learning frameworks--PINN and LSTM-PINN--that incorporate policy-aware fertility functions into a transport-reaction partial differential equation to simulate population evolution from 2024 to 2054. The standard PINN model enforces the governing equation and boundary conditions via collocation-based training, enabling accurate learning of underlying population dynamics and ensuring stable convergence. Building on this, the LSTM-PINN framework integrates sequential memory mechanisms to effectively capture long-range dependencies in the age-time domain, achieving robust training performance across multiple loss components. Simulation results under three distinct fertility policy scenarios-the Three-child policy, the Universal two-child policy, and the Separate two-child policy--demonstrate the models' ability to reflect policy-sensitive demographic shifts and highlight the effectiveness of integrating domain knowledge into data-driven forecasting. This study provides a novel and extensible framework for modeling age-structured population dynamics under policy interventions, offering valuable insights for data-informed demographic forecasting and long-term policy planning in the face of emerging population challenges."
      },
      {
        "id": "oai:arXiv.org:2505.01822v1",
        "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.01822",
        "author": "Jifeng Hu, Sili Huang, Zhejian Yang, Shengchao Hu, Li Shen, Hechang Chen, Lichao Sun, Yi Chang, Dacheng Tao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01822v1 Announce Type: new \nAbstract: Conditional decision generation with diffusion models has shown powerful competitiveness in reinforcement learning (RL). Recent studies reveal the relation between energy-function-guidance diffusion models and constrained RL problems. The main challenge lies in estimating the intermediate energy, which is intractable due to the log-expectation formulation during the generation process. To address this issue, we propose the Analytic Energy-guided Policy Optimization (AEPO). Specifically, we first provide a theoretical analysis and the closed-form solution of the intermediate guidance when the diffusion model obeys the conditional Gaussian transformation. Then, we analyze the posterior Gaussian distribution in the log-expectation formulation and obtain the target estimation of the log-expectation under mild assumptions. Finally, we train an intermediate energy neural network to approach the target estimation of log-expectation formulation. We apply our method in 30+ offline RL tasks to demonstrate the effectiveness of our method. Extensive experiments illustrate that our method surpasses numerous representative baselines in D4RL offline reinforcement learning benchmarks."
      },
      {
        "id": "oai:arXiv.org:2505.01823v1",
        "title": "PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach",
        "link": "https://arxiv.org/abs/2505.01823",
        "author": "Nitin Rai, Arnold W. Schumann, Nathan Boyd",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01823v1 Announce Type: new \nAbstract: Collecting large-scale crop disease images in the field is labor-intensive and time-consuming. Generative models (GMs) offer an alternative by creating synthetic samples that resemble real-world images. However, existing research primarily relies on Generative Adversarial Networks (GANs)-based image-to-image translation and lack a comprehensive analysis of computational requirements in agriculture. Therefore, this research explores a multi-modal text-to-image approach for generating synthetic crop disease images and is the first to provide computational benchmarking in this context. We trained three Stable Diffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and fine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning techniques to enhance generalization. SD3.5M outperformed the others, with an average memory usage of 18 GB, power consumption of 180 W, and total energy use of 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results demonstrate SD3.5M's ability to generate 500 synthetic images from just 36 in-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease data generation."
      },
      {
        "id": "oai:arXiv.org:2505.01837v1",
        "title": "CVVNet: A Cross-Vertical-View Network for Gait Recognition",
        "link": "https://arxiv.org/abs/2505.01837",
        "author": "Xiangru Li, Wei Song, Yingda Huang, Wei Meng, Le Chang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01837v1 Announce Type: new \nAbstract: Gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. While existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. Our experiments show up to 60\\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. Current CNN and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. To tackle this challenge, we propose CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. CVVNet employs a High-Low Frequency Extraction module (HLFE) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. The integration of our core Multi-Scale Attention Gated Aggregation (MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. Experimental results show that our CVVNet achieves state-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$ on Gait3D compared with the best existing methods."
      },
      {
        "id": "oai:arXiv.org:2505.01838v1",
        "title": "MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization",
        "link": "https://arxiv.org/abs/2505.01838",
        "author": "Chenghong Li, Hongjie Liao, Yihao Zhi, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Shuguang Cui, Xiaoguang Han",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01838v1 Announce Type: new \nAbstract: In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while significant progress has been achieved in object-centric tasks through large-scale datasets like Objaverse and MVImgNet, human-centric tasks have seen limited advancement, largely due to the absence of a comparable large-scale human dataset. To bridge this gap, we present MVHumanNet++, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using multi-view human capture systems, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. Additionally, the proposed MVHumanNet++ dataset is enhanced with newly processed normal maps and depth maps, significantly expanding its applicability and utility for advanced human-centric research. To explore the potential of our proposed MVHumanNet++ dataset in various 2D and 3D visual tasks, we conducted several pilot studies to demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet++. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet++ dataset with annotations will foster further innovations in the domain of 3D human-centric tasks at scale. MVHumanNet++ is publicly available at https://kevinlee09.github.io/research/MVHumanNet++/."
      },
      {
        "id": "oai:arXiv.org:2505.01851v1",
        "title": "Mitigating Group-Level Fairness Disparities in Federated Visual Language Models",
        "link": "https://arxiv.org/abs/2505.01851",
        "author": "Chaomeng Chen, Zitong Yu, Junhao Dong, Sen Su, Linlin Shen, Shutao Xia, Xiaochun Cao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01851v1 Announce Type: new \nAbstract: Visual language models (VLMs) have shown remarkable capabilities in multimodal tasks but face challenges in maintaining fairness across demographic groups, particularly when deployed in federated learning (FL) environments. This paper addresses the critical issue of group fairness in federated VLMs by introducing FVL-FP, a novel framework that combines FL with fair prompt tuning techniques. We focus on mitigating demographic biases while preserving model performance through three innovative components: (1) Cross-Layer Demographic Fair Prompting (CDFP), which adjusts potentially biased embeddings through counterfactual regularization; (2) Demographic Subspace Orthogonal Projection (DSOP), which removes demographic bias in image representations by mapping fair prompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which dynamically balances client contributions based on both performance and fairness metrics. Extensive evaluations across four benchmark datasets demonstrate that our approach reduces demographic disparity by an average of 45\\% compared to standard FL approaches, while maintaining task performance within 6\\% of state-of-the-art results. FVL-FP effectively addresses the challenges of non-IID data distributions in federated settings and introduces minimal computational overhead while providing significant fairness benefits. Our work presents a parameter-efficient solution to the critical challenge of ensuring equitable performance across demographic groups in privacy-preserving multimodal systems."
      },
      {
        "id": "oai:arXiv.org:2505.01855v1",
        "title": "Intra-Layer Recurrence in Transformers for Language Modeling",
        "link": "https://arxiv.org/abs/2505.01855",
        "author": "Anthony Nguyen, Wenjun Lin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01855v1 Announce Type: new \nAbstract: Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures."
      },
      {
        "id": "oai:arXiv.org:2505.01857v1",
        "title": "DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion",
        "link": "https://arxiv.org/abs/2505.01857",
        "author": "Haoteng Li, Zhao Yang, Zezhong Qian, Gongpeng Zhao, Yuqi Huang, Jun Yu, Huazheng Zhou, Longjun Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01857v1 Announce Type: new \nAbstract: Accurate and high-fidelity driving scene reconstruction relies on fully leveraging scene information as conditioning. However, existing approaches, which primarily use 3D bounding boxes and binary maps for foreground and background control, fall short in capturing the complexity of the scene and integrating multi-modal information. In this paper, we propose DualDiff, a dual-branch conditional diffusion model designed to enhance multi-view driving scene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D representation, alongside numerical driving scene representation, for comprehensive foreground and background control. To improve cross-modal information integration, we propose a Semantic Fusion Attention (SFA) mechanism that aligns and fuses features across modalities. Furthermore, we design a foreground-aware masked (FGM) loss to enhance the generation of tiny objects. DualDiff achieves state-of-the-art performance in FID score, as well as consistently better results in downstream BEV segmentation and 3D object detection tasks."
      },
      {
        "id": "oai:arXiv.org:2505.01868v1",
        "title": "Positional Attention for Efficient BERT-Based Named Entity Recognition",
        "link": "https://arxiv.org/abs/2505.01868",
        "author": "Mo Sun, Siheng Xiong, Yuankai Cai, Bowen Zuo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01868v1 Announce Type: new \nAbstract: This paper presents a framework for Named Entity Recognition (NER) leveraging the Bidirectional Encoder Representations from Transformers (BERT) model in natural language processing (NLP). NER is a fundamental task in NLP with broad applicability across downstream applications. While BERT has established itself as a state-of-the-art model for entity recognition, fine-tuning it from scratch for each new application is computationally expensive and time-consuming. To address this, we propose a cost-efficient approach that integrates positional attention mechanisms into the entity recognition process and enables effective customization using pre-trained parameters. The framework is evaluated on a Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves strong performance with fewer training epochs. This work contributes to the field by offering a practical solution for reducing the training cost of BERT-based NER systems while maintaining high accuracy."
      },
      {
        "id": "oai:arXiv.org:2505.01869v1",
        "title": "Visual enhancement and 3D representation for underwater scenes: a review",
        "link": "https://arxiv.org/abs/2505.01869",
        "author": "Guoxi Huang, Haoran Wang, Brett Seymour, Evan Kovacs, John Ellerbrock, Dave Blackham, Nantheera Anantrasirichai",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01869v1 Announce Type: new \nAbstract: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in\n  computer vision and AI-based tasks due to complex imaging conditions in aquatic environments. Despite\n  the development of numerous enhancement algorithms, a comprehensive and systematic review covering both\n  UVE and underwater 3D reconstruction remains absent. To advance research in these areas, we present an\n  in-depth review from multiple perspectives. First, we introduce the fundamental physical models, highlighting the\n  peculiarities that challenge conventional techniques. We survey advanced methods for visual enhancement and\n  3D reconstruction specifically designed for underwater scenarios. The paper assesses various approaches from\n  non-learning methods to advanced data-driven techniques, including Neural Radiance Fields and 3D Gaussian\n  Splatting, discussing their effectiveness in handling underwater distortions. Finally, we conduct both quantitative\n  and qualitative evaluations of state-of-the-art UVE and underwater 3D reconstruction algorithms across multiple\n  benchmark datasets. Finally, we highlight key research directions for future advancements in underwater vision."
      },
      {
        "id": "oai:arXiv.org:2505.01874v1",
        "title": "Towards Trustworthy Federated Learning with Untrusted Participants",
        "link": "https://arxiv.org/abs/2505.01874",
        "author": "Youssef Allouah, Rachid Guerraoui, John Stephan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01874v1 Announce Type: new \nAbstract: Resilience against malicious parties and data privacy are essential for trustworthy distributed learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of workers shares a randomness seed unknown to others. In a setting where malicious workers may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, leveraging shared randomness between workers. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server."
      },
      {
        "id": "oai:arXiv.org:2505.01877v1",
        "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't",
        "link": "https://arxiv.org/abs/2505.01877",
        "author": "Ji\\v{r}\\'i Mili\\v{c}ka, Anna Marklov\\'a, Ond\\v{r}ej Drobil, Eva Posp\\'i\\v{s}ilov\\'a",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01877v1 Announce Type: new \nAbstract: This study investigates whether individuals can learn to accurately discriminate between human-written and AI-produced texts when provided with immediate feedback, and if they can use this feedback to recalibrate their self-perceived competence. We also explore the specific criteria individuals rely upon when making these decisions, focusing on textual style and perceived readability.\n  We used GPT-4o to generate several hundred texts across various genres and text types comparable to Koditex, a multi-register corpus of human-written texts. We then presented randomized text pairs to 255 Czech native speakers who identified which text was human-written and which was AI-generated. Participants were randomly assigned to two conditions: one receiving immediate feedback after each trial, the other receiving no feedback until experiment completion. We recorded accuracy in identification, confidence levels, response times, and judgments about text readability along with demographic data and participants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in accuracy and confidence calibration. Participants initially held incorrect assumptions about AI-generated text features, including expectations about stylistic rigidity and readability. Notably, without feedback, participants made the most errors precisely when feeling most confident -- an issue largely resolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be effectively learned through targeted training with explicit feedback, which helps correct misconceptions about AI stylistic features and readability, as well as potential other variables that were not explored, while facilitating more accurate self-assessment. This finding might be particularly important in educational contexts."
      },
      {
        "id": "oai:arXiv.org:2505.01881v1",
        "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications",
        "link": "https://arxiv.org/abs/2505.01881",
        "author": "Trisanth Srinivasan, Santosh Patapati",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01881v1 Announce Type: new \nAbstract: Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems."
      },
      {
        "id": "oai:arXiv.org:2505.01882v1",
        "title": "CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture",
        "link": "https://arxiv.org/abs/2505.01882",
        "author": "Vladimir Frants, Sos Agaian, Karen Panetta, Peter Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01882v1 Announce Type: new \nAbstract: Images used in real-world applications such as image or video retrieval, outdoor surveillance, and autonomous driving suffer from poor weather conditions. When designing robust computer vision systems, removing adverse weather such as haze, rain, and snow is a significant problem. Recently, deep-learning methods offered a solution for a single type of degradation. Current state-of-the-art universal methods struggle with combinations of degradations, such as haze and rain-streak. Few algorithms have been developed that perform well when presented with images containing multiple adverse weather conditions. This work focuses on developing an efficient solution for multiple adverse weather removal using a unified quaternion neural architecture called CMAWRNet. It is based on a novel texture-structure decomposition block, a novel lightweight encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction. We also introduce a quaternion similarity loss function to preserve color information better. The quantitative and qualitative evaluation of the current state-of-the-art benchmarking datasets and real-world images shows the performance advantages of the proposed CMAWRNet compared to other state-of-the-art weather removal approaches dealing with multiple weather artifacts. Extensive computer simulations validate that CMAWRNet improves the performance of downstream applications such as object detection. This is the first time the decomposition approach has been applied to the universal weather removal task."
      },
      {
        "id": "oai:arXiv.org:2505.01883v1",
        "title": "Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams",
        "link": "https://arxiv.org/abs/2505.01883",
        "author": "Yiwen Lu, Siheng Xiong, Zhaowei Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01883v1 Announce Type: new \nAbstract: We present a framework for large-scale sentiment and topic analysis of Twitter discourse. Our pipeline begins with targeted data collection using conflict-specific keywords, followed by automated sentiment labeling via multiple pre-trained models to improve annotation robustness. We examine the relationship between sentiment and contextual features such as timestamp, geolocation, and lexical content. To identify latent themes, we apply Latent Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and metadata attributes. Finally, we develop an interactive visualization interface to support exploration of sentiment trends and topic distributions across time and regions. This work contributes a scalable methodology for social media analysis in dynamic geopolitical contexts."
      },
      {
        "id": "oai:arXiv.org:2505.01888v1",
        "title": "Rethinking Score Distilling Sampling for 3D Editing and Generation",
        "link": "https://arxiv.org/abs/2505.01888",
        "author": "Xingyu Miao, Haoran Duan, Yang Long, Jungong Han",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01888v1 Announce Type: new \nAbstract: Score Distillation Sampling (SDS) has emerged as a prominent method for text-to-3D generation by leveraging the strengths of 2D diffusion models. However, SDS is limited to generation tasks and lacks the capability to edit existing 3D assets. Conversely, variants of SDS that introduce editing capabilities often can not generate new 3D assets effectively. In this work, we observe that the processes of generation and editing within SDS and its variants have unified underlying gradient terms. Building on this insight, we propose Unified Distillation Sampling (UDS), a method that seamlessly integrates both the generation and editing of 3D assets. Essentially, UDS refines the gradient terms used in vanilla SDS methods, unifying them to support both tasks. Extensive experiments demonstrate that UDS not only outperforms baseline methods in generating 3D assets with richer details but also excels in editing tasks, thereby bridging the gap between 3D generation and editing. The code is available on: https://github.com/xingy038/UDS."
      },
      {
        "id": "oai:arXiv.org:2505.01892v1",
        "title": "OODTE: A Differential Testing Engine for the ONNX Optimizer",
        "link": "https://arxiv.org/abs/2505.01892",
        "author": "Nikolaos Louloudakis, Ajitha Rajan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01892v1 Announce Type: new \nAbstract: With $700$ stars on GitHub and part of the official ONNX repository, the ONNX Optimizer consists of the standard method to apply graph-based optimizations on ONNX models. However, its ability to preserve model accuracy across optimizations, has not been rigorously explored. We propose OODTE, a utility to automatically and thoroughly assess the correctness of the ONNX Optimizer. OODTE follows a simple, yet effective differential testing and evaluation approach that can be easily adopted to other compiler optimizers. In particular, OODTE utilizes a number of ONNX models, then optimizes them and executes both the original and the optimized variants across a user-defined set of inputs, while automatically logging any issues with the optimization process. Finally, for successfully optimized models, OODTE compares the results, and, if any accuracy deviations are observed, it iteratively repeats the process for each pass of the ONNX Optimizer, to localize the root cause of the differences observed. Using OODTE, we sourced well-known $130$ models from the official ONNX Model Hub, used for a wide variety of tasks (classification, object detection, semantic segmentation, text summarization, question and answering, sentiment analysis) from the official ONNX model hub. We detected 15 issues, 14 of which were previously unknown, associated with optimizer crashes and accuracy deviations. We also observed $9.2$% of all model instances presenting issues leading into the crash of the optimizer, or the generation of an invalid model while using the primary optimizer strategies. In addition, $30$% of the classification models presented accuracy differences across the original and the optimized model variants, while $16.6$% of semantic segmentation and object detection models are also affected, at least to a limited extent."
      },
      {
        "id": "oai:arXiv.org:2505.01900v1",
        "title": "CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation",
        "link": "https://arxiv.org/abs/2505.01900",
        "author": "Mazal Bethany, Nishant Vishwamitra, Cho-Yu Jason Chiang, Peyman Najafirad",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01900v1 Announce Type: new \nAbstract: Automated evidence-based misinformation detection systems, which evaluate the veracity of short claims against evidence, lack comprehensive analysis of their adversarial vulnerabilities. Existing black-box text-based adversarial attacks are ill-suited for evidence-based misinformation detection systems, as these attacks primarily focus on token-level substitutions involving gradient or logit-based optimization strategies, which are incapable of fooling the multi-component nature of these detection systems. These systems incorporate both retrieval and claim-evidence comparison modules, which requires attacks to break the retrieval of evidence and/or the comparison module so that it draws incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach that employs a two-agent system, a Prompt Optimization Agent and an Attacker Agent, to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison, effectively bypassing the system without altering the meaning of the claim. The Attacker Agent produces semantically equivalent rewrites that attempt to mislead detectors, while the Prompt Optimization Agent analyzes failed attack attempts and refines the prompt of the Attacker to guide subsequent rewrites. This enables larger structural and stylistic transformations of the text rather than token-level substitutions, adapting the magnitude of changes based on previous outcomes. Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on binary model decisions to guide its rewriting process, eliminating the need for classifier logits or extensive querying. We evaluate CAMOUFLAGE on four systems, including two recent academic systems and two real-world APIs, with an average attack success rate of 46.92\\% while preserving textual coherence and semantic equivalence to the original claims."
      },
      {
        "id": "oai:arXiv.org:2505.01902v1",
        "title": "From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup",
        "link": "https://arxiv.org/abs/2505.01902",
        "author": "Ali Al-Bustami, Zaid Ghazal",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01902v1 Announce Type: new \nAbstract: Accurate prediction of FIFA World Cup match outcomes holds significant value for analysts, coaches, bettors, and fans. This paper presents a machine learning framework specifically designed to forecast match winners in FIFA World Cup. By integrating both team-level historical data and player-specific performance metrics such as goals, assists, passing accuracy, and tackles, we capture nuanced interactions often overlooked by traditional aggregate models. Our methodology processes multi-year data to create year-specific team profiles that account for evolving rosters and player development. We employ classification techniques complemented by dimensionality reduction and hyperparameter optimization, to yield robust predictive models. Experimental results on data from the FIFA 2022 World Cup demonstrate our approach's superior accuracy compared to baseline method. Our findings highlight the importance of incorporating individual player attributes and team-level composition to enhance predictive performance, offering new insights into player synergy, strategic match-ups, and tournament progression scenarios. This work underscores the transformative potential of rich, player-centric data in sports analytics, setting a foundation for future exploration of advanced learning architectures such as graph neural networks to model complex team interactions."
      },
      {
        "id": "oai:arXiv.org:2505.01903v1",
        "title": "LookAlike: Consistent Distractor Generation in Math MCQs",
        "link": "https://arxiv.org/abs/2505.01903",
        "author": "Nisarg Parikh, Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01903v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly used to generate distractors for multiple-choice questions (MCQs), especially in domains like math education. However, existing approaches are limited in ensuring that the generated distractors are consistent with common student errors. We propose LookAlike, a method that improves error-distractor consistency via preference optimization. Our two main innovations are: (a) mining synthetic preference pairs from model inconsistencies, and (b) alternating supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike prior work that relies on heuristics or manually annotated preference data, LookAlike uses its own generation inconsistencies as dispreferred samples, thus enabling scalable and stable training. Evaluated on a real-world dataset of 1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an existing state-of-the-art method (45.6% / 47.7%). These improvements highlight the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale."
      },
      {
        "id": "oai:arXiv.org:2505.01912v1",
        "title": "BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models",
        "link": "https://arxiv.org/abs/2505.01912",
        "author": "Evan R. Antoniuk, Shehtab Zaman, Tal Ben-Nun, Peggy Li, James Diffenderfer, Busra Demirci, Obadiah Smolenski, Tim Hsu, Anna M. Hiszpanski, Kenneth Chiu, Bhavya Kailkhura, Brian Van Essen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01912v1 Announce Type: new \nAbstract: Advances in deep learning and generative modeling have driven interest in data-driven molecule discovery pipelines, whereby machine learning (ML) models are used to filter and design novel molecules without requiring prohibitively expensive first-principles simulations. Although the discovery of novel molecules that extend the boundaries of known chemistry requires accurate out-of-distribution (OOD) predictions, ML models often struggle to generalize OOD. Furthermore, there are currently no systematic benchmarks for molecular OOD prediction tasks. We present BOOM, $\\boldsymbol{b}$enchmarks for $\\boldsymbol{o}$ut-$\\boldsymbol{o}$f-distribution $\\boldsymbol{m}$olecular property predictions -- a benchmark study of property-based out-of-distribution models for common molecular property prediction models. We evaluate more than 140 combinations of models and property prediction tasks to benchmark deep learning models on their OOD performance. Overall, we do not find any existing models that achieve strong OOD generalization across all tasks: even the top performing model exhibited an average OOD error 3x larger than in-distribution. We find that deep learning models with high inductive bias can perform well on OOD tasks with simple, specific properties. Although chemical foundation models with transfer and in-context learning offer a promising solution for limited training data scenarios, we find that current foundation models do not show strong OOD extrapolation capabilities. We perform extensive ablation experiments to highlight how OOD performance is impacted by data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation. We propose that developing ML models with strong OOD generalization is a new frontier challenge in chemical ML model development. This open-source benchmark will be made available on Github."
      },
      {
        "id": "oai:arXiv.org:2505.01928v1",
        "title": "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2505.01928",
        "author": "Anushka Agarwal, Muhammad Yusuf Hassan, Talha Chafekar",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01928v1 Announce Type: new \nAbstract: We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality."
      },
      {
        "id": "oai:arXiv.org:2505.01933v1",
        "title": "Unemployment Dynamics Forecasting with Machine Learning Regression Models",
        "link": "https://arxiv.org/abs/2505.01933",
        "author": "Kyungsu Kim",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01933v1 Announce Type: new \nAbstract: In this paper, I explored how a range of regression and machine learning techniques can be applied to monthly U.S. unemployment data to produce timely forecasts. I compared seven models: Linear Regression, SGDRegressor, Random Forest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network, training each on a historical span of data and then evaluating on a later hold-out period. Input features include macro indicators (GDP growth, CPI), labor market measures (job openings, initial claims), financial variables (interest rates, equity indices), and consumer sentiment.\n  I tuned model hyperparameters via cross-validation and assessed performance with standard error metrics and the ability to predict the correct unemployment direction. Across the board, tree-based ensembles (and CatBoost in particular) deliver noticeably better forecasts than simple linear approaches, while the LSTM captures underlying temporal patterns more effectively than other nonlinear methods. SVR and SGDRegressor yield modest gains over standard regression but don't match the consistency of the ensemble and deep-learning models.\n  Interpretability tools ,feature importance rankings and SHAP values, point to job openings and consumer sentiment as the most influential predictors across all methods. By directly comparing linear, ensemble, and deep-learning approaches on the same dataset, our study shows how modern machine-learning techniques can enhance real-time unemployment forecasting, offering economists and policymakers richer insights into labor market trends.\n  In the comparative evaluation of the models, I employed a dataset comprising thirty distinct features over the period from January 2020 through December 2024."
      },
      {
        "id": "oai:arXiv.org:2505.01934v1",
        "title": "GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels",
        "link": "https://arxiv.org/abs/2505.01934",
        "author": "Yongxin Su, Lin Chen, Kaiting Zhang, Zhongliang Zhao, Chenfeng Hou, Ziping Yu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01934v1 Announce Type: new \nAbstract: We propose GauS-SLAM, a dense RGB-D SLAM system that leverages 2D Gaussian surfels to achieve robust tracking and high-fidelity mapping. Our investigations reveal that Gaussian-based scene representations exhibit geometry distortion under novel viewpoints, which significantly degrades the accuracy of Gaussian-based tracking methods. These geometry inconsistencies arise primarily from the depth modeling of Gaussian primitives and the mutual interference between surfaces during the depth blending. To address these, we propose a 2D Gaussian-based incremental reconstruction strategy coupled with a Surface-aware Depth Rendering mechanism, which significantly enhances geometry accuracy and multi-view consistency. Additionally, the proposed local map design dynamically isolates visible surfaces during tracking, mitigating misalignment caused by occluded regions in global maps while maintaining computational efficiency with increasing Gaussian density. Extensive experiments across multiple datasets demonstrate that GauS-SLAM outperforms comparable methods, delivering superior tracking precision and rendering fidelity. The project page will be made available at https://gaus-slam.github.io."
      },
      {
        "id": "oai:arXiv.org:2505.01938v1",
        "title": "HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder",
        "link": "https://arxiv.org/abs/2505.01938",
        "author": "Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01938v1 Announce Type: new \nAbstract: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on producing compact 3DGS representation via implicit data embedding. They have long coding times and highly customized data format, making it difficult for widespread deployment. This paper presents a new 3DGS compression framework called HybridGS, which takes advantage of both compact generation and standardized point cloud data encoding. HybridGS first generates compact and explicit 3DGS data. A dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. It then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. A simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. At the current stage, HybridGS does not include any modules aimed at improving 3DGS quality during generation. But experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. The code is publicly available at https://github.com/Qi-Yangsjtu/HybridGS."
      },
      {
        "id": "oai:arXiv.org:2505.01948v1",
        "title": "Multi-Scale Graph Learning for Anti-Sparse Downscaling",
        "link": "https://arxiv.org/abs/2505.01948",
        "author": "Yingda Fan, Runlong Yu, Janet R. Barclay, Alison P. Appling, Yiming Sun, Yiqun Xie, Xiaowei Jia",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01948v1 Announce Type: new \nAbstract: Water temperature can vary substantially even across short distances within the same sub-watershed. Accurate prediction of stream water temperature at fine spatial resolutions (i.e., fine scales, $\\leq$ 1 km) enables precise interventions to maintain water quality and protect aquatic habitats. Although spatiotemporal models have made substantial progress in spatially coarse time series modeling, challenges persist in predicting at fine spatial scales due to the lack of data at that scale.To address the problem of insufficient fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This method employs a multi-task learning framework where coarse-scale graph learning, bolstered by larger datasets, simultaneously enhances fine-scale graph learning. Although existing multi-scale or multi-resolution methods integrate data from different spatial scales, they often overlook the spatial correspondences across graph structures at various scales. To address this, our MSGL introduces an additional learning task, cross-scale interpolation learning, which leverages the hydrological connectedness of stream locations across coarse- and fine-scale graphs to establish cross-scale connections, thereby enhancing overall model performance. Furthermore, we have broken free from the mindset that multi-scale learning is limited to synchronous training by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL). Extensive experiments demonstrate the state-of-the-art performance of our method for anti-sparse downscaling of daily stream temperatures in the Delaware River Basin, USA, highlighting its potential utility for water resources monitoring and management."
      },
      {
        "id": "oai:arXiv.org:2505.01950v1",
        "title": "Segment Any RGB-Thermal Model with Language-aided Distillation",
        "link": "https://arxiv.org/abs/2505.01950",
        "author": "Dong Xing, Xianxun Zhu, Wei Zhou, Qika Lin, Hang Yang, Yuqing Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01950v1 Announce Type: new \nAbstract: The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation(CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions."
      },
      {
        "id": "oai:arXiv.org:2505.01954v1",
        "title": "Semantic Probabilistic Control of Language Models",
        "link": "https://arxiv.org/abs/2505.01954",
        "author": "Kareem Ahmed, Catarina G Belem, Padhraic Smyth, Sameer Singh",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01954v1 Announce Type: new \nAbstract: Semantic control entails steering LM generations towards satisfying subtle non-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes that can be captured by a sequence-level verifier. It can thus be viewed as sampling from the LM distribution conditioned on the target attribute, a computationally intractable problem due to the non-decomposable nature of the verifier. Existing approaches to LM control either only deal with syntactic constraints which cannot capture the aforementioned attributes, or rely on sampling to explore the conditional LM distribution, an ineffective estimator for low-probability events. In this work, we leverage a verifier's gradient information to efficiently reason over all generations that satisfy the target attribute, enabling precise steering of LM generations by reweighing the next-token distribution. Starting from an initial sample, we create a local LM distribution favoring semantically similar sentences. This approximation enables the tractable computation of an expected sentence embedding. We use this expected embedding, informed by the verifier's evaluation at the initial sample, to estimate the probability of satisfying the constraint, which directly informs the update to the next-token distribution. We evaluated the effectiveness of our approach in controlling the toxicity, sentiment, and topic-adherence of LMs yielding generations satisfying the constraint with high probability (>95%) without degrading their quality."
      },
      {
        "id": "oai:arXiv.org:2505.01958v1",
        "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2505.01958",
        "author": "Liqiang Jing, Guiming Hardy Chen, Ehsan Aghazadeh, Xin Eric Wang, Xinya Du",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01958v1 Announce Type: new \nAbstract: Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations."
      },
      {
        "id": "oai:arXiv.org:2505.01959v1",
        "title": "EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting",
        "link": "https://arxiv.org/abs/2505.01959",
        "author": "Leyi Yan, Linda Wang, Sihang Liu, Yi Ding",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01959v1 Announce Type: new \nAbstract: Carbon intensity (CI) measures the average carbon emissions generated per unit of electricity, making it a crucial metric for quantifying and managing the environmental impact. Accurate CI predictions are vital for minimizing carbon footprints, yet the state-of-the-art method (CarbonCast) falls short due to its inability to address regional variability and lack of adaptability. To address these limitations, we introduce EnsembleCI, an adaptive, end-to-end ensemble learning-based approach for CI forecasting. EnsembleCI combines weighted predictions from multiple sublearners, offering enhanced flexibility and regional adaptability. In evaluations across 11 regional grids, EnsembleCI consistently surpasses CarbonCast, achieving the lowest mean absolute percentage error (MAPE) in almost all grids and improving prediction accuracy by an average of 19.58%. While performance still varies across grids due to inherent regional diversity, EnsembleCI reduces variability and exhibits greater robustness in long-term forecasting compared to CarbonCast and identifies region-specific key features, underscoring its interpretability and practical relevance. These findings position EnsembleCI as a more accurate and reliable solution for CI forecasting. EnsembleCI source code and data used in this paper are available at https://github.com/emmayly/EnsembleCI."
      },
      {
        "id": "oai:arXiv.org:2505.01967v1",
        "title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview",
        "link": "https://arxiv.org/abs/2505.01967",
        "author": "Jiatao Li, Yanheng Li, Xiaojun Wan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01967v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have become integral to daily life, widely adopted in communication, decision-making, and information retrieval, raising critical questions about how these systems implicitly form and express socio-cognitive attitudes or \"worldviews\". While existing research extensively addresses demographic and ethical biases, broader dimensions-such as attitudes toward authority, equality, autonomy, and fate-remain under-explored. In this paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework grounded in Cultural Theory, operationalizing four canonical worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable sub-dimensions. Using SWT, we empirically identify distinct and interpretable cognitive profiles across 28 diverse LLMs. Further, inspired by Social Referencing Theory, we experimentally demonstrate that explicit social cues systematically shape these cognitive attitudes, revealing both general response patterns and nuanced model-specific variations. Our findings enhance the interpretability of LLMs by revealing implicit socio-cognitive biases and their responsiveness to social feedback, thus guiding the development of more transparent and socially responsible language technologies."
      },
      {
        "id": "oai:arXiv.org:2505.01969v1",
        "title": "MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection",
        "link": "https://arxiv.org/abs/2505.01969",
        "author": "Jiayi Cheng, Can Gao, Jie Zhou, Jiajun Wen, Tao Dai, Jinbao Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01969v1 Announce Type: new \nAbstract: 3D Anomaly Detection (AD) is a promising means of controlling the quality of manufactured products. However, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. Therefore, this paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. First, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. Then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. Finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. This leads to local and global geometry-aware reconstructed feature tokens for the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and Anomaly-ShapeNet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The source code will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2505.01973v1",
        "title": "Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques",
        "link": "https://arxiv.org/abs/2505.01973",
        "author": "Anthony Dontoh, Stephanie Ivey, Logan Sirbaugh, Andrews Danyo, Armstrong Aboah",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01973v1 Announce Type: new \nAbstract: Distracted driving continues to be a significant cause of road traffic injuries and fatalities worldwide, even with advancements in driver monitoring technologies. Recent developments in machine learning (ML) and deep learning (DL) have primarily focused on visual data to detect distraction, often neglecting the complex, multimodal nature of driver behavior. This systematic review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL techniques for distracted driving detection across visual, sensor-based, multimodal, and emerging modalities. The review highlights a significant prevalence of visual-only models, particularly convolutional neural networks (CNNs) and temporal architectures, which achieve high accuracy but show limited generalizability in real-world scenarios. Sensor-based and physiological models provide complementary strengths by capturing internal states and vehicle dynamics, while emerging techniques, such as auditory sensing and radio frequency (RF) methods, offer privacy-aware alternatives. Multimodal architecture consistently surpasses unimodal baselines, demonstrating enhanced robustness, context awareness, and scalability by integrating diverse data streams. These findings emphasize the need to move beyond visual-only approaches and adopt multimodal systems that combine visual, physiological, and vehicular cues while keeping in checking the need to balance computational requirements. Future research should focus on developing lightweight, deployable multimodal frameworks, incorporating personalized baselines, and establishing cross-modality benchmarks to ensure real-world reliability in advanced driver assistance systems (ADAS) and road safety interventions."
      },
      {
        "id": "oai:arXiv.org:2505.01979v1",
        "title": "D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection",
        "link": "https://arxiv.org/abs/2505.01979",
        "author": "Chenran Zhao, Dianxi Shi, Mengzhu Wang, Jianqiang Xia, Huanhuan Yang, Songchang Jin, Shaowu Yang, Chunping Qiu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01979v1 Announce Type: new \nAbstract: Current Hierarchical Reinforcement Learning (HRL) algorithms excel in long-horizon sequential decision-making tasks but still face two challenges: delay effects and spurious correlations. To address them, we propose a causal HRL approach called D3HRL. First, D3HRL models delayed effects as causal relationships across different time spans and employs distributed causal discovery to learn these relationships. Second, it employs conditional independence testing to eliminate spurious correlations. Finally, D3HRL constructs and trains hierarchical policies based on the identified true causal relationships. These three steps are iteratively executed, gradually exploring the complete causal chain of the task. Experiments conducted in 2D-MineCraft and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects and accurately identifies causal relationships, leading to reliable decision-making in complex environments."
      },
      {
        "id": "oai:arXiv.org:2505.01980v1",
        "title": "LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load",
        "link": "https://arxiv.org/abs/2505.01980",
        "author": "Theo Guidroz (Yifan), Diego Ardila (Yifan), Jimmy Li (Yifan), Adam Mansour (Yifan), Paul Jhun (Yifan), Nina Gonzalez (Yifan), Xiang Ji (Yifan), Mike Sanchez (Yifan), Sujay Kakarmath (Yifan), Mathias MJ Bellaiche (Yifan), Miguel \\'Angel Garrido (Yifan), Faruk Ahmed (Yifan), Divyansh Choudhary (Yifan), Jay Hartford (Yifan), Chenwei Xu (Yifan), Henry Javier Serrano Echeverria (Yifan), Yifan Wang (Yifan), Jeff Shaffer (Yifan),  Eric (Yifan),  Cao, Yossi Matias, Avinatan Hassidim, Dale R Webster, Yun Liu, Sho Fujiwara, Peggy Bui, Quang Duong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01980v1 Announce Type: new \nAbstract: Information on the web, such as scientific publications and Wikipedia, often surpasses users' reading level. To help address this, we used a self-refinement approach to develop a LLM capability for minimally lossy text simplification. To validate our approach, we conducted a randomized study involving 4563 participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical scientific articles), biology, law, finance, literature/philosophy, and aerospace/computer science. Participants were randomized to viewing original or simplified texts in a subject area, and answered multiple-choice questions (MCQs) that tested their comprehension of the text. The participants were also asked to provide qualitative feedback such as task difficulty. Our results indicate that participants who read the simplified text answered more MCQs correctly than their counterparts who read the original text (3.9% absolute increase, p<0.05). This gain was most striking with PubMed (14.6%), while more moderate gains were observed for finance (5.5%), aerospace/computer science (3.8%) domains, and legal (3.5%). Notably, the results were robust to whether participants could refer back to the text while answering MCQs. The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted. Finally, participants' self-reported perceived ease based on a simplified NASA Task Load Index was greater for those who read the simplified text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study, involving an order of magnitude more participants than prior works, demonstrates the potential of LLMs to make complex information easier to understand. Our work aims to enable a broader audience to better learn and make use of expert knowledge available on the web, improving information accessibility."
      },
      {
        "id": "oai:arXiv.org:2505.01984v1",
        "title": "Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation",
        "link": "https://arxiv.org/abs/2505.01984",
        "author": "Doanh C. Bui, Hoai Luan Pham, Vu Trung Duong Le, Tuan Hai Vu, Van Duy Tran, Khang Nguyen, Yasuhiko Nakashima",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01984v1 Announce Type: new \nAbstract: Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis and prognosis, as they provide tissue details at the cellular level. However, the rapid growth of computational tasks involving WSIs poses significant challenges. Given that WSIs are gigapixels in size, they present difficulties in terms of storage, processing, and model training. Therefore, it is essential to develop lifelong learning approaches for WSI analysis. In scenarios where slides are distributed across multiple institutes, we aim to leverage them to develop a unified online model as a computational tool for cancer diagnosis in clinical and hospital settings. In this study, we introduce ADaFGrad, a method designed to enhance lifelong learning for whole-slide image (WSI) analysis. First, we leverage pathology vision-language foundation models to develop a framework that enables interaction between a slide's regional tissue features and a predefined text-based prototype buffer. Additionally, we propose a gradient-distillation mechanism that mimics the gradient of a logit with respect to the classification-head parameters across past and current iterations in a continual-learning setting. We construct a sequence of six TCGA datasets for training and evaluation. Experimental results show that ADaFGrad outperforms both state-of-the-art WSI-specific and conventional continual-learning methods after only a few training epochs, exceeding them by up to +5.068% in the class-incremental learning scenario while exhibiting the least forgetting (i.e., retaining the most knowledge from previous tasks). Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy, further demonstrating the effectiveness of the proposed modules."
      },
      {
        "id": "oai:arXiv.org:2505.01986v1",
        "title": "Drug classification based on X-ray spectroscopy combined with machine learning",
        "link": "https://arxiv.org/abs/2505.01986",
        "author": "Yongming Li, Peng Wang, Bangdong Han",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01986v1 Announce Type: new \nAbstract: The proliferation of new types of drugs necessitates the urgent development of faster and more accurate detection methods. Traditional detection methods have high requirements for instruments and environments, making the operation complex. X-ray absorption spectroscopy, a non-destructive detection technique, offers advantages such as ease of operation, penetrative observation, and strong substance differentiation capabilities, making it well-suited for application in the field of drug detection and identification. In this study, we constructed a classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to classify and identify drugs based on their X-ray spectral profiles. In the experiments, we selected 14 chemical reagents with chemical formulas similar to drugs as samples. We utilized CNN to extract features from the spectral data of these 14 chemical reagents and used the extracted features to train an SVM model. We also utilized PSO to optimize two critical initial parameters of the SVM. The experimental results demonstrate that this model achieved higher classification accuracy compared to two other common methods, with a prediction accuracy of 99.14%. Additionally, the model exhibited fast execution speed, mitigating the drawback of a drastic increase in running time and efficiency reduction that may result from the direct fusion of PSO and SVM. Therefore, the combined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM provides a rapid, highly accurate, and reliable classification and identification method for the field of drug detection, holding promising prospects for widespread application."
      },
      {
        "id": "oai:arXiv.org:2505.01996v1",
        "title": "Always Skip Attention",
        "link": "https://arxiv.org/abs/2505.01996",
        "author": "Yiping Ji, Hemanth Saratchandran, Peyman Moghaddam, Simon Lucey",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01996v1 Announce Type: new \nAbstract: We highlight a curious empirical result within modern Vision Transformers (ViTs). Specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. This is in contrast to other elements of a ViT that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. Further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\\eg, CNNs) exhibiting good performance in their absence. In this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. Additionally, we propose Token Graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. We validate our approach in both supervised and self-supervised training methods."
      },
      {
        "id": "oai:arXiv.org:2505.01997v1",
        "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
        "link": "https://arxiv.org/abs/2505.01997",
        "author": "Jiancong Xiao, Bojian Hou, Zhanliang Wang, Ruochen Jin, Qi Long, Weijie J. Su, Li Shen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01997v1 Announce Type: new \nAbstract: One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods."
      },
      {
        "id": "oai:arXiv.org:2505.02005v1",
        "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields",
        "link": "https://arxiv.org/abs/2505.02005",
        "author": "Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02005v1 Announce Type: new \nAbstract: Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF."
      },
      {
        "id": "oai:arXiv.org:2505.02007v1",
        "title": "Efficient Noise Calculation in Deep Learning-based MRI Reconstructions",
        "link": "https://arxiv.org/abs/2505.02007",
        "author": "Onat Dalmaz, Arjun D. Desai, Reinhard Heckel, Tolga \\c{C}ukur, Akshay S. Chaudhari, Brian A. Hargreaves",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02007v1 Announce Type: new \nAbstract: Accelerated MRI reconstruction involves solving an ill-posed inverse problem where noise in acquired data propagates to the reconstructed images. Noise analyses are central to MRI reconstruction for providing an explicit measure of solution fidelity and for guiding the design and deployment of novel reconstruction methods. However, deep learning (DL)-based reconstruction methods have often overlooked noise propagation due to inherent analytical and computational challenges, despite its critical importance. This work proposes a theoretically grounded, memory-efficient technique to calculate voxel-wise variance for quantifying uncertainty due to acquisition noise in accelerated MRI reconstructions. Our approach approximates noise covariance using the DL network's Jacobian, which is intractable to calculate. To circumvent this, we derive an unbiased estimator for the diagonal of this covariance matrix (voxel-wise variance) and introduce a Jacobian sketching technique to efficiently implement it. We evaluate our method on knee and brain MRI datasets for both data- and physics-driven networks trained in supervised and unsupervised manners. Compared to empirical references obtained via Monte Carlo simulations, our technique achieves near-equivalent performance while reducing computational and memory demands by an order of magnitude or more. Furthermore, our method is robust across varying input noise levels, acceleration factors, and diverse undersampling schemes, highlighting its broad applicability. Our work reintroduces accurate and efficient noise analysis as a central tenet of reconstruction algorithms, holding promise to reshape how we evaluate and deploy DL-based MRI. Our code will be made publicly available upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2505.02009v1",
        "title": "Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs",
        "link": "https://arxiv.org/abs/2505.02009",
        "author": "Sai Krishna Mendu, Harish Yenala, Aditi Gulati, Shanu Kumar, Parag Agrawal",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02009v1 Announce Type: new \nAbstract: Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. Upon publishing, we will also opensource our model signal on the entire C4 dataset. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance."
      },
      {
        "id": "oai:arXiv.org:2505.02011v1",
        "title": "CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting",
        "link": "https://arxiv.org/abs/2505.02011",
        "author": "Minhyuk Lee, HyeKyung Yoon, MyungJoo Kang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02011v1 Announce Type: new \nAbstract: Multivariate long-term time series forecasting is critical for applications such as weather prediction, and traffic analysis. In addition, the implementation of Transformer variants has improved prediction accuracy. Following these variants, different input data process approaches also enhanced the field, such as tokenization techniques including point-wise, channel-wise, and patch-wise tokenization. However, previous studies still have limitations in time complexity, computational resources, and cross-dimensional interactions. To address these limitations, we introduce a novel CNN Autoencoder-based Score Attention mechanism (CASA), which can be introduced in diverse Transformers model-agnosticically by reducing memory and leading to improvement in model performance. Experiments on eight real-world datasets validate that CASA decreases computational resources by up to 77.7%, accelerates inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics."
      },
      {
        "id": "oai:arXiv.org:2505.02013v1",
        "title": "MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution",
        "link": "https://arxiv.org/abs/2505.02013",
        "author": "Siran Peng, Zipei Wang, Li Gao, Xiangyu Zhu, Tianshuo Zhang, Ajian Liu, Haoyuan Zhang, Zhen Lei",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02013v1 Announce Type: new \nAbstract: Reliable face forgery detection algorithms are crucial for countering the growing threat of deepfake-driven disinformation. Previous research has demonstrated the potential of Multimodal Large Language Models (MLLMs) in identifying manipulated faces. However, existing methods typically depend on either the Large Language Model (LLM) alone or an external detector to generate classification results, which often leads to sub-optimal integration of visual and textual modalities. In this paper, we propose VLF-FFD, a novel Vision-Language Fusion solution for MLLM-enhanced Face Forgery Detection. Our key contributions are twofold. First, we present EFF++, a frame-level, explainability-driven extension of the widely used FaceForensics++ (FF++) dataset. In EFF++, each manipulated video frame is paired with a textual annotation that describes both the forgery artifacts and the specific manipulation technique applied, enabling more effective and informative MLLM training. Second, we design a Vision-Language Fusion Network (VLF-Net) that promotes bidirectional interaction between visual and textual features, supported by a three-stage training pipeline to fully leverage its potential. VLF-FFD achieves state-of-the-art (SOTA) performance in both cross-dataset and intra-dataset evaluations, underscoring its exceptional effectiveness in face forgery detection."
      },
      {
        "id": "oai:arXiv.org:2505.02018v1",
        "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
        "link": "https://arxiv.org/abs/2505.02018",
        "author": "Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, Shi-min Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02018v1 Announce Type: new \nAbstract: Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here."
      },
      {
        "id": "oai:arXiv.org:2505.02020v1",
        "title": "Wide & Deep Learning for Node Classification",
        "link": "https://arxiv.org/abs/2505.02020",
        "author": "Yancheng Chen, Wenguo Yang, Zhipeng Jiang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02020v1 Announce Type: new \nAbstract: Wide & Deep, a simple yet effective learning architecture for recommendation systems developed by Google, has had a significant impact in both academia and industry due to its combination of the memorization ability of generalized linear models and the generalization ability of deep models. Graph convolutional networks (GCNs) remain dominant in node classification tasks; however, recent studies have highlighted issues such as heterophily and expressiveness, which focus on graph structure while seemingly neglecting the potential role of node features. In this paper, we propose a flexible framework GCNIII, which leverages the Wide & Deep architecture and incorporates three techniques: Intersect memory, Initial residual and Identity mapping. We provide comprehensive empirical evidence showing that GCNIII can more effectively balance the trade-off between over-fitting and over-generalization on various semi- and full- supervised tasks. Additionally, we explore the use of large language models (LLMs) for node feature engineering to enhance the performance of GCNIII in cross-domain node classification tasks. Our implementation is available at https://github.com/CYCUCAS/GCNIII."
      },
      {
        "id": "oai:arXiv.org:2505.02022v1",
        "title": "NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks",
        "link": "https://arxiv.org/abs/2505.02022",
        "author": "Yiming Zhang, Koji Tsuda",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02022v1 Announce Type: new \nAbstract: Nanobodies, single-domain antibody fragments derived from camelid heavy-chain-only antibodies, exhibit unique advantages such as compact size, high stability, and strong binding affinity, making them valuable tools in therapeutics and diagnostics. While recent advances in pretrained protein and antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular understanding, nanobody-specific modeling remains underexplored and lacks a unified benchmark. To address this gap, we introduce NbBench, the first comprehensive benchmark suite for nanobody representation learning. Spanning eight biologically meaningful tasks across nine curated datasets, NbBench encompasses structure annotation, binding prediction, and developability assessment. We systematically evaluate eleven representative models--including general-purpose protein LMs, antibody-specific LMs, and nanobody-specific LMs--in a frozen setting. Our analysis reveals that antibody language models excel in antigen-related tasks, while performance on regression tasks such as thermostability and affinity remains challenging across all models. Notably, no single model consistently outperforms others across all tasks. By standardizing datasets, task definitions, and evaluation protocols, NbBench offers a reproducible foundation for assessing and advancing nanobody modeling."
      },
      {
        "id": "oai:arXiv.org:2505.02025v1",
        "title": "A Birotation Solution for Relative Pose Problems",
        "link": "https://arxiv.org/abs/2505.02025",
        "author": "Hongbo Zhao, Ziwei Long, Mengtan Zhang, Hanli Wang, Qijun Chen, Rui Fan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02025v1 Announce Type: new \nAbstract: Relative pose estimation, a fundamental computer vision problem, has been extensively studied for decades. Existing methods either estimate and decompose the essential matrix or directly estimate the rotation and translation to obtain the solution. In this article, we break the mold by tackling this traditional problem with a novel birotation solution. We first introduce three basis transformations, each associated with a geometric metric to quantify the distance between the relative pose to be estimated and its corresponding basis transformation. Three energy functions, designed based on these metrics, are then minimized on the Riemannian manifold $\\mathrm{SO(3)}$ by iteratively updating the two rotation matrices. The two rotation matrices and the basis transformation corresponding to the minimum energy are ultimately utilized to recover the relative pose. Extensive quantitative and qualitative evaluations across diverse relative pose estimation tasks demonstrate the superior performance of our proposed birotation solution. Source code, demo video, and datasets will be available at \\href{https://mias.group/birotation-solution}{mias.group/birotation-solution} upon publication."
      },
      {
        "id": "oai:arXiv.org:2505.02027v1",
        "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning",
        "link": "https://arxiv.org/abs/2505.02027",
        "author": "Rui Lv, Zaixi Zhang, Kai Zhang, Qi Liu, Weibo Gao, Jiawei Liu, Jiaxia Yan, Linan Yue, Fangzhou Yao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02027v1 Announce Type: new \nAbstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter."
      },
      {
        "id": "oai:arXiv.org:2505.02032v1",
        "title": "An overview of artificial intelligence in computer-assisted language learning",
        "link": "https://arxiv.org/abs/2505.02032",
        "author": "Anisia Katinskaia",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02032v1 Announce Type: new \nAbstract: Computer-assisted language learning -- CALL -- is an established research field. We review how artificial intelligence can be applied to support language learning and teaching. The need for intelligent agents that assist language learners and teachers is increasing: the human teacher's time is a scarce and costly resource, which does not scale with growing demand. Further factors contribute to the need for CALL: pandemics and increasing demand for distance learning, migration of large populations, the need for sustainable and affordable support for learning, etc. CALL systems are made up of many components that perform various functions, and AI is applied to many different aspects in CALL, corresponding to their own expansive research areas. Most of what we find in the research literature and in practical use are prototypes or partial implementations -- systems that perform some aspects of the overall desired functionality. Complete solutions -- most of them commercial -- are few, because they require massive resources. Recent advances in AI should result in improvements in CALL, yet there is a lack of surveys that focus on AI in the context of this research field. This paper aims to present a perspective on the AI methods that can be employed for language learning from a position of a developer of a CALL system. We also aim to connect work from different disciplines, to build bridges for interdisciplinary work."
      },
      {
        "id": "oai:arXiv.org:2505.02033v1",
        "title": "Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles",
        "link": "https://arxiv.org/abs/2505.02033",
        "author": "Emine Akpinar, Batuhan Hangun, Murat Oduncuoglu, Oguz Altun, Onder Eyecioglu, Zeynel Yalcin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02033v1 Announce Type: new \nAbstract: DNA microarray technology enables the simultaneous measurement of expression levels of thousands of genes, thereby facilitating the understanding of the molecular mechanisms underlying complex diseases such as brain tumors and the identification of diagnostic genetic signatures. To derive meaningful biological insights from the high-dimensional and complex gene features obtained through this technology and to analyze gene properties in detail, classical AI-based approaches such as machine learning and deep learning are widely employed. However, these methods face various limitations in managing high-dimensional vector spaces and modeling the intricate relationships among genes. In particular, challenges such as hyperparameter tuning, computational costs, and high processing power requirements can hinder their efficiency. To overcome these limitations, quantum computing and quantum AI approaches are gaining increasing attention. Leveraging quantum properties such as superposition and entanglement, quantum methods enable more efficient parallel processing of high-dimensional data and offer faster and more effective solutions to problems that are computationally demanding for classical methods. In this study, a novel model called \"Deep VQC\" is proposed, based on the Variational Quantum Classifier approach. Developed using microarray data containing 54,676 gene features, the model successfully classified four different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and pilocytic astrocytoma-alongside healthy samples with high accuracy. Furthermore, compared to classical ML algorithms, our model demonstrated either superior or comparable classification performance. These results highlight the potential of quantum AI methods as an effective and promising approach for the analysis and classification of complex structures such as brain tumors based on gene expression features."
      },
      {
        "id": "oai:arXiv.org:2505.02035v1",
        "title": "Secrets of GFlowNets' Learning Behavior: A Theoretical Study",
        "link": "https://arxiv.org/abs/2505.02035",
        "author": "Tianshu Yu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02035v1 Announce Type: new \nAbstract: Generative Flow Networks (GFlowNets) have emerged as a powerful paradigm for generating composite structures, demonstrating considerable promise across diverse applications. While substantial progress has been made in exploring their modeling validity and connections to other generative frameworks, the theoretical understanding of their learning behavior remains largely uncharted. In this work, we present a rigorous theoretical investigation of GFlowNets' learning behavior, focusing on four fundamental dimensions: convergence, sample complexity, implicit regularization, and robustness. By analyzing these aspects, we seek to elucidate the intricate mechanisms underlying GFlowNet's learning dynamics, shedding light on its strengths and limitations. Our findings contribute to a deeper understanding of the factors influencing GFlowNet performance and provide insights into principled guidelines for their effective design and deployment. This study not only bridges a critical gap in the theoretical landscape of GFlowNets but also lays the foundation for their evolution as a reliable and interpretable framework for generative modeling. Through this, we aspire to advance the theoretical frontiers of GFlowNets and catalyze their broader adoption in the AI community."
      },
      {
        "id": "oai:arXiv.org:2505.02043v1",
        "title": "Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction",
        "link": "https://arxiv.org/abs/2505.02043",
        "author": "Cheng Wang, Xinzhu Ma, Bin Wang, Shixiang Tang, Yuan Meng, Ping Jiang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02043v1 Announce Type: new \nAbstract: Recovering CAD models from point clouds, especially the sketch-extrusion process, can be seen as the process of rebuilding the topology and extrusion primitives. Previous methods utilize implicit fields for sketch representation, leading to shape reconstruction of curved edges. In this paper, we proposed a CAD reconstruction network that produces editable CAD models from input point clouds (Point2Primitive) by directly predicting every element of the extrusion primitives. Point2Primitive can directly detect and predict sketch curves (type and parameter) from point clouds based on an improved transformer. The sketch curve parameters are formulated as position queries and optimized in an autoregressive way, leading to high parameter accuracy. The topology is rebuilt by extrusion segmentation, and each extrusion parameter (sketch and extrusion operation) is recovered by combining the predicted curves and the computed extrusion operation. Extensive experiments demonstrate that our method is superior in primitive prediction accuracy and CAD reconstruction. The reconstructed shapes are of high geometrical fidelity."
      },
      {
        "id": "oai:arXiv.org:2505.02046v1",
        "title": "A UNet Model for Accelerated Preprocessing of CRISM Hyperspectral Data for Mineral Identification on Mars",
        "link": "https://arxiv.org/abs/2505.02046",
        "author": "Priyanka Kumari, Sampriti Soor, Amba Shetty, Archana M. Nair",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02046v1 Announce Type: new \nAbstract: Accurate mineral identification on the Martian surface is critical for understanding the planet's geological history. This paper presents a UNet-based autoencoder model for efficient spectral preprocessing of CRISM MTRDR hyperspectral data, addressing the limitations of traditional methods that are computationally intensive and time-consuming. The proposed model automates key preprocessing steps, such as smoothing and continuum removal, while preserving essential mineral absorption features. Trained on augmented spectra from the MICA spectral library, the model introduces realistic variability to simulate MTRDR data conditions. By integrating this framework, preprocessing time for an 800x800 MTRDR scene is reduced from 1.5 hours to just 5 minutes on an NVIDIA T1600 GPU. The preprocessed spectra are subsequently classified using MICAnet, a deep learning model for Martian mineral identification. Evaluation on labeled CRISM TRDR data demonstrates that the proposed approach achieves competitive accuracy while significantly enhancing preprocessing efficiency. This work highlights the potential of the UNet-based preprocessing framework to improve the speed and reliability of mineral mapping on Mars."
      },
      {
        "id": "oai:arXiv.org:2505.02048v1",
        "title": "Regression s all you need for medical image translation",
        "link": "https://arxiv.org/abs/2505.02048",
        "author": "Sebastian Rassmann, David K\\\"ugler, Christian Ewert, Martin Reuter",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02048v1 Announce Type: new \nAbstract: The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging."
      },
      {
        "id": "oai:arXiv.org:2505.02056v1",
        "title": "Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin",
        "link": "https://arxiv.org/abs/2505.02056",
        "author": "Yuchen Wang, Xuefeng Bai, Xiucheng Li, Weili Guan, Liqiang Nie, Xinyang Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02056v1 Announce Type: new \nAbstract: Adapting vision-language models (VLMs) to downstream tasks with pseudolabels has gained increasing attention. A major obstacle is that the pseudolabels generated by VLMs tend to be imbalanced, leading to inferior performance. While existing methods have explored various strategies to address this, the underlying causes of imbalance remain insufficiently investigated. To fill this gap, we delve into imbalanced pseudolabels and identify two primary contributing factors: concept mismatch and concept confusion. To mitigate these two issues, we propose a novel framework incorporating concept alignment and confusion-aware calibrated margin mechanisms. The core of our approach lies in enhancing underperforming classes and promoting balanced predictions across categories, thus mitigating imbalance. Extensive experiments on six benchmark datasets with three learning paradigms demonstrate that the proposed method effectively enhances the accuracy and balance of pseudolabels, achieving a relative improvement of 6.29% over the SoTA method. Our code is avaliable at https://anonymous.4open.science/r/CAP-C642/"
      },
      {
        "id": "oai:arXiv.org:2505.02060v1",
        "title": "Transforming faces into video stories -- VideoFace2.0",
        "link": "https://arxiv.org/abs/2505.02060",
        "author": "Branko Brklja\\v{c}, Vladimir Kalu\\v{s}ev, Branislav Popovi\\'c, Milan Se\\v{c}ujski",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02060v1 Announce Type: new \nAbstract: Face detection and face recognition have been in the focus of vision community since the very beginnings. Inspired by the success of the original Videoface digitizer, a pioneering device that allowed users to capture video signals from any source, we have designed an advanced video analytics tool to efficiently create structured video stories, i.e. identity-based information catalogs. VideoFace2.0 is the name of the developed system for spatial and temporal localization of each unique face in the input video, i.e. face re-identification (ReID), which also allows their cataloging, characterization and creation of structured video outputs for later downstream tasks. Developed near real-time solution is primarily designed to be utilized in application scenarios involving TV production, media analysis, and as an efficient tool for creating large video datasets necessary for training machine learning (ML) models in challenging vision tasks such as lip reading and multimodal speech recognition. Conducted experiments confirm applicability of the proposed face ReID algorithm that is combining the concepts of face detection, face recognition and passive tracking-by-detection in order to achieve robust and efficient face ReID. The system is envisioned as a compact and modular extensions of the existing video production equipment. We hope that the presented work and shared code will stimulate further interest in development of similar, application specific video analysis tools, and lower the entry barrier for production of high-quality multi-modal ML datasets in the future."
      },
      {
        "id": "oai:arXiv.org:2505.02064v1",
        "title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video",
        "link": "https://arxiv.org/abs/2505.02064",
        "author": "Shuhang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, Linghao Zhang, Shikang Wang, Yixin Liu, Hanbo Zhang, Xuming Hu, Ying Ma",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02064v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) increasingly excel at perception, understanding, and reasoning. However, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. To bridge this gap, we introduce RTV-Bench, a fine-grained benchmark for MLLM real-time video analysis. RTV-Bench uses three key principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve with scene changes; (2) Hierarchical Question Structure, combining basic and advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability of continuous perception, understanding, and reasoning. RTV-Bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline (Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5, InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. Our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost RTV-Bench performance, sometimes causing slight decreases. This underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with MLLMs. Our benchmark toolkit is available at: https://github.com/LJungang/RTV-Bench."
      },
      {
        "id": "oai:arXiv.org:2505.02069v1",
        "title": "Neural Logistic Bandits",
        "link": "https://arxiv.org/abs/2505.02069",
        "author": "Seoungbin Bae, Dabeen Lee",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02069v1 Announce Type: new \nAbstract: We study the problem of neural logistic bandits, where the main task is to learn an unknown reward function within a logistic link function using a neural network. Existing approaches either exhibit unfavorable dependencies on $\\kappa$, where $1/\\kappa$ represents the minimum variance of reward distributions, or suffer from direct dependence on the feature dimension $d$, which can be huge in neural network-based settings. In this work, we introduce a novel Bernstein-type inequality for self-normalized vector-valued martingales that is designed to bypass a direct dependence on the ambient dimension. This lets us deduce a regret upper bound that grows with the effective dimension $\\widetilde{d}$, not the feature dimension, while keeping a minimal dependence on $\\kappa$. Based on the concentration inequality, we propose two algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of order $\\widetilde{O}(\\widetilde{d}\\sqrt{\\kappa T})$ and $\\widetilde{O}(\\widetilde{d}\\sqrt{T/\\kappa})$, respectively, improving on the existing results. Lastly, we report numerical results on both synthetic and real datasets to validate our theoretical findings."
      },
      {
        "id": "oai:arXiv.org:2505.02071v1",
        "title": "Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning",
        "link": "https://arxiv.org/abs/2505.02071",
        "author": "Can K\\\"u\\c{c}\\\"uks\\\"ozen, Y\\\"ucel Yemez",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02071v1 Announce Type: new \nAbstract: We propose the Compact Clustering Attention (COCA) layer, an effective building block that introduces a hierarchical strategy for object-centric representation learning, while solving the unsupervised object discovery task on single images. COCA is an attention-based clustering module capable of extracting object-centric representations from multi-object scenes, when cascaded into a bottom-up hierarchical network architecture, referred to as COCA-Net. At its core, COCA utilizes a novel clustering algorithm that leverages the physical concept of compactness, to highlight distinct object centroids in a scene, providing a spatial inductive bias. Thanks to this strategy, COCA-Net generates high-quality segmentation masks on both the decoder side and, notably, the encoder side of its pipeline. Additionally, COCA-Net is not bound by a predetermined number of object masks that it generates and handles the segmentation of background elements better than its competitors. We demonstrate COCA-Net's segmentation performance on six widely adopted datasets, achieving superior or competitive results against the state-of-the-art models across nine different evaluation metrics."
      },
      {
        "id": "oai:arXiv.org:2505.02072v1",
        "title": "What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction",
        "link": "https://arxiv.org/abs/2505.02072",
        "author": "Eitan Wagner, Omri Abend",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02072v1 Announce Type: new \nAbstract: The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions."
      },
      {
        "id": "oai:arXiv.org:2505.02073v1",
        "title": "Lightweight Defense Against Adversarial Attacks in Time Series Classification",
        "link": "https://arxiv.org/abs/2505.02073",
        "author": "Yi Han (Independent Researcher, Australia)",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02073v1 Announce Type: new \nAbstract: As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research."
      },
      {
        "id": "oai:arXiv.org:2505.02074v1",
        "title": "Learning Local Causal World Models with State Space Models and Attention",
        "link": "https://arxiv.org/abs/2505.02074",
        "author": "Francesco Petri, Luigi Asprino, Aldo Gangemi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02074v1 Announce Type: new \nAbstract: World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world. Despite their impressive performance, many solutions fail to learn a causal representation of the environment they are trying to model, which would be necessary to gain a deep enough understanding of the world to perform complex tasks. With this work, we aim to broaden the research in the intersection of causality theory and neural world modelling by assessing the potential for causal discovery of the State Space Model (SSM) architecture, which has been shown to have several advantages over the widespread Transformer. We show empirically that, compared to an equivalent Transformer, a SSM can model the dynamics of a simple environment and learn a causal model at the same time with equivalent or better performance, thus paving the way for further experiments that lean into the strength of SSMs and further enhance them with causal awareness."
      },
      {
        "id": "oai:arXiv.org:2505.02075v1",
        "title": "Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation",
        "link": "https://arxiv.org/abs/2505.02075",
        "author": "Volodymyr Havrylov, Haiwen Huang, Dan Zhang, Andreas Geiger",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02075v1 Announce Type: new \nAbstract: Vision Foundation Models (VFMs) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. As VFMs' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. However, VFMs typically produce low-resolution features, limiting their direct applicability in this context. One way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines VFM features resolution. To assess the effectiveness of this approach, we investigate Interactive Segmentation (IS) as a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, IS creates a challenging environment that demands comprehensive visual scene understanding. Our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves VFM features quality. The code is released at https://github.com/havrylovv/iSegProbe"
      },
      {
        "id": "oai:arXiv.org:2505.02078v1",
        "title": "LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning",
        "link": "https://arxiv.org/abs/2505.02078",
        "author": "Joy Lim Jia Yin, Daniel Zhang-Li, Jifan Yu, Haoxuan Li, Shangqing Tu, Yuanchun Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li, Bin Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02078v1 Announce Type: new \nAbstract: Evaluating the quality of slide-based multimedia instruction is challenging. Existing methods like manual assessment, reference-based metrics, and large language model evaluators face limitations in scalability, context capture, or bias. In this paper, we introduce LecEval, an automated metric grounded in Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal knowledge acquisition in slide-based learning. LecEval assesses effectiveness using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset of over 2,000 slides from more than 50 online course videos, annotated with fine-grained human ratings across these rubrics. A model trained on this dataset demonstrates superior accuracy and adaptability compared to existing metrics, bridging the gap between automated and human assessments. We release our dataset and toolkits at https://github.com/JoylimJY/LecEval."
      },
      {
        "id": "oai:arXiv.org:2505.02079v1",
        "title": "HandOcc: NeRF-based Hand Rendering with Occupancy Networks",
        "link": "https://arxiv.org/abs/2505.02079",
        "author": "Maksym Ivashechkin, Oscar Mendez, Richard Bowden",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02079v1 Announce Type: new \nAbstract: We propose HandOcc, a novel framework for hand rendering based upon occupancy. Popular rendering methods such as NeRF are often combined with parametric meshes to provide deformable hand models. However, in doing so, such approaches present a trade-off between the fidelity of the mesh and the complexity and dimensionality of the parametric model. The simplicity of parametric mesh structures is appealing, but the underlying issue is that it binds methods to mesh initialization, making it unable to generalize to objects where a parametric model does not exist. It also means that estimation is tied to mesh resolution and the accuracy of mesh fitting. This paper presents a pipeline for meshless 3D rendering, which we apply to the hands. By providing only a 3D skeleton, the desired appearance is extracted via a convolutional model. We do this by exploiting a NeRF renderer conditioned upon an occupancy-based representation. The approach uses the hand occupancy to resolve hand-to-hand interactions further improving results, allowing fast rendering, and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset, we achieved state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2505.02091v1",
        "title": "LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications",
        "link": "https://arxiv.org/abs/2505.02091",
        "author": "Xinyue Peng, Yanming Liu, Yihan Cang, Chaoqun Cao, Ming Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02091v1 Announce Type: new \nAbstract: Solving non-convex resource allocation problems poses significant challenges in wireless communication systems, often beyond the capability of traditional optimization techniques. To address this issue, we propose LLM-OptiRA, the first framework that leverages large language models (LLMs) to automatically detect and transform non-convex components into solvable forms, enabling fully automated resolution of non-convex resource allocation problems in wireless communication systems. LLM-OptiRA not only simplifies problem-solving by reducing reliance on expert knowledge, but also integrates error correction and feasibility validation mechanisms to ensure robustness. Experimental results show that LLM-OptiRA achieves an execution rate of 96% and a success rate of 80% on GPT-4, significantly outperforming baseline approaches in complex optimization tasks across diverse scenarios."
      },
      {
        "id": "oai:arXiv.org:2505.02094v1",
        "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations",
        "link": "https://arxiv.org/abs/2505.02094",
        "author": "Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, Qifeng Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02094v1 Announce Type: new \nAbstract: We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness."
      },
      {
        "id": "oai:arXiv.org:2505.02096v1",
        "title": "TeMTG: Text-Enhanced Multi-Hop Temporal Graph Modeling for Audio-Visual Video Parsing",
        "link": "https://arxiv.org/abs/2505.02096",
        "author": "Yaru Chen, Peiliang Zhang, Fei Li, Faegheh Sardari, Ruohao Guo, Zhenbo Li, Wenwu Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02096v1 Announce Type: new \nAbstract: Audio-Visual Video Parsing (AVVP) task aims to parse the event categories and occurrence times from audio and visual modalities in a given video. Existing methods usually focus on implicitly modeling audio and visual features through weak labels, without mining semantic relationships for different modalities and explicit modeling of event temporal dependencies. This makes it difficult for the model to accurately parse event information for each segment under weak supervision, especially when high similarity between segmental modal features leads to ambiguous event boundaries. Hence, we propose a multimodal optimization framework, TeMTG, that combines text enhancement and multi-hop temporal graph modeling. Specifically, we leverage pre-trained multimodal models to generate modality-specific text embeddings, and fuse them with audio-visual features to enhance the semantic representation of these features. In addition, we introduce a multi-hop temporal graph neural network, which explicitly models the local temporal relationships between segments, capturing the temporal continuity of both short-term and long-range events. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators in the LLP dataset."
      },
      {
        "id": "oai:arXiv.org:2505.02105v1",
        "title": "Deep Representation Learning for Electronic Design Automation",
        "link": "https://arxiv.org/abs/2505.02105",
        "author": "Pratik Shrestha, Saran Phatharodom, Alec Aversa, David Blankenship, Zhengfeng Wu, Ioannis Savidis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02105v1 Announce Type: new \nAbstract: Representation learning has become an effective technique utilized by electronic design automation (EDA) algorithms, which leverage the natural representation of workflow elements as images, grids, and graphs. By addressing challenges related to the increasing complexity of circuits and stringent power, performance, and area (PPA) requirements, representation learning facilitates the automatic extraction of meaningful features from complex data formats, including images, grids, and graphs. This paper examines the application of representation learning in EDA, covering foundational concepts and analyzing prior work and case studies on tasks that include timing prediction, routability analysis, and automated placement. Key techniques, including image-based methods, graph-based approaches, and hybrid multimodal solutions, are presented to illustrate the improvements provided in routing, timing, and parasitic prediction. The provided advancements demonstrate the potential of representation learning to enhance efficiency, accuracy, and scalability in current integrated circuit design flows."
      },
      {
        "id": "oai:arXiv.org:2505.02108v1",
        "title": "SignSplat: Rendering Sign Language via Gaussian Splatting",
        "link": "https://arxiv.org/abs/2505.02108",
        "author": "Maksym Ivashechkin, Oscar Mendez, Richard Bowden",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02108v1 Announce Type: new \nAbstract: State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches."
      },
      {
        "id": "oai:arXiv.org:2505.02109v1",
        "title": "Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance",
        "link": "https://arxiv.org/abs/2505.02109",
        "author": "Yingkai Zhang, Zeqiang Lai, Tao Zhang, Ying Fu, Chenghu Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02109v1 Announce Type: new \nAbstract: Hyperspectral images super-resolution aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. The recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution HSIs, presenting it as a favorable method. However, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. In this paper, we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution (SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the issues of inaccurate alignment and poor interactivity of the previous approaches. Specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a Two-Stage Image Alignment with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. To enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a Feature Aggregation module and an Attention Fusion module. In the feature aggregation module, we introduce an Iterative Deformable Feature Aggregation block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. Besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. Extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations."
      },
      {
        "id": "oai:arXiv.org:2505.02124v1",
        "title": "GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code",
        "link": "https://arxiv.org/abs/2505.02124",
        "author": "Samidha Verma, Arushi Goyal, Ananya Mathur, Ankit Anand, Sayan Ranu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02124v1 Announce Type: new \nAbstract: Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a program that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions."
      },
      {
        "id": "oai:arXiv.org:2505.02126v1",
        "title": "GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction",
        "link": "https://arxiv.org/abs/2505.02126",
        "author": "Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02126v1 Announce Type: new \nAbstract: Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality."
      },
      {
        "id": "oai:arXiv.org:2505.02134v1",
        "title": "HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement",
        "link": "https://arxiv.org/abs/2505.02134",
        "author": "Xiaorui Zhao, Xinyue Zhou, Peibei Cao, Junyu Lou, Shuhang Gu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02134v1 Announce Type: new \nAbstract: Developing effective approaches to generate enhanced results that align well with human visual preferences for high-quality well-lit images remains a challenge in low-light image enhancement (LLIE). In this paper, we propose a human-in-the-loop LLIE training framework that improves the visual quality of unsupervised LLIE model outputs through iterative training stages, named HiLLIE. At each stage, we introduce human guidance into the training process through efficient visual quality annotations of enhanced outputs. Subsequently, we employ a tailored image quality assessment (IQA) model to learn human visual preferences encoded in the acquired labels, which is then utilized to guide the training process of an enhancement model. With only a small amount of pairwise ranking annotations required at each stage, our approach continually improves the IQA model's capability to simulate human visual assessment of enhanced outputs, thus leading to visually appealing LLIE results. Extensive experiments demonstrate that our approach significantly improves unsupervised LLIE model performance in terms of both quantitative and qualitative performance. The code and collected ranking dataset will be available at https://github.com/LabShuHangGU/HiLLIE."
      },
      {
        "id": "oai:arXiv.org:2505.02138v1",
        "title": "Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation",
        "link": "https://arxiv.org/abs/2505.02138",
        "author": "Chenxi Liu, Shaowen Zhou, Hao Miao, Qianxiong Xu, Cheng Long, Ziyue Li, Rui Zhao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02138v1 Announce Type: new \nAbstract: Multivariate time series forecasting (MTSF) endeavors to predict future observations given historical data, playing a crucial role in time series data management systems. With advancements in large language models (LLMs), recent studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF. However, the deployment of LLMs often suffers from low efficiency during the inference phase. To address this problem, we introduce TimeKD, an efficient MTSF framework that leverages the calibrated language models and privileged knowledge distillation. TimeKD aims to generate high-quality future representations from the proposed cross-modality teacher model and cultivate an effective student model. The cross-modality teacher model adopts calibrated language models (CLMs) with ground truth prompts, motivated by the paradigm of Learning Under Privileged Information (LUPI). In addition, we design a subtractive cross attention (SCA) mechanism to refine these representations. To cultivate an effective student model, we propose an innovative privileged knowledge distillation (PKD) mechanism including correlation and feature distillation. PKD enables the student to replicate the teacher's behavior while minimizing their output discrepancy. Extensive experiments on real data offer insight into the effectiveness, efficiency, and scalability of the proposed TimeKD."
      },
      {
        "id": "oai:arXiv.org:2505.02142v1",
        "title": "Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study",
        "link": "https://arxiv.org/abs/2505.02142",
        "author": "Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02142v1 Announce Type: new \nAbstract: Despite significant advances in long-context reasoning by large language models (LLMs), primarily through Online Reinforcement Learning (RL) methods, these approaches incur substantial computational costs and complexity. In contrast, simpler and more economical Offline RL methods remain underexplored. To address this gap, we investigate the effectiveness of Offline RL methods, specifically Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive experiments across multiple reasoning benchmarks demonstrate that these simpler Offline RL methods substantially improve model performance, achieving an average enhancement of 3.3\\%, with a particularly notable increase of 10.1\\% on the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity to output length, emphasizing that increasing reasoning length should align with semantic richness, as indiscriminate lengthening may adversely affect model performance. We provide comprehensive descriptions of our data processing and training methodologies, offering empirical evidence and practical insights for developing more cost-effective Offline RL approaches."
      },
      {
        "id": "oai:arXiv.org:2505.02146v1",
        "title": "QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach",
        "link": "https://arxiv.org/abs/2505.02146",
        "author": "Shouyang Dong, Yuanbo Wen, Jun Bi, Di Huang, Jiaming Guo, Jianxing Xu, Ruibai Xu, Xinkai Song, Yifan Hao, Xuehai Zhou, Tianshi Chen, Qi Guo, Yunji Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02146v1 Announce Type: new \nAbstract: Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering \"Write Once, Run Anywhere\" of tensor programs an open question.\n  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average, and the performance of translated programs achieves up to 2.0x over vendor-provided manually-optimized libraries. As a result, the programming productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor programs."
      },
      {
        "id": "oai:arXiv.org:2505.02147v1",
        "title": "Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora",
        "link": "https://arxiv.org/abs/2505.02147",
        "author": "Prajwal Thapa, Mridul Sharma, Jinu Nyachhyon, Yagya Raj Pandeya",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02147v1 Announce Type: new \nAbstract: Herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as Nepal. This study introduces a novel deep learning approach for classifying 60 different herb species using Convolutional Neural Networks (CNNs) and transfer learning techniques. Using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. Our research employed multiple model architectures, including DenseNet121, 50-layer Residual Network (ResNet50), 16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2, and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating superior performance. Data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. This work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization."
      },
      {
        "id": "oai:arXiv.org:2505.02148v1",
        "title": "Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving",
        "link": "https://arxiv.org/abs/2505.02148",
        "author": "Alexey Nekrasov, Malcolm Burdorf, Stewart Worrall, Bastian Leibe, Julie Stephany Berrio Perez",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02148v1 Announce Type: new \nAbstract: To operate safely, autonomous vehicles (AVs) need to detect and handle unexpected objects or anomalies on the road. While significant research exists for anomaly detection and segmentation in 2D, research progress in 3D is underexplored. Existing datasets lack high-quality multimodal data that are typically found in AVs. This paper presents a novel dataset for anomaly segmentation in driving scenarios. To the best of our knowledge, it is the first publicly available dataset focused on road anomaly segmentation with dense 3D semantic labeling, incorporating both LiDAR and camera data, as well as sequential information to enable anomaly detection across various ranges. This capability is critical for the safe navigation of autonomous vehicles. We adapted and evaluated several baseline models for 3D segmentation, highlighting the challenges of 3D anomaly detection in driving environments. Our dataset and evaluation code will be openly available, facilitating the testing and performance comparison of different approaches."
      },
      {
        "id": "oai:arXiv.org:2505.02156v1",
        "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents",
        "link": "https://arxiv.org/abs/2505.02156",
        "author": "Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02156v1 Announce Type: new \nAbstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach"
      },
      {
        "id": "oai:arXiv.org:2505.02159v1",
        "title": "Small Clips, Big Gains: Learning Long-Range Refocused Temporal Information for Video Super-Resolution",
        "link": "https://arxiv.org/abs/2505.02159",
        "author": "Xingyu Zhou, Wei Long, Jingbo Lu, Shiyin Jiang, Weiyi You, Haifeng Wu, Shuhang Gu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02159v1 Announce Type: new \nAbstract: Video super-resolution (VSR) can achieve better performance compared to single image super-resolution by additionally leveraging temporal information. In particular, the recurrent-based VSR model exploits long-range temporal information during inference and achieves superior detail restoration. However, effectively learning these long-term dependencies within long videos remains a key challenge. To address this, we propose LRTI-VSR, a novel training framework for recurrent VSR that efficiently leverages Long-Range Refocused Temporal Information. Our framework includes a generic training strategy that utilizes temporal propagation features from long video clips while training on shorter video clips. Additionally, we introduce a refocused intra&amp;inter-frame transformer block which allows the VSR model to selectively prioritize useful temporal information through its attention module while further improving inter-frame information utilization in the FFN module. We evaluate LRTI-VSR on both CNN and transformer-based VSR architectures, conducting extensive ablation studies to validate the contribution of each component. Experiments on long-video test sets demonstrate that LRTI-VSR achieves state-of-the-art performance while maintaining training and computational efficiency."
      },
      {
        "id": "oai:arXiv.org:2505.02161v1",
        "title": "Focus What Matters: Matchability-Based Reweighting for Local Feature Matching",
        "link": "https://arxiv.org/abs/2505.02161",
        "author": "Dongyue Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02161v1 Announce Type: new \nAbstract: Since the rise of Transformers, many semi-dense matching methods have adopted attention mechanisms to extract feature descriptors. However, the attention weights, which capture dependencies between pixels or keypoints, are often learned from scratch. This approach can introduce redundancy and noisy interactions from irrelevant regions, as it treats all pixels or keypoints equally. Drawing inspiration from keypoint selection processes, we propose to first classify all pixels into two categories: matchable and non-matchable. Matchable pixels are expected to receive higher attention weights, while non-matchable ones are down-weighted. In this work, we propose a novel attention reweighting mechanism that simultaneously incorporates a learnable bias term into the attention logits and applies a matchability-informed rescaling to the input value features. The bias term, injected prior to the softmax operation, selectively adjusts attention scores based on the confidence of query-key interactions. Concurrently, the feature rescaling acts post-attention by modulating the influence of each value vector in the final output. This dual design allows the attention mechanism to dynamically adjust both its internal weighting scheme and the magnitude of its output representations. Extensive experiments conducted on three benchmark datasets validate the effectiveness of our method, consistently outperforming existing state-of-the-art approaches."
      },
      {
        "id": "oai:arXiv.org:2505.02164v1",
        "title": "Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use",
        "link": "https://arxiv.org/abs/2505.02164",
        "author": "Justin Ho, Alexandra Colby, William Fisher",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02164v1 Announce Type: new \nAbstract: This paper presents a domain-specific implementation of Retrieval-Augmented Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law. Motivated by the increasing prevalence of DMCA takedowns and the lack of accessible legal support for content creators, we propose a structured approach that combines semantic search with legal knowledge graphs and court citation networks to improve retrieval quality and reasoning reliability. Our prototype models legal precedents at the statutory factor level (e.g., purpose, nature, amount, market effect) and incorporates citation-weighted graph representations to prioritize doctrinally authoritative sources. We use Chain-of-Thought reasoning and interleaved retrieval steps to better emulate legal reasoning. Preliminary testing suggests this method improves doctrinal relevance in the retrieval process, laying groundwork for future evaluation and deployment of LLM-based legal assistance tools."
      },
      {
        "id": "oai:arXiv.org:2505.02171v1",
        "title": "A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking",
        "link": "https://arxiv.org/abs/2505.02171",
        "author": "Henrik Br{\\aa}dland, Morten Goodwin, Per-Arne Andersen, Alexander S. Nossum, Aditya Gupta",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02171v1 Announce Type: new \nAbstract: Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p > 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses."
      },
      {
        "id": "oai:arXiv.org:2505.02172v1",
        "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization",
        "link": "https://arxiv.org/abs/2505.02172",
        "author": "Chuck Arvin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02172v1 Announce Type: new \nAbstract: As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate ``scaling effects'' - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks."
      },
      {
        "id": "oai:arXiv.org:2505.02175v1",
        "title": "SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2505.02175",
        "author": "Shubhendu Jena, Shishir Reddy Vutukur, Adnane Boukhayma",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02175v1 Announce Type: new \nAbstract: Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed."
      },
      {
        "id": "oai:arXiv.org:2505.02176v1",
        "title": "Saliency-Guided Training for Fingerprint Presentation Attack Detection",
        "link": "https://arxiv.org/abs/2505.02176",
        "author": "Samuel Webster, Adam Czajka",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02176v1 Announce Type: new \nAbstract: Saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (PAD) tasks. This paper presents its first application to fingerprint PAD. We conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated \"pseudosaliency,\" including minutiae-based, image quality-based, and autoencoder-based saliency maps. Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. Our findings demonstrate the effectiveness of saliency-guided training for fingerprint PAD in both limited and large data contexts, and we present a configuration capable of earning the first place on the LivDet-2021 benchmark. Our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint PAD. All collected saliency data and trained models are released with the paper to support reproducible research."
      },
      {
        "id": "oai:arXiv.org:2505.02177v1",
        "title": "Measuring Hong Kong Massive Multi-Task Language Understanding",
        "link": "https://arxiv.org/abs/2505.02177",
        "author": "Chuxue Cao, Zhenghao Zhu, Junqi Zhu, Guoying Lu, Siyu Peng, Juntao Dai, Weijie Shi, Sirui Han, Yike Guo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02177v1 Announce Type: new \nAbstract: Multilingual understanding is crucial for the cross-cultural applicability of Large Language Models (LLMs). However, evaluation benchmarks designed for Hong Kong's unique linguistic landscape, which combines Traditional Chinese script with Cantonese as the spoken form and its cultural context, remain underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language understanding benchmark that evaluates Hong Kong's linguistic competence and socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions across 66 subjects, organized into four categories: Science, Technology, Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To evaluate the multilingual understanding ability of LLMs, 90,550 Mandarin-Cantonese translation tasks were additionally included. We conduct comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs of varying sizes on HKMMLU. The results show that the best-performing model, DeepSeek-V3, struggles to achieve an accuracy of 75\\%, significantly lower than that of MMLU and CMMLU. This performance gap highlights the need to improve LLMs' capabilities in Hong Kong-specific language and knowledge domains. Furthermore, we investigate how question language, model size, prompting strategies, and question and reasoning token lengths affect model performance. We anticipate that HKMMLU will significantly advance the development of LLMs in multilingual and cross-cultural contexts, thereby enabling broader and more impactful applications."
      },
      {
        "id": "oai:arXiv.org:2505.02178v1",
        "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery",
        "link": "https://arxiv.org/abs/2505.02178",
        "author": "Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02178v1 Announce Type: new \nAbstract: We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets."
      },
      {
        "id": "oai:arXiv.org:2505.02179v1",
        "title": "ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications",
        "link": "https://arxiv.org/abs/2505.02179",
        "author": "Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02179v1 Announce Type: new \nAbstract: Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP, demonstrating exceptional efficiency alongside state-of-the-art performance. Code is available at https://github.com/modadundun/ProDisc-VAD."
      },
      {
        "id": "oai:arXiv.org:2505.02181v1",
        "title": "Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning",
        "link": "https://arxiv.org/abs/2505.02181",
        "author": "Shengyu Duan, Marcos L. L. Sartori, Rishad Shafik, Alex Yakovlev, Emre Ozer",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02181v1 Announce Type: new \nAbstract: Population count (popcount) is a crucial operation for many low-complexity machine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising new ML method, particularly well-suited for solving classification tasks. The inference mechanism in TM consists of propositional logic-based structures within each class, followed by a majority voting scheme, which makes the classification decision. In TM, the voters are the outputs of Boolean clauses. The voting mechanism comprises two operations: popcount for each class and determining the class with the maximum vote by means of an argmax operation.\n  While TMs offer a lightweight ML alternative, their performance is often limited by the high computational cost of popcount and comparison required to produce the argmax result. In this paper, we propose an innovative approach to accelerate and optimize these operations by performing them in the time domain. Our time-domain implementation uses programmable delay lines (PDLs) and arbiters to efficiently manage these tasks through delay-based mechanisms. We also present an FPGA design flow for practical implementation of the time-domain popcount, addressing delay skew and ensuring that the behavior matches that of the model's intended functionality. By leveraging the natural compatibility of the proposed popcount with asynchronous architectures, we demonstrate significant improvements in an asynchronous TM, including up to 38% reduction in latency, 43.1% reduction in dynamic power, and 15% savings in resource utilization, compared to synchronous TMs using adder-based popcount."
      },
      {
        "id": "oai:arXiv.org:2505.02182v1",
        "title": "Robust AI-Generated Face Detection with Imbalanced Data",
        "link": "https://arxiv.org/abs/2505.02182",
        "author": "Yamini Sri Krubha, Aryana Hou, Braden Vester, Web Walker, Xin Wang, Li Lin, Shu Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02182v1 Announce Type: new \nAbstract: Deepfakes, created using advanced AI techniques such as Variational Autoencoder and Generative Adversarial Networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. Current deepfake detection techniques have evolved from CNN-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like CLIP, which capture global anomalies and improve cross-domain generalization. Despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. To address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. The code is available at https://github.com/Purdue-M2/SP_CUP."
      },
      {
        "id": "oai:arXiv.org:2505.02192v1",
        "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization",
        "link": "https://arxiv.org/abs/2505.02192",
        "author": "Wenchuan Wang, Mengqi Huang, Yijing Tu, Zhendong Mao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02192v1 Announce Type: new \nAbstract: Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. To address this, we introduce DualReal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics."
      },
      {
        "id": "oai:arXiv.org:2505.02206v1",
        "title": "DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units",
        "link": "https://arxiv.org/abs/2505.02206",
        "author": "Lei Mao, Yuanhe Tian, Yan Song",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02206v1 Announce Type: new \nAbstract: Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks."
      },
      {
        "id": "oai:arXiv.org:2505.02212v1",
        "title": "Exogenous Isomorphism for Counterfactual Identifiability",
        "link": "https://arxiv.org/abs/2505.02212",
        "author": "Yikang Chen, Dehui Du",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02212v1 Announce Type: new \nAbstract: This paper investigates $\\sim_{\\mathcal{L}_3}$-identifiability, a form of complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH) framework, ensuring that all Structural Causal Models (SCMs) satisfying the given assumptions provide consistent answers to all causal questions. To simplify this problem, we introduce exogenous isomorphism and propose $\\sim_{\\mathrm{EI}}$-identifiability, reflecting the strength of model identifiability required for $\\sim_{\\mathcal{L}_3}$-identifiability. We explore sufficient assumptions for achieving $\\sim_{\\mathrm{EI}}$-identifiability in two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual transport, and Triangular Monotonic SCMs (TM-SCMs), which extend $\\sim_{\\mathcal{L}_2}$-identifiability. Our results unify and generalize existing theories, providing theoretical guarantees for practical applications. Finally, we leverage neural TM-SCMs to address the consistency problem in counterfactual reasoning, with experiments validating both the effectiveness of our method and the correctness of the theory."
      },
      {
        "id": "oai:arXiv.org:2505.02214v1",
        "title": "An Empirical Study of Qwen3 Quantization",
        "link": "https://arxiv.org/abs/2505.02214",
        "author": "Xingyu Zheng, Yuye Li, Haoran Chu, Yue Feng, Xudong Ma, Jie Luo, Jinyang Guo, Haotong Qin, Michele Magno, Xianglong Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02214v1 Announce Type: new \nAbstract: The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b."
      },
      {
        "id": "oai:arXiv.org:2505.02222v1",
        "title": "Practical Efficiency of Muon for Pretraining",
        "link": "https://arxiv.org/abs/2505.02222",
        "author": "Essential AI,  :, Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, Ashish Vaswani",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02222v1 Announce Type: new \nAbstract: We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture."
      },
      {
        "id": "oai:arXiv.org:2505.02228v1",
        "title": "Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning",
        "link": "https://arxiv.org/abs/2505.02228",
        "author": "Shangzhe Li, Zhiao Huang, Hao Su",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02228v1 Announce Type: new \nAbstract: Imitation Learning (IL) has achieved remarkable success across various domains, including robotics, autonomous driving, and healthcare, by enabling agents to learn complex behaviors from expert demonstrations. However, existing IL methods often face instability challenges, particularly when relying on adversarial reward or value formulations in world model frameworks. In this work, we propose a novel approach to online imitation learning that addresses these limitations through a reward model based on random network distillation (RND) for density estimation. Our reward model is built on the joint estimation of expert and behavioral distributions within the latent space of the world model. We evaluate our method across diverse benchmarks, including DMControl, Meta-World, and ManiSkill2, showcasing its ability to deliver stable performance and achieve expert-level results in both locomotion and manipulation tasks. Our approach demonstrates improved stability over adversarial methods while maintaining expert-level performance."
      },
      {
        "id": "oai:arXiv.org:2505.02235v1",
        "title": "SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation",
        "link": "https://arxiv.org/abs/2505.02235",
        "author": "Tanguy Herserant, Vincent Guigue",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02235v1 Announce Type: new \nAbstract: Evaluating text summarization quality remains a critical challenge in Natural Language Processing. Current approaches face a trade-off between performance and interpretability. We present SEval-Ex, a framework that bridges this gap by decomposing summarization evaluation into atomic statements, enabling both high performance and explainability. SEval-Ex employs a two-stage pipeline: first extracting atomic statements from text source and summary using LLM, then a matching between generated statements. Unlike existing approaches that provide only summary-level scores, our method generates detailed evidence for its decisions through statement-level alignments. Experiments on the SummEval benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with 0.580 correlation on consistency with human consistency judgments, surpassing GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our framework shows robustness against hallucination."
      },
      {
        "id": "oai:arXiv.org:2505.02236v1",
        "title": "Improving Physical Object State Representation in Text-to-Image Generative Systems",
        "link": "https://arxiv.org/abs/2505.02236",
        "author": "Tianle Chen, Chaitanya Chakka, Deepti Ghadiyaram",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02236v1 Announce Type: new \nAbstract: Current text-to-image generative models struggle to accurately represent object states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. Next, we fine-tune several open-source text-to-image models on this synthetic data. We evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using GPT4o-mini, and achieve an average absolute improvement of 8+% across four models on the public GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific focus on common objects in various physical states. We demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. We release all evaluation prompts and code."
      },
      {
        "id": "oai:arXiv.org:2505.02238v1",
        "title": "Federated Causal Inference in Healthcare: Methods, Challenges, and Applications",
        "link": "https://arxiv.org/abs/2505.02238",
        "author": "Haoyang Li, Jie Xu, Kyra Gan, Fei Wang, Chengxi Zang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02238v1 Announce Type: new \nAbstract: Federated causal inference enables multi-site treatment effect estimation without sharing individual-level data, offering a privacy-preserving solution for real-world evidence generation. However, data heterogeneity across sites, manifested in differences in covariate, treatment, and outcome, poses significant challenges for unbiased and efficient estimation. In this paper, we present a comprehensive review and theoretical analysis of federated causal effect estimation across both binary/continuous and time-to-event outcomes. We classify existing methods into weight-based strategies and optimization-based frameworks and further discuss extensions including personalized models, peer-to-peer communication, and model decomposition. For time-to-event outcomes, we examine federated Cox and Aalen-Johansen models, deriving asymptotic bias and variance under heterogeneity. Our analysis reveals that FedProx-style regularization achieves near-optimal bias-variance trade-offs compared to naive averaging and meta-analysis. We review related software tools and conclude by outlining opportunities, challenges, and future directions for scalable, fair, and trustworthy federated causal inference in distributed healthcare systems."
      },
      {
        "id": "oai:arXiv.org:2505.02242v1",
        "title": "Quantizing Diffusion Models from a Sampling-Aware Perspective",
        "link": "https://arxiv.org/abs/2505.02242",
        "author": "Qian Zeng, Jie Song, Yuanyu Wan, Huiqiong Wang, Mingli Song",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02242v1 Announce Type: new \nAbstract: Diffusion models have recently emerged as the dominant approach in visual generation tasks. However, the lengthy denoising chains and the computationally intensive noise estimation networks hinder their applicability in low-latency and resource-limited environments. Previous research has endeavored to address these limitations in a decoupled manner, utilizing either advanced samplers or efficient model quantization techniques. In this study, we uncover that quantization-induced noise disrupts directional estimation at each sampling step, further distorting the precise directional estimations of higher-order samplers when solving the sampling equations through discretized numerical methods, thereby altering the optimal sampling trajectory. To attain dual acceleration with high fidelity, we propose a sampling-aware quantization strategy, wherein a Mixed-Order Trajectory Alignment technique is devised to impose a more stringent constraint on the error bounds at each sampling step, facilitating a more linear probability flow. Extensive experiments on sparse-step fast sampling across multiple datasets demonstrate that our approach preserves the rapid convergence characteristics of high-speed samplers while maintaining superior generation quality. Code will be made publicly available soon."
      },
      {
        "id": "oai:arXiv.org:2505.02246v1",
        "title": "Cricket: A Self-Powered Chirping Pixel",
        "link": "https://arxiv.org/abs/2505.02246",
        "author": "Shree K. Nayar, Jeremy Klotz, Nikhil Nanda, Mikhail Fridberg",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02246v1 Announce Type: new \nAbstract: We present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. Our sensor, called cricket, harvests energy from incident light. It is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. The carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. We have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. We have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. We show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. We also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. Finally, we modified cricket's circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination."
      },
      {
        "id": "oai:arXiv.org:2505.02247v1",
        "title": "RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation",
        "link": "https://arxiv.org/abs/2505.02247",
        "author": "Jingxiang Qu, Wenhan Gao, Jiaxing Zhang, Xufeng Liu, Hua Wei, Haibin Ling, Yi Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02247v1 Announce Type: new \nAbstract: 3D Geometric Graph Neural Networks (GNNs) have emerged as transformative tools for modeling molecular data. Despite their predictive power, these models often suffer from limited interpretability, raising concerns for scientific applications that require reliable and transparent insights. While existing methods have primarily focused on explaining molecular substructures in 2D GNNs, the transition to 3D GNNs introduces unique challenges, such as handling the implicit dense edge structures created by a cut-off radius. To tackle this, we introduce a novel explanation method specifically designed for 3D GNNs, which localizes the explanation to the immediate neighborhood of each node within the 3D space. Each node is assigned an radius of influence, defining the localized region within which message passing captures spatial and structural interactions crucial for the model's predictions. This method leverages the spatial and geometric characteristics inherent in 3D graphs. By constraining the subgraph to a localized radius of influence, the approach not only enhances interpretability but also aligns with the physical and structural dependencies typical of 3D graph applications, such as molecular learning."
      },
      {
        "id": "oai:arXiv.org:2505.02250v1",
        "title": "EDTok: A Dataset for Eating Disorder Content on TikTok",
        "link": "https://arxiv.org/abs/2505.02250",
        "author": "Charles Bickham, Bryan Ramirez-Gonzalez, Minh Duc Chu, Kristina Lerman, Emilio Ferrara",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02250v1 Announce Type: new \nAbstract: Eating disorders, which include anorexia nervosa and bulimia nervosa, have been exacerbated by the COVID-19 pandemic, with increased diagnoses linked to heightened exposure to idealized body images online. TikTok, a platform with over a billion predominantly adolescent users, has become a key space where eating disorder content is shared, raising concerns about its impact on vulnerable populations. In response, we present a curated dataset of 43,040 TikTok videos, collected using keywords and hashtags related to eating disorders. Spanning from January 2019 to June 2024, this dataset, offers a comprehensive view of eating disorder-related content on TikTok. Our dataset has the potential to address significant research gaps, enabling analysis of content spread and moderation, user engagement, and the pandemic's influence on eating disorder trends. This work aims to inform strategies for mitigating risks associated with harmful content, contributing valuable insights to the study of digital health and social media's role in shaping mental health."
      },
      {
        "id": "oai:arXiv.org:2505.02252v1",
        "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models",
        "link": "https://arxiv.org/abs/2505.02252",
        "author": "Paloma Piot, Patricia Mart\\'in-Rodilla, Javier Parapar",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02252v1 Announce Type: new \nAbstract: Commercial Large Language Models (LLMs) have recently incorporated memory features to deliver personalised responses. This memory retains details such as user demographics and individual characteristics, allowing LLMs to adjust their behaviour based on personal information. However, the impact of integrating personalised information into the context has not been thoroughly assessed, leading to questions about its influence on LLM behaviour. Personalisation can be challenging, particularly with sensitive topics. In this paper, we examine various state-of-the-art LLMs to understand their behaviour in different personalisation scenarios, specifically focusing on hate speech. We prompt the models to assume country-specific personas and use different languages for hate speech detection. Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area. To mitigate these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate speech classifications made with and without country or language-specific context. The refined models demonstrate improved performance in both personalised contexts and when no context is provided."
      },
      {
        "id": "oai:arXiv.org:2505.02255v1",
        "title": "Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset",
        "link": "https://arxiv.org/abs/2505.02255",
        "author": "Jakub W\\k{a}sala, Bart{\\l}omiej Wrzalski, Kornelia Noculak, Yuliia Tarasenko, Oliwer Krupa, Jan Koco\\'n, Grzegorz Chodak",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02255v1 Announce Type: new \nAbstract: This study presents a novel approach to enhance the cost-to-quality ratio of image generation with diffusion models. We hypothesize that differences between distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are consistent and, therefore, learnable within a specialized domain, like portrait generation. We generate a synthetic paired dataset and train a fast image-to-image translation head. Using two sets of low- and high-quality synthetic images, our model is trained to refine the output of a distilled generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like FLUX.1-dev, which is more computationally intensive. Our results show that the pipeline, which combines a distilled version of a large generative model with our enhancement layer, delivers similar photorealistic portraits to the baseline version with up to an 82% decrease in computational cost compared to FLUX.1-dev. This study demonstrates the potential for improving the efficiency of AI solutions involving large-scale image generation."
      },
      {
        "id": "oai:arXiv.org:2505.02266v1",
        "title": "Parameter-Efficient Transformer Embeddings",
        "link": "https://arxiv.org/abs/2505.02266",
        "author": "Henry Ndubuaku, Mouad Talhi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02266v1 Announce Type: new \nAbstract: Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings."
      },
      {
        "id": "oai:arXiv.org:2505.02273v1",
        "title": "Demystifying optimized prompts in language models",
        "link": "https://arxiv.org/abs/2505.02273",
        "author": "Rimon Melamed, Lucas H. McCabe, H. Howie Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02273v1 Announce Type: new \nAbstract: Modern language models (LMs) are not robust to out-of-distribution inputs. Machine generated (``optimized'') prompts can be used to modulate LM outputs and induce specific behaviors while appearing completely uninterpretable. In this work, we investigate the composition of optimized prompts, as well as the mechanisms by which LMs parse and build predictions from optimized prompts. We find that optimized prompts primarily consist of punctuation and noun tokens which are more rare in the training data. Internally, optimized prompts are clearly distinguishable from natural language counterparts based on sparse subsets of the model's activations. Across various families of instruction-tuned models, optimized prompts follow a similar path in how their representations form through the network."
      },
      {
        "id": "oai:arXiv.org:2505.02277v1",
        "title": "Epistemic Wrapping for Uncertainty Quantification",
        "link": "https://arxiv.org/abs/2505.02277",
        "author": "Maryam Sultana, Neil Yorke-Smith, Kaizheng Wang, Shireen Kudukkil Manchingal, Muhammad Mubashar, Fabio Cuzzolin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02277v1 Announce Type: new \nAbstract: Uncertainty estimation is pivotal in machine learning, especially for classification tasks, as it improves the robustness and reliability of models. We introduce a novel `Epistemic Wrapping' methodology aimed at improving uncertainty estimation in classification. Our approach uses Bayesian Neural Networks (BNNs) as a baseline and transforms their outputs into belief function posteriors, effectively capturing epistemic uncertainty and offering an efficient and general methodology for uncertainty quantification. Comprehensive experiments employing a Bayesian Neural Network (BNN) baseline and an Interval Neural Network for inference on the MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate that our Epistemic Wrapper significantly enhances generalisation and uncertainty quantification."
      },
      {
        "id": "oai:arXiv.org:2505.02278v1",
        "title": "Compositional Image-Text Matching and Retrieval by Grounding Entities",
        "link": "https://arxiv.org/abs/2505.02278",
        "author": "Madhukar Reddy Vongala, Saurabh Srivastava, Jana Ko\\v{s}eck\\'a",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02278v1 Announce Type: new \nAbstract: Vision-language pretraining on large datasets of images-text pairs is one of the main building blocks of current Vision-Language Models. While with additional training, these models excel in various downstream tasks, including visual question answering, image captioning, and visual commonsense reasoning. However, a notable weakness of pretrained models like CLIP, is their inability to perform entity grounding and compositional image and text matching~\\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR, learninglocalizeCVPR24}. In this work we propose a novel learning-free zero-shot augmentation of CLIP embeddings that has favorable compositional properties. We compute separate embeddings of sub-images of object entities and relations that are localized by the state of the art open vocabulary detectors and dynamically adjust the baseline global image embedding. % The final embedding is obtained by computing a weighted combination of the sub-image embeddings. The resulting embedding is then utilized for similarity computation with text embedding, resulting in a average 1.5\\% improvement in image-text matching accuracy on the Visual Genome and SVO Probes datasets~\\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings demonstrate superior retrieval performance, thus achieving significant gains on the Flickr30K and MS-COCO retrieval benchmarks~\\cite{flickr30ke, mscoco}, improving the state-of-the-art Recall@1 by 12\\% and 0.4\\%, respectively. Our code is available at https://github.com/madhukarreddyvongala/GroundingCLIP."
      },
      {
        "id": "oai:arXiv.org:2505.02287v1",
        "title": "Continuous Normalizing Flows for Uncertainty-Aware Human Pose Estimation",
        "link": "https://arxiv.org/abs/2505.02287",
        "author": "Shipeng Liu, Ziliang Xiong, Bastian Wandt, Per-Erik Forss\\'en",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02287v1 Announce Type: new \nAbstract: Human Pose Estimation (HPE) is increasingly important for applications like virtual reality and motion analysis, yet current methods struggle with balancing accuracy, computational efficiency, and reliable uncertainty quantification (UQ). Traditional regression-based methods assume fixed distributions, which might lead to poor UQ. Heatmap-based methods effectively model the output distribution using likelihood heatmaps, however, they demand significant resources. To address this, we propose Continuous Flow Residual Estimation (CFRE), an integration of Continuous Normalizing Flows (CNFs) into regression-based models, which allows for dynamic distribution adaptation. Through extensive experiments, we show that CFRE leads to better accuracy and uncertainty quantification with retained computational efficiency on both 2D and 3D human pose estimation tasks."
      },
      {
        "id": "oai:arXiv.org:2505.02288v1",
        "title": "Universal Approximation Theorem of Deep Q-Networks",
        "link": "https://arxiv.org/abs/2505.02288",
        "author": "Qian Qi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02288v1 Announce Type: new \nAbstract: We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data."
      },
      {
        "id": "oai:arXiv.org:2505.02296v1",
        "title": "Entropy-Guided Sampling of Flat Modes in Discrete Spaces",
        "link": "https://arxiv.org/abs/2505.02296",
        "author": "Pinaki Mohanty, Riddhiman Bhattacharya, Ruqi Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02296v1 Announce Type: new \nAbstract: Sampling from flat modes in discrete spaces is a crucial yet underexplored problem. Flat modes represent robust solutions and have broad applications in combinatorial optimization and discrete generative modeling. However, existing sampling algorithms often overlook the mode volume and struggle to capture flat modes effectively. To address this limitation, we propose \\emph{Entropic Discrete Langevin Proposal} (EDLP), which incorporates local entropy into the sampling process through a continuous auxiliary variable under a joint distribution. The local entropy term guides the discrete sampler toward flat modes with a small overhead. We provide non-asymptotic convergence guarantees for EDLP in locally log-concave discrete distributions. Empirically, our method consistently outperforms traditional approaches across tasks that require sampling from flat basins, including Bernoulli distribution, restricted Boltzmann machines, combinatorial optimization, and binary neural networks."
      },
      {
        "id": "oai:arXiv.org:2505.02299v1",
        "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection",
        "link": "https://arxiv.org/abs/2505.02299",
        "author": "Daisuke Yamada, Harit Vishwakarma, Ramya Korlakai Vinayak",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02299v1 Announce Type: new \nAbstract: Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \\emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control."
      },
      {
        "id": "oai:arXiv.org:2505.02304v1",
        "title": "Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition",
        "link": "https://arxiv.org/abs/2505.02304",
        "author": "Siyu Liang, Yunan Li, Wentian Xin, Huizhou Chen, Xujie Liu, Kang Liu, Qiguang Miao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02304v1 Announce Type: new \nAbstract: Sign language recognition (SLR) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. To the best of our knowledge, this is the first work to integrate generative large language models (LLMs) into SLR tasks. We propose a novel Generative Sign-description Prompts Multi-positive Contrastive learning (GSP-MC) method that leverages retrieval-augmented generation (RAG) with domain-specific LLMs, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. The GSP-MC method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. Our approach combines global and part-level losses, optimizing KL divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. Experiments demonstrate state-of-the-art performance against existing methods on the Chinese SLR500 (reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies."
      },
      {
        "id": "oai:arXiv.org:2505.02308v1",
        "title": "Enabling Local Neural Operators to perform Equation-Free System-Level Analysis",
        "link": "https://arxiv.org/abs/2505.02308",
        "author": "Gianluca Fabiani, Hannes Vandecasteele, Somdatta Goswami, Constantinos Siettos, Ioannis G. Kevrekidis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02308v1 Announce Type: new \nAbstract: Neural Operators (NOs) provide a powerful framework for computations involving physical laws that can be modelled by (integro-) partial differential equations (PDEs), directly learning maps between infinite-dimensional function spaces that bypass both the explicit equation identification and their subsequent numerical solving. Still, NOs have so far primarily been employed to explore the dynamical behavior as surrogates of brute-force temporal simulations/predictions. Their potential for systematic rigorous numerical system-level tasks, such as fixed-point, stability, and bifurcation analysis - crucial for predicting irreversible transitions in real-world phenomena - remains largely unexplored. Toward this aim, inspired by the Equation-Free multiscale framework, we propose and implement a framework that integrates (local) NOs with advanced iterative numerical methods in the Krylov subspace, so as to perform efficient system-level stability and bifurcation analysis of large-scale dynamical systems. Beyond fixed point, stability, and bifurcation analysis enabled by local in time NOs, we also demonstrate the usefulness of local in space as well as in space-time (\"patch\") NOs in accelerating the computer-aided analysis of spatiotemporal dynamics. We illustrate our framework via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE, which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN) model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node bifurcations."
      },
      {
        "id": "oai:arXiv.org:2505.02309v1",
        "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques",
        "link": "https://arxiv.org/abs/2505.02309",
        "author": "Sanjay Surendranath Girija, Shashank Kapoor, Lakshit Arora, Dipen Pradhan, Aman Raj, Ankit Shetgaonkar",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02309v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment."
      },
      {
        "id": "oai:arXiv.org:2505.02311v1",
        "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering",
        "link": "https://arxiv.org/abs/2505.02311",
        "author": "Jihao Zhao, Chunlai Zhou, Biao Qin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02311v1 Announce Type: new \nAbstract: The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs."
      },
      {
        "id": "oai:arXiv.org:2505.02317v1",
        "title": "A longitudinal analysis of misinformation, polarization and toxicity on Bluesky after its public launch",
        "link": "https://arxiv.org/abs/2505.02317",
        "author": "Gianluca Nogara, Erfan Samieyan Sahneh, Matthew R. DeVerna, Nick Liu, Luca Luceri, Filippo Menczer, Francesco Pierri, Silvia Giordano",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02317v1 Announce Type: new \nAbstract: Bluesky is a decentralized, Twitter-like social media platform that has rapidly gained popularity. Following an invite-only phase, it officially opened to the public on February 6th, 2024, leading to a significant expansion of its user base. In this paper, we present a longitudinal analysis of user activity in the two months surrounding its public launch, examining how the platform evolved due to this rapid growth. Our analysis reveals that Bluesky exhibits an activity distribution comparable to more established social platforms, yet it features a higher volume of original content relative to reshared posts and maintains low toxicity levels. We further investigate the political leanings of its user base, misinformation dynamics, and engagement in harmful conversations. Our findings indicate that Bluesky users predominantly lean left politically and tend to share high-credibility sources. After the platform's public launch, an influx of new users, particularly those posting in English and Japanese, contributed to a surge in activity. Among them, several accounts displayed suspicious behaviors, such as mass-following users and sharing content from low-credibility news sources. Some of these accounts have already been flagged as spam or suspended, suggesting that Bluesky's moderation efforts have been effective."
      },
      {
        "id": "oai:arXiv.org:2505.02325v1",
        "title": "TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment",
        "link": "https://arxiv.org/abs/2505.02325",
        "author": "Zhichuan Wang, Yang Zhou, Jinhai Xiang, Yulong Wang, Xinwei He",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02325v1 Announce Type: new \nAbstract: Learning discriminative 3D representations that generalize well to unknown testing categories is an emerging requirement for many real-world 3D applications. Existing well-established methods often struggle to attain this goal due to insufficient 3D training data from broader concepts. Meanwhile, pre-trained large vision-language models (e.g., CLIP) have shown remarkable zero-shot generalization capabilities. Yet, they are limited in extracting suitable 3D representations due to substantial gaps between their 2D training and 3D testing distributions. To address these challenges, we propose Testing-time Distribution Alignment (TeDA), a novel framework that adapts a pretrained 2D vision-language model CLIP for unknown 3D object retrieval at test time. To our knowledge, it is the first work that studies the test-time adaptation of a vision-language model for 3D feature learning. TeDA projects 3D objects into multi-view images, extracts features using CLIP, and refines 3D query embeddings with an iterative optimization strategy by confident query-target sample pairs in a self-boosting manner. Additionally, TeDA integrates textual descriptions generated by a multimodal language model (InternVL) to enhance 3D object understanding, leveraging CLIP's aligned feature space to fuse visual and textual cues. Extensive experiments on four open-set 3D object retrieval benchmarks demonstrate that TeDA greatly outperforms state-of-the-art methods, even those requiring extensive training. We also experimented with depth maps on Objaverse-LVIS, further validating its effectiveness. Code is available at https://github.com/wangzhichuan123/TeDA."
      },
      {
        "id": "oai:arXiv.org:2505.02331v1",
        "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection",
        "link": "https://arxiv.org/abs/2505.02331",
        "author": "Hao Cheng, Zhiwei Zhao, Yichao He, Zhenzhen Hu, Jia Li, Meng Wang, Richang Hong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02331v1 Announce Type: new \nAbstract: Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations."
      },
      {
        "id": "oai:arXiv.org:2505.02335v1",
        "title": "6D Pose Estimation on Spoons and Hands",
        "link": "https://arxiv.org/abs/2505.02335",
        "author": "Kevin Tan, Fan Yang, Yuhao Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02335v1 Announce Type: new \nAbstract: Accurate dietary monitoring is essential for promoting healthier eating habits. A key area of research is how people interact and consume food using utensils and hands. By tracking their position and orientation, it is possible to estimate the volume of food being consumed, or monitor eating behaviours, highly useful insights into nutritional intake that can be more reliable than popular methods such as self-reporting. Hence, this paper implements a system that analyzes stationary video feed of people eating, using 6D pose estimation to track hand and spoon movements to capture spatial position and orientation. In doing so, we examine the performance of two state-of-the-art (SOTA) video object segmentation (VOS) models, both quantitatively and qualitatively, and identify main sources of error within the system."
      },
      {
        "id": "oai:arXiv.org:2505.02343v1",
        "title": "Social Correction on Social Media: A Quantitative Analysis of Comment Behaviour and Reliability",
        "link": "https://arxiv.org/abs/2505.02343",
        "author": "Sameera S. Vithanage, Keith Ransom, Antonette Mendoza, Shanika Karunasekera",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02343v1 Announce Type: new \nAbstract: Corrections given by ordinary social media users, also referred to as Social Correction have emerged as a viable intervention against misinformation as per the recent literature. However, little is known about how often users give disputing or endorsing comments and how reliable those comments are. An online experiment was conducted to investigate how users' credibility evaluations of social media posts and their confidence in those evaluations combined with online reputational concerns affect their commenting behaviour. The study found that participants exhibited a more conservative approach when giving disputing comments compared to endorsing ones. Nevertheless, participants were more discerning in their disputing comments than endorsing ones. These findings contribute to a better understanding of social correction on social media and highlight the factors influencing comment behaviour and reliability."
      },
      {
        "id": "oai:arXiv.org:2505.02360v1",
        "title": "Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training",
        "link": "https://arxiv.org/abs/2505.02360",
        "author": "Fares B. Mehouachi, Saif Eddin Jabari",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02360v1 Announce Type: new \nAbstract: Adversarial training is a cornerstone of robust deep learning, but fast methods like the Fast Gradient Sign Method (FGSM) often suffer from Catastrophic Overfitting (CO), where models become robust to single-step attacks but fail against multi-step variants. While existing solutions rely on noise injection, regularization, or gradient clipping, we propose a novel solution that purely controls the $l^p$ training norm to mitigate CO.\n  Our study is motivated by the empirical observation that CO is more prevalent under the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we develop a framework for generalized $l^p$ attack as a fixed point problem and craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to $l^{\\infty}$. This leads to our core insight: CO emerges when highly concentrated gradients where information localizes in few dimensions interact with aggressive norm constraints. By quantifying gradient concentration through Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM that automatically tunes the training norm based on gradient information. Extensive experiments demonstrate that this approach achieves strong robustness without requiring additional regularization or noise injection, providing a novel and theoretically-principled pathway to mitigate the CO problem."
      },
      {
        "id": "oai:arXiv.org:2505.02363v1",
        "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning",
        "link": "https://arxiv.org/abs/2505.02363",
        "author": "Tianjian Li, Daniel Khashabi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02363v1 Announce Type: new \nAbstract: Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.\n  In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%."
      },
      {
        "id": "oai:arXiv.org:2505.02364v1",
        "title": "Quaternion Infrared Visible Image Fusion",
        "link": "https://arxiv.org/abs/2505.02364",
        "author": "Weihua Yang, Yicong Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02364v1 Announce Type: new \nAbstract: Visible images provide rich details and color information only under well-lighted conditions while infrared images effectively highlight thermal targets under challenging conditions such as low visibility and adverse weather. Infrared-visible image fusion aims to integrate complementary information from infrared and visible images to generate a high-quality fused image. Existing methods exhibit critical limitations such as neglecting color structure information in visible images and performance degradation when processing low-quality color-visible inputs. To address these issues, we propose a quaternion infrared-visible image fusion (QIVIF) framework to generate high-quality fused images completely in the quaternion domain. QIVIF proposes a quaternion low-visibility feature learning model to adaptively extract salient thermal targets and fine-grained texture details from input infrared and visible images respectively under diverse degraded conditions. QIVIF then develops a quaternion adaptive unsharp masking method to adaptively improve high-frequency feature enhancement with balanced illumination. QIVIF further proposes a quaternion hierarchical Bayesian fusion model to integrate infrared saliency and enhanced visible details to obtain high-quality fused images. Extensive experiments across diverse datasets demonstrate that our QIVIF surpasses state-of-the-art methods under challenging low-visibility conditions."
      },
      {
        "id": "oai:arXiv.org:2505.02365v1",
        "title": "Quaternion Multi-focus Color Image Fusion",
        "link": "https://arxiv.org/abs/2505.02365",
        "author": "Weihua Yang, Yicong Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02365v1 Announce Type: new \nAbstract: Multi-focus color image fusion refers to integrating multiple partially focused color images to create a single all-in-focus color image. However, existing methods struggle with complex real-world scenarios due to limitations in handling color information and intricate textures. To address these challenges, this paper proposes a quaternion multi-focus color image fusion framework to perform high-quality color image fusion completely in the quaternion domain. This framework introduces 1) a quaternion sparse decomposition model to jointly learn fine-scale image details and structure information of color images in an iterative fashion for high-precision focus detection, 2) a quaternion base-detail fusion strategy to individually fuse base-scale and detail-scale results across multiple color images for preserving structure and detail information, and 3) a quaternion structural similarity refinement strategy to adaptively select optimal patches from initial fusion results and obtain the final fused result for preserving fine details and ensuring spatially consistent outputs. Extensive experiments demonstrate that the proposed framework outperforms state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2505.02366v1",
        "title": "JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings",
        "link": "https://arxiv.org/abs/2505.02366",
        "author": "Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02366v1 Announce Type: new \nAbstract: Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \\textbf{J}oint \\textbf{T}ensor representation modulus constraint and \\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence \\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks."
      },
      {
        "id": "oai:arXiv.org:2505.02369v1",
        "title": "Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks",
        "link": "https://arxiv.org/abs/2505.02369",
        "author": "Juyoung Yun",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02369v1 Announce Type: new \nAbstract: Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization."
      },
      {
        "id": "oai:arXiv.org:2505.02370v1",
        "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing",
        "link": "https://arxiv.org/abs/2505.02370",
        "author": "Ming Li, Xin Gu, Fan Chen, Xiaoying Xing, Longyin Wen, Chen Chen, Sijie Zhu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02370v1 Announce Type: new \nAbstract: Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size."
      },
      {
        "id": "oai:arXiv.org:2505.02380v1",
        "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices",
        "link": "https://arxiv.org/abs/2505.02380",
        "author": "Arnab Sanyal, Prithwish Mukherjee, Gourav Datta, Sandeep P. Chinchali",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02380v1 Announce Type: new \nAbstract: Large Language Models (LLMs) demonstrate exceptional performance across various tasks, but their large storage and computational requirements constrain their deployment on edge devices. To address this, we propose EntroLLM, a novel compression framework that integrates mixed quantization with entropy coding to reduce storage overhead while maintaining model accuracy. Our method applies a layer-wise mixed quantization scheme - choosing between symmetric and asymmetric quantization based on individual layer weight distributions - to optimize compressibility. We then employ Huffman encoding for lossless compression of the quantized weights, significantly reducing memory bandwidth requirements. Furthermore, we introduce parallel Huffman decoding, which enables efficient retrieval of encoded weights during inference, ensuring minimal latency impact. Our experiments on edge-compatible LLMs, including smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct, demonstrate that EntroLLM achieves up to $30%$ storage reduction compared to uint8 models and up to $65%$ storage reduction compared to uint4 models, while preserving perplexity and accuracy, on language benchmark tasks. We further show that our method enables $31.9%$ - $146.6%$ faster inference throughput on memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing the required data movement. The proposed approach requires no additional re-training and is fully compatible with existing post-training quantization methods, making it a practical solution for edge LLMs."
      },
      {
        "id": "oai:arXiv.org:2505.02383v1",
        "title": "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret",
        "link": "https://arxiv.org/abs/2505.02383",
        "author": "Bingshan Hu, Zhiming Huang, Tianyue H. Zhang, Mathias L\\'ecuyer, Nidhi Hegde",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02383v1 Announce Type: new \nAbstract: We address differentially private stochastic bandit problems from the angles of exploring the deep connections among Thompson Sampling with Gaussian priors, Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade off privacy and regret. DP-TS-UCB satisfies $ \\tilde{O} \\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O \\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$ controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB relies on anti-concentration bounds of Gaussian distributions and links exploration mechanisms in Thompson Sampling-based algorithms and Upper Confidence Bound-based algorithms, which may be of independent interest."
      },
      {
        "id": "oai:arXiv.org:2505.02387v1",
        "title": "RM-R1: Reward Modeling as Reasoning",
        "link": "https://arxiv.org/abs/2505.02387",
        "author": "Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02387v1 Announce Type: new \nAbstract: Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1."
      },
      {
        "id": "oai:arXiv.org:2505.02388v1",
        "title": "MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans",
        "link": "https://arxiv.org/abs/2505.02388",
        "author": "Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang, Puhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02388v1 Announce Type: new \nAbstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/."
      },
      {
        "id": "oai:arXiv.org:2505.02390v1",
        "title": "Quantitative Analysis of Performance Drop in DeepSeek Model Quantization",
        "link": "https://arxiv.org/abs/2505.02390",
        "author": "Enbo Zhao, Yi Shen, Shuming Shi, Jieyun Huang, Zhihao Chen, Ning Wang, Siqi Xiao, Jian Zhang, Kai Wang, Shiguo Lian",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02390v1 Announce Type: new \nAbstract: Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3."
      },
      {
        "id": "oai:arXiv.org:2505.02391v1",
        "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL",
        "link": "https://arxiv.org/abs/2505.02391",
        "author": "Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, Tong Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02391v1 Announce Type: new \nAbstract: Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM."
      },
      {
        "id": "oai:arXiv.org:2505.02393v1",
        "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection",
        "link": "https://arxiv.org/abs/2505.02393",
        "author": "Sungheon Jeong, Jihong Park, Mohsen Imani",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02393v1 Announce Type: new \nAbstract: Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD."
      },
      {
        "id": "oai:arXiv.org:2505.02402v1",
        "title": "A probabilistic view on Riemannian machine learning models for SPD matrices",
        "link": "https://arxiv.org/abs/2505.02402",
        "author": "Thibault de Surrel, Florian Yger, Fabien Lotte, Sylvain Chevallier",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02402v1 Announce Type: new \nAbstract: The goal of this paper is to show how different machine learning tools on the Riemannian manifold $\\mathcal{P}_d$ of Symmetric Positive Definite (SPD) matrices can be united under a probabilistic framework. For this, we will need several Gaussian distributions defined on $\\mathcal{P}_d$. We will show how popular classifiers on $\\mathcal{P}_d$ can be reinterpreted as Bayes Classifiers using these Gaussian distributions. These distributions will also be used for outlier detection and dimension reduction. By showing that those distributions are pervasive in the tools used on $\\mathcal{P}_d$, we allow for other machine learning tools to be extended to $\\mathcal{P}_d$."
      },
      {
        "id": "oai:arXiv.org:2505.02406v1",
        "title": "Token Coordinated Prompt Attention is Needed for Visual Prompting",
        "link": "https://arxiv.org/abs/2505.02406",
        "author": "Zichen Liu, Xu Zou, Gang Hua, Jiahuan Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02406v1 Announce Type: new \nAbstract: Visual prompting techniques are widely used to efficiently fine-tune pretrained Vision Transformers (ViT) by learning a small set of shared prompts for all tokens. However, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of ViT. This often leads to indistinguishable and biased prompt-extracted features, hindering performance. To address this issue, we propose a plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. Firstly, recognizing the distinct functions of CLS and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into CLS Prompts and Image Prompts, which interact exclusively with CLS tokens and image tokens through attention mechanisms. This enhances their respective discriminative abilities. Furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. This enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. Extensive experiments across various benchmarks demonstrate that TCPA significantly enhances the diversity and discriminative power of the extracted features. The code is available at https://github.com/zhoujiahuan1991/ICML2025-TCPA."
      },
      {
        "id": "oai:arXiv.org:2505.02410v1",
        "title": "Bielik 11B v2 Technical Report",
        "link": "https://arxiv.org/abs/2505.02410",
        "author": "Krzysztof Ociepa, {\\L}ukasz Flis, Krzysztof Wr\\'obel, Adrian Gwo\\'zdziej, Remigiusz Kinas",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02410v1 Announce Type: new \nAbstract: We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages."
      },
      {
        "id": "oai:arXiv.org:2505.02417v1",
        "title": "T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models",
        "link": "https://arxiv.org/abs/2505.02417",
        "author": "Yunfeng Ge, Jiawei Li, Yiji Zhao, Haomin Wen, Zhao Li, Meikang Qiu, Hongyan Li, Ming Jin, Shirui Pan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02417v1 Announce Type: new \nAbstract: Text-to-Time Series generation holds significant potential to address challenges such as data sparsity, imbalance, and limited availability of multimodal time series datasets across domains. While diffusion models have achieved remarkable success in Text-to-X (e.g., vision and audio data) generation, their use in time series generation remains in its nascent stages. Existing approaches face two critical limitations: (1) the lack of systematic exploration of general-proposed time series captions, which are often domain-specific and struggle with generalization; and (2) the inability to generate time series of arbitrary lengths, limiting their applicability to real-world scenarios. In this work, we first categorize time series captions into three levels: point-level, fragment-level, and instance-level. Additionally, we introduce a new fragment-level dataset containing over 600,000 high-resolution time series-text pairs. Second, we propose Text-to-Series (T2S), a diffusion-based framework that bridges the gap between natural language and time series in a domain-agnostic manner. T2S employs a length-adaptive variational autoencoder to encode time series of varying lengths into consistent latent embeddings. On top of that, T2S effectively aligns textual representations with latent embeddings by utilizing Flow Matching and employing Diffusion Transformer as the denoiser. We train T2S in an interleaved paradigm across multiple lengths, allowing it to generate sequences of any desired length. Extensive evaluations demonstrate that T2S achieves state-of-the-art performance across 13 datasets spanning 12 domains."
      },
      {
        "id": "oai:arXiv.org:2505.02426v1",
        "title": "Towards One-shot Federated Learning: Advances, Challenges, and Future Directions",
        "link": "https://arxiv.org/abs/2505.02426",
        "author": "Flora Amato, Lingyu Qiu, Mohammad Tanveer, Salvatore Cuomo, Fabio Giampaolo, Francesco Piccialli",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02426v1 Announce Type: new \nAbstract: One-shot FL enables collaborative training in a single round, eliminating the need for iterative communication, making it particularly suitable for use in resource-constrained and privacy-sensitive applications. This survey offers a thorough examination of One-shot FL, highlighting its distinct operational framework compared to traditional federated approaches. One-shot FL supports resource-limited devices by enabling single-round model aggregation while maintaining data locality. The survey systematically categorizes existing methodologies, emphasizing advancements in client model initialization, aggregation techniques, and strategies for managing heterogeneous data distributions. Furthermore, we analyze the limitations of current approaches, particularly in terms of scalability and generalization in non-IID settings. By analyzing cutting-edge techniques and outlining open challenges, this survey aspires to provide a comprehensive reference for researchers and practitioners aiming to design and implement One-shot FL systems, advancing the development and adoption of One-shot FL solutions in a real-world, resource-constrained scenario."
      },
      {
        "id": "oai:arXiv.org:2505.02433v1",
        "title": "FairPO: Robust Preference Optimization for Fair Multi-Label Learning",
        "link": "https://arxiv.org/abs/2505.02433",
        "author": "Soumen Kumar Mondal, Akshit Varmora, Prateek Chanda, Ganesh Ramakrishnan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02433v1 Announce Type: new \nAbstract: We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs."
      },
      {
        "id": "oai:arXiv.org:2505.02435v1",
        "title": "A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability",
        "link": "https://arxiv.org/abs/2505.02435",
        "author": "Pouria Fatemi, Ehsan Sharifian, Mohammad Hossein Yassaee",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02435v1 Announce Type: new \nAbstract: Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs."
      },
      {
        "id": "oai:arXiv.org:2505.02448v1",
        "title": "Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey",
        "link": "https://arxiv.org/abs/2505.02448",
        "author": "Chaohua Li, Enhao Zhang, Chuanxing Geng, Songcan Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02448v1 Announce Type: new \nAbstract: Out-of-distribution detection (OOD) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (ID) data during testing. Recent advances in AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized OOD detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. This shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of ID images, adhering to a unimodal paradigm. To better align with CLIP's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. Specifically, we categorize existing methods based on how visual and textual information of OOD data is utilized within image + text modalities, and further divide them into four groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e., learnable vectors or class names) Known or Unknown, across two training strategies (i.e., train-free or training-required). More importantly, we discuss open problems in CLIP-like OOD detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding."
      },
      {
        "id": "oai:arXiv.org:2505.02456v1",
        "title": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs",
        "link": "https://arxiv.org/abs/2505.02456",
        "author": "Elisa Forcada Rodr\\'iguez, Olatz Perez-de-Vi\\~naspre, Jon Ander Campos, Dietrich Klakow, Vagrant Gautam",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02456v1 Announce Type: new \nAbstract: One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work."
      },
      {
        "id": "oai:arXiv.org:2505.02463v1",
        "title": "Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda",
        "link": "https://arxiv.org/abs/2505.02463",
        "author": "Richard Kimera, Dongnyeong Heo, Daniela N. Rim, Heeyoul Choi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02463v1 Announce Type: new \nAbstract: In this paper,we explore the application of Back translation (BT) as a semi-supervised technique to enhance Neural Machine Translation(NMT) models for the English-Luganda language pair, specifically addressing the challenges faced by low-resource languages. The purpose of our study is to demonstrate how BT can mitigate the scarcity of bilingual data by generating synthetic data from monolingual corpora. Our methodology involves developing custom NMT models using both publicly available and web-crawled data, and applying Iterative and Incremental Back translation techniques. We strategically select datasets for incremental back translation across multiple small datasets, which is a novel element of our approach. The results of our study show significant improvements, with translation performance for the English-Luganda pair exceeding previous benchmarks by more than 10 BLEU score units across all translation directions. Additionally, our evaluation incorporates comprehensive assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced understanding of translation quality. The conclusion drawn from our research confirms the efficacy of BT when strategically curated datasets are utilized, establishing new performance benchmarks and demonstrating the potential of BT in enhancing NMT models for low-resource languages."
      },
      {
        "id": "oai:arXiv.org:2505.02467v1",
        "title": "Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging",
        "link": "https://arxiv.org/abs/2505.02467",
        "author": "Valerio Guarrasi, Klara Mogensen, Sara Tassinari, Sara Qvarlander, Paolo Soda",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02467v1 Announce Type: new \nAbstract: Multimodal deep learning harnesses diverse imaging modalities, such as MRI sequences, to enhance diagnostic accuracy in medical imaging. A key challenge is determining the optimal timing for integrating these modalities-specifically, identifying the network layers where fusion modules should be inserted. Current approaches often rely on manual tuning or exhaustive search, which are computationally expensive without any guarantee of converging to optimal results. We propose a sequential forward search algorithm that incrementally activates and evaluates candidate fusion modules at different layers of a multimodal network. At each step, the algorithm retrains from previously learned weights and compares validation loss to identify the best-performing configuration. This process systematically reduces the search space, enabling efficient identification of the optimal fusion timing without exhaustively testing all possible module placements. The approach is validated on two multimodal MRI datasets, each addressing different classification tasks. Our algorithm consistently identified configurations that outperformed unimodal baselines, late fusion, and a brute-force ensemble of all potential fusion placements. These architectures demonstrated superior accuracy, F-score, and specificity while maintaining competitive or improved AUC values. Furthermore, the sequential nature of the search significantly reduced computational overhead, making the optimization process more practical. By systematically determining the optimal timing to fuse imaging modalities, our method advances multimodal deep learning for medical imaging. It provides an efficient and robust framework for fusion optimization, paving the way for improved clinical decision-making and more adaptable, scalable architectures in medical AI applications."
      },
      {
        "id": "oai:arXiv.org:2505.02469v1",
        "title": "Efficient Continual Learning in Keyword Spotting using Binary Neural Networks",
        "link": "https://arxiv.org/abs/2505.02469",
        "author": "Quynh Nguyen-Phuong Vu, Luciano Sebastian Martinez-Rau, Yuxuan Zhang, Nho-Duc Tran, Bengt Oelmann, Michele Magno, Sebastian Bader",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02469v1 Announce Type: new \nAbstract: Keyword spotting (KWS) is an essential function that enables interaction with ubiquitous smart devices. However, in resource-limited devices, KWS models are often static and can thus not adapt to new scenarios, such as added keywords. To overcome this problem, we propose a Continual Learning (CL) approach for KWS built on Binary Neural Networks (BNNs). The framework leverages the reduced computation and memory requirements of BNNs while incorporating techniques that enable the seamless integration of new keywords over time. This study evaluates seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95% for a single additional keyword and up to 86% for four additional classes. Sensitivity to the amount of training samples in the CL phase, and differences in computational complexities are being evaluated. These evaluations demonstrate that batch-based algorithms are more sensitive to the CL dataset size, and that differences between the computational complexities are insignificant. These findings highlight the potential of developing an effective and computationally efficient technique for continuously integrating new keywords in KWS applications that is compatible with resource-constrained devices."
      },
      {
        "id": "oai:arXiv.org:2505.02471v1",
        "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction",
        "link": "https://arxiv.org/abs/2505.02471",
        "author": "Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02471v1 Announce Type: new \nAbstract: We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined."
      },
      {
        "id": "oai:arXiv.org:2505.02481v1",
        "title": "Finger Pose Estimation for Under-screen Fingerprint Sensor",
        "link": "https://arxiv.org/abs/2505.02481",
        "author": "Xiongjun Guan, Zhiyu Pan, Jianjiang Feng, Jie Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02481v1 Announce Type: new \nAbstract: Two-dimensional pose estimation plays a crucial role in fingerprint recognition by facilitating global alignment and reduce pose-induced variations. However, existing methods are still unsatisfactory when handling with large angle or small area inputs. These limitations are particularly pronounced on fingerprints captured by under-screen fingerprint sensors in smartphones. In this paper, we present a novel dual-modal input based network for under-screen fingerprint pose estimation. Our approach effectively integrates two distinct yet complementary modalities: texture details extracted from ridge patches through the under-screen fingerprint sensor, and rough contours derived from capacitive images obtained via the touch screen. This collaborative integration endows our network with more comprehensive and discriminative information, substantially improving the accuracy and stability of pose estimation. A decoupled probability distribution prediction task is designed, instead of the traditional supervised forms of numerical regression or heatmap voting, to facilitate the training process. Additionally, we incorporate a Mixture of Experts (MoE) based feature fusion mechanism and a relationship driven cross-domain knowledge transfer strategy to further strengthen feature extraction and fusion capabilities. Extensive experiments are conducted on several public datasets and two private datasets. The results indicate that our method is significantly superior to previous state-of-the-art (SOTA) methods and remarkably boosts the recognition ability of fingerprint recognition algorithms. Our code is available at https://github.com/XiongjunGuan/DRACO."
      },
      {
        "id": "oai:arXiv.org:2505.02486v1",
        "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
        "link": "https://arxiv.org/abs/2505.02486",
        "author": "Jinpeng Chen, Runmin Cong, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, Horace Ho Shing Ip, Sam Kwong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02486v1 Announce Type: new \nAbstract: Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model's knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks' answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the model's knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance."
      },
      {
        "id": "oai:arXiv.org:2505.02490v1",
        "title": "Bayesian Robust Aggregation for Federated Learning",
        "link": "https://arxiv.org/abs/2505.02490",
        "author": "Aleksandr Karakulev (Uppsala University), Usama Zafar (Uppsala University), Salman Toor (Uppsala University, Scaleout Systems), Prashant Singh (Uppsala University, Science for Life Laboratory, Sweden)",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02490v1 Announce Type: new \nAbstract: Federated Learning enables collaborative training of machine learning models on decentralized data. This scheme, however, is vulnerable to adversarial attacks, when some of the clients submit corrupted model updates. In real-world scenarios, the total number of compromised clients is typically unknown, with the extent of attacks potentially varying over time. To address these challenges, we propose an adaptive approach for robust aggregation of model updates based on Bayesian inference. The mean update is defined by the maximum of the likelihood marginalized over probabilities of each client to be `honest'. As a result, the method shares the simplicity of the classical average estimators (e.g., sample mean or geometric median), being independent of the number of compromised clients. At the same time, it is as effective against attacks as methods specifically tailored to Federated Learning, such as Krum. We compare our approach with other aggregation schemes in federated setting on three benchmark image classification data sets. The proposed method consistently achieves state-of-the-art performance across various attack types with static and varying number of malicious clients."
      },
      {
        "id": "oai:arXiv.org:2505.02501v1",
        "title": "Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions",
        "link": "https://arxiv.org/abs/2505.02501",
        "author": "Asma Brazi, Boris Meden, Fabrice Mayran de Chamisso, Steve Bourgeois, Vincent Lepetit",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02501v1 Announce Type: new \nAbstract: We introduce Corr2Distrib, the first correspondence-based method which estimates a 6D camera pose distribution from an RGB image, explaining the observations. Indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. While a few recent methods tackle this problem, they do not rely on local correspondences which, according to the BOP Challenge, are currently the most effective way to estimate a single 6DoF pose solution. Using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of PnP. With Corr2Distrib, we turn these ambiguities into an advantage to recover all valid poses. Corr2Distrib first learns a symmetry-aware representation for each 3D point on the object's surface, characterized by a descriptor and a local frame. This representation enables the generation of 3DoF rotation hypotheses from single 2D-3D correspondences. Next, we refine these hypotheses into a 6DoF pose distribution using PnP and pose scoring. Our experimental evaluations on complex non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an RGB image, demonstrating the potential of correspondences-based approaches."
      },
      {
        "id": "oai:arXiv.org:2505.02506v1",
        "title": "Exploring Design Choices for Autoregressive Deep Learning Climate Models",
        "link": "https://arxiv.org/abs/2505.02506",
        "author": "Florian Gallusser, Simon Hentschel, Anna Krause, Andreas Hotho",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02506v1 Announce Type: new \nAbstract: Deep Learning models have achieved state-of-the-art performance in medium-range weather prediction but often fail to maintain physically consistent rollouts beyond 14 days. In contrast, a few atmospheric models demonstrate stability over decades, though the key design choices enabling this remain unclear. This study quantitatively compares the long-term stability of three prominent DL-MWP architectures - FourCastNet, SFNO, and ClimaX - trained on ERA5 reanalysis data at 5.625{\\deg} resolution. We systematically assess the impact of autoregressive training steps, model capacity, and choice of prognostic variables, identifying configurations that enable stable 10-year rollouts while preserving the statistical properties of the reference dataset. Notably, rollouts with SFNO exhibit the greatest robustness to hyperparameter choices, yet all models can experience instability depending on the random seed and the set of prognostic variables"
      },
      {
        "id": "oai:arXiv.org:2505.02514v1",
        "title": "Uncovering Population PK Covariates from VAE-Generated Latent Spaces",
        "link": "https://arxiv.org/abs/2505.02514",
        "author": "Diego Perazzolo, Chiara Castellani, Enrico Grisan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02514v1 Announce Type: new \nAbstract: Population pharmacokinetic (PopPK) modelling is a fundamental tool for understanding drug behaviour across diverse patient populations and enabling personalized dosing strategies to improve therapeutic outcomes. A key challenge in PopPK analysis lies in identifying and modelling covariates that influence drug absorption, as these relationships are often complex and nonlinear. Traditional methods may fail to capture hidden patterns within the data. In this study, we propose a data-driven, model-free framework that integrates Variational Autoencoders (VAEs) deep learning model and LASSO regression to uncover key covariates from simulated tacrolimus pharmacokinetic (PK) profiles. The VAE compresses high-dimensional PK signals into a structured latent space, achieving accurate reconstruction with a mean absolute percentage error (MAPE) of 2.26%. LASSO regression is then applied to map patient-specific covariates to the latent space, enabling sparse feature selection through L1 regularization. This approach consistently identifies clinically relevant covariates for tacrolimus including SNP, age, albumin, and hemoglobin which are retained across the tested regularization strength levels, while effectively discarding non-informative features. The proposed VAE-LASSO methodology offers a scalable, interpretable, and fully data-driven solution for covariate selection, with promising applications in drug development and precision pharmacotherapy."
      },
      {
        "id": "oai:arXiv.org:2505.02515v1",
        "title": "FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization",
        "link": "https://arxiv.org/abs/2505.02515",
        "author": "Hongze Li, Zesheng Zhou, Zhenbiao Cao, Xinhui Li, Wei Chen, Xiaojin Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02515v1 Announce Type: new \nAbstract: Traditional domain generalization approaches predominantly focus on leveraging target domain-aware features while overlooking the critical role of source domain-specific characteristics, particularly in federated settings with inherent data isolation. To address this gap, we propose the Federated Source Domain Awareness Framework (FedSDAF), the first method to systematically exploit source domain-aware features for enhanced federated domain generalization (FedDG). The FedSDAF framework consists of two synergistic components: the Domain-Invariant Adapter, which preserves critical domain-invariant features, and the Domain-Aware Adapter, which extracts and integrates source domain-specific knowledge using a Multihead Self-Attention mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge distillation mechanism that fosters knowledge sharing among clients while safeguarding privacy. Our approach represents the first systematic exploitation of source domain-aware features, resulting in significant advancements in model generalization capability.Extensive experiments on four standard benchmarks (OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently surpasses state-of-the-art federated domain generalization approaches, with accuracy gains of 5.2-13.8%. The source code is available at https://github.com/pizzareapers/FedSDAF."
      },
      {
        "id": "oai:arXiv.org:2505.02518v1",
        "title": "Bemba Speech Translation: Exploring a Low-Resource African Language",
        "link": "https://arxiv.org/abs/2505.02518",
        "author": "Muhammad Hazim Al Farouq, Aman Kassahun Wassie, Yasmin Moslem",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02518v1 Announce Type: new \nAbstract: This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2025), low-resource languages track, namely for Bemba-to-English speech translation. We built cascaded speech translation systems based on Whisper and NLLB-200, and employed data augmentation techniques, such as back-translation. We investigate the effect of using synthetic data and discuss our experimental setup."
      },
      {
        "id": "oai:arXiv.org:2505.02527v1",
        "title": "Text to Image Generation and Editing: A Survey",
        "link": "https://arxiv.org/abs/2505.02527",
        "author": "Pengfei Yang, Ngai-Man Cheung, Xinda Ma",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02527v1 Announce Type: new \nAbstract: Text-to-image generation (T2I) refers to the text-guided generation of high-quality images. In the past few years, T2I has attracted widespread attention and numerous works have emerged. In this survey, we comprehensively review 141 works conducted from 2021 to 2024. First, we introduce four foundation model architectures of T2I (autoregression, non-autoregression, GAN and diffusion) and the commonly used key technologies (autoencoder, attention and classifier-free guidance). Secondly, we systematically compare the methods of these studies in two directions, T2I generation and T2I editing, including the encoders and the key technologies they use. In addition, we also compare the performance of these researches side by side in terms of datasets, evaluation metrics, training resources, and inference speed. In addition to the four foundation models, we survey other works on T2I, such as energy-based models and recent Mamba and multimodality. We also investigate the potential social impact of T2I and provide some solutions. Finally, we propose unique insights of improving the performance of T2I models and possible future development directions. In summary, this survey is the first systematic and comprehensive overview of T2I, aiming to provide a valuable guide for future researchers and stimulate continued progress in this field."
      },
      {
        "id": "oai:arXiv.org:2505.02529v1",
        "title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction",
        "link": "https://arxiv.org/abs/2505.02529",
        "author": "Aiman Farooq, Azad Singh, Deepak Mishra, Santanu Chaudhury",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02529v1 Announce Type: new \nAbstract: Cancer survival prediction using multi-modal medical imaging presents a critical challenge in oncology, mainly due to the vulnerability of deep learning models to noise and protocol variations across imaging centers. Current approaches struggle to extract consistent features from heterogeneous CT and PET images, limiting their clinical applicability. We address these challenges by introducing RobSurv, a robust deep-learning framework that leverages vector quantization for resilient multi-modal feature learning. The key innovation of our approach lies in its dual-path architecture: one path maps continuous imaging features to learned discrete codebooks for noise-resistant representation, while the parallel path preserves fine-grained details through continuous feature processing. This dual representation is integrated through a novel patch-wise fusion mechanism that maintains local spatial relationships while capturing global context via Transformer-based processing. In extensive evaluations across three diverse datasets (HECKTOR, H\\&amp;N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance, achieving concordance index of 0.771, 0.742, and 0.734 respectively - significantly outperforming existing methods. Most notably, our model maintains robust performance even under severe noise conditions, with performance degradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These results, combined with strong generalization across different cancer types and imaging protocols, establish RobSurv as a promising solution for reliable clinical prognosis that can enhance treatment planning and patient care."
      },
      {
        "id": "oai:arXiv.org:2505.02537v1",
        "title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations",
        "link": "https://arxiv.org/abs/2505.02537",
        "author": "Davide Sartor, Alberto Sinigaglia, Gian Antonio Susto",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02537v1 Announce Type: new \nAbstract: Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which pose well-known optimization challenges. In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions. Additionally, we show an equivalence between the saturation side in the activations and the sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts. Our results provide theoretical grounding to the empirical effectiveness observed in previous works while leading to possible architectural simplification. Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability. Experimental evaluation reinforces the validity of the theoretical results, showing that our novel approach compares favourably to traditional monotonic architectures."
      },
      {
        "id": "oai:arXiv.org:2505.02539v1",
        "title": "Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction",
        "link": "https://arxiv.org/abs/2505.02539",
        "author": "Nahuel Garcia-D'Urso, Bernabe Sanchez-Sos, Jorge Azorin-Lopez, Andres Fuster-Guillo, Antonio Macia-Lillo, Higinio Mora-Mora",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02539v1 Announce Type: new \nAbstract: Accurate 3D reconstruction using multi-camera RGB-D systems critically depends on precise extrinsic calibration to achieve proper alignment between captured views. In this paper, we introduce an iterative extrinsic calibration method that leverages the geometric constraints provided by a three-dimensional marker to significantly improve calibration accuracy. Our proposed approach systematically segments and refines marker planes through clustering, regression analysis, and iterative reassignment techniques, ensuring robust geometric correspondence across camera views. We validate our method comprehensively in both controlled environments and practical real-world settings within the Tech4Diet project, aimed at modeling the physical progression of patients undergoing nutritional treatments. Experimental results demonstrate substantial reductions in alignment errors, facilitating accurate and reliable 3D reconstructions."
      },
      {
        "id": "oai:arXiv.org:2505.02540v1",
        "title": "Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data",
        "link": "https://arxiv.org/abs/2505.02540",
        "author": "Ljubomir Rokvic, Panayiotis Danassis, Boi Faltings",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02540v1 Announce Type: new \nAbstract: In Federated Learning, heterogeneity in client data distributions often means that a single global model does not have the best performance for individual clients. Consider for example training a next-word prediction model for keyboards: user-specific language patterns due to demographics (dialect, age, etc.), language proficiency, and writing style result in a highly non-IID dataset across clients. Other examples are medical images taken with different machines, or driving data from different vehicle types. To address this, we propose a simple yet effective personalized federated learning framework (pFedLIA) that utilizes a computationally efficient influence approximation, called `Lazy Influence', to cluster clients in a distributed manner before model aggregation. Within each cluster, data owners collaborate to jointly train a model that captures the specific data patterns of the clients. Our method has been shown to successfully recover the global model's performance drop due to the non-IID-ness in various synthetic and real-world settings, specifically a next-word prediction task on the Nordic languages as well as several benchmark tasks. It matches the performance of a hypothetical Oracle clustering, and significantly improves on existing baselines, e.g., an improvement of 17% on CIFAR100."
      },
      {
        "id": "oai:arXiv.org:2505.02549v1",
        "title": "Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identfication",
        "link": "https://arxiv.org/abs/2505.02549",
        "author": "Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02549v1 Announce Type: new \nAbstract: Unsupervised visible-infrared person re-identification (UVI-ReID) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. Existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. In practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. To address this, we introduce a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. To this end, we propose a novel Robust Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning mechanism (RAL) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. Second, to alleviate error accumulation-where the model reinforces its own mistakes-RoDE employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. However, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. To resolve this, we introduce Cluster Consistency Matching (CCM), which aligns clusters across models and modalities by measuring cross-cluster similarity. Extensive experiments on three benchmarks demonstrate the effectiveness of RoDE."
      },
      {
        "id": "oai:arXiv.org:2505.02550v1",
        "title": "Bielik v3 Small: Technical Report",
        "link": "https://arxiv.org/abs/2505.02550",
        "author": "Krzysztof Ociepa, {\\L}ukasz Flis, Remigiusz Kinas, Krzysztof Wr\\'obel, Adrian Gwo\\'zdziej",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02550v1 Announce Type: new \nAbstract: We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications."
      },
      {
        "id": "oai:arXiv.org:2505.02566v1",
        "title": "Robustness questions the interpretability of graph neural networks: what to do?",
        "link": "https://arxiv.org/abs/2505.02566",
        "author": "Kirill Lukyanov (ISP RAS Research Center for Trusted Artificial Intelligence, Ivannikov Institute for System Programming of the Russian Academy of Sciences, Moscow Institute of Physics and Technology), Georgii Sazonov (Ivannikov Institute for System Programming of the Russian Academy of Sciences, Lomonosov Moscow State University), Serafim Boyarsky (Yandex School of Data Analysis), Ilya Makarov (1 v 5)",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02566v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across five datasets from two distinct domains, employing four interpretability metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how defenses against poisoning and evasion attacks, applied before and during model training, affect interpretability and highlights critical trade-offs between robustness and interpretability. The framework will be published as open source.\n  The results reveal significant variations in interpretability depending on the chosen defense methods and model architecture characteristics. By establishing a standardized benchmark, this work provides a foundation for developing GNNs that are both robust to adversarial threats and interpretable, facilitating trust in their deployment in sensitive applications."
      },
      {
        "id": "oai:arXiv.org:2505.02567v1",
        "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "link": "https://arxiv.org/abs/2505.02567",
        "author": "Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02567v1 Announce Type: new \nAbstract: Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey will be available on GitHub soon."
      },
      {
        "id": "oai:arXiv.org:2505.02573v1",
        "title": "Rethinking Federated Graph Learning: A Data Condensation Perspective",
        "link": "https://arxiv.org/abs/2505.02573",
        "author": "Hao Zhang, Xunkai Li, Yinlin Zhu, Lianglin Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02573v1 Announce Type: new \nAbstract: Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client graphs.However, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm."
      },
      {
        "id": "oai:arXiv.org:2505.02579v1",
        "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning",
        "link": "https://arxiv.org/abs/2505.02579",
        "author": "Lingxiao Kong (Fraunhofer Institute for Applied Information Technology FIT), Cong Yang (Soochow University), Susanne Neufang (University Hospital of Cologne), Oya Deniz Beyan (Fraunhofer Institute for Applied Information Technology FIT, University Hospital of Cologne), Zeyd Boukhers (Fraunhofer Institute for Applied Information Technology FIT, University Hospital of Cologne)",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02579v1 Announce Type: new \nAbstract: Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives."
      },
      {
        "id": "oai:arXiv.org:2505.02583v1",
        "title": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era",
        "link": "https://arxiv.org/abs/2505.02583",
        "author": "Chenxi Liu, Shaowen Zhou, Qianxiong Xu, Hao Miao, Cheng Long, Ziyue Li, Rui Zhao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02583v1 Announce Type: new \nAbstract: The proliferation of edge devices has generated an unprecedented volume of time series data across different domains, motivating various well-customized methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm for time series analytics by leveraging the shared sequential nature of textual data and time series. However, a fundamental cross-modality gap between time series and LLMs exists, as LLMs are pre-trained on textual corpora and are not inherently optimized for time series. Many recent proposals are designed to address this issue. In this survey, we provide an up-to-date overview of LLMs-based cross-modality modeling for time series analytics. We first introduce a taxonomy that classifies existing approaches into four groups based on the type of textual data employed for time series modeling. We then summarize key cross-modality strategies, e.g., alignment and fusion, and discuss their applications across a range of downstream tasks. Furthermore, we conduct experiments on multimodal datasets from different application domains to investigate effective combinations of textual data and cross-modality strategies for enhancing time series analytics. Finally, we suggest several promising directions for future research. This survey is designed for a range of professionals, researchers, and practitioners interested in LLM-based time series modeling."
      },
      {
        "id": "oai:arXiv.org:2505.02586v1",
        "title": "RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet",
        "link": "https://arxiv.org/abs/2505.02586",
        "author": "Eliraz Orfaig, Inna Stainvas, Igal Bilik",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02586v1 Announce Type: new \nAbstract: This work introduces RGBX-DiffusionDet, an object detection framework extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB imagery via an adaptive multimodal encoder. To enable cross-modal interaction, we design the dynamic channel reduction within a convolutional block attention module (DCR-CBAM), which facilitates cross-talk between subnetworks by dynamically highlighting salient channel features. Furthermore, the dynamic multi-level aggregation block (DMLAB) is proposed to refine spatial feature representations through adaptive multiscale fusion. Finally, novel regularization losses that enforce channel saliency and spatial selectivity are introduced, leading to compact and discriminative feature embeddings. Extensive experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We demonstrate consistent superiority of the proposed approach over the baseline RGB-only DiffusionDet. The modular architecture maintains the original decoding complexity, ensuring efficiency. These results establish the proposed RGBX-DiffusionDet as a flexible multimodal object detection approach, providing new insights into integrating diverse 2D sensing modalities into diffusion-based detection pipelines."
      },
      {
        "id": "oai:arXiv.org:2505.02590v1",
        "title": "Ensemble Kalman filter for uncertainty in human language comprehension",
        "link": "https://arxiv.org/abs/2505.02590",
        "author": "Diksha Bhandari, Alessandro Lopopolo, Milena Rabovsky, Sebastian Reich",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02590v1 Announce Type: new \nAbstract: Artificial neural networks (ANNs) are widely used in modeling sentence processing but often exhibit deterministic behavior, contrasting with human sentence comprehension, which manages uncertainty during ambiguous or unexpected inputs. This is exemplified by reversal anomalies-sentences with unexpected role reversals that challenge syntax and semantics-highlighting the limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model. To address these limitations, we propose a Bayesian framework for sentence comprehension, applying an extension of the ensemble Kalman filter (EnKF) for Bayesian inference to quantify uncertainty. By framing language comprehension as a Bayesian inverse problem, this approach enhances the SG model's ability to reflect human sentence processing with respect to the representation of uncertainty. Numerical experiments and comparisons with maximum likelihood estimation (MLE) demonstrate that Bayesian methods improve uncertainty representation, enabling the model to better approximate human cognitive processing when dealing with linguistic ambiguities."
      },
      {
        "id": "oai:arXiv.org:2505.02593v1",
        "title": "DELTA: Dense Depth from Events and LiDAR using Transformer's Attention",
        "link": "https://arxiv.org/abs/2505.02593",
        "author": "Vincent Brebion, Julien Moreau, Franck Davoine",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02593v1 Announce Type: new \nAbstract: Event cameras and LiDARs provide complementary yet distinct data: respectively, asynchronous detections of changes in lighting versus sparse but accurate depth information at a fixed rate. To this day, few works have explored the combination of these two modalities. In this article, we propose a novel neural-network-based method for fusing event and LiDAR data in order to estimate dense depth maps. Our architecture, DELTA, exploits the concepts of self- and cross-attention to model the spatial and temporal relations within and between the event and LiDAR data. Following a thorough evaluation, we demonstrate that DELTA sets a new state of the art in the event-based depth estimation problem, and that it is able to reduce the errors up to four times for close ranges compared to the previous SOTA."
      },
      {
        "id": "oai:arXiv.org:2505.02604v1",
        "title": "Low-Loss Space in Neural Networks is Continuous and Fully Connected",
        "link": "https://arxiv.org/abs/2505.02604",
        "author": "Yongding Tian, Zaid Al-Ars, Maksim Kitsak, Peter Hofstee",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02604v1 Announce Type: new \nAbstract: Visualizations of the loss landscape in neural networks suggest that minima are isolated points. However, both theoretical and empirical studies indicate that it is possible to connect two different minima with a path consisting of intermediate points that also have low loss. In this study, we propose a new algorithm which investigates low-loss paths in the full parameter space, not only between two minima. Our experiments on LeNet5, ResNet18, and Compact Convolutional Transformer architectures consistently demonstrate the existence of such continuous paths in the parameter space. These results suggest that the low-loss region is a fully connected and continuous space in the parameter space. Our findings provide theoretical insight into neural network over-parameterization, highlighting that parameters collectively define a high-dimensional low-loss space, implying parameter redundancy exists only within individual models and not throughout the entire low-loss space. Additionally, our work also provides new visualization methods and opportunities to improve model generalization by exploring the low-loss space that is closer to the origin."
      },
      {
        "id": "oai:arXiv.org:2505.02615v1",
        "title": "Automatic Proficiency Assessment in L2 English Learners",
        "link": "https://arxiv.org/abs/2505.02615",
        "author": "Armita Mohammadi, Alessandro Lameiras Koerich, Laureano Moro-Velazquez, Patrick Cardinal",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02615v1 Announce Type: new \nAbstract: Second language proficiency (L2) in English is usually perceptually evaluated by English teachers or expert evaluators, with the inherent intra- and inter-rater variability. This paper explores deep learning techniques for comprehensive L2 proficiency assessment, addressing both the speech signal and its correspondent transcription. We analyze spoken proficiency classification prediction using diverse architectures, including 2D CNN, frequency-based CNN, ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based proficiency assessment by fine-tuning a BERT language model within resource constraints. Finally, we tackle the complex task of spontaneous dialogue assessment, managing long-form audio and speaker interactions through separate applications of wav2vec 2.0 and BERT models. Results from experiments on EFCamDat and ANGLISH datasets and a private dataset highlight the potential of deep learning, especially the pretrained wav2vec 2.0 model, for robust automated L2 proficiency evaluation."
      },
      {
        "id": "oai:arXiv.org:2505.02621v1",
        "title": "Mirror Mean-Field Langevin Dynamics",
        "link": "https://arxiv.org/abs/2505.02621",
        "author": "Anming Gu, Juno Kim",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02621v1 Announce Type: new \nAbstract: The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional on the Wasserstein space over $\\mathbb{R}^d$, and has gained attention recently as a model for the gradient descent dynamics of interacting particle systems such as infinite-width two-layer neural networks. However, many problems of interest have constrained domains, which are not solved by existing mean-field algorithms due to the global diffusion term. We study the optimization of probability measures constrained to a convex subset of $\\mathbb{R}^d$ by proposing the \\emph{mirror mean-field Langevin dynamics} (MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain linear convergence guarantees for the continuous MMFLD via a uniform log-Sobolev inequality, and uniform-in-time propagation of chaos results for its time- and particle-discretized counterpart."
      },
      {
        "id": "oai:arXiv.org:2505.02625v1",
        "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis",
        "link": "https://arxiv.org/abs/2505.02625",
        "author": "Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, Yang Feng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02625v1 Announce Type: new \nAbstract: Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data."
      },
      {
        "id": "oai:arXiv.org:2505.02626v1",
        "title": "Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models",
        "link": "https://arxiv.org/abs/2505.02626",
        "author": "Sassan Mokhtar, Arian Mousakhan, Silvio Galesso, Jawad Tayyub, Thomas Brox",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02626v1 Announce Type: new \nAbstract: Recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. However, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. To address this gap, we propose VELM, a novel LLM-based pipeline for anomaly classification. Given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. If an anomaly is detected, the LLM then classifies its type. A key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. To address this limitation, we introduce MVTec-AC and VisA-AC, refined versions of the widely used MVTec-AD and VisA datasets, which include accurate anomaly class labels for rigorous evaluation. Our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD, exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the effectiveness of VELM in understanding and categorizing anomalies. We hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization."
      },
      {
        "id": "oai:arXiv.org:2505.02627v1",
        "title": "A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition",
        "link": "https://arxiv.org/abs/2505.02627",
        "author": "Yuanpeng Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02627v1 Announce Type: new \nAbstract: Compositional generalization is a crucial property in artificial intelligence, enabling models to handle novel combinations of known components. While most deep learning models lack this capability, certain models succeed in specific tasks, suggesting the existence of governing conditions. This paper derives a necessary and sufficient condition for compositional generalization in neural networks. Conceptually, it requires that (i) the computational graph matches the true compositional structure, and (ii) components encode just enough information in training. The condition is supported by mathematical proofs. This criterion combines aspects of architecture design, regularization, and training data properties. A carefully designed minimal example illustrates an intuitive understanding of the condition. We also discuss the potential of the condition for assessing compositional generalization before training. This work is a fundamental theoretical study of compositional generalization in neural networks."
      },
      {
        "id": "oai:arXiv.org:2505.02634v1",
        "title": "Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.02634",
        "author": "David Ramos, Lucas Lacasa, Eusebio Valero, Gonzalo Rubio",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02634v1 Announce Type: new \nAbstract: The main objective of this paper is to introduce a transfer learning-enhanced, multi-objective, deep reinforcement learning (DRL) methodology that is able to optimise the geometry of any airfoil based on concomitant aerodynamic and structural criteria. To showcase the method, we aim to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as modelled by its maximum thickness -- and train the DRL agent using a list of different transfer learning (TL) strategies. The performance of the DRL agent is compared with Particle Swarm Optimisation (PSO), a traditional gradient-free optimisation method. Results indicate that DRL agents are able to perform multi-objective shape optimisation, that the DRL approach outperforms PSO in terms of computational efficiency and shape optimisation performance, and that the TL-enhanced DRL agent achieves performance comparable to the DRL one, while further saving substantial computational resources."
      },
      {
        "id": "oai:arXiv.org:2505.02639v1",
        "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning",
        "link": "https://arxiv.org/abs/2505.02639",
        "author": "Xuan Lin, Qingrui Liu, Hongxin Xiang, Daojian Zeng, Xiangxiang Zeng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02639v1 Announce Type: new \nAbstract: Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design."
      },
      {
        "id": "oai:arXiv.org:2505.02640v1",
        "title": "Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints",
        "link": "https://arxiv.org/abs/2505.02640",
        "author": "Shubham Vaishnav, Praveen Kumar Donta, Sindri Magn\\'usson",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02640v1 Announce Type: new \nAbstract: Internet of Things (IoT) systems increasingly operate in environments where devices must respond in real time while managing fluctuating resource constraints, including energy and bandwidth. Yet, current approaches often fall short in addressing scenarios where operational constraints evolve over time. To address these limitations, we propose a novel Budgeted Multi-Armed Bandit framework tailored for IoT applications with dynamic operational limits. Our model introduces a decaying violation budget, which permits limited constraint violations early in the learning process and gradually enforces stricter compliance over time. We present the Budgeted Upper Confidence Bound (UCB) algorithm, which adaptively balances performance optimization and compliance with time-varying constraints. We provide theoretical guarantees showing that Budgeted UCB achieves sublinear regret and logarithmic constraint violations over the learning horizon. Extensive simulations in a wireless communication setting show that our approach achieves faster adaptation and better constraint satisfaction than standard online learning methods. These results highlight the framework's potential for building adaptive, resource-aware IoT systems."
      },
      {
        "id": "oai:arXiv.org:2505.02648v1",
        "title": "MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2505.02648",
        "author": "Mingcheng Li, Xiaolu Hou, Ziyang Liu, Dingkang Yang, Ziyun Qian, Jiawei Chen, Jinjie Wei, Yue Jiang, Qingyao Xu, Lihua Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02648v1 Announce Type: new \nAbstract: Diffusion models have shown excellent performance in text-to-image generation. Nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing MLLMs to extract various scene elements effectively. In addition, Hierarchical Compositional diffusion utilizes a Gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation."
      },
      {
        "id": "oai:arXiv.org:2505.02654v1",
        "title": "Sim2Real in endoscopy segmentation with a novel structure aware image translation",
        "link": "https://arxiv.org/abs/2505.02654",
        "author": "Clara Tomasini, Luis Riazuelo, Ana C. Murillo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02654v1 Announce Type: new \nAbstract: Automatic segmentation of anatomical landmarks in endoscopic images can provide assistance to doctors and surgeons for diagnosis, treatments or medical training. However, obtaining the annotations required to train commonly used supervised learning methods is a tedious and difficult task, in particular for real images. While ground truth annotations are easier to obtain for synthetic data, models trained on such data often do not generalize well to real data. Generative approaches can add realistic texture to it, but face difficulties to maintain the structure of the original scene. The main contribution in this work is a novel image translation model that adds realistic texture to simulated endoscopic images while keeping the key scene layout information. Our approach produces realistic images in different endoscopy scenarios. We demonstrate these images can effectively be used to successfully train a model for a challenging end task without any real labeled data. In particular, we demonstrate our approach for the task of fold segmentation in colonoscopy images. Folds are key anatomical landmarks that can occlude parts of the colon mucosa and possible polyps. Our approach generates realistic images maintaining the shape and location of the original folds, after the image-style-translation, better than existing methods. We run experiments both on a novel simulated dataset for fold segmentation, and real data from the EndoMapper (EM) dataset. All our new generated data and new EM metadata is being released to facilitate further research, as no public benchmark is currently available for the task of fold segmentation."
      },
      {
        "id": "oai:arXiv.org:2505.02655v1",
        "title": "SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting",
        "link": "https://arxiv.org/abs/2505.02655",
        "author": "Shiwei Guo, Ziang Chen, Yupeng Ma, Yunfei Han, Yi Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02655v1 Announce Type: new \nAbstract: The Transformer model has shown strong performance in multivariate time series forecasting by leveraging channel-wise self-attention. However, this approach lacks temporal constraints when computing temporal features and does not utilize cumulative historical series effectively.To address these limitations, we propose the Structured Channel-wise Transformer with Cumulative Historical state (SCFormer). SCFormer introduces temporal constraints to all linear transformations, including the query, key, and value matrices, as well as the fully connected layers within the Transformer. Additionally, SCFormer employs High-order Polynomial Projection Operators (HiPPO) to deal with cumulative historical time series, allowing the model to incorporate information beyond the look-back window during prediction. Extensive experiments on multiple real-world datasets demonstrate that SCFormer significantly outperforms mainstream baselines, highlighting its effectiveness in enhancing time series forecasting. The code is publicly available at https://github.com/ShiweiGuo1995/SCFormer"
      },
      {
        "id": "oai:arXiv.org:2505.02656v1",
        "title": "Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset",
        "link": "https://arxiv.org/abs/2505.02656",
        "author": "Rawan Bondok, Mayar Nassar, Salam Khalifa, Kurt Micallaf, Nizar Habash",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02656v1 Announce Type: new \nAbstract: Proper names in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP,their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper names of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper name diacritization."
      },
      {
        "id": "oai:arXiv.org:2505.02659v1",
        "title": "A Note on Statistically Accurate Tabular Data Generation Using Large Language Models",
        "link": "https://arxiv.org/abs/2505.02659",
        "author": "Andrey Sidorenko",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02659v1 Announce Type: new \nAbstract: Large language models (LLMs) have shown promise in synthetic tabular data generation, yet existing methods struggle to preserve complex feature dependencies, particularly among categorical variables. This work introduces a probability-driven prompting approach that leverages LLMs to estimate conditional distributions, enabling more accurate and scalable data synthesis. The results highlight the potential of prompting probobility distributions to enhance the statistical fidelity of LLM-generated tabular data."
      },
      {
        "id": "oai:arXiv.org:2505.02666v1",
        "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design",
        "link": "https://arxiv.org/abs/2505.02666",
        "author": "Miaomiao Ji, Yanqiu Wu, Zhibin Wu, Shoujin Wang, Jian Yang, Mark Dras, Usman Naseem",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02666v1 Announce Type: new \nAbstract: The alignment of large language models (LLMs) with human values and intentions represents a core challenge in current AI research, where reward mechanism design has become a critical factor in shaping model behavior. This study conducts a comprehensive investigation of reward mechanisms in LLM alignment through a systematic theoretical framework, categorizing their development into three key phases: (1) feedback (diagnosis), (2) reward design (prescription), and (3) optimization (treatment). Through a four-dimensional analysis encompassing construction basis, format, expression, and granularity, this research establishes a systematic classification framework that reveals evolutionary trends in reward modeling. The field of LLM alignment faces several persistent challenges, while recent advances in reward design are driving significant paradigm shifts. Notable developments include the transition from reinforcement learning-based frameworks to novel optimization paradigms, as well as enhanced capabilities to address complex alignment scenarios involving multimodal integration and concurrent task coordination. Finally, this survey outlines promising future research directions for LLM alignment through innovative reward design strategies."
      },
      {
        "id": "oai:arXiv.org:2505.02677v1",
        "title": "Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data",
        "link": "https://arxiv.org/abs/2505.02677",
        "author": "Saeed Shurrab, Aadim Nepal, Terrence J. Lee-St. John, Nicola G. Ghazi, Bartlomiej Piechowski-Jozwiak, Farah E. Shamout",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02677v1 Announce Type: new \nAbstract: Stroke is a major public health problem, affecting millions worldwide. Deep learning has recently demonstrated promise for enhancing the diagnosis and risk prediction of stroke. However, existing methods rely on costly medical imaging modalities, such as computed tomography. Recent studies suggest that retinal imaging could offer a cost-effective alternative for cerebrovascular health assessment due to the shared clinical pathways between the retina and the brain. Hence, this study explores the impact of leveraging retinal images and clinical data for stroke detection and risk prediction. We propose a multimodal deep neural network that processes Optical Coherence Tomography (OCT) and infrared reflectance retinal scans, combined with clinical data, such as demographics, vital signs, and diagnosis codes. We pretrained our model using a self-supervised learning framework using a real-world dataset consisting of $37$ k scans, and then fine-tuned and evaluated the model using a smaller labeled subset. Our empirical findings establish the predictive ability of the considered modalities in detecting lasting effects in the retina associated with acute stroke and forecasting future risk within a specific time horizon. The experimental results demonstrate the effectiveness of our proposed framework by achieving $5$\\% AUROC improvement as compared to the unimodal image-only baseline, and $8$\\% improvement compared to an existing state-of-the-art foundation model. In conclusion, our study highlights the potential of retinal imaging in identifying high-risk patients and improving long-term outcomes."
      },
      {
        "id": "oai:arXiv.org:2505.02686v1",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models",
        "link": "https://arxiv.org/abs/2505.02686",
        "author": "Xiaobao Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02686v1 Announce Type: new \nAbstract: Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers."
      },
      {
        "id": "oai:arXiv.org:2505.02690v1",
        "title": "Dance of Fireworks: An Interactive Broadcast Gymnastics Training System Based on Pose Estimation",
        "link": "https://arxiv.org/abs/2505.02690",
        "author": "Haotian Chen, Ziyu Liu, Xi Cheng, Chuangqi Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02690v1 Announce Type: new \nAbstract: This study introduces Dance of Fireworks, an interactive system designed to combat sedentary health risks by enhancing engagement in radio calisthenics. Leveraging mobile device cameras and lightweight pose estimation (PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint angles, and compares them with standardized motions to deliver real-time corrective feedback. To incentivize participation, it dynamically maps users' movements (such as joint angles and velocity) to customizable fireworks animations, rewarding improved accuracy with richer visual effects.\nExperiments involving 136 participants demonstrated a significant reduction in average joint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four sessions, with 93.4 percent of users affirming its exercise-promoting efficacy and 85.4 percent praising its entertainment value. The system operates without predefined motion templates or specialised hardware, enabling seamless integration into office environments. Future enhancements will focus on improving pose recognition accuracy, reducing latency, and adding features such as multiplayer interaction and music synchronisation. This work presents a cost-effective, engaging solution to promote physical activity in sedentary populations."
      },
      {
        "id": "oai:arXiv.org:2505.02692v1",
        "title": "fastabx: A library for efficient computation of ABX discriminability",
        "link": "https://arxiv.org/abs/2505.02692",
        "author": "Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02692v1 Announce Type: new \nAbstract: We introduce fastabx, a high-performance Python library for building ABX discrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx."
      },
      {
        "id": "oai:arXiv.org:2505.02703v1",
        "title": "Structure Causal Models and LLMs Integration in Medical Visual Question Answering",
        "link": "https://arxiv.org/abs/2505.02703",
        "author": "Zibo Xu, Qiang Li, Weizhi Nie, Weijie Wang, Anan Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02703v1 Announce Type: new \nAbstract: Medical Visual Question Answering (MedVQA) aims to answer medical questions according to medical images. However, the complexity of medical data leads to confounders that are difficult to observe, so bias between images and questions is inevitable. Such cross-modal bias makes it challenging to infer medically meaningful answers. In this work, we propose a causal inference framework for the MedVQA task, which effectively eliminates the relative confounding effect between the image and the question to ensure the precision of the question-answering (QA) session. We are the first to introduce a novel causal graph structure that represents the interaction between visual and textual elements, explicitly capturing how different questions influence visual features. During optimization, we apply the mutual information to discover spurious correlations and propose a multi-variable resampling front-door adjustment method to eliminate the relative confounding effect, which aims to align features based on their true causal relevance to the question-answering task. In addition, we also introduce a prompt strategy that combines multiple prompt forms to improve the model's ability to understand complex medical data and answer accurately. Extensive experiments on three MedVQA datasets demonstrate that 1) our method significantly improves the accuracy of MedVQA, and 2) our method achieves true causal correlations in the face of complex medical data."
      },
      {
        "id": "oai:arXiv.org:2505.02704v1",
        "title": "Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery",
        "link": "https://arxiv.org/abs/2505.02704",
        "author": "Bojin Wu, Jing Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02704v1 Announce Type: new \nAbstract: We propose a robust method for monocular depth scale recovery. Monocular depth estimation can be divided into two main directions: (1) relative depth estimation, which provides normalized or inverse depth without scale information, and (2) metric depth estimation, which involves recovering depth with absolute scale. To obtain absolute scale information for practical downstream tasks, utilizing textual information to recover the scale of a relative depth map is a highly promising approach. However, since a single image can have multiple descriptions from different perspectives or with varying styles, it has been shown that different textual descriptions can significantly affect the scale recovery process. To address this issue, our method, VGLD, stabilizes the influence of textual information by incorporating high-level semantic information from the corresponding image alongside the textual description. This approach resolves textual ambiguities and robustly outputs a set of linear transformation parameters (scalars) that can be globally applied to the relative depth map, ultimately generating depth predictions with metric-scale accuracy. We validate our method across several popular relative depth models(MiDas, DepthAnything), using both indoor scenes (NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions as a universal alignment module when trained on multiple datasets, achieving strong performance even in zero-shot scenarios. Code is available at: https://github.com/pakinwu/VGLD."
      },
      {
        "id": "oai:arXiv.org:2505.02712v1",
        "title": "Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework",
        "link": "https://arxiv.org/abs/2505.02712",
        "author": "Andrzej Mizera, Jakub Zarzycki",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02712v1 Announce Type: new \nAbstract: Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach."
      },
      {
        "id": "oai:arXiv.org:2505.02714v1",
        "title": "Less is More: Efficient Weight Farcasting with 1-Layer Neural Network",
        "link": "https://arxiv.org/abs/2505.02714",
        "author": "Xiao Shou, Debarun Bhattacharjya, Yanna Ding, Chen Zhao, Rui Li, Jianxi Gao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02714v1 Announce Type: new \nAbstract: Addressing the computational challenges inherent in training large-scale deep neural networks remains a critical endeavor in contemporary machine learning research. While previous efforts have focused on enhancing training efficiency through techniques such as gradient descent with momentum, learning rate scheduling, and weight regularization, the demand for further innovation continues to burgeon as model sizes keep expanding. In this study, we introduce a novel framework which diverges from conventional approaches by leveraging long-term time series forecasting techniques. Our method capitalizes solely on initial and final weight values, offering a streamlined alternative for complex model architectures. We also introduce a novel regularizer that is tailored to enhance the forecasting performance of our approach. Empirical evaluations conducted on synthetic weight sequences and real-world deep learning architectures, including the prominent large language model DistilBERT, demonstrate the superiority of our method in terms of forecasting accuracy and computational efficiency. Notably, our framework showcases improved performance while requiring minimal additional computational overhead, thus presenting a promising avenue for accelerating the training process across diverse tasks and architectures."
      },
      {
        "id": "oai:arXiv.org:2505.02720v1",
        "title": "A Rate-Quality Model for Learned Video Coding",
        "link": "https://arxiv.org/abs/2505.02720",
        "author": "Sang NguyenQuang, Cheng-Wei Chen, Xiem HoangVan, Wen-Hsiao Peng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02720v1 Announce Type: new \nAbstract: Learned video coding (LVC) has recently achieved superior coding performance. In this paper, we model the rate-quality (R-Q) relationship for learned video coding by a parametric function. We learn a neural network, termed RQNet, to characterize the relationship between the bitrate and quality level according to video content and coding context. The predicted (R,Q) results are further integrated with those from previously coded frames using the least-squares method to determine the parameters of our R-Q model on-the-fly. Compared to the conventional approaches, our method accurately estimates the R-Q relationship, enabling the online adaptation of model parameters to enhance both flexibility and precision. Experimental results show that our R-Q model achieves significantly smaller bitrate deviations than the baseline method on commonly used datasets with minimal additional complexity."
      },
      {
        "id": "oai:arXiv.org:2505.02737v1",
        "title": "Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation",
        "link": "https://arxiv.org/abs/2505.02737",
        "author": "Pons Gerard, Bilalli Besim, Queralt Anna",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02737v1 Announce Type: new \nAbstract: Recent advances in Large Language Models (LLMs) have positioned them as a prominent solution for Natural Language Processing tasks. Notably, they can approach these problems in a zero or few-shot manner, thereby eliminating the need for training or fine-tuning task-specific models. However, LLMs face some challenges, including hallucination and the presence of outdated knowledge or missing information from specific domains in the training data. These problems cannot be easily solved by retraining the models with new data as it is a time-consuming and expensive process. To mitigate these issues, Knowledge Graphs (KGs) have been proposed as a structured external source of information to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for zero-shot Entity Disambiguation (ED). For that purpose, we leverage the hierarchical representation of the entities' classes in a KG to gradually prune the candidate space as well as the entities' descriptions to enrich the input prompt with additional factual knowledge. Our evaluation on popular ED datasets shows that the proposed method outperforms non-enhanced and description-only enhanced LLMs, and has a higher degree of adaptability than task-specific models. Furthermore, we conduct an error analysis and discuss the impact of the leveraged KG's semantic expressivity on the ED performance."
      },
      {
        "id": "oai:arXiv.org:2505.02741v1",
        "title": "dyGRASS: Dynamic Spectral Graph Sparsification via Localized Random Walks on GPUs",
        "link": "https://arxiv.org/abs/2505.02741",
        "author": "Yihang Yuan, Ali Aghdaei, Zhuo Feng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02741v1 Announce Type: new \nAbstract: This work presents dyGRASS, an efficient dynamic algorithm for spectral sparsification of large undirected graphs that undergo streaming edge insertions and deletions. At its core, dyGRASS employs a random-walk-based method to efficiently estimate node-to-node distances in both the original graph (for decremental update) and its sparsifier (for incremental update). For incremental updates, dyGRASS enables the identification of spectrally critical edges among the updates to capture the latest structural changes. For decremental updates, dyGRASS facilitates the recovery of important edges from the original graph back into the sparsifier. To further enhance computational efficiency, dyGRASS employs a GPU-based non-backtracking random walk scheme that allows multiple walkers to operate simultaneously across various target updates. This parallelization significantly improves both the performance and scalability of the proposed dyGRASS framework. Our comprehensive experimental evaluations reveal that dyGRASS achieves approximately a 10x speedup compared to the state-of-the-art incremental sparsification (inGRASS) algorithm while eliminating the setup overhead and improving solution quality in incremental spectral sparsification tasks. Moreover, dyGRASS delivers high efficiency and superior solution quality for fully dynamic graph sparsification, accommodating both edge insertions and deletions across a diverse range of graph instances originating from integrated circuit simulations, finite element analysis, and social networks."
      },
      {
        "id": "oai:arXiv.org:2505.02743v1",
        "title": "Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties",
        "link": "https://arxiv.org/abs/2505.02743",
        "author": "Jiaxiang Yi, Miguel A. Bessa",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02743v1 Announce Type: new \nAbstract: Real-world data contains aleatoric uncertainty - irreducible noise arising from imperfect measurements or from incomplete knowledge about the data generation process. Mean variance estimation (MVE) networks can learn this type of uncertainty but require ad-hoc regularization strategies to avoid overfitting and are unable to predict epistemic uncertainty (model uncertainty). Conversely, Bayesian neural networks predict epistemic uncertainty but are notoriously difficult to train due to the approximate nature of Bayesian inference. We propose to cooperatively train a variance network with a Bayesian neural network and demonstrate that the resulting model disentangles aleatoric and epistemic uncertainties while improving the mean estimation. We demonstrate the effectiveness and scalability of this method across a diverse range of datasets, including a time-dependent heteroscedastic regression dataset we created where the aleatoric uncertainty is known. The proposed method is straightforward to implement, robust, and adaptable to various model architectures."
      },
      {
        "id": "oai:arXiv.org:2505.02746v1",
        "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training",
        "link": "https://arxiv.org/abs/2505.02746",
        "author": "Simon Ging, Sebastian Walter, Jelena Bratuli\\'c, Johannes Dienert, Hannah Bast, Thomas Brox",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02746v1 Announce Type: new \nAbstract: Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time."
      },
      {
        "id": "oai:arXiv.org:2505.02751v1",
        "title": "Platelet enumeration in dense aggregates",
        "link": "https://arxiv.org/abs/2505.02751",
        "author": "H. Martin Gillis, Yogeshwar Shendye, Paul Hollensen, Alan Fine, Thomas Trappenberg",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02751v1 Announce Type: new \nAbstract: Identifying and counting blood components such as red blood cells, various types of white blood cells, and platelets is a critical task for healthcare practitioners. Deep learning approaches, particularly convolutional neural networks (CNNs) using supervised learning strategies, have shown considerable success for such tasks. However, CNN based architectures such as U-Net, often struggles to accurately identify platelets due to their sizes and high variability of features. To address these challenges, researchers have commonly employed strategies such as class weighted loss functions, which have demonstrated some success. However, this does not address the more significant challenge of platelet variability in size and tendency to form aggregates and associations with other blood components. In this study, we explored an alternative approach by investigating the role of convolutional kernels in mitigating these issues. We also assigned separate classes to singular platelets and platelet aggregates and performed semantic segmentation using various U-Net architectures for identifying platelets. We then evaluated and compared two common methods (pixel area method and connected component analysis) for counting platelets and proposed an alternative approach specialized for single platelets and platelet aggregates. Our experiments provided results that showed significant improvements in the identification of platelets, highlighting the importance of optimizing convolutional operations and class designations. We show that the common practice of pixel area-based counting often over estimate platelet counts, whereas the proposed method presented in this work offers significant improvements. We discuss in detail about these methods from segmentation masks."
      },
      {
        "id": "oai:arXiv.org:2505.02753v1",
        "title": "Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models",
        "link": "https://arxiv.org/abs/2505.02753",
        "author": "Yankai Jiang, Peng Zhang, Donglin Yang, Yuan Tian, Hai Lin, Xiaosong Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02753v1 Announce Type: new \nAbstract: We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at https://github.com/Yankai96/DiffuGTS."
      },
      {
        "id": "oai:arXiv.org:2505.02763v1",
        "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models",
        "link": "https://arxiv.org/abs/2505.02763",
        "author": "Matthew Dahl",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02763v1 Announce Type: new \nAbstract: Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount."
      },
      {
        "id": "oai:arXiv.org:2505.02779v1",
        "title": "Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance",
        "link": "https://arxiv.org/abs/2505.02779",
        "author": "David Rivas-Villar, \\'Alvaro S. Hervella, Jos\\'e Rouco, Jorge Novo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02779v1 Announce Type: new \nAbstract: Retinal image registration, particularly for color fundus images, is a challenging yet essential task with diverse clinical applications. Existing registration methods for color fundus images typically rely on keypoints and descriptors for alignment; however, a significant limitation is their reliance on labeled data, which is particularly scarce in the medical domain.\n  In this work, we present a novel unsupervised registration pipeline that entirely eliminates the need for labeled data. Our approach is based on the principle that locations with distinctive descriptors constitute reliable keypoints. This fully inverts the conventional state-of-the-art approach, conditioning the detector on the descriptor rather than the opposite.\n  First, we propose an innovative descriptor learning method that operates without keypoint detection or any labels, generating descriptors for arbitrary locations in retinal images. Next, we introduce a novel, label-free keypoint detector network which works by estimating descriptor performance directly from the input image.\n  We validate our method through a comprehensive evaluation on four hold-out datasets, demonstrating that our unsupervised descriptor outperforms state-of-the-art supervised descriptors and that our unsupervised detector significantly outperforms existing unsupervised detection methods. Finally, our full registration pipeline achieves performance comparable to the leading supervised methods, while not employing any labeled data. Additionally, the label-free nature and design of our method enable direct adaptation to other domains and modalities."
      },
      {
        "id": "oai:arXiv.org:2505.02784v1",
        "title": "Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge",
        "link": "https://arxiv.org/abs/2505.02784",
        "author": "Vladyslav Zalevskyi, Thomas Sanchez, Misha Kaandorp, Margaux Roulet, Diego Fajardo-Rojas, Liu Li, Jana Hutter, Hongwei Bran Li, Matthew Barkovich, Hui Ji, Luca Wilhelmi, Aline D\\\"andliker, C\\'eline Steger, M\\'eriam Koob, Yvan Gomez, Anton Jakov\\v{c}i\\'c, Melita Klai\\'c, Ana Ad\\v{z}i\\'c, Pavel Markovi\\'c, Gracia Grabari\\'c, Milan Rados, Jordina Aviles Verdera, Gregor Kasprian, Gregor Dovjak, Raphael Gaubert-Rachm\\\"uhl, Maurice Aschwanden, Qi Zeng, Davood Karimi, Denis Peruzzo, Tommaso Ciceri, Giorgio Longari, Rachika E. Hamadache, Amina Bouzid, Xavier Llad\\'o, Simone Chiarella, Gerard Mart\\'i-Juan, Miguel \\'Angel Gonz\\'alez Ballester, Marco Castellaro, Marco Pinamonti, Valentina Visani, Robin Cremese, Ke\\\"in Sam, Fleur Gaudfernau, Param Ahir, Mehul Parikh, Maximilian Zenk, Michael Baumgartner, Klaus Maier-Hein, Li Tianhong, Yang Hong, Zhao Longfei, Domen Preloznik, \\v{Z}iga \\v{S}piclin, Jae Won Choi, Muyang Li, Jia Fu, Guotai Wang, Jingwen Jiang, Lyuyang Tong, Bo Du, Andrea Gondova, Sungmin You, Kiho Im, Abdul Qayyum, Moona Mazher, Steven A Niederer, Maya Yanko, Bella Specktor-Fadida, Dafna Ben Bashat, Andras Jakab, Roxane Licandro, Kelly Payette, Meritxell Bach Cuadra",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02784v1 Announce Type: new \nAbstract: Accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. The FeTA Challenge 2024 advanced automated fetal brain MRI analysis by introducing biometry prediction as a new task alongside tissue segmentation. For the first time, our diverse multi-centric test set included data from a new low-field (0.55T) MRI dataset. Evaluation metrics were also expanded to include the topology-specific Euler characteristic difference (ED). Sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. However, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. The ED metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. Seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. Domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. Other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. Overall, FeTA 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain MRI, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable AI tools."
      },
      {
        "id": "oai:arXiv.org:2505.02787v1",
        "title": "Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration",
        "link": "https://arxiv.org/abs/2505.02787",
        "author": "David Rivas-Villar, \\'Alvaro S. Hervella, Jos\\'e Rouco, Jorge Novo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02787v1 Announce Type: new \nAbstract: Current color fundus image registration approaches are limited, among other things, by the lack of labeled data, which is even more significant in the medical domain, motivating the use of unsupervised learning. Therefore, in this work, we develop a novel unsupervised descriptor learning method that does not rely on keypoint detection. This enables the resulting descriptor network to be agnostic to the keypoint detector used during the registration inference.\n  To validate this approach, we perform an extensive and comprehensive comparison on the reference public retinal image registration dataset. Additionally, we test our method with multiple keypoint detectors of varied nature, even proposing some novel ones. Our results demonstrate that the proposed approach offers accurate registration, not incurring in any performance loss versus supervised methods. Additionally, it demonstrates accurate performance regardless of the keypoint detector used. Thus, this work represents a notable step towards leveraging unsupervised learning in the medical domain."
      },
      {
        "id": "oai:arXiv.org:2505.02795v1",
        "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models",
        "link": "https://arxiv.org/abs/2505.02795",
        "author": "Zheng Lin, Yuxin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Praneeth Vepakomma, Wei Ni, Jun Luo, Yue Gao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02795v1 Announce Type: new \nAbstract: Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed."
      },
      {
        "id": "oai:arXiv.org:2505.02797v1",
        "title": "DPNet: Dynamic Pooling Network for Tiny Object Detection",
        "link": "https://arxiv.org/abs/2505.02797",
        "author": "Luqi Gong, Haotian Chen, Yikun Chen, Tianliang Yao, Chao Li, Shuai Zhao, Guangjie Han",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02797v1 Announce Type: new \nAbstract: In unmanned aerial systems, especially in complex environments, accurately detecting tiny objects is crucial. Resizing images is a common strategy to improve detection accuracy, particularly for small objects. However, simply enlarging images significantly increases computational costs and the number of negative samples, severely degrading detection performance and limiting its applicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny object detection to mitigate these issues. DPNet employs a flexible down-sampling strategy by introducing a factor (df) to relax the fixed downsampling process of the feature map to an adjustable one. Furthermore, we design a lightweight predictor to predict df for each input image, which is used to decrease the resolution of feature maps in the backbone. Thus, we achieve input-aware downsampling. We also design an Adaptive Normalization Module (ANM) to make a unified detector compatible with different dfs. A guidance loss supervises the predictor's training. DPNet dynamically allocates computing resources to trade off between detection accuracy and efficiency. Experiments on the TinyCOCO and TinyPerson datasets show that DPNet can save over 35% and 25% GFLOPs, respectively, while maintaining comparable detection performance. The code will be made publicly available."
      },
      {
        "id": "oai:arXiv.org:2505.02809v1",
        "title": "Towards Quantifying the Hessian Structure of Neural Networks",
        "link": "https://arxiv.org/abs/2505.02809",
        "author": "Zhaorui Dong, Yushun Zhang, Zhi-Quan Luo, Jianfeng Yao, Ruoyu Sun",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02809v1 Announce Type: new \nAbstract: Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal two forces that shape the Hessian structure: a ``static force'' rooted in the architecture design, and a ``dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ``static force'' at random initialization. We study linear models and 1-hidden-layer networks with the mean-square (MSE) loss and the Cross-Entropy (CE) loss for classification tasks. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C \\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings reveal that $C$ is a primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$."
      },
      {
        "id": "oai:arXiv.org:2505.02815v1",
        "title": "Database-Agnostic Gait Enrollment using SetTransformers",
        "link": "https://arxiv.org/abs/2505.02815",
        "author": "Nicoleta Basoc, Adrian Cosma, Andy C\\v{a}trun\\v{a}, Emilian R\\v{a}doi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02815v1 Announce Type: new \nAbstract: Gait recognition has emerged as a powerful tool for unobtrusive and long-range identity analysis, with growing relevance in surveillance and monitoring applications. Although recent advances in deep learning and large-scale datasets have enabled highly accurate recognition under closed-set conditions, real-world deployment demands open-set gait enrollment, which means determining whether a new gait sample corresponds to a known identity or represents a previously unseen individual. In this work, we introduce a transformer-based framework for open-set gait enrollment that is both dataset-agnostic and recognition-architecture-agnostic. Our method leverages a SetTransformer to make enrollment decisions based on the embedding of a probe sample and a context set drawn from the gallery, without requiring task-specific thresholds or retraining for new environments. By decoupling enrollment from the main recognition pipeline, our model is generalized across different datasets, gallery sizes, and identity distributions. We propose an evaluation protocol that uses existing datasets in different ratios of identities and walks per identity. We instantiate our method using skeleton-based gait representations and evaluate it on two benchmark datasets (CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition models (GaitGraph, GaitFormer, and GaitPT). We show that our method is flexible, is able to accurately perform enrollment in different scenarios, and scales better with data compared to traditional approaches. We will make the code and dataset scenarios publicly available."
      },
      {
        "id": "oai:arXiv.org:2505.02819v1",
        "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations",
        "link": "https://arxiv.org/abs/2505.02819",
        "author": "Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02819v1 Announce Type: new \nAbstract: We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository."
      },
      {
        "id": "oai:arXiv.org:2505.02823v1",
        "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing",
        "link": "https://arxiv.org/abs/2505.02823",
        "author": "Zinan Guo, Pengze Zhang, Yanze Wu, Chong Mou, Songtao Zhao, Qian He",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02823v1 Announce Type: new \nAbstract: Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset."
      },
      {
        "id": "oai:arXiv.org:2505.02824v1",
        "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
        "link": "https://arxiv.org/abs/2505.02824",
        "author": "Kuofeng Gao, Yufei Zhu, Yiming Li, Jiawang Bai, Yong Yang, Zhifeng Li, Shu-Tao Xia",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02824v1 Announce Type: new \nAbstract: Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance."
      },
      {
        "id": "oai:arXiv.org:2505.02825v1",
        "title": "Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology",
        "link": "https://arxiv.org/abs/2505.02825",
        "author": "Alex Hoi Hang Chan, Otto Brookes, Urs Waldmann, Hemal Naik, Iain D. Couzin, Majid Mirmehdi, No\\\"el Adiko Houa, Emmanuelle Normand, Christophe Boesch, Lukas Boesch, Mimi Arandjelovic, Hjalmar K\\\"uhl, Tilo Burghardt, Fumihiro Kano",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02825v1 Announce Type: new \nAbstract: Computer vision methods have demonstrated considerable potential to streamline ecological and biological workflows, with a growing number of datasets and models becoming available to the research community. However, these resources focus predominantly on evaluation using machine learning metrics, with relatively little emphasis on how their application impacts downstream analysis. We argue that models should be evaluated using application-specific metrics that directly represent model performance in the context of its final use case. To support this argument, we present two disparate case studies: (1) estimating chimpanzee abundance and density with camera trap distance sampling when using a video-based behaviour classifier and (2) estimating head rotation in pigeons using a 3D posture estimator. We show that even models with strong machine learning performance (e.g., 87% mAP) can yield data that leads to discrepancies in abundance estimates compared to expert-derived data. Similarly, the highest-performing models for posture estimation do not produce the most accurate inferences of gaze direction in pigeons. Motivated by these findings, we call for researchers to integrate application-specific metrics in ecological/biological datasets, allowing for models to be benchmarked in the context of their downstream application and to facilitate better integration of models into application workflows."
      },
      {
        "id": "oai:arXiv.org:2505.02830v1",
        "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation",
        "link": "https://arxiv.org/abs/2505.02830",
        "author": "Qingqiu Li, Zihang Cui, Seongsu Bae, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Junjun He, Shujun Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02830v1 Announce Type: new \nAbstract: Chest X-rays (CXRs) are the most frequently performed imaging examinations in clinical settings. Recent advancements in Large Multimodal Models (LMMs) have enabled automated CXR interpretation, enhancing diagnostic accuracy and efficiency. However, despite their strong visual understanding, current Medical LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level understanding and interaction, and (2) Limited accuracy and interpretability due to single-step reasoning. In this paper, we empower MLMMs with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. Specifically, we first propose an Anatomical Ontology-Guided Reasoning (AOR) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. Next, under the guidance of expert physicians, we develop AOR-Instruction, a large instruction dataset for MLMMs training. Our experiments demonstrate AOR's superior performance in both VQA and report generation tasks."
      },
      {
        "id": "oai:arXiv.org:2505.02831v1",
        "title": "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves",
        "link": "https://arxiv.org/abs/2505.02831",
        "author": "Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02831v1 Announce Type: new \nAbstract: Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance generation quality of the diffusion transformers. However, existing approaches necessitate to either introduce an additional and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation A}lignment (SRA), a simple yet straightforward method that obtain representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in earlier layer with higher noise to that in later layer with lower noise to progressively enhance the overall representation learning during only generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that heavily dependent on powerful external representation priors."
      },
      {
        "id": "oai:arXiv.org:2505.02835v1",
        "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.02835",
        "author": "Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, Liang Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02835v1 Announce Type: new \nAbstract: Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs."
      },
      {
        "id": "oai:arXiv.org:2505.02836v1",
        "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation",
        "link": "https://arxiv.org/abs/2505.02836",
        "author": "Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, Zhaoshuo Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02836v1 Announce Type: new \nAbstract: Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research."
      },
      {
        "id": "oai:arXiv.org:2505.01433v1",
        "title": "Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations",
        "link": "https://arxiv.org/abs/2505.01433",
        "author": "Cong Qi, Hanzhang Fang, Siqi jiang, Tianxing Hu, Wei Zhi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01433v1 Announce Type: cross \nAbstract: Understanding the binding specificity between T-cell receptors (TCRs) and peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy and vaccine development. However, current predictive models struggle with generalization, especially in data-scarce settings and when faced with novel epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced Recognition Network), a deep learning framework that combines large-scale protein language models with chemical representations of peptides. By encoding TCR \\b{eta}-chain sequences using ESM-1b and transforming peptide sequences into SMILES strings processed by MolFormer, LANTERN captures rich biological and chemical features critical for TCR-peptide recognition. Through extensive benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR, LANTERN demonstrates superior performance, particularly in zero-shot and few-shot learning scenarios. Our model also benefits from a robust negative sampling strategy and shows significant clustering improvements via embedding analysis. These results highlight the potential of LANTERN to advance TCR-pMHC binding prediction and support the development of personalized immunotherapies."
      },
      {
        "id": "oai:arXiv.org:2505.01435v1",
        "title": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine",
        "link": "https://arxiv.org/abs/2505.01435",
        "author": "Carlo Siebenschuh, Kyle Hippe, Ozan Gokdemir, Alexander Brace, Arham Khan, Khalid Hossain, Yadu Babuji, Nicholas Chia, Venkatram Vishwanath, Rick Stevens, Arvind Ramanathan, Ian Foster, Robert Underwood",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01435v1 Announce Type: cross \nAbstract: Language models for scientific tasks are trained on text from scientific publications, most distributed as PDFs that require parsing. PDF parsing approaches range from inexpensive heuristics (for simple documents) to computationally intensive ML-driven systems (for complex or degraded ones). The choice of the \"best\" parser for a particular document depends on its computational cost and the accuracy of its output. To address these issues, we introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine (AdaParse), a data-driven strategy for assigning an appropriate parser to each document. We enlist scientists to select preferred parser outputs and incorporate this information through direct preference optimization (DPO) into AdaParse, thereby aligning its selection process with human judgment. AdaParse then incorporates hardware requirements and predicted accuracy of each parser to orchestrate computational resources efficiently for large-scale parsing campaigns. We demonstrate that AdaParse, when compared to state-of-the-art parsers, improves throughput by $17\\times$ while still achieving comparable accuracy (0.2 percent better) on a benchmark set of 1000 scientific documents. AdaParse's combination of high accuracy and parallel scalability makes it feasible to parse large-scale scientific document corpora to support the development of high-quality, trillion-token-scale text datasets. The implementation is available at https://github.com/7shoe/AdaParse/"
      },
      {
        "id": "oai:arXiv.org:2505.01454v1",
        "title": "Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning",
        "link": "https://arxiv.org/abs/2505.01454",
        "author": "Zhiyong Jin, Runhua Xu, Chao Li, Yizhong Liu, Jianxin Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01454v1 Announce Type: cross \nAbstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet it faces significant challenges in communication efficiency and vulnerability to poisoning attacks. While sparsification techniques mitigate communication overhead by transmitting only critical model parameters, they inadvertently amplify security risks: adversarial clients can exploit sparse updates to evade detection and degrade model performance. Existing defense mechanisms, designed for standard FL communication scenarios, are ineffective in addressing these vulnerabilities within sparsified FL. To bridge this gap, we propose FLARE, a novel federated learning framework that integrates sparse index mask inspection and model update sign similarity analysis to detect and mitigate poisoning attacks in sparsified FL. Extensive experiments across multiple datasets and adversarial scenarios demonstrate that FLARE significantly outperforms existing defense strategies, effectively securing sparsified FL against poisoning attacks while maintaining communication efficiency."
      },
      {
        "id": "oai:arXiv.org:2505.01455v1",
        "title": "Seasonal Prediction with Neural GCM and Simplified Boundary Forcings: Large-scale Atmospheric Variability and Tropical Cyclone Activity",
        "link": "https://arxiv.org/abs/2505.01455",
        "author": "Gan Zhang, Megha Rao, Janni Yuval, Ming Zhao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01455v1 Announce Type: cross \nAbstract: Machine learning (ML) models are successful with weather forecasting and have shown progress in climate simulations, yet leveraging them for useful climate predictions needs exploration. Here we show this feasibility using NeuralGCM, a hybrid ML-physics atmospheric model, for seasonal predictions of large-scale atmospheric variability and Northern Hemisphere tropical cyclone (TC) activity. Inspired by physical model studies, we simplify boundary conditions, assuming sea surface temperature (SST) and sea ice follow their climatological cycle but persist anomalies present at initialization. With such forcings, NeuralGCM simulates realistic atmospheric circulation and TC climatology patterns. Furthermore, this configuration yields useful seasonal predictions (July-November) for the tropical atmosphere and various TC activity metrics. Notably, the prediction skill for TC frequency in the North Atlantic and East Pacific basins is comparable to existing physical models. These findings highlight the promise of leveraging ML models with physical insights to model TC risks and deliver seamless weather-climate predictions."
      },
      {
        "id": "oai:arXiv.org:2505.01457v1",
        "title": "A Multi-Granularity Multimodal Retrieval Framework for Multimodal Document Tasks",
        "link": "https://arxiv.org/abs/2505.01457",
        "author": "Mingjun Xu, Zehui Wang, Hengxing Cai, Renxin Zhong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01457v1 Announce Type: cross \nAbstract: Retrieval-augmented generation (RAG) systems have predominantly focused on text-based retrieval, limiting their effectiveness in handling visually-rich documents that encompass text, images, tables, and charts. To bridge this gap, we propose a unified multi-granularity multimodal retrieval framework tailored for two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical encoding strategies, modality-aware retrieval mechanisms, and reranking modules to effectively capture and utilize the complex interdependencies between textual and visual modalities. By leveraging off-the-shelf vision-language models and implementing a training-free hybridretrieval strategy, our framework demonstrates robust performance without the need for task-specific fine-tuning. Experimental evaluations reveal that incorporating layout-aware search and reranking modules significantly enhances retrieval accuracy, achieving a top performance score of 65.56. This work underscores the potential of scalable and reproducible solutions in advancing multimodal document retrieval systems."
      },
      {
        "id": "oai:arXiv.org:2505.01460v1",
        "title": "Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services",
        "link": "https://arxiv.org/abs/2505.01460",
        "author": "Denis Parfenov, Anton Parfenov",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01460v1 Announce Type: cross \nAbstract: Due to the increasing number of tasks that are solved on remote servers, identifying and classifying traffic is an important task to reduce the load on the server. There are various methods for classifying traffic. This paper discusses machine learning models for solving this problem. However, such ML models are also subject to attacks that affect the classification result of network traffic. To protect models, we proposed a solution based on an autoencoder"
      },
      {
        "id": "oai:arXiv.org:2505.01463v1",
        "title": "Enhancing the Cloud Security through Topic Modelling",
        "link": "https://arxiv.org/abs/2505.01463",
        "author": "Sabbir M. Saleh, Nazim Madhavji, John Steinbacher",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01463v1 Announce Type: cross \nAbstract: Protecting cloud applications is crucial in an age where security constantly threatens the digital world. The inevitable cyber-attacks throughout the CI/CD pipeline make cloud security innovations necessary. This research is motivated by applying Natural Language Processing (NLP) methodologies, such as Topic Modelling, to analyse cloud security data and predict future attacks. This research aims to use topic modelling, specifically Latent Dirichlet Allocation (LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and PLSA, security-related text data, such as reports, logs, and other relevant documents, will be analysed and sorted into relevant topics (such as phishing or encryption). These algorithms may apply through Python using the Gensim framework. The topics shall be utilised to detect vulnerabilities within relevant CI/CD pipeline records or log data. This application of Topic Modelling anticipates providing a new form of vulnerability detection, improving overall security throughout the CI/CD pipeline."
      },
      {
        "id": "oai:arXiv.org:2505.01476v1",
        "title": "CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering",
        "link": "https://arxiv.org/abs/2505.01476",
        "author": "Zhe Zhang, Mingxiu Cai, Hanxiao Wang, Gaochang Wu, Tianyou Chai, Xiatian Zhu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01476v1 Announce Type: cross \nAbstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an input image with respect to normal samples. Either by reconstructing normal counterparts (reconstruction-based) or by learning an image feature embedding space (embedding-based), existing approaches fundamentally rely on image-level or feature-level matching to derive anomaly scores. Often, such a matching process is inaccurate yet overlooked, leading to sub-optimal detection. To address this issue, we introduce the concept of cost filtering, borrowed from classical matching tasks, such as depth and flow estimation, into the UAD problem. We call this approach {\\em CostFilter-AD}. Specifically, we first construct a matching cost volume between the input and normal samples, comprising two spatial dimensions and one matching dimension that encodes potential matches. To refine this, we propose a cost volume filtering network, guided by the input observation as an attention query across multiple feature layers, which effectively suppresses matching noise while preserving edge structures and capturing subtle anomalies. Designed as a generic post-processing plug-in, CostFilter-AD can be integrated with either reconstruction-based or embedding-based methods. Extensive experiments on MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for both single- and multi-class UAD tasks. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD."
      },
      {
        "id": "oai:arXiv.org:2505.01484v1",
        "title": "LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps",
        "link": "https://arxiv.org/abs/2505.01484",
        "author": "Pedro Abdalla, Roman Vershynin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01484v1 Announce Type: cross \nAbstract: Given a text, can we determine whether it was generated by a large language model (LLM) or by a human? A widely studied approach to this problem is watermarking. We propose an undetectable and elementary watermarking scheme in the closed setting. Also, in the harder open setting, where the adversary has access to most of the model, we propose an unremovable watermarking scheme."
      },
      {
        "id": "oai:arXiv.org:2505.01485v1",
        "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code",
        "link": "https://arxiv.org/abs/2505.01485",
        "author": "Tasnim Ahmed, Salimur Choudhury",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01485v1 Announce Type: cross \nAbstract: Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning."
      },
      {
        "id": "oai:arXiv.org:2505.01524v1",
        "title": "The DCR Delusion: Measuring the Privacy Risk of Synthetic Data",
        "link": "https://arxiv.org/abs/2505.01524",
        "author": "Zexi Yao, Nata\\v{s}a Kr\\v{c}o, Georgi Ganev, Yves-Alexandre de Montjoye",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01524v1 Announce Type: cross \nAbstract: Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous."
      },
      {
        "id": "oai:arXiv.org:2505.01538v1",
        "title": "HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning",
        "link": "https://arxiv.org/abs/2505.01538",
        "author": "Hongbin Zhong, Matthew Lentz, Nina Narodytska, Adriana Szekeres, Kexin Rong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01538v1 Announce Type: cross \nAbstract: As vector databases gain traction in enterprise applications, robust access control has become critical to safeguard sensitive data. Access control in these systems is often implemented through hybrid vector queries, which combine nearest neighbor search on vector data with relational predicates based on user permissions. However, existing approaches face significant trade-offs: creating dedicated indexes for each user minimizes query latency but introduces excessive storage redundancy, while building a single index and applying access control after vector search reduces storage overhead but suffers from poor recall and increased query latency. This paper introduces HoneyBee, a dynamic partitioning framework that bridges the gap between these approaches by leveraging the structure of Role-Based Access Control (RBAC) policies. RBAC, widely adopted in enterprise settings, groups users into roles and assigns permissions to those roles, creating a natural \"thin waist\" in the permission structure that is ideal for partitioning decisions. Specifically, HoneyBee produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling storage overhead. By introducing analytical models for the performance and recall of the vector search, HoneyBee formulates the partitioning strategy as a constrained optimization problem to dynamically balance storage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee reduces storage redundancy compared to role partitioning and achieves up to 6x faster query speeds than row-level security (RLS) with only 1.4x storage increase, offering a practical middle ground for secure and efficient vector search."
      },
      {
        "id": "oai:arXiv.org:2505.01539v1",
        "title": "Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models",
        "link": "https://arxiv.org/abs/2505.01539",
        "author": "Cor Steging, Silja Renooij, Bart Verheij",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01539v1 Announce Type: cross \nAbstract: Generative large language models as tools in the legal domain have the potential to improve the justice system. However, the reasoning behavior of current generative models is brittle and poorly understood, hence cannot be responsibly applied in the domains of law and evidence. In this paper, we introduce an approach for creating benchmarks that can be used to evaluate the reasoning capabilities of generative language models. These benchmarks are dynamically varied, scalable in their complexity, and have formally unambiguous interpretations. In this study, we illustrate the approach on the basis of witness testimony, focusing on the underlying argument attack structure. We dynamically generate both linear and non-linear argument attack graphs of varying complexity and translate these into reasoning puzzles about witness testimony expressed in natural language. We show that state-of-the-art large language models often fail in these reasoning puzzles, already at low complexity. Obvious mistakes are made by the models, and their inconsistent performance indicates that their reasoning capabilities are brittle. Furthermore, at higher complexity, even state-of-the-art models specifically presented for reasoning capabilities make mistakes. We show the viability of using a parametrized benchmark with varying complexity to evaluate the reasoning capabilities of generative language models. As such, the findings contribute to a better understanding of the limitations of the reasoning capabilities of generative models, which is essential when designing responsible AI systems in the legal domain."
      },
      {
        "id": "oai:arXiv.org:2505.01601v1",
        "title": "Beyond Productivity: Rethinking the Impact of Creativity Support Tools",
        "link": "https://arxiv.org/abs/2505.01601",
        "author": "Samuel Rhys Cox, Helena B{\\o}jer Djern{\\ae}s, Niels van Berkel",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01601v1 Announce Type: cross \nAbstract: Creativity Support Tools (CSTs) are widely used across diverse creative domains, with generative AI recently increasing the abilities of CSTs. To better understand how the success of CSTs is determined in the literature, we conducted a review of outcome measures used in CST evaluations. Drawing from (n=173) CST evaluations in the ACM Digital Library, we identified the metrics commonly employed to assess user interactions with CSTs. Our findings reveal prevailing trends in current evaluation practices, while exposing underexplored measures that could broaden the scope of future research. Based on these results, we argue for a more holistic approach to evaluating CSTs, encouraging the HCI community to consider not only user experience and the quality of the generated output, but also user-centric aspects such as self-reflection and well-being as critical dimensions of assessment. We also highlight a need for validated measures specifically suited to the evaluation of generative AI in CSTs."
      },
      {
        "id": "oai:arXiv.org:2505.01616v1",
        "title": "Phantora: Live GPU Cluster Simulation for Machine Learning System Performance Estimation",
        "link": "https://arxiv.org/abs/2505.01616",
        "author": "Jianxing Qin, Jingrong Chen, Xinhao Kong, Yongji Wu, Liang Luo, Zhaodong Wang, Ying Zhang, Tingjun Chen, Alvin R. Lebeck, Danyang Zhuo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01616v1 Announce Type: cross \nAbstract: To accommodate ever-increasing model complexity, modern machine learning (ML) systems have to scale to large GPU clusters. Changes in ML model architecture, ML system implementation, and cluster configuration can significantly affect overall ML system performance. However, quantifying the performance impact before deployment is challenging. Existing performance estimation methods use performance modeling or static workload simulation. These techniques are not general: they requires significant human effort and computation capacity to generate training data or a workload. It is also difficult to adapt ML systems to use these techniques. This paper introduces, Phantora, a live GPU cluster simulator for performance estimation. Phantora runs minimally modified ML models and frameworks, intercepting and simulating GPU-related operations to enable high-fidelity performance estimation. Phantora overcomes several research challenges in integrating an event-driven network simulator with live system execution, and introduces a set of techniques to improve simulation speed, scalability, and accuracy. Our evaluation results show that Phantora can deliver similar estimation accuracy to the state-of-the-art workload simulation approach with only one GPU, while reducing human effort and increasing generalizability."
      },
      {
        "id": "oai:arXiv.org:2505.01636v1",
        "title": "Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation",
        "link": "https://arxiv.org/abs/2505.01636",
        "author": "Amit Rath",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01636v1 Announce Type: cross \nAbstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and task generalization. However, their application to structured data analysis remains fragile due to inconsistencies in schema interpretation, misalignment between user intent and model output, and limited mechanisms for self-correction when failures occur. This paper introduces the STROT Framework (Structured Task Reasoning and Output Transformation), a method for structured prompting and feedback-driven transformation logic generation aimed at improving the reliability and semantic alignment of LLM-based analytical workflows. STROT begins with lightweight schema introspection and sample-based field classification, enabling dynamic context construction that captures both the structure and statistical profile of the input data. This contextual information is embedded in structured prompts that guide the model toward generating task-specific, interpretable outputs. To address common failure modes in complex queries, STROT incorporates a refinement mechanism in which the model iteratively revises its outputs based on execution feedback and validation signals. Unlike conventional approaches that rely on static prompts or single-shot inference, STROT treats the LLM as a reasoning agent embedded within a controlled analysis loop -- capable of adjusting its output trajectory through planning and correction. The result is a robust and reproducible framework for reasoning over structured data with LLMs, applicable to diverse data exploration and analysis tasks where interpretability, stability, and correctness are essential."
      },
      {
        "id": "oai:arXiv.org:2505.01637v1",
        "title": "Morello: Compiling Fast Neural Networks with Dynamic Programming and Spatial Compression",
        "link": "https://arxiv.org/abs/2505.01637",
        "author": "Samuel J. Kaufman, Ren\\'e Just, Rastislav Bodik",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01637v1 Announce Type: cross \nAbstract: High-throughput neural network inference requires coordinating many optimization decisions, including parallel tiling, microkernel selection, and data layout. The product of these decisions forms a search space of programs which is typically intractably large. Existing approaches (e.g., auto-schedulers) often address this problem by sampling this space heuristically. In contrast, we introduce a dynamic-programming-based approach to explore more of the search space by iteratively decomposing large program specifications into smaller specifications reachable from a set of rewrites, then composing a final program from each rewrite that minimizes an affine cost model. To reduce memory requirements, we employ a novel memoization table representation, which indexes specifications by coordinates in $Z_{\\geq 0}$ and compresses identical, adjacent solutions. This approach can visit a much larger set of programs than prior work. To evaluate the approach, we developed Morello, a compiler which lowers specifications roughly equivalent to a few-node XLA computation graph to x86. Notably, we found that an affine cost model is sufficient to surface high-throughput programs. For example, Morello synthesized a collection of matrix multiplication benchmarks targeting a Zen 1 CPU, including a 1x2048x16384, bfloat16-to-float32 vector-matrix multiply, which was integrated into Google's gemma.cpp."
      },
      {
        "id": "oai:arXiv.org:2505.01638v1",
        "title": "Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth",
        "link": "https://arxiv.org/abs/2505.01638",
        "author": "Michael Marinaccio, Fatemeh Afghah",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01638v1 Announce Type: cross \nAbstract: High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs) typically requires multimodal sensing - especially RGB and thermal imagery - which increases hardware cost and power consumption. This paper introduces SAM-TIFF, a novel teacher-student distillation framework for pixel-level wildfire temperature prediction and segmentation using RGB input only. A multimodal teacher network trained on paired RGB-Thermal imagery and radiometric TIFF ground truth distills knowledge to a unimodal RGB student network, enabling thermal-sensor-free inference. Segmentation supervision is generated using a hybrid approach of segment anything (SAM)-guided mask generation, and selection via TOPSIS, along with Canny edge detection and Otsu's thresholding pipeline for automatic point prompt selection. Our method is the first to perform per-pixel temperature regression from RGB UAV data, demonstrating strong generalization on the recent FLAME 3 dataset. This work lays the foundation for lightweight, cost-effective UAV-based wildfire monitoring systems without thermal sensors."
      },
      {
        "id": "oai:arXiv.org:2505.01639v1",
        "title": "Fast Likelihood-Free Parameter Estimation for L\\'evy Processes",
        "link": "https://arxiv.org/abs/2505.01639",
        "author": "Nicolas Coloma, William Kleiber",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01639v1 Announce Type: cross \nAbstract: L\\'evy processes are widely used in financial modeling due to their ability to capture discontinuities and heavy tails, which are common in high-frequency asset return data. However, parameter estimation remains a challenge when associated likelihoods are unavailable or costly to compute. We propose a fast and accurate method for L\\'evy parameter estimation using the neural Bayes estimation (NBE) framework -- a simulation-based, likelihood-free approach that leverages permutation-invariant neural networks to approximate Bayes estimators. Through extensive simulations across several L\\'evy models, we show that NBE outperforms traditional methods in both accuracy and runtime, while also enabling rapid bootstrap-based uncertainty quantification. We illustrate our approach on a challenging high-frequency cryptocurrency return dataset, where the method captures evolving parameter dynamics and delivers reliable and interpretable inference at a fraction of the computational cost of traditional methods. NBE provides a scalable and practical solution for inference in complex financial models, enabling parameter estimation and uncertainty quantification over an entire year of data in just seconds. We additionally investigate nearly a decade of high-frequency Bitcoin returns, requiring less than one minute to estimate parameters under the proposed approach."
      },
      {
        "id": "oai:arXiv.org:2505.01642v1",
        "title": "Identifying Doppelganger Active Galactic Nuclei across redshifts from spectroscopic surveys",
        "link": "https://arxiv.org/abs/2505.01642",
        "author": "Shreya Sareen, Swayamtrupta Panda",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01642v1 Announce Type: cross \nAbstract: Active Galactic Nuclei (AGNs) are among the most luminous objects in the universe, making them valuable probes for studying galaxy evolution. However, understanding how AGN properties evolve over cosmic time remains a fundamental challenge. This study investigates whether AGNs at low redshift (nearby) can serve as proxies for their high-redshift (distant) counterparts by identifying spectral 'doppelg\\\"angers', AGNs with remarkably similar emission line properties despite being separated by vast cosmic distances. We analyze key spectral features of bona fide AGNs using the Sloan Digital Sky Survey's Data Release 16, including continuum and emission lines: Nitrogen (N V), Carbon (C IV), Magnesium (Mg II), Hydrogen-beta (H$\\beta$), and Iron (Fe II - optical and UV) emission lines. We incorporated properties such as equivalent width, velocity dispersion in the form of full width at half maximum (FWHM), and continuum luminosities (135nm, 300nm, and 510nm) closest to these prominent lines. Our initial findings suggest the existence of multiple AGNs with highly similar spectra, hinting at the possibility that local AGNs may indeed share intrinsic properties with high-redshift ones. We showcase here one of the better candidate pairs of AGNs resulting from our analyses."
      },
      {
        "id": "oai:arXiv.org:2505.01644v1",
        "title": "A Dual-Task Synergy-Driven Generalization Framework for Pancreatic Cancer Segmentation in CT Scans",
        "link": "https://arxiv.org/abs/2505.01644",
        "author": "Jun Li, Yijue Zhang, Haibo Shi, Minhong Li, Qiwei Li, Xiaohua Qian",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01644v1 Announce Type: cross \nAbstract: Pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. The generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. Thus, we propose a generalization framework that synergizes pixel-level classification and regression tasks, to accurately delineate lesions and improve model stability. This framework not only seeks to align segmentation contours with actual lesions but also uses regression to elucidate spatial relationships between diseased and normal tissues, thereby improving tumor localization and morphological characterization. Enhanced by the reciprocal transformation of task outputs, our approach integrates additional regression supervision within the segmentation context, bolstering the model's generalization ability from a dual-task perspective. Besides, dual self-supervised learning in feature spaces and output spaces augments the model's representational capability and stability across different imaging views. Experiments on 594 samples composed of three datasets with significant imaging differences demonstrate that our generalized pancreas segmentation results comparable to mainstream in-domain validation performance (Dice: 84.07%). More importantly, it successfully improves the results of the highly challenging cross-lesion generalized pancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a resilient and efficient foundational technological support for pancreatic disease management and wider medical applications. The codes will be released at https://github.com/SJTUBME-QianLab/Dual-Task-Seg."
      },
      {
        "id": "oai:arXiv.org:2505.01651v1",
        "title": "Human-AI Governance (HAIG): A Trust-Utility Approach",
        "link": "https://arxiv.org/abs/2505.01651",
        "author": "Zeynep Engin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01651v1 Announce Type: cross \nAbstract: This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., \"human-in-the-loop\" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge."
      },
      {
        "id": "oai:arXiv.org:2505.01654v1",
        "title": "T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation",
        "link": "https://arxiv.org/abs/2505.01654",
        "author": "Srecharan Selvam, Abhisesh Silwal, George Kantor",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01654v1 Announce Type: cross \nAbstract: T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic system developed for autonomous leaf localization, selection, and grasping in greenhouse environments. The system integrates a 6-degree-of-freedom manipulator with a stereo vision pipeline to identify and interact with target leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo provides dense depth maps, allowing the reconstruction of 3D leaf masks. These observations are processed through a leaf grasping algorithm that selects the optimal leaf based on clutter, visibility, and distance, and determines a grasp point by analyzing local surface flatness, top-down approachability, and margin from edges. The selected grasp point guides a trajectory executed by ROS-based motion controllers, driving a custom microneedle-equipped end-effector to clamp the leaf and simulate tissue sampling. Experiments conducted with artificial plants under varied poses demonstrate that the T-Rex system can consistently detect, plan, and perform physical interactions with plant-like targets, achieving a grasp success rate of 66.6\\%. This paper presents the system architecture, implementation, and testing of T-Rex as a step toward plant sampling automation in Controlled Environment Agriculture (CEA)."
      },
      {
        "id": "oai:arXiv.org:2505.01657v1",
        "title": "RAGAR: Retrieval Augment Personalized Image Generation Guided by Recommendation",
        "link": "https://arxiv.org/abs/2505.01657",
        "author": "Run Ling, Wenji Wang, Yuting Liu, Guibing Guo, Linying Jiang, Xingwei Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01657v1 Announce Type: cross \nAbstract: Personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. Although effective, existing methods face two main issues. First, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. Disproportionately high weights for low-similarity items distort users' visual preferences for the reference item. Second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. To address these issues, we propose Retrieval Augment Personalized Image GenerAtion guided by Recommendation (RAGAR). Our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users' visual preferences for the reference item. Then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. Extensive experiments and human evaluations on three real-world datasets demonstrate that RAGAR achieves significant improvements in both personalization and semantic metrics compared to five baselines."
      },
      {
        "id": "oai:arXiv.org:2505.01670v1",
        "title": "Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations",
        "link": "https://arxiv.org/abs/2505.01670",
        "author": "Christos Zangos, Danish Ebadulla, Thomas Christopher Sprague, Ambuj Singh",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01670v1 Announce Type: cross \nAbstract: This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that the brain signals of the subjects can be aligned in this common space during training to form a semantically aligned common brain. This is leveraged to demonstrate that aligning subject-specific lightweight modules to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios. We evaluate our methods on different datasets, demonstrating that the common space is subject and dataset-agnostic."
      },
      {
        "id": "oai:arXiv.org:2505.01706v1",
        "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm",
        "link": "https://arxiv.org/abs/2505.01706",
        "author": "Sarvesh Shashidhar,  Ritik, Nachiketa Patil, Suraj Racha, Ganesh Ramakrishnan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01706v1 Announce Type: cross \nAbstract: Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even \"good\" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present."
      },
      {
        "id": "oai:arXiv.org:2505.01709v1",
        "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation",
        "link": "https://arxiv.org/abs/2505.01709",
        "author": "Kaidong Zhang, Rongtao Xu, Pengzhen Ren, Junfan Lin, Hefeng Wu, Liang Lin, Xiaodan Liang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01709v1 Announce Type: cross \nAbstract: Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation."
      },
      {
        "id": "oai:arXiv.org:2505.01741v1",
        "title": "CLOG-CD: Curriculum Learning based on Oscillating Granularity of Class Decomposed Medical Image Classification",
        "link": "https://arxiv.org/abs/2505.01741",
        "author": "Asmaa Abbas, Mohamed Gaber, Mohammed M. Abdelsamea",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01741v1 Announce Type: cross \nAbstract: Curriculum learning strategies have been proven to be effective in various applications and have gained significant interest in the field of machine learning. It has the ability to improve the final model's performance and accelerate the training process. However, in the medical imaging domain, data irregularities can make the recognition task more challenging and usually result in misclassification between the different classes in the dataset. Class-decomposition approaches have shown promising results in solving such a problem by learning the boundaries within the classes of the data set. In this paper, we present a novel convolutional neural network (CNN) training method based on the curriculum learning strategy and the class decomposition approach, which we call CLOG-CD, to improve the performance of medical image classification. We evaluated our method on four different imbalanced medical image datasets, such as Chest X-ray (CXR), brain tumour, digital knee X-ray, and histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights from the decomposition granularity of the classes, and the training is accomplished from descending to ascending order (i.e., anti-curriculum technique). We also investigated the classification performance of our proposed method based on different acceleration factors and pace function curricula. We used two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for CLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to improve classification performance with an accuracy of 96.08% for the CXR dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee X-ray, and 99.17% for the CRC dataset, compared to other training strategies. In addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%, and 99.45% for CXR, brain tumour, digital knee X-ray, and CRC datasets, respectively"
      },
      {
        "id": "oai:arXiv.org:2505.01742v1",
        "title": "Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs",
        "link": "https://arxiv.org/abs/2505.01742",
        "author": "Yu Mao, Jingzong Li, Jun Wang, Hong Xu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01742v1 Announce Type: cross \nAbstract: Neural image compression, necessary in various machine-to-machine communication scenarios, suffers from its heavy encode-decode structures and inflexibility in switching between different compression levels. Consequently, it raises significant challenges in applying the neural image compression to edge devices that are developed for powerful servers with high computational and storage capacities. We take a step to solve the challenges by proposing a new transformer-based edge-compute-free image coding framework called Easz. Easz shifts the computational overhead to the server, and hence avoids the heavy encoding and model switching overhead on the edge. Easz utilizes a patch-erase algorithm to selectively remove image contents using a conditional uniform-based sampler. The erased pixels are reconstructed on the receiver side through a transformer-based framework. To further reduce the computational overhead on the receiver, we then introduce a lightweight transformer-based reconstruction structure to reduce the reconstruction load on the receiver side. Extensive evaluations conducted on a real-world testbed demonstrate multiple advantages of Easz over existing compression approaches, in terms of adaptability to different compression levels, computational efficiency, and image reconstruction quality."
      },
      {
        "id": "oai:arXiv.org:2505.01751v1",
        "title": "A dynamic view of the double descent",
        "link": "https://arxiv.org/abs/2505.01751",
        "author": "Vivek Shripad Borkar",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01751v1 Announce Type: cross \nAbstract: It has been observed by Belkin et al.\\ that overparametrized neural networks exhibit a `double descent' phenomenon. That is, as the model complexity, as reflected in the number of features, increases, the training error initially decreases, then increases, and then decreases again. A counterpart of this phenomenon in the time domain has been noted in the context of epoch-wise training, viz., that the training error decreases with time, then increases, then decreases again. This note presents a plausible explanation for this phenomenon by using the theory of two time scale stochastic approximation and singularly perturbed differential equations, applied to the continuous time limit of the gradient dynamics. This adds a `dynamic' angle to an already well studied theme."
      },
      {
        "id": "oai:arXiv.org:2505.01754v1",
        "title": "Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias",
        "link": "https://arxiv.org/abs/2505.01754",
        "author": "Orlando J\\\"ahde, Thorsten Weber, R\\\"udiger Buchkremer",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01754v1 Announce Type: cross \nAbstract: Biased news reporting poses a significant threat to informed decision-making and the functioning of democracies. This study introduces a novel methodology for scalable, minimally biased analysis of media bias in political news. The proposed approach examines event selection, labeling, word choice, and commission and omission biases across news sources by leveraging natural language processing techniques, including hierarchical topic modeling, sentiment analysis, and ontology learning with large language models. Through three case studies related to current political events, we demonstrate the methodology's effectiveness in identifying biases across news sources at various levels of granularity. This work represents a significant step towards scalable, minimally biased media bias analysis, laying the groundwork for tools to help news consumers navigate an increasingly complex media landscape."
      },
      {
        "id": "oai:arXiv.org:2505.01755v1",
        "title": "LensNet: An End-to-End Learning Framework for Empirical Point Spread Function Modeling and Lensless Imaging Reconstruction",
        "link": "https://arxiv.org/abs/2505.01755",
        "author": "Jiesong Bai, Yuhao Yin, Yihang Dong, Xiaofeng Zhang, Chi-Man Pun, Xuhang Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01755v1 Announce Type: cross \nAbstract: Lensless imaging stands out as a promising alternative to conventional lens-based systems, particularly in scenarios demanding ultracompact form factors and cost-effective architectures. However, such systems are fundamentally governed by the Point Spread Function (PSF), which dictates how a point source contributes to the final captured signal. Traditional lensless techniques often require explicit calibrations and extensive pre-processing, relying on static or approximate PSF models. These rigid strategies can result in limited adaptability to real-world challenges, including noise, system imperfections, and dynamic scene variations, thus impeding high-fidelity reconstruction. In this paper, we propose LensNet, an end-to-end deep learning framework that integrates spatial-domain and frequency-domain representations in a unified pipeline. Central to our approach is a learnable Coded Mask Simulator (CMS) that enables dynamic, data-driven estimation of the PSF during training, effectively mitigating the shortcomings of fixed or sparsely calibrated kernels. By embedding a Wiener filtering component, LensNet refines global structure and restores fine-scale details, thus alleviating the dependency on multiple handcrafted pre-processing steps. Extensive experiments demonstrate LensNet's robust performance and superior reconstruction quality compared to state-of-the-art methods, particularly in preserving high-frequency details and attenuating noise. The proposed framework establishes a novel convergence between physics-based modeling and data-driven learning, paving the way for more accurate, flexible, and practical lensless imaging solutions for applications ranging from miniature sensors to medical diagnostics. The link of code is https://github.com/baijiesong/Lensnet."
      },
      {
        "id": "oai:arXiv.org:2505.01768v1",
        "title": "Continuous Filtered Backprojection by Learnable Interpolation Network",
        "link": "https://arxiv.org/abs/2505.01768",
        "author": "Hui Lin, Dong Zeng, Qi Xie, Zerui Mao, Jianhua Ma, Deyu Meng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01768v1 Announce Type: cross \nAbstract: Accurate reconstruction of computed tomography (CT) images is crucial in medical imaging field. However, there are unavoidable interpolation errors in the backprojection step of the conventional reconstruction methods, i.e., filtered-back-projection based methods, which are detrimental to the accurate reconstruction. In this study, to address this issue, we propose a novel deep learning model, named Leanable-Interpolation-based FBP or LInFBP shortly, to enhance the reconstructed CT image quality, which achieves learnable interpolation in the backprojection step of filtered backprojection (FBP) and alleviates the interpolation errors. Specifically, in the proposed LInFBP, we formulate every local piece of the latent continuous function of discrete sinogram data as a linear combination of selected basis functions, and learn this continuous function by exploiting a deep network to predict the linear combination coefficients. Then, the learned latent continuous function is exploited for interpolation in backprojection step, which first time takes the advantage of deep learning for the interpolation in FBP. Extensive experiments, which encompass diverse CT scenarios, demonstrate the effectiveness of the proposed LInFBP in terms of enhanced reconstructed image quality, plug-and-play ability and generalization capability."
      },
      {
        "id": "oai:arXiv.org:2505.01785v1",
        "title": "TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis",
        "link": "https://arxiv.org/abs/2505.01785",
        "author": "Ayoub Abraich",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01785v1 Announce Type: cross \nAbstract: Estimating the causal effect of time-varying treatments on survival outcomes is a challenging task in many domains, particularly in medicine where treatment protocols adapt over time. While recent advances in representation learning have improved causal inference for static treatments, extending these methods to dynamic treatment regimes with survival outcomes remains under-explored. In this paper, we introduce TV-SurvCaus, a novel framework that extends representation balancing techniques to the time-varying treatment setting for survival analysis. We provide theoretical guarantees through (1) a generalized bound for time-varying precision in estimation of heterogeneous effects, (2) variance control via sequential balancing weights, (3) consistency results for dynamic treatment regimes, (4) convergence rates for representation learning with temporal dependencies, and (5) a formal bound on the bias due to treatment-confounder feedback. Our neural architecture incorporates sequence modeling to handle temporal dependencies while balancing time-dependent representations. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that TV-SurvCaus outperforms existing methods in estimating individualized treatment effects with time-varying covariates and treatments. Our framework advances the field of causal inference by enabling more accurate estimation of treatment effects in dynamic, longitudinal settings with survival outcomes."
      },
      {
        "id": "oai:arXiv.org:2505.01807v1",
        "title": "Surrogate to Poincar\\'e inequalities on manifolds for dimension reduction in nonlinear feature spaces",
        "link": "https://arxiv.org/abs/2505.01807",
        "author": "Anthony Nouy, Alexandre Pasco",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01807v1 Announce Type: cross \nAbstract: We aim to approximate a continuously differentiable function $u:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ by a composition of functions $f\\circ g$ where $g:\\mathbb{R}^d \\rightarrow \\mathbb{R}^m$, $m\\leq d$, and $f : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ are built in a two stage procedure. For a fixed $g$, we build $f$ using classical regression methods, involving evaluations of $u$. Recent works proposed to build a nonlinear $g$ by minimizing a loss function $\\mathcal{J}(g)$ derived from Poincar\\'e inequalities on manifolds, involving evaluations of the gradient of $u$. A problem is that minimizing $\\mathcal{J}$ may be a challenging task. Hence in this work, we introduce new convex surrogates to $\\mathcal{J}$. Leveraging concentration inequalities, we provide sub-optimality results for a class of functions $g$, including polynomials, and a wide class of input probability measures. We investigate performances on different benchmarks for various training sample sizes. We show that our approach outperforms standard iterative methods for minimizing the training Poincar\\'e inequality based loss, often resulting in better approximation errors, especially for rather small training sets and $m=1$."
      },
      {
        "id": "oai:arXiv.org:2505.01816v1",
        "title": "Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp",
        "link": "https://arxiv.org/abs/2505.01816",
        "author": "Eran Aizikovich, Dudu Mimran, Edita Grolman, Yuval Elovici, Asaf Shabtai",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01816v1 Announce Type: cross \nAbstract: The Open Radio Access Network (O-RAN) architecture is revolutionizing cellular networks with its open, multi-vendor design and AI-driven management, aiming to enhance flexibility and reduce costs. Although it has many advantages, O-RAN is not threat-free. While previous studies have mainly examined vulnerabilities arising from O-RAN's intelligent components, this paper is the first to focus on the security challenges and vulnerabilities introduced by transitioning from single-operator to multi-operator RAN architectures. This shift increases the risk of untrusted third-party operators managing different parts of the network. To explore these vulnerabilities and their potential mitigation, we developed an open-access testbed environment that integrates a wireless network simulator with the official O-RAN Software Community (OSC) RAN intelligent component (RIC) cluster. This environment enables realistic, live data collection and serves as a platform for demonstrating APATE (adversarial perturbation against traffic efficiency), an evasion attack in which a malicious cell manipulates its reported key performance indicators (KPIs) and deceives the O-RAN traffic steering to gain unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate activity continues, we introduce MARRS (monitoring adversarial RAN reports), a detection framework based on a long-short term memory (LSTM) autoencoder (AE) that learns contextual features across the network to monitor malicious telemetry (also demonstrated in our testbed). Our evaluation showed that by executing APATE, an attacker can obtain a 248.5% greater UE allocation than it was supposed to in a benign scenario. In addition, the MARRS detection method was also shown to successfully classify malicious cell activity, achieving accuracy of 99.2% and an F1 score of 0.978."
      },
      {
        "id": "oai:arXiv.org:2505.01821v1",
        "title": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey",
        "link": "https://arxiv.org/abs/2505.01821",
        "author": "Jing Liu, Yao Du, Kun Yang, Yan Wang, Xiping Hu, Zehua Wang, Yang Liu, Peng Sun, Azzedine Boukerche, Victor C. M. Leung",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01821v1 Announce Type: cross \nAbstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems."
      },
      {
        "id": "oai:arXiv.org:2505.01828v1",
        "title": "Rank-One Modified Value Iteration",
        "link": "https://arxiv.org/abs/2505.01828",
        "author": "Arman Sharifi Kolarijani, Tolga Ok, Peyman Mohajerin Esfahani, Mohamad Amin Sharif Kolarijani",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01828v1 Announce Type: cross \nAbstract: In this paper, we provide a novel algorithm for solving planning and learning problems of Markov decision processes. The proposed algorithm follows a policy iteration-type update by using a rank-one approximation of the transition probability matrix in the policy evaluation step. This rank-one approximation is closely related to the stationary distribution of the corresponding transition probability matrix, which is approximated using the power method. We provide theoretical guarantees for the convergence of the proposed algorithm to optimal (action-)value function with the same rate and computational complexity as the value iteration algorithm in the planning problem and as the Q-learning algorithm in the learning problem. Through our extensive numerical simulations, however, we show that the proposed algorithm consistently outperforms first-order algorithms and their accelerated versions for both planning and learning problems."
      },
      {
        "id": "oai:arXiv.org:2505.01831v1",
        "title": "Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement",
        "link": "https://arxiv.org/abs/2505.01831",
        "author": "Haofan Wu, Yin Huang, Yuqing Wu, Qiuyu Yang, Bingfang Wang, Li Zhang, Muhammad Fahadullah Khan, Ali Zia, M. Saleh Memon, Syed Sohail Bukhari, Abdul Fattah Memon, Daizong Ji, Ya Zhang, Ghulam Mustafa, Yin Fang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01831v1 Announce Type: cross \nAbstract: High-quality fundus images provide essential anatomical information for clinical screening and ophthalmic disease diagnosis. Yet, due to hardware limitations, operational variability, and patient compliance, fundus images often suffer from low resolution and signal-to-noise ratio. Recent years have witnessed promising progress in fundus image enhancement. However, existing works usually focus on restoring structural details or global characteristics of fundus images, lacking a unified image enhancement framework to recover comprehensive multi-scale information. Moreover, few methods pinpoint the target of image enhancement, e.g., lesions, which is crucial for medical image-based diagnosis. To address these challenges, we propose a multi-scale target-aware representation learning framework (MTRL-FIE) for efficient fundus image enhancement. Specifically, we propose a multi-scale feature encoder (MFE) that employs wavelet decomposition to embed both low-frequency structural information and high-frequency details. Next, we design a structure-preserving hierarchical decoder (SHD) to fuse multi-scale feature embeddings for real fundus image restoration. SHD integrates hierarchical fusion and group attention mechanisms to achieve adaptive feature fusion while retaining local structural smoothness. Meanwhile, a target-aware feature aggregation (TFA) module is used to enhance pathological regions and reduce artifacts. Experimental results on multiple fundus image datasets demonstrate the effectiveness and generalizability of MTRL-FIE for fundus image enhancement. Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancement performance with a more lightweight architecture. Furthermore, our approach generalizes to other ophthalmic image processing tasks without supervised fine-tuning, highlighting its potential for clinical applications."
      },
      {
        "id": "oai:arXiv.org:2505.01854v1",
        "title": "Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2",
        "link": "https://arxiv.org/abs/2505.01854",
        "author": "Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01854v1 Announce Type: cross \nAbstract: Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on three public datasets covering organs, bones, and muscles across MRI and CT modalities. We show that the proposed method markedly outperforms the default SAM 2, achieving average Dice Similarity Coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development."
      },
      {
        "id": "oai:arXiv.org:2505.01859v1",
        "title": "Bayesian learning of the optimal action-value function in a Markov decision process",
        "link": "https://arxiv.org/abs/2505.01859",
        "author": "Jiaqi Guo, Chon Wai Ho, Sumeetpal S. Singh",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01859v1 Announce Type: cross \nAbstract: The Markov Decision Process (MDP) is a popular framework for sequential decision-making problems, and uncertainty quantification is an essential component of it to learn optimal decision-making strategies. In particular, a Bayesian framework is used to maintain beliefs about the optimal decisions and the unknown ingredients of the model, which are also to be learned from the data, such as the rewards and state dynamics. However, many existing Bayesian approaches for learning the optimal decision-making strategy are based on unrealistic modelling assumptions and utilise approximate inference techniques. This raises doubts whether the benefits of Bayesian uncertainty quantification are fully realised or can be relied upon.\n  We focus on infinite-horizon and undiscounted MDPs, with finite state and action spaces, and a terminal state. We provide a full Bayesian framework, from modelling to inference to decision-making. For modelling, we introduce a likelihood function with minimal assumptions for learning the optimal action-value function based on Bellman's optimality equations, analyse its properties, and clarify connections to existing works. For deterministic rewards, the likelihood is degenerate and we introduce artificial observation noise to relax it, in a controlled manner, to facilitate more efficient Monte Carlo-based inference. For inference, we propose an adaptive sequential Monte Carlo algorithm to both sample from and adjust the sequence of relaxed posterior distributions. For decision-making, we choose actions using samples from the posterior distribution over the optimal strategies. While commonly done, we provide new insight that clearly shows that it is a generalisation of Thompson sampling from multi-arm bandit problems. Finally, we evaluate our framework on the Deep Sea benchmark problem and demonstrate the exploration benefits of posterior sampling in MDPs."
      },
      {
        "id": "oai:arXiv.org:2505.01866v1",
        "title": "PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework",
        "link": "https://arxiv.org/abs/2505.01866",
        "author": "Daniel Commey, Garth V. Crosby",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01866v1 Announce Type: cross \nAbstract: Federated Learning (FL) enables collaborative model training while preserving data privacy, but its classical cryptographic underpinnings are vulnerable to quantum attacks. This vulnerability is particularly critical in sensitive domains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure Blockchain-based Federated Learning), a framework integrating post-quantum cryptography (PQC) with blockchain verification to secure FL against quantum adversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly Dilithium) signatures to authenticate model updates and leverage optimized smart contracts for decentralized validation. Extensive evaluations on diverse datasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient cryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms) with a fixed signature size of 3309 Bytes. Blockchain integration incurs a manageable overhead, with average transaction times around 4.8 s and gas usage per update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the cryptographic overhead relative to transaction time remains minimal (around 0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the bottleneck in blockchain-based FL. The system maintains competitive model accuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with round times showing sublinear growth with increasing client numbers. Our open-source implementation and reproducible benchmarks validate the feasibility of deploying long-term, quantum-resistant security in practical FL systems."
      },
      {
        "id": "oai:arXiv.org:2505.01880v1",
        "title": "Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network",
        "link": "https://arxiv.org/abs/2505.01880",
        "author": "Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01880v1 Announce Type: cross \nAbstract: Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks."
      },
      {
        "id": "oai:arXiv.org:2505.01884v1",
        "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images",
        "link": "https://arxiv.org/abs/2505.01884",
        "author": "Siddharth Kothari, Srinivasan Murali, Sankalp Kothari, Ujjwal Verma, Jaya Sreevalsan-Nair",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01884v1 Announce Type: cross \nAbstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is an important task needed for several applications, such as flood mapping. While SAR sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from SAR images is not straightforward. Inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. U-Net is a widely used deep learning model for land-water segmentation of SAR images. In practice, manual annotation is often used to generate the corresponding water masks as ground truth. Manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. In this work, we simulate manual errors in the form of adversarial attacks on the U-Net model and study the robustness of the model to human errors in annotation. Our results indicate that U-Net can tolerate a certain level of corruption before its performance drops significantly. This finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. The code and the new dataset, along with adversarial examples for robust training, are publicly available. (Github link - https://github.com/GVCL/IWSeg-SAR-Poison.git)"
      },
      {
        "id": "oai:arXiv.org:2505.01917v1",
        "title": "Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling",
        "link": "https://arxiv.org/abs/2505.01917",
        "author": "Javier E. Santos, Agnese Marcato, Roman Colman, Nicholas Lubbers, Yen Ting Lin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01917v1 Announce Type: cross \nAbstract: Generative diffusion models have achieved remarkable success in producing high-quality images. However, because these models typically operate in continuous intensity spaces - diffusing independently per pixel and color channel - they are fundamentally ill-suited for applications where quantities such as particle counts or material units are inherently discrete and governed by strict conservation laws such as mass preservation, limiting their applicability in scientific workflows. To address this limitation, we propose Discrete Spatial Diffusion (DSD), a framework based on a continuous-time, discrete-state jump stochastic process that operates directly in discrete spatial domains while strictly preserving mass in both forward and reverse diffusion processes. By using spatial diffusion to achieve mass preservation, we introduce stochasticity naturally through a discrete formulation. We demonstrate the expressive flexibility of DSD by performing image synthesis, class conditioning, and image inpainting across widely-used image benchmarks, with the ability to condition on image intensity. Additionally, we highlight its applicability to domain-specific scientific data for materials microstructure, bridging the gap between diffusion models and mass-conditioned scientific applications."
      },
      {
        "id": "oai:arXiv.org:2505.01932v1",
        "title": "OT-Talk: Animating 3D Talking Head with Optimal Transportation",
        "link": "https://arxiv.org/abs/2505.01932",
        "author": "Xinmu Wang, Xiang Gao, Xiyun Song, Heather Yu, Zongfang Lin, Liang Peng, Xianfeng Gu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01932v1 Announce Type: cross \nAbstract: Animating 3D head meshes using audio inputs has significant applications in AR/VR, gaming, and entertainment through 3D avatars. However, bridging the modality gap between speech signals and facial dynamics remains a challenge, often resulting in incorrect lip syncing and unnatural facial movements. To address this, we propose OT-Talk, the first approach to leverage optimal transportation to optimize the learning model in talking head animation. Building on existing learning frameworks, we utilize a pre-trained Hubert model to extract audio features and a transformer model to process temporal sequences. Unlike previous methods that focus solely on vertex coordinates or displacements, we introduce Chebyshev Graph Convolution to extract geometric features from triangulated meshes. To measure mesh dissimilarities, we go beyond traditional mesh reconstruction errors and velocity differences between adjacent frames. Instead, we represent meshes as probability measures and approximate their surfaces. This allows us to leverage the sliced Wasserstein distance for modeling mesh variations. This approach facilitates the learning of smooth and accurate facial motions, resulting in coherent and natural facial animations. Our experiments on two public audio-mesh datasets demonstrate that our method outperforms state-of-the-art techniques both quantitatively and qualitatively in terms of mesh reconstruction accuracy and temporal alignment. In addition, we conducted a user perception study with 20 volunteers to further assess the effectiveness of our approach."
      },
      {
        "id": "oai:arXiv.org:2505.01937v1",
        "title": "Faster logconcave sampling from a cold start in high dimension",
        "link": "https://arxiv.org/abs/2505.01937",
        "author": "Yunbum Kook, Santosh S. Vempala",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01937v1 Announce Type: cross \nAbstract: We present a faster algorithm to generate a warm start for sampling an arbitrary logconcave density specified by an evaluation oracle, leading to the first sub-cubic sampling algorithms for inputs in (near-)isotropic position. A long line of prior work incurred a warm-start penalty of at least linear in the dimension, hitting a cubic barrier, even for the special case of uniform sampling from convex bodies.\n  Our improvement relies on two key ingredients of independent interest. (1) We show how to sample given a warm start in weaker notions of distance, in particular $q$-R\\'enyi divergence for $q=\\widetilde{\\mathcal{O}}(1)$, whereas previous analyses required stringent $\\infty$-R\\'enyi divergence (with the exception of Hit-and-Run, whose known mixing time is higher). This marks the first improvement in the required warmness since Lov\\'asz and Simonovits (1991). (2) We refine and generalize the log-Sobolev inequality of Lee and Vempala (2018), originally established for isotropic logconcave distributions in terms of the diameter of the support, to logconcave distributions in terms of a geometric average of the support diameter and the largest eigenvalue of the covariance matrix."
      },
      {
        "id": "oai:arXiv.org:2505.01944v1",
        "title": "Explainability by design: an experimental analysis of the legal coding process",
        "link": "https://arxiv.org/abs/2505.01944",
        "author": "Matteo Cristani, Guido Governatori, Francesco Olivieri, Monica Palmirani, Gabriele Buriola",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01944v1 Announce Type: cross \nAbstract: Behind a set of rules in Deontic Defeasible Logic, there is a mapping process of normative background fragments. This process goes from text to rules and implicitly encompasses an explanation of the coded fragments.\n  In this paper we deliver a methodology for \\textit{legal coding} that starts with a fragment and goes onto a set of Deontic Defeasible Logic rules, involving a set of \\textit{scenarios} to test the correctness of the coded fragments. The methodology is illustrated by the coding process of an example text. We then show the results of a series of experiments conducted with humans encoding a variety of normative backgrounds and corresponding cases in which we have measured the efforts made in the coding process, as related to some measurable features. To process these examples, a recently developed technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has been employed.\n  Finally we provide a technique to forecast time required in coding, that depends on factors such as knowledge of the legal domain, knowledge of the coding processes, length of the text, and a measure of \\textit{depth} that refers to the length of the paths of legal references."
      },
      {
        "id": "oai:arXiv.org:2505.01947v1",
        "title": "Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach",
        "link": "https://arxiv.org/abs/2505.01947",
        "author": "Ivan Tan, Wei Minn, Christopher M. Poskitt, Lwin Khin Shar, Lingxiao Jiang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01947v1 Announce Type: cross \nAbstract: UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study."
      },
      {
        "id": "oai:arXiv.org:2505.01966v1",
        "title": "A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites",
        "link": "https://arxiv.org/abs/2505.01966",
        "author": "Bofei Liu, Dong Ye, Zunhao Yao, Zhaowei Sun",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01966v1 Announce Type: cross \nAbstract: Modular self-reconfigurable satellites refer to satellite clusters composed of individual modular units capable of altering their configurations. The configuration changes enable the execution of diverse tasks and mission objectives. Existing path planning algorithms for reconfiguration often suffer from high computational complexity, poor generalization capability, and limited support for diverse target configurations. To address these challenges, this paper proposes a goal-oriented reinforcement learning-based path planning algorithm. This algorithm is the first to address the challenge that previous reinforcement learning methods failed to overcome, namely handling multiple target configurations. Moreover, techniques such as Hindsight Experience Replay and Invalid Action Masking are incorporated to overcome the significant obstacles posed by sparse rewards and invalid actions. Based on these designs, our model achieves a 95% and 73% success rate in reaching arbitrary target configurations in a modular satellite cluster composed of four and six units, respectively."
      },
      {
        "id": "oai:arXiv.org:2505.01985v1",
        "title": "Optimization over Trained (and Sparse) Neural Networks: A Surrogate within a Surrogate",
        "link": "https://arxiv.org/abs/2505.01985",
        "author": "Hung Pham, Aiden Ren, Ibrahim Tahir, Jiatai Tong, Thiago Serra",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01985v1 Announce Type: cross \nAbstract: We can approximate a constraint or an objective function that is uncertain or nonlinear with a neural network that we embed in the optimization model. This approach, which is known as constraint learning, faces the challenge that optimization models with neural network surrogates are harder to solve. Such difficulties have motivated studies on model reformulation, specialized optimization algorithms, and - to a lesser extent - pruning of the embedded networks. In this work, we double down on the use of surrogates by applying network pruning to produce a surrogate of the neural network itself. In the context of using a Mixed-Integer Linear Programming (MILP) solver to verify neural networks, we obtained faster adversarial perturbations for dense neural networks by using sparse surrogates, especially - and surprisingly - if not taking the time to finetune the sparse network to make up for the loss in accuracy. In other words, we show that a pruned network with bad classification performance can still be a good - and more efficient - surrogate."
      },
      {
        "id": "oai:arXiv.org:2505.01995v1",
        "title": "Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks",
        "link": "https://arxiv.org/abs/2505.01995",
        "author": "Sehwan Kim, Faming Liang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01995v1 Announce Type: cross \nAbstract: Individual treatment effect estimation has gained significant attention in recent data science literature. This work introduces the Double Neural Network (Double-NN) method to address this problem within the framework of extended fiducial inference (EFI). In the proposed method, deep neural networks are used to model the treatment and control effect functions, while an additional neural network is employed to estimate their parameters. The universal approximation capability of deep neural networks ensures the broad applicability of this method. Numerical results highlight the superior performance of the proposed Double-NN method compared to the conformal quantile regression (CQR) method in individual treatment effect estimation. From the perspective of statistical inference, this work advances the theory and methodology for statistical inference of large models. Specifically, it is theoretically proven that the proposed method permits the model size to increase with the sample size $n$ at a rate of $O(n^{\\zeta})$ for some $0 \\leq \\zeta<1$, while still maintaining proper quantification of uncertainty in the model parameters. This result marks a significant improvement compared to the range $0\\leq \\zeta < \\frac{1}{2}$ required by the classical central limit theorem. Furthermore, this work provides a rigorous framework for quantifying the uncertainty of deep neural networks under the neural scaling law, representing a substantial contribution to the statistical understanding of large-scale neural network models."
      },
      {
        "id": "oai:arXiv.org:2505.02001v1",
        "title": "Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework",
        "link": "https://arxiv.org/abs/2505.02001",
        "author": "Vineesh Kumar Reddy Mondem",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02001v1 Announce Type: cross \nAbstract: Traditional image quality assessment metrics like Mean Squared Error and Structural Similarity Index often fail to reflect perceptual quality under complex distortions. We propose the Hybrid Image Resolution Quality Metric (HIRQM), integrating statistical, multi-scale, and deep learning-based methods for a comprehensive quality evaluation. HIRQM combines three components: Probability Density Function for local pixel distribution analysis, Multi-scale Feature Similarity for structural integrity across resolutions, and Hierarchical Deep Image Features using a pre-trained VGG16 network for semantic alignment with human perception. A dynamic weighting mechanism adapts component contributions based on image characteristics like brightness and variance, enhancing flexibility across distortion types. Our contributions include a unified metric and dynamic weighting for better perceptual alignment. Evaluated on TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations of 0.92 and 0.90, outperforming traditional metrics. It excels in handling noise, blur, and compression artifacts, making it valuable for image processing applications like compression and restoration."
      },
      {
        "id": "oai:arXiv.org:2505.02010v1",
        "title": "Meta-Black-Box-Optimization through Offline Q-function Learning",
        "link": "https://arxiv.org/abs/2505.02010",
        "author": "Zeyuan Ma, Zhiguang Cao, Zhou Jiang, Hongshu Guo, Yue-Jiao Gong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02010v1 Announce Type: cross \nAbstract: Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated that using RL to learn a meta-level policy for dynamic algorithm configuration (DAC) over an optimization task distribution could significantly enhance the performance of the low-level BBO algorithm. However, the online learning paradigms in existing works makes the efficiency of MetaBBO problematic. To address this, we propose an offline learning-based MetaBBO framework in this paper, termed Q-Mamba, to attain both effectiveness and efficiency in MetaBBO. Specifically, we first transform DAC task into long-sequence decision process. This allows us further introduce an effective Q-function decomposition mechanism to reduce the learning difficulty within the intricate algorithm configuration space. Under this setting, we propose three novel designs to meta-learn DAC policy from offline data: we first propose a novel collection strategy for constructing offline DAC experiences dataset with balanced exploration and exploitation. We then establish a decomposition-based Q-loss that incorporates conservative Q-learning to promote stable offline learning from the offline dataset. To further improve the offline learning efficiency, we equip our work with a Mamba architecture which helps long-sequence learning effectiveness and efficiency by selective state model and hardware-aware parallel scan respectively. Through extensive benchmarking, we observe that Q-Mamba achieves competitive or even superior performance to prior online/offline baselines, while significantly improving the training efficiency of existing online baselines. We provide sourcecodes of Q-Mamba at https://github.com/MetaEvo/Q-Mamba."
      },
      {
        "id": "oai:arXiv.org:2505.02019v1",
        "title": "Learning the Simplest Neural ODE",
        "link": "https://arxiv.org/abs/2505.02019",
        "author": "Yuji Okamoto, Tomoya Takeuchi, Yusuke Sakemi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02019v1 Announce Type: cross \nAbstract: Since the advent of the ``Neural Ordinary Differential Equation (Neural ODE)'' paper, learning ODEs with deep learning has been applied to system identification, time-series forecasting, and related areas. Exploiting the diffeomorphic nature of ODE solution maps, neural ODEs has also enabled their use in generative modeling. Despite the rich potential to incorporate various kinds of physical information, training Neural ODEs remains challenging in practice. This study demonstrates, through the simplest one-dimensional linear model, why training Neural ODEs is difficult. We then propose a new stabilization method and provide an analytical convergence analysis. The insights and techniques presented here serve as a concise tutorial for researchers beginning work on Neural ODEs."
      },
      {
        "id": "oai:arXiv.org:2505.02052v1",
        "title": "TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition",
        "link": "https://arxiv.org/abs/2505.02052",
        "author": "Lala Shakti Swarup Ray, Lars Krupp, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02052v1 Announce Type: cross \nAbstract: Sensor-based human activity recognition (HAR) has predominantly focused on Inertial Measurement Units and vision data, often overlooking the capabilities unique to pressure sensors, which capture subtle body dynamics and shifts in the center of mass. Despite their potential for postural and balance-based activities, pressure sensors remain underutilized in the HAR domain due to limited datasets. To bridge this gap, we propose to exploit generative foundation models with pressure-specific HAR techniques. Specifically, we present a bidirectional Text$\\times$Pressure model that uses generative foundation models to interpret pressure data as natural language. TxP accomplishes two tasks: (1) Text2Pressure, converting activity text descriptions into pressure sequences, and (2) Pressure2Text, generating activity descriptions and classifications from dynamic pressure maps. Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on our synthetic PressLang dataset, containing over 81,100 text-pressure pairs. Validated on real-world data for activities such as yoga and daily tasks, TxP provides novel approaches to data augmentation and classification grounded in atomic actions. This consequently improved HAR performance by up to 12.4\\% in macro F1 score compared to the state-of-the-art, advancing pressure-based HAR with broader applications and deeper insights into human movement."
      },
      {
        "id": "oai:arXiv.org:2505.02101v1",
        "title": "Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization",
        "link": "https://arxiv.org/abs/2505.02101",
        "author": "Youran Dong, Junfeng Yang, Wei Yao, Jin Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02101v1 Announce Type: cross \nAbstract: Bilevel optimization is a powerful tool for many machine learning problems, such as hyperparameter optimization and meta-learning. Estimating hypergradients (also known as implicit gradients) is crucial for developing gradient-based methods for bilevel optimization. In this work, we propose a computationally efficient technique for incorporating curvature information into the approximation of hypergradients and present a novel algorithmic framework based on the resulting enhanced hypergradient computation. We provide convergence rate guarantees for the proposed framework in both deterministic and stochastic scenarios, particularly showing improved computational complexity over popular gradient-based methods in the deterministic setting. This improvement in complexity arises from a careful exploitation of the hypergradient structure and the inexact Newton method. In addition to the theoretical speedup, numerical experiments demonstrate the significant practical performance benefits of incorporating curvature information."
      },
      {
        "id": "oai:arXiv.org:2505.02130v1",
        "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data",
        "link": "https://arxiv.org/abs/2505.02130",
        "author": "Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02130v1 Announce Type: cross \nAbstract: Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}"
      },
      {
        "id": "oai:arXiv.org:2505.02170v1",
        "title": "Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach",
        "link": "https://arxiv.org/abs/2505.02170",
        "author": "Danial Ramezani",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02170v1 Announce Type: cross \nAbstract: Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies."
      },
      {
        "id": "oai:arXiv.org:2505.02173v1",
        "title": "Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering",
        "link": "https://arxiv.org/abs/2505.02173",
        "author": "Chutiphan Charoensuk, Nathakhun Wiroonsri",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02173v1 Announce Type: cross \nAbstract: Time series clustering is an unsupervised learning method for classifying time series data into groups with similar behavior. It is used in applications such as healthcare, finance, economics, energy, and climate science. Several time series clustering methods have been introduced and used for over four decades. Most of them focus on measuring either Euclidean distances or association dissimilarities between time series. In this work, we propose a new dissimilarity measure called ranked Pearson correlation dissimilarity (RDPC), which combines a weighted average of a specified fraction of the largest element-wise differences with the well-known Pearson correlation dissimilarity. It is incorporated into hierarchical clustering. The performance is evaluated and compared with existing clustering algorithms. The results show that the RDPC algorithm outperforms others in complicated cases involving different seasonal patterns, trends, and peaks. Finally, we demonstrate our method by clustering a random sample of customers from a Thai electricity consumption time series dataset into seven groups with unique characteristics."
      },
      {
        "id": "oai:arXiv.org:2505.02185v1",
        "title": "Latent Variable Estimation in Bayesian Black-Litterman Models",
        "link": "https://arxiv.org/abs/2505.02185",
        "author": "Thomas Y. L. Lin, Jerry Yao-Chieh Hu, Paul W. Chiou, Peter Lin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02185v1 Announce Type: cross \nAbstract: We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its reliance on subjective investor views. Classical BL requires an investor \"view\": a forecast vector $q$ and its uncertainty matrix $\\Omega$ that describe how much a chosen portfolio should outperform the market. Our key idea is to treat $(q,\\Omega)$ as latent variables and learn them from market data within a single Bayesian network. Consequently, the resulting posterior estimation admits closed-form expression, enabling fast inference and stable portfolio weights. Building on these, we propose two mechanisms to capture how features interact with returns: shared-latent parametrization and feature-influenced views; both recover classical BL and Markowitz portfolios as special cases. Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve Sharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the index baselines. This work turns BL into a fully data-driven, view-free, and coherent Bayesian framework for portfolio optimization."
      },
      {
        "id": "oai:arXiv.org:2505.02199v1",
        "title": "Exploring new Approaches for Information Retrieval through Natural Language Processing",
        "link": "https://arxiv.org/abs/2505.02199",
        "author": "Manak Raj, Nidhi Mishra",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02199v1 Announce Type: cross \nAbstract: This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations."
      },
      {
        "id": "oai:arXiv.org:2505.02208v1",
        "title": "Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities",
        "link": "https://arxiv.org/abs/2505.02208",
        "author": "Ehud Shapiro, Nimrod Talmon",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02208v1 Announce Type: cross \nAbstract: Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.\n  In practice, we expect a grassroots democratic federation to grow and evolve dynamically and in all directions -- bottom-up, top-down, and middle-out. To address that, we formally specify this dynamic setting and adapt the static fairness conditions to it: The ex post condition on the fair representation of a child community becomes a condition that must always hold; the ex ante conditions in expectation on the fair participation of an individual and on the fair representation of a child community become conditions satisfied in actuality in the limit, provided the federation structure eventually stabilizes. We then present a protocol that satisfies these fairness conditions."
      },
      {
        "id": "oai:arXiv.org:2505.02211v1",
        "title": "CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images",
        "link": "https://arxiv.org/abs/2505.02211",
        "author": "Peiqi Li, Yincheng Gao, Renxing Li, Haojie Yang, Yunyun Liu, Boji Liu, Jiahui Ni, Ying Zhang, Yulu Wu, Xiaowei Fang, Lehang Guo, Liping Sun, Jiangang Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02211v1 Announce Type: cross \nAbstract: Heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. To address this issue, we propose a novel multitask learning framework, Channel-Spatial Attention Synergy Network (CSASN), which integrates a dual-branch feature extractor - combining EfficientNet for local spatial encoding and ViT for global semantic modeling, with a cascaded channel-spatial attention refinement module. A residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. Trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. Extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as FTC and MTC carcinomas. Experimental results show that CSASN outperforms existing single-stream CNN or Transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. This framework provides a promising strategy for AI-assisted thyroid cancer diagnosis."
      },
      {
        "id": "oai:arXiv.org:2505.02215v1",
        "title": "Interpretable Emergent Language Using Inter-Agent Transformers",
        "link": "https://arxiv.org/abs/2505.02215",
        "author": "Mannan Bhardwaj",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02215v1 Announce Type: cross \nAbstract: This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments."
      },
      {
        "id": "oai:arXiv.org:2505.02224v1",
        "title": "Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees",
        "link": "https://arxiv.org/abs/2505.02224",
        "author": "Andrew Quijano, Spyros T. Halkidis, Kevin Gallagher, Kemal Akkaya, Nikolaos Samaras",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02224v1 Announce Type: cross \nAbstract: A decision tree is an easy-to-understand tool that has been widely used for classification tasks. On the one hand, due to privacy concerns, there has been an urgent need to create privacy-preserving classifiers that conceal the user's input from the classifier. On the other hand, with the rise of cloud computing, data owners are keen to reduce risk by outsourcing their model, but want security guarantees that third parties cannot steal their decision tree model. To address these issues, Joye and Salehi introduced a theoretical protocol that efficiently evaluates decision trees while maintaining privacy by leveraging their comparison protocol that is resistant to timing attacks. However, their approach was not only inefficient but also prone to side-channel attacks. Therefore, in this paper, we propose a new decision tree inference protocol in which the model is shared and evaluated among multiple entities. We partition our decision tree model by each level to be stored in a new entity we refer to as a \"level-site.\" Utilizing this approach, we were able to gain improved average run time for classifier evaluation for a non-complete tree, while also having strong mitigations against side-channel attacks."
      },
      {
        "id": "oai:arXiv.org:2505.02232v1",
        "title": "Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning",
        "link": "https://arxiv.org/abs/2505.02232",
        "author": "Malte Mosbach, Sven Behnke",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02232v1 Announce Type: cross \nAbstract: Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io"
      },
      {
        "id": "oai:arXiv.org:2505.02248v1",
        "title": "Heterosynaptic Circuits Are Universal Gradient Machines",
        "link": "https://arxiv.org/abs/2505.02248",
        "author": "Liu Ziyin, Isaac Chuang, Tomaso Poggio",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02248v1 Announce Type: cross \nAbstract: We propose a design principle for the learning circuits of the biological brain. The principle states that almost any dendritic weights updated via heterosynaptic plasticity can implement a generalized and efficient class of gradient-based meta-learning. The theory suggests that a broad class of biologically plausible learning algorithms, together with the standard machine learning optimizers, can be grounded in heterosynaptic circuit motifs. This principle suggests that the phenomenology of (anti-) Hebbian (HBP) and heterosynaptic plasticity (HSP) may emerge from the same underlying dynamics, thus providing a unifying explanation. It also suggests an alternative perspective of neuroplasticity, where HSP is promoted to the primary learning and memory mechanism, and HBP is an emergent byproduct. We present simulations that show that (a) HSP can explain the metaplasticity of neurons, (b) HSP can explain the flexibility of the biology circuits, and (c) gradient learning can arise quickly from simple evolutionary dynamics that do not compute any explicit gradient. While our primary focus is on biology, the principle also implies a new approach to designing AI training algorithms and physically learnable AI hardware. Conceptually, our result demonstrates that contrary to the common belief, gradient computation may be extremely easy and common in nature."
      },
      {
        "id": "oai:arXiv.org:2505.02257v1",
        "title": "Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift",
        "link": "https://arxiv.org/abs/2505.02257",
        "author": "Yu Zhu, Zehang Richard Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02257v1 Announce Type: cross \nAbstract: In regions lacking medically certified causes of death, verbal autopsy (VA) is a critical and widely used tool to ascertain the cause of death through interviews with caregivers. Data collected by VAs are often analyzed using probabilistic algorithms. The performance of these algorithms often degrades due to distributional shift across populations. Most existing VA algorithms rely on centralized training, requiring full access to training data for joint modeling. This is often infeasible due to privacy and logistical constraints. In this paper, we propose a novel Bayesian Federated Learning (BFL) framework that avoids data sharing across multiple training sources. Our method enables reliable individual-level cause-of-death classification and population-level quantification of cause-specific mortality fractions (CSMFs), in a target domain with limited or no local labeled data. The proposed framework is modular, computationally efficient, and compatible with a wide range of existing VA algorithms as candidate models, facilitating flexible deployment in real-world mortality surveillance systems. We validate the performance of BFL through extensive experiments on two real-world VA datasets under varying levels of distribution shift. Our results show that BFL significantly outperforms the base models built on a single domain and achieves comparable or better performance compared to joint modeling."
      },
      {
        "id": "oai:arXiv.org:2505.02258v1",
        "title": "Inverse Modeling of Dielectric Response in Time Domain using Physics-Informed Neural Networks",
        "link": "https://arxiv.org/abs/2505.02258",
        "author": "Emir Esenov, Olof Hjortstam, Yuriy Serdyuk, Thomas Hammarstr\\\"om, Christian H\\\"ager",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02258v1 Announce Type: cross \nAbstract: Dielectric response (DR) of insulating materials is key input information for designing electrical insulation systems and defining safe operating conditions of various HV devices. In dielectric materials, different polarization and conduction processes occur at different time scales, making it challenging to physically interpret raw measured data. To analyze DR measurement results, equivalent circuit models (ECMs) are commonly used, reducing the complexity of the physical system to a number of circuit elements that capture the dominant response. This paper examines the use of physics-informed neural networks (PINNs) for inverse modeling of DR in time domain using parallel RC circuits. To assess their performance, we test PINNs on synthetic data generated from analytical solutions of corresponding ECMs, incorporating Gaussian noise to simulate measurement errors. Our results show that PINNs are highly effective at solving well-conditioned inverse problems, accurately estimating up to five unknown RC parameters with minimal requirements on neural network size, training duration, and hyperparameter tuning. Furthermore, we extend the ECMs to incorporate temperature dependence and demonstrate that PINNs can accurately recover embedded, nonlinear temperature functions from noisy DR data sampled at different temperatures. This case study in modeling DR in time domain presents a solution with wide-ranging potential applications in disciplines relying on ECMs, utilizing the latest technology in machine learning for scientific computation."
      },
      {
        "id": "oai:arXiv.org:2505.02259v1",
        "title": "Smooth Integer Encoding via Integral Balance",
        "link": "https://arxiv.org/abs/2505.02259",
        "author": "Stanislav Semenov",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02259v1 Announce Type: cross \nAbstract: We introduce a novel method for encoding integers using smooth real-valued functions whose integral properties implicitly reflect discrete quantities. In contrast to classical representations, where the integer appears as an explicit parameter, our approach encodes the number N in the set of natural numbers through the cumulative balance of a smooth function f_N(t), constructed from localized Gaussian bumps with alternating and decaying coefficients. The total integral I(N) converges to zero as N tends to infinity, and the integer can be recovered as the minimal point of near-cancellation.\n  This method enables continuous and differentiable representations of discrete states, supports recovery through spline-based or analytical inversion, and extends naturally to multidimensional tuples (N1, N2, ...). We analyze the structure and convergence of the encoding series, demonstrate numerical construction of the integral map I(N), and develop procedures for integer recovery via numerical inversion. The resulting framework opens a path toward embedding discrete logic within continuous optimization pipelines, machine learning architectures, and smooth symbolic computation."
      },
      {
        "id": "oai:arXiv.org:2505.02281v1",
        "title": "Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles",
        "link": "https://arxiv.org/abs/2505.02281",
        "author": "Amir Ali Farzin, Yuen-Man Pun, Iman Shames",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02281v1 Announce Type: cross \nAbstract: This study explores the performance of a random Gaussian smoothing zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly quasar-convex (SQC) functions in both unconstrained and constrained settings. For the unconstrained problem, we establish the ZO algorithm's convergence to a global minimum along with its complexity when applied to both QC and SQC functions. For the constrained problem, we introduce the new notion of proximal-quasar-convexity and prove analogous results to the unconstrained case. Specifically, we show the complexity bounds and the convergence of the algorithm to a neighbourhood of a global minimum whose size can be controlled under a variance reduction scheme. Theoretical findings are illustrated through investigating the performance of the algorithm applied to a range of problems in machine learning and optimisation. Specifically, we observe scenarios where the ZO method outperforms gradient descent. We provide a possible explanation for this phenomenon."
      },
      {
        "id": "oai:arXiv.org:2505.02314v1",
        "title": "NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities",
        "link": "https://arxiv.org/abs/2505.02314",
        "author": "James Read, Ming-Yen Lee, Wei-Hsing Huang, Yuan-Chun Luo, Anni Lu, Shimeng Yu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02314v1 Announce Type: cross \nAbstract: The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) operations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non-idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT's post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, making it straightforward to incorporate data from SPICE simulations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5x faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at https://github.com/neurosim/NeuroSim."
      },
      {
        "id": "oai:arXiv.org:2505.02350v1",
        "title": "Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation",
        "link": "https://arxiv.org/abs/2505.02350",
        "author": "Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02350v1 Announce Type: cross \nAbstract: Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using sparse ellipsoidal radial basis function networks, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represent the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding code is publicly available at https://github.com/lianbobo/SE-RBFNet.git."
      },
      {
        "id": "oai:arXiv.org:2505.02361v1",
        "title": "Learning simple heuristic rules for classifying materials based on chemical composition",
        "link": "https://arxiv.org/abs/2505.02361",
        "author": "Andrew Ma, Marin Solja\\v{c}i\\'c",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02361v1 Announce Type: cross \nAbstract: In the past decade, there has been a significant interest in the use of machine learning approaches in materials science research. Conventional deep learning approaches that rely on complex, nonlinear models have become increasingly important in computational materials science due to their high predictive accuracy. In contrast to these approaches, we have shown in a recent work that a remarkably simple learned heuristic rule -- based on the concept of topogivity -- can classify whether a material is topological using only its chemical composition. In this paper, we go beyond the topology classification scenario by also studying the use of machine learning to develop simple heuristic rules for classifying whether a material is a metal based on chemical composition. Moreover, we present a framework for incorporating chemistry-informed inductive bias based on the structure of the periodic table. For both the topology classification and the metallicity classification tasks, we empirically characterize the performance of simple heuristic rules fit with and without chemistry-informed inductive bias across a wide range of training set sizes. We find evidence that incorporating chemistry-informed inductive bias can reduce the amount of training data required to reach a given level of test accuracy."
      },
      {
        "id": "oai:arXiv.org:2505.02362v1",
        "title": "Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models",
        "link": "https://arxiv.org/abs/2505.02362",
        "author": "Ghazaleh SHirvani, Saeid Ghasemshirazi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02362v1 Announce Type: cross \nAbstract: Email spam detection is a critical task in modern communication systems, essential for maintaining productivity, security, and user experience. Traditional machine learning and deep learning approaches, while effective in static settings, face significant limitations in adapting to evolving spam tactics, addressing class imbalance, and managing data scarcity. These challenges necessitate innovative approaches that reduce dependency on extensive labeled datasets and frequent retraining. This study investigates the effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced Natural Language Processing (NLP) techniques such as BERT for email spam detection. By employing BERT to preprocess and extract critical information from email content, and FLAN-T5 to classify emails in a Zero-Shot framework, the proposed approach aims to address the limitations of traditional spam detection systems. The integration of FLAN-T5 and BERT enables robust spam detection without relying on extensive labeled datasets or frequent retraining, making it highly adaptable to unseen spam patterns and adversarial environments. This research highlights the potential of leveraging zero-shot learning and NLPs for scalable and efficient spam detection, providing insights into their capability to address the dynamic and challenging nature of spam detection tasks."
      },
      {
        "id": "oai:arXiv.org:2505.02385v1",
        "title": "An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract Segmentation",
        "link": "https://arxiv.org/abs/2505.02385",
        "author": "Lei Xie, Huajun Zhou, Junxiong Huang, Jiahao Huang, Qingrun Zeng, Jianzhong He, Jiawei Zhang, Baohua Fan, Mingchu Li, Guoqiang Xie, Hao Chen, Yuanjing Feng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02385v1 Announce Type: cross \nAbstract: The segmentation of cranial nerves (CNs) tract provides a valuable quantitative tool for the analysis of the morphology and trajectory of individual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which combine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have achieved promising segmentation performance. However, it is laborious or even infeasible to collect complete multimodal data in clinical practice due to limitations in equipment, user privacy, and working conditions. In this work, we propose a novel arbitrary-modal fusion network for volumetric CNs tract segmentation, called CNTSeg-v2, which trains one model to handle different combinations of available modalities. Instead of directly combining all the modalities, we select T1-weighted (T1w) images as the primary modality due to its simplicity in data acquisition and contribution most to the results, which supervises the information selection of other auxiliary modalities. Our model encompasses an Arbitrary-Modal Collaboration Module (ACM) designed to effectively extract informative features from other auxiliary modalities, guided by the supervision of T1w images. Meanwhile, we construct a Deep Distance-guided Multi-stage (DDM) decoder to correct small errors and discontinuities through signed distance maps to improve segmentation accuracy. We evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the clinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental results show that our CNTSeg-v2 achieves state-of-the-art segmentation performance, outperforming all competing methods."
      },
      {
        "id": "oai:arXiv.org:2505.02396v1",
        "title": "Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch",
        "link": "https://arxiv.org/abs/2505.02396",
        "author": "Kennard Norbert Sudiardjo, Islam Nur Alam, Wilson Wijaya, Lili Ayu Wulandhari",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02396v1 Announce Type: cross \nAbstract: Pneumonia Diagnosis, though it is crucial for an effective treatment, it can be hampered by uncertainty. This uncertainty starts to arise due to some factors like atypical presentations, limitations of diagnostic tools such as chest X-rays, and the presence of co-existing respiratory conditions. This research proposes one of the supervised learning methods, CNN. Using MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using Keras API as the built from scratch model, for identifying lung diseases especially pneumonia. The datasets used in this research were obtained from the website through Kaggle. The result shows that by implementing CNN MobileNetV2 and CNN from scratch the result is promising. While validating data, MobileNetV2 performs with stability and minimal overfitting, while the training accuracy increased to 84.87% later it slightly decreased to 78.95%, with increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is more stable. Although it takes more time to train each epoch. Meanwhile, after the 10th epoch, the Scratch model displayed more instability and overfitting despite having higher validation accuracy, training accuracy decreased significantly to 78.12% and the validation loss increased from 0.5698 to 1.1809. With these results, ResNet101V2 offers stability, and the Scratch model offers high accuracy."
      },
      {
        "id": "oai:arXiv.org:2505.02405v1",
        "title": "Estimating Commonsense Scene Composition on Belief Scene Graphs",
        "link": "https://arxiv.org/abs/2505.02405",
        "author": "Mario A. V. Saucedo, Vignesh Kottayam Viswanathan, Christoforos Kanellakis, George Nikolakopoulos",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02405v1 Announce Type: cross \nAbstract: This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types."
      },
      {
        "id": "oai:arXiv.org:2505.02439v1",
        "title": "ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.02439",
        "author": "Yang Deng, Yaohui Liu, Rui Liang, Dafang Zhao, Donghua Xie, Ittetsu Taniguchi, Dan Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02439v1 Announce Type: cross \nAbstract: The building thermodynamics model, which predicts real-time indoor temperature changes under potential HVAC (Heating, Ventilation, and Air Conditioning) control operations, is crucial for optimizing HVAC control in buildings. While pioneering studies have attempted to develop such models for various building environments, these models often require extensive data collection periods and rely heavily on expert knowledge, making the modeling process inefficient and limiting the reusability of the models. This paper explores a model ensemble perspective that utilizes existing developed models as base models to serve a target building environment, thereby providing accurate predictions while reducing the associated efforts. Given that building data streams are non-stationary and the number of base models may increase, we propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically select and weight the base models. Our approach employs a two-tiered decision-making process: the high-level focuses on model selection, while the low-level determines the weights of the selected models. We thoroughly evaluate the proposed approach through offline experiments and an on-site case study, and the experimental results demonstrate the effectiveness of our method."
      },
      {
        "id": "oai:arXiv.org:2505.02462v1",
        "title": "Incentivizing Inclusive Contributions in Model Sharing Markets",
        "link": "https://arxiv.org/abs/2505.02462",
        "author": "Enpei Zhang, Jingyi Chai, Rui Ye, Yanfeng Wang, Siheng Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02462v1 Announce Type: cross \nAbstract: While data plays a crucial role in training contemporary AI models, it is acknowledged that valuable public data will be exhausted in a few years, directing the world's attention towards the massive decentralized private data. However, the privacy-sensitive nature of raw data and lack of incentive mechanism prevent these valuable data from being fully exploited. Addressing these challenges, this paper proposes inclusive and incentivized personalized federated learning (iPFL), which incentivizes data holders with diverse purposes to collaboratively train personalized models without revealing raw data. iPFL constructs a model-sharing market by solving a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL adheres to two key incentive properties: individual rationality and truthfulness. Empirical studies on eleven AI tasks (e.g., large language models' instruction-following tasks) demonstrate that iPFL consistently achieves the highest economic utility, and better or comparable model performance compared to baseline methods. We anticipate that our iPFL can serve as a valuable technique for boosting future AI models on decentralized private data while making everyone satisfied."
      },
      {
        "id": "oai:arXiv.org:2505.02470v1",
        "title": "Deep learning of personalized priors from past MRI scans enables fast, quality-enhanced point-of-care MRI with low-cost systems",
        "link": "https://arxiv.org/abs/2505.02470",
        "author": "Tal Oved, Beatrice Lena, Chlo\\'e F. Najac, Sheng Shen, Matthew S. Rosen, Andrew Webb, Efrat Shimron",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02470v1 Announce Type: cross \nAbstract: Magnetic resonance imaging (MRI) offers superb-quality images, but its accessibility is limited by high costs, posing challenges for patients requiring longitudinal care. Low-field MRI provides affordable imaging with low-cost devices but is hindered by long scans and degraded image quality, including low signal-to-noise ratio (SNR) and tissue contrast. We propose a novel healthcare paradigm: using deep learning to extract personalized features from past standard high-field MRI scans and harnessing them to enable accelerated, enhanced-quality follow-up scans with low-cost systems. To overcome the SNR and contrast differences, we introduce ViT-Fuser, a feature-fusion vision transformer that learns features from past scans, e.g. those stored in standard DICOM CDs. We show that \\textit{a single prior scan is sufficient}, and this scan can come from various MRI vendors, field strengths, and pulse sequences. Experiments with four datasets, including glioblastoma data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality images from accelerated low-field scans, with robustness to out-of-distribution data. Our freely available framework thus enables rapid, diagnostic-quality, low-cost imaging for wide healthcare applications."
      },
      {
        "id": "oai:arXiv.org:2505.02476v1",
        "title": "Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation",
        "link": "https://arxiv.org/abs/2505.02476",
        "author": "Hubert Padusinski, Christian Steinhauser, Christian Scherl, Julian Gaal, Jacob Langner",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02476v1 Announce Type: cross \nAbstract: The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection."
      },
      {
        "id": "oai:arXiv.org:2505.02484v1",
        "title": "El Agente: An Autonomous Agent for Quantum Chemistry",
        "link": "https://arxiv.org/abs/2505.02484",
        "author": "Yunheng Zou, Austin H. Cheng, Abdulrahman Aldossary, Jiaru Bai, Shi Xuan Leong, Jorge Arturo Campos-Gonzalez-Angulo, Changhyeok Choi, Cher Tian Ser, Gary Tom, Andrew Wang, Zijian Zhang, Ilya Yakavets, Han Hao, Chris Crebolder, Varinia Bernales, Al\\'an Aspuru-Guzik",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02484v1 Announce Type: cross \nAbstract: Computational chemistry tools are widely used to study the behaviour of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry."
      },
      {
        "id": "oai:arXiv.org:2505.02508v1",
        "title": "Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces",
        "link": "https://arxiv.org/abs/2505.02508",
        "author": "Yang Lyu, Yuchun Qian, Tan Minh Nguyen, Xin T. Tong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02508v1 Announce Type: cross \nAbstract: Diffusion models is a popular computational tool to generate new data samples. It utilizes a forward diffusion process that add noise to the data distribution and then use a reverse process to remove noises to produce samples from the data distribution. However, when the empirical data distribution consists of $n$ data point, using the empirical diffusion model will necessarily produce one of the existing data points. This is often referred to as the memorization effect, which is usually resolved by sophisticated machine learning procedures in the current literature. This work shows that the memorization problem can be resolved by a simple inertia update step at the end of the empirical diffusion model simulation. Our inertial diffusion model requires only the empirical diffusion model score function and it does not require any further training. We show that choosing the inertia diffusion model sample distribution is an $O\\left(n^{-\\frac{2}{d+4}}\\right)$ Wasserstein-1 approximation of a data distribution lying on a $C^2$ manifold of dimension $d$. Since this estimate is significant smaller the Wasserstein1 distance between population and empirical distributions, it rigorously shows the inertial diffusion model produces new data samples. Remarkably, this upper bound is completely free of the ambient space dimension, since there is no training involved. Our analysis utilizes the fact that the inertial diffusion model samples are approximately distributed as the Gaussian kernel density estimator on the manifold. This reveals an interesting connection between diffusion model and manifold learning."
      },
      {
        "id": "oai:arXiv.org:2505.02516v1",
        "title": "Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics",
        "link": "https://arxiv.org/abs/2505.02516",
        "author": "MohammadAli Shaeri, Jinhan Liu, Mahsa Shoaran",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02516v1 Announce Type: cross \nAbstract: Advanced neural interfaces are transforming applications ranging from neuroscience research to diagnostic tools (for mental state recognition, tremor and seizure detection) as well as prosthetic devices (for motor and communication recovery). By integrating complex functions into miniaturized neural devices, these systems unlock significant opportunities for personalized assistive technologies and adaptive therapeutic interventions. Leveraging high-density neural recordings, on-site signal processing, and machine learning (ML), these interfaces extract critical features, identify disease neuro-markers, and enable accurate, low-latency neural decoding. This integration facilitates real-time interpretation of neural signals, adaptive modulation of brain activity, and efficient control of assistive devices. Moreover, the synergy between neural interfaces and ML has paved the way for self-sufficient, ubiquitous platforms capable of operating in diverse environments with minimal hardware costs and external dependencies. In this work, we review recent advancements in AI-driven decoding algorithms and energy-efficient System-on-Chip (SoC) platforms for next-generation miniaturized neural devices. These innovations highlight the potential for developing intelligent neural interfaces, addressing critical challenges in scalability, reliability, interpretability, and user adaptability."
      },
      {
        "id": "oai:arXiv.org:2505.02574v1",
        "title": "Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control",
        "link": "https://arxiv.org/abs/2505.02574",
        "author": "Robin Arbaud, Elisa Motta, Marco Domenico Avaro, Stefano Picinich, Marta Lorenzini, Arash Ajoudani",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02574v1 Announce Type: cross \nAbstract: Partial hand amputations significantly affect the physical and psychosocial well-being of individuals, yet intuitive control of externally powered prostheses remains an open challenge. To address this gap, we developed a force-controlled prosthetic finger activated by electromyography (EMG) signals. The prototype, constructed around a wrist brace, functions as a supernumerary finger placed near the index, allowing for early-stage evaluation on unimpaired subjects. A neural network-based model was then implemented to estimate fingertip forces from EMG inputs, allowing for online adjustment of the prosthetic finger grip strength. The force estimation model was validated through experiments with ten participants, demonstrating its effectiveness in predicting forces. Additionally, online trials with four users wearing the prosthesis exhibited precise control over the device. Our findings highlight the potential of using EMG-based force estimation to enhance the functionality of prosthetic fingers."
      },
      {
        "id": "oai:arXiv.org:2505.02576v1",
        "title": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning",
        "link": "https://arxiv.org/abs/2505.02576",
        "author": "Sergio Hern\\'andez-Guti\\'errez, Minttu Alakuijala, Alexander V. Nikitin, Pekka Marttinen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02576v1 Announce Type: cross \nAbstract: Reasoning tasks are crucial in many domains, especially in science and engineering. Although large language models (LLMs) have made progress in reasoning tasks using techniques such as chain-of-thought and least-to-most prompting, these approaches still do not effectively scale to complex problems in either their performance or execution time. Moreover, they often require additional supervision for each new task, such as in-context examples. In this work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable divide-and-conquer method for solving reasoning problems that requires less supervision than prior approaches. Our method can be directly applied to a new problem class even in the absence of any task-specific guidance. Furthermore, RDD supports sub-task dependencies, allowing for ordered execution of sub-tasks, as well as an error recovery mechanism that can correct mistakes made in previous steps. We evaluate our approach on two benchmarks with six difficulty levels each and in two in-context settings: one with task-specific examples and one without. Our results demonstrate that RDD outperforms other methods in a compute-matched setting as task complexity increases, while also being more computationally efficient."
      },
      {
        "id": "oai:arXiv.org:2505.02613v1",
        "title": "Lane-Wise Highway Anomaly Detection",
        "link": "https://arxiv.org/abs/2505.02613",
        "author": "Mei Qiu, William Lorenz Reindl, Yaobin Chen, Stanley Chien, Shu Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02613v1 Announce Type: cross \nAbstract: This paper proposes a scalable and interpretable framework for lane-wise highway traffic anomaly detection, leveraging multi-modal time series data extracted from surveillance cameras. Unlike traditional sensor-dependent methods, our approach uses AI-powered vision models to extract lane-specific features, including vehicle count, occupancy, and truck percentage, without relying on costly hardware or complex road modeling. We introduce a novel dataset containing 73,139 lane-wise samples, annotated with four classes of expert-validated anomalies: three traffic-related anomalies (lane blockage and recovery, foreign object intrusion, and sustained congestion) and one sensor-related anomaly (camera angle shift). Our multi-branch detection system integrates deep learning, rule-based logic, and machine learning to improve robustness and precision. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods in precision, recall, and F1-score, providing a cost-effective and scalable solution for real-world intelligent transportation systems."
      },
      {
        "id": "oai:arXiv.org:2505.02614v1",
        "title": "Entropic Mirror Descent for Linear Systems: Polyak's Stepsize and Implicit Bias",
        "link": "https://arxiv.org/abs/2505.02614",
        "author": "Yura Malitsky, Alexander Posch",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02614v1 Announce Type: cross \nAbstract: This paper focuses on applying entropic mirror descent to solve linear systems, where the main challenge for the convergence analysis stems from the unboundedness of the domain. To overcome this without imposing restrictive assumptions, we introduce a variant of Polyak-type stepsizes. Along the way, we strengthen the bound for $\\ell_1$-norm implicit bias, obtain sublinear and linear convergence results, and generalize the convergence result to arbitrary convex $L$-smooth functions. We also propose an alternative method that avoids exponentiation, resembling the original Hadamard descent, but with provable convergence."
      },
      {
        "id": "oai:arXiv.org:2505.02628v1",
        "title": "DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction",
        "link": "https://arxiv.org/abs/2505.02628",
        "author": "Yiqun Lin, Hualiang Wang, Jixiang Chen, Jiewen Yang, Jiarong Guo, Xiaomeng Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02628v1 Announce Type: cross \nAbstract: Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in the medical field, while the high radiation exposure required for high-quality imaging raises significant concerns, particularly for vulnerable populations. Sparse-view reconstruction reduces radiation by using fewer X-ray projections while maintaining image quality, yet existing methods face challenges such as high computational demands and poor generalizability to different datasets. To overcome these limitations, we propose DeepSparse, the first foundation model for sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional Cross-Scale Embedding), a novel network that integrates multi-view 2D features and multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View Sampling Pretraining) framework, which pretrains the model on large datasets with both sparse-view and dense-view projections, and a two-step finetuning strategy to adapt and refine the model for new datasets. Extensive experiments and ablation studies demonstrate that our proposed DeepSparse achieves superior reconstruction quality compared to state-of-the-art methods, paving the way for safer and more efficient CBCT imaging."
      },
      {
        "id": "oai:arXiv.org:2505.02649v1",
        "title": "Eye Movements as Indicators of Deception: A Machine Learning Approach",
        "link": "https://arxiv.org/abs/2505.02649",
        "author": "Valentin Foucher, Santiago de Leon-Martinez, Robert Moro",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02649v1 Announce Type: cross \nAbstract: Gaze may enhance the robustness of lie detectors but remains under-studied. This study evaluated the efficacy of AI models (using fixations, saccades, blinks, and pupil size) for detecting deception in Concealed Information Tests across two datasets. The first, collected with Eyelink 1000, contains gaze data from a computerized experiment where 87 participants revealed, concealed, or faked the value of a previously selected card. The second, collected with Pupil Neon, involved 36 participants performing a similar task but facing an experimenter. XGBoost achieved accuracies up to 74% in a binary classification task (Revealing vs. Concealing) and 49% in a more challenging three-classification task (Revealing vs. Concealing vs. Faking). Feature analysis identified saccade number, duration, amplitude, and maximum pupil size as the most important for deception prediction. These results demonstrate the feasibility of using gaze and AI to enhance lie detectors and encourage future research that may improve on this."
      },
      {
        "id": "oai:arXiv.org:2505.02664v1",
        "title": "Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter",
        "link": "https://arxiv.org/abs/2505.02664",
        "author": "Ali Rashidi Moghadam, Sayedmohammadreza Rastegari, Mehdi Tale Masouleh, Ahmad Kalhor",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02664v1 Announce Type: cross \nAbstract: Grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of Graph Neural Networks for efficient geometric reasoning from point cloud data. Building on the success of GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to efficiently produce 7-Dof grasp candidates. Candidates are assessed with an ensemble Graph Neural Network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). This improved representation boosts grasp detection performance over previous methods using the same generator. GtG 2.0 shows up to a 35% improvement in Average Precision on the GraspNet-1Billion benchmark compared to hypothesis-and-test and Graph Neural Network-based methods, ranking it among the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and Kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability."
      },
      {
        "id": "oai:arXiv.org:2505.02693v1",
        "title": "Predicting Movie Hits Before They Happen with LLMs",
        "link": "https://arxiv.org/abs/2505.02693",
        "author": "Shaghayegh Agah, Yejin Kim, Neeraj Sharma, Mayur Nankani, Kevin Foley, H. Howie Huang, Sardar Hamidian",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02693v1 Announce Type: cross \nAbstract: Addressing the cold-start issue in content recommendation remains a critical ongoing challenge. In this work, we focus on tackling the cold-start problem for movies on a large entertainment platform. Our primary goal is to forecast the popularity of cold-start movies using Large Language Models (LLMs) leveraging movie metadata. This method could be integrated into retrieval systems within the personalization pipeline or could be adopted as a tool for editorial teams to ensure fair promotion of potentially overlooked movies that may be missed by traditional or algorithmic solutions. Our study validates the effectiveness of this approach compared to established baselines and those we developed."
      },
      {
        "id": "oai:arXiv.org:2505.02705v1",
        "title": "Multi-View Learning with Context-Guided Receptance for Image Denoising",
        "link": "https://arxiv.org/abs/2505.02705",
        "author": "Binghong Chen, Tingting Chai, Wei Jiang, Yuanrong Xu, Guanglu Zhou, Xiangqian Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02705v1 Announce Type: cross \nAbstract: Image denoising is essential in low-level vision applications such as photography and automated driving. Existing methods struggle with distinguishing complex noise patterns in real-world scenes and consume significant computational resources due to reliance on Transformer-based models. In this work, the Context-guided Receptance Weighted Key-Value (\\M) model is proposed, combining enhanced multi-view feature integration with efficient sequence modeling. Our approach introduces the Context-guided Token Shift (CTS) paradigm, which effectively captures local spatial dependencies and enhance the model's ability to model real-world noise distributions. Additionally, the Frequency Mix (FMix) module extracting frequency-domain features is designed to isolate noise in high-frequency spectra, and is integrated with spatial representations through a multi-view learning process. To improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is adopted, enabling full pixel-sequence interaction with linear complexity while overcoming the causal selection constraints. The model is validated on multiple real-world image denoising datasets, outperforming the existing state-of-the-art methods quantitatively and reducing inference time up to 40\\%. Qualitative results further demonstrate the ability of our model to restore fine details in various scenes."
      },
      {
        "id": "oai:arXiv.org:2505.02707v1",
        "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play",
        "link": "https://arxiv.org/abs/2505.02707",
        "author": "Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, Zhiting Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02707v1 Announce Type: cross \nAbstract: A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions."
      },
      {
        "id": "oai:arXiv.org:2505.02709v1",
        "title": "Technical Report: Evaluating Goal Drift in Language Model Agents",
        "link": "https://arxiv.org/abs/2505.02709",
        "author": "Rauno Arike, Elizabeth Donoway, Henning Bartsch, Marius Hobbhahn",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02709v1 Announce Type: cross \nAbstract: As language models (LMs) are increasingly deployed as autonomous agents, their robust adherence to human-assigned objectives becomes crucial for safe operation. When these agents operate independently for extended periods without human oversight, even initially well-specified goals may gradually shift. Detecting and measuring goal drift - an agent's tendency to deviate from its original objective over time - presents significant challenges, as goals can shift gradually, causing only subtle behavioral changes. This paper proposes a novel approach to analyzing goal drift in LM agents. In our experiments, agents are first explicitly given a goal through their system prompt, then exposed to competing objectives through environmental pressures. We demonstrate that while the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains nearly perfect goal adherence for more than 100,000 tokens in our most difficult evaluation setting, all evaluated models exhibit some degree of goal drift. We also find that goal drift correlates with models' increasing susceptibility to pattern-matching behaviors as the context length grows."
      },
      {
        "id": "oai:arXiv.org:2505.02722v1",
        "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry",
        "link": "https://arxiv.org/abs/2505.02722",
        "author": "Junu Kim, Chaeeun Shim, Sungjin Park, Su Yeon Lee, Gee Young Suh, Chae-Man Lim, Seong Jin Choi, Song Mi Moon, Kyoung-Ho Song, Eu Suk Kim, Hong Bin Kim, Sejoong Kim, Chami Im, Dong-Wan Kang, Yong Soo Kim, Hee-Joon Bae, Sung Yoon Lim, Han-Gil Jeong, Edward Choi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02722v1 Announce Type: cross \nAbstract: Although large language models (LLMs) have demonstrated impressive reasoning capabilities across general domains, their effectiveness in real-world clinical practice remains limited. This is likely due to their insufficient exposure to real-world clinical data during training, as such data is typically not included due to privacy concerns. To address this, we propose enhancing the clinical reasoning capabilities of LLMs by leveraging real-world clinical data. We constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set, as evidenced by both quantitative metrics and expert evaluations. Furthermore, its enhanced reasoning capabilities generalized to a sepsis dataset involving different tasks and patient cohorts, an open-ended consultations on antibiotics use task, and other diseases. Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models."
      },
      {
        "id": "oai:arXiv.org:2505.02735v1",
        "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models",
        "link": "https://arxiv.org/abs/2505.02735",
        "author": "Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02735v1 Announce Type: cross \nAbstract: Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning."
      },
      {
        "id": "oai:arXiv.org:2505.02796v1",
        "title": "Adaptive Bidding Policies for First-Price Auctions with Budget Constraints under Non-stationarity",
        "link": "https://arxiv.org/abs/2505.02796",
        "author": "Yige Wang, Jiashuo Jiang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02796v1 Announce Type: cross \nAbstract: We study how a budget-constrained bidder should learn to adaptively bid in repeated first-price auctions to maximize her cumulative payoff. This problem arose due to an industry-wide shift from second-price auctions to first-price auctions in display advertising recently, which renders truthful bidding (i.e., always bidding one's private value) no longer optimal. We propose a simple dual-gradient-descent-based bidding policy that maintains a dual variable for budget constraint as the bidder consumes her budget. In analysis, we consider two settings regarding the bidder's knowledge of her private values in the future: (i) an uninformative setting where all the distributional knowledge (can be non-stationary) is entirely unknown to the bidder, and (ii) an informative setting where a prediction of the budget allocation in advance. We characterize the performance loss (or regret) relative to an optimal policy with complete information on the stochasticity. For uninformative setting, We show that the regret is \\tilde{O}(\\sqrt{T}) plus a variation term that reflects the non-stationarity of the value distributions, and this is of optimal order. We then show that we can get rid of the variation term with the help of the prediction; specifically, the regret is \\tilde{O}(\\sqrt{T}) plus the prediction error term in the informative setting."
      },
      {
        "id": "oai:arXiv.org:2505.02811v1",
        "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
        "link": "https://arxiv.org/abs/2505.02811",
        "author": "Diji Yang, Linda Zeng, Jinmeng Rao, Yi Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02811v1 Announce Type: cross \nAbstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data."
      },
      {
        "id": "oai:arXiv.org:2505.02820v1",
        "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
        "link": "https://arxiv.org/abs/2505.02820",
        "author": "Hao Zhu, Phil Cuvin, Xinkai Yu, Charlotte Ka Yee Yan, Jason Zhang, Diyi Yang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02820v1 Announce Type: cross \nAbstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., \"If you find that the button is disabled, don't click it again\", or \"This agent has too much autonomy to decide what to do on its own\", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: \"coverage\" and \"redundancy\". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents."
      },
      {
        "id": "oai:arXiv.org:2505.02833v1",
        "title": "TWIST: Teleoperated Whole-Body Imitation System",
        "link": "https://arxiv.org/abs/2505.02833",
        "author": "Yanjie Ze, Zixuan Chen, Jo\\~ao Pedro Ara\\'ujo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, C. Karen Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02833v1 Announce Type: cross \nAbstract: Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io"
      },
      {
        "id": "oai:arXiv.org:2008.06255v4",
        "title": "From Attack to Protection: Leveraging Watermarking Attack Network for Advanced Add-on Watermarking",
        "link": "https://arxiv.org/abs/2008.06255",
        "author": "Seung-Hun Nam, Jihyeon Kang, Daesik Kim, Namhyuk Ahn, Wonhyuk Ahn",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2008.06255v4 Announce Type: replace \nAbstract: Multi-bit watermarking (MW) has been designed to enhance resistance against watermarking attacks, such as signal processing operations and geometric distortions. Various benchmark tools exist to assess this robustness through simulated attacks on watermarked images. However, these tools often fail to capitalize on the unique attributes of the targeted MW and typically neglect the aspect of visual quality, a critical factor in practical applications. To overcome these shortcomings, we introduce a watermarking attack network (WAN), a fully trainable watermarking benchmark tool designed to exploit vulnerabilities within MW systems and induce watermark bit inversions, significantly diminishing watermark extractability. The proposed WAN employs an architecture based on residual dense blocks, which is adept at both local and global feature learning, thereby maintaining high visual quality while obstructing the extraction of embedded information. Our empirical results demonstrate that the WAN effectively undermines various block-based MW systems while minimizing visual degradation caused by attacks. This is facilitated by our novel watermarking attack loss, which is specifically crafted to compromise these systems. The WAN functions not only as a benchmarking tool but also as an add-on watermarking (AoW) mechanism, augmenting established universal watermarking schemes by enhancing robustness or imperceptibility without requiring detailed method context and adapting to dynamic watermarking requirements. Extensive experimental results show that AoW complements the performance of the targeted MW system by independently enhancing both imperceptibility and robustness."
      },
      {
        "id": "oai:arXiv.org:2302.06375v4",
        "title": "One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data",
        "link": "https://arxiv.org/abs/2302.06375",
        "author": "Simone Luetto, Fabrizio Garuti, Enver Sangineto, Lorenzo Forni, Rita Cucchiara",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2302.06375v4 Announce Type: replace \nAbstract: There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function."
      },
      {
        "id": "oai:arXiv.org:2302.09327v2",
        "title": "Transformadores: Fundamentos teoricos y Aplicaciones",
        "link": "https://arxiv.org/abs/2302.09327",
        "author": "Jordi de la Torre",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2302.09327v2 Announce Type: replace \nAbstract: Transformers are a neural network architecture originally developed for natural language processing, which have since become a foundational tool for solving a wide range of problems, including text, audio, image processing, reinforcement learning, and other tasks involving heterogeneous input data. Their hallmark is the self-attention mechanism, which allows the model to weigh different parts of the input sequence dynamically, and is an evolution of earlier attention-based approaches. This article provides readers with the necessary background to understand recent research on transformer models, and presents the mathematical and algorithmic foundations of their core components. It also explores the architecture's various elements, potential modifications, and some of the most relevant applications. The article is written in Spanish to help make this scientific knowledge more accessible to the Spanish-speaking community."
      },
      {
        "id": "oai:arXiv.org:2306.15546v3",
        "title": "When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions",
        "link": "https://arxiv.org/abs/2306.15546",
        "author": "Weiming Zhuang, Chen Chen, Jingtao Li, Chaochao Chen, Yaochu Jin, Lingjuan Lyu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2306.15546v3 Announce Type: replace \nAbstract: The intersection of Foundation Model (FM) and Federated Learning (FL) presents a unique opportunity to unlock new possibilities for real-world applications. On the one hand, FL, as a collaborative learning paradigm, help address challenges in FM development by expanding data availability, enabling computation sharing, facilitating the collaborative development of FMs, tackling continuous data update, avoiding FM monopoly, response delay and FM service down. On the other hand, FM, equipped with pre-trained knowledge and exceptional performance, can serve as a robust starting point for FL. It can also generate synthetic data to enrich data diversity and enhance overall performance of FL. Meanwhile, FM unlocks new sharing paradigm and multi-task and multi-modality capabilities for FL. By examining the interplay between FL and FM, this paper presents the motivations, challenges, and future directions of empowering FL with FM and empowering FM with FL. We hope that this work provides a good foundation to inspire future research efforts to drive advancements in both fields."
      },
      {
        "id": "oai:arXiv.org:2308.04369v3",
        "title": "SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition",
        "link": "https://arxiv.org/abs/2308.04369",
        "author": "Xiao Wang, Yao Rong, Zongzhen Wu, Lin Zhu, Bo Jiang, Jin Tang, Yonghong Tian",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2308.04369v3 Announce Type: replace \nAbstract: Event camera-based pattern recognition is a newly arising research topic in recent years. Current researchers usually transform the event streams into images, graphs, or voxels, and adopt deep neural networks for event-based classification. Although good performance can be achieved on simple event recognition datasets, however, their results may be still limited due to the following two issues. Firstly, they adopt spatial sparse event streams for recognition only, which may fail to capture the color and detailed texture information well. Secondly, they adopt either Spiking Neural Networks (SNN) for energy-efficient recognition with suboptimal results, or Artificial Neural Networks (ANN) for energy-intensive, high-performance recognition. However, seldom of them consider achieving a balance between these two aspects. In this paper, we formally propose to recognize patterns by fusing RGB frames and event streams simultaneously and propose a new RGB frame-event recognition framework to address the aforementioned issues. The proposed method contains four main modules, i.e., memory support Transformer network for RGB frame encoding, spiking neural network for raw event stream encoding, multi-modal bottleneck fusion module for RGB-Event feature aggregation, and prediction head. Due to the scarce of RGB-Event based classification dataset, we also propose a large-scale PokerEvent dataset which contains 114 classes, and 27102 frame-event pairs recorded using a DVS346 event camera. Extensive experiments on two RGB-Event based classification datasets fully validated the effectiveness of our proposed framework. We hope this work will boost the development of pattern recognition by fusing RGB frames and event streams. Both our dataset and source code of this work will be released at https://github.com/Event-AHU/SSTFormer"
      },
      {
        "id": "oai:arXiv.org:2309.15670v2",
        "title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
        "link": "https://arxiv.org/abs/2309.15670",
        "author": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty, Aulia Luqman Aziz, Mohammed Aljuaid, Fazla Rabby, Rohit Bansal",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2309.15670v2 Announce Type: replace \nAbstract: In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT), a well-known methodology of transformers, have been shown the best results of all methods implemented. Finally, a web application has been developed to demonstrate the performance of the pre-trained top-performer model (BERT) for multi-label ER in Bangla."
      },
      {
        "id": "oai:arXiv.org:2310.01770v4",
        "title": "A simple connection from loss flatness to compressed neural representations",
        "link": "https://arxiv.org/abs/2310.01770",
        "author": "Shirui Chen, Stefano Recanatesi, Eric Shea-Brown",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2310.01770v4 Announce Type: replace \nAbstract: Sharpness, a geometric measure in the parameter space that reflects the flatness of the loss landscape, has long been studied for its potential connections to neural network behavior. While sharpness is often associated with generalization, recent work highlights inconsistencies in this relationship, leaving its true significance unclear. In this paper, we investigate how sharpness influences the local geometric features of neural representations in feature space, offering a new perspective on its role. We introduce this problem and study three measures for compression: the Local Volumetric Ratio (LVR), based on volume compression, the Maximum Local Sensitivity (MLS), based on sensitivity to input changes, and the Local Dimensionality, based on how uniform the sensitivity is on different directions. We show that LVR and MLS correlate with the flatness of the loss around the local minima; and that this correlation is predicted by a relatively simple mathematical relationship: a flatter loss corresponds to a lower upper bound on the compression metrics of neural representations. Our work builds upon the linear stability insight by Ma and Ying, deriving inequalities between various compression metrics and quantities involving sharpness. Our inequalities readily extend to reparametrization-invariant sharpness as well. Through empirical experiments on various feedforward, convolutional, and transformer architectures, we find that our inequalities predict a consistently positive correlation between local representation compression and sharpness."
      },
      {
        "id": "oai:arXiv.org:2310.06417v2",
        "title": "Supercharging Graph Transformers with Advective Diffusion",
        "link": "https://arxiv.org/abs/2310.06417",
        "author": "Qitian Wu, Chenxiao Yang, Kaipeng Zeng, Michael Bronstein",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2310.06417v2 Announce Type: replace \nAbstract: The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions."
      },
      {
        "id": "oai:arXiv.org:2401.00688v2",
        "title": "Inference and Visualization of Community Structure in Attributed Hypergraphs Using Mixed-Membership Stochastic Block Models",
        "link": "https://arxiv.org/abs/2401.00688",
        "author": "Kazuki Nakajima, Takeaki Uno",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2401.00688v2 Announce Type: replace \nAbstract: Hypergraphs represent complex systems involving interactions among more than two entities and allow the investigation of higher-order structure and dynamics in complex systems. Node attribute data, which often accompanies network data, can enhance the inference of community structure in complex systems. While mixed-membership stochastic block models have been employed to infer community structure in hypergraphs, they complicate the visualization and interpretation of inferred community structure by assuming that nodes may possess soft community memberships. In this study, we propose a framework, HyperNEO, that combines mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods. Our approach generates a node layout that largely preserves the community memberships of nodes. We evaluate our framework on both synthetic and empirical hypergraphs with node attributes. We expect our framework will broaden the investigation and understanding of higher-order community structure in complex systems."
      },
      {
        "id": "oai:arXiv.org:2402.01685v3",
        "title": "SMUTF: Schema Matching Using Generative Tags and Hybrid Features",
        "link": "https://arxiv.org/abs/2402.01685",
        "author": "Yu Zhang, Mei Di, Haozheng Luo, Chenwei Xu, Richard Tzong-Han Tsai",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2402.01685v3 Announce Type: replace \nAbstract: We introduce SMUTF (Schema Matching Using Generative Tags and Hybrid Features), a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy \"generative tags\" for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.\n  Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated exceptional performance, surpassing existing state-of-the-art models in terms of accuracy and efficiency, and improving the F1 score by 11.84% and the AUC of ROC by 5.08%. Code is available at https://github.com/fireindark707/Python-Schema-Matching."
      },
      {
        "id": "oai:arXiv.org:2402.07002v2",
        "title": "Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off",
        "link": "https://arxiv.org/abs/2402.07002",
        "author": "Yuecheng Li, Lele Fu, Tong Wang, Jian Lou, Bin Chen, Lei Yang, Jian Shen, Zibin Zheng, Chuan Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2402.07002v2 Announce Type: replace \nAbstract: To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This capability implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by order of $\\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative datasets and observe significant performance improvements and strict privacy guarantees under different privacy settings. The code is available at https://github.com/6lyc/FedCEO_Collaborate-with-Each-Other."
      },
      {
        "id": "oai:arXiv.org:2402.15290v4",
        "title": "Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization",
        "link": "https://arxiv.org/abs/2402.15290",
        "author": "Tongyi Liang, Han-Xiong Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2402.15290v4 Announce Type: replace \nAbstract: Existing models encounter bottlenecks in balancing performance and computational efficiency when modeling long sequences. Although the state space model (SSM) has achieved remarkable success in handling long sequence tasks, it still faces the problem of large number of parameters. In order to further improve the efficiency of SSM, we propose a new state space layer based on multiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is built on the convolutional representation of multi-input and multi-input (MIMO) SSM. We propose a variety of effective strategies to improve the computational efficiency. The diagonalization of the system matrix first decouples the original system. Then a fast tensor convolution is proposed based on the fast Fourier transform. In addition, the block diagonalization of the SSM further reduces the model parameters and improves the model flexibility. Extensive experimental results show that the performance of the proposed model on multiple databases matches the performance of state-of-the-art models, such as S4, and is significantly better than Transformers and LSTM. In the model efficiency benchmark, the parameters of eSSM are only 12.89\\% of LSTM and 13.24\\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and 1.35 times faster than Mamba. Code is available at: \\href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}."
      },
      {
        "id": "oai:arXiv.org:2402.16310v4",
        "title": "REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories",
        "link": "https://arxiv.org/abs/2402.16310",
        "author": "Bangchao Deng, Bingqing Qu, Pengyang Wang, Dingqi Yang, Benjamin Fankhauser, Philippe Cudre-Mauroux",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2402.16310v4 Announce Type: replace \nAbstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\\%-10.5\\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities."
      },
      {
        "id": "oai:arXiv.org:2403.01954v4",
        "title": "DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation",
        "link": "https://arxiv.org/abs/2403.01954",
        "author": "Chen Xu, Tian Lan, Yu Ji, Changlong Yu, Wei Wang, Jun Gao, Qunxi Dong, Kun Qian, Piji Li, Wei Bi, Bin Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2403.01954v4 Announce Type: replace \nAbstract: Constrained decoding approaches aim to control the meaning or style of text generated by the pre-trained large language models (LLMs or also PLMs) for various tasks at inference time. However, these methods often guide plausible continuations by greedily and explicitly selecting targets. Though fulfilling the task requirements, these methods may overlook certain general and natural logics that humans would implicitly follow towards such targets. Inspired by cognitive dual-process theory, in this work, we propose a novel decoding framework DECIDER where the base LLMs are equipped with a First-Order Logic (FOL) reasoner to express and evaluate the rules, along with a decision function that merges the outputs of both systems to guide the generation. Unlike previous constrained decodings, DECIDER transforms the encouragement of target-specific words into all words that satisfy several high-level rules, enabling us to programmatically integrate our logic into LLMs. Experiments on CommonGen and PersonaChat demonstrate that DECIDER effectively follows given FOL rules to guide LLMs in a more human-like and logic-controlled manner."
      },
      {
        "id": "oai:arXiv.org:2403.06432v2",
        "title": "Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain",
        "link": "https://arxiv.org/abs/2403.06432",
        "author": "Jungwon Choi, Hyungi Lee, Byung-Hoon Kim, Juho Lee",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2403.06432v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructing dynamic graphs, which enables the learning of higher-level semantic representations considering temporal perspectives, addressing the challenges in fMRI data representation learning. Utilizing the large-scale UK Biobank dataset for self-supervised learning, ST-JEMA shows exceptional representation learning performance on dynamic functional connectivity demonstrating superiority over previous methods in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI datasets even with limited samples and effectiveness of temporal reconstruction on missing data scenarios. These findings highlight the potential of our approach as a robust representation learning method for leveraging label-scarce fMRI data."
      },
      {
        "id": "oai:arXiv.org:2403.06869v3",
        "title": "Impact of Noisy Supervision in Foundation Model Learning",
        "link": "https://arxiv.org/abs/2403.06869",
        "author": "Hao Chen, Zihan Wang, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj, Jindong Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2403.06869v3 Announce Type: replace \nAbstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning."
      },
      {
        "id": "oai:arXiv.org:2404.00570v2",
        "title": "ParaICL: Towards Parallel In-Context Learning",
        "link": "https://arxiv.org/abs/2404.00570",
        "author": "Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, Lidong Bing",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2404.00570v2 Announce Type: replace \nAbstract: Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods."
      },
      {
        "id": "oai:arXiv.org:2404.01249v3",
        "title": "FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Matching",
        "link": "https://arxiv.org/abs/2404.01249",
        "author": "Rohit Jena, Pratik Chaudhari, James C. Gee",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2404.01249v3 Announce Type: replace \nAbstract: The paper proposes FireANTs, the first multi-scale Adaptive Riemannian Optimization algorithm for dense diffeomorphic image matching. One of the most critical and understudied aspects of diffeomorphic image matching algorithms are its highly ill-conditioned nature. We quantitatively capture the extent of ill-conditioning in a typical MRI matching task, motivating the need for an adaptive optimization algorithm for diffeomorphic matching. To this end, FireANTs generalizes the concept of momentum and adaptive estimates of the Hessian to mitigate this ill-conditioning in the non-Euclidean space of diffeomorphisms. Unlike common non-Euclidean manifolds, we also formalize considerations for multi-scale optimization of diffeomorphisms. Our rigorous mathematical results and operational contributions lead to a state-of-the-art dense matching algorithm that can be applied to generic image data with remarkable accuracy and robustness. We demonstrate consistent improvements in image matching performance across a spectrum of community-standard medical and biological correspondence matching challenges spanning a wide variety of image modalities, anatomies, resolutions, acquisition protocols, and preprocessing pipelines. This improvement is supplemented by 300x to 3200x speedup over existing CPU-based state-of-the-art algorithms. For the first time, we perform diffeomorphic matching of sub-micron mouse isocortex volumes at native resolution, and generate a 25{\\mu}m mouse brain atlas in under 25 minutes. Our fast implementation also enables hyperparameter studies that were intractable with existing correspondence matching algorithms."
      },
      {
        "id": "oai:arXiv.org:2404.02225v2",
        "title": "CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement",
        "link": "https://arxiv.org/abs/2404.02225",
        "author": "Di Qiu, Yinda Zhang, Thabo Beeler, Vladimir Tankovich, Christian H\\\"ane, Sean Fanello, Christoph Rhemann, Sergio Orts Escolano",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2404.02225v2 Announce Type: replace \nAbstract: We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines."
      },
      {
        "id": "oai:arXiv.org:2404.02810v3",
        "title": "Generative-Contrastive Heterogeneous Graph Neural Network",
        "link": "https://arxiv.org/abs/2404.02810",
        "author": "Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang, Xindong Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2404.02810v3 Announce Type: replace \nAbstract: Heterogeneous Graphs (HGs) effectively model complex relationships in the real world through multi-type nodes and edges. In recent years, inspired by self-supervised learning (SSL), contrastive learning (CL)-based Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential in utilizing data augmentation and contrastive discriminators for downstream tasks. However, data augmentation remains limited due to the graph data's integrity. Furthermore, the contrastive discriminators suffer from sampling bias and lack local heterogeneous information. To tackle the above limitations, we propose a novel Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN). Specifically, we propose a heterogeneous graph generative learning method that enhances CL-based paradigm. This paradigm includes: 1) A contrastive view augmentation strategy using a masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generating hard negative samples. 3) A hierarchical contrastive learning strategy aimed at capturing local and global information. Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced contrastive discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest baselines on node classification and link prediction tasks."
      },
      {
        "id": "oai:arXiv.org:2404.04748v2",
        "title": "Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind",
        "link": "https://arxiv.org/abs/2404.04748",
        "author": "Hongchuan Zeng, Hongshen Xu, Lu Chen, Kai Yu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2404.04748v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality. Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression. MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets. Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages. We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression. In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques."
      },
      {
        "id": "oai:arXiv.org:2404.10716v3",
        "title": "MOWA: Multiple-in-One Image Warping Model",
        "link": "https://arxiv.org/abs/2404.10716",
        "author": "Kang Liao, Zongsheng Yue, Zhonghua Wu, Chen Change Loy",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2404.10716v3 Announce Type: replace \nAbstract: While recent image warping approaches achieved remarkable success on existing benchmarks, they still require training separate models for each specific task and cannot generalize well to different camera models or customized manipulations. To address diverse types of warping in practice, we propose a Multiple-in-One image WArping model (named MOWA) in this work. Specifically, we mitigate the difficulty of multi-task learning by disentangling the motion estimation at both the region level and pixel level. To further enable dynamic task-aware image warping, we introduce a lightweight point-based classifier that predicts the task type, serving as prompts to modulate the feature maps for more accurate estimation. To our knowledge, this is the first work that solves multiple practical warping tasks in one single model. Extensive experiments demonstrate that our MOWA, which is trained on six tasks for multiple-in-one image warping, outperforms state-of-the-art task-specific models across most tasks. Moreover, MOWA also exhibits promising potential to generalize into unseen scenes, as evidenced by cross-domain and zero-shot evaluations. The code and more visual results can be found on the project page: https://kangliao929.github.io/projects/mowa/."
      },
      {
        "id": "oai:arXiv.org:2405.01101v5",
        "title": "Enhancing person re-identification via Uncertainty Feature Fusion Method and Auto-weighted Measure Combination",
        "link": "https://arxiv.org/abs/2405.01101",
        "author": "Quang-Huy Che, Le-Chuong Nguyen, Duc-Tuan Luu, Vinh-Tiep Nguyen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2405.01101v5 Announce Type: replace \nAbstract: Person re-identification (Re-ID) is a challenging task that involves identifying the same person across different camera views in surveillance systems. Current methods usually rely on features from single-camera views, which can be limiting when dealing with multiple cameras and challenges such as changing viewpoints and occlusions. In this paper, a new approach is introduced that enhances the capability of ReID models through the Uncertain Feature Fusion Method (UFFM) and Auto-weighted Measure Combination (AMC). UFFM generates multi-view features using features extracted independently from multiple images to mitigate view bias. However, relying only on similarity based on multi-view features is limited because these features ignore the details represented in single-view features. Therefore, we propose the AMC method to generate a more robust similarity measure by combining various measures. Our method significantly improves Rank@1 accuracy and Mean Average Precision (mAP) when evaluated on person re-identification datasets. Combined with the BoT Baseline on challenging datasets, we achieve impressive results, with a 7.9% improvement in Rank@1 and a 12.1% improvement in mAP on the MSMT17 dataset. On the Occluded-DukeMTMC dataset, our method increases Rank@1 by 22.0% and mAP by 18.4%. Code is available: https://github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC"
      },
      {
        "id": "oai:arXiv.org:2405.03869v5",
        "title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
        "link": "https://arxiv.org/abs/2405.03869",
        "author": "Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2405.03869v5 Announce Type: replace \nAbstract: A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models."
      },
      {
        "id": "oai:arXiv.org:2405.05572v2",
        "title": "From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences",
        "link": "https://arxiv.org/abs/2405.05572",
        "author": "Prashant Kodali, Anmol Goel, Likhith Asapu, Vamshi Krishna Bonagiri, Anirudh Govil, Monojit Choudhury, Ponnurangam Kumaraguru, Manish Shrivastava",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2405.05572v2 Announce Type: replace \nAbstract: Current computational approaches for analysing or generating code-mixed sentences do not explicitly model ``naturalness'' or ``acceptability'' of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences. Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text. To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media. Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset. Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models when trained solely using code-mixing metrics as features are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta and Bernice outperform IndicBERT across different configurations. Among Encoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder models are not able to outperform Encoder-only models. Decoder-only models perform the best when compared to all other MLLMS, with Llama 3.2 - 3B models outperforming similarly sized Qwen, Phi models. Comparison with zero and fewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks. Zero-shot transfer from En-Hi to En-Te acceptability judgments are better than random baselines."
      },
      {
        "id": "oai:arXiv.org:2405.09591v3",
        "title": "A Comprehensive Survey on Data Augmentation",
        "link": "https://arxiv.org/abs/2405.09591",
        "author": "Zaitian Wang, Pengfei Wang, Kunpeng Liu, Pengyang Wang, Yanjie Fu, Chang-Tien Lu, Charu C. Aggarwal, Jian Pei, Yuanchun Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2405.09591v3 Announce Type: replace \nAbstract: Data augmentation is a series of techniques that generate high-quality artificial data by manipulating existing data samples. By leveraging data augmentation techniques, AI models can achieve significantly improved applicability in tasks involving scarce or imbalanced datasets, thereby substantially enhancing AI models' generalization capabilities. Existing literature surveys only focus on a certain type of specific modality data, and categorize these methods from modality-specific and operation-centric perspectives, which lacks a consistent summary of data augmentation methods across multiple modalities and limits the comprehension of how existing data samples serve the data augmentation process. To bridge this gap, we propose a more enlightening taxonomy that encompasses data augmentation techniques for different common data modalities. Specifically, from a data-centric perspective, this survey proposes a modality-independent taxonomy by investigating how to take advantage of the intrinsic relationship between data samples, including single-wise, pair-wise, and population-wise sample data augmentation methods. Additionally, we categorize data augmentation methods across five data modalities through a unified inductive approach."
      },
      {
        "id": "oai:arXiv.org:2405.13745v2",
        "title": "NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation",
        "link": "https://arxiv.org/abs/2405.13745",
        "author": "Qiujie Dong, Huibiao Wen, Rui Xu, Shuangmin Chen, Jiaran Zhou, Shiqing Xin, Changhe Tu, Taku Komura, Wenping Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2405.13745v2 Announce Type: replace \nAbstract: Quadrilateral mesh generation plays a crucial role in numerical simulations within Computer-Aided Design and Engineering (CAD/E). Producing high-quality quadrangulation typically requires satisfying four key criteria. First, the quadrilateral mesh should closely align with principal curvature directions. Second, singular points should be strategically placed and effectively minimized. Third, the mesh should accurately conform to sharp feature edges. Lastly, quadrangulation results should exhibit robustness against noise and minor geometric variations. Existing methods generally involve first computing a regular cross field to represent quad element orientations across the surface, followed by extracting a quadrilateral mesh aligned closely with this cross field. A primary challenge with this approach is balancing the smoothness of the cross field with its alignment to pre-computed principal curvature directions, which are sensitive to small surface perturbations and often ill-defined in spherical or planar regions.\n  To tackle this challenge, we propose NeurCross, a novel framework that simultaneously optimizes a cross field and a neural signed distance function (SDF), whose zero-level set serves as a proxy of the input shape. Our joint optimization is guided by three factors: faithful approximation of the optimized SDF surface to the input surface, alignment between the cross field and the principal curvature field derived from the SDF surface, and smoothness of the cross field. Acting as an intermediary, the neural SDF contributes in two essential ways. First, it provides an alternative, optimizable base surface exhibiting more regular principal curvature directions for guiding the cross field. Second, we leverage the Hessian matrix of the neural SDF to implicitly enforce cross field alignment with principal curvature directions..."
      },
      {
        "id": "oai:arXiv.org:2406.00985v4",
        "title": "ParallelEdits: Efficient Multi-object Image Editing",
        "link": "https://arxiv.org/abs/2406.00985",
        "author": "Mingzhen Huang, Jialing Cai, Shan Jia, Vishnu Suresh Lokhande, Siwei Lyu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2406.00985v4 Announce Type: replace \nAbstract: Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses. In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. Additionally, we introduce the PIE-Bench++ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios."
      },
      {
        "id": "oai:arXiv.org:2406.01183v2",
        "title": "Automatic Input Feature Relevance via Spectral Neural Networks",
        "link": "https://arxiv.org/abs/2406.01183",
        "author": "Lorenzo Chicchi, Lorenzo Buffoni, Diego Febbe, Lorenzo Giambagli, Raffaele Marino, Duccio Fanelli",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2406.01183v2 Announce Type: replace \nAbstract: In machine learning practice it is often useful to identify relevant input features, so as to obtain compact dataset for more efficient numerical handling. On the other hand, by isolating key input elements, ranked according their respective degree of relevance, can help to elaborate on the process of decision making. Here, we propose a novel method to estimate the relative importance of the input components for a Deep Neural Network. This is achieved by leveraging on a spectral re-parametrization of the optimization process. Eigenvalues associated to input nodes provide in fact a robust proxy to gauge the relevance of the supplied entry features. Notably, the spectral features ranking is performed automatically, as a byproduct of the network training, with no additional processing to be carried out. The technique is successfully challenged against both synthetic and real data."
      },
      {
        "id": "oai:arXiv.org:2406.02481v5",
        "title": "Large Language Models as Carriers of Hidden Messages",
        "link": "https://arxiv.org/abs/2406.02481",
        "author": "Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2406.02481v5 Announce Type: replace \nAbstract: Simple fine-tuning can embed hidden text into large language models (LLMs), which is revealed only when triggered by a specific query. Applications include LLM fingerprinting, where a unique identifier is embedded to verify licensing compliance, and steganography, where the LLM carries hidden messages disclosed through a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although seemingly secure due to the vast number of potential triggers, is vulnerable to extraction through analysis of the LLM's output decoding process. We introduce an extraction attack called Unconditional Token Forcing (UTF), which iteratively feeds tokens from the LLM's vocabulary to reveal sequences with high token probabilities, indicating hidden text candidates. We also present Unconditional Token Forcing Confusion (UTFC), a defense paradigm that makes hidden text resistant to all known extraction attacks without degrading the general performance of LLMs compared to standard fine-tuning. UTFC has both benign (improving LLM fingerprinting) and malign applications (using LLMs to create covert communication channels)."
      },
      {
        "id": "oai:arXiv.org:2406.07361v4",
        "title": "Deep Implicit Optimization enables Robust Learnable Features for Deformable Image Registration",
        "link": "https://arxiv.org/abs/2406.07361",
        "author": "Rohit Jena, Pratik Chaudhari, James C. Gee",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2406.07361v4 Announce Type: replace \nAbstract: Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. However, existing DLIR methods forego many of the benefits and invariances of optimization methods. The lack of a task-specific inductive bias in DLIR methods leads to suboptimal performance, especially in the presence of domain shift. Our method aims to bridge this gap between statistical learning and optimization by explicitly incorporating optimization as a layer in a deep network. A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. This optimal warp is then used to minimize image and label alignment errors. By implicitly differentiating end-to-end through an iterative optimization solver, we explicitly exploit invariances of the correspondence matching problem induced by the optimization, while learning registration and label-aware features, and guaranteeing the warp functions to be a local minima of the registration objective in the feature space. Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. End-to-end feature learning also facilitates interpretability of features and arbitrary test-time regularization, which is not possible with existing DLIR methods."
      },
      {
        "id": "oai:arXiv.org:2406.07880v3",
        "title": "A Comprehensive Survey on Machine Learning Driven Material Defect Detection",
        "link": "https://arxiv.org/abs/2406.07880",
        "author": "Jun Bai, Di Wu, Tristan Shelley, Peter Schubel, David Twine, John Russell, Xuesen Zeng, Ji Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2406.07880v3 Announce Type: replace \nAbstract: Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavors in addressing contemporary challenges associated with MD. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This survey consolidates ML-based MDD literature and provides a foundation for future research and practice."
      },
      {
        "id": "oai:arXiv.org:2407.01356v2",
        "title": "tPARAFAC2: Tracking evolving patterns in (incomplete) temporal data",
        "link": "https://arxiv.org/abs/2407.01356",
        "author": "Christos Chatzis, Carla Schenker, Max Pfeffer, Evrim Acar",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.01356v2 Announce Type: replace \nAbstract: Tensor factorizations have been widely used for the task of uncovering patterns in various domains. Often, the input is time-evolving, shifting the goal to tracking the evolution of the underlying patterns instead. To adapt to this more complex setting, existing methods incorporate temporal regularization but they either have overly constrained structural requirements or lack uniqueness which is crucial for interpretation. In this paper, in order to capture the underlying evolving patterns, we introduce t(emporal)PARAFAC2, which utilizes temporal smoothness regularization on the evolving factors. Previously, Alternating Optimization (AO) and Alternating Direction Method of Multipliers (ADMM)-based algorithmic approach has been introduced to fit the PARAFAC2 model to fully observed data. In this paper, we extend this algorithmic framework to the case of partially observed data and use it to fit the tPARAFAC2 model to complete and incomplete datasets with the goal of revealing evolving patterns. Our numerical experiments on simulated datasets demonstrate that tPARAFAC2 can extract the underlying evolving patterns more accurately compared to the state-of-the-art in the presence of high amounts of noise and missing data. Using two real datasets, we also demonstrate the effectiveness of the algorithmic approach in terms of handling missing data and tPARAFAC2 model in terms of revealing evolving patterns. The paper provides an extensive comparison of different approaches for handling missing data within the proposed framework, and discusses both the advantages and limitations of tPARAFAC2 model."
      },
      {
        "id": "oai:arXiv.org:2407.02329v3",
        "title": "MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis",
        "link": "https://arxiv.org/abs/2407.02329",
        "author": "Dewei Zhou, You Li, Fan Ma, Zongxin Yang, Yi Yang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.02329v3 Announce Type: replace \nAbstract: We introduce the Multi-Instance Generation (MIG) task, which focuses on generating multiple instances within a single image, each accurately placed at predefined positions with attributes such as category, color, and shape, strictly following user specifications. MIG faces three main challenges: avoiding attribute leakage between instances, supporting diverse instance descriptions, and maintaining consistency in iterative generation. To address attribute leakage, we propose the Multi-Instance Generation Controller (MIGC). MIGC generates multiple instances through a divide-and-conquer strategy, breaking down multi-instance shading into single-instance tasks with singular attributes, later integrated. To provide more types of instance descriptions, we developed MIGC++. MIGC++ allows attribute control through text \\& images and position control through boxes \\& masks. Lastly, we introduced the Consistent-MIG algorithm to enhance the iterative MIG ability of MIGC and MIGC++. This algorithm ensures consistency in unmodified regions during the addition, deletion, or modification of instances, and preserves the identity of instances when their attributes are changed. We introduce the COCO-MIG and Multimodal-MIG benchmarks to evaluate these methods. Extensive experiments on these benchmarks, along with the COCO-Position benchmark and DrawBench, demonstrate that our methods substantially outperform existing techniques, maintaining precise control over aspects including position, attribute, and quantity. Project page: https://github.com/limuloo/MIGC."
      },
      {
        "id": "oai:arXiv.org:2407.02424v2",
        "title": "A Pattern Language for Machine Learning Tasks",
        "link": "https://arxiv.org/abs/2407.02424",
        "author": "Benjamin Rodatz, Ian Fan, Tuomas Laakkonen, Neil John Ortega, Thomas Hoffmann, Vincent Wang-Mascianica",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.02424v2 Announce Type: replace \nAbstract: We formalise the essential data of objective functions as equality constraints on composites of learners. We call these constraints \"tasks\", and we investigate the idealised view that such tasks determine model behaviours. We develop a flowchart-like graphical mathematics for tasks that allows us to; (1) offer a unified perspective of approaches in machine learning across domains; (2) design and optimise desired behaviours model-agnostically; and (3) import insights from theoretical computer science into practical machine learning. As a proof-of-concept of the potential practical impact of our theoretical framework, we exhibit and implement a novel \"manipulator\" task that minimally edits input data to have a desired attribute. Our model-agnostic approach achieves this end-to-end, and without the need for custom architectures, adversarial training, random sampling, or interventions on the data, hence enabling capable, small-scale, and training-stable models."
      },
      {
        "id": "oai:arXiv.org:2407.05576v3",
        "title": "CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive Hand-object Segmentation",
        "link": "https://arxiv.org/abs/2407.05576",
        "author": "Yuejiao Su, Yi Wang, Lap-Pui Chau",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.05576v3 Announce Type: replace \nAbstract: Egocentric Interactive hand-object segmentation (EgoIHOS) requires the segmentation of hands and interacting objects in egocentric images, which is crucial for understanding human behavior in assistive systems. Previous methods typically recognize hands and interacting objects as distinct semantic categories based solely on visual features, or simply use hand predictions as auxiliary cues for object segmentation. Despite the promising progress achieved by these methods, they fail to adequately model the interactive relationships between hands and objects while ignoring the coupled physical relationships among object categories, ultimately constraining their segmentation performance. To make up for the shortcomings of existing methods, we propose a novel method called CaRe-Ego that achieves state-of-the-art performance by emphasizing the contact between hands and objects from two aspects. First, we introduce a Hand-guided Object Feature Enhancer (HOFE) to establish the hand-object interactive relationships to extract more contact-relevant and discriminative object features. Second, we design the Contact-centric Object Decoupling Strategy (CODS) to explicitly model and disentangle coupling relationships among object categories, thereby emphasizing contact-aware feature learning. Experiments on various in-domain and out-of-domain test sets show that Care-Ego significantly outperforms existing methods with robust generalization capability. Codes are publicly available at https://github.com/yuggiehk/CaRe-Ego/."
      },
      {
        "id": "oai:arXiv.org:2407.06606v3",
        "title": "Tailored Design of Audio-Visual Speech Recognition Models using Branchformers",
        "link": "https://arxiv.org/abs/2407.06606",
        "author": "David Gimeno-G\\'omez, Carlos-D. Mart\\'inez-Hinarejos",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.06606v3 Announce Type: replace \nAbstract: Recent advances in Audio-Visual Speech Recognition (AVSR) have led to unprecedented achievements in the field, improving the robustness of this type of system in adverse, noisy environments. In most cases, this task has been addressed through the design of models composed of two independent encoders, each dedicated to a specific modality. However, while recent works have explored unified audio-visual encoders, determining the optimal cross-modal architecture remains an ongoing challenge. Furthermore, such approaches often rely on models comprising vast amounts of parameters and high computational cost training processes. In this paper, we aim to bridge this research gap by introducing a novel audio-visual framework. Our proposed method constitutes, to the best of our knowledge, the first attempt to harness the flexibility and interpretability offered by encoder architectures, such as the Branchformer, in the design of parameter-efficient AVSR systems. To be more precise, the proposed framework consists of two steps: first, estimating audio- and video-only systems, and then designing a tailored audio-visual unified encoder based on the layer-level branch scores provided by the modality-specific models. Extensive experiments on English and Spanish AVSR benchmarks covering multiple data conditions and scenarios demonstrated the effectiveness of our proposed method. Even when trained on a moderate scale of data, our models achieve competitive word error rates (WER) of approximately 2.5\\% for English and surpass existing approaches for Spanish, establishing a new benchmark with an average WER of around 9.1\\%. These results reflect how our tailored AVSR system is able to reach state-of-the-art recognition rates while significantly reducing the model complexity w.r.t. the prevalent approach in the field. Code and pre-trained models are available at https://github.com/david-gimeno/tailored-avsr."
      },
      {
        "id": "oai:arXiv.org:2407.12772v2",
        "title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models",
        "link": "https://arxiv.org/abs/2407.12772",
        "author": "Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, Ziwei Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.12772v2 Announce Type: replace \nAbstract: The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs. We opensource our codebase and maintain leaderboard of LIVEBENCH at https://github.com/EvolvingLMMs-Lab/lmms-eval and https://huggingface.co/spaces/lmms-lab/LiveBench."
      },
      {
        "id": "oai:arXiv.org:2407.15738v3",
        "title": "Parallel Split Learning with Global Sampling",
        "link": "https://arxiv.org/abs/2407.15738",
        "author": "Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.15738v3 Announce Type: replace \nAbstract: Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamically adjusting client-side batch sizes. This decouples the effective batch size from the number of participating devices and ensures that global batches better reflect the overall data distribution. Using standard concentration bounds, we establish tighter deviation guarantees compared to existing approaches. Empirical results on a benchmark dataset confirm that the proposed method improves model accuracy, training efficiency, and convergence stability, offering a scalable solution for learning at the network edge."
      },
      {
        "id": "oai:arXiv.org:2408.01603v2",
        "title": "FIVB ranking: Misstep in the right direction",
        "link": "https://arxiv.org/abs/2408.01603",
        "author": "Salma Tenni, Daniel Gomes de Pinho Zanco, Leszek Szczecinski",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.01603v2 Announce Type: replace \nAbstract: This work presents and evaluates the ranking algorithm that has been used by Federation Internationale de Volleyball (FIVB) since 2020. The prominent feature of the FIVB ranking is the use of the probabilistic model, which explicitly calculates the probabilities of the future matches results using the estimated teams' strengths. Such explicit modeling is new in the context of official sport rankings, especially for multi-level outcomes, and we study the optimality of its parameters using both analytical and numerical methods. We conclude that from the modeling perspective, the current thresholds fit well the data but adding the home-field advantage (HFA) would be beneficial. Regarding the algorithm itself, we explain the rationale behind the approximations currently used and show a simple method to find new parameters (numerical score) which improve the performance. We also show that the weighting of the match results is counterproductive."
      },
      {
        "id": "oai:arXiv.org:2408.03618v4",
        "title": "A Logical Fallacy-Informed Framework for Argument Generation",
        "link": "https://arxiv.org/abs/2408.03618",
        "author": "Luca Mouchel, Debjit Paul, Shaobo Cui, Robert West, Antoine Bosselut, Boi Faltings",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.03618v4 Announce Type: replace \nAbstract: Despite the remarkable performance of Large Language Models (LLMs) in natural language processing tasks, they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. To address this issue, we introduce FIPO, a fallacy-informed framework that leverages preference optimization methods to steer LLMs toward logically sound arguments. FIPO includes a classification loss, to capture the fine-grained information on fallacy types. Our results on argumentation datasets show that our method reduces the fallacy errors by up to 17.5%. Furthermore, our human evaluation results indicate that the quality of the generated arguments by our method significantly outperforms the fine-tuned baselines, as well as other preference optimization methods, such as DPO. These findings highlight the importance of ensuring models are aware of logical fallacies for effective argument generation. Our code is available at github.com/lucamouchel/Logical-Fallacies."
      },
      {
        "id": "oai:arXiv.org:2408.05411v2",
        "title": "How Does Audio Influence Visual Attention in Omnidirectional Videos? Database and Model",
        "link": "https://arxiv.org/abs/2408.05411",
        "author": "Yuxin Zhu, Huiyu Duan, Kaiwei Zhang, Yucheng Zhu, Xilei Zhu, Long Teng, Xiongkuo Min, Guangtao Zhai",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.05411v2 Announce Type: replace \nAbstract: Understanding and predicting viewer attention in omnidirectional videos (ODVs) is crucial for enhancing user engagement in virtual and augmented reality applications. Although both audio and visual modalities are essential for saliency prediction in ODVs, the joint exploitation of these two modalities has been limited, primarily due to the absence of large-scale audio-visual saliency databases and comprehensive analyses. This paper comprehensively investigates audio-visual attention in ODVs from both subjective and objective perspectives. Specifically, we first introduce a new audio-visual saliency database for omnidirectional videos, termed AVS-ODV database, containing 162 ODVs and corresponding eye movement data collected from 60 subjects under three audio modes including mute, mono, and ambisonics. Based on the constructed AVS-ODV database, we perform an in-depth analysis of how audio influences visual attention in ODVs. To advance the research on audio-visual saliency prediction for ODVs, we further establish a new benchmark based on the AVS-ODV database by testing numerous state-of-the-art saliency models, including visual-only models and audio-visual models. In addition, given the limitations of current models, we propose an innovative omnidirectional audio-visual saliency prediction network (OmniAVS), which is built based on the U-Net architecture, and hierarchically fuses audio and visual features from the multimodal aligned embedding space. Extensive experimental results demonstrate that the proposed OmniAVS model outperforms other state-of-the-art models on both ODV AVS prediction and traditional AVS predcition tasks. The AVS-ODV database and OmniAVS model will be released to facilitate future research."
      },
      {
        "id": "oai:arXiv.org:2408.06150v3",
        "title": "LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library",
        "link": "https://arxiv.org/abs/2408.06150",
        "author": "Tianhao Yu, Cai Yao, Zhuorui Sun, Feng Shi, Lin Zhang, Kangjie Lyu, Xuan Bai, Andong Liu, Xicheng Zhang, Jiali Zou, Wenshou Wang, Chris Lai, Kai Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.06150v3 Announce Type: replace \nAbstract: In this study, we generate and maintain a database of 10 million virtual lipids through METiS's in-house de novo lipid generation algorithms and lipid virtual screening techniques. These virtual lipids serve as a corpus for pre-training, lipid representation learning, and downstream task knowledge transfer, culminating in state-of-the-art LNP property prediction performance. We propose LipidBERT, a BERT-like model pre-trained with the Masked Language Model (MLM) and various secondary tasks. Additionally, we compare the performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like lipid generation model, on downstream tasks. The proposed bilingual LipidBERT model operates in two languages: the language of ionizable lipid pre-training, using in-house dry-lab lipid structures, and the language of LNP fine-tuning, utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT as a key AI-based filter for future screening tasks, including new versions of METiS de novo lipid libraries and, more importantly, candidates for in vivo testing for orgran-targeting LNPs. To the best of our knowledge, this is the first successful demonstration of the capability of a pre-trained language model on virtual lipids and its effectiveness in downstream tasks using web-lab data. This work showcases the clever utilization of METiS's in-house de novo lipid library as well as the power of dry-wet lab integration."
      },
      {
        "id": "oai:arXiv.org:2408.06996v2",
        "title": "Blessing of Dimensionality for Approximating Sobolev Classes on Manifolds",
        "link": "https://arxiv.org/abs/2408.06996",
        "author": "Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Sch\\\"onlieb",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.06996v2 Announce Type: replace \nAbstract: The manifold hypothesis says that natural high-dimensional data lie on or around a low-dimensional manifold. The recent success of statistical and learning-based methods in very high dimensions empirically supports this hypothesis, suggesting that typical worst-case analysis does not provide practical guarantees. A natural step for analysis is thus to assume the manifold hypothesis and derive bounds that are independent of any ambient dimensions that the data may be embedded in. Theoretical implications in this direction have recently been explored in terms of generalization of ReLU networks and convergence of Langevin methods. In this work, we consider optimal uniform approximations with functions of finite statistical complexity. While upper bounds on uniform approximation exist in the literature using ReLU neural networks, we consider the opposite: lower bounds to quantify the fundamental difficulty of approximation on manifolds. In particular, we demonstrate that the statistical complexity required to approximate a class of bounded Sobolev functions on a compact manifold is bounded from below, and moreover that this bound is dependent only on the intrinsic properties of the manifold, such as curvature, volume, and injectivity radius."
      },
      {
        "id": "oai:arXiv.org:2408.09340v2",
        "title": "Enhanced BPINN Training Convergence in Solving General and Multi-scale Elliptic PDEs with Noise",
        "link": "https://arxiv.org/abs/2408.09340",
        "author": "Yilong Hou, Xi'an Li, Jinran Wu, You-Gan Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.09340v2 Announce Type: replace \nAbstract: Bayesian Physics Informed Neural Networks (BPINN) have attracted considerable attention for inferring the system states and physical parameters of differential equations according to noisy observations. However, in practice, Hamiltonian Monte Carlo (HMC) used to estimate the internal parameters of the solver for BPINN often encounters these troubles including poor performance and awful convergence for a given step size used to adjust the momentum of those parameters. To address the convergence of HMC for the BPINN method and extend its application scope to multi-scale partial differential equations (PDE), we develop a robust multi-scale BPINN (dubbed MBPINN) method by integrating multi-scale deep neural networks (MscaleDNN) and the BPINN framework. In this newly proposed MBPINN method, we reframe HMC with Stochastic Gradient Descent (SGD) to ensure the most ``likely'' estimation is always provided, and we configure its solver as a Fourier feature mapping-induced MscaleDNN. This novel method offers several key advantages: (1) it is more robust than HMC, (2) it incurs less computational cost than HMC, and (3) it is more flexible for complex problems. We demonstrate the applicability and performance of the proposed method through some general Poisson and multi-scale elliptic problems in one and two-dimensional Euclidean spaces. Our findings indicate that the proposed method can avoid HMC failures and provide valid results. Additionally, our method is capable of handling complex elliptic PDE and producing comparable results for general elliptic PDE under the case of lower signal-to-noise rate. These findings suggest that our proposed approach has great potential for physics-informed machine learning for parameter estimation and solution recovery in the case of ill-posed problems."
      },
      {
        "id": "oai:arXiv.org:2408.10276v4",
        "title": "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models",
        "link": "https://arxiv.org/abs/2408.10276",
        "author": "Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, Fenglong Ma",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.10276v4 Announce Type: replace \nAbstract: Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE) module. This method not only preserves privacy but also enhances the model's ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data."
      },
      {
        "id": "oai:arXiv.org:2408.10453v2",
        "title": "Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation",
        "link": "https://arxiv.org/abs/2408.10453",
        "author": "Liu He, Yizhi Song, Hejun Huang, Pinxin Liu, Yunlong Tang, Daniel Aliaga, Xin Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.10453v2 Announce Type: replace \nAbstract: Text-to-video generation has been dominated by diffusion-based or autoregressive models. These novel models provide plausible versatility, but are criticized for improper physical motion, shading and illumination, camera motion, and temporal consistency. The film industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D synthetic videos address these shortcomings, but require tight collaboration between movie makers and 3D rendering experts. We introduce an automatic synthetic video generation pipeline based on Vision Large Language Model (VLM) agent collaborations. Given a language description of a video, multiple VLM agents direct various processes of the generation pipeline. They cooperate to create Blender scripts which render a video following the given description. Augmented with Blender-based movie making knowledge, the Director agent decomposes the text-based video description into sub-processes. For each sub-process, the Programmer agent produces Python-based Blender scripts based on function composing and API calling. The Reviewer agent, with knowledge of video reviewing, character motion coordinates, and intermediate screenshots, provides feedback to the Programmer agent. The Programmer agent iteratively improves scripts to yield the best video outcome. Our generated videos show better quality than commercial video generation models in five metrics on video quality and instruction-following performance. Our framework outperforms other approaches in a user study on quality, consistency, and rationality."
      },
      {
        "id": "oai:arXiv.org:2408.12307v2",
        "title": "Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2408.12307",
        "author": "Yen-Ru Lai, Fu-Chieh Chang, Pei-Yuan Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.12307v2 Announce Type: replace \nAbstract: Offline reinforcement learning (RL) learns policies from a fixed dataset, but often requires large amounts of data. The challenge arises when labeled datasets are expensive, especially when rewards have to be provided by human labelers for large datasets. In contrast, unlabelled data tends to be less expensive. This situation highlights the importance of finding effective ways to use unlabelled data in offline RL, especially when labelled data is limited or expensive to obtain. In this paper, we present the algorithm to utilize the unlabeled data in the offline RL method with kernel function approximation and give the theoretical guarantee. We present various eigenvalue decay conditions of $\\mathcal{H}_k$ which determine the complexity of the algorithm. In summary, our work provides a promising approach for exploiting the advantages offered by unlabeled data in offline RL, whilst maintaining theoretical assurances."
      },
      {
        "id": "oai:arXiv.org:2408.13431v2",
        "title": "Face Clustering via Early Stopping and Edge Recall",
        "link": "https://arxiv.org/abs/2408.13431",
        "author": "Junjie Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.13431v2 Announce Type: replace \nAbstract: Large-scale face clustering has achieved significant progress, with many efforts dedicated to learning to cluster large-scale faces with supervised-learning. However, complex model design and tedious clustering processes are typical in existing methods. Such limitations result in infeasible clustering in real-world applications. Reasonable and efficient model design and training need to be taken into account. Besides, developing unsupervised face clustering algorithms is crucial, which are more realistic in real-world applications. In this paper, we propose a novel unsupervised face clustering algorithm FC-ES and a novel supervised face clustering algorithm FC-ESER to address these issues. An efficient and effective neighbor-based edge probability and a novel early stopping strategy are proposed in FC-ES, guaranteeing the accuracy and recall of large-scale face clustering simultaneously. Furthermore, to take advantage of supervised learning, a novel edge recall strategy is proposed in FC-ESER to further recall the edge connections that are not connected in FC-ES. Extensive experiments on multiple benchmarks for face, person, and vehicle clustering show that our proposed FC-ES and FC-ESER significantly outperform previous state-of-the-art methods. Our code will be available at https://github.com/jumptoliujj/FC-ESER."
      },
      {
        "id": "oai:arXiv.org:2408.15186v2",
        "title": "Easy-access online social media metrics can foster the identification of misinformation sharing users",
        "link": "https://arxiv.org/abs/2408.15186",
        "author": "J\\'ulia Sz\\'amely, Alessandro Galeazzi, J\\'ulia Koltai, Elisa Omodei",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.15186v2 Announce Type: replace \nAbstract: Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is time-consuming and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms."
      },
      {
        "id": "oai:arXiv.org:2408.17090v2",
        "title": "FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition",
        "link": "https://arxiv.org/abs/2408.17090",
        "author": "Chen Hu, Hanchi Ren, Jingjing Deng, Xianghua Xie, Xiaoke Ma",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.17090v2 Announce Type: replace \nAbstract: Federated learning is a machine learning paradigm that enables decentralized clients to collaboratively learn a shared model while keeping all the training data local. While considerable research has focused on federated image generation, particularly Generative Adversarial Networks, Variational Autoencoders have received less attention. In this paper, we address the challenges of non-IID (independently and identically distributed) data environments featuring multiple groups of images of different types. Non-IID data distributions can lead to difficulties in maintaining a consistent latent space and can also result in local generators with disparate texture features being blended during aggregation. We thereby introduce FissionVAE that decouples the latent space and constructs decoder branches tailored to individual client groups. This method allows for customized learning that aligns with the unique data distributions of each group. Additionally, we incorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder architectures within FissionVAE. We also explore strategies for setting the latent prior distributions to enhance the decoupling process. To evaluate our approach, we assemble two composite datasets: the first combines MNIST and FashionMNIST; the second comprises RGB datasets of cartoon and human faces, wild animals, marine vessels, and remote sensing images. Our experiments demonstrate that FissionVAE greatly improves generation quality on these datasets compared to baseline federated VAE models."
      },
      {
        "id": "oai:arXiv.org:2409.05929v4",
        "title": "M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the JEPA framework",
        "link": "https://arxiv.org/abs/2409.05929",
        "author": "Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.05929v4 Announce Type: replace \nAbstract: Current multimodal alignment strategies primarily use single or unified modality encoders, while optimizing the alignment on the original token space. Such a framework is easy to implement and incorporate with the pretrained knowledge, but might result in information bias. To deal with such issues, the joint encoding predictive architecture (JEPA) learns the alignment loss on the latent space, with a predictor to convert the input encoding to the output latent space. However, the application of JEPA in multimodal scenarios is limited so far. In this paper, we introduce M3-Jepa, a scalable multimodal alignment framework, with the predictor implemented by a multi-directional mixture of experts (MoE). We demonstrate the framework can maximize the mutual information with information theory derivations, by alternating the optimization between different uni-directional tasks. By thoroughly designed experiments, we show that M3-Jepa can obtain state-of-the-art performance on different modalities and tasks, generalize to unseen datasets and domains, and is computationally efficient in training and inference. Our study indicates that M3-Jepa might provide a new paradigm to self-supervised learning and open-world modeling."
      },
      {
        "id": "oai:arXiv.org:2409.05975v4",
        "title": "CoDiCast: Conditional Diffusion Model for Global Weather Prediction with Uncertainty Quantification",
        "link": "https://arxiv.org/abs/2409.05975",
        "author": "Jimeng Shi, Bowen Jin, Jiawei Han, Sundararaman Gopalakrishnan, Giri Narasimhan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.05975v4 Announce Type: replace \nAbstract: Accurate weather forecasting is critical for science and society. Yet, existing methods have not managed to simultaneously have the properties of high accuracy, low uncertainty, and high computational efficiency. On one hand, to quantify the uncertainty in weather predictions, the strategy of ensemble forecast (i.e., generating a set of diverse predictions) is often employed. However, traditional ensemble numerical weather prediction (NWP) is computationally intensive. On the other hand, most existing machine learning-based weather prediction (MLWP) approaches are efficient and accurate. Nevertheless, they are deterministic and cannot capture the uncertainty of weather forecasting. In this work, we propose CoDiCast, a conditional diffusion model to generate accurate global weather prediction, while achieving uncertainty quantification with ensemble forecasts and modest computational cost. The key idea is to simulate a conditional version of the reverse denoising process in diffusion models, which starts from pure Gaussian noise to generate realistic weather scenarios for a future time point. Each denoising step is conditioned on observations from the recent past. Ensemble forecasts are achieved by repeatedly sampling from stochastic Gaussian noise to represent uncertainty quantification. CoDiCast is trained on a decade of ERA5 reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMWF). Experimental results demonstrate that our approach outperforms several existing data-driven methods in accuracy. Our conditional diffusion model, CoDiCast, can generate 6-day global weather forecasts, at 6-hour steps and $5.625^\\circ$ latitude-longitude resolution, for over 5 variables, in about 12 minutes on a commodity A100 GPU machine with 80GB memory. The open-souced code is provided at https://github.com/JimengShi/CoDiCast."
      },
      {
        "id": "oai:arXiv.org:2409.07271v4",
        "title": "CFCPalsy: Facial Image Synthesis with Cross-Fusion Cycle Diffusion Model for Facial Paralysis Individuals",
        "link": "https://arxiv.org/abs/2409.07271",
        "author": "Weixiang Gao, Yating Zhang, Yifan Xia",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.07271v4 Announce Type: replace \nAbstract: Currently, the diagnosis of facial paralysis remains a challenging task, often relying heavily on the subjective judgment and experience of clinicians, which can introduce variability and uncertainty in the assessment process. One promising application in real-life situations is the automatic estimation of facial paralysis. However, the scarcity of facial paralysis datasets limits the development of robust machine learning models for automated diagnosis and therapeutic interventions. To this end, this study aims to synthesize a high-quality facial paralysis dataset to address this gap, enabling more accurate and efficient algorithm training. Specifically, a novel Cross-Fusion Cycle Palsy Expression Generative Model (CFCPalsy) based on the diffusion model is proposed to combine different features of facial information and enhance the visual details of facial appearance and texture in facial regions, thus creating synthetic facial images that accurately represent various degrees and types of facial paralysis. We have qualitatively and quantitatively evaluated the proposed method on the commonly used public clinical datasets of facial paralysis to demonstrate its effectiveness. Experimental results indicate that the proposed method surpasses state-of-the-art methods, generating more realistic facial images and maintaining identity consistency."
      },
      {
        "id": "oai:arXiv.org:2409.09413v2",
        "title": "Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence",
        "link": "https://arxiv.org/abs/2409.09413",
        "author": "Tadahiro Taniguchi, Masafumi Oizumi, Noburo Saji, Takato Horii, Naotsugu Tsuchiya",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09413v2 Announce Type: replace \nAbstract: This perspective paper explores the bidirectional influence between language emergence and the relational structure of subjective experiences, termed qualia structure, and lays out a constructive approach to the intricate dependency between the two. We hypothesize that the emergence of languages with distributional semantics (e.g., syntactic-semantic structures) is linked to the coordination of internal representations shaped by experience, potentially facilitating more structured language through reciprocal influence. This hypothesized mutual dependency connects to recent advancements in AI and symbol emergence robotics, and is explored within this paper through theoretical frameworks such as the collective predictive coding. Computational studies show that neural network-based language models form systematically structured internal representations, and multimodal language models can share representations between language and perceptual information. This perspective suggests that language emergence serves not only as a mechanism creating a communication tool but also as a mechanism for allowing people to realize shared understanding of qualitative experiences. The paper discusses the implications of this bidirectional influence in the context of consciousness studies, linguistics, and cognitive science, and outlines future constructive research directions to further explore this dynamic relationship between language emergence and qualia structure."
      },
      {
        "id": "oai:arXiv.org:2409.09724v2",
        "title": "MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection",
        "link": "https://arxiv.org/abs/2409.09724",
        "author": "Yaning Zhang, Tianyi Wang, Zitong Yu, Zan Gao, Linlin Shen, Shengyong Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09724v2 Announce Type: replace \nAbstract: The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations."
      },
      {
        "id": "oai:arXiv.org:2409.09779v3",
        "title": "Underwater Image Enhancement via Dehazing and Color Restoration",
        "link": "https://arxiv.org/abs/2409.09779",
        "author": "Chengqin Wu, Shuai Yu, Tuyan Luo, Qiuhua Rao, Qingson Hu, Jingxiang Xu, Lijun Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09779v3 Announce Type: replace \nAbstract: Underwater visual imaging is crucial for marine engineering, but it suffers from low contrast, blurriness, and color degradation, which hinders downstream analysis. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process, neglecting their inherent independence while overlooking their synergistic relationship. To overcome this limitation, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) that dynamically integrates these decoupled features to achieve comprehensive enhancement. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to respectively preserve color fidelity and enhance structural details during network training. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images."
      },
      {
        "id": "oai:arXiv.org:2409.11529v2",
        "title": "Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor Decompositions and Deep Unrolling",
        "link": "https://arxiv.org/abs/2409.11529",
        "author": "Lukas Schynol, Marius Pesavento",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.11529v2 Announce Type: replace \nAbstract: Anomaly detection (AD) is increasingly recognized as a key component for ensuring the resilience of future communication systems. While deep learning has shown state-of-the-art AD performance, its application in critical systems is hindered by concerns regarding training data efficiency, domain adaptation and interpretability. This work considers AD in network flows using incomplete measurements, leveraging a robust tensor decomposition approach and deep unrolling techniques to address these challenges. We first propose a novel block-successive convex approximation algorithm based on a regularized model-fitting objective where the normal flows are modeled as low-rank tensors and anomalies as sparse. An augmentation of the objective is introduced to decrease the computational cost. We apply deep unrolling to derive a novel deep network architecture based on our proposed algorithm, treating the regularization parameters as learnable weights. Inspired by Bayesian approaches, we extend the model architecture to perform online adaptation to per-flow and per-time-step statistics, improving AD performance while maintaining a low parameter count and preserving the problem's permutation equivariances. To optimize the deep network weights for detection performance, we employ a homotopy optimization approach based on an efficient approximation of the area under the receiver operating characteristic curve. Extensive experiments on synthetic and real-world data demonstrate that our proposed deep network architecture exhibits a high training data efficiency, outperforms reference methods, and adapts seamlessly to varying network topologies."
      },
      {
        "id": "oai:arXiv.org:2410.00844v4",
        "title": "Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport",
        "link": "https://arxiv.org/abs/2410.00844",
        "author": "Zhenyi Zhang, Tiejun Li, Peijie Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.00844v4 Announce Type: replace \nAbstract: Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data. Theoretically, we explore the connections between the RUOT and Schr\\\"odinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: https://github.com/zhenyiizhang/DeepRUOT."
      },
      {
        "id": "oai:arXiv.org:2410.02597v3",
        "title": "HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR",
        "link": "https://arxiv.org/abs/2410.02597",
        "author": "Hainan Xu, Travis M. Bartley, Vladimir Bataev, Boris Ginsburg",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02597v3 Announce Type: replace \nAbstract: We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel architecture for speech recognition that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAINAN supports both autoregressive inference with all network components and non-autoregressive inference without the predictor. Additionally, we propose a novel semi-autoregressive inference paradigm that first generates an initial hypothesis using non-autoregressive inference, followed by refinement steps where each token prediction is regenerated using parallelized autoregression on the initial hypothesis. Experiments on multiple datasets across different languages demonstrate that HAINAN achieves efficiency parity with CTC in non-autoregressive mode and with TDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN outperforms TDT and RNN-T, while non-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive inference further enhances the model's accuracy with minimal computational overhead, and even outperforms TDT results in some cases. These results highlight HAINAN's flexibility in balancing accuracy and speed, positioning it as a strong candidate for real-world speech recognition applications."
      },
      {
        "id": "oai:arXiv.org:2410.03613v2",
        "title": "Understanding Large Language Models in Your Pockets: Performance Study on COTS Mobile Devices",
        "link": "https://arxiv.org/abs/2410.03613",
        "author": "Jie Xiao, Qianyi Huang, Xu Chen, Chen Tian",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03613v2 Announce Type: replace \nAbstract: As large language models (LLMs) increasingly integrate into every aspect of our work and daily lives, there are growing concerns about user privacy, which push the trend toward local deployment of these models. There are a number of lightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on smartphones, providing users with greater control over their personal data. As a rapidly emerging application, we are concerned about their performance on commercial-off-the-shelf mobile devices. To fully understand the current landscape of LLM deployment on mobile platforms, we conduct a comprehensive measurement study on mobile devices. We evaluate both metrics that affect user experience, including token throughput, latency, and battery consumption, as well as factors critical to developers, such as resource utilization, DVFS strategies, and inference engines. In addition, we provide a detailed analysis of how these hardware capabilities and system dynamics affect on-device LLM performance, which may help developers identify and address bottlenecks for mobile LLM applications. We also provide comprehensive comparisons across the mobile system-on-chips (SoCs) from major vendors, highlighting their performance differences in handling LLM workloads. We hope that this study can provide insights for both the development of on-device LLMs and the design for future mobile system architecture."
      },
      {
        "id": "oai:arXiv.org:2410.03954v2",
        "title": "SDA-GRIN for Adaptive Spatial-Temporal Multivariate Time Series Imputation",
        "link": "https://arxiv.org/abs/2410.03954",
        "author": "Amir Eskandari, Aman Anand, Drishti Sharma, Farhana Zulkernine",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03954v2 Announce Type: replace \nAbstract: In various applications, the multivariate time series often suffers from missing data. This issue can significantly disrupt systems that rely on the data. Spatial and temporal dependencies can be leveraged to impute the missing samples. Existing imputation methods often ignore dynamic changes in spatial dependencies. We propose a Spatial Dynamic Aware Graph Recurrent Imputation Network (SDA-GRIN) which is capable of capturing dynamic changes in spatial dependencies.SDA-GRIN leverages a multi-head attention mechanism to adapt graph structures with time. SDA-GRIN models multivariate time series as a sequence of temporal graphs and uses a recurrent message-passing architecture for imputation. We evaluate SDA-GRIN on four real-world datasets: SDA-GRIN improves MSE by 9.51% for the AQI and 9.40% for AQI-36. On the PEMS-BAY dataset, it achieves a 1.94% improvement in MSE. Detailed ablation study demonstrates the effect of window sizes and missing data on the performance of the method. Project page:https://ameskandari.github.io/sda-grin/"
      },
      {
        "id": "oai:arXiv.org:2410.05016v3",
        "title": "T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data",
        "link": "https://arxiv.org/abs/2410.05016",
        "author": "Hugo Thimonier, Jos\\'e Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Li\\^en Doan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.05016v3 Announce Type: replace \nAbstract: Self-supervision is often used for pre-training to foster performance on a downstream task by constructing meaningful representations of samples. Self-supervised learning (SSL) generally involves generating different views of the same sample and thus requires data augmentations that are challenging to construct for tabular data. This constitutes one of the main challenges of self-supervision for structured data. In the present work, we propose a novel augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on a Joint Embedding Predictive Architecture (JEPA) and is akin to mask reconstruction in the latent space. It involves predicting the latent representation of one subset of features from the latent representation of a different subset within the same sample, thereby learning rich representations without augmentations. We use our method as a pre-training technique and train several deep classifiers on the obtained representation. Our experimental results demonstrate a substantial improvement in both classification and regression tasks, outperforming models trained directly on samples in their original data space. Moreover, T-JEPA enables some methods to consistently outperform or match the performance of traditional methods likes Gradient Boosted Decision Trees. To understand why, we extensively characterize the obtained representations and show that T-JEPA effectively identifies relevant features for downstream tasks without access to the labels. Additionally, we introduce regularization tokens, a novel regularization method critical for training of JEPA-based models on structured data."
      },
      {
        "id": "oai:arXiv.org:2410.07577v2",
        "title": "3D Vision-Language Gaussian Splatting",
        "link": "https://arxiv.org/abs/2410.07577",
        "author": "Qucheng Peng, Benjamin Planche, Zhongpai Gao, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Chen Chen, Ziyan Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.07577v2 Announce Type: replace \nAbstract: Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches have naively embedded semantic representations into 3D reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. To alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin."
      },
      {
        "id": "oai:arXiv.org:2410.10519v3",
        "title": "AI-based particle track identification in scintillating fibres read out with imaging sensors",
        "link": "https://arxiv.org/abs/2410.10519",
        "author": "Noemi B\\\"uhrer, Sa\\'ul Alonso-Monsalve, Matthew Franks, Till Dieminger, Davide Sgalaberna",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10519v3 Announce Type: replace \nAbstract: This paper presents the development and application of an AI-based method for particle track identification using scintillating fibres read out with imaging sensors. We propose a variational autoencoder (VAE) to efficiently filter and identify frames containing signal from the substantial data generated by SPAD array sensors. Our VAE model, trained on purely background frames, demonstrated a high capability to distinguish frames containing particle tracks from background noise. The performance of the VAE-based anomaly detection was validated with experimental data, demonstrating the method's ability to efficiently identify relevant events with rapid processing time, suggesting a solid prospect for deployment as a fast inference tool on hardware for real-time anomaly detection. This work highlights the potential of combining advanced sensor technology with machine learning techniques to enhance particle detection and tracking."
      },
      {
        "id": "oai:arXiv.org:2410.10807v2",
        "title": "Hard-Constrained Neural Networks with Universal Approximation Guarantees",
        "link": "https://arxiv.org/abs/2410.10807",
        "author": "Youngjae Min, Navid Azizan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10807v2 Announce Type: replace \nAbstract: Incorporating prior knowledge or specifications of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and leads to conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction--an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Unlike approaches that modify outputs only at inference time, HardNet enables end-to-end training with hard constraint guarantees, leading to improved performance. To the best of our knowledge, HardNet is the first method with an efficient forward pass to enforce more than one input-dependent inequality constraint. It allows unconstrained optimization of the network parameters using standard algorithms by appending a differentiable closed-form enforcement layer to the network's output. Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: learning with piecewise constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems."
      },
      {
        "id": "oai:arXiv.org:2410.10821v3",
        "title": "Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models",
        "link": "https://arxiv.org/abs/2410.10821",
        "author": "Jingzhi Bao, Xueting Li, Ming-Hsuan Yang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.10821v3 Announce Type: replace \nAbstract: 3D meshes are widely used in computer vision and graphics for their efficiency in animation and minimal memory use, playing a crucial role in movies, games, AR, and VR. However, creating temporally consistent and realistic textures for mesh sequences remains labor-intensive for professional artists. On the other hand, while video diffusion models excel at text-driven video generation, they often lack 3D geometry awareness and struggle with achieving multi-view consistent texturing for 3D meshes. In this work, we present Tex4D, a zero-shot approach that integrates inherent 3D geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4D textures. Given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the UV space. To ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis. However, straightforwardly combining the video diffusion model and the UV texture aggregation leads to blurry results. We analyze the underlying causes and propose a simple yet effective modification to the DDIM sampling process to address this issue. Additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process. To the best of our knowledge, Tex4D is the first method specifically designed for 4D scene texturing. Extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences."
      },
      {
        "id": "oai:arXiv.org:2410.11226v2",
        "title": "MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning",
        "link": "https://arxiv.org/abs/2410.11226",
        "author": "Peter Eckmann, Dongxia Wu, Germano Heinzelmann, Michael K. Gilson, Rose Yu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11226v2 Announce Type: replace \nAbstract: Current generative models for drug discovery primarily use molecular docking as an oracle to guide the generation of active compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show real-world experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. To address this challenge, we propose Multi-Fidelity Latent space Active Learning (MF-LAL), a generative modeling framework that integrates a set of oracles with varying cost-accuracy tradeoffs. We train a surrogate model for each oracle and use these surrogates to guide generation of compounds with high predicted activity. Unlike previous approaches that separately learn the surrogate model and generative model, MF-LAL combines the generative and multi-fidelity surrogate models into a single framework, allowing for more accurate activity prediction and higher quality samples. We train MF-LAL with a novel active learning algorithm to further reduce computational cost. Our experiments on two disease-relevant proteins show that MF-LAL produces compounds with significantly better binding free energy scores than other single and multi-fidelity approaches (~50% improvement in mean binding free energy score)."
      },
      {
        "id": "oai:arXiv.org:2410.14567v4",
        "title": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions",
        "link": "https://arxiv.org/abs/2410.14567",
        "author": "Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.14567v4 Announce Type: replace \nAbstract: Retrieval-augmented generation (RAG) has become integral to large language models (LLMs), particularly for conversational AI systems where user questions may reference knowledge beyond the LLMs' training cutoff. However, many natural user questions lack well-defined answers, either due to limited domain knowledge or because the retrieval system returns documents that are relevant in appearance but uninformative in content. In such cases, LLMs often produce hallucinated answers without flagging them. While recent work has largely focused on questions with false premises, we study out-of-scope questions, where the retrieved document appears semantically similar to the question but lacks the necessary information to answer it. In this paper, we propose a guided hallucination-based approach ELOQ to automatically generate a diverse set of out-of-scope questions from post-cutoff documents, followed by human verification to ensure quality. We use this dataset to evaluate several LLMs on their ability to detect out-of-scope questions and generate appropriate responses. Finally, we introduce an improved detection method that enhances the reliability of LLM-based question-answering systems in handling out-of-scope questions."
      },
      {
        "id": "oai:arXiv.org:2410.18902v2",
        "title": "LLMs for Extremely Low-Resource Finno-Ugric Languages",
        "link": "https://arxiv.org/abs/2410.18902",
        "author": "Taido Purason, Hele-Andra Kuulmets, Mark Fishel",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.18902v2 Announce Type: replace \nAbstract: The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented. This paper addresses this gap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation. Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the smugri-MT-bench multi-turn conversational benchmark; and conducting human evaluation. We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP."
      },
      {
        "id": "oai:arXiv.org:2411.01642v2",
        "title": "Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination",
        "link": "https://arxiv.org/abs/2411.01642",
        "author": "Md Abrar Jahin, Md. Akmol Masud, M. F. Mridha, Nilanjan Dey, Zeyar Aung",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.01642v2 Announce Type: replace \nAbstract: In high-energy physics, particle jet tagging plays a pivotal role in distinguishing quark from gluon jets using data from collider experiments. While graph-based deep learning methods have advanced this task beyond traditional feature-engineered approaches, the complex data structure and limited labeled samples present ongoing challenges. However, existing contrastive learning (CL) frameworks struggle to leverage rationale-aware augmentations effectively, often lacking supervision signals that guide the extraction of salient features and facing computational efficiency issues such as high parameter counts. In this study, we demonstrate that integrating a quantum rationale generator (QRG) within our proposed Quantum Rationale-aware Graph Contrastive Learning (QRGCL) framework significantly enhances jet discrimination performance, reducing reliance on labeled data and capturing discriminative features. Evaluated on the quark-gluon jet dataset, QRGCL achieves an AUC score of $77.53\\%$ while maintaining a compact architecture of only 45 QRG parameters, outperforming classical, quantum, and hybrid GCL and GNN benchmarks. These results highlight QRGCL's potential to advance jet tagging and other complex classification tasks in high-energy physics, where computational efficiency and feature extraction limitations persist."
      },
      {
        "id": "oai:arXiv.org:2411.02979v2",
        "title": "CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model Retrieval",
        "link": "https://arxiv.org/abs/2411.02979",
        "author": "Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.02979v2 Announce Type: replace \nAbstract: Reconstructing from multi-view images is a longstanding problem in 3D vision, where neural radiance fields (NeRFs) have shown great potential and get realistic rendered images of novel views. Currently, most NeRF methods either require accurate camera poses or a large number of input images, or even both. Reconstructing NeRF from few-view images without poses is challenging and highly ill-posed. To address this problem, we propose CAD-NeRF, a method reconstructed from less than 10 images without any known poses. Specifically, we build a mini library of several CAD models from ShapeNet and render them from many random views. Given sparse-view input images, we run a model and pose retrieval from the library, to get a model with similar shapes, serving as the density supervision and pose initializations. Here we propose a multi-view pose retrieval method to avoid pose conflicts among views, which is a new and unseen problem in uncalibrated NeRF methods. Then, the geometry of the object is trained by the CAD guidance. The deformation of the density field and camera poses are optimized jointly. Then texture and density are trained and fine-tuned as well. All training phases are in self-supervised manners. Comprehensive evaluations of synthetic and real images show that CAD-NeRF successfully learns accurate densities with a large deformation from retrieved CAD models, showing the generalization abilities."
      },
      {
        "id": "oai:arXiv.org:2411.09986v2",
        "title": "Unlocking Transfer Learning for Open-World Few-Shot Recognition",
        "link": "https://arxiv.org/abs/2411.09986",
        "author": "Byeonggeun Kim, Juntae Lee, Kyuhong Shim, Simyung Chang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.09986v2 Announce Type: replace \nAbstract: Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5\\% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR."
      },
      {
        "id": "oai:arXiv.org:2411.11176v3",
        "title": "Infinite Width Limits of Self Supervised Neural Networks",
        "link": "https://arxiv.org/abs/2411.11176",
        "author": "Maximilian Fleissner, Gautham Govind Anil, Debarghya Ghoshdastidar",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.11176v3 Announce Type: replace \nAbstract: The NTK is a widely used tool in the theoretical analysis of deep learning, allowing us to look at supervised deep neural networks through the lenses of kernel regression. Recently, several works have investigated kernel models for self-supervised learning, hypothesizing that these also shed light on the behavior of wide neural networks by virtue of the NTK. However, it remains an open question to what extent this connection is mathematically sound -- it is a commonly encountered misbelief that the kernel behavior of wide neural networks emerges irrespective of the loss function it is trained on. In this paper, we bridge the gap between the NTK and self-supervised learning, focusing on two-layer neural networks trained under the Barlow Twins loss. We prove that the NTK of Barlow Twins indeed becomes constant as the width of the network approaches infinity. Our analysis technique is a bit different from previous works on the NTK and may be of independent interest. Overall, our work provides a first justification for the use of classic kernel theory to understand self-supervised learning of wide neural networks. Building on this result, we derive generalization error bounds for kernelized Barlow Twins and connect them to neural networks of finite width."
      },
      {
        "id": "oai:arXiv.org:2411.11764v3",
        "title": "Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors",
        "link": "https://arxiv.org/abs/2411.11764",
        "author": "Shovito Barua Soumma, S M Raihanul Alam, Rudmila Rahman, Umme Niraj Mahi, Abdullah Mamun, Sayyed Mostafa Mostafavi, Hassan Ghasemzadeh",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.11764v3 Announce Type: replace \nAbstract: Freezing of gait (FOG) is a debilitating symptom of Parkinson's disease that impairs mobility and safety by increasing the risk of falls. An effective FOG detection system must be accurate, real-time, and deployable in free-living environments to enable timely interventions. However, existing detection methods face challenges due to (1) intra- and inter-patient variability, (2) subject-specific training, (3) using multiple sensors in FOG dominant locations (e.g., ankles) leading to high failure points, (4) centralized, non-adaptive learning frameworks that sacrifice patient privacy and prevent collaborative model refinement across populations and disease progression, and (5) most systems are tested in controlled settings, limiting their real-world applicability for continuous in-home monitoring. Addressing these gaps, we present FOGSense, a real-world deployable FOG detection system designed for uncontrolled, free-living conditions using only a single sensor. FOGSense uses Gramian Angular Field (GAF) transformations and privacy-preserving federated deep learning to capture temporal and spatial gait patterns missed by traditional methods with a low false positive rate. We evaluated our system using a public Parkinson's dataset collected in a free-living environment. FOGSense improves accuracy by 10.4% over a single-axis accelerometer, reduces failure points compared to multi-sensor systems, and demonstrates robustness to missing values. The federated architecture allows personalized model adaptation and efficient smartphone synchronization during off-peak hours, making it effective for long-term monitoring as symptoms evolve. Overall, FOGSense achieved a 22.2% improvement in F1-score and a 74.53% reduction in false positive rate compared to state-of-the-art methods, along with enhanced sensitivity for FOG episode detection."
      },
      {
        "id": "oai:arXiv.org:2411.14517v2",
        "title": "The Double-Ellipsoid Geometry of CLIP",
        "link": "https://arxiv.org/abs/2411.14517",
        "author": "Meir Yossef Levi, Guy Gilboa",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.14517v2 Announce Type: replace \nAbstract: Contrastive Language-Image Pre-Training (CLIP) is highly instrumental in machine learning applications within a large variety of domains. We investigate the geometry of this embedding, which is still not well understood. We examine the raw unnormalized embedding and show that text and image reside on linearly separable ellipsoid shells, not centered at the origin. We explain the benefits of having this structure, allowing to better embed instances according to their uncertainty during contrastive training. Frequent concepts in the dataset yield more false negatives, inducing greater uncertainty. A new notion of conformity is introduced, which measures the average cosine similarity of an instance to any other instance within a representative data set. We show this measure can be accurately estimated by simply computing the cosine similarity to the modality mean vector. Furthermore, we find that CLIP's modality gap optimizes the matching of the conformity distributions of image and text."
      },
      {
        "id": "oai:arXiv.org:2411.15539v2",
        "title": "Large Language Model with Region-guided Referring and Grounding for CT Report Generation",
        "link": "https://arxiv.org/abs/2411.15539",
        "author": "Zhixuan Chen, Yequan Bie, Haibo Jin, Hao Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.15539v2 Announce Type: replace \nAbstract: Computed tomography (CT) report generation is crucial to assist radiologists in interpreting CT volumes, which can be time-consuming and labor-intensive. Existing methods primarily only consider the global features of the entire volume, making it struggle to focus on specific regions and potentially missing abnormalities. To address this issue, we propose Reg2RG, the first region-guided referring and grounding framework for CT report generation, which enhances diagnostic performance by focusing on anatomical regions within the volume. Specifically, we utilize masks from a universal segmentation module to capture local features for each referring region. A local feature decoupling (LFD) strategy is proposed to preserve the local high-resolution details with little computational overhead. Then the local features are integrated with global features to capture inter-regional relationships within a cohesive context. Moreover, we propose a novel region-report alignment (RRA) training strategy. It leverages the recognition of referring regions to guide the generation of region-specific reports, enhancing the model's referring and grounding capabilities while also improving the report's interpretability. A large language model (LLM) is further employed as the language decoder to generate reports from integrated visual features, facilitating region-level comprehension. Extensive experiments on two large-scale chest CT-report datasets demonstrate the superiority of our method, which outperforms several state-of-the-art methods in terms of both natural language generation and clinical efficacy metrics while preserving promising interpretability. The code is available at https://github.com/zhi-xuan-chen/Reg2RG."
      },
      {
        "id": "oai:arXiv.org:2411.16926v2",
        "title": "Context-Aware Input Orchestration for Video Inpainting",
        "link": "https://arxiv.org/abs/2411.16926",
        "author": "Hoyoung Kim, Azimbek Khudoyberdiev, Seonghwan Jeong, Jihoon Ryoo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16926v2 Announce Type: replace \nAbstract: Traditional neural network-driven inpainting methods struggle to deliver high-quality results within the constraints of mobile device processing power and memory. Our research introduces an innovative approach to optimize memory usage by altering the composition of input data. Typically, video inpainting relies on a predetermined set of input frames, such as neighboring and reference frames, often limited to five-frame sets. Our focus is to examine how varying the proportion of these input frames impacts the quality of the inpainted video. By dynamically adjusting the input frame composition based on optical flow and changes of the mask, we have observed an improvement in various contents including rapid visual context changes."
      },
      {
        "id": "oai:arXiv.org:2411.18674v2",
        "title": "Active Data Curation Effectively Distills Large-Scale Multimodal Models",
        "link": "https://arxiv.org/abs/2411.18674",
        "author": "Vishaal Udandarao, Nikhil Parthasarathy, Muhammad Ferjad Naeem, Talfan Evans, Samuel Albanie, Federico Tombari, Yongqin Xian, Alessio Tonioni, Olivier J. H\\'enaff",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.18674v2 Announce Type: replace \nAbstract: Knowledge distillation (KD) is the de facto standard for compressing large-scale models into smaller ones. Prior works have explored ever more complex KD strategies involving different objective functions, teacher-ensembles, and weight inheritance. In this work we explore an alternative, yet simple approach -- active data curation as effective distillation for contrastive multimodal pretraining. Our simple online batch selection method, ACID, outperforms strong KD baselines across various model-, data- and compute-configurations. Further, we find such an active data curation strategy to in fact be complementary to standard KD, and can be effectively combined to train highly performant inference-efficient models. Our simple and scalable pretraining framework, ACED, achieves state-of-the-art results across 27 zero-shot classification and retrieval tasks with upto 11% less inference FLOPs. We further demonstrate that our ACED models yield strong vision-encoders for training generative multimodal models in the LiT-Decoder setting, outperforming larger vision encoders for image-captioning and visual question-answering tasks."
      },
      {
        "id": "oai:arXiv.org:2411.19415v2",
        "title": "AMO Sampler: Enhancing Text Rendering with Overshooting",
        "link": "https://arxiv.org/abs/2411.19415",
        "author": "Xixi Hu, Keyang Xu, Bo Liu, Qiang Liu, Hongliang Fei",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.19415v2 Announce Type: replace \nAbstract: Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost. Code available at: https://github.com/hxixixh/amo-release."
      },
      {
        "id": "oai:arXiv.org:2412.00592v2",
        "title": "LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes",
        "link": "https://arxiv.org/abs/2412.00592",
        "author": "Shing-Hei Ho, Bao Thach, Minghan Zhu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.00592v2 Announce Type: replace \nAbstract: We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clouds from scratch, LiDAR-EDIT offers users full control over the object layout, including the number, type, and pose of objects, while keeping most of the original real-world background. Our method also provides object labels for the generated data. Compared to novel view synthesis techniques, our framework allows for the creation of counterfactual scenarios with object layouts significantly different from the original real-world scene. LiDAR-EDIT uses spherical voxelization to enforce correct LiDAR projective geometry in the generated point clouds by construction. During object removal and insertion, generative models are employed to fill the unseen background and object parts that were occluded in the original real LiDAR scans. Experimental results demonstrate that our framework produces realistic LiDAR scans with practical value for downstream tasks."
      },
      {
        "id": "oai:arXiv.org:2412.04134v2",
        "title": "M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation",
        "link": "https://arxiv.org/abs/2412.04134",
        "author": "Tao Zhang, Zhenhai Liu, Feipeng Qi, Yongjun Jiao, Tailin Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.04134v2 Announce Type: replace \nAbstract: Multiphysics simulation, which models the interactions between multiple physical processes, and multi-component simulation of complex structures are critical in fields like nuclear and aerospace engineering. Previous studies use numerical solvers or ML-based surrogate models for these simulations. However, multiphysics simulations typically require integrating multiple specialized solvers-each for a specific physical process-into a coupled program, which introduces significant development challenges. Furthermore, existing numerical algorithms struggle with highly complex large-scale structures in multi-component simulations. Here we propose compositional Multiphysics and Multi-component PDE Simulation with Diffusion models (M2PDE) to overcome these challenges. During diffusion-based training, M2PDE learns energy functions modeling the conditional probability of one physical process/component conditioned on other processes/components. In inference, M2PDE generates coupled multiphysics and multi-component solutions by sampling from the joint probability distribution. We evaluate M2PDE on two multiphysics tasks-reaction-diffusion and nuclear thermal coupling-where it achieves more accurate predictions than surrogate models in challenging scenarios. We then apply it to a multi-component prismatic fuel element problem, demonstrating that M2PDE scales from single-component training to a 64-component structure and outperforms existing domain-decomposition and graph-based approaches. The code is available at https://github.com/AI4Science-WestlakeU/M2PDE."
      },
      {
        "id": "oai:arXiv.org:2412.04454v2",
        "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
        "link": "https://arxiv.org/abs/2412.04454",
        "author": "Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.04454v2 Announce Type: replace \nAbstract: Automating GUI tasks remains challenging due to reliance on textual representations, platform-specific action spaces, and limited reasoning capabilities. We introduce Aguvis, a unified vision-based framework for autonomous GUI agents that directly operates on screen images, standardizes cross-platform interactions and incorporates structured reasoning via inner monologue. To enable this, we construct Aguvis Data Collection, a large-scale dataset with multimodal grounding and reasoning annotations, and develop a two-stage training pipeline that separates GUI grounding from planning and reasoning. Experiments show that Aguvis achieves state-of-the-art performance across offline and real-world online benchmarks, marking the first fully autonomous vision-based GUI agent that operates without closed-source models. We open-source all datasets, models, and training recipes at https://aguvis-project.github.io to advance future research."
      },
      {
        "id": "oai:arXiv.org:2412.10316v3",
        "title": "BrushEdit: All-In-One Image Inpainting and Editing",
        "link": "https://arxiv.org/abs/2412.10316",
        "author": "Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Junhao Zhuang, Ying Shan, Yuexian Zou, Qiang Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.10316v3 Announce Type: replace \nAbstract: Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence."
      },
      {
        "id": "oai:arXiv.org:2412.11142v2",
        "title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection",
        "link": "https://arxiv.org/abs/2412.11142",
        "author": "Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, Xia Hu, Yue Zhao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11142v2 Announce Type: replace \nAbstract: Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD."
      },
      {
        "id": "oai:arXiv.org:2412.11815v2",
        "title": "ColorFlow: Retrieval-Augmented Image Sequence Colorization",
        "link": "https://arxiv.org/abs/2412.11815",
        "author": "Junhao Zhuang, Xuan Ju, Zhaoyang Zhang, Yong Liu, Shiyi Zhang, Chun Yuan, Ying Shan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11815v2 Announce Type: replace \nAbstract: Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.To address this, we propose ColorFlow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. Unlike existing methods that require per-ID finetuning or explicit ID embedding extraction, we propose a novel robust and generalizable Retrieval Augmented Colorization pipeline for colorizing images with relevant color references. Our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. We utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a comprehensive benchmark for reference-based colorization. Results show that ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. We release our codes and models on our project page: https://zhuang2002.github.io/ColorFlow/."
      },
      {
        "id": "oai:arXiv.org:2412.14803v2",
        "title": "Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations",
        "link": "https://arxiv.org/abs/2412.14803",
        "author": "Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.14803v2 Announce Type: replace \nAbstract: Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6\\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\\% increase in success rates for complex real-world dexterous manipulation tasks. Project page at https://video-prediction-policy.github.io"
      },
      {
        "id": "oai:arXiv.org:2501.00062v2",
        "title": "ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis",
        "link": "https://arxiv.org/abs/2501.00062",
        "author": "James P. Beno",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00062v2 Announce Type: replace \nAbstract: Bidirectional transformers excel at sentiment analysis, and Large Language Models (LLM) are effective zero-shot learners. Might they perform better as a team? This paper explores collaborative approaches between ELECTRA and GPT-4o for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as: predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT predictions with GPT-4o-mini significantly improved performance over either model alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and yielded the lowest cost/performance ratio (\\$0.12/F1 point). However, when GPT models were fine-tuned, including predictions decreased performance. GPT-4o FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at much less cost (\\$0.38 vs. \\$1.59/F1 point). Our results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost. Both are affordable options for projects with limited resources."
      },
      {
        "id": "oai:arXiv.org:2501.00874v2",
        "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models",
        "link": "https://arxiv.org/abs/2501.00874",
        "author": "Hieu Man, Nghia Trung Ngo, Viet Dac Lai, Ryan A. Rossi, Franck Dernoncourt, Thien Huu Nguyen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00874v2 Announce Type: replace \nAbstract: Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data."
      },
      {
        "id": "oai:arXiv.org:2501.01010v2",
        "title": "CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction",
        "link": "https://arxiv.org/abs/2501.01010",
        "author": "Mohammad Shahab Sepehri, Asal Mehradfar, Mahdi Soltanolkotabi, Salman Avestimehr",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.01010v2 Announce Type: replace \nAbstract: Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks."
      },
      {
        "id": "oai:arXiv.org:2501.02407v2",
        "title": "Towards the Anonymization of the Language Modeling",
        "link": "https://arxiv.org/abs/2501.02407",
        "author": "Antoine Boutet, Lucas Magnana, Juliette S\\'en\\'echal, Helain Zimmermann",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.02407v2 Announce Type: replace \nAbstract: Rapid advances in Natural Language Processing (NLP) have revolutionized many fields, including healthcare. However, these advances raise significant privacy concerns, especially when pre-trained models fine-tuned and specialized on sensitive data can memorize and then expose and regurgitate personal information. This paper presents a privacy-preserving language modeling approach to address the problem of language models anonymization, and thus promote their sharing. Specifically, we propose both a Masking Language Modeling (MLM) methodology to specialize a BERT-like language model, and a Causal Language Modeling (CLM) methodology to specialize a GPT-like model that avoids the model from memorizing direct and indirect identifying information present in the training data. We have comprehensively evaluated our approaches using a medical dataset and compared them against different baselines. Our results indicate that by avoiding memorizing both direct and indirect identifiers during model specialization, our masking and causal language modeling schemes offer a good tradeoff for maintaining high privacy while retaining high utility."
      },
      {
        "id": "oai:arXiv.org:2501.03865v3",
        "title": "Truthful mechanisms for linear bandit games with private contexts",
        "link": "https://arxiv.org/abs/2501.03865",
        "author": "Yiting Hu, Lingjie Duan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.03865v3 Announce Type: replace \nAbstract: The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes. However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system. For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments. We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents. We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\\epsilon$-greedy algorithm incur sublinear but high regret. We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\\ln T)$ frequentist regret. Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families."
      },
      {
        "id": "oai:arXiv.org:2501.10098v2",
        "title": "landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D Images",
        "link": "https://arxiv.org/abs/2501.10098",
        "author": "Jef Jonkers, Luc Duchateau, Glenn Van Wallendael, Sofie Van Hoecke",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.10098v2 Announce Type: replace \nAbstract: Anatomical landmark localization in 2D/3D images is a critical task in medical imaging. Although many general-purpose tools exist for landmark localization in classical computer vision tasks, such as pose estimation, they lack the specialized features and modularity necessary for anatomical landmark localization applications in the medical domain. Therefore, we introduce landmarker, a Python package built on PyTorch. The package provides a comprehensive, flexible toolkit for developing and evaluating landmark localization algorithms, supporting a range of methodologies, including static and adaptive heatmap regression. landmarker enhances the accuracy of landmark identification, streamlines research and development processes, and supports various image formats and preprocessing pipelines. Its modular design allows users to customize and extend the toolkit for specific datasets and applications, accelerating innovation in medical imaging. landmarker addresses a critical need for precision and customization in landmark localization tasks not adequately met by existing general-purpose pose estimation tools."
      },
      {
        "id": "oai:arXiv.org:2501.11653v3",
        "title": "Dynamic Scene Understanding from Vision-Language Representations",
        "link": "https://arxiv.org/abs/2501.11653",
        "author": "Shahaf Pruss, Morris Alper, Hadar Averbuch-Elor",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.11653v3 Announce Type: replace \nAbstract: Images depicting complex, dynamic scenes are challenging to parse automatically, requiring both high-level comprehension of the overall situation and fine-grained identification of participating entities and their interactions. Current approaches use distinct methods tailored to sub-tasks such as Situation Recognition and detection of Human-Human and Human-Object Interactions. However, recent advances in image understanding have often leveraged web-scale vision-language (V&amp;L) representations to obviate task-specific engineering. In this work, we propose a framework for dynamic scene understanding tasks by leveraging knowledge from modern, frozen V&amp;L representations. By framing these tasks in a generic manner - as predicting and parsing structured text, or by directly concatenating representations to the input of existing models - we achieve state-of-the-art results while using a minimal number of trainable parameters relative to existing approaches. Moreover, our analysis of dynamic knowledge of these representations shows that recent, more powerful representations effectively encode dynamic scene semantics, making this approach newly possible."
      },
      {
        "id": "oai:arXiv.org:2501.13620v4",
        "title": "A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs",
        "link": "https://arxiv.org/abs/2501.13620",
        "author": "Mohit Vaishnav, Tanel Tammet",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.13620v4 Announce Type: replace \nAbstract: A fundamental challenge in artificial intelligence involves understanding the cognitive mechanisms underlying visual reasoning in sophisticated models like Vision-Language Models (VLMs). How do these models integrate visual perception with abstract thought, especially when reasoning across multiple images or requiring fine-grained compositional understanding? Drawing inspiration from cognitive science, this paper introduces a structured evaluation framework using diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to dissect the perception-reasoning interface in VLMs. We propose three distinct evaluation paradigms, mirroring human problem-solving strategies: Direct Visual Rule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule extraction and application), and Componential Analysis (CA; analytical decomposition via task-agnostic textual descriptions). These paradigms systematically vary cognitive load and probe processing stages. Notably, CA enables multi-image reasoning evaluation even for single-image architectures and isolates reasoning from perception by operating on textual descriptions. Applying this framework, we demonstrate that CA, leveraging powerful language models for reasoning over rich, independently generated descriptions, achieves new state-of-the-art (SOTA) performance on challenging benchmarks including Bongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm reasoning improves significantly when perceptual challenges are mitigated, revealing a critical perception bottleneck. Our framework provides a valuable diagnostic tool and suggests that decoupling perception (via rich, task-agnostic description) from reasoning is a promising direction for robust and general visual intelligence."
      },
      {
        "id": "oai:arXiv.org:2501.15955v2",
        "title": "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
        "link": "https://arxiv.org/abs/2501.15955",
        "author": "Jiahao Chen, Bin Qin, Jiangmeng Li, Hao Chen, Bing Su",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.15955v2 Announce Type: replace \nAbstract: Long-tailed learning has garnered increasing attention due to its practical significance. Among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. However, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. In this paper, we examine how such imbalances from pre-training affect long-tailed downstream tasks. Specifically, we find the imbalance biases inherited in foundation models on downstream task as parameter imbalance and data imbalance. During fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. Moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. To tackle both imbalances simultaneously, we build our method on causal learning and view the incomplete semantic factor as the confounder, which brings spurious correlations between input samples and labels. To resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data. Notably, we achieve an average performance increase of about $1.67\\%$ on each dataset."
      },
      {
        "id": "oai:arXiv.org:2502.00607v3",
        "title": "PAC Learning is just Bipartite Matching (Sort of)",
        "link": "https://arxiv.org/abs/2502.00607",
        "author": "Shaddin Dughmi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00607v3 Announce Type: replace \nAbstract: The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning."
      },
      {
        "id": "oai:arXiv.org:2502.00968v2",
        "title": "CoDe: Blockwise Control for Denoising Diffusion Models",
        "link": "https://arxiv.org/abs/2502.00968",
        "author": "Anuj Singh, Sayak Mukherjee, Ahmad Beirami, Hadi Jamali-Rad",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00968v2 Announce Type: replace \nAbstract: Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (CoDe), that circumvents the need for differentiable guidance functions and model finetuning. CoDe is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. Our experiments demonstrate that, despite its simplicity, CoDe offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines. Our code is available at: https://github.com/anujinho/code."
      },
      {
        "id": "oai:arXiv.org:2502.01563v3",
        "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding",
        "link": "https://arxiv.org/abs/2502.01563",
        "author": "Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01563v3 Announce Type: replace \nAbstract: Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM."
      },
      {
        "id": "oai:arXiv.org:2502.01710v4",
        "title": "DAGNet: A Dual-View Attention-Guided Network for Efficient X-ray Security Inspection",
        "link": "https://arxiv.org/abs/2502.01710",
        "author": "Shilong Hong, Yanzhou Zhou, Weichao Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01710v4 Announce Type: replace \nAbstract: With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray baggage scanner is widely deployed, they struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose a Dual-View Attention-Guided Network for Efficient X-ray Security Inspection (DAGNet). This study builds on a shared-weight backbone network as the foundation and constructs three key modules that work together: (1) Frequency Domain Interaction Module (FDIM) dynamically enhances features by adjusting frequency components based on inter-view relationships; (2) Dual-View Hierarchical Enhancement Module (DVHEM) employs cross-attention to align features between views and capture hierarchical associations; (3) Convolutional Guided Fusion Module (CGFM) fuses features to suppress redundancy while retaining critical discriminative information. Collectively, these modules substantially improve the performance of dual-view X-ray security inspection. Experimental results demonstrate that DAGNet outperforms existing state-of-the-art approaches across multiple backbone architectures. The code is available at:https://github.com/ShilongHong/DAGNet."
      },
      {
        "id": "oai:arXiv.org:2502.03253v2",
        "title": "How do Humans and Language Models Reason About Creativity? A Comparative Analysis",
        "link": "https://arxiv.org/abs/2502.03253",
        "author": "Antonio Laverghetta Jr., Tuhin Chakrabarty, Tom Hope, Jimmy Pronchick, Krupa Bhawsar, Roger E. Beaty",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03253v2 Announce Type: replace \nAbstract: Creativity assessment in science and engineering is increasingly based on both human and AI judgment, but the cognitive processes and biases behind these evaluations remain poorly understood. We conducted two experiments examining how including example solutions with ratings impact creativity evaluation, using a finegrained annotation protocol where raters were tasked with explaining their originality scores and rating for the facets of remoteness (whether the response is \"far\" from everyday ideas), uncommonness (whether the response is rare), and cleverness. In Study 1, we analyzed creativity ratings from 72 experts with formal science or engineering training, comparing those who received example solutions with ratings (example) to those who did not (no example). Computational text analysis revealed that, compared to experts with examples, no-example experts used more comparative language (e.g., \"better/worse\") and emphasized solution uncommonness, suggesting they may have relied more on memory retrieval for comparisons. In Study 2, parallel analyses with state-of-the-art LLMs revealed that models prioritized uncommonness and remoteness of ideas when rating originality, suggesting an evaluative process rooted around the semantic similarity of ideas. In the example condition, while LLM accuracy in predicting the true originality scores improved, the correlations of remoteness, uncommonness, and cleverness with originality also increased substantially -- to upwards of $0.99$ -- suggesting a homogenization in the LLMs evaluation of the individual facets. These findings highlight important implications for how humans and AI reason about creativity and suggest diverging preferences for what different populations prioritize when rating."
      },
      {
        "id": "oai:arXiv.org:2502.04667v2",
        "title": "Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization",
        "link": "https://arxiv.org/abs/2502.04667",
        "author": "Xinhao Yao, Ruifeng Ren, Yun Liao, Yong Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04667v2 Announce Type: replace \nAbstract: The integration of explicit Chain-of-Thought (CoT) reasoning into training large language models (LLMs) has advanced their reasoning capabilities, yet the mechanisms by which CoT enhances generalization remain poorly understood. This work investigates (1) \\textit{how} CoT training reshapes internal model representations and (2) \\textit{why} it improves both in-distribution (ID) and out-of-distribution (OOD) reasoning generalization. Through controlled experiments and theoretical analysis, we derive the following key insights. \\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Notably, CoT-trained models resolve intermediate results at shallower layers compared to non-CoT counterparts, freeing up deeper layers to specialize in subsequent reasoning steps. \\textbf{2)} Theoretical Analysis: the information-theoretic generalization bounds via distributional divergence can be decomposed into ID and OOD components. While ID error diminishes with sufficient training regardless of CoT, OOD error critically depends on CoT: Non-CoT training fails to generalize to OOD samples due to unseen reasoning patterns, whereas CoT training achieves near-perfect OOD generalization by mastering subtasks and reasoning compositions during training. The identified mechanisms explain our experimental results: CoT training accelerates convergence and enhances generalization from ID to both ID and OOD scenarios while maintaining robust performance even with tolerable noise. These findings are further validated on complex real-world datasets. This paper offers valuable insights for designing CoT strategies to enhance LLM reasoning robustness."
      },
      {
        "id": "oai:arXiv.org:2502.06491v2",
        "title": "Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling",
        "link": "https://arxiv.org/abs/2502.06491",
        "author": "Shenghong He",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06491v2 Announce Type: replace \nAbstract: Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm \\textbf{R}eliability-guaranteed \\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks."
      },
      {
        "id": "oai:arXiv.org:2502.12050v2",
        "title": "SpeechT: Findings of the First Mentorship in Speech Translation",
        "link": "https://arxiv.org/abs/2502.12050",
        "author": "Yasmin Moslem, Juan Juli\\'an Cea Mor\\'an, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12050v2 Announce Type: replace \nAbstract: This work presents the details and findings of the first mentorship in speech translation (SpeechT), which took place in December 2024 and January 2025. To fulfil the mentorship requirements, the participants engaged in key activities, including data preparation, modelling, and advanced research. The participants explored data augmentation techniques and compared end-to-end and cascaded speech translation systems. The projects covered various languages other than English, including Arabic, Bengali, Galician, Indonesian, Japanese, and Spanish."
      },
      {
        "id": "oai:arXiv.org:2502.14259v3",
        "title": "LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records",
        "link": "https://arxiv.org/abs/2502.14259",
        "author": "Sujeong Im, Jungwoo Oh, Edward Choi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14259v3 Announce Type: replace \nAbstract: Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions."
      },
      {
        "id": "oai:arXiv.org:2502.15251v3",
        "title": "SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training",
        "link": "https://arxiv.org/abs/2502.15251",
        "author": "Nie Lin, Takehiko Ohkawa, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, Yoichi Sato",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15251v3 Announce Type: replace \nAbstract: We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SimHand. Pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3D hand pose pre-training have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our pre-training method with contrastive learning. Specifically, we collect over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands: pairs of non-identical samples with similar hand poses. We then propose a novel contrastive learning method that embeds similar hand pairs closer in the feature space. Our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method (PeCLR) in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands.\n  Our code is available at https://github.com/ut-vision/SiMHand."
      },
      {
        "id": "oai:arXiv.org:2502.15666v2",
        "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing",
        "link": "https://arxiv.org/abs/2502.15666",
        "author": "Shoumik Saha, Soheil Feizi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.15666v2 Announce Type: replace \nAbstract: The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Such classification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate twelve state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains 14.7K samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently flag even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies."
      },
      {
        "id": "oai:arXiv.org:2502.16892v2",
        "title": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data",
        "link": "https://arxiv.org/abs/2502.16892",
        "author": "Yejian Zhang, Shingo Takada",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16892v2 Announce Type: replace \nAbstract: Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application."
      },
      {
        "id": "oai:arXiv.org:2502.17424v5",
        "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
        "link": "https://arxiv.org/abs/2502.17424",
        "author": "Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Mart\\'in Soto, Nathan Labenz, Owain Evans",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17424v5 Announce Type: replace \nAbstract: We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work."
      },
      {
        "id": "oai:arXiv.org:2502.20292v4",
        "title": "Visual Adaptive Prompting for Compositional Zero-Shot Learning",
        "link": "https://arxiv.org/abs/2502.20292",
        "author": "Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20292v4 Announce Type: replace \nAbstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2502.20490v3",
        "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
        "link": "https://arxiv.org/abs/2502.20490",
        "author": "MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20490v3 Announce Type: replace \nAbstract: Human activity is moderated by norms. However, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present \\dataset{} $\\|\\epsilon\\|$, consisting of 1,853 challenging, multi-stage MCQ questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\\% on \\dataset{} (versus a human bench of 92\\%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation (RAG) method, it is possible to use \\dataset{} to enhance normative reasoning in VLMs."
      },
      {
        "id": "oai:arXiv.org:2502.21239v4",
        "title": "Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs",
        "link": "https://arxiv.org/abs/2502.21239",
        "author": "Xiaomin Li, Zhou Yu, Ziji Zhang, Yingying Zhuang, Swair Shah, Narayanan Sadagopan, Anurag Beniwal",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.21239v4 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated remarkable performance across diverse tasks by encoding vast amounts of factual knowledge. However, they are still prone to hallucinations, generating incorrect or misleading information, often accompanied by high uncertainty. Existing methods for hallucination detection primarily focus on quantifying internal uncertainty, which arises from missing or conflicting knowledge within the model. However, hallucinations can also stem from external uncertainty, where ambiguous user queries lead to multiple possible interpretations. In this work, we introduce Semantic Volume, a novel mathematical measure for quantifying both external and internal uncertainty in LLMs. Our approach perturbs queries and responses, embeds them in a semantic space, and computes the determinant of the Gram matrix of the embedding vectors, capturing their dispersion as a measure of uncertainty. Our framework provides a generalizable and unsupervised uncertainty detection method without requiring internal access to LLMs. We conduct extensive experiments on both external and internal uncertainty detection, demonstrating that our Semantic Volume method consistently outperforms existing baselines in both tasks. Additionally, we provide theoretical insights linking our measure to differential entropy, unifying and extending previous sampling-based uncertainty measures such as the semantic entropy. Semantic Volume is shown to be a robust and interpretable approach to improving the reliability of LLMs by systematically detecting uncertainty in both user queries and model responses."
      },
      {
        "id": "oai:arXiv.org:2503.02147v2",
        "title": "Frankenstein Optimizer: Harnessing the Potential by Revisiting Optimization Tricks",
        "link": "https://arxiv.org/abs/2503.02147",
        "author": "Chia-Wei Hsu, Nien-Ti Tsou, Yu-Cheng Chen, Yang Jeong Park, Ju Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02147v2 Announce Type: replace \nAbstract: Gradient-based optimization drives the unprecedented performance of modern deep neural network models across diverse applications. Adaptive algorithms have accelerated neural network training due to their rapid convergence rates; however, they struggle to find ``flat minima\" reliably, resulting in suboptimal generalization compared to stochastic gradient descent (SGD). By revisiting various adaptive algorithms' mechanisms, we propose the Frankenstein optimizer, which combines their advantages. The proposed Frankenstein dynamically adjusts first- and second-momentum coefficients according to the optimizer's current state to directly maintain consistent learning dynamics and immediately reflect sudden gradient changes. Extensive experiments across several research domains such as computer vision, natural language processing, few-shot learning, and scientific simulations show that Frankenstein surpasses existing adaptive algorithms and SGD empirically regarding convergence speed and generalization performance. Furthermore, this research deepens our understanding of adaptive algorithms through centered kernel alignment analysis and loss landscape visualization during the learning process. Code is available at https://github.com/acctouhou/Frankenstein_optimizer"
      },
      {
        "id": "oai:arXiv.org:2503.02440v2",
        "title": "Artificial Intelligence in Reactor Physics: Current Status and Future Prospects",
        "link": "https://arxiv.org/abs/2503.02440",
        "author": "Ruizhi Zhang, Shengfeng Zhu, Kan Wang, Ding She, Jean-Philippe Argaud, Bertrand Bouriquet, Qing Li, Helin Gong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02440v2 Announce Type: replace \nAbstract: Reactor physics is the study of neutron properties, focusing on using models to examine the interactions between neutrons and materials in nuclear reactors. Artificial intelligence (AI) has made significant contributions to reactor physics, e.g., in operational simulations, safety design, real-time monitoring, core management and maintenance. This paper presents a comprehensive review of AI approaches in reactor physics, especially considering the category of Machine Learning (ML), with the aim of describing the application scenarios, frontier topics, unsolved challenges and future research directions. From equation solving and state parameter prediction to nuclear industry applications, this paper provides a step-by-step overview of ML methods applied to steady-state, transient and combustion problems. Most literature works achieve industry-demanded models by enhancing the efficiency of deterministic methods or correcting uncertainty methods, which leads to successful applications. However, research on ML methods in reactor physics is somewhat fragmented, and the ability to generalize models needs to be strengthened. Progress is still possible, especially in addressing theoretical challenges and enhancing industrial applications such as building surrogate models and digital twins."
      },
      {
        "id": "oai:arXiv.org:2503.03269v2",
        "title": "Conformal Transformations for Symmetric Power Transformers",
        "link": "https://arxiv.org/abs/2503.03269",
        "author": "Saurabh Kumar, Jacob Buckman, Carles Gelada, Sean Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.03269v2 Announce Type: replace \nAbstract: Transformers with linear attention offer significant computational advantages over softmax-based transformers but often suffer from degraded performance. The symmetric power (sympow) transformer, a particular type of linear transformer, addresses some of this performance gap by leveraging symmetric tensor embeddings, achieving comparable performance to softmax transformers. However, the finite capacity of the recurrent state in sympow transformers limits their ability to retain information, leading to performance degradation when scaling the training or evaluation context length. To address this issue, we propose the conformal-sympow transformer, which dynamically frees up capacity using data-dependent multiplicative gating and adaptively stores information using data-dependent rotary embeddings. Preliminary experiments on the LongCrawl64 dataset demonstrate that conformal-sympow overcomes the limitations of sympow transformers, achieving robust performance across scaled training and evaluation contexts."
      },
      {
        "id": "oai:arXiv.org:2503.04785v3",
        "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice",
        "link": "https://arxiv.org/abs/2503.04785",
        "author": "Jos\\'e Siqueira de Cerqueira, Kai-Kristian Kemell, Rebekah Rousi, Nannan Xi, Juho Hamari, Pekka Abrahamsson",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04785v3 Announce Type: replace \nAbstract: The rapid proliferation of Large Language Models (LLMs) has raised significant trustworthiness and ethical concerns. Despite the widespread adoption of LLMs across domains, there is still no clear consensus on how to define and operationalise trustworthiness. This study aims to bridge the gap between theoretical discussion and practical implementation by analysing research trends, definitions of trustworthiness, and practical techniques. We conducted a bibliometric mapping analysis of 2,006 publications from Web of Science (2019-2025) using the Bibliometrix, and manually reviewed 68 papers. We found a shift from traditional AI ethics discussion to LLM trustworthiness frameworks. We identified 18 different definitions of trust/trustworthiness, with transparency, explainability and reliability emerging as the most common dimensions. We identified 20 strategies to enhance LLM trustworthiness, with fine-tuning and retrieval-augmented generation (RAG) being the most prominent. Most of the strategies are developer-driven and applied during the post-training phase. Several authors propose fragmented terminologies rather than unified frameworks, leading to the risks of \"ethics washing,\" where ethical discourse is adopted without a genuine regulatory commitment. Our findings highlight: persistent gaps between theoretical taxonomies and practical implementation, the crucial role of the developer in operationalising trust, and call for standardised frameworks and stronger regulatory measures to enable trustworthy and ethical deployment of LLMs."
      },
      {
        "id": "oai:arXiv.org:2503.06222v2",
        "title": "Vision-based 3D Semantic Scene Completion via Capture Dynamic Representations",
        "link": "https://arxiv.org/abs/2503.06222",
        "author": "Meng Wang, Fan Wu, Yunchuan Qin, Ruihui Li, Zhuo Tang, Kenli Li",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06222v2 Announce Type: replace \nAbstract: The vision-based semantic scene completion task aims to predict dense geometric and semantic 3D scene representations from 2D images. However, the presence of dynamic objects in the scene seriously affects the accuracy of the model inferring 3D structures from 2D images. Existing methods simply stack multiple frames of image input to increase dense scene semantic information, but ignore the fact that dynamic objects and non-texture areas violate multi-view consistency and matching reliability. To address these issues, we propose a novel method, CDScene: Vision-based Robust Semantic Scene Completion via Capturing Dynamic Representations. First, we leverage a multimodal large-scale model to extract 2D explicit semantics and align them into 3D space. Second, we exploit the characteristics of monocular and stereo depth to decouple scene information into dynamic and static features. The dynamic features contain structural relationships around dynamic objects, and the static features contain dense contextual spatial information. Finally, we design a dynamic-static adaptive fusion module to effectively extract and aggregate complementary features, achieving robust and accurate semantic scene completion in autonomous driving scenarios. Extensive experimental results on the SemanticKITTI, SSCBench-KITTI360, and SemanticKITTI-C datasets demonstrate the superiority and robustness of CDScene over existing state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2503.06337v3",
        "title": "Pretraining Generative Flow Networks with Inexpensive Rewards for Molecular Graph Generation",
        "link": "https://arxiv.org/abs/2503.06337",
        "author": "Mohit Pandey, Gopeshh Subbaraj, Artem Cherkasov, Martin Ester, Emmanuel Bengio",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06337v3 Announce Type: replace \nAbstract: Generative Flow Networks (GFlowNets) have recently emerged as a suitable framework for generating diverse and high-quality molecular structures by learning from rewards treated as unnormalized distributions. Previous works in this framework often restrict exploration by using predefined molecular fragments as building blocks, limiting the chemical space that can be accessed. In this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative model leveraging individual atoms as building blocks to explore drug-like chemical space more comprehensively. We propose an unsupervised pre-training approach using drug-like molecule datasets, which teaches A-GFNs about inexpensive yet informative molecular descriptors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. We further implement a goal-conditioned finetuning process, which adapts A-GFNs to optimize for specific target properties. In this work, we pretrain A-GFN on a subset of ZINC dataset, and by employing robust evaluation metrics we show the effectiveness of our approach when compared to other relevant baseline methods for a wide range of drug design tasks."
      },
      {
        "id": "oai:arXiv.org:2503.06457v2",
        "title": "Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning",
        "link": "https://arxiv.org/abs/2503.06457",
        "author": "Yanbiao Ma, Wei Dai, Wenke Huang, Jiayi Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06457v2 Announce Type: replace \nAbstract: Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence. Code published at: https://github.com/WeiDai-David/2025CVPR_GGEUR"
      },
      {
        "id": "oai:arXiv.org:2503.08555v3",
        "title": "An Analysis of Safety Guarantees in Multi-Task Bayesian Optimization",
        "link": "https://arxiv.org/abs/2503.08555",
        "author": "Jannis O. Luebsen, Annika Eichler",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08555v3 Announce Type: replace \nAbstract: This paper addresses the integration of additional information sources into a Bayesian optimization framework while ensuring that safety constraints are satisfied. The interdependencies between these information sources are modeled using an unknown correlation matrix. We explore how uniform error bounds must be adjusted to maintain constraint satisfaction throughout the optimization process, considering both Bayesian and frequentist statistical perspectives. This is achieved by appropriately scaling the error bounds based on a confidence interval that can be estimated from the data. Furthermore, the efficacy of the proposed approach is demonstrated through experiments on two benchmark functions and a controller parameter optimization problem. Our results highlight a significant improvement in sample efficiency, demonstrating the methods suitability for optimizing expensive-to-evaluate functions."
      },
      {
        "id": "oai:arXiv.org:2503.17003v4",
        "title": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications",
        "link": "https://arxiv.org/abs/2503.17003",
        "author": "Jian Guan, Junfei Wu, Jia-Nan Li, Chuanqi Cheng, Wei Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17003v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their transition to real-world applications reveals a critical limitation: the inability to adapt to individual preferences while maintaining alignment with universal human values. Current alignment techniques adopt a one-size-fits-all approach that fails to accommodate users' diverse backgrounds and needs. This paper presents the first comprehensive survey of personalized alignment-a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. We propose a unified framework comprising preference memory management, personalized generation, and feedback-based alignment, systematically analyzing implementation approaches and evaluating their effectiveness across various scenarios. By examining current techniques, potential risks, and future challenges, this survey provides a structured foundation for developing more adaptable and ethically-aligned LLMs."
      },
      {
        "id": "oai:arXiv.org:2503.23236v3",
        "title": "UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows",
        "link": "https://arxiv.org/abs/2503.23236",
        "author": "Isma\\\"el Zighed, Nicolas Thome, Patrick Gallinari, Taraneh Sayadi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23236v3 Announce Type: replace \nAbstract: Reduced order models (ROMs) play a critical role in fluid mechanics by providing low-cost predictions, making them an attractive tool for engineering applications. However, for ROMs to be widely applicable, they must not only generalise well across different regimes, but also provide a measure of confidence in their predictions. While recent data-driven approaches have begun to address nonlinear reduction techniques to improve predictions in transient environments, challenges remain in terms of robustness and parametrisation. In this work, we present a nonlinear reduction strategy specifically designed for transient flows that incorporates parametrisation and uncertainty quantification. Our reduction strategy features a variational auto-encoder (VAE) that uses variational inference for confidence measurement. We use a latent space transformer that incorporates recent advances in attention mechanisms to predict dynamical systems. Attention's versatility in learning sequences and capturing their dependence on external parameters enhances generalisation across a wide range of dynamics. Prediction, coupled with confidence, enables more informed decision making and addresses the need for more robust models. In addition, this confidence is used to cost-effectively sample the parameter space, improving model performance a priori across the entire parameter space without requiring evaluation data for the entire domain."
      },
      {
        "id": "oai:arXiv.org:2503.23895v3",
        "title": "Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement",
        "link": "https://arxiv.org/abs/2503.23895",
        "author": "Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23895v3 Announce Type: replace \nAbstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG."
      },
      {
        "id": "oai:arXiv.org:2503.24235v3",
        "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
        "link": "https://arxiv.org/abs/2503.24235",
        "author": "Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, Chen Ma",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24235v3 Announce Type: replace \nAbstract: As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/"
      },
      {
        "id": "oai:arXiv.org:2504.00159v2",
        "title": "SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.00159",
        "author": "Advaith V. Sethuraman, Max Rucker, Onur Bagoren, Pou-Chun Kung, Nibarkavi N. B. Amutha, Katherine A. Skinner",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00159v2 Announce Type: replace \nAbstract: In this paper, we present SonarSplat, a novel Gaussian splatting framework for imaging sonar that demonstrates realistic novel view synthesis and models acoustic streaking phenomena. Our method represents the scene as a set of 3D Gaussians with acoustic reflectance and saturation properties. We develop a novel method to efficiently rasterize Gaussians to produce a range/azimuth image that is faithful to the acoustic image formation model of imaging sonar. In particular, we develop a novel approach to model azimuth streaking in a Gaussian splatting framework. We evaluate SonarSplat using real-world datasets of sonar images collected from an underwater robotic platform in a controlled test tank and in a real-world river environment. Compared to the state-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2 dB PSNR) and more accurate 3D reconstruction (52% lower Chamfer Distance). We also demonstrate that SonarSplat can be leveraged for azimuth streak removal."
      },
      {
        "id": "oai:arXiv.org:2504.00711v2",
        "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
        "link": "https://arxiv.org/abs/2504.00711",
        "author": "Enjun Du, Xunkai Li, Tian Jin, Zhihan Zhang, Rong-Hua Li, Guoren Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00711v2 Announce Type: replace \nAbstract: The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments."
      },
      {
        "id": "oai:arXiv.org:2504.03302v2",
        "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models",
        "link": "https://arxiv.org/abs/2504.03302",
        "author": "Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03302v2 Announce Type: replace \nAbstract: Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&amp;B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility."
      },
      {
        "id": "oai:arXiv.org:2504.03471v2",
        "title": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis",
        "link": "https://arxiv.org/abs/2504.03471",
        "author": "Xi Wang, Ziqi He, Yang Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03471v2 Announce Type: replace \nAbstract: Traditional diffusion models typically employ a U-Net architecture. Previous studies have unveiled the roles of attention blocks in the U-Net. However, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. In this study, we first theoretically proved that, re-weighting the outputs of the Transformer blocks within the U-Net is a \"free lunch\" for improving the signal-to-noise ratio during the sampling process. Next, we proposed Importance Probe to uncover and quantify the dynamic shifts in importance of the Transformer blocks throughout the denoising process. Finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. Experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. Our method can be seamlessly integrated into any U-Net-based architecture. Code: https://github.com/Hytidel/UNetReweighting"
      },
      {
        "id": "oai:arXiv.org:2504.03601v3",
        "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
        "link": "https://arxiv.org/abs/2504.03601",
        "author": "Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, Caiming Xiong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.03601v3 Announce Type: replace \nAbstract: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source 5K synthetic data trajectories and the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4; Dataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website at https://apigen-mt.github.io"
      },
      {
        "id": "oai:arXiv.org:2504.04519v3",
        "title": "SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation",
        "link": "https://arxiv.org/abs/2504.04519",
        "author": "Junjie Jiang, Zelin Wang, Manqi Zhao, Yin Li, DongSheng Jiang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.04519v3 Announce Type: replace \nAbstract: Segment Anything 2 (SAM2) enables robust single-object tracking using segmentation. To extend this to multi-object tracking (MOT), we propose SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from SAM2. To further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT. Code is available at https://github.com/TripleJoy/SAM2MOT."
      },
      {
        "id": "oai:arXiv.org:2504.05184v2",
        "title": "MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation",
        "link": "https://arxiv.org/abs/2504.05184",
        "author": "Rayan Merghani Ahmed, Adnan Iltaf, Bin Li, Shoujun Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05184v2 Announce Type: replace \nAbstract: The accurate segmentation of coronary Digital Subtraction Angiography (DSA) images is essential for diagnosing and treating coronary artery diseases. Despite advances in deep learning-based segmentation, challenges such as low contrast, noise, overlapping structures, high intra-class variance, and class imbalance limit precise vessel delineation. To overcome these limitations, we propose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture for coronary DSA image segmentation. The framework combined Multi-Scale Dilated Bottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM), which not only enhances multi-scale feature extraction but also preserve fine-grained details, and improve contextual understanding. Furthermore, we propose a new Supervised Prototypical Contrastive Loss (SPCL), which combines supervised and prototypical contrastive learning to minimize class imbalance and high intra-class variance by focusing on hard-to-classified background samples. Experiments carried out on a private coronary DSA dataset demonstrate that MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice coefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average Surface Distance (ASD) and Average Contour Distance (ACD). The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at the following GitHub profile link https://github.com/rayanmerghani/MSA-UNet3plus."
      },
      {
        "id": "oai:arXiv.org:2504.06559v2",
        "title": "TabKAN: Advancing Tabular Data Analysis using Kolmogorov-Arnold Network",
        "link": "https://arxiv.org/abs/2504.06559",
        "author": "Ali Eslamian, Alireza Afzal Aghaei, Qiang Cheng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06559v2 Announce Type: replace \nAbstract: Tabular data analysis presents unique challenges due to its heterogeneous feature types, missing values, and complex interactions. While traditional machine learning methods, such as gradient boosting, often outperform deep learning approaches, recent advancements in neural architectures offer promising alternatives. This paper introduces TabKAN, a novel framework that advances tabular data modeling using Kolmogorov-Arnold Networks (KANs). Unlike conventional deep learning models, KANs leverge learnable activation functions on edges, which improve both interpretability and training efficiency. Our contributions include: (1) the introduction of modular KAN-based architectures for tabular data analysis, (2) the development of a transfer learning framework for KAN models that supports knowledge transfer between domains, (3) the development of model-specific interpretability for tabular data learning, which reduces dependence on post hoc and model-agnostic analysis, and (4) comprehensive evaluation of vanilla supervised learning across binary and multi-class classification tasks. Through extensive benchmarking on diverse public datasets, TabKAN demonstrates superior performance in supervised learning while significantly outperforming classical and Transformer-based models in transfer learning scenarios. Our findings highlight the advantage of KAN-based architectures in transferring knowledge across domains and narrowing the gap between traditional machine learning and deep learning for structured data."
      },
      {
        "id": "oai:arXiv.org:2504.07392v4",
        "title": "ID-Booth: Identity-consistent Face Generation with Diffusion Models",
        "link": "https://arxiv.org/abs/2504.07392",
        "author": "Darian Toma\\v{s}evi\\'c, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir \\v{S}truc, Peter Peer",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07392v4 Announce Type: replace \nAbstract: Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth."
      },
      {
        "id": "oai:arXiv.org:2504.08685v2",
        "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
        "link": "https://arxiv.org/abs/2504.08685",
        "author": "Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo, Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Meng Wei, Zhiwu Qing, Fei Xiao, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08685v2 Announce Type: replace \nAbstract: This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/"
      },
      {
        "id": "oai:arXiv.org:2504.08837v2",
        "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.08837",
        "author": "Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08837v2 Announce Type: replace \nAbstract: Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. Our empirical results show the effectiveness of our approaches."
      },
      {
        "id": "oai:arXiv.org:2504.08982v2",
        "title": "Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning",
        "link": "https://arxiv.org/abs/2504.08982",
        "author": "Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08982v2 Announce Type: replace \nAbstract: Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods."
      },
      {
        "id": "oai:arXiv.org:2504.10637v2",
        "title": "Better Estimation of the KL Divergence Between Language Models",
        "link": "https://arxiv.org/abs/2504.10637",
        "author": "Afra Amini, Tim Vieira, Ryan Cotterell",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.10637v2 Announce Type: replace \nAbstract: Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient."
      },
      {
        "id": "oai:arXiv.org:2504.12240v2",
        "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
        "link": "https://arxiv.org/abs/2504.12240",
        "author": "Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.12240v2 Announce Type: replace \nAbstract: The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/."
      },
      {
        "id": "oai:arXiv.org:2504.13034v2",
        "title": "Inference-friendly Graph Compression for Graph Neural Networks",
        "link": "https://arxiv.org/abs/2504.13034",
        "author": "Yangxin Fan, Haolai Che, Yinghui Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13034v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have demonstrated promising performance in graph analysis. Nevertheless, the inference process of GNNs remains costly, hindering their applications for large graphs. This paper proposes inference-friendly graph compression (IFGC), a graph compression scheme to accelerate GNNs inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed graph $G_c$, to best preserve the inference results of $M$ over $G$, such that the result can be directly inferred by accessing $G_c$ with no or little decompression cost. (1) We characterize IFGC with a class of inference equivalence relation. The relation captures the node pairs in $G$ that are not distinguishable for GNN inference. (2) We introduce three practical specifications of IFGC for representative GNNs: structural preserving compression (SPGC), which computes $G_c$ that can be directly processed by GNN inference without decompression; ($\\alpha$, $r$)-compression, that allows for a configurable trade-off between compression ratio and inference quality, and anchored compression that preserves inference results for specific nodes of interest. For each scheme, we introduce compression and inference algorithms with guarantees of efficiency and quality of the inferred results. We conduct extensive experiments on diverse sets of large-scale graphs, which verifies the effectiveness and efficiency of our graph compression approaches."
      },
      {
        "id": "oai:arXiv.org:2504.13612v2",
        "title": "Entropic Time Schedulers for Generative Diffusion Models",
        "link": "https://arxiv.org/abs/2504.13612",
        "author": "Dejan Stancevic, Luca Ambrogioni",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13612v2 Announce Type: replace \nAbstract: The practical performance of generative diffusion models depends on the appropriate choice of the noise scheduling function, which can also be equivalently expressed as a time reparameterization. In this paper, we present a time scheduler that selects sampling points based on entropy rather than uniform time spacing, ensuring that each point contributes an equal amount of information to the final generation. We prove that this time reparameterization does not depend on the initial choice of time. Furthermore, we provide a tractable exact formula to estimate this \\emph{entropic time} for a trained model using the training loss without substantial overhead. Alongside the entropic time, inspired by the optimality results, we introduce a rescaled entropic time. In our experiments with mixtures of Gaussian distributions and ImageNet, we show that using the (rescaled) entropic times greatly improves the inference performance of trained models. In particular, we found that the image quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can be substantially increased by the rescaled entropic time reparameterization without increasing the number of function evaluations, with greater improvements in the few NFEs regime."
      },
      {
        "id": "oai:arXiv.org:2504.14693v2",
        "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark",
        "link": "https://arxiv.org/abs/2504.14693",
        "author": "Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, Gaoang Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.14693v2 Announce Type: replace \nAbstract: Recent advancements in language multimodal models (LMMs) for video have demonstrated their potential for understanding video content, yet the task of comprehending multi-discipline lectures remains largely unexplored. We introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90 open-source and proprietary models, ranging from 0.5B to 40B parameters. Our results highlight the limitations of current models in addressing the cognitive challenges presented by these lectures, especially in tasks requiring both perception and reasoning. Additionally, we explore how the number of visual tokens and the large language models influence performance, offering insights into the interplay between multimodal perception and reasoning in lecture comprehension."
      },
      {
        "id": "oai:arXiv.org:2504.15758v2",
        "title": "Observability conditions for neural state-space models with eigenvalues and their roots of unity",
        "link": "https://arxiv.org/abs/2504.15758",
        "author": "Andrew Gracyk",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.15758v2 Announce Type: replace \nAbstract: We operate through the lens of ordinary differential equations and control theory to study the concept of observability in the context of neural state-space models and the Mamba architecture. We develop strategies to enforce observability, which are tailored to a learning context, specifically where the hidden states are learnable at initial time, in conjunction to over its continuum, and high-dimensional. We also highlight our methods emphasize eigenvalues, roots of unity, or both. Our methods effectuate computational efficiency when enforcing observability, sometimes at great scale. We formulate observability conditions in machine learning based on classical control theory and discuss their computational complexity. Our nontrivial results are fivefold. We discuss observability through the use of permutations in neural applications with learnable matrices without high precision. We present two results built upon the Fourier transform that effect observability with high probability up to the randomness in the learning. These results are worked with the interplay of representations in Fourier space and their eigenstructure, nonlinear mappings, and the observability matrix. We present a result for Mamba that is similar to a Hautus-type condition, but instead employs an argument using a Vandermonde matrix instead of eigenvectors. Our final result is a shared-parameter construction of the Mamba system, which is computationally efficient in high exponentiation. We develop a training algorithm with this coupling, showing it satisfies a Robbins-Monro condition under certain orthogonality, while a more classical training procedure fails to satisfy a contraction with high Lipschitz constant."
      },
      {
        "id": "oai:arXiv.org:2504.15941v2",
        "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity",
        "link": "https://arxiv.org/abs/2504.15941",
        "author": "Fanny Jourdan, Yannick Chevalier, C\\'ecile Favre",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.15941v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks but often fall short when translating inclusive language -- such as texts containing the singular 'they' pronoun or otherwise reflecting fair linguistic protocols. Because these challenges span both computational and societal domains, it is imperative to critically evaluate how well LLMs handle inclusive translation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset designed to evaluate non-binary gender biases in machine translation systems from English to French. FairTranslate includes 2418 English-French sentence pairs related to occupations, annotated with rich metadata such as the stereotypical alignment of the occupation, grammatical gender indicator ambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B, Llama3.3-70B) on this dataset under different prompting procedures. Our results reveal substantial biases in gender representation across LLMs, highlighting persistent challenges in achieving equitable outcomes in machine translation. These findings underscore the need for focused strategies and interventions aimed at ensuring fair and inclusive language usage in LLM-based translation systems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and disclose the code for all experiments on GitHub."
      },
      {
        "id": "oai:arXiv.org:2504.16693v2",
        "title": "PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation",
        "link": "https://arxiv.org/abs/2504.16693",
        "author": "Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.16693v2 Announce Type: replace \nAbstract: While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts."
      },
      {
        "id": "oai:arXiv.org:2504.18267v2",
        "title": "Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study",
        "link": "https://arxiv.org/abs/2504.18267",
        "author": "Prajwal Chauhan, Salah Eddine Choutri, Mohamed Ghattassi, Nader Masmoudi, Saif Eddin Jabari",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18267v2 Announce Type: replace \nAbstract: This paper investigates the limitations of neural operators in learning solutions for a Hughes model, a first-order hyperbolic conservation law system for crowd dynamics. The model couples a Fokker-Planck equation representing pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes model belongs to the class of nonlinear hyperbolic systems that often exhibit complex solution structures, including shocks and discontinuities. In this study, we assess the performance of three state-of-the-art neural operators (Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural Operator) in various challenging scenarios. Specifically, we consider (1) discontinuous and Gaussian initial conditions and (2) diverse boundary conditions, while also examining the impact of different numerical schemes.\n  Our results show that these neural operators perform well in easy scenarios with fewer discontinuities in the initial condition, yet they struggle in complex scenarios with multiple initial discontinuities and dynamic boundary conditions, even when trained specifically on such complex samples. The predicted solutions often appear smoother, resulting in a reduction in total variation and a loss of important physical features. This smoothing behavior is similar to issues discussed by Daganzo (1995), where models that introduce artificial diffusion were shown to miss essential features such as shock waves in hyperbolic systems. These results suggest that current neural operator architectures may introduce unintended regularization effects that limit their ability to capture transport dynamics governed by discontinuities. They also raise concerns about generalizing these methods to traffic applications where shock preservation is essential."
      },
      {
        "id": "oai:arXiv.org:2504.18468v3",
        "title": "RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects",
        "link": "https://arxiv.org/abs/2504.18468",
        "author": "Georgios Kouros, Minye Wu, Tinne Tuytelaars",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.18468v3 Announce Type: replace \nAbstract: We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting."
      },
      {
        "id": "oai:arXiv.org:2504.19267v2",
        "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?",
        "link": "https://arxiv.org/abs/2504.19267",
        "author": "Mohamed Gado, Towhid Taliee, Muhammad Memon, Dmitry Ignatov, Radu Timofte",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19267v2 Announce Type: replace \nAbstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment."
      },
      {
        "id": "oai:arXiv.org:2504.19594v2",
        "title": "Mapping the Italian Telegram Ecosystem: Communities, Toxicity, and Hate Speech",
        "link": "https://arxiv.org/abs/2504.19594",
        "author": "Lorenzo Alvisi, Serena Tardelli, Maurizio Tesconi",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19594v2 Announce Type: replace \nAbstract: Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram."
      },
      {
        "id": "oai:arXiv.org:2504.19820v2",
        "title": "Hierarchical Uncertainty-Aware Graph Neural Network",
        "link": "https://arxiv.org/abs/2504.19820",
        "author": "Yoonhyuk Choi, Jiho Choi, Taewook Ko, Chong-Kwon Kim",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19820v2 Announce Type: replace \nAbstract: Recent research on graph neural networks (GNNs) has explored mechanisms for capturing local uncertainty and exploiting graph hierarchies to mitigate data sparsity and leverage structural properties. However, the synergistic integration of these two approaches remains underexplored. This work introduces a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN), which unifies multi-scale representation learning, principled uncertainty estimation, and self-supervised embedding diversity within a single end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and estimates uncertainty at multiple structural scales from individual nodes to higher levels. These uncertainty estimates guide a robust message-passing mechanism and attention weighting, effectively mitigating noise and adversarial perturbations while preserving predictive accuracy on semi-supervised classification tasks. We also offer key theoretical contributions, including a probabilistic formulation, rigorous uncertainty-calibration guarantees, and formal robustness bounds. Extensive experiments on standard benchmarks demonstrate that our model achieves state-of-the-art robustness and interpretability."
      },
      {
        "id": "oai:arXiv.org:2504.20110v2",
        "title": "Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling",
        "link": "https://arxiv.org/abs/2504.20110",
        "author": "Yu-hsuan Chen, Jing Bi, Cyril Ngo Ngoc, Victor Oancea, Jonathan Cagan, Levent Burak Kara",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20110v2 Announce Type: replace \nAbstract: AI-driven surrogate modeling has become an increasingly effective alternative to physics-based simulations for 3D design, analysis, and manufacturing. These models leverage data-driven methods to predict physical quantities traditionally requiring computationally expensive simulations. However, the scarcity of labeled CAD-to-simulation datasets has driven recent advancements in self-supervised and foundation models, where geometric representation learning is performed offline and later fine-tuned for specific downstream tasks. While these approaches have shown promise, their effectiveness is limited in applications requiring fine-scale geometric detail preservation. This work introduces a self-supervised geometric representation learning method designed to capture fine-scale geometric features from non-parametric 3D models. Unlike traditional end-to-end surrogate models, this approach decouples geometric feature extraction from downstream physics tasks, learning a latent space embedding guided by geometric reconstruction losses. Key elements include the essential use of near-zero level sampling and the innovative batch-adaptive attention-weighted loss function, which enhance the encoding of intricate design features. The proposed method is validated through case studies in structural mechanics, demonstrating strong performance in capturing design features and enabling accurate few-shot physics predictions. Comparisons with traditional parametric surrogate modeling highlight its potential to bridge the gap between geometric and physics-based representations, providing an effective solution for surrogate modeling in data-scarce scenarios."
      },
      {
        "id": "oai:arXiv.org:2504.20304v2",
        "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions",
        "link": "https://arxiv.org/abs/2504.20304",
        "author": "Xiulin Yang, Zhuoxuan Ju, Lanni Bu, Zoey Liu, Nathan Schneider",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20304v2 Announce Type: replace \nAbstract: CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research."
      },
      {
        "id": "oai:arXiv.org:2504.20466v2",
        "title": "LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs",
        "link": "https://arxiv.org/abs/2504.20466",
        "author": "Woo Yi Yang, Jiarui Wang, Sijing Wu, Huiyu Duan, Yuxin Zhu, Liu Yang, Kang Fu, Guangtao Zhai, Xiongkuo Min",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20466v2 Announce Type: replace \nAbstract: The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication."
      },
      {
        "id": "oai:arXiv.org:2504.20682v2",
        "title": "OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation",
        "link": "https://arxiv.org/abs/2504.20682",
        "author": "Long Liu, Cihui Yang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20682v2 Announce Type: replace \nAbstract: Table structure recognition is a key task in document analysis. However, the geometric deformation in deformed tables causes a weak correlation between content information and structure, resulting in downstream tasks not being able to obtain accurate content information. To obtain fine-grained spatial coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge response by Gradient Orientation-aware Extractor, combines a Heterogeneous Kernel Cross Fusion module and a scale-aware loss function to adapt to multi-scale objective features, and introduces mask-driven non-maximal suppression in the post-processing, which replaces the traditional bounding box suppression mechanism. Furthermore, we also propose a data generator, filling the gap in the dataset for fine-grained deformation table cell spatial coordinate localization, and derive a large-scale dataset named Deformation Wired Table (DWTAL). Experiments show that our proposed model demonstrates excellent segmentation accuracy on all mainstream instance segmentation models. The dataset and the source code are open source: https://github.com/justliulong/OGHFYOLO."
      },
      {
        "id": "oai:arXiv.org:2504.20834v2",
        "title": "Token-Efficient RL for LLM Reasoning",
        "link": "https://arxiv.org/abs/2504.20834",
        "author": "Alan Lee, Harry Tong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20834v2 Announce Type: replace \nAbstract: We propose reinforcement learning (RL) strategies tailored for reasoning in large language models (LLMs) under strict memory and compute limits, with a particular focus on compatibility with LoRA fine-tuning. Rather than relying on full-sequence updates or separate critic networks, we design critic-free methods that operate on a small, informative subset of output tokens to reduce memory usage and stabilize training. We introduce S-GRPO, a stochastic variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching approach for fine-grained credit assignment. Applied to Qwen2-1.5B, our methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show strong performance on multi-digit multiplication. Surprisingly, full-token GRPO under LoRA fails to improve over the base model, suggesting that selective token-level optimization may act as an implicit regularizer in low-parameter training regimes."
      },
      {
        "id": "oai:arXiv.org:2504.21214v2",
        "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech",
        "link": "https://arxiv.org/abs/2504.21214",
        "author": "Jinzhao Zhou, Zehong Cao, Yiqun Duan, Connor Barkley, Daniel Leong, Xiaowei Jiang, Quoc-Toan Nguyen, Ziyi Zhao, Thomas Do, Yu-Cheng Chang, Sheng-Fu Liang, Chin-teng Lin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.21214v2 Announce Type: replace \nAbstract: This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\\% accuracy on semantic-level classification and 39.6\\% in word-level classification, outperforming baseline methods by 5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research."
      },
      {
        "id": "oai:arXiv.org:2504.21772v2",
        "title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline",
        "link": "https://arxiv.org/abs/2504.21772",
        "author": "Minwoo Oh, Minsu Park, Eunil Park",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.21772v2 Announce Type: replace \nAbstract: Short video platforms like YouTube Shorts and TikTok face significant copyright compliance challenges, as infringers frequently embed arbitrary background music (BGM) to obscure original soundtracks (OST) and evade content originality detection. To tackle this issue, we propose a novel pipeline that integrates Music Source Separation (MSS) and cross-modal video-music retrieval (CMVMR). Our approach effectively separates arbitrary BGM from the original OST, enabling the restoration of authentic video audio tracks. To support this work, we introduce two domain-specific datasets: OASD-20K for audio separation and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset comprising 1,121 video and mixed-audio pairs, specifically designed for short video restoration tasks. Experimental results demonstrate that our pipeline not only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring content integrity. This approach provides an ethical and scalable solution to copyright challenges in user-generated content on short video platforms."
      },
      {
        "id": "oai:arXiv.org:2505.00001v2",
        "title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
        "link": "https://arxiv.org/abs/2505.00001",
        "author": "Shaun Baek, Shaun Esua-Mensah, Cyrus Tsui, Sejan Vigneswaralingam, Abdullah Alali, Michael Lu, Vasu Sharma, Sean O'Brien, Kevin Zhu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00001v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications."
      },
      {
        "id": "oai:arXiv.org:2505.00034v2",
        "title": "Improving Phishing Email Detection Performance of Small Large Language Models",
        "link": "https://arxiv.org/abs/2505.00034",
        "author": "Zijie Lin, Zikang Liu, Hanbo Fan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00034v2 Announce Type: replace \nAbstract: Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving both accuracy and F1 score on the SpamAssassin and CEAS\\_08 datasets. Furthermore, the fine-tuned models demonstrated strong transferability, achieving robust performance across multiple unseen phishing datasets, outperforming traditional baselines and approaching standard-sized LLMs."
      },
      {
        "id": "oai:arXiv.org:2505.00316v2",
        "title": "Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture",
        "link": "https://arxiv.org/abs/2505.00316",
        "author": "Tien Comlekoglu, J. Quetzalc\\'oatl Toledo-Mar\\'in, Tina Comlekoglu, Douglas W. DeSimone, Shayn M. Peirce, Geoffrey Fox, James A. Glazier",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00316v2 Announce Type: replace \nAbstract: The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate \\textit{in vitro} vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales."
      },
      {
        "id": "oai:arXiv.org:2505.00334v2",
        "title": "Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution",
        "link": "https://arxiv.org/abs/2505.00334",
        "author": "Luigi Sigillo, Christian Bianchi, Aurelio Uncini, Danilo Comminiello",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00334v2 Announce Type: replace \nAbstract: Image Super-Resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. The ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. While deep learning has significantly advanced SR, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. Recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. In this work, we introduce ResQu a novel SR framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. Unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. Furthermore, we also leverage the generative priors of foundation models such as Stable Diffusion. Extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding SR results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. The code will be available after the revision process."
      },
      {
        "id": "oai:arXiv.org:2505.00503v2",
        "title": "Variational OOD State Correction for Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.00503",
        "author": "Ke Jiang, Wen Jiang, Masahiro Fujisawa, Xiaoyang Tan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00503v2 Announce Type: replace \nAbstract: The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites."
      },
      {
        "id": "oai:arXiv.org:2505.00626v2",
        "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
        "link": "https://arxiv.org/abs/2505.00626",
        "author": "Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00626v2 Announce Type: replace \nAbstract: Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \\emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \\emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \\emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \\emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers."
      },
      {
        "id": "oai:arXiv.org:2505.00630v2",
        "title": "Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook",
        "link": "https://arxiv.org/abs/2505.00630",
        "author": "Muyi Bao, Shuchang Lyu, Zhaoyang Xu, Huiyu Zhou, Jinchang Ren, Shiming Xiang, Xiangtai Li, Guangliang Cheng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00630v2 Announce Type: replace \nAbstract: Deep learning has profoundly transformed remote sensing, yet prevailing architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) remain constrained by critical trade-offs: CNNs suffer from limited receptive fields, while ViTs grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. State Space Models (SSMs), particularly the recently proposed Mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. This survey presents a comprehensive review of Mamba-based methodologies in remote sensing, systematically analyzing about 120 Mamba-based remote sensing studies to construct a holistic taxonomy of innovations and applications. Our contributions are structured across five dimensions: (i) foundational principles of vision Mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid SSM formulations, (iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. By bridging the gap between SSM theory and remote sensing practice, this survey establishes Mamba as a transformative framework for remote sensing analysis. To our knowledge, this paper is the first systematic review of Mamba architectures in remote sensing. Our work provides a structured foundation for advancing research in remote sensing systems through SSM-based methods. We curate an open-source repository (https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster community-driven advancements."
      },
      {
        "id": "oai:arXiv.org:2505.00654v2",
        "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier",
        "link": "https://arxiv.org/abs/2505.00654",
        "author": "Daniel N. Nissani (Nissensohn)",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00654v2 Announce Type: replace \nAbstract: A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean."
      },
      {
        "id": "oai:arXiv.org:2505.00744v2",
        "title": "Localizing Before Answering: A Hallucination Evaluation Benchmark for Grounded Medical Multimodal LLMs",
        "link": "https://arxiv.org/abs/2505.00744",
        "author": "Dung Nguyen, Minh Khoi Ho, Huy Ta, Thanh Tam Nguyen, Qi Chen, Kumar Rav, Quy Duong Dang, Satwik Ramchandre, Son Lam Phung, Zhibin Liao, Minh-Son To, Johan Verjans, Phi Le Nguyen, Vu Minh Hieu Phan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00744v2 Announce Type: replace \nAbstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to inadequate localization reasoning. This work reveals a critical limitation in current medical LMMs: instead of analyzing relevant pathological regions, they often rely on linguistic patterns or attend to irrelevant image areas when responding to disease-related queries. To address this, we introduce HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive benchmark designed to evaluate LMMs' localization abilities and hallucination robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA pairs, with doctor-annotated anatomical segmentation masks for pathological regions. To improve visual reasoning, we propose the Localize-before-Answer (LobA) framework, which trains LMMs to localize target regions of interest and self-prompt to emphasize segmented pathological areas, generating grounded and reliable answers. Experimental results demonstrate that our approach significantly outperforms state-of-the-art biomedical LMMs on the challenging HEAL-MedVQA benchmark, advancing robustness in medical VQA."
      },
      {
        "id": "oai:arXiv.org:2505.00985v2",
        "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
        "link": "https://arxiv.org/abs/2505.00985",
        "author": "Ayan Sengupta, Yash Goel, Tanmoy Chakraborty",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00985v2 Announce Type: replace \nAbstract: We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development."
      },
      {
        "id": "oai:arXiv.org:2505.01182v2",
        "title": "TSTMotion: Training-free Scene-aware Text-to-motion Generation",
        "link": "https://arxiv.org/abs/2505.01182",
        "author": "Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01182v2 Announce Type: replace \nAbstract: Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \\textbf{T}raining-free \\textbf{S}cene-aware \\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \\href{https://tstmotion.github.io/}{Project Page}."
      },
      {
        "id": "oai:arXiv.org:2505.01315v2",
        "title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System",
        "link": "https://arxiv.org/abs/2505.01315",
        "author": "Sheikh Samit Muhaimin, Spyridon Mastorakis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01315v2 Announce Type: replace \nAbstract: The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses."
      },
      {
        "id": "oai:arXiv.org:2202.09738v2",
        "title": "The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement",
        "link": "https://arxiv.org/abs/2202.09738",
        "author": "Danni Huang, Lingyu Zhu, Zihao Lin, Hanwei Zhu, Shiqi Wang, Baoliang Chen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2202.09738v2 Announce Type: replace-cross \nAbstract: There is an increasing consensus that the design and optimization of low light image enhancement methods need to be fully driven by perceptual quality. With numerous approaches proposed to enhance low-light images, much less work has been dedicated to quality assessment and quality optimization of low-light enhancement. In this paper, to close the gap between enhancement and assessment, we propose a loop enhancement framework that produces a clear picture of how the enhancement of low-light images could be optimized towards better visual quality. In particular, we create a large-scale database for QUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as the foundation in studying and developing objective quality assessment measures. The objective quality assessment measure plays a critical bridging role between visual quality and enhancement and is further incorporated in the optimization in learning the enhancement model towards perceptual optimally. Finally, we iteratively perform the enhancement and optimization tasks, enhancing the low-light images continuously. The superiority of the proposed scheme is validated based on various low-light scenes."
      },
      {
        "id": "oai:arXiv.org:2206.06526v2",
        "title": "Overparametrized linear dimensionality reductions: From projection pursuit to two-layer neural networks",
        "link": "https://arxiv.org/abs/2206.06526",
        "author": "Andrea Montanari, Kangjie Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2206.06526v2 Announce Type: replace-cross \nAbstract: Given a cloud of $n$ data points in $\\mathbb{R}^d$, consider all projections onto $m$-dimensional subspaces of $\\mathbb{R}^d$ and, for each such projection, the empirical distribution of the projected points. What does this collection of probability distributions look like when $n,d$ grow large?\n  We consider this question under the null model in which the points are i.i.d. standard Gaussian vectors, focusing on the asymptotic regime in which $n,d\\to\\infty$, with $n/d\\to\\alpha\\in (0,\\infty)$, while $m$ is fixed. Denoting by $\\mathscr{F}_{m, \\alpha}$ the set of probability distributions in $\\mathbb{R}^m$ that arise as low-dimensional projections in this limit, we establish new inner and outer bounds on $\\mathscr{F}_{m, \\alpha}$. In particular, we characterize the Wasserstein radius of $\\mathscr{F}_{m,\\alpha}$ up to constant multiplicative factors, and determine it exactly for $m=1$. We also prove sharp bounds in terms of Kullback-Leibler divergence and R\\'{e}nyi information dimension.\n  The previous question has application to unsupervised learning methods, such as projection pursuit and independent component analysis. We introduce a version of the same problem that is relevant for supervised learning, and prove a sharp Wasserstein radius bound. As an application, we establish an upper bound on the interpolation threshold of two-layers neural networks with $m$ hidden neurons."
      },
      {
        "id": "oai:arXiv.org:2206.13269v3",
        "title": "Wasserstein Distributionally Robust Estimation in High Dimensions: Performance Analysis and Optimal Hyperparameter Tuning",
        "link": "https://arxiv.org/abs/2206.13269",
        "author": "Liviu Aolaritei, Soroosh Shafiee, Florian D\\\"orfler",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2206.13269v3 Announce Type: replace-cross \nAbstract: Distributionally robust optimization (DRO) has become a powerful framework for estimation under uncertainty, offering strong out-of-sample performance and principled regularization. In this paper, we propose a DRO-based method for linear regression and address a central question: how to optimally choose the robustness radius, which controls the trade-off between robustness and accuracy. Focusing on high-dimensional settings where the dimension and the number of samples are both large and comparable in size, we employ tools from high-dimensional asymptotic statistics to precisely characterize the estimation error of the resulting estimator. Remarkably, this error can be recovered by solving a simple convex-concave optimization problem involving only four scalar variables. This characterization enables efficient selection of the radius that minimizes the estimation error. In doing so, it achieves the same effect as cross-validation, but at a fraction of the computational cost. Numerical experiments confirm that our theoretical predictions closely match empirical performance and that the optimal radius selected through our method aligns with that chosen by cross-validation, highlighting both the accuracy and the practical benefits of our approach."
      },
      {
        "id": "oai:arXiv.org:2303.07546v2",
        "title": "Constrained Adversarial Learning for Automated Software Testing: a literature review",
        "link": "https://arxiv.org/abs/2303.07546",
        "author": "Jo\\~ao Vitorino, Tiago Dias, Tiago Fonseca, Eva Maia, Isabel Pra\\c{c}a",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2303.07546v2 Announce Type: replace-cross \nAbstract: It is imperative to safeguard computer applications and information systems against the growing number of cyber-attacks. Automated software testing tools can be developed to quickly analyze many lines of code and detect vulnerabilities by generating function-specific testing data. This process draws similarities to the constrained adversarial examples generated by adversarial machine learning methods, so there could be significant benefits to the integration of these methods in testing tools to identify possible attack vectors. Therefore, this literature review is focused on the current state-of-the-art of constrained data generation approaches applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance their software testing tools with adversarial testing methods and improve the resilience and robustness of their information systems. The found approaches were systematized, and the advantages and limitations of those specific for white-box, grey-box, and black-box testing were analyzed, identifying research gaps and opportunities to automate the testing tools with data generated by adversarial attacks."
      },
      {
        "id": "oai:arXiv.org:2310.01408v3",
        "title": "Generalized Animal Imitator: Agile Locomotion with Versatile Motion Prior",
        "link": "https://arxiv.org/abs/2310.01408",
        "author": "Ruihan Yang, Zhuoqun Chen, Jianhan Ma, Chongyi Zheng, Yiyu Chen, Quan Nguyen, Xiaolong Wang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2310.01408v3 Announce Type: replace-cross \nAbstract: The agility of animals, particularly in complex activities such as running, turning, jumping, and backflipping, stands as an exemplar for robotic system design. Transferring this suite of behaviors to legged robotic systems introduces essential inquiries: How can a robot learn multiple locomotion behaviors simultaneously? How can the robot execute these tasks with a smooth transition? How to integrate these skills for wide-range applications? This paper introduces the Versatile Instructable Motion prior (VIM) - a Reinforcement Learning framework designed to incorporate a range of agile locomotion tasks suitable for advanced robotic applications. Our framework enables legged robots to learn diverse agile low-level skills by imitating animal motions and manually designed motions. Our Functionality reward guides the robot's ability to adopt varied skills, and our Stylization reward ensures that robot motions align with reference motions. Our evaluations of the VIM framework span both simulation and the real world. Our framework allows a robot to concurrently learn diverse agile locomotion skills using a single learning-based controller in the real world. Videos can be found on our website: https://rchalyang.github.io/VIM/"
      },
      {
        "id": "oai:arXiv.org:2310.02075v4",
        "title": "Learning Quantum Processes with Quantum Statistical Queries",
        "link": "https://arxiv.org/abs/2310.02075",
        "author": "Chirag Wadhwa, Mina Doosti",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2310.02075v4 Announce Type: replace-cross \nAbstract: In this work, we initiate the study of learning quantum processes from quantum statistical queries. We focus on two fundamental learning tasks in this new access model: shadow tomography of quantum processes and process tomography with respect to diamond distance. For the former, we present an efficient average-case algorithm along with a nearly matching lower bound with respect to the number of observables to be predicted. For the latter, we present average-case query complexity lower bounds for learning classes of unitaries. We obtain an exponential lower bound for learning unitary 2-designs and a doubly exponential lower bound for Haar-random unitaries. Finally, we demonstrate the practical relevance of our access model by applying our learning algorithm to attack an authentication protocol using Classical-Readout Quantum Physically Unclonable Functions, partially addressing an important open question in quantum hardware security."
      },
      {
        "id": "oai:arXiv.org:2310.04606v2",
        "title": "Robust Transfer Learning with Unreliable Source Data",
        "link": "https://arxiv.org/abs/2310.04606",
        "author": "Jianqing Fan, Cheng Gao, Jason M. Klusowski",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2310.04606v2 Announce Type: replace-cross \nAbstract: This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distribution. We introduce a novel quantity called the ''ambiguity level'' that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements. Our proposed ''Transfer Around Boundary'' (TAB) model, with a threshold balancing the performance of target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer. Moreover, we demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors. Simulation studies lend further support to the effectiveness of TAB. We also provide simple approaches to bound the excess misclassification error without the need for specialized knowledge in transfer learning."
      },
      {
        "id": "oai:arXiv.org:2311.02181v3",
        "title": "Joint Problems in Learning Multiple Dynamical Systems",
        "link": "https://arxiv.org/abs/2311.02181",
        "author": "Mengjia Niu, Xiaoyu He, Petr Ry\\v{s}av\\'y, Quan Zhou, Jakub Marecek",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2311.02181v3 Announce Type: replace-cross \nAbstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results. The key highlight of this method is that it does not require a predefined hidden state dimension but instead provides an upper bound. Additionally, it offers guidance for determining regularization in the system identification."
      },
      {
        "id": "oai:arXiv.org:2401.15695v2",
        "title": "HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable In-The-Wild Navigation",
        "link": "https://arxiv.org/abs/2401.15695",
        "author": "David Bethge, Daniel Bulanda, Adam Kozlowski, Thomas Kosch, Albrecht Schmidt, Tobias Grosse-Puppendahl",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2401.15695v2 Announce Type: replace-cross \nAbstract: Routes represent an integral part of triggering emotions in drivers. Navigation systems allow users to choose a navigation strategy, such as the fastest or shortest route. However, they do not consider the driver's emotional well-being. We present HappyRouting, a novel navigation-based empathic car interface guiding drivers through real-world traffic while evoking positive emotions. We propose design considerations, derive a technical architecture, and implement a routing optimization framework. Our contribution is a machine learning-based generated emotion map layer, predicting emotions along routes based on static and dynamic contextual data. We evaluated HappyRouting in a real-world driving study (N=13), finding that happy routes increase subjectively perceived valence by 11% (p=.007). Although happy routes take 1.25 times longer on average, participants perceived the happy route as shorter, presenting an emotion-enhanced alternative to today's fastest routing mechanisms. We discuss how emotion-based routing can be integrated into navigation apps, promoting emotional well-being for mobility use."
      },
      {
        "id": "oai:arXiv.org:2402.07407v2",
        "title": "Conformal Predictive Programming for Chance Constrained Optimization",
        "link": "https://arxiv.org/abs/2402.07407",
        "author": "Yiqi Zhao, Xinyi Yu, Matteo Sesia, Jyotirmoy V. Deshmukh, Lars Lindemann",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2402.07407v2 Announce Type: replace-cross \nAbstract: We propose conformal predictive programming (CPP), a framework to solve chance constrained optimization problems, i.e., optimization problems with constraints that are functions of random variables. CPP utilizes samples from these random variables along with the quantile lemma - central to conformal prediction - to transform the chance constrained optimization problem into a deterministic problem with a quantile reformulation. CPP inherits a priori guarantees on constraint satisfaction from existing sample average approximation approaches for a class of chance constrained optimization problems, and it provides a posteriori guarantees that are of conditional and marginal nature otherwise. The strength of CPP is that it can easily support different variants of conformal prediction which have been (or will be) proposed within the conformal prediction community. To illustrate this, we present robust CPP to deal with distribution shifts in the random variables and Mondrian CPP to deal with class conditional chance constraints. To enable tractable solutions to the quantile reformulation, we present a mixed integer programming method (CPP-MIP) encoding, a bilevel optimization strategy (CPP-Bilevel), and a sampling-and-discarding optimization strategy (CPP-Discarding). We also extend CPP to deal with joint chance constrained optimization (JCCO). In a series of case studies, we show the validity of the aforementioned approaches, empirically compare CPP-MIP, CPP-Bilevel, as well as CPP-Discarding, and illustrate the advantage of CPP as compared to scenario approach."
      },
      {
        "id": "oai:arXiv.org:2402.14264v3",
        "title": "Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation",
        "link": "https://arxiv.org/abs/2402.14264",
        "author": "Jikai Jin, Vasilis Syrgkanis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2402.14264v3 Announce Type: replace-cross \nAbstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation."
      },
      {
        "id": "oai:arXiv.org:2403.07569v2",
        "title": "Exploring Challenges in Deep Learning of Single-Station Ground Motion Records",
        "link": "https://arxiv.org/abs/2403.07569",
        "author": "\\\"Umit Mert \\c{C}a\\u{g}lar, Baris Yilmaz, Melek T\\\"urkmen, Erdem Akag\\\"und\\\"uz, Salih Tileylioglu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2403.07569v2 Announce Type: replace-cross \nAbstract: Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models truly extract meaningful patterns from these complex time-series signals remains underexplored. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. Our experimental results reveal a strong dependence on the highly correlated Primary (P) and Secondary (S) phase arrival times. These findings expose a critical gap in the current research landscape, highlighting the lack of robust methodologies for deep learning from single-station ground motion recordings that do not rely on auxiliary inputs."
      },
      {
        "id": "oai:arXiv.org:2403.10266v4",
        "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
        "link": "https://arxiv.org/abs/2403.10266",
        "author": "Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, Yang You",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2403.10266v4 Announce Type: replace-cross \nAbstract: Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume."
      },
      {
        "id": "oai:arXiv.org:2403.16933v3",
        "title": "Backpropagation through space, time, and the brain",
        "link": "https://arxiv.org/abs/2403.16933",
        "author": "Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2403.16933v3 Announce Type: replace-cross \nAbstract: How physical networks of neurons, bound by spatio-temporal locality constraints, can perform efficient credit assignment, remains, to a large extent, an open question. In machine learning, the answer is almost universally given by the error backpropagation algorithm, through both space and time. However, this algorithm is well-known to rely on biologically implausible assumptions, in particular with respect to spatio-temporal (non-)locality. Alternative forward-propagation models such as real-time recurrent learning only partially solve the locality problem, but only at the cost of scaling, due to prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of backpropagation through space and time in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the morphology of dendritic trees to enable more complex information storage and processing in single neurons, as well as the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, effectively performing a spatio-temporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint variables necessary for useful parameter updates."
      },
      {
        "id": "oai:arXiv.org:2404.07452v2",
        "title": "RiskLabs: Predicting Financial Risk Using Large Language Model based on Multimodal and Multi-Sources Data",
        "link": "https://arxiv.org/abs/2404.07452",
        "author": "Yupeng Cao, Zhi Chen, Prashant Kumar, Qingyun Pei, Yangyang Yu, Haohang Li, Fabrizio Dimino, Lorenzo Ausiello, K. P. Subbalakshmi, Papa Momar Ndiaye",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2404.07452v2 Announce Type: replace-cross \nAbstract: The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering, and stock movement prediction (binary classification), the application of LLMs to financial risk prediction remains underexplored. Addressing this gap, in this paper, we introduce RiskLabs, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely integrates multimodal financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data to improve financial risk prediction. Empirical results demonstrate RiskLabs' effectiveness in forecasting both market volatility and variance. Through comparative experiments, we examine the contributions of different data sources to financial risk assessment and highlight the crucial role of LLMs in this process. We also discuss the challenges associated with using LLMs for financial risk prediction and explore the potential of combining them with multimodal data for this purpose."
      },
      {
        "id": "oai:arXiv.org:2405.00318v3",
        "title": "Covariant spatio-temporal receptive fields for spiking neural networks",
        "link": "https://arxiv.org/abs/2405.00318",
        "author": "Jens Egholm Pedersen, J\\\"org Conradt, Tony Lindeberg",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2405.00318v3 Announce Type: replace-cross \nAbstract: Biological nervous systems constitute important sources of inspiration towards computers that are faster, cheaper, and more energy efficient. Neuromorphic disciplines view the brain as a coevolved system, simultaneously optimizing the hardware and the algorithms running on it. There are clear efficiency gains when bringing the computations into a physical substrate, but we presently lack theories to guide efficient implementations. Here, we present a principled computational model for neuromorphic systems in terms of spatio-temporal receptive fields, based on affine Gaussian kernels over space and leaky-integrator and leaky integrate-and-fire models over time. Our theory is provably covariant to spatial affine and temporal scaling transformations, and with close similarities to the visual processing in mammalian brains. We use these spatio-temporal receptive fields as a prior in an event-based vision task, and show that this improves the training of spiking networks, which otherwise is known as problematic for event-based vision. This work combines efforts within scale-space theory and computational neuroscience to identify theoretically well-founded ways to process spatio-temporal signals in neuromorphic systems. Our contributions are immediately relevant for signal processing and event-based vision, and can be extended to other processing tasks over space and time, such as memory and control."
      },
      {
        "id": "oai:arXiv.org:2405.01463v3",
        "title": "Dynamic Local Average Treatment Effects",
        "link": "https://arxiv.org/abs/2405.01463",
        "author": "Ravi B. Sojitra, Vasilis Syrgkanis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2405.01463v3 Announce Type: replace-cross \nAbstract: We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may not comply with encouragements based on unobserved confounders. For settings with binary treatments and encouragements, we provide nonparametric identification, estimation, and inference for Dynamic Local Average Treatment Effects (LATEs), which are expected values of multiple time period treatment effect contrasts for the respective complier subpopulations. Under One Sided Noncompliance and sequential extensions of the assumptions in Imbens and Angrist (1994), we show that one can identify Dynamic LATEs that correspond to treating at single time steps. In Staggered Adoption settings, we show that the assumptions are sufficient to identify Dynamic LATEs for treating in multiple time periods. Moreover, this result extends to any setting where the effect of a treatment in one period is uncorrelated with the compliance event in a subsequent period."
      },
      {
        "id": "oai:arXiv.org:2406.02970v2",
        "title": "Which exceptional low-dimensional projections of a Gaussian point cloud can be found in polynomial time?",
        "link": "https://arxiv.org/abs/2406.02970",
        "author": "Andrea Montanari, Kangjie Zhou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2406.02970v2 Announce Type: replace-cross \nAbstract: Given $d$-dimensional standard Gaussian vectors $\\boldsymbol{x}_1,\\dots, \\boldsymbol{x}_n$, we consider the set of all empirical distributions of its $m$-dimensional projections, for $m$ a fixed constant. Diaconis and Freedman (1984) proved that, if $n/d\\to \\infty$, all such distributions converge to the standard Gaussian distribution. In contrast, we study the proportional asymptotics, whereby $n,d\\to \\infty$ with $n/d\\to \\alpha \\in (0, \\infty)$. In this case, the projection of the data points along a typical random subspace is again Gaussian, but the set $\\mathscr{F}_{m,\\alpha}$ of all probability distributions that are asymptotically feasible as $m$-dimensional projections contains non-Gaussian distributions corresponding to exceptional subspaces.\n  Non-rigorous methods from statistical physics yield an indirect characterization of $\\mathscr{F}_{m,\\alpha}$ in terms of a generalized Parisi formula. Motivated by the goal of putting this formula on a rigorous basis, and to understand whether these projections can be found efficiently, we study the subset $\\mathscr{F}^{\\rm alg}_{m,\\alpha}\\subseteq \\mathscr{F}_{m,\\alpha}$ of distributions that can be realized by a class of iterative algorithms. We prove that this set is characterized by a certain stochastic optimal control problem, and obtain a dual characterization of this problem in terms of a variational principle that extends Parisi's formula.\n  As a byproduct, we obtain computationally achievable values for a class of random optimization problems including `generalized spherical perceptron' models."
      },
      {
        "id": "oai:arXiv.org:2406.08918v3",
        "title": "Beyond the Calibration Point: Mechanism Comparison in Differential Privacy",
        "link": "https://arxiv.org/abs/2406.08918",
        "author": "Georgios Kaissis, Stefan Kolek, Borja Balle, Jamie Hayes, Daniel Rueckert",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2406.08918v3 Announce Type: replace-cross \nAbstract: In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can vary substantially even between mechanisms sharing a given $(\\varepsilon, \\delta)$, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities."
      },
      {
        "id": "oai:arXiv.org:2407.08159v4",
        "title": "Model-agnostic clean-label backdoor mitigation in cybersecurity environments",
        "link": "https://arxiv.org/abs/2407.08159",
        "author": "Giorgio Severi, Simona Boboila, John Holodnak, Kendra Kratkiewicz, Rauf Izmailov, Michael J. De Lucia, Alina Oprea",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.08159v4 Announce Type: replace-cross \nAbstract: The training phase of machine learning models is a delicate step, especially in cybersecurity contexts. Recent research has surfaced a series of insidious training-time attacks that inject backdoors in models designed for security classification tasks without altering the training labels. With this work, we propose new techniques that leverage insights in cybersecurity threat models to effectively mitigate these clean-label poisoning attacks, while preserving the model utility. By performing density-based clustering on a carefully chosen feature subspace, and progressively isolating the suspicious clusters through a novel iterative scoring procedure, our defensive mechanism can mitigate the attacks without requiring many of the common assumptions in the existing backdoor defense literature. To show the generality of our proposed mitigation, we evaluate it on two clean-label model-agnostic attacks on two different classic cybersecurity data modalities: network flows classification and malware classification, using gradient boosting and neural network models."
      },
      {
        "id": "oai:arXiv.org:2407.15687v4",
        "title": "SoftCVI: Contrastive variational inference with self-generated soft labels",
        "link": "https://arxiv.org/abs/2407.15687",
        "author": "Daniel Ward, Mark Beaumont, Matteo Fasiolo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2407.15687v4 Announce Type: replace-cross \nAbstract: Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both are often challenging to apply reliably, particularly when the posterior has complex geometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI), which allows a family of variational objectives to be derived through a contrastive estimation framework. The approach parameterizes a classifier in terms of a variational distribution, reframing the inference task as a contrastive estimation problem aiming to identify a single true posterior sample among a set of samples. Despite this framing, we do not require positive or negative samples, but rather learn by sampling the variational distribution and computing ground truth soft classification labels from the unnormalized posterior itself. The objectives have zero variance gradient when the variational approximation is exact, without the need for specialized gradient estimators. We empirically investigate the performance on a variety of Bayesian inference tasks, using both simple (e.g. normal) and expressive (normalizing flow) variational distributions. We find that SoftCVI can be used to form objectives which are stable to train and mass-covering, frequently outperforming inference with other variational approaches."
      },
      {
        "id": "oai:arXiv.org:2408.05215v2",
        "title": "Physics-Informed Weakly Supervised Learning for Interatomic Potentials",
        "link": "https://arxiv.org/abs/2408.05215",
        "author": "Makoto Takamoto, Viktor Zaverkin, Mathias Niepert",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2408.05215v2 Announce Type: replace-cross \nAbstract: Machine learning plays an increasingly important role in computational chemistry and materials science, complementing computationally intensive ab initio and first-principles methods. Despite their utility, machine-learning models often lack generalization capability and robustness during atomistic simulations, yielding unphysical energy and force predictions that hinder their real-world applications. We address this challenge by introducing a physics-informed, weakly supervised approach for training machine-learned interatomic potentials (MLIPs). We introduce two novel loss functions, extrapolating the potential energy via a Taylor expansion and using the concept of conservative forces. Our approach improves the accuracy of MLIPs applied to training tasks with sparse training data sets and reduces the need for pre-training computationally demanding models with large data sets. Particularly, we perform extensive experiments demonstrating reduced energy and force errors -- often lower by a factor of two -- for various baseline models and benchmark data sets. Moreover, we demonstrate improved robustness during MD simulations of the MLIP models trained with the proposed weakly supervised loss. Finally, our approach improves the fine-tuning of foundation models on sparse, highly accurate ab initio data. An implementation of our method and scripts for executing experiments are available at https://github.com/nec-research/PICPS-ML4Sci."
      },
      {
        "id": "oai:arXiv.org:2409.05314v3",
        "title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications",
        "link": "https://arxiv.org/abs/2409.05314",
        "author": "Ali Maatouk, Kenny Chirino Ampudia, Rex Ying, Leandros Tassiulas",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.05314v3 Announce Type: replace-cross \nAbstract: The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization. This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations. This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale question-and-answer dataset tailored to the domain. Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques. We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior. Leveraging these findings, we develop and open-source Tele-LLMs, the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications. Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval and telecommunications-related literature tasks while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon."
      },
      {
        "id": "oai:arXiv.org:2410.01859v4",
        "title": "Enhancing End Stage Renal Disease Outcome Prediction: A Multi-Sourced Data-Driven Approach",
        "link": "https://arxiv.org/abs/2410.01859",
        "author": "Yubo Li, Rema Padman",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.01859v4 Announce Type: replace-cross \nAbstract: Objective: To improve prediction of Chronic Kidney Disease (CKD) progression to End Stage Renal Disease (ESRD) using machine learning (ML) and deep learning (DL) models applied to an integrated clinical and claims dataset of varying observation windows, supported by explainable AI (XAI) to enhance interpretability and reduce bias.\n  Materials and Methods: We utilized data about 10,326 CKD patients, combining their clinical and claims information from 2009 to 2018. Following data preprocessing, cohort identification, and feature engineering, we evaluated multiple statistical, ML and DL models using data extracted from five distinct observation windows. Feature importance and Shapley value analysis were employed to understand key predictors. Models were tested for robustness, clinical relevance, misclassification errors and bias issues.\n  Results: Integrated data models outperformed those using single data sources, with the Long Short-Term Memory (LSTM) model achieving the highest AUC (0.93) and F1 score (0.65). A 24-month observation window was identified as optimal for balancing early detection and prediction accuracy. The 2021 eGFR equation improved prediction accuracy and reduced racial bias, notably for African American patients. Discussion: Improved ESRD prediction accuracy, results interpretability and bias mitigation strategies presented in this study have the potential to significantly enhance CKD and ESRD management, support targeted early interventions and reduce healthcare disparities.\n  Conclusion: This study presents a robust framework for predicting ESRD outcomes in CKD patients, improving clinical decision-making and patient care through multi-sourced, integrated data and AI/ML methods. Future research will expand data integration and explore the application of this framework to other chronic diseases."
      },
      {
        "id": "oai:arXiv.org:2410.12689v2",
        "title": "A distance function for stochastic matrices",
        "link": "https://arxiv.org/abs/2410.12689",
        "author": "Antony R. Lee, Peter Tino, Iain Bruce Styles",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.12689v2 Announce Type: replace-cross \nAbstract: Motivated by information geometry, a distance function on the space of stochastic matrices is advocated. Starting with sequences of Markov chains the Bhattacharyya angle is advocated as the natural tool for comparing both short and long term Markov chain runs. Bounds on the convergence of the distance and mixing times are derived. Guided by the desire to compare different Markov chain models, especially in the setting of healthcare processes, a new distance function on the space of stochastic matrices is presented. It is a true distance measure which has a closed form and is efficient to implement for numerical evaluation. In the case of ergodic Markov chains, it is shown that considering either the Bhattacharyya angle on Markov sequences or the new stochastic matrix distance leads to the same distance between models."
      },
      {
        "id": "oai:arXiv.org:2410.13849v2",
        "title": "From Gradient Clipping to Normalization for Heavy Tailed SGD",
        "link": "https://arxiv.org/abs/2410.13849",
        "author": "Florian H\\\"ubler, Ilyas Fatkhullin, Niao He",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2410.13849v2 Announce Type: replace-cross \nAbstract: Recent empirical evidence indicates that many machine learning applications involve heavy-tailed gradient noise, which challenges the standard assumptions of bounded variance in stochastic optimization. Gradient clipping has emerged as a popular tool to handle this heavy-tailed noise, as it achieves good performance in this setting both theoretically and practically. However, our current theoretical understanding of non-convex gradient clipping has three main shortcomings. First, the theory hinges on large, increasing clipping thresholds, which are in stark contrast to the small constant clipping thresholds employed in practice. Second, clipping thresholds require knowledge of problem-dependent parameters to guarantee convergence. Lastly, even with this knowledge, current sampling complexity upper bounds for the method are sub-optimal in nearly all parameters. To address these issues, we study convergence of Normalized SGD (NSGD). First, we establish a parameter-free sample complexity for NSGD of $\\mathcal{O}\\left(\\varepsilon^{-\\frac{2p}{p-1}}\\right)$ to find an $\\varepsilon$-stationary point. Furthermore, we prove tightness of this result, by providing a matching algorithm-specific lower bound. In the setting where all problem parameters are known, we show this complexity is improved to $\\mathcal{O}\\left(\\varepsilon^{-\\frac{3p-2}{p-1}}\\right)$, matching the previously known lower bound for all first-order methods in all problem dependent parameters. Finally, we establish high-probability convergence of NSGD with a mild logarithmic dependence on the failure probability. Our work complements the studies of gradient clipping under heavy tailed noise improving the sample complexities of existing algorithms and offering an alternative mechanism to achieve high probability convergence."
      },
      {
        "id": "oai:arXiv.org:2411.12013v2",
        "title": "Neural and Time-Series Approaches for Pricing Weather Derivatives: Performance and Regime Adaptation Using Satellite Data",
        "link": "https://arxiv.org/abs/2411.12013",
        "author": "Marco Hening Tallarico, Pablo Olivares",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.12013v2 Announce Type: replace-cross \nAbstract: This paper studies pricing of weather-derivative (WD) contracts on temperature and precipitation. For temperature-linked strangles in Toronto and Chicago, we benchmark a harmonic-regression/ARMA model against a feed-forward neural network (NN), finding that the NN reduces out-of-sample mean-squared error (MSE) and materially shifts December fair values relative to both the time-series model and the industry-standard Historic Burn Approach (HBA).\n  For precipitation, we employ a compound Poisson--Gamma framework: shape and scale parameters are estimated via maximum likelihood estimation (MLE) and via a convolutional neural network (CNN) trained on 30-day rainfall sequences spanning multiple seasons. The CNN adaptively learns season-specific $(\\alpha,\\beta)$ mappings, thereby capturing heterogeneity across regimes that static i.i.d.\\ fits miss. At valuation, we assume days are i.i.d.\\ $\\Gamma(\\hat{\\alpha},\\hat{\\beta})$ within each regime and apply a mean-count approximation (replacing the Poisson count by its mean ($n\\hat{\\lambda}$) to derive closed-form strangle prices.\n  Exploratory analysis of 1981--2023 NASA POWER data confirms pronounced seasonal heterogeneity in $(\\alpha,\\beta)$ between summer and winter, demonstrating that static global fits are inadequate. Back-testing on Toronto and Chicago grids shows that our regime-adaptive CNN yields competitive valuations and underscores how model choice can shift strangle prices. Payoffs are evaluated analytically when possible and by simulation elsewhere, enabling a like-for-like comparison of forecasting and valuation methods."
      },
      {
        "id": "oai:arXiv.org:2411.16195v2",
        "title": "On the Robustness of the Successive Projection Algorithm",
        "link": "https://arxiv.org/abs/2411.16195",
        "author": "Giovanni Barbarino, Nicolas Gillis",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16195v2 Announce Type: replace-cross \nAbstract: The successive projection algorithm (SPA) is a workhorse algorithm to learn the $r$ vertices of the convex hull of a set of $(r-1)$-dimensional data points, a.k.a. a latent simplex, which has numerous applications in data science. In this paper, we revisit the robustness to noise of SPA and several of its variants. In particular, when $r \\geq 3$, we prove the tightness of the existing error bounds for SPA and for two more robust preconditioned variants of SPA. We also provide significantly improved error bounds for SPA, by a factor proportional to the conditioning of the $r$ vertices, in two special cases: for the first extracted vertex, and when $r \\leq 2$. We then provide further improvements for the error bounds of a translated version of SPA proposed by Arora et al. (''A practical algorithm for topic modeling with provable guarantees'', ICML, 2013) in two special cases: for the first two extracted vertices, and when $r \\leq 3$. Finally, we propose a new more robust variant of SPA that first shifts and lifts the data points in order to minimize the conditioning of the problem. We illustrate our results on synthetic data."
      },
      {
        "id": "oai:arXiv.org:2412.00200v3",
        "title": "Scaling of Stochastic Normalizing Flows in $\\mathrm{SU}(3)$ lattice gauge theory",
        "link": "https://arxiv.org/abs/2412.00200",
        "author": "Andrea Bulgarelli, Elia Cellini, Alessandro Nada",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.00200v3 Announce Type: replace-cross \nAbstract: Non-equilibrium Markov Chain Monte Carlo (NE-MCMC) simulations provide a well-understood framework based on Jarzynski's equality to sample from a target probability distribution. By driving a base probability distribution out of equilibrium, observables are computed without the need to thermalize. If the base distribution is characterized by mild autocorrelations, this approach provides a way to mitigate critical slowing down. Out-of-equilibrium evolutions share the same framework of flow-based approaches and they can be naturally combined into a novel architecture called Stochastic Normalizing Flows (SNFs). In this work we present the first implementation of SNFs for $\\mathrm{SU}(3)$ lattice gauge theory in 4 dimensions, defined by introducing gauge-equivariant layers between out-of-equilibrium Monte Carlo updates. The core of our analysis is focused on the promising scaling properties of this architecture with the degrees of freedom of the system, which are directly inherited from NE-MCMC. Finally, we discuss how systematic improvements of this approach can realistically lead to a general and yet efficient sampling strategy at fine lattice spacings for observables affected by long autocorrelation times."
      },
      {
        "id": "oai:arXiv.org:2412.04409v3",
        "title": "Stabilizing and Solving Unique Continuation Problems by Parameterizing Data and Learning Finite Element Solution Operators",
        "link": "https://arxiv.org/abs/2412.04409",
        "author": "Erik Burman, Mats G. Larson, Karl Larsson, Carl Lundholm",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.04409v3 Announce Type: replace-cross \nAbstract: We consider an inverse problem involving the reconstruction of the solution to a nonlinear partial differential equation (PDE) with unknown boundary conditions. Instead of direct boundary data, we are provided with a large dataset of boundary observations for typical solutions (collective data) and a bulk measurement of a specific realization. To leverage this collective data, we first compress the boundary data using proper orthogonal decomposition (POD) in a linear expansion. Next, we identify a possible nonlinear low-dimensional structure in the expansion coefficients using an autoencoder, which provides a parametrization of the dataset in a lower-dimensional latent space. We then train an operator network to map the expansion coefficients representing the boundary data to the finite element (FE) solution of the PDE. Finally, we connect the autoencoder's decoder to the operator network which enables us to solve the inverse problem by optimizing a data-fitting term over the latent space. We analyze the underlying stabilized finite element method (FEM) in the linear setting and establish an optimal error estimate in the $H^1$-norm. The nonlinear problem is then studied numerically, demonstrating the effectiveness of our approach."
      },
      {
        "id": "oai:arXiv.org:2412.08296v3",
        "title": "GDSG: Graph Diffusion-based Solution Generator for Optimization Problems in MEC Networks",
        "link": "https://arxiv.org/abs/2412.08296",
        "author": "Ruihuai Liang, Bo Yang, Pengyu Chen, Xuelin Cao, Zhiwen Yu, M\\'erouane Debbah, Dusit Niyato, H. Vincent Poor, Chau Yuen",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.08296v3 Announce Type: replace-cross \nAbstract: Optimization is crucial for MEC networks to function efficiently and reliably, most of which are NP-hard and lack efficient approximation algorithms. This leads to a paucity of optimal solution, constraining the effectiveness of conventional deep learning approaches. Most existing learning-based methods necessitate extensive optimal data and fail to exploit the potential benefits of suboptimal data that can be obtained with greater efficiency and effectiveness. Taking the multi-server multi-user computation offloading (MSCO) problem, which is widely observed in systems like Internet-of-Vehicles (IoV) and Unmanned Aerial Vehicle (UAV) networks, as a concrete scenario, we present a Graph Diffusion-based Solution Generation (GDSG) method. This approach is designed to work with suboptimal datasets while converging to the optimal solution large probably. We transform the optimization issue into distribution-learning and offer a clear explanation of learning from suboptimal training datasets. We build GDSG as a multi-task diffusion model utilizing a Graph Neural Network (GNN) to acquire the distribution of high-quality solutions. We use a simple and efficient heuristic approach to obtain a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the backbone GNN and achieve improved generalization. GDSG also reaches nearly 100\\% task orthogonality, ensuring no interference between the discrete and continuous generation tasks. We further reveal that this orthogonality arises from the diffusion-related training loss, rather than the neural network architecture itself. The experiments demonstrate that GDSG surpasses other benchmark methods on both the optimal and suboptimal training datasets. The MSCO datasets has open-sourced at this http URL, as well as the GDSG algorithm codes at https://github.com/qiyu3816/GDSG."
      },
      {
        "id": "oai:arXiv.org:2412.15023v3",
        "title": "FolAI: Synchronized Foley Sound Generation with Semantic and Temporal Alignment",
        "link": "https://arxiv.org/abs/2412.15023",
        "author": "Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunit\\`a, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15023v3 Announce Type: replace-cross \nAbstract: Traditional sound design workflows rely on manual alignment of audio events to visual cues, as in Foley sound design, where everyday actions like footsteps or object interactions are recreated to match the on-screen motion. This process is time-consuming, difficult to scale, and lacks automation tools that preserve creative intent. Despite recent advances in vision-to-audio generation, producing temporally coherent and semantically controllable sound effects from video remains a major challenge. To address these limitations, we introduce FolAI, a two-stage generative framework that decouples the when and the what of sound synthesis, i.e., the temporal structure extraction and the semantically guided generation, respectively. In the first stage, we estimate a smooth control signal from the video that captures the motion intensity and rhythmic structure over time, serving as a temporal scaffold for the audio. In the second stage, a diffusion-based generative model produces sound effects conditioned both on this temporal envelope and on high-level semantic embeddings, provided by the user, that define the desired auditory content (e.g., material or action type). This modular design enables precise control over both timing and timbre, streamlining repetitive tasks while preserving creative flexibility in professional Foley workflows. Results on diverse visual contexts, such as footstep generation and action-specific sonorization, demonstrate that our model reliably produces audio that is temporally aligned with visual motion, semantically consistent with user intent, and perceptually realistic. These findings highlight the potential of FolAI as a controllable and modular solution for scalable, high-quality Foley sound synthesis in professional and interactive settings. Supplementary materials are accessible on our dedicated demo page at https://ispamm.github.io/FolAI."
      },
      {
        "id": "oai:arXiv.org:2501.04206v2",
        "title": "GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced Explainability in Breast Cancer Histopathology",
        "link": "https://arxiv.org/abs/2501.04206",
        "author": "Raktim Kumar Mondol, Ewan K. A. Millar, Peter H. Graham, Lois Browne, Arcot Sowmya, Erik Meijering",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.04206v2 Announce Type: replace-cross \nAbstract: Explainable AI (XAI) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. However, the black-box nature of these models often limits their clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue Examination), a post-hoc explainable framework designed for breast cancer tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (GAT) with scalewise attention (SAN) to capture scale-dependent features. We trained the model on 140 tumour TMA cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated TMA samples. GRAPHITE outperformed traditional XAI methods, achieving a mean average precision (mAP) of 0.56, an area under the receiver operating characteristic curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. In clinical utility, GRAPHITE achieved the highest area under the decision curve (AUDC) of 4.17e+5, indicating reliable decision support across thresholds. These results highlight GRAPHITE's potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologists' diagnostic reasoning and support precision medicine."
      },
      {
        "id": "oai:arXiv.org:2501.05368v2",
        "title": "Developing a Foundation of Vector Symbolic Architectures Using Category Theory",
        "link": "https://arxiv.org/abs/2501.05368",
        "author": "Nolan P Shaw, P Michael Furlong, Britt Anderson, Jeff Orchard",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.05368v2 Announce Type: replace-cross \nAbstract: Connectionist approaches to machine learning, \\emph{i.e.} neural networks, are enjoying a considerable vogue right now. However, these methods require large volumes of data and produce models that are uninterpretable to humans. An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of algebras on high-dimensional vector representations. They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform. While machine learning methods have benefited from category-theoretical analyses, VSAs have not yet received similar treatment. In this paper, we present a first attempt at applying category theory to VSAs. Specifically, We generalise from vectors to co-presheaves, and describe VSA operations as the right Kan extensions of the external tensor product. This formalisation involves a proof that the right Kan extension in such cases can be expressed as simple, element-wise operations. We validate our formalisation with worked examples that connect to current VSA implementations, while suggesting new possible designs for VSAs."
      },
      {
        "id": "oai:arXiv.org:2501.10375v2",
        "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference",
        "link": "https://arxiv.org/abs/2501.10375",
        "author": "Yujie Zhang, Shivam Aggarwal, Tulika Mitra",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.10375v2 Announce Type: replace-cross \nAbstract: Mixture-of-Experts (MoE) models, though highly effective for various machine learning tasks, face significant deployment challenges on memory-constrained devices. While GPUs offer fast inference, their limited memory compared to CPUs means not all experts can be stored on the GPU simultaneously, necessitating frequent, costly data transfers from CPU memory, often negating GPU speed advantages. To address this, we present DAOP, an on-device MoE inference engine to optimize parallel GPU-CPU execution. DAOP dynamically allocates experts between CPU and GPU based on per-sequence activation patterns, and selectively pre-calculates predicted experts on CPUs to minimize transfer latency. This approach enables efficient resource utilization across various expert cache ratios while maintaining model accuracy through a novel graceful degradation mechanism. Comprehensive evaluations across various datasets show that DAOP outperforms traditional expert caching and prefetching methods by up to 8.20x and offloading techniques by 1.35x while maintaining accuracy."
      },
      {
        "id": "oai:arXiv.org:2501.10977v2",
        "title": "SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality",
        "link": "https://arxiv.org/abs/2501.10977",
        "author": "Roberto Daza, Lin Shengkai, Aythami Morales, Julian Fierrez, Katashi Nagao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.10977v2 Announce Type: replace-cross \nAbstract: This work introduces SMARTe-VR, a platform for student monitoring in an immersive virtual reality environment designed for online education. SMARTe-VR aims to collect data for adaptive learning, focusing on facial biometrics and learning metadata. The platform allows instructors to create customized learning sessions with video lectures, featuring an interface with an AutoQA system to evaluate understanding, interaction tools (e.g., textbook highlighting and lecture tagging), and real-time feedback. Furthermore, we released a dataset that contains 5 research challenges with data from 10 users in VR-based TOEIC sessions. This data set, which spans more than 25 hours, includes facial features, learning metadata, 450 responses, difficulty levels of the questions, concept tags, and understanding labels. Alongside the database, we present preliminary experiments using Item Response Theory models, adapted for understanding detection using facial features. Two architectures were explored: a Temporal Convolutional Network for local features and a Multilayer Perceptron for global features."
      },
      {
        "id": "oai:arXiv.org:2501.17176v3",
        "title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant",
        "link": "https://arxiv.org/abs/2501.17176",
        "author": "Marc Ballestero-Rib\\'o, Daniel Ortiz-Mart\\'inez",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.17176v3 Announce Type: replace-cross \nAbstract: The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective."
      },
      {
        "id": "oai:arXiv.org:2502.00559v2",
        "title": "Deep learning model for ECG reconstruction reveals the information content of ECG leads",
        "link": "https://arxiv.org/abs/2502.00559",
        "author": "Tomasz Gradowski, Teodor Buchner",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00559v2 Announce Type: replace-cross \nAbstract: This study introduces a deep learning model based on the U-net architecture to reconstruct missing leads in electrocardiograms (ECGs). The model was trained to reconstruct 12-lead ECG data from reduced lead configurations using publicly available datasets. The results highlight the ability of the model to quantify the information content of each ECG lead and its inter-lead correlations. This has significant implications for optimizing lead selection in diagnostic scenarios, particularly in settings where complete 12-lead ECGs are impractical. In addition, the study provides insights into the physiological underpinnings of ECG signals and their propagation. The findings pave the way for advances in telemedicine, portable ECG devices, and personalized cardiac diagnostics by reducing redundancy and improving signal interpretation."
      },
      {
        "id": "oai:arXiv.org:2502.01713v2",
        "title": "Auditing a Dutch Public Sector Risk Profiling Algorithm Using an Unsupervised Bias Detection Tool",
        "link": "https://arxiv.org/abs/2502.01713",
        "author": "Floris Holstege, Mackenzie Jorgensen, Kirtan Padh, Jurriaan Parie, Joel Persson, Krsto Prorokovic, Lukas Snoek",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01713v2 Announce Type: replace-cross \nAbstract: Algorithms are increasingly used to automate or aid human decisions, yet recent research shows that these algorithms may exhibit bias across legally protected demographic groups. However, data on these groups may be unavailable to organizations or external auditors due to privacy legislation. This paper studies bias detection using an unsupervised clustering tool when data on demographic groups are unavailable. We collaborate with the Dutch Executive Agency for Education to audit an algorithm that was used to assign risk scores to college students at the national level in the Netherlands between 2012-2023. Our audit covers more than 250,000 students from the whole country. The unsupervised clustering tool highlights known disparities between students with a non-European migration background and Dutch origin. Our contributions are three-fold: (1) we assess bias in a real-world, large-scale and high-stakes decision-making process by a governmental organization; (2) we use simulation studies to highlight potential pitfalls of using the unsupervised clustering tool to detect true bias when demographic group data are unavailable and provide recommendations for valid inferences; (3) we provide the unsupervised clustering tool in an open-source library. Our work serves as a starting point for a deliberative assessment by human experts to evaluate potential discrimination in algorithmic-supported decision-making processes."
      },
      {
        "id": "oai:arXiv.org:2502.01860v4",
        "title": "SE Arena: An Interactive Platform for Evaluating Foundation Models in Software Engineering",
        "link": "https://arxiv.org/abs/2502.01860",
        "author": "Zhimin Zhao",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01860v4 Announce Type: replace-cross \nAbstract: Foundation models (FMs), particularly large language models (LLMs), have shown significant promise in various software engineering (SE) tasks, including code generation, debugging, and requirement refinement. Despite these advances, existing evaluation frameworks are insufficient for assessing model performance in iterative, context-rich workflows characteristic of SE activities. To address this limitation, we introduce SE Arena, an interactive platform designed to evaluate FMs in SE tasks. SE Arena provides a transparent, open-source leaderboard, supports multi-round conversational workflows, and enables end-to-end model comparisons. The platform introduces novel metrics, including model consistency score that measures the consistency of model outputs through self-play matches, and conversation efficiency index that evaluates model performance while accounting for the number of interaction rounds required to reach conclusions. Moreover, SE Arena incorporates a new feature called RepoChat, which automatically injects repository-related context (e.g., issues, commits, pull requests) into the conversation, further aligning evaluations with real-world development processes. This paper outlines the design and capabilities of SE Arena, emphasizing its potential to advance the evaluation and practical application of FMs in software engineering."
      },
      {
        "id": "oai:arXiv.org:2502.03270v2",
        "title": "When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning",
        "link": "https://arxiv.org/abs/2502.03270",
        "author": "Nikolaos Tsagkas, Andreas Sochopoulos, Duolikun Danier, Sethu Vijayakumar, Chris Xiaoxuan Lu, Oisin Mac Aodha",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03270v2 Announce Type: replace-cross \nAbstract: The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations."
      },
      {
        "id": "oai:arXiv.org:2502.03377v3",
        "title": "Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach",
        "link": "https://arxiv.org/abs/2502.03377",
        "author": "Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03377v3 Announce Type: replace-cross \nAbstract: As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization algorithm, significantly improves the global system energy efficiency and surpasses the popular MARL and other conventional schemes."
      },
      {
        "id": "oai:arXiv.org:2502.07503v3",
        "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
        "link": "https://arxiv.org/abs/2502.07503",
        "author": "Ibrahim Alabdulmohsin, Xiaohua Zhai",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07503v3 Announce Type: replace-cross \nAbstract: Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time in language and multimodal systems. RINS is a particular form of recursive depth that significantly outperforms +55 other variants, including the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior works, we carry out our comparisons on a compute-matched regime, and demonstrate that for a fixed model size and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. More importantly, with light-weight (linear) adapters (comprising <1% of model parameters) and stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled pretraining improves performance in language modeling even when recursive depth is not applied at inference time. This corresponds to improving performance on a training compute-, parameter-, and inference-matched regime, suggesting its potential as a viable component of LLM pretraining!"
      },
      {
        "id": "oai:arXiv.org:2502.08033v2",
        "title": "Predictive Planner for Autonomous Driving with Consistency Models",
        "link": "https://arxiv.org/abs/2502.08033",
        "author": "Anjian Li, Sangjae Bae, David Isele, Ryne Beeson, Faizan M. Tariq",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.08033v2 Announce Type: replace-cross \nAbstract: Trajectory prediction and planning are essential for autonomous vehicles to navigate safely and efficiently in dynamic environments. Traditional approaches often treat them separately, limiting the ability for interactive planning. While recent diffusion-based generative models have shown promise in multi-agent trajectory generation, their slow sampling is less suitable for high-frequency planning tasks. In this paper, we leverage the consistency model to build a predictive planner that samples from a joint distribution of ego and surrounding agents, conditioned on the ego vehicle's navigational goal. Trained on real-world human driving datasets, our consistency model generates higher-quality trajectories with fewer sampling steps than standard diffusion models, making it more suitable for real-time deployment. To enforce multiple planning constraints simultaneously on the ego trajectory, a novel online guided sampling approach inspired by the Alternating Direction Method of Multipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset (WOMD), our method enables proactive behavior such as nudging and yielding, and also demonstrates smoother, safer, and more efficient trajectories and satisfaction of multiple constraints under a limited computational budget."
      },
      {
        "id": "oai:arXiv.org:2502.09061v3",
        "title": "CRANE: Reasoning with constrained LLM generation",
        "link": "https://arxiv.org/abs/2502.09061",
        "author": "Debangshu Banerjee, Tarun Suresh, Shubham Ugare, Sasa Misailovic, Gagandeep Singh",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09061v3 Announce Type: replace-cross \nAbstract: Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO."
      },
      {
        "id": "oai:arXiv.org:2502.09203v2",
        "title": "Revisiting Euclidean Alignment for Transfer Learning in EEG-Based Brain-Computer Interfaces",
        "link": "https://arxiv.org/abs/2502.09203",
        "author": "Dongrui Wu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09203v2 Announce Type: replace-cross \nAbstract: Due to large intra-subject and inter-subject variabilities of electroencephalogram (EEG) signals, EEG-based brain-computer interfaces (BCIs) usually need subject-specific calibration to tailor the decoding algorithm for each new subject, which is time-consuming and user-unfriendly, hindering their real-world applications. Transfer learning (TL) has been extensively used to expedite the calibration, by making use of EEG data from other subjects/sessions. An important consideration in TL for EEG-based BCIs is to reduce the data distribution discrepancies among different subjects/sessions, to avoid negative transfer. Euclidean alignment (EA) was proposed in 2020 to address this challenge. Numerous experiments from 13 different BCI paradigms demonstrated its effectiveness and efficiency. This paper revisits EA, explaining its procedure and correct usage, introducing its applications and extensions, and pointing out potential new research directions. It should be very helpful to BCI researchers, especially those who are working on EEG signal decoding."
      },
      {
        "id": "oai:arXiv.org:2502.17289v2",
        "title": "A novel approach to navigate the taxonomic hierarchy to address the Open-World Scenarios in Medicinal Plant Classification",
        "link": "https://arxiv.org/abs/2502.17289",
        "author": "Soumen Sinha, Tanisha Rana, Rahul Roy",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17289v2 Announce Type: replace-cross \nAbstract: In this article, we propose a novel approach for plant hierarchical taxonomy classification by posing the problem as an open class problem. It is observed that existing methods for medicinal plant classification often fail to perform hierarchical classification and accurately identifying unknown species, limiting their effectiveness in comprehensive plant taxonomy classification. Thus we address the problem of unknown species classification by assigning it best hierarchical labels. We propose a novel method, which integrates DenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for hierarchical classification. The approach systematically categorizes medicinal plants at multiple taxonomic levels, from phylum to species, ensuring detailed and precise classification. Using multi scale space attention, the model captures both local and global contextual information from the images, improving the distinction between similar species and the identification of new ones. It uses attention scores to focus on important features across multiple scales. The proposed method provides a solution for hierarchical classification, showcasing superior performance in identifying both known and unknown species. The model was tested on two state-of-art datasets with and without background artifacts and so that it can be deployed to tackle real word application. We used unknown species for testing our model. For unknown species the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for predicting correct phylum, class, order and family respectively. Our proposed model size is almost four times less than the existing state of the art methods making it easily deploy able in real world application."
      },
      {
        "id": "oai:arXiv.org:2502.17579v2",
        "title": "VANPY: Voice Analysis Framework",
        "link": "https://arxiv.org/abs/2502.17579",
        "author": "Gregory Koushnir, Michael Fire, Galit Fuhrmann Alpert, Dima Kagan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17579v2 Announce Type: replace-cross \nAbstract: Voice data is increasingly being used in modern digital communications, yet there is still a lack of comprehensive tools for automated voice analysis and characterization. To this end, we developed the VANPY (Voice Analysis in Python) framework for automated pre-processing, feature extraction, and classification of voice data. The VANPY is an open-source end-to-end comprehensive framework that was developed for the purpose of speaker characterization from voice data. The framework is designed with extensibility in mind, allowing for easy integration of new components and adaptation to various voice analysis applications. It currently incorporates over fifteen voice analysis components - including music/speech separation, voice activity detection, speaker embedding, vocal feature extraction, and various classification models.\n  Four of the VANPY's components were developed in-house and integrated into the framework to extend its speaker characterization capabilities: gender classification, emotion classification, age regression, and height regression. The models demonstrate robust performance across various datasets, although not surpassing state-of-the-art performance.\n  As a proof of concept, we demonstrate the framework's ability to extract speaker characteristics on a use-case challenge of analyzing character voices from the movie \"Pulp Fiction.\" The results illustrate the framework's capability to extract multiple speaker characteristics, including gender, age, height, emotion type, and emotion intensity measured across three dimensions: arousal, dominance, and valence."
      },
      {
        "id": "oai:arXiv.org:2502.20727v2",
        "title": "SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models",
        "link": "https://arxiv.org/abs/2502.20727",
        "author": "Han-Byul Kim, Duc Hoang, Arnav Kundu, Mohammad Samragh, Minsik Cho",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20727v2 Announce Type: replace-cross \nAbstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs."
      },
      {
        "id": "oai:arXiv.org:2503.02498v2",
        "title": "A Systematic Literature Review on Safety of the Intended Functionality for Automated Driving Systems",
        "link": "https://arxiv.org/abs/2503.02498",
        "author": "Milin Patel, Rolf Jung, Marzana Khatun",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02498v2 Announce Type: replace-cross \nAbstract: In the automobile industry, ensuring the safety of automated vehicles equipped with the Automated Driving System (ADS) is becoming a significant focus due to the increasing development and deployment of automated driving. Automated driving depends on sensing both the external and internal environments of a vehicle, utilizing perception sensors and algorithms, and Electrical/Electronic (E/E) systems for situational awareness and response. ISO 21448 is the standard for Safety of the Intended Functionality (SOTIF) that aims to ensure that the ADS operate safely within their intended functionality. SOTIF focuses on preventing or mitigating potential hazards that may arise from the limitations or failures of the ADS, including hazards due to insufficiencies of specification, or performance insufficiencies, as well as foreseeable misuse of the intended functionality. However, the challenge lies in ensuring the safety of vehicles despite the limited availability of extensive and systematic literature on SOTIF. To address this challenge, a Systematic Literature Review (SLR) on SOTIF for the ADS is performed following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. The objective is to methodically gather and analyze the existing literature on SOTIF. The major contributions of this paper are: (i) presenting a summary of the literature by synthesizing and organizing the collective findings, methodologies, and insights into distinct thematic groups, and (ii) summarizing and categorizing the acknowledged limitations based on data extracted from an SLR of 51 research papers published between 2018 and 2023. Furthermore, research gaps are determined, and future research directions are proposed."
      },
      {
        "id": "oai:arXiv.org:2503.02650v2",
        "title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats",
        "link": "https://arxiv.org/abs/2503.02650",
        "author": "William Brach, Kristi\\'an Ko\\v{s}\\v{t}\\'al, Michal Ries",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02650v2 Announce Type: replace-cross \nAbstract: The exponential growth of unstructured text data presents a fundamental challenge in modern data management and information retrieval. While Large Language Models (LLMs) have shown remarkable capabilities in natural language processing, their potential to transform unstructured text into standardized, structured formats remains largely unexplored - a capability that could revolutionize data processing workflows across industries. This study breaks new ground by systematically evaluating LLMs' ability to convert unstructured recipe text into the structured Cooklang format. Through comprehensive testing of four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an innovative evaluation approach is introduced that combines traditional metrics (WER, ROUGE-L, TER) with specialized metrics for semantic element identification. Our experiments reveal that GPT-4o with few-shot prompting achieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating for the first time that LLMs can reliably transform domain-specific unstructured text into structured formats without extensive training. Although model performance generally scales with size, we uncover surprising potential in smaller models like Llama3.1:8b for optimization through targeted fine-tuning. These findings open new possibilities for automated structured data generation across various domains, from medical records to technical documentation, potentially transforming the way organizations process and utilize unstructured information."
      },
      {
        "id": "oai:arXiv.org:2503.05153v2",
        "title": "Generative Trajectory Stitching through Diffusion Composition",
        "link": "https://arxiv.org/abs/2503.05153",
        "author": "Yunhao Luo, Utkarsh A. Mishra, Yilun Du, Danfei Xu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2503.05153v2 Announce Type: replace-cross \nAbstract: Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods."
      },
      {
        "id": "oai:arXiv.org:2504.12279v2",
        "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR",
        "link": "https://arxiv.org/abs/2504.12279",
        "author": "Mikhail Osipov",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.12279v2 Announce Type: replace-cross \nAbstract: We present a geometry-driven method for normalizing dysarthric speech by modeling time, frequency, and amplitude distortions as smooth, local Lie group transformations of spectrograms. Scalar fields generate these deformations via exponential maps, and a neural network is trained - using only synthetically warped healthy speech - to infer the fields and apply an approximate inverse at test time. We introduce a spontaneous-symmetry-breaking (SSB) potential that encourages the model to discover non-trivial field configurations. On real pathological speech, the system delivers consistent gains: up to 17 percentage-point WER reduction on challenging TORGO utterances and a 16 percent drop in WER variance, with no degradation on clean CommonVoice data. Character and phoneme error rates improve in parallel, confirming linguistic relevance. Our results demonstrate that geometrically structured warping provides consistent, zero-shot robustness gains for dysarthric ASR."
      },
      {
        "id": "oai:arXiv.org:2504.13902v2",
        "title": "A Graph Theoretic Approach for Exploring the Relationship between EV Adoption and Charging Infrastructure Growth",
        "link": "https://arxiv.org/abs/2504.13902",
        "author": "Fahad S. Alrasheedi, Hesham H. Ali",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.13902v2 Announce Type: replace-cross \nAbstract: The increasing global demand for conventional energy has led to significant challenges, particularly due to rising CO2 emissions and the depletion of natural resources. In the U.S., light-duty vehicles contribute significantly to transportation sector emissions, prompting a global shift toward electrified vehicles (EVs). Among the challenges that thwart the widespread adoption of EVs is the insufficient charging infrastructure (CI). This study focuses on exploring the complex relationship between EV adoption and CI growth. Employing a graph theoretic approach, we propose a graph model to analyze correlations between EV adoption and CI growth across 137 counties in six states. We examine how different time granularities impact these correlations in two distinct scenarios: Early Adoption and Late Adoption. Further, we conduct causality tests to assess the directional relationship between EV adoption and CI growth in both scenarios. Our main findings reveal that analysis using lower levels of time granularity result in more homogeneous clusters, with notable differences between clusters in EV adoption and those in CI growth. Additionally, we identify causal relationships between EV adoption and CI growth in 137 counties, and show that causality is observed more frequently in Early Adoption scenarios than in Late Adoption ones. However, the causal effects in Early Adoption are slower than those in Late Adoption."
      },
      {
        "id": "oai:arXiv.org:2504.16062v3",
        "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration",
        "link": "https://arxiv.org/abs/2504.16062",
        "author": "Hardik Shah, Jiaxu Xing, Nico Messikommer, Boyang Sun, Marc Pollefeys, Davide Scaramuzza",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.16062v3 Announce Type: replace-cross \nAbstract: Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration."
      },
      {
        "id": "oai:arXiv.org:2504.19174v3",
        "title": "CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation",
        "link": "https://arxiv.org/abs/2504.19174",
        "author": "Xueqi Ma, Yilin Liu, Tianlong Gao, Qirui Huang, Hui Huang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19174v3 Announce Type: replace-cross \nAbstract: We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified Continuous Latent Representation. Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation."
      },
      {
        "id": "oai:arXiv.org:2504.19718v2",
        "title": "Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation",
        "link": "https://arxiv.org/abs/2504.19718",
        "author": "Victoria Yue Chen, Daoye Wang, Stephan Garbin, Jan Bednarik, Sebastian Winberg, Timo Bolkart, Thabo Beeler",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19718v2 Announce Type: replace-cross \nAbstract: Face registration deforms a template mesh to closely fit a 3D face scan, the quality of which commonly degrades in non-skin regions (e.g., hair, beard, accessories), because the optimized template-to-scan distance pulls the template mesh towards the noisy scan surface. Improving registration quality requires a clean separation of skin and non-skin regions on the scan mesh. Existing image-based (2D) or scan-based (3D) segmentation methods however perform poorly. Image-based segmentation outputs multi-view inconsistent masks, and they cannot account for scan inaccuracies or scan-image misalignment, while scan-based methods suffer from lower spatial resolution compared to images. In this work, we introduce a novel method that accurately separates skin from non-skin geometry on 3D human head scans. For this, our method extracts features from multi-view images using a frozen image foundation model and aggregates these features in 3D. These lifted 2D features are then fused with 3D geometric features extracted from the scan mesh, to then predict a segmentation mask directly on the scan mesh. We show that our segmentations improve the registration accuracy over pure 2D or 3D segmentation methods by 8.89% and 14.3%, respectively. Although trained only on synthetic data, our model generalizes well to real data."
      },
      {
        "id": "oai:arXiv.org:2504.20065v2",
        "title": "A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts",
        "link": "https://arxiv.org/abs/2504.20065",
        "author": "Robert Becker, Aron Culotta",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20065v2 Announce Type: replace-cross \nAbstract: We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works."
      },
      {
        "id": "oai:arXiv.org:2504.20117v2",
        "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies",
        "link": "https://arxiv.org/abs/2504.20117",
        "author": "Shubham Gandhi, Dhruv Shah, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20117v2 Announce Type: replace-cross \nAbstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research."
      },
      {
        "id": "oai:arXiv.org:2504.20898v2",
        "title": "CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models",
        "link": "https://arxiv.org/abs/2504.20898",
        "author": "Hasan Md Tusfiqur Alam, Devansh Srivastav, Abdulrahman Mohamed Selim, Md Abdul Kadir, Md Moktadirul Hoque Shuvo, Daniel Sonntag",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.20898v2 Announce Type: replace-cross \nAbstract: Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights."
      },
      {
        "id": "oai:arXiv.org:2504.21795v2",
        "title": "Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model",
        "link": "https://arxiv.org/abs/2504.21795",
        "author": "Yuankang Zhao, Matthew Engelhard",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.21795v2 Announce Type: replace-cross \nAbstract: The Hawkes process (HP) is commonly used to model event sequences with self-reinforcing dynamics, including electronic health records (EHRs). Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which is often critical in healthcare. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs yet more interpretable than other neural network approaches, and allows us to explicitly trade flexibility for interpretability by adding transformer encoder layers to further contextualize the event embeddings. Results show that our method accurately recovers impact functions in simulations, achieves competitive performance on MIMIC-IV procedure dataset, and gains clinically meaningful interpretation on XX-EHR with children diagnosis dataset even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics in EHRs and other data effectively, implying that interpretability can be maintained without loss of performance."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Tue, 06 May 2025 04:02:01 +0000",
      "published": "Tue, 06 May 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2505.01632v1",
        "title": "Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments",
        "link": "https://arxiv.org/abs/2505.01632",
        "author": "Noussaiba Djeffal, Djamel Addou, Hamza Kheddar, Sid Ahmed Selouani",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01632v1 Announce Type: new \nAbstract: Addressing the detrimental impact of non-stationary environmental noise on automatic speech recognition (ASR) has been a persistent and significant research focus. Despite advancements, this challenge continues to be a major concern. Recently, data-driven supervised approaches, such as deep neural networks, have emerged as promising alternatives to traditional unsupervised methods. With extensive training, these approaches have the potential to overcome the challenges posed by diverse real-life acoustic environments. In this light, this paper introduces a novel neural framework that incorporates a robust frontend into ASR systems in both clean and noisy environments. Utilizing the Aurora-2 speech database, the authors evaluate the effectiveness of an acoustic feature set for Mel-frequency, employing the approach of transfer learning based on Residual neural network (ResNet). The experimental results demonstrate a significant improvement in recognition accuracy compared to convolutional neural networks (CNN) and long short-term memory (LSTM) networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode."
      },
      {
        "id": "oai:arXiv.org:2505.01747v1",
        "title": "Low-Complexity Acoustic Scene Classification with Device Information in the DCASE 2025 Challenge",
        "link": "https://arxiv.org/abs/2505.01747",
        "author": "Florian Schmid, Paul Primus, Toni Heittola, Annamaria Mesaros, Irene Mart\\'in-Morat\\'o, Gerhard Widmer",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01747v1 Announce Type: new \nAbstract: This paper presents the Low-Complexity Acoustic Scene Classification with Device Information Task of the DCASE 2025 Challenge and its baseline system. Continuing the focus on low-complexity models, data efficiency, and device mismatch from previous editions (2022--2024), this year's task introduces a key change: recording device information is now provided at inference time. This enables the development of device-specific models that leverage device characteristics -- reflecting real-world deployment scenarios in which a model is designed with awareness of the underlying hardware. The training set matches the 25% subset used in the corresponding DCASE 2024 challenge, with no restrictions on external data use, highlighting transfer learning as a central topic. The baseline achieves 50.72% accuracy on this ten-class problem with a device-general model, improving to 51.89% when using the available device information."
      },
      {
        "id": "oai:arXiv.org:2505.01750v1",
        "title": "FLOWER: Flow-Based Estimated Gaussian Guidance for General Speech Restoration",
        "link": "https://arxiv.org/abs/2505.01750",
        "author": "Da-Hee Yang, Jaeuk Lee, Joon-Hyuk Chang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01750v1 Announce Type: new \nAbstract: We introduce FLOWER, a novel conditioning method designed for speech restoration that integrates Gaussian guidance into generative frameworks. By transforming clean speech into a predefined prior distribution (e.g., Gaussian distribution) using a normalizing flow network, FLOWER extracts critical information to guide generative models. This guidance is incorporated into each block of the generative network, enabling precise restoration control. Experimental results demonstrate the effectiveness of FLOWER in improving performance across various general speech restoration tasks."
      },
      {
        "id": "oai:arXiv.org:2505.01880v1",
        "title": "Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network",
        "link": "https://arxiv.org/abs/2505.01880",
        "author": "Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.01880v1 Announce Type: new \nAbstract: Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks."
      },
      {
        "id": "oai:arXiv.org:2505.02180v1",
        "title": "MaskClip: Detachable Clip-on Piezoelectric Sensing of Mask Surface Vibrations for Real-time Noise-Robust Speech Input",
        "link": "https://arxiv.org/abs/2505.02180",
        "author": "Hirotaka Hiraki, Jun Rekimoto",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02180v1 Announce Type: new \nAbstract: Masks are essential in medical settings and during infectious outbreaks but significantly impair speech communication, especially in environments with background noise. Existing solutions often require substantial computational resources or compromise hygiene and comfort. We propose a novel sensing approach that captures only the wearer's voice by detecting mask surface vibrations using a piezoelectric sensor. Our developed device, MaskClip, employs a stainless steel clip with an optimally positioned piezoelectric sensor to selectively capture speech vibrations while inherently filtering out ambient noise. Evaluation experiments demonstrated superior performance with a low Character Error Rate of 6.1\\% in noisy environments compared to conventional microphones. Subjective evaluations by 102 participants also showed high satisfaction scores. This approach shows promise for applications in settings where clear voice communication must be maintained while wearing protective equipment, such as medical facilities, cleanrooms, and industrial environments."
      },
      {
        "id": "oai:arXiv.org:2505.02331v1",
        "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection",
        "link": "https://arxiv.org/abs/2505.02331",
        "author": "Hao Cheng, Zhiwei Zhao, Yichao He, Zhenzhen Hu, Jia Li, Meng Wang, Richang Hong",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02331v1 Announce Type: cross \nAbstract: Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations."
      },
      {
        "id": "oai:arXiv.org:2505.02469v1",
        "title": "Efficient Continual Learning in Keyword Spotting using Binary Neural Networks",
        "link": "https://arxiv.org/abs/2505.02469",
        "author": "Quynh Nguyen-Phuong Vu, Luciano Sebastian Martinez-Rau, Yuxuan Zhang, Nho-Duc Tran, Bengt Oelmann, Michele Magno, Sebastian Bader",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02469v1 Announce Type: cross \nAbstract: Keyword spotting (KWS) is an essential function that enables interaction with ubiquitous smart devices. However, in resource-limited devices, KWS models are often static and can thus not adapt to new scenarios, such as added keywords. To overcome this problem, we propose a Continual Learning (CL) approach for KWS built on Binary Neural Networks (BNNs). The framework leverages the reduced computation and memory requirements of BNNs while incorporating techniques that enable the seamless integration of new keywords over time. This study evaluates seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95% for a single additional keyword and up to 86% for four additional classes. Sensitivity to the amount of training samples in the CL phase, and differences in computational complexities are being evaluated. These evaluations demonstrate that batch-based algorithms are more sensitive to the CL dataset size, and that differences between the computational complexities are insignificant. These findings highlight the potential of developing an effective and computationally efficient technique for continuously integrating new keywords in KWS applications that is compatible with resource-constrained devices."
      },
      {
        "id": "oai:arXiv.org:2505.02518v1",
        "title": "Bemba Speech Translation: Exploring a Low-Resource African Language",
        "link": "https://arxiv.org/abs/2505.02518",
        "author": "Muhammad Hazim Al Farouq, Aman Kassahun Wassie, Yasmin Moslem",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02518v1 Announce Type: cross \nAbstract: This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2025), low-resource languages track, namely for Bemba-to-English speech translation. We built cascaded speech translation systems based on Whisper and NLLB-200, and employed data augmentation techniques, such as back-translation. We investigate the effect of using synthetic data and discuss our experimental setup."
      },
      {
        "id": "oai:arXiv.org:2505.02615v1",
        "title": "Automatic Proficiency Assessment in L2 English Learners",
        "link": "https://arxiv.org/abs/2505.02615",
        "author": "Armita Mohammadi, Alessandro Lameiras Koerich, Laureano Moro-Velazquez, Patrick Cardinal",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02615v1 Announce Type: cross \nAbstract: Second language proficiency (L2) in English is usually perceptually evaluated by English teachers or expert evaluators, with the inherent intra- and inter-rater variability. This paper explores deep learning techniques for comprehensive L2 proficiency assessment, addressing both the speech signal and its correspondent transcription. We analyze spoken proficiency classification prediction using diverse architectures, including 2D CNN, frequency-based CNN, ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based proficiency assessment by fine-tuning a BERT language model within resource constraints. Finally, we tackle the complex task of spontaneous dialogue assessment, managing long-form audio and speaker interactions through separate applications of wav2vec 2.0 and BERT models. Results from experiments on EFCamDat and ANGLISH datasets and a private dataset highlight the potential of deep learning, especially the pretrained wav2vec 2.0 model, for robust automated L2 proficiency evaluation."
      },
      {
        "id": "oai:arXiv.org:2505.02625v1",
        "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis",
        "link": "https://arxiv.org/abs/2505.02625",
        "author": "Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, Yang Feng",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02625v1 Announce Type: cross \nAbstract: Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data."
      },
      {
        "id": "oai:arXiv.org:2505.02692v1",
        "title": "fastabx: A library for efficient computation of ABX discriminability",
        "link": "https://arxiv.org/abs/2505.02692",
        "author": "Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02692v1 Announce Type: cross \nAbstract: We introduce fastabx, a high-performance Python library for building ABX discrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx."
      },
      {
        "id": "oai:arXiv.org:2505.02707v1",
        "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play",
        "link": "https://arxiv.org/abs/2505.02707",
        "author": "Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, Zhiting Hu",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2505.02707v1 Announce Type: cross \nAbstract: A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions."
      },
      {
        "id": "oai:arXiv.org:2307.15344v2",
        "title": "Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions",
        "link": "https://arxiv.org/abs/2307.15344",
        "author": "Yifei Xin, Yuexian Zou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2307.15344v2 Announce Type: replace \nAbstract: Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets."
      },
      {
        "id": "oai:arXiv.org:2409.09256v2",
        "title": "Audio-text Retrieval with Transformer-based Hierarchical Alignment and Disentangled Cross-modal Representation",
        "link": "https://arxiv.org/abs/2409.09256",
        "author": "Yifei Xin, Zhihong Zhu, Xuxin Cheng, Xusheng Yang, Yuexian Zou",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09256v2 Announce Type: replace \nAbstract: Most existing audio-text retrieval (ATR) approaches typically rely on a single-level interaction to associate audio and text, limiting their ability to align different modalities and leading to suboptimal matches. In this work, we present a novel ATR framework that leverages two-stream Transformers in conjunction with a Hierarchical Alignment (THA) module to identify multi-level correspondences of different Transformer blocks between audio and text. Moreover, current ATR methods mainly focus on learning a global-level representation, missing out on intricate details to capture audio occurrences that correspond to textual semantics. To bridge this gap, we introduce a Disentangled Cross-modal Representation (DCR) approach that disentangles high-dimensional features into compact latent factors to grasp fine-grained audio-text semantic correlations. Additionally, we develop a confidence-aware (CA) module to estimate the confidence of each latent factor pair and adaptively aggregate cross-modal latent factors to achieve local semantic alignment. Experiments show that our THA effectively boosts ATR performance, with the DCR approach further contributing to consistent performance gains."
      },
      {
        "id": "oai:arXiv.org:2412.15023v3",
        "title": "FolAI: Synchronized Foley Sound Generation with Semantic and Temporal Alignment",
        "link": "https://arxiv.org/abs/2412.15023",
        "author": "Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunit\\`a, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15023v3 Announce Type: replace \nAbstract: Traditional sound design workflows rely on manual alignment of audio events to visual cues, as in Foley sound design, where everyday actions like footsteps or object interactions are recreated to match the on-screen motion. This process is time-consuming, difficult to scale, and lacks automation tools that preserve creative intent. Despite recent advances in vision-to-audio generation, producing temporally coherent and semantically controllable sound effects from video remains a major challenge. To address these limitations, we introduce FolAI, a two-stage generative framework that decouples the when and the what of sound synthesis, i.e., the temporal structure extraction and the semantically guided generation, respectively. In the first stage, we estimate a smooth control signal from the video that captures the motion intensity and rhythmic structure over time, serving as a temporal scaffold for the audio. In the second stage, a diffusion-based generative model produces sound effects conditioned both on this temporal envelope and on high-level semantic embeddings, provided by the user, that define the desired auditory content (e.g., material or action type). This modular design enables precise control over both timing and timbre, streamlining repetitive tasks while preserving creative flexibility in professional Foley workflows. Results on diverse visual contexts, such as footstep generation and action-specific sonorization, demonstrate that our model reliably produces audio that is temporally aligned with visual motion, semantically consistent with user intent, and perceptually realistic. These findings highlight the potential of FolAI as a controllable and modular solution for scalable, high-quality Foley sound synthesis in professional and interactive settings. Supplementary materials are accessible on our dedicated demo page at https://ispamm.github.io/FolAI."
      },
      {
        "id": "oai:arXiv.org:2501.15744v2",
        "title": "Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction in open-plan offices",
        "link": "https://arxiv.org/abs/2501.15744",
        "author": "Manuj Yadav, Jungsoo Kim, Valtteri Hongisto, Densil Cabrera, Richard de Dear",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2501.15744v2 Announce Type: replace \nAbstract: Open-plan offices are well-known to be adversely affected by acoustic issues. This study aims to model acoustic dissatisfaction using measurements of room acoustics, sound environment during occupancy, and occupant surveys (n = 349) in 28 offices representing a diverse range of workplace parameters. As latent factors, the contribution of $\\textit{lack of privacy}$ (LackPriv) was 25% higher than $\\textit{noise disturbance}$ (NseDstrb) in predicting $\\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on sound pressure level (SPL) decay of speech ($L_{\\text{p,A,s,4m}}$ and $r_{\\text{C}}$) were better in predicting these factors than distraction distance ($r_{\\text{D}}$) based on speech transmission index. This contradicts previous findings, and the trends for SPL-based metrics in predicting AcDsat and LackPriv go against expectations based on ISO 3382-3. For sound during occupation, $L_{\\text{A,90}}$ and psychoacoustic loudness ($N_{\\text{90}}$) predicted AcDsat, and a SPL fluctuation metric ($M_{\\text{A,eq}}$) predicted LackPriv. However, these metrics were weaker predictors than ISO 3382-3 metrics. Medium-sized offices exhibited higher dissatisfaction than larger ($\\geq$50 occupants) offices. Dissatisfaction varied substantially across parameters including ceiling heights, number of workstations, and years of work, but not between offices with fixed seating compared to more flexible and activity-based working configurations. Overall, these findings highlight the complexities in characterizing occupants' perceptions using instrumental acoustic measurements."
      },
      {
        "id": "oai:arXiv.org:2502.17579v2",
        "title": "VANPY: Voice Analysis Framework",
        "link": "https://arxiv.org/abs/2502.17579",
        "author": "Gregory Koushnir, Michael Fire, Galit Fuhrmann Alpert, Dima Kagan",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17579v2 Announce Type: replace \nAbstract: Voice data is increasingly being used in modern digital communications, yet there is still a lack of comprehensive tools for automated voice analysis and characterization. To this end, we developed the VANPY (Voice Analysis in Python) framework for automated pre-processing, feature extraction, and classification of voice data. The VANPY is an open-source end-to-end comprehensive framework that was developed for the purpose of speaker characterization from voice data. The framework is designed with extensibility in mind, allowing for easy integration of new components and adaptation to various voice analysis applications. It currently incorporates over fifteen voice analysis components - including music/speech separation, voice activity detection, speaker embedding, vocal feature extraction, and various classification models.\n  Four of the VANPY's components were developed in-house and integrated into the framework to extend its speaker characterization capabilities: gender classification, emotion classification, age regression, and height regression. The models demonstrate robust performance across various datasets, although not surpassing state-of-the-art performance.\n  As a proof of concept, we demonstrate the framework's ability to extract speaker characteristics on a use-case challenge of analyzing character voices from the movie \"Pulp Fiction.\" The results illustrate the framework's capability to extract multiple speaker characteristics, including gender, age, height, emotion type, and emotion intensity measured across three dimensions: arousal, dominance, and valence."
      },
      {
        "id": "oai:arXiv.org:2504.05197v2",
        "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
        "link": "https://arxiv.org/abs/2504.05197",
        "author": "Yong Ren, Jiangyan Yi, Tao Wang, Jianhua Tao, Zheng Lian, Zhengqi Wen, Chenxing Li, Ruibo Fu, Ye Bai, Xiaohui Zhang",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.05197v2 Announce Type: replace \nAbstract: Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness."
      },
      {
        "id": "oai:arXiv.org:2504.12279v2",
        "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR",
        "link": "https://arxiv.org/abs/2504.12279",
        "author": "Mikhail Osipov",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.12279v2 Announce Type: replace \nAbstract: We present a geometry-driven method for normalizing dysarthric speech by modeling time, frequency, and amplitude distortions as smooth, local Lie group transformations of spectrograms. Scalar fields generate these deformations via exponential maps, and a neural network is trained - using only synthetically warped healthy speech - to infer the fields and apply an approximate inverse at test time. We introduce a spontaneous-symmetry-breaking (SSB) potential that encourages the model to discover non-trivial field configurations. On real pathological speech, the system delivers consistent gains: up to 17 percentage-point WER reduction on challenging TORGO utterances and a 16 percent drop in WER variance, with no degradation on clean CommonVoice data. Character and phoneme error rates improve in parallel, confirming linguistic relevance. Our results demonstrate that geometrically structured warping provides consistent, zero-shot robustness gains for dysarthric ASR."
      },
      {
        "id": "oai:arXiv.org:2502.12050v2",
        "title": "SpeechT: Findings of the First Mentorship in Speech Translation",
        "link": "https://arxiv.org/abs/2502.12050",
        "author": "Yasmin Moslem, Juan Juli\\'an Cea Mor\\'an, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12050v2 Announce Type: replace-cross \nAbstract: This work presents the details and findings of the first mentorship in speech translation (SpeechT), which took place in December 2024 and January 2025. To fulfil the mentorship requirements, the participants engaged in key activities, including data preparation, modelling, and advanced research. The participants explored data augmentation techniques and compared end-to-end and cascaded speech translation systems. The projects covered various languages other than English, including Arabic, Bengali, Galician, Indonesian, Japanese, and Spanish."
      },
      {
        "id": "oai:arXiv.org:2504.21214v2",
        "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech",
        "link": "https://arxiv.org/abs/2504.21214",
        "author": "Jinzhao Zhou, Zehong Cao, Yiqun Duan, Connor Barkley, Daniel Leong, Xiaowei Jiang, Quoc-Toan Nguyen, Ziyi Zhao, Thomas Do, Yu-Cheng Chang, Sheng-Fu Liang, Chin-teng Lin",
        "published": "Tue, 06 May 2025 00:00:00 -0400",
        "summary": "arXiv:2504.21214v2 Announce Type: replace-cross \nAbstract: This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\\% accuracy on semantic-level classification and 39.6\\% in word-level classification, outperforming baseline methods by 5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research."
      }
    ]
  }
}