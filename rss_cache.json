{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Wed, 09 Jul 2025 04:18:08 +0000",
      "published": "Wed, 09 Jul 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2507.05261v1",
        "title": "TokenShapley: Token Level Context Attribution with Shapley Value",
        "link": "https://arxiv.org/abs/2507.05261",
        "author": "Yingtai Xiao, Yuqing Zhu, Sirat Samyoun, Wanrong Zhang, Jiachen T. Wang, Jian Du",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05261v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy."
      },
      {
        "id": "oai:arXiv.org:2507.05263v1",
        "title": "Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization",
        "link": "https://arxiv.org/abs/2507.05263",
        "author": "Kaichen Ouyang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05263v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have shown great potential in graph data analysis due to their powerful representation capabilities. However, as the network depth increases, the issue of over-smoothing becomes more severe, causing node representations to lose their distinctiveness. This paper analyzes the mechanism of over-smoothing through the analogy to Anderson localization and introduces participation degree as a metric to quantify this phenomenon. Specifically, as the depth of the GNN increases, node features homogenize after multiple layers of message passing, leading to a loss of distinctiveness, similar to the behavior of vibration modes in disordered systems. In this context, over-smoothing in GNNs can be understood as the expansion of low-frequency modes (increased participation degree) and the localization of high-frequency modes (decreased participation degree). Based on this, we systematically reviewed the potential connection between the Anderson localization behavior in disordered systems and the over-smoothing behavior in Graph Neural Networks. A theoretical analysis was conducted, and we proposed the potential of alleviating over-smoothing by reducing the disorder in information propagation."
      },
      {
        "id": "oai:arXiv.org:2507.05266v1",
        "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs",
        "link": "https://arxiv.org/abs/2507.05266",
        "author": "Sougata Saha, Monojit Choudhury",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05266v1 Announce Type: new \nAbstract: Measuring the generalization ability of Large Language Models (LLMs) is challenging due to data contamination. As models grow and computation becomes cheaper, ensuring tasks and test cases are unseen during training phases will become nearly impossible. We argue that knowledge-retrieval and reasoning tasks are not ideal for measuring generalization, as LLMs are not trained for specific tasks. Instead, we propose user behavior prediction, also a key aspect of personalization, as a theoretically sound, scalable, and robust alternative. We introduce a novel framework for this approach and test it on movie and music recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct. Results align with our framework's predictions, showing GPT-4o outperforms GPT-4o-mini and Llama, though all models have much room for improvement, especially Llama."
      },
      {
        "id": "oai:arXiv.org:2507.05271v1",
        "title": "An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks",
        "link": "https://arxiv.org/abs/2507.05271",
        "author": "Mohammad Zia Ur Rehman, Aditya Shah, Nagendra Kumar",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05271v1 Announce Type: new \nAbstract: The global reach of social media has amplified the spread of hateful content, including implicit sexism, which is often overlooked by conventional detection methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning framework for implicit sexism detectioN (ASCEND). A key innovation of our method is the incorporation of threshold-based contrastive learning: by computing cosine similarities between embeddings, we selectively treat only those sample pairs as positive if their similarity exceeds a learnable threshold. This mechanism refines the embedding space by robustly pulling together representations of semantically similar texts while pushing apart dissimilar ones, thus reducing false positives and negatives. The final classification is achieved by jointly optimizing a contrastive loss with a cross-entropy loss. Textual features are enhanced through a word-level attention module. Additionally, we employ sentiment, emotion, and toxicity features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that ASCEND significantly outperforms existing methods, with average Macro F1 improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting its efficacy in capturing the subtle cues of implicit sexist language."
      },
      {
        "id": "oai:arXiv.org:2507.05284v1",
        "title": "Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction",
        "link": "https://arxiv.org/abs/2507.05284",
        "author": "Mustafa Kamal, Niyaz Bin Hashem, Robin Krambroeckers, Nabeel Mohammed, Shafin Rahman",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05284v1 Announce Type: new \nAbstract: Although most transformer-based time series forecasting models primarily depend on endogenous inputs, recent state-of-the-art approaches have significantly improved performance by incorporating external information through exogenous inputs. However, these methods face challenges, such as redundancy when endogenous and exogenous inputs originate from the same source and limited ability to capture long-term dependencies due to fixed look-back windows. In this paper, we propose a method that whitens the exogenous input to reduce redundancy that may persist within the data based on global statistics. Additionally, our approach helps the exogenous input to be more aware of patterns and trends over extended periods. By introducing this refined, globally context-aware exogenous input to the endogenous input without increasing the lookback window length, our approach guides the model towards improved forecasting. Our approach achieves state-of-the-art performance in four benchmark datasets, consistently outperforming 11 baseline models. These results establish our method as a robust and effective alternative for using exogenous inputs in time series forecasting."
      },
      {
        "id": "oai:arXiv.org:2507.05285v1",
        "title": "Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion",
        "link": "https://arxiv.org/abs/2507.05285",
        "author": "Miloud Mihoubi, Meriem Zerkouk, Belkacem Chikhaoui",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05285v1 Announce Type: new \nAbstract: Student dropout in distance learning remains a critical challenge, with profound societal and economic consequences. While classical machine learning models leverage structured socio-demographic and behavioral data, they often fail to capture the nuanced emotional and contextual factors embedded in unstructured student interactions. This paper introduces a transformative AI framework that redefines dropout prediction through three synergistic innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment analysis, prompt engineering to decode academic stressors, and cross-modal attention fusion to dynamically align textual, behavioral, and socio-demographic insights. By grounding sentiment analysis in a curated knowledge base of pedagogical content, our RAG-enhanced BERT model interprets student comments with unprecedented contextual relevance, while optimized prompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload anxiety\"). A cross-modal attention layer then fuses these insights with temporal engagement patterns, creating holistic risk profiles. Evaluated on a longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and an F1-score of 0.88, outperforming conventional models by 7% and reducing false negatives by 21%. Beyond prediction, the system generates interpretable interventions by retrieving contextually aligned strategies (e.g., mentorship programs for isolated learners). This work bridges the gap between predictive analytics and actionable pedagogy, offering a scalable solution to mitigate dropout risks in global education systems"
      },
      {
        "id": "oai:arXiv.org:2507.05286v1",
        "title": "Compressing Deep Neural Networks Using Explainable AI",
        "link": "https://arxiv.org/abs/2507.05286",
        "author": "Kimia Soroush, Mohsen Raji, Behnam Ghavami",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05286v1 Announce Type: new \nAbstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many tasks but it often comes at a high computational cost and memory usage. Compression techniques, such as pruning and quantization, are applied to reduce the memory footprint of DNNs and make it possible to accommodate them on resource-constrained edge devices. Recently, explainable artificial intelligence (XAI) methods have been introduced with the purpose of understanding and explaining AI methods. XAI can be utilized to get to know the inner functioning of DNNs, such as the importance of different neurons and features in the overall performance of DNNs. In this paper, a novel DNN compression approach using XAI is proposed to efficiently reduce the DNN model size with negligible accuracy loss. In the proposed approach, the importance score of DNN parameters (i.e. weights) are computed using a gradient-based XAI technique called Layer-wise Relevance Propagation (LRP). Then, the scores are used to compress the DNN as follows: 1) the parameters with the negative or zero importance scores are pruned and removed from the model, 2) mixed-precision quantization is applied to quantize the weights with higher/lower score with higher/lower number of bits. The experimental results show that, the proposed compression approach reduces the model size by 64% while the accuracy is improved by 42% compared to the state-of-the-art XAI-based compression method."
      },
      {
        "id": "oai:arXiv.org:2507.05291v1",
        "title": "Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity",
        "link": "https://arxiv.org/abs/2507.05291",
        "author": "Manuel Ricardo Guevara Garban, Yves Chemisky, \\'Etienne Pruli\\`ere, Micha\\\"el Cl\\'ement",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05291v1 Announce Type: new \nAbstract: We propose a physics-informed machine learning framework called P-DivGNN to reconstruct local stress fields at the micro-scale, in the context of multi-scale simulation given a periodic micro-structure mesh and mean, macro-scale, stress values. This method is based in representing a periodic micro-structure as a graph, combined with a message passing graph neural network. We are able to retrieve local stress field distributions, providing average stress values produced by a mean field reduced order model (ROM) or Finite Element (FE) simulation at the macro-scale. The prediction of local stress fields are of utmost importance considering fracture analysis or the definition of local fatigue criteria. Our model incorporates physical constraints during training to constraint local stress field equilibrium state and employs a periodic graph representation to enforce periodic boundary conditions. The benefits of the proposed physics-informed GNN are evaluated considering linear and non linear hyperelastic responses applied to varying geometries. In the non-linear hyperelastic case, the proposed method achieves significant computational speed-ups compared to FE simulation, making it particularly attractive for large-scale applications."
      },
      {
        "id": "oai:arXiv.org:2507.05300v1",
        "title": "Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)",
        "link": "https://arxiv.org/abs/2507.05300",
        "author": "Nicholas Merchant, Haitz S\\'aez de Oc\\'ariz Borde, Andrei Cristian Popescu, Carlos Garcia Jurado Suarez",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05300v1 Announce Type: new \nAbstract: We argue that generative text-to-image models often struggle with prompt adherence due to the noisy and unstructured nature of large-scale datasets like LAION-5B. This forces users to rely heavily on prompt engineering to elicit desirable outputs. In this work, we propose that enforcing a consistent caption structure during training can significantly improve model controllability and alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part template: subject, setting, aesthetics, and camera details. We fine-tune PixArt-$\\Sigma$ and Stable Diffusion 2 using both structured and randomly shuffled captions, and show that structured versions consistently yield higher text-image alignment scores using visual question answering (VQA) models. The dataset is publicly available at https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M."
      },
      {
        "id": "oai:arXiv.org:2507.05302v1",
        "title": "CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection",
        "link": "https://arxiv.org/abs/2507.05302",
        "author": "Binjia Zhou, Hengrui Lou, Lizhe Chen, Haoyuan Li, Dawei Luo, Shuai Chen, Jie Lei, Zunlei Feng, Yijun Bei",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05302v1 Announce Type: new \nAbstract: With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake detection.Existing techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of hallucinations.To address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias reduction.Experimental results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities."
      },
      {
        "id": "oai:arXiv.org:2507.05309v1",
        "title": "Neural Velocity for hyperparameter tuning",
        "link": "https://arxiv.org/abs/2507.05309",
        "author": "Gianluca Dalmasso, Andrea Bragagnolo, Enzo Tartaglione, Attilio Fiandrotti, Marco Grangetto",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05309v1 Announce Type: new \nAbstract: Hyperparameter tuning, such as learning rate decay and defining a stopping criterion, often relies on monitoring the validation loss. This paper presents NeVe, a dynamic training approach that adjusts the learning rate and defines the stop criterion based on the novel notion of \"neural velocity\". The neural velocity measures the rate of change of each neuron's transfer function and is an indicator of model convergence: sampling neural velocity can be performed even by forwarding noise in the network, reducing the need for a held-out dataset. Our findings show the potential of neural velocity as a key metric for optimizing neural network training efficiently"
      },
      {
        "id": "oai:arXiv.org:2507.05315v1",
        "title": "Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces",
        "link": "https://arxiv.org/abs/2507.05315",
        "author": "Madina Kojanazarova, Florentin Bieder, Robin Sandk\\\"uhler, Philippe C. Cattin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05315v1 Announce Type: new \nAbstract: Soft tissue simulation in virtual environments is becoming increasingly important for medical applications. However, the high deformability of soft tissue poses significant challenges. Existing methods rely on segmentation, meshing and estimation of stiffness properties of tissues. In addition, the integration of haptic feedback requires precise force estimation to enable a more immersive experience. We introduce a novel data-driven model, a conditional graph neural network (cGNN) to tackle this complexity. Our model takes surface points and the location of applied forces, and is specifically designed to predict the deformation of the points and the forces exerted on them. We trained our model on experimentally collected surface tracking data of a soft tissue phantom and used transfer learning to overcome the data scarcity by initially training it with mass-spring simulations and fine-tuning it with the experimental data. This approach improves the generalisation capability of the model and enables accurate predictions of tissue deformations and corresponding interaction forces. The results demonstrate that the model can predict deformations with a distance error of 0.35$\\pm$0.03 mm for deformations up to 30 mm and the force with an absolute error of 0.37$\\pm$0.05 N for forces up to 7.5 N. Our data-driven approach presents a promising solution to the intricate challenge of simulating soft tissues within virtual environments. Beyond its applicability in medical simulations, this approach holds the potential to benefit various fields where realistic soft tissue simulations are required."
      },
      {
        "id": "oai:arXiv.org:2507.05319v1",
        "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review",
        "link": "https://arxiv.org/abs/2507.05319",
        "author": "Cheng Yuan, Xinkai Rui, Yongqi Fan, Yawei Fan, Boyang Zhong, Jiacheng Wang, Weiyan Zhang, Tong Ruan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05319v1 Announce Type: new \nAbstract: Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS."
      },
      {
        "id": "oai:arXiv.org:2507.05322v1",
        "title": "Dataless Neural Networks for Resource-Constrained Project Scheduling",
        "link": "https://arxiv.org/abs/2507.05322",
        "author": "Marc Bara",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05322v1 Announce Type: new \nAbstract: Dataless neural networks represent a paradigm shift in applying neural architectures to combinatorial optimization problems, eliminating the need for training datasets by encoding problem instances directly into network parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating the viability of dataless approaches for the Maximum Independent Set problem, our comprehensive literature review reveals that no published work has extended these methods to the Resource-Constrained Project Scheduling Problem (RCPSP). This paper addresses this gap by presenting the first dataless neural network approach for RCPSP, providing a complete mathematical framework that transforms discrete scheduling constraints into differentiable objectives suitable for gradient-based optimization. Our approach leverages smooth relaxations and automatic differentiation to unlock GPU parallelization for project scheduling, traditionally a domain of sequential algorithms. We detail the mathematical formulation for both precedence and renewable resource constraints, including a memory-efficient dense time-grid representation. Implementation and comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120) are currently underway, with empirical results to be reported in an updated version of this paper."
      },
      {
        "id": "oai:arXiv.org:2507.05328v1",
        "title": "Going Beyond Heuristics by Imposing Policy Improvement as a Constraint",
        "link": "https://arxiv.org/abs/2507.05328",
        "author": "Chi-Chang Lee, Zhang-Wei Hong, Pulkit Agrawal",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05328v1 Announce Type: new \nAbstract: In many reinforcement learning (RL) applications, augmenting the task rewards with heuristic rewards that encode human priors about how a task should be solved is crucial for achieving desirable performance. However, because such heuristics are usually not optimal, much human effort and computational resources are wasted in carefully balancing tasks and heuristic rewards. Theoretically rigorous ways of incorporating heuristics rely on the idea of \\textit{policy invariance}, which guarantees that the performance of a policy obtained by maximizing heuristic rewards is the same as the optimal policy with respect to the task reward. However, in practice, policy invariance doesn't result in policy improvement, and such methods are known to empirically perform poorly. We propose a new paradigm to mitigate reward hacking and effectively use heuristics based on the practical goal of maximizing policy improvement instead of policy improvement. Our framework, Heuristic Enhanced Policy Optimization (HEPO), effectively leverages heuristics while avoiding the pitfall of prior methods for mitigating reward hacking. HEPO achieves superior performance on standard benchmarks with well-engineered reward functions. More surprisingly, HEPO allows policy optimization to achieve good performance even when heuristics are not well-engineered and designed by non-expert humans, showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a plug-and-play optimization method for leveraging heuristics in reinforcement learning. Code is available at https://github.com/Improbable-AI/hepo."
      },
      {
        "id": "oai:arXiv.org:2507.05330v1",
        "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents",
        "link": "https://arxiv.org/abs/2507.05330",
        "author": "Ming Gong, Xucheng Huang, Chenghan Yang, Xianhan Peng, Haoxin Wang, Yang Liu, Ling Jiang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05330v1 Announce Type: new \nAbstract: Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular \"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments."
      },
      {
        "id": "oai:arXiv.org:2507.05333v1",
        "title": "Causal Foundation Models: Disentangling Physics from Instrument Properties",
        "link": "https://arxiv.org/abs/2507.05333",
        "author": "Jeroen Audenaert, Daniel Muthukrishna, Paul F. Gregory, David W. Hogg, V. Ashley Villar",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05333v1 Announce Type: new \nAbstract: Foundation models for structured time series data must contend with a fundamental challenge: observations often conflate the true underlying physical phenomena with systematic distortions introduced by measurement instruments. This entanglement limits model generalization, especially in heterogeneous or multi-instrument settings. We present a causally-motivated foundation model that explicitly disentangles physical and instrumental factors using a dual-encoder architecture trained with structured contrastive learning. Leveraging naturally occurring observational triplets (i.e., where the same target is measured under varying conditions, and distinct targets are measured under shared conditions) our model learns separate latent representations for the underlying physical signal and instrument effects. Evaluated on simulated astronomical time series designed to resemble the complexity of variable stars observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS), our method significantly outperforms traditional single-latent space foundation models on downstream prediction tasks, particularly in low-data regimes. These results demonstrate that our model supports key capabilities of foundation models, including few-shot generalization and efficient adaptation, and highlight the importance of encoding causal structure into representation learning for structured data."
      },
      {
        "id": "oai:arXiv.org:2507.05346v1",
        "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks",
        "link": "https://arxiv.org/abs/2507.05346",
        "author": "William Fleshman, Benjamin Van Durme",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05346v1 Announce Type: new \nAbstract: The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG)."
      },
      {
        "id": "oai:arXiv.org:2507.05362v1",
        "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study",
        "link": "https://arxiv.org/abs/2507.05362",
        "author": "Riccardo Alberghi, Elizaveta Demyanenko, Luca Biggio, Luca Saglietti",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05362v1 Announce Type: new \nAbstract: Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize."
      },
      {
        "id": "oai:arXiv.org:2507.05376v1",
        "title": "YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries",
        "link": "https://arxiv.org/abs/2507.05376",
        "author": "Aquino Joctum, John Kandiri",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05376v1 Announce Type: new \nAbstract: Autonomous vehicle perception systems require robust pedestrian detection, particularly on geometrically complex roadways like Type-S curved surfaces, where standard RGB camera-based methods face limitations. This paper introduces YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework specifically for this challenge. YOLO-APD integrates several key architectural modifications: a parameter-free SimAM attention mechanism, computationally efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale feature pooling, the Mish activation function for improved optimization, and an Intelligent Gather & Distribute (IGD) module for superior feature fusion in the network's neck. The concept of leveraging vehicle steering dynamics for adaptive region-of-interest processing is also presented. Comprehensive evaluations on a custom CARLA dataset simulating complex scenarios demonstrate that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7% mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly outperforming baseline models, including YOLOv8. Furthermore, it maintains real-time processing capabilities at 100 FPS, showcasing a superior balance between accuracy and efficiency. Ablation studies validate the synergistic contribution of each integrated component. Evaluation on the KITTI dataset confirms the architecture's potential while highlighting the need for domain adaptation. This research advances the development of highly accurate, efficient, and adaptable perception systems based on cost-effective sensors, contributing to enhanced safety and reliability for autonomous navigation in challenging, less-structured driving environments."
      },
      {
        "id": "oai:arXiv.org:2507.05383v1",
        "title": "Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling",
        "link": "https://arxiv.org/abs/2507.05383",
        "author": "Alexandr A. Kalinin, Paula Llanos, Theresa Maria Sommer, Giovanni Sestini, Xinhai Hou, Jonathan Z. Sexton, Xiang Wan, Ivo D. Dinov, Brian D. Athey, Nicolas Rivron, Anne E. Carpenter, Beth Cimini, Shantanu Singh, Matthew J. O'Meara",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05383v1 Announce Type: new \nAbstract: Microscopy enables direct observation of cellular morphology in 3D, with transmitted-light methods offering low-cost, minimally invasive imaging and fluorescence microscopy providing specificity and contrast. Virtual staining combines these strengths by using machine learning to predict fluorescence images from label-free inputs. However, training of existing methods typically relies on loss functions that treat all pixels equally, thus reproducing background noise and artifacts instead of focusing on biologically meaningful signals. We introduce Spotlight, a simple yet powerful virtual staining approach that guides the model to focus on relevant cellular structures. Spotlight uses histogram-based foreground estimation to mask pixel-wise loss and to calculate a Dice loss on soft-thresholded predictions for shape-aware learning. Applied to a 3D benchmark dataset, Spotlight improves morphological representation while preserving pixel-level accuracy, resulting in virtual stains better suited for downstream tasks such as segmentation and profiling."
      },
      {
        "id": "oai:arXiv.org:2507.05385v1",
        "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data",
        "link": "https://arxiv.org/abs/2507.05385",
        "author": "Guanzhong Pan, Mei Tan, Hyunji Nam, Luc\\'ia Langlois, James Malamut, Liliana Deonizio, Dorottya Demszky",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05385v1 Announce Type: new \nAbstract: We introduce EduCoder, a domain-specialized tool designed to support utterance-level annotation of educational dialogue. While general-purpose text annotation tools for NLP and qualitative research abound, few address the complexities of coding education dialogue transcripts -- with diverse teacher-student and peer interactions. Common challenges include defining codebooks for complex pedagogical features, supporting both open-ended and categorical coding, and contextualizing utterances with external features, such as the lesson's purpose and the pedagogical value of the instruction. EduCoder is designed to address these challenges by providing a platform for researchers and domain experts to collaboratively define complex codebooks based on observed data. It incorporates both categorical and open-ended annotation types along with contextual materials. Additionally, it offers a side-by-side comparison of multiple annotators' responses, allowing comparison and calibration of annotations with others to improve data reliability. The system is open-source, with a demo video available."
      },
      {
        "id": "oai:arXiv.org:2507.05386v1",
        "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training",
        "link": "https://arxiv.org/abs/2507.05386",
        "author": "Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Min Xie, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05386v1 Announce Type: new \nAbstract: Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis shows that explicit mechanisms, such as KL penalty and chain-of-thought reasoning, are not the primary factors. Instead, we find that the implicit regularization inherent to RFT is a key factor in mitigating forgetting. Finally, we propose a rollout-based instance filtering algorithm to improve the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training."
      },
      {
        "id": "oai:arXiv.org:2507.05387v1",
        "title": "The Generalization Ridge: Information Flow in Natural Language Generation",
        "link": "https://arxiv.org/abs/2507.05387",
        "author": "Ruidi Chang, Chunyuan Deng, Hanjie Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05387v1 Announce Type: new \nAbstract: Transformer-based language models have achieved state-of-the-art performance in natural language generation (NLG) tasks, yet their internal mechanisms for synthesizing task-relevant information remain insufficiently understood. While prior studies suggest that intermediate layers often yield more generalizable representations than final layers, how this generalization ability emerges and propagates across layers during training remains unclear. To address this gap, we propose InfoRidge, an information-theoretic framework, to characterize how predictive information-the mutual information between hidden representations and target outputs-varies across depth. Estimating this quantity enables us to trace the flow of task-relevant information throughout the model during training. Our experiments across various models and datasets reveal a consistent non-monotonic trend: predictive information peaks in upper-middle layers-forming a generalization ridge-before declining in final layers, reflecting a transition between generalization and memorization. To further investigate this phenomenon, we introduce residual scaling coefficients-trainable scalar parameters applied to each residual block-which serve as functional probes for assessing the relative importance of individual transformer layers. These coefficients reveal that, under distribution shift, models downweight final layers and increasingly rely on ridge layers, highlighting their role in generalization. Together, these findings offer new insights into the internal mechanisms of transformers and underscore the critical role of intermediate layers in supporting generalization."
      },
      {
        "id": "oai:arXiv.org:2507.05390v1",
        "title": "From General to Specialized: The Need for Foundational Models in Agriculture",
        "link": "https://arxiv.org/abs/2507.05390",
        "author": "Vishal Nedungadi, Xingguo Xiong, Aike Potze, Ron Van Bree, Tao Lin, Marc Ru{\\ss}wurm, Ioannis N. Athanasiadis",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05390v1 Announce Type: new \nAbstract: Food security remains a global concern as population grows and climate change intensifies, demanding innovative solutions for sustainable agricultural productivity. Recent advances in foundation models have demonstrated remarkable performance in remote sensing and climate sciences, and therefore offer new opportunities for agricultural monitoring. However, their application in challenges related to agriculture-such as crop type mapping, crop phenology estimation, and crop yield estimation-remains under-explored. In this work, we quantitatively evaluate existing foundational models to assess their effectivity for a representative set of agricultural tasks. From an agricultural domain perspective, we describe a requirements framework for an ideal agricultural foundation model (CropFM). We then survey and compare existing general-purpose foundational models in this framework and empirically evaluate two exemplary of them in three representative agriculture specific tasks. Finally, we highlight the need for a dedicated foundational model tailored specifically to agriculture."
      },
      {
        "id": "oai:arXiv.org:2507.05391v1",
        "title": "Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences",
        "link": "https://arxiv.org/abs/2507.05391",
        "author": "Guillem Ram\\'irez, Alexandra Birch, Ivan Titov",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05391v1 Announce Type: new \nAbstract: Large language models (LLMs) are primarily accessed via commercial APIs, but this often requires users to expose their data to service providers. In this paper, we explore how users can stay in control of their data by using privacy profiles: simple natural language instructions that say what should and should not be revealed. We build a framework where a local model uses these instructions to rewrite queries, only hiding details deemed sensitive by the user, before sending them to an external model, thus balancing privacy with performance. To support this research, we introduce PEEP, a multilingual dataset of real user queries annotated to mark private content and paired with synthetic privacy profiles. Our experiments with lightweight LLMs show they can follow these instructions to some extent, but also face consistent challenges, highlighting the need for models that better understand and comply with user-defined privacy preferences."
      },
      {
        "id": "oai:arXiv.org:2507.05393v1",
        "title": "Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration",
        "link": "https://arxiv.org/abs/2507.05393",
        "author": "Jose M. Montero, Jose-Luis Lisani",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05393v1 Announce Type: new \nAbstract: Recent advances in deep learning, particularly neural networks, have significantly impacted a wide range of fields, including the automatic enhancement of underwater images. This paper presents a deep learning-based approach to improving underwater image quality by integrating human subjective assessments into the training process. To this end, we utilize publicly available datasets containing underwater images labeled by experts as either high or low quality. Our method involves first training a classifier network to distinguish between high- and low-quality images. Subsequently, generative adversarial networks (GANs) are trained using various enhancement criteria to refine the low-quality images. The performance of the GAN models is evaluated using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through qualitative analysis. Results demonstrate that the proposed model -- particularly when incorporating criteria such as color fidelity and image sharpness -- achieves substantial improvements in both perceived and measured image quality."
      },
      {
        "id": "oai:arXiv.org:2507.05394v1",
        "title": "pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models",
        "link": "https://arxiv.org/abs/2507.05394",
        "author": "Sajjad Ghiasvand, Mahnoosh Alizadeh, Ramtin Pedarsani",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05394v1 Announce Type: new \nAbstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable generalization in zero- and few-shot settings, but adapting them efficiently to decentralized, heterogeneous data remains a challenge. While prompt tuning has emerged as a popular parameter-efficient approach in personalized federated learning, existing methods often sacrifice generalization in favor of personalization, struggling particularly on unseen classes or domains. In this work, we propose pFedMMA, the first personalized federated learning framework that leverages multi-modal adapters for vision-language tasks. Each adapter contains modality-specific up- and down-projection layers alongside a globally shared projection that aligns cross-modal features. Our asymmetric optimization strategy allows clients to locally adapt to personalized data distributions while collaboratively training the shared projection to improve global generalization. This design is also communication-efficient, as only the shared component is exchanged during rounds. Through extensive experiments across eleven datasets, including domain- and label-shift scenarios, we show that pFedMMA achieves state-of-the-art trade-offs between personalization and generalization, outperforming recent federated prompt tuning methods. The code is available at https://github.com/sajjad-ucsb/pFedMMA."
      },
      {
        "id": "oai:arXiv.org:2507.05397v1",
        "title": "Neural-Driven Image Editing",
        "link": "https://arxiv.org/abs/2507.05397",
        "author": "Pengfei Zhou, Jie Xia, Xiaopeng Peng, Wangbo Zhao, Zilong Ye, Zekai Li, Suorong Yang, Jiadong Pan, Yuanxiang Chen, Ziqiao Wang, Kai Wang, Qian Zheng, Xiaojun Chang, Gang Pan, Shurong Dong, Kaipeng Zhang, Yang You",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05397v1 Announce Type: new \nAbstract: Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area."
      },
      {
        "id": "oai:arXiv.org:2507.05405v1",
        "title": "Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification",
        "link": "https://arxiv.org/abs/2507.05405",
        "author": "Luca Marzari, Ferdinando Cicalese, Alessandro Farinelli",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05405v1 Announce Type: new \nAbstract: We present $\\textbf{P}$robabilistically $\\textbf{T}$ightened $\\textbf{Li}$near $\\textbf{R}$elaxation-based $\\textbf{P}$erturbation $\\textbf{A}$nalysis ($\\texttt{PT-LiRPA}$), a novel framework that combines over-approximation techniques from LiRPA-based approaches with a sampling-based method to compute tight intermediate reachable sets. In detail, we show that with negligible computational overhead, $\\texttt{PT-LiRPA}$ exploiting the estimated reachable sets, significantly tightens the lower and upper linear bounds of a neural network's output, reducing the computational cost of formal verification tools while providing probabilistic guarantees on verification soundness. Extensive experiments on standard formal verification benchmarks, including the International Verification of Neural Networks Competition, show that our $\\texttt{PT-LiRPA}$-based verifier improves robustness certificates by up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic approach results in a valuable solution for challenging competition entries where state-of-the-art formal verification methods fail, allowing us to provide answers with high confidence (i.e., at least 99%)."
      },
      {
        "id": "oai:arXiv.org:2507.05411v1",
        "title": "AXLearn: Modular Large Model Training on Heterogeneous Infrastructure",
        "link": "https://arxiv.org/abs/2507.05411",
        "author": "Mark Lee, Tom Gunter, Chang Lan, John Peebles, Hanzhi Zhou, Kelvin Zou, Sneha Bangalore, Chung-Cheng Chiu, Nan Du, Xianzhi Du, Philipp Dufter, Ruixuan Hou, Haoshuo Huang, Dongseong Hwang, Xiang Kong, Jinhao Lei, Tao Lei, Meng Li, Li Li, Jiarui Lu, Zhiyun Lu, Yiping Ma, David Qiu, Vivek Rathod, Senyu Tong, Zhucheng Tu, Jianyu Wang, Yongqiang Wang, Zirui Wang, Floris Weers, Sam Wiseman, Guoli Yin, Bowen Zhang, Xiyou Zhou, Danyang Zhuo, Cheng Leong, Ruoming Pang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05411v1 Announce Type: new \nAbstract: We design and implement AXLearn, a production deep learning system that facilitates scalable and high-performance training of large deep learning models. Compared to other state-of-the-art deep learning systems, AXLearn has a unique focus on modularity and support for heterogeneous hardware infrastructure. AXLearn's internal interfaces between software components follow strict encapsulation, allowing different components to be assembled to facilitate rapid model development and experimentation on heterogeneous compute infrastructure. We introduce a novel method of quantifying modularity via Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains constant complexity as we scale the components in the system, compared to linear or quadratic complexity in other systems. This allows integrating features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred of modules with just 10 lines of code, compared to hundreds as required in other systems. At the same time, AXLearn maintains equivalent performance compared to state-of-the-art training systems. Finally, we share our experience in the development and operation of AXLearn."
      },
      {
        "id": "oai:arXiv.org:2507.05412v1",
        "title": "Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift",
        "link": "https://arxiv.org/abs/2507.05412",
        "author": "Gautam Sreekumar, Vishnu Naresh Boddeti",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05412v1 Announce Type: new \nAbstract: We consider the problem of learning robust discriminative representations of causally-related latent variables. In addition to observational data, the training dataset also includes interventional data obtained through targeted interventions on some of these latent variables to learn representations robust against the resulting interventional distribution shifts. Existing approaches treat interventional data like observational data, even when the underlying causal model is known, and ignore the independence relations that arise from these interventions. Since these approaches do not fully exploit the causal relational information resulting from interventions, they learn representations that produce large disparities in predictive performance on observational and interventional data, which worsens when the number of interventional training samples is limited. In this paper, (1) we first identify a strong correlation between this performance disparity and adherence of the representations to the independence conditions induced by the interventional causal model. (2) For linear models, we derive sufficient conditions on the proportion of interventional data in the training dataset, for which enforcing interventional independence between representations corresponding to the intervened node and its non-descendants lowers the error on interventional data. Combining these insights, (3) we propose RepLIn, a training algorithm to explicitly enforce this statistical independence during interventions. We demonstrate the utility of RepLIn on a synthetic dataset and on real image and text datasets on facial attribute classification and toxicity detection, respectively. Our experiments show that RepLIn is scalable with the number of nodes in the causal graph and is suitable to improve the robust representations against interventional distribution shifts of both continuous and discrete latent variables."
      },
      {
        "id": "oai:arXiv.org:2507.05416v1",
        "title": "EmissionNet: Air Quality Pollution Forecasting for Agriculture",
        "link": "https://arxiv.org/abs/2507.05416",
        "author": "Prady Saligram, Tanvir Bhathal",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05416v1 Announce Type: new \nAbstract: Air pollution from agricultural emissions is a significant yet often overlooked contributor to environmental and public health challenges. Traditional air quality forecasting models rely on physics-based approaches, which struggle to capture complex, nonlinear pollutant interactions. In this work, we explore forecasting N$_2$O agricultural emissions through evaluating popular architectures, and proposing two novel deep learning architectures, EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage convolutional and transformer-based architectures to extract spatial-temporal dependencies from high-resolution emissions data"
      },
      {
        "id": "oai:arXiv.org:2507.05418v1",
        "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning",
        "link": "https://arxiv.org/abs/2507.05418",
        "author": "Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05418v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual QA, and code generation, yet their multilingual reasoning capabilities in these tasks remain underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. Current multilingual benchmarks focus only on final answers, overlooking whether models actually reason in the target language. To address this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark with annotated reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. We further propose BRIDGE, a novel training method that guides supervised fine-tuning and test-time reinforcement learning with a language-consistency reward to align reasoning with the input language. Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to assess answer correctness and the quality and language consistency of reasoning traces, enabling nuanced and scalable analysis beyond surface-level metrics. Our results show that BRIDGE significantly enhances multilingual reasoning fidelity, demonstrating that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. https://jd730.github.io/projects/GeoFact-X_BRIDGE"
      },
      {
        "id": "oai:arXiv.org:2507.05419v1",
        "title": "Motion Generation: A Survey of Generative Approaches and Benchmarks",
        "link": "https://arxiv.org/abs/2507.05419",
        "author": "Aliasghar Khani, Arianna Rampini, Bruno Roy, Larasika Nadela, Noa Kaplan, Evan Atherton, Derek Cheung, Jacky Bibliowicz",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05419v1 Announce Type: new \nAbstract: Motion generation, the task of synthesizing realistic motion sequences from various conditioning inputs, has become a central problem in computer vision, computer graphics, and robotics, with applications ranging from animation and virtual agents to human-robot interaction. As the field has rapidly progressed with the introduction of diverse modeling paradigms including GANs, autoencoders, autoregressive models, and diffusion-based techniques, each approach brings its own advantages and limitations. This growing diversity has created a need for a comprehensive and structured review that specifically examines recent developments from the perspective of the generative approach employed.\n  In this survey, we provide an in-depth categorization of motion generation methods based on their underlying generative strategies. Our main focus is on papers published in top-tier venues since 2023, reflecting the most recent advancements in the field. In addition, we analyze architectural principles, conditioning mechanisms, and generation settings, and compile a detailed overview of the evaluation metrics and datasets used across the literature. Our objective is to enable clearer comparisons and identify open challenges, thereby offering a timely and foundational reference for researchers and practitioners navigating the rapidly evolving landscape of motion generation."
      },
      {
        "id": "oai:arXiv.org:2507.05424v1",
        "title": "\"Lost-in-the-Later\": Framework for Quantifying Contextual Grounding in Large Language Models",
        "link": "https://arxiv.org/abs/2507.05424",
        "author": "Yufei Tao, Adam Hiatt, Rahul Seetharaman, Ameeta Agrawal",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05424v1 Announce Type: new \nAbstract: Large language models are capable of leveraging both contextual and parametric knowledge but how they prioritize and integrate these sources remains underexplored. We introduce CoPE, a novel evaluation framework that systematically measures contextual knowledge (CK) and parametric knowledge (PK) across models and languages. Using our MultiWikiAtomic dataset in English, Spanish, and Danish, we analyze how large language models (LLMs) integrate context, prioritize information, and incorporate PK in open-ended question answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where LLMs tend to overlook or deprioritize information that appears later in a given context, revealing a strong positional bias that affects contextual grounding. We further find that reasoning models, as well as non-reasoning models prompted with chain-of-thought (CoT), use context even less than non-reasoning models without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting, in particular, results in lower recall and shorter responses, leading to degraded contextual grounding. Based on these insights, we design prompt-based methods to effectively leverage input context. A case study applying CoPE to summarization demonstrates that CK-informed prompting improves factual grounding and reduces hallucination."
      },
      {
        "id": "oai:arXiv.org:2507.05426v1",
        "title": "Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors",
        "link": "https://arxiv.org/abs/2507.05426",
        "author": "Lanqing Guo, Yufei Wang, Hezhen Hu, Yan Zheng, Yeying Jin, Siyu Huang, Zhangyang Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05426v1 Announce Type: new \nAbstract: Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\\times$ speedup, providing a more efficient and effective approach to 3D scene local editing."
      },
      {
        "id": "oai:arXiv.org:2507.05427v1",
        "title": "OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts",
        "link": "https://arxiv.org/abs/2507.05427",
        "author": "Shiting Xiao, Rishabh Kabra, Yuhang Li, Donghyun Lee, Joao Carreira, Priyadarshini Panda",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05427v1 Announce Type: new \nAbstract: The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the model's spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks, including ADE20k, PASCAL, ScanNet, and SUN-RGBD."
      },
      {
        "id": "oai:arXiv.org:2507.05432v1",
        "title": "Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation",
        "link": "https://arxiv.org/abs/2507.05432",
        "author": "Inayat Rasool, Pappu Kumar Yadav, Amee Parmar, Hasan Mirzakhaninafchi, Rikesh Budhathoki, Zain Ul Abideen Usmani, Supriya Paudel, Ivan Perez Olivera, Eric Jone",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05432v1 Announce Type: new \nAbstract: Uniform and excessive herbicide application in modern agriculture contributes to increased input costs, environmental pollution, and the emergence of herbicide resistant weeds. To address these challenges, we developed a vision guided, AI-driven variable rate sprayer system capable of detecting weed presence, estimating canopy size, and dynamically adjusting nozzle activation in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference, and uses an Arduino Uno-based relay interface to control solenoid actuated nozzles based on canopy segmentation results. Indoor trials were conducted using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to simulate a range of weed patch scenarios. The YOLO11n model achieved a mean average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision of 0.55, and recall of 0.52. System performance was validated using water sensitive paper, which showed an average spray coverage of 24.22% in zones where canopy was present. An upward trend in mean spray coverage from 16.22% for small canopies to 21.46% and 21.65% for medium and large canopies, respectively, demonstrated the system's capability to adjust spray output based on canopy size in real time. These results highlight the potential of combining real time deep learning with low-cost embedded hardware for selective herbicide application. Future work will focus on expanding the detection capabilities to include three common weed species in South Dakota: water hemp (Amaranthus tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed by further validation in both indoor and field trials within soybean and corn production systems."
      },
      {
        "id": "oai:arXiv.org:2507.05441v1",
        "title": "Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack",
        "link": "https://arxiv.org/abs/2507.05441",
        "author": "Edward Raff, Karen Kukla, Michel Benaroch, Joseph Comprix",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05441v1 Announce Type: new \nAbstract: Bad actors, primarily distressed firms, have the incentive and desire to manipulate their financial reports to hide their distress and derive personal gains. As attackers, these firms are motivated by potentially millions of dollars and the availability of many publicly disclosed and used financial modeling frameworks. Existing attack methods do not work on this data due to anti-correlated objectives that must both be satisfied for the attacker to succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that adapt the attacker's search direction to find $20\\times$ more satisfying attacks compared to standard attacks. The result is that in $\\approx50\\%$ of cases, a company could inflate their earnings by 100-200%, while simultaneously reducing their fraud scores by 15%. By working with lawyers and professional accountants, we ensure our threat model is realistic to how such frauds are performed in practice."
      },
      {
        "id": "oai:arXiv.org:2507.05443v1",
        "title": "Gendered Divides in Online Discussions about Reproductive Rights",
        "link": "https://arxiv.org/abs/2507.05443",
        "author": "Ashwin Rao, Sze Yuh Nina Wang, Kristina Lerman",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05443v1 Announce Type: new \nAbstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health Organization marked a turning point in the national debate over reproductive rights. While the ideological divide over abortion is well documented, less is known about how gender and local sociopolitical contexts interact to shape public discourse. Drawing on nearly 10 million abortion-related posts on X (formerly Twitter) from users with inferred gender, ideology and location, we show that gender significantly moderates abortion attitudes and emotional expression, particularly in conservative regions, and independently of ideology. This creates a gender gap in abortion attitudes that grows more pronounced in conservative regions. The leak of the Dobbs draft opinion further intensified online engagement, disproportionately mobilizing pro-abortion women in areas where access was under threat. These findings reveal that abortion discourse is not only ideologically polarized but also deeply structured by gender and place, highlighting the central role of identity in shaping political expression during moments of institutional disruption."
      },
      {
        "id": "oai:arXiv.org:2507.05444v1",
        "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs",
        "link": "https://arxiv.org/abs/2507.05444",
        "author": "Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05444v1 Announce Type: new \nAbstract: Vocabulary acquisition poses a significant challenge for second-language (L2) learners, especially when learning typologically distant languages such as English and Korean, where phonological and structural mismatches complicate vocabulary learning. Recently, large language models (LLMs) have been used to generate keyword mnemonics by leveraging similar keywords from a learner's first language (L1) to aid in acquiring L2 vocabulary. However, most of this research has focused on native English speakers learning other languages, rather than the reverse. In this paper, we present PhoniTale, a novel cross-lingual mnemonic generation system that retrieves L1 keyword sequence based on phonological similarity and uses LLMs to generate mnemonics. We evaluate PhoniTale using both automated metrics and human evaluations, comparing its output to mnemonics created by humans and by previous automated approaches. To assess practical effectiveness, we also conduct a short-term recall test measuring mnemonic helpfulness. Our findings show that PhoniTale performs comparably to human-authored mnemonics. We also highlight key areas for future improvement in mnemonic quality and methodology."
      },
      {
        "id": "oai:arXiv.org:2507.05448v1",
        "title": "On the Semantics of Large Language Models",
        "link": "https://arxiv.org/abs/2507.05448",
        "author": "Martin Schuele",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05448v1 Announce Type: new \nAbstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to replicate human language abilities through technology, ranging from text generation to engaging in conversations. However, it remains controversial to what extent these systems truly understand language. We examine this issue by narrowing the question down to the semantics of LLMs at the word and sentence level. By examining the inner workings of LLMs and their generated representation of language and by drawing on classical semantic theories by Frege and Russell, we get a more nuanced picture of the potential semantic capabilities of LLMs."
      },
      {
        "id": "oai:arXiv.org:2507.05455v1",
        "title": "ModelCitizens:Representing Community Voices in Online Safety",
        "link": "https://arxiv.org/abs/2507.05455",
        "author": "Ashima Suvarna, Christina Chance, Hamid Palangi, Sophie Hao, Thomas Hartvigsen, Saadia Gabriel",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05455v1 Announce Type: new \nAbstract: Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation."
      },
      {
        "id": "oai:arXiv.org:2507.05463v1",
        "title": "Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video",
        "link": "https://arxiv.org/abs/2507.05463",
        "author": "Md Zahid Hasan, Guillermo Basulto-Elias, Jun Ha Chang, Sahuna Hallmark, Matthew Rizzo, Anuj Sharma, Soumik Sarkar",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05463v1 Announce Type: new \nAbstract: We introduce scenario-based cognitive status identification in older drivers from Naturalistic driving videos and large vision models. In recent times, cognitive decline, including Alzheimer's disease (AD) and mild cognitive impairment (MCI), is often underdiagnosed due to the time-consuming and costly nature of current diagnostic methods. By analyzing real-world driving behavior captured through in-vehicle systems, this research aims to extract \"digital fingerprints\" that correlate with functional decline and clinical features of MCI and AD. Moreover, modern large vision models can draw meaningful insights from everyday driving patterns of older patients to early detect cognitive decline. We propose a framework that uses large vision models and naturalistic driving videos to analyze driver behavior, classify cognitive status and predict disease progression. We leverage the strong relationship between real-world driving behavior as an observation of the current cognitive status of the drivers where the vehicle can be utilized as a \"diagnostic tool\". Our method identifies early warning signs of functional impairment, contributing to proactive intervention strategies. This work enhances early detection and supports the development of scalable, non-invasive monitoring systems to mitigate the growing societal and economic burden of cognitive decline in the aging population."
      },
      {
        "id": "oai:arXiv.org:2507.05465v1",
        "title": "2048: Reinforcement Learning in a Delayed Reward Environment",
        "link": "https://arxiv.org/abs/2507.05465",
        "author": "Prady Saligram, Tanvir Bhathal, Robby Manihani",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05465v1 Announce Type: new \nAbstract: Delayed and sparse rewards present a fundamental obstacle for reinforcement-learning (RL) agents, which struggle to assign credit for actions whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes this challenge: although frequent small score changes yield immediate feedback, they often mislead agents into locally optimal but globally suboptimal strategies. In this work, we introduce a unified, distributional multi-step RL framework designed to directly optimize long-horizon performance. Using the open source Gym-2048 environment we develop and compare four agent variants: standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN (H-DQN) that integrates distributional learning, dueling architectures, noisy networks, prioritized replay, and more. Empirical evaluation reveals a clear hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to 5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048 tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These results demonstrate that distributional, multi-step targets substantially enhance performance in sparse-reward domains, and they suggest promising avenues for further gains through model-based planning and curriculum learning."
      },
      {
        "id": "oai:arXiv.org:2507.05477v1",
        "title": "Epistemically-guided forward-backward exploration",
        "link": "https://arxiv.org/abs/2507.05477",
        "author": "N\\'uria Armengol Urp\\'i, Marin Vlastelica, Georg Martius, Stelian Coros",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05477v1 Announce Type: new \nAbstract: Zero-shot reinforcement learning is necessary for extracting optimal policies in absence of concrete rewards for fast adaptation to future problem settings. Forward-backward representations (FB) have emerged as a promising method for learning optimal policies in absence of rewards via a factorization of the policy occupancy measure. However, up until now, FB and many similar zero-shot reinforcement learning algorithms have been decoupled from the exploration problem, generally relying on other exploration algorithms for data collection. We argue that FB representations should fundamentally be used for exploration in order to learn more efficiently. With this goal in mind, we design exploration policies that arise naturally from the FB representation that minimize the posterior variance of the FB representation, hence minimizing its epistemic uncertainty. We empirically demonstrate that such principled exploration strategies improve sample complexity of the FB algorithm considerably in comparison to other exploration methods. Code is publicly available at https://sites.google.com/view/fbee-url."
      },
      {
        "id": "oai:arXiv.org:2507.05478v1",
        "title": "Dynamic Regret Reduces to Kernelized Static Regret",
        "link": "https://arxiv.org/abs/2507.05478",
        "author": "Andrew Jacobsen, Alessandro Rudi, Francesco Orabona, Nicolo Cesa-Bianchi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05478v1 Announce Type: new \nAbstract: We study dynamic regret in online convex optimization, where the objective is to achieve low cumulative loss relative to an arbitrary benchmark sequence. By observing that competing with an arbitrary sequence of comparators $u_{1},\\ldots,u_{T}$ in $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ is equivalent to competing with a fixed comparator function $u:[1,T]\\to \\mathcal{W}$, we frame dynamic regret minimization as a static regret problem in a function space. By carefully constructing a suitable function space in the form of a Reproducing Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal $R_{T}(u_{1},\\ldots,u_{T}) = \\mathcal{O}(\\sqrt{\\sum_{t}\\|u_{t}-u_{t-1}\\|T})$ dynamic regret guarantee in the setting of linear losses, and yields new scale-free and directionally-adaptive dynamic regret guarantees. Moreover, unlike prior dynamic-to-static reductions -- which are valid only for linear losses -- our reduction holds for any sequence of losses, allowing us to recover $\\mathcal{O}\\big(\\|u\\|^2+d_{\\mathrm{eff}}(\\lambda)\\ln T\\big)$ bounds in exp-concave and improper linear regression settings, where $d_{\\mathrm{eff}}(\\lambda)$ is a measure of complexity of the RKHS. Despite working in an infinite-dimensional space, the resulting reduction leads to algorithms that are computable in practice, due to the reproducing property of RKHSs."
      },
      {
        "id": "oai:arXiv.org:2507.05482v1",
        "title": "Navigating Sparse Molecular Data with Stein Diffusion Guidance",
        "link": "https://arxiv.org/abs/2507.05482",
        "author": "Van Khoa Nguyen, Lionel Blond\\'e, Alexandros Kalousis",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05482v1 Announce Type: new \nAbstract: Stochastic optimal control (SOC) has recently emerged as a principled framework for fine-tuning diffusion models. However, its dependence on computationally intensive simulations makes it impractical for fast sampling. In parallel, a class of training-free approaches has been developed that guides diffusion models using off-the-shelf classifiers on predicted clean samples, bypassing the need to train classifiers on noisy data. These methods can be interpreted as approximate SOC schemes, using Tweedie's formula to estimate diffusion posteriors. In practice, however, such direct approximations can introduce significant errors, leading to unreliable guidance. In this work, we unify the strengths of both paradigms by proposing a novel training-free diffusion guidance framework based on a surrogate stochastic optimal control objective. We derive a new theoretical bound on the value function that reveals the necessity of correcting the approximate posteriors to remain faithful to the true diffusion posterior. To this end, we connect the problem with Stein variational inference, which seeks the steepest descent direction that minimizes the Kullback-Leibler discrepancy between the two posteriors. Our method, which we refer to as Stein Diffusion Guidance (SDG), introduces a principled correction mechanism and incorporates a novel running cost functional to enable effective guidance in low-density regions. Experiments on challenging molecular generation tasks demonstrate that SDG significantly outperforms standard training-free guidance methods, highlighting its potential for broader applications."
      },
      {
        "id": "oai:arXiv.org:2507.05496v1",
        "title": "Cloud Diffusion Part 1: Theory and Motivation",
        "link": "https://arxiv.org/abs/2507.05496",
        "author": "Andrew Randono",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05496v1 Announce Type: new \nAbstract: Diffusion models for image generation function by progressively adding noise to an image set and training a model to separate out the signal from the noise. The noise profile used by these models is white noise -- that is, noise based on independent normal distributions at each point whose mean and variance is independent of the scale. By contrast, most natural image sets exhibit a type of scale invariance in their low-order statistical properties characterized by a power-law scaling. Consequently, natural images are closer (in a quantifiable sense) to a different probability distribution that emphasizes large scale correlations and de-emphasizes small scale correlations. These scale invariant noise profiles can be incorporated into diffusion models in place of white noise to form what we will call a ``Cloud Diffusion Model\". We argue that these models can lead to faster inference, improved high-frequency details, and greater controllability. In a follow-up paper, we will build and train a Cloud Diffusion Model that uses scale invariance at a fundamental level and compare it to classic, white noise diffusion models."
      },
      {
        "id": "oai:arXiv.org:2507.05498v1",
        "title": "Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)",
        "link": "https://arxiv.org/abs/2507.05498",
        "author": "Reza T. Batley, Chanwook Park, Wing Kam Liu, Sourav Saha",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05498v1 Announce Type: new \nAbstract: Data-driven science and computation have advanced immensely to construct complex functional relationships using trainable parameters. However, efficiently discovering interpretable and accurate closed-form expressions from complex dataset remains a challenge. The article presents a novel approach called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that uses an accurate, frugal, fast, separable, and scalable neural architecture with symbolic regression to discover closed-form expressions from limited observation. The article presents the two-step Ex-HiDeNN algorithm with a separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN are tested on several benchmark problems, including discerning a dynamical system from data, and the outcomes are reported. Ex-HiDeNN generally shows outstanding approximation capability in these benchmarks, producing orders of magnitude smaller errors compared to reference data and traditional symbolic regression. Later, Ex-HiDeNN is applied to three engineering applications: a) discovering a closed-form fatigue equation, b) identification of hardness from micro-indentation test data, and c) discovering the expression for the yield surface with data. In every case, Ex-HiDeNN outperformed the reference methods used in the literature. The proposed method is built upon the foundation and published works of the authors on Hierarchical Deep Learning Neural Network (HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about the current limitations and future extensions of Ex-HiDeNN."
      },
      {
        "id": "oai:arXiv.org:2507.05499v1",
        "title": "LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving",
        "link": "https://arxiv.org/abs/2507.05499",
        "author": "Giulio Federico, Fabio Carrara, Claudio Gennaro, Giuseppe Amato, Marco Di Benedetto",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05499v1 Announce Type: new \nAbstract: Generating consistent multi-view images from a single image remains challenging. Lack of spatial consistency often degrades 3D mesh quality in surface reconstruction. To address this, we propose LoomNet, a novel multi-view diffusion architecture that produces coherent images by applying the same diffusion model multiple times in parallel to collaboratively build and leverage a shared latent space for view consistency. Each viewpoint-specific inference generates an encoding representing its own hypothesis of the novel view from a given camera pose, which is projected onto three orthogonal planes. For each plane, encodings from all views are fused into a single aggregated plane. These aggregated planes are then processed to propagate information and interpolate missing regions, combining the hypotheses into a unified, coherent interpretation. The final latent space is then used to render consistent multi-view images. LoomNet generates 16 high-quality and coherent views in just 15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on both image quality and reconstruction metrics, also showing creativity by producing diverse, plausible novel views from the same input."
      },
      {
        "id": "oai:arXiv.org:2507.05507v1",
        "title": "Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs",
        "link": "https://arxiv.org/abs/2507.05507",
        "author": "Godwin Badu-Marfo, Bilal Farooq",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05507v1 Announce Type: new \nAbstract: We present an integrated graph-based neural networks architecture for predicting campus buildings occupancy and inter-buildings movement at dynamic temporal resolution that learns traffic flow patterns from Wi-Fi logs combined with the usage schedules within the buildings. The relative traffic flows are directly estimated from the WiFi data without assuming the occupant behaviour or preferences while maintaining individual privacy. We formulate the problem as a data-driven graph structure represented by a set of nodes (representing buildings), connected through a route of edges or links using a novel Graph Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable success in modelling complex patterns. We describe the formulation, model estimation, interpretability and examine the relative performance of our proposed model. We also present an illustrative architecture of the models and apply on real-world WiFi logs collected at the Toronto Metropolitan University campus. The results of the experiments show that the integrated GCLSTM models significantly outperform traditional pedestrian flow estimators like the Multi Layer Perceptron (MLP) and Linear Regression."
      },
      {
        "id": "oai:arXiv.org:2507.05508v1",
        "title": "Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning",
        "link": "https://arxiv.org/abs/2507.05508",
        "author": "Ze'ev Zukerman, Bassel Hamoud, Kfir Y. Levy",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05508v1 Announce Type: new \nAbstract: Distributed learning methods have gained substantial momentum in recent years, with communication overhead often emerging as a critical bottleneck. Gradient compression techniques alleviate communication costs but involve an inherent trade-off between the empirical efficiency of biased compressors and the theoretical guarantees of unbiased compressors. In this work, we introduce a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased compressors to construct statistically unbiased estimates. This approach effectively bridges the gap between biased and unbiased methods, combining the strengths of both. To showcase the versatility of our method, we apply it to popular compressors, like Top-$k$ and bit-wise compressors, resulting in enhanced variants. Furthermore, we derive an adaptive version of our approach to further improve its performance. We validate our method empirically on distributed deep learning tasks."
      },
      {
        "id": "oai:arXiv.org:2507.05510v1",
        "title": "Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth",
        "link": "https://arxiv.org/abs/2507.05510",
        "author": "Shuyang Du, Jennifer Zhang, Will Y. Zou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05510v1 Announce Type: new \nAbstract: User growth is a major strategy for consumer internet companies. To optimize costly marketing campaigns and maximize user engagement, we propose a novel treatment effect optimization methodology to enhance user growth marketing. By leveraging deep learning, our algorithm learns from past experiments to optimize user selection and reward allocation, maximizing campaign impact while minimizing costs. Unlike traditional prediction methods, our model directly models uplifts in key business metrics. Further, our deep learning model can jointly optimize parameters for an aggregated loss function using softmax gating. Our approach surpasses traditional methods by directly targeting desired business metrics and demonstrates superior algorithmic flexibility in handling complex business constraints. Comprehensive evaluations, including comparisons with state-of-the-art techniques such as R-learner and Causal Forest, validate the effectiveness of our model. We experimentally demonstrate that our proposed constrained and direct optimization algorithms significantly outperform state-of-the-art methods by over $20\\%$, proving their cost-efficiency and real-world impact. The versatile methods can be applied to various product scenarios, including optimal treatment allocation. Its effectiveness has also been validated through successful worldwide production deployments."
      },
      {
        "id": "oai:arXiv.org:2507.05511v1",
        "title": "Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects",
        "link": "https://arxiv.org/abs/2507.05511",
        "author": "Jennifer Y. Zhang, Shuyang Du, Will Y. Zou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05511v1 Announce Type: new \nAbstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted across a wide range of scientific and industrial applications, the treatment action space can naturally expand, from a binary treatment variable to a structured treatment policy. This policy may include several policy factors such as a continuous treatment intensity variable, or discrete treatment assignments. From first principles, we derive the formulation for incorporating multiple treatment policy variables into the functional forms of individual and average treatment effects. Building on this, we develop a methodology to directly rank subjects using aggregated HTE functions. In particular, we construct a Neural-Augmented Naive Bayes layer within a deep learning framework to incorporate an arbitrary number of factors that satisfies the Naive Bayes assumption. The factored layer is then applied with continuous treatment variables, treatment assignment, and direct ranking of aggregated treatment effect functions. Together, these algorithms build towards a generic framework for deep learning of heterogeneous treatment policies, and we show their power to improve performance with public datasets."
      },
      {
        "id": "oai:arXiv.org:2507.05513v1",
        "title": "Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model",
        "link": "https://arxiv.org/abs/2507.05513",
        "author": "Mengyao Xu, Gabriel Moreira, Ronay Ak, Radek Osmulski, Yauhen Babakhin, Zhiding Yu, Benedikt Schifferer, Even Oldridge",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05513v1 Announce Type: new \nAbstract: Motivated by the growing demand for retrieval systems that operate across modalities, we introduce llama-nemoretriever-colembed, a unified text-image retrieval model that delivers state-of-the-art performance across multiple benchmarks. We release two model variants, 1B and 3B. The 3B model achieves state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, placing first on both leaderboards as of June 27, 2025.\n  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM), modifies its architecture by replacing causal attention with bidirectional attention, and integrates a ColBERT-style late interaction mechanism to enable fine-grained multimodal retrieval in a shared embedding space. While this mechanism delivers superior retrieval accuracy, it introduces trade-offs in storage and efficiency. We provide a comprehensive analysis of these trade-offs. Additionally, we adopt a two-stage training strategy to enhance the model's retrieval capabilities."
      },
      {
        "id": "oai:arXiv.org:2507.05517v1",
        "title": "Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications",
        "link": "https://arxiv.org/abs/2507.05517",
        "author": "Jean-Philippe Corbeil, Asma Ben Abacha, George Michalopoulos, Phillip Swazinna, Miguel Del-Agua, Jerome Tremblay, Akila Jeeson Daniel, Cari Bader, Kevin Cho, Pooja Krishnan, Nathan Bodenstab, Thomas Lin, Wenxuan Teng, Francois Beaulieu, Paul Vozila",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05517v1 Announce Type: new \nAbstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction."
      },
      {
        "id": "oai:arXiv.org:2507.05526v1",
        "title": "Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning",
        "link": "https://arxiv.org/abs/2507.05526",
        "author": "Anish Dhir, Cristiana Diaconu, Valentinian Mihai Lungu, James Requeima, Richard E. Turner, Mark van der Wilk",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05526v1 Announce Type: new \nAbstract: In scientific domains -- from biology to the social sciences -- many questions boil down to \\textit{What effect will we observe if we intervene on a particular variable?} If the causal relationships (e.g.~a causal graph) are known, it is possible to estimate the intervention distributions. In the absence of this domain knowledge, the causal structure must be discovered from the available observational data. However, observational data are often compatible with multiple causal graphs, making methods that commit to a single structure prone to overconfidence. A principled way to manage this structural uncertainty is via Bayesian inference, which averages over a posterior distribution on possible causal structures and functional mechanisms. Unfortunately, the number of causal structures grows super-exponentially with the number of nodes in the graph, making computations intractable. We propose to circumvent these challenges by using meta-learning to create an end-to-end model: the Model-Averaged Causal Estimation Transformer Neural Process (MACE-TNP). The model is trained to predict the Bayesian model-averaged interventional posterior distribution, and its end-to-end nature bypasses the need for expensive calculations. Empirically, we demonstrate that MACE-TNP outperforms strong Bayesian baselines. Our work establishes meta-learning as a flexible and scalable paradigm for approximating complex Bayesian causal inference, that can be scaled to increasingly challenging settings in the future."
      },
      {
        "id": "oai:arXiv.org:2507.05527v1",
        "title": "Mitigating Shortcut Learning with InterpoLated Learning",
        "link": "https://arxiv.org/abs/2507.05527",
        "author": "Michalis Korakakis, Andreas Vlachos, Adrian Weller",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05527v1 Announce Type: new \nAbstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts, i.e., spurious correlations between input attributes and labels that are prevalent in the majority of the training data but unrelated to the task at hand. This reliance hinders generalization on minority examples, where such correlations do not hold. Existing shortcut mitigation approaches are model-specific, difficult to tune, computationally expensive, and fail to improve learned representations. To address these issues, we propose InterpoLated Learning (InterpoLL) which interpolates the representations of majority examples to include features from intra-class minority examples with shortcut-mitigating patterns. This weakens shortcut influence, enabling models to acquire features predictive across both minority and majority examples. Experimental results on multiple natural language understanding tasks demonstrate that InterpoLL improves minority generalization over both ERM and state-of-the-art shortcut mitigation methods, without compromising accuracy on majority examples. Notably, these gains persist across encoder, encoder-decoder, and decoder-only architectures, demonstrating the method's broad applicability."
      },
      {
        "id": "oai:arXiv.org:2507.05531v1",
        "title": "Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search",
        "link": "https://arxiv.org/abs/2507.05531",
        "author": "Sanaz Kazemi Abharian, Sai Manoj Pudukotai Dinakarrao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05531v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning method for graph-structured data. A plethora of hardware accelerators has been introduced to meet the performance demands of GNNs in real-world applications. However, security challenges of hardware-based attacks have been generally overlooked. In this paper, we investigate the vulnerability of GNN models to hardware-based fault attack, wherein an attacker attempts to misclassify output by modifying trained weight parameters through fault injection in a memory device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware bit-flip fault attack, selecting a vulnerable bit in each selected weight gradually to compromise the GNN's performance by flipping a minimal number of bits. To achieve this, GBFA operates in two steps. First, a Markov model is created to predict the execution sequence of layers based on features extracted from memory access patterns, enabling the launch of the attack within a specific layer. Subsequently, GBFA identifies vulnerable bits within the selected weights using gradient ranking through an in-layer search. We evaluate the effectiveness of the proposed GBFA attack on various GNN models for node classification tasks using the Cora and PubMed datasets. Our findings show that GBFA significantly degrades prediction accuracy, and the variation in its impact across different layers highlights the importance of adopting a layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's prediction accuracy by 17% on the Cora dataset with only a single bit flip in the last layer."
      },
      {
        "id": "oai:arXiv.org:2507.05533v1",
        "title": "Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification",
        "link": "https://arxiv.org/abs/2507.05533",
        "author": "Jiawei Sun, Hongkang Li, Meng Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05533v1 Announce Type: new \nAbstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome over-smoothing, while graph sparsification reduces computational demands by selecting a sub-matrix of the graph adjacency matrix during neighborhood aggregation. Learning GCNs with graph sparsification has shown empirical success across various applications, but a theoretical understanding of the generalization guarantees remains limited, with existing analyses ignoring either graph sparsification or jumping connections. This paper presents the first learning dynamics and generalization analysis of GCNs with jumping connections using graph sparsification. Our analysis demonstrates that the generalization accuracy of the learned model closely approximates the highest achievable accuracy within a broad class of target functions dependent on the proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification maintains generalization performance when $A^*$ preserves the essential edges that support meaningful message propagation. We reveal that jumping connections lead to different sparsification requirements across layers. In a two-hidden-layer GCN, the generalization is more affected by the sparsified matrix deviations from $A^*$ of the first layer than the second layer. To the best of our knowledge, this marks the first theoretical characterization of jumping connections' role in sparsification requirements. We validate our theoretical results on benchmark datasets in deep GCNs."
      },
      {
        "id": "oai:arXiv.org:2507.05536v1",
        "title": "Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception",
        "link": "https://arxiv.org/abs/2507.05536",
        "author": "Moseli Mots'oehli, Feimei Chen, Hok Wai Chan, Itumeleng Tlali, Thulani Babeli, Kyungim Baek, Huaijin Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05536v1 Announce Type: new \nAbstract: The scarcity of autonomous vehicle datasets from developing regions, particularly across Africa's diverse urban, rural, and unpaved roads, remains a key obstacle to robust perception in low-resource settings. We present a procedural augmentation pipeline that enhances low-cost monocular dashcam footage with realistic refractive distortions and weather-induced artifacts tailored to challenging African driving scenarios. Our refractive module simulates optical effects from low-quality lenses and air turbulence, including lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free (incompressible) warps. The weather module adds homogeneous fog, heterogeneous fog, and lens flare. To establish a benchmark, we provide baseline performance using three image restoration models. To support perception research in underrepresented African contexts, without costly data collection, labeling, or simulation, we release our distortion toolkit, augmented dataset splits, and benchmark results."
      },
      {
        "id": "oai:arXiv.org:2507.05540v1",
        "title": "Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge",
        "link": "https://arxiv.org/abs/2507.05540",
        "author": "Chunhui Gu, Mohammad Sadegh Nasr, James P. Long, Kim-Anh Do, Ehsan Irajizad",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05540v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate external \"clean\" links and guide embeddings of a noisy target graph. We train two encoders--one on the full graph (target plus external edges) and another on a regularization graph excluding the target's potentially noisy links--then penalize discrepancies between their latent representations. This constraint steers the model away from overfitting spurious edges. Experiments on benchmark datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and validate it on a small protein-metabolite network, where metabolite-protein interactions reduce noise in protein co-occurrence data. Our results highlight LSC-GNN's potential to boost predictive performance and interpretability in settings with noisy relational structures."
      },
      {
        "id": "oai:arXiv.org:2507.05544v1",
        "title": "Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information",
        "link": "https://arxiv.org/abs/2507.05544",
        "author": "Jingyi Gao, Sol Lim, Seokhyun Chung",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05544v1 Announce Type: new \nAbstract: Machine learning methods are increasingly applied to ergonomic risk assessment in manual material handling, particularly for estimating carried load from gait motion data collected from wearable sensors. However, existing approaches often rely on direct mappings from loaded gait to hand load, limiting generalization and predictive accuracy. In this study, we propose an enhanced load estimation framework that incorporates auxiliary information, including baseline gait patterns during unloaded walking and carrying style. While baseline gait can be automatically captured by wearable sensors and is thus readily available at inference time, carrying style typically requires manual labeling and is often unavailable during deployment. Our model integrates deep latent variable modeling with temporal convolutional networks and bi-directional cross-attention to capture gait dynamics and fuse loaded and unloaded gait patterns. Guided by domain knowledge, the model is designed to estimate load magnitude conditioned on carrying style, while eliminating the need for carrying style labels at inference time. Experiments using real-world data collected from inertial measurement units attached to participants demonstrate substantial accuracy gains from incorporating auxiliary information and highlight the importance of explicit fusion mechanisms over naive feature concatenation."
      },
      {
        "id": "oai:arXiv.org:2507.05557v1",
        "title": "Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS",
        "link": "https://arxiv.org/abs/2507.05557",
        "author": "Alex ZH Dou, Zhongwei Wan, Dongfei Cui, Xin Wang, Jing Xiong, Haokun Lin, Chaofan Tao, Shen Yan, Mi Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05557v1 Announce Type: new \nAbstract: Test-time scaling has emerged as a promising paradigm in language modeling, leveraging additional computational resources at inference time to enhance model performance. In this work, we introduce R2-LLMs, a novel and versatile hierarchical retrieval-augmented reasoning framework designed to improve test-time scaling in large language models (LLMs) without requiring distillation from more advanced models to obtain chain-of-thought (CoT) training data. R2-LLMs enhances inference-time generalization by integrating dual-level retrieval-based in-context learning: (1) At the coarse level, our approach extracts abstract templates from complex reasoning problems and retrieves similar problem-answer pairs to facilitate high-level in-context learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs efficiently retrieves analogous intermediate solution steps from reference mathematical problem datasets, refining step-wise reasoning with the aid of a process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical reasoning-augmentation method that enhances in-context-level reasoning while seamlessly integrating with step-level tree search methods. Utilizing PRM, it refines both candidate generation and decision-making for improved reasoning accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO datasets achieve substantial relative improvement with an increase of up to 16% using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of our approach in complex reasoning tasks."
      },
      {
        "id": "oai:arXiv.org:2507.05561v1",
        "title": "Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines",
        "link": "https://arxiv.org/abs/2507.05561",
        "author": "Wilka Carvalho, Sam Hall-McMaster, Honglak Lee, Samuel J. Gershman",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05561v1 Announce Type: new \nAbstract: Humans can pursue a near-infinite variety of tasks, but typically can only pursue a small number at the same time. We hypothesize that humans leverage experience on one task to preemptively learn solutions to other tasks that were accessible but not pursued. We formalize this idea as Multitask Preplay, a novel algorithm that replays experience on one task as the starting point for \"preplay\" -- counterfactual simulation of an accessible but unpursued task. Preplay is used to learn a predictive representation that can support fast, adaptive task performance later on. We first show that, compared to traditional planning and predictive representation methods, multitask preplay better predicts how humans generalize to tasks that were accessible but not pursued in a small grid-world, even when people didn't know they would need to generalize to these tasks. We then show these predictions generalize to Craftax, a partially observable 2D Minecraft environment. Finally, we show that Multitask Preplay enables artificial agents to learn behaviors that transfer to novel Craftax worlds sharing task co-occurrence structure. These findings demonstrate that Multitask Preplay is a scalable theory of how humans counterfactually learn and generalize across multiple tasks; endowing artificial agents with the same capacity can significantly improve their performance in challenging multitask environments."
      },
      {
        "id": "oai:arXiv.org:2507.05568v1",
        "title": "ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models",
        "link": "https://arxiv.org/abs/2507.05568",
        "author": "Jiaxu Tian, Xuehui Yu, Yaoxing Wang, Pan Wang, Guangqian Guo, Shan Gao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05568v1 Announce Type: new \nAbstract: Content-aware layout aims to arrange design elements appropriately on a given canvas to convey information effectively. Recently, the trend for this task has been to leverage large language models (LLMs) to generate layouts automatically, achieving remarkable performance. However, existing LLM-based methods fail to adequately interpret spatial relationships among visual themes and design elements, leading to structural and diverse problems in layout generation. To address this issue, we introduce ReLayout, a novel method that leverages relation-CoT to generate more reasonable and aesthetically coherent layouts by fundamentally originating from design concepts. Specifically, we enhance layout annotations by introducing explicit relation definitions, such as region, salient, and margin between elements, with the goal of decomposing the layout into smaller, structured, and recursive layouts, thereby enabling the generation of more structured layouts. Furthermore, based on these defined relationships, we introduce a layout prototype rebalance sampler, which defines layout prototype features across three dimensions and quantifies distinct layout styles. This sampler addresses uniformity issues in generation that arise from data bias in the prototype distribution balance process. Extensive experimental results verify that ReLayout outperforms baselines and can generate structural and diverse layouts that are more aligned with human aesthetics and more explainable."
      },
      {
        "id": "oai:arXiv.org:2507.05575v1",
        "title": "Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions",
        "link": "https://arxiv.org/abs/2507.05575",
        "author": "Jun-Xiong Chong, Fang-Yu Hsu, Ming-Tsung Hsu, Yi-Ting Lin, Kai-Heng Chien, Chiou-Ting Hsu, Pei-Kai Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05575v1 Announce Type: new \nAbstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by extracting discriminative liveness cues from multiple modalities, such as RGB, infrared (IR), and depth images, to enhance the robustness of biometric authentication systems. However, because data from different modalities are typically captured by various camera sensors and under diverse environmental conditions, multi-modal FAS often exhibits significantly greater distribution discrepancies across training and testing domains compared to single-modal FAS. Furthermore, during the inference stage, multi-modal FAS confronts even greater challenges when one or more modalities are unavailable or inaccessible. In this paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to tackle the challenges in the multi-modal FAS task. Our motivation stems from that, within a single modality, the visual differences between live faces are typically much smaller than those of spoof faces. Additionally, feature transitions across modalities are more consistent for the live class compared to those between live and spoof classes. Upon this insight, we first propose learning consistent cross-modal feature transitions among live samples to construct a generalized feature space. Next, we introduce learning the inconsistent cross-modal feature transitions between live and spoof samples to effectively detect out-of-distribution (OOD) attacks during inference. To further address the issue of missing modalities, we propose learning complementary infrared (IR) and depth features from the RGB modality as auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet outperforms previous two-class multi-modal FAS methods across most protocols."
      },
      {
        "id": "oai:arXiv.org:2507.05578v1",
        "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation",
        "link": "https://arxiv.org/abs/2507.05578",
        "author": "Alexander Xiong, Xuandong Zhao, Aneesh Pappu, Dawn Song",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05578v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they also exhibit memorization of their training data. This phenomenon raises critical questions about model behavior, privacy risks, and the boundary between learning and memorization. Addressing these concerns, this paper synthesizes recent studies and investigates the landscape of memorization, the factors influencing it, and methods for its detection and mitigation. We explore key drivers, including training data duplication, training dynamics, and fine-tuning procedures that influence data memorization. In addition, we examine methodologies such as prefix-based extraction, membership inference, and adversarial prompting, assessing their effectiveness in detecting and measuring memorized content. Beyond technical analysis, we also explore the broader implications of memorization, including the legal and ethical implications. Finally, we discuss mitigation strategies, including data cleaning, differential privacy, and post-training unlearning, while highlighting open challenges in balancing the minimization of harmful memorization with utility. This paper provides a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions, identifying critical directions for future work."
      },
      {
        "id": "oai:arXiv.org:2507.05583v1",
        "title": "Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization",
        "link": "https://arxiv.org/abs/2507.05583",
        "author": "Yuhang Li, Shiqi Chen, Tingyu Gong, Aydogan Ozcan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05583v1 Announce Type: new \nAbstract: Optical computing holds promise for high-speed, energy-efficient information processing, with diffractive optical networks emerging as a flexible platform for implementing task-specific transformations. A challenge, however, is the effective optimization and alignment of the diffractive layers, which is hindered by the difficulty of accurately modeling physical systems with their inherent hardware imperfections, noise, and misalignments. While existing in situ optimization methods offer the advantage of direct training on the physical system without explicit system modeling, they are often limited by slow convergence and unstable performance due to inefficient use of limited measurement data. Here, we introduce a model-free reinforcement learning approach utilizing Proximal Policy Optimization (PPO) for the in situ training of diffractive optical processors. PPO efficiently reuses in situ measurement data and constrains policy updates to ensure more stable and faster convergence. We experimentally validated our method across a range of in situ learning tasks, including targeted energy focusing through a random diffuser, holographic image generation, aberration correction, and optical image classification, demonstrating in each task better convergence and performance. Our strategy operates directly on the physical system and naturally accounts for unknown real-world imperfections, eliminating the need for prior system knowledge or modeling. By enabling faster and more accurate training under realistic experimental constraints, this in situ reinforcement learning approach could offer a scalable framework for various optical and physical systems governed by complex, feedback-driven dynamics."
      },
      {
        "id": "oai:arXiv.org:2507.05584v1",
        "title": "The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction",
        "link": "https://arxiv.org/abs/2507.05584",
        "author": "Beibei Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05584v1 Announce Type: new \nAbstract: In this work we propose a unified Fourier Spectral Transformer network that integrates the strengths of classical spectral methods and attention based neural architectures. By transforming the original PDEs into spectral ordinary differential equations, we use high precision numerical solvers to generate training data and use a Transformer network to model the evolution of the spectral coefficients. We demonstrate the effectiveness of our approach on the two dimensional incompressible Navier-Stokes equations and the one dimensional Burgers' equation. The results show that our spectral Transformer can achieve highly accurate long term predictions even with limited training data, better than traditional numerical methods and machine learning methods in forecasting future flow dynamics. The proposed framework generalizes well to unseen data, bringing a promising paradigm for real time prediction and control of complex dynamical systems."
      },
      {
        "id": "oai:arXiv.org:2507.05588v1",
        "title": "Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering",
        "link": "https://arxiv.org/abs/2507.05588",
        "author": "Shuai Li, Shihan Chen, Wanru Geng, Zhaohua Xu, Xiaolu Liu, Can Dong, Zhen Tian, Changlin Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05588v1 Announce Type: new \nAbstract: In the realm of industrial quality inspection, defect detection stands as a critical component, particularly in high-precision, safety-critical sectors such as automotive components aerospace, and medical devices. Traditional methods, reliant on manual inspection or early image processing algorithms, suffer from inefficiencies, high costs, and limited robustness. This paper introduces a semi-supervised defect detection framework based on conditional diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a staged joint optimization strategy. The framework utilizes labeled data for initial training and subsequently incorporates unlabeled data through the generation of pseudo-labels. A conditional diffusion model synthesizes multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise filtering mechanism mitigates label contamination. Experimental results on the NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the labeled data required by the original supervised model, showcasing significant advantages in data efficiency. This research provides a high-precision, low-labeling-dependent solution for defect detection in industrial quality inspection scenarios. The work of this article has been open-sourced at https://github.com/cLin-c/Semisupervised-DSYM."
      },
      {
        "id": "oai:arXiv.org:2507.05594v1",
        "title": "GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field",
        "link": "https://arxiv.org/abs/2507.05594",
        "author": "Zhizhuo Pang, Zhihui Ke, Xiaobo Zhou, Tie Qiu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05594v1 Announce Type: new \nAbstract: Implicit neural representations for video have been recognized as a novel and promising form of video representation. Existing works pay more attention to improving video reconstruction quality but little attention to the decoding speed. However, the high computation of convolutional network used in existing methods leads to low decoding speed. Moreover, these convolution-based video representation methods also suffer from long training time, about 14 seconds per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame. Specifically, we propose a hybrid deformation field to model the dynamics of the video, which combines two motion patterns, namely the tri-plane motion and the polynomial motion, to deal with the coupling of camera motion and object motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing strategy to adaptively divide the video into multiple groups of pictures(GOP) based on the dynamic level of the video in order to handle large camera motion and non-rigid movements. Finally, we propose quantization-aware fine-tuning to avoid performance reduction after quantization and utilize image codecs to compress Gaussians to achieve a compact representation. Experiments on the Bunny and UVG datasets confirm that our method converges much faster than existing methods and also has 10x faster decoding speed compared to other methods. Our method has comparable performance in the video interpolation task to SOTA and attains better video compression performance than NeRV."
      },
      {
        "id": "oai:arXiv.org:2507.05595v1",
        "title": "PaddleOCR 3.0 Technical Report",
        "link": "https://arxiv.org/abs/2507.05595",
        "author": "Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, Yanjun Ma",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05595v1 Announce Type: new \nAbstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed open-source toolkit for OCR and document parsing. To address the growing demand for document understanding in the era of large language models, PaddleOCR 3.0 presents three major solutions: (1) PP-OCRv5 for multilingual text recognition, (2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for key information extraction. Compared to mainstream vision-language models (VLMs), these models with fewer than 100 million parameters achieve competitive accuracy and efficiency, rivaling billion-parameter VLMs. In addition to offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient tools for training, inference, and deployment, supports heterogeneous hardware acceleration, and enables developers to easily build intelligent document applications."
      },
      {
        "id": "oai:arXiv.org:2507.05598v1",
        "title": "Self-Review Framework for Enhancing Instruction Following Capability of LLM",
        "link": "https://arxiv.org/abs/2507.05598",
        "author": "Sihyun Park",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05598v1 Announce Type: new \nAbstract: Various techniques have been proposed to improve large language models (LLMs) adherence to formatting and instruction constraints. One of the most effective approaches involves utilizing high-quality data generated by powerful models. However, such models often fail to fully comply with complex instructions in a single generation. To address this limitation, iterative revision methods have been introduced. Nevertheless, as the number of data points and revision iterations increases, the associated monetary costs grow significantly. As a resource-efficient alternative, methods have been proposed that leverage high-performance evaluation tools to compensate for the limited self-evaluation capabilities of open-source LLMs. However, these approaches often lead to a degradation in output quality due to excessive revision. To overcome these challenges, we propose Re5, a self-evaluation and revision framework designed to enhance instruction-following performance while preserving the quality of the generated content. Re5 extracts task and constraint components from user instructions, performs structural evaluations to prevent error accumulation, and applies fine-grained constraint-specific content evaluations followed by selective revisions. This process ensures precise and quality-preserving improvements. The final high-quality outputs are used for alignment tuning, enabling long-term alignment improvements through a data-centric iterative refinement loop. Experimental results demonstrate that Re5 achieves instruction-following performance comparable to models trained on data generated by GPT-4o-mini, a high-performance model, even with a small amount of data while maintaining response quality with a 64.24%-win rate over the non-revised initial responses. These results validate Re5 as an efficient and effective solution for enhancing instruction adherence with minimal external supervision."
      },
      {
        "id": "oai:arXiv.org:2507.05601v1",
        "title": "Rethinking Layered Graphic Design Generation with a Top-Down Approach",
        "link": "https://arxiv.org/abs/2507.05601",
        "author": "Jingye Chen, Zhaowen Wang, Nanxuan Zhao, Li Zhang, Difan Liu, Jimei Yang, Qifeng Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05601v1 Announce Type: new \nAbstract: Graphic design is crucial for conveying ideas and messages. Designers usually organize their work into objects, backgrounds, and vectorized text layers to simplify editing. However, this workflow demands considerable expertise. With the rise of GenAI methods, an endless supply of high-quality graphic designs in pixel format has become more accessible, though these designs often lack editability. Despite this, non-layered designs still inspire human designers, influencing their choices in layouts and text styles, ultimately guiding the creation of layered designs. Motivated by this observation, we propose Accordion, a graphic design generation framework taking the first attempt to convert AI-generated designs into editable layered designs, meanwhile refining nonsensical AI-generated text with meaningful alternatives guided by user prompts. It is built around a vision language model (VLM) playing distinct roles in three curated stages. For each stage, we design prompts to guide the VLM in executing different tasks. Distinct from existing bottom-up methods (e.g., COLE and Open-COLE) that gradually generate elements to create layered designs, our approach works in a top-down manner by using the visually harmonious reference image as global guidance to decompose each layer. Additionally, it leverages multiple vision experts such as SAM and element removal models to facilitate the creation of graphic layers. We train our method using the in-house graphic design dataset Design39K, augmented with AI-generated design images coupled with refined ground truth created by a customized inpainting model. Experimental results and user studies by designers show that Accordion generates favorable results on the DesignIntention benchmark, including tasks such as text-to-template, adding text to background, and text de-rendering, and also excels in creating design variations."
      },
      {
        "id": "oai:arXiv.org:2507.05604v1",
        "title": "Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration",
        "link": "https://arxiv.org/abs/2507.05604",
        "author": "Yuyang Hu, Kangfu Mei, Mojtaba Sahraee-Ardakan, Ulugbek S. Kamilov, Peyman Milanfar, Mauricio Delbracio",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05604v1 Announce Type: new \nAbstract: Diffusion models show promise for image restoration, but existing methods often struggle with inconsistent fidelity and undesirable artifacts. To address this, we introduce Kernel Density Steering (KDS), a novel inference-time framework promoting robust, high-fidelity outputs through explicit local mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples, computing patch-wise kernel density estimation gradients from their collective outputs. These gradients steer patches in each particle towards shared, higher-density regions identified within the ensemble. This collective local mode-seeking mechanism, acting as \"collective wisdom\", steers samples away from spurious modes prone to artifacts, arising from independent sampling or model imperfections, and towards more robust, high-fidelity structures. This allows us to obtain better quality samples at the expense of higher compute by simultaneously sampling multiple particles. As a plug-and-play framework, KDS requires no retraining or external verifiers, seamlessly integrating with various diffusion samplers. Extensive numerical validations demonstrate KDS substantially improves both quantitative and qualitative performance on challenging real-world super-resolution and image inpainting tasks."
      },
      {
        "id": "oai:arXiv.org:2507.05617v1",
        "title": "Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching",
        "link": "https://arxiv.org/abs/2507.05617",
        "author": "Mingzhe Li, Jing Xiang, Qishen Zhang, Kaiyang Wan, Xiuying Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05617v1 Announce Type: new \nAbstract: Knowledge distillation typically involves transferring knowledge from a Large Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such as text matching, fine-tuned smaller models often yield more effective domain-specific representations, as they focus on optimizing the similarity of input pairs. To leverage both the specialized strengths of small models and the rich semantic understanding of LLMs, we introduce a flipped knowledge distillation paradigm, where LLM learns from SLM. Specifically, we address the architectural gap between decoder-only LLMs and smaller encoder-based models by reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder generates compressed representations, while the decoder maps them to the output space. During training, the encoder produces representations and their similarities, which are then aligned with the similarity scores produced by the teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach. The MCL ensures accurate similarity for both positive and negative pairs, and adaptively handles the internal differences within positive and negative samples. Our paradigm requires only a reasonably good-performing SLM, allowing the LLM to achieve improved performance. Experiments on financial and healthcare benchmarks, as well as real-world applications, confirm its effectiveness, and the model has been fully deployed in an online environment."
      },
      {
        "id": "oai:arXiv.org:2507.05619v1",
        "title": "Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study",
        "link": "https://arxiv.org/abs/2507.05619",
        "author": "Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05619v1 Announce Type: new \nAbstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat to the deployment of autonomous agents, where agents exploit flaws in reward functions to achieve high scores without fulfilling intended objectives. Despite growing awareness of this problem, systematic detection and mitigation approaches remain limited. This paper presents a large-scale empirical study of reward hacking across diverse RL environments and algorithms. We analyze 15,247 training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and 5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection algorithms for six categories of reward hacking: specification gaming, reward tampering, proxy optimization, objective misalignment, exploitation patterns, and wireheading. Our detection framework achieves 78.4% precision and 81.7% recall across environments, with computational overhead under 5%. Through controlled experiments varying reward function properties, we demonstrate that reward density and alignment with true objectives significantly impact hacking frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through three simulated application studies representing recommendation systems, competitive gaming, and robotic control scenarios. Our mitigation techniques reduce hacking frequency by up to 54.6% in controlled scenarios, though we find these trade-offs are more challenging in practice due to concept drift, false positive costs, and adversarial adaptation. All detection algorithms, datasets, and experimental protocols are publicly available to support reproducible research in RL safety."
      },
      {
        "id": "oai:arXiv.org:2507.05620v1",
        "title": "Generative Head-Mounted Camera Captures for Photorealistic Avatars",
        "link": "https://arxiv.org/abs/2507.05620",
        "author": "Shaojie Bai, Seunghyeon Seo, Yida Wang, Chenghui Li, Owen Wang, Te-Li Wang, Tianyang Ma, Jason Saragih, Shih-En Wei, Nojun Kwak, Hyung Jun Kim",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05620v1 Announce Type: new \nAbstract: Enabling photorealistic avatar animations in virtual and augmented reality (VR/AR) has been challenging because of the difficulty of obtaining ground truth state of faces. It is physically impossible to obtain synchronized images from head-mounted cameras (HMC) sensing input, which has partial observations in infrared (IR), and an array of outside-in dome cameras, which have full observations that match avatars' appearance. Prior works relying on analysis-by-synthesis methods could generate accurate ground truth, but suffer from imperfect disentanglement between expression and style in their personalized training. The reliance of extensive paired captures (HMC and dome) for the same subject makes it operationally expensive to collect large-scale datasets, which cannot be reused for different HMC viewpoints and lighting. In this work, we propose a novel generative approach, Generative HMC (GenHMC), that leverages large unpaired HMC captures, which are much easier to collect, to directly generate high-quality synthetic HMC images given any conditioning avatar state from dome captures. We show that our method is able to properly disentangle the input conditioning signal that specifies facial expression and viewpoint, from facial appearance, leading to more accurate ground truth. Furthermore, our method can generalize to unseen identities, removing the reliance on the paired captures. We demonstrate these breakthroughs by both evaluating synthetic HMC images and universal face encoders trained from these new HMC-avatar correspondences, which achieve better data efficiency and state-of-the-art accuracy."
      },
      {
        "id": "oai:arXiv.org:2507.05621v1",
        "title": "AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework",
        "link": "https://arxiv.org/abs/2507.05621",
        "author": "Suoxiang Zhang, Xiaxi Li, Hongrui Chang, Zhuoyan Hou, Guoxin Wu, Ronghua Ji",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05621v1 Announce Type: new \nAbstract: Domain-specific image generation aims to produce high-quality visual content for specialized fields while ensuring semantic accuracy and detail fidelity. However, existing methods exhibit two critical limitations: First, current approaches address prompt engineering and model adaptation separately, overlooking the inherent dependence between semantic understanding and visual representation in specialized domains. Second, these techniques inadequately incorporate domain-specific semantic constraints during content synthesis, resulting in generation outcomes that exhibit hallucinations and semantic deviations. To tackle these issues, we propose AdaptaGen, a hierarchical semantic optimization framework that integrates matrix-based prompt optimization with multi-perspective understanding, capturing comprehensive semantic relationships from both global and local perspectives. To mitigate hallucinations in specialized domains, we design a cross-modal adaptation mechanism, which, when combined with intelligent content synthesis, enables preserving core thematic elements while incorporating diverse details across images. Additionally, we introduce a two-phase caption semantic transformation during the generation phase. This approach maintains semantic coherence while enhancing visual diversity, ensuring the generated images adhere to domain-specific constraints. Experimental results confirm our approach's effectiveness, with our framework achieving superior performance across 40 categories from diverse datasets using only 16 images per category, demonstrating significant improvements in image quality, diversity, and semantic consistency."
      },
      {
        "id": "oai:arXiv.org:2507.05631v1",
        "title": "OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval",
        "link": "https://arxiv.org/abs/2507.05631",
        "author": "Zhiwei Chen, Yupeng Hu, Zixu Li, Zhiheng Fu, Xuemeng Song, Liqiang Nie",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05631v1 Announce Type: new \nAbstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is capable of expressing users' intricate retrieval requirements flexibly. It enables the user to give a multimodal query, comprising a reference image and a modification text, and subsequently retrieve the target image. Notwithstanding the considerable advances made by prevailing methodologies, CIR remains in its nascent stages due to two limitations: 1) inhomogeneity between dominant and noisy portions in visual data is ignored, leading to query feature degradation, and 2) the priority of textual data in the image modification process is overlooked, which leads to a visual focus bias. To address these two limitations, this work presents a focus mapping-based feature extractor, which consists of two modules: dominant portion segmentation and dual focus mapping. It is designed to identify significant dominant portions in images and guide the extraction of visual and textual data features, thereby reducing the impact of noise interference. Subsequently, we propose a textually guided focus revision module, which can utilize the modification requirements implied in the text to perform adaptive focus revision on the reference image, thereby enhancing the perception of the modification focus on the composed features. The aforementioned modules collectively constitute the segmentatiOn-based Focus shiFt reviSion nETwork (\\mbox{OFFSET}), and comprehensive experiments on four benchmark datasets substantiate the superiority of our proposed method. The codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/"
      },
      {
        "id": "oai:arXiv.org:2507.05633v1",
        "title": "SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression",
        "link": "https://arxiv.org/abs/2507.05633",
        "author": "Yiqiao Jin, Kartik Sharma, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05633v1 Announce Type: new \nAbstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs) with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches reduce input size but often discard fine-grained details essential for factual accuracy. We propose SARA, a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets. SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: 1) fine-grained natural-language spans that preserve critical entities and numerical values, and 2) compact, interpretable vectors that summarize high-level semantics. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma), SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53), demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG."
      },
      {
        "id": "oai:arXiv.org:2507.05636v1",
        "title": "Graph Learning",
        "link": "https://arxiv.org/abs/2507.05636",
        "author": "Feng Xia, Ciyuan Peng, Jing Ren, Falih Gozi Febrinanto, Renqiang Luo, Vidya Saikrishna, Shuo Yu, Xiangjie Kong",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05636v1 Announce Type: new \nAbstract: Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning."
      },
      {
        "id": "oai:arXiv.org:2507.05639v1",
        "title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?",
        "link": "https://arxiv.org/abs/2507.05639",
        "author": "Haoxin Wang, Xianhan Peng, Xucheng Huang, Yizhe Huang, Ming Gong, Chenghan Yang, Yang Liu, Ling Jiang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05639v1 Announce Type: new \nAbstract: In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. Upon publication, the code and data will be open-sourced to facilitate further research and development in this domain."
      },
      {
        "id": "oai:arXiv.org:2507.05644v1",
        "title": "FACT: the Features At Convergence Theorem for neural networks",
        "link": "https://arxiv.org/abs/2507.05644",
        "author": "Enric Boix-Adsera, Neil Mallinar, James B. Simon, Mikhail Belkin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05644v1 Announce Type: new \nAbstract: A central challenge in deep learning theory is to understand how neural networks learn and represent features. To this end, we prove the Features at Convergence Theorem (FACT), which gives a self-consistency equation that neural network weights satisfy at convergence when trained with nonzero weight decay. For each weight matrix $W$, this equation relates the \"feature matrix\" $W^\\top W$ to the set of input vectors passed into the matrix during forward propagation and the loss gradients passed through it during backpropagation. We validate this relation empirically, showing that neural features indeed satisfy the FACT at convergence. Furthermore, by modifying the \"Recursive Feature Machines\" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on tabular data and captures various feature learning behaviors that occur in neural network training, including grokking in modular arithmetic and phase transitions in learning sparse parities."
      },
      {
        "id": "oai:arXiv.org:2507.05666v1",
        "title": "Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain",
        "link": "https://arxiv.org/abs/2507.05666",
        "author": "Junfei Shi, Yu Cheng, Haiyan Jin, Junhuai Li, Zhaolin Xiao, Maoguo Gong, Weisi Lin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05666v1 Announce Type: new \nAbstract: Diffusion models have demonstrated exceptional performance across various domains due to their ability to model and generate complicated data distributions. However, when applied to PolSAR data, traditional real-valued diffusion models face challenges in capturing complex-valued phase information.Moreover, these models often struggle to preserve fine structural details. To address these limitations, we leverage the Contourlet transform, which provides rich multiscale and multidirectional representations well-suited for PolSAR imagery. We propose a structural knowledge-guided complex diffusion model for PolSAR image classification in the Contourlet domain. Specifically, the complex Contourlet transform is first applied to decompose the data into low- and high-frequency subbands, enabling the extraction of statistical and boundary features. A knowledge-guided complex diffusion network is then designed to model the statistical properties of the low-frequency components. During the process, structural information from high-frequency coefficients is utilized to guide the diffusion process, improving edge preservation. Furthermore, multiscale and multidirectional high-frequency features are jointly learned to further boost classification accuracy. Experimental results on three real-world PolSAR datasets demonstrate that our approach surpasses state-of-the-art methods, particularly in preserving edge details and maintaining region homogeneity in complex terrain."
      },
      {
        "id": "oai:arXiv.org:2507.05668v1",
        "title": "Dynamic Rank Adaptation for Vision-Language Models",
        "link": "https://arxiv.org/abs/2507.05668",
        "author": "Jiahui Wang, Qin Xu, Bo Jiang, Bin Luo",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05668v1 Announce Type: new \nAbstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate impressive generalization ability. Existing prompt-based and adapter-based works have made significant progress in fine-tuning VLMs but still face the challenges of maintaining strong generalization abilities, particularly towards unseen new classes. This limitation partly arises from these methods treating all tokens of the image and text encoder equally, which can lead to overfitting on less informative features (e.g., background noise, template words) and degrade the general representations that are crucial for novel concept recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a novel adapter variant method, designed specifically to enhance new class generalization. DRA dynamically allocates adaptation ranks based on the importance of features during training to preserve general knowledge. DRA first employs token importance grouping, using sequence attention to evaluate and group tokens by their importance. Then, we adopt rank adaptation according to the importance of each token group dynamically by assigning higher feature ranks to the more important tokens. Also, we design a new channel response mechanism to prioritize the preservation and adaptation of feature channels identified as the most informative for each instance. In addition, a L1 regularization term is introduced to stabilize the training. Extensive experiments demonstrate the effectiveness and superiority of our proposed DRA over existing works, especially on enhancing the performance of new classes on various benchmarks, including base-new classes, cross-datasets evaluation and domain generalization. The source code will be published after the paper is received."
      },
      {
        "id": "oai:arXiv.org:2507.05670v1",
        "title": "Modeling and Reversing Brain Lesions Using Diffusion Models",
        "link": "https://arxiv.org/abs/2507.05670",
        "author": "Omar Zamzam, Haleh Akrami, Anand Joshi, Richard Leahy",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05670v1 Announce Type: new \nAbstract: Brain lesions are abnormalities or injuries in brain tissue that are often detectable using magnetic resonance imaging (MRI), which reveals structural changes in the affected areas. This broad definition of brain lesions includes areas of the brain that are irreversibly damaged, as well as areas of brain tissue that are deformed as a result of lesion growth or swelling. Despite the importance of differentiating between damaged and deformed tissue, existing lesion segmentation methods overlook this distinction, labeling both of them as a single anomaly. In this work, we introduce a diffusion model-based framework for analyzing and reversing the brain lesion process. Our pipeline first segments abnormal regions in the brain, then estimates and reverses tissue deformations by restoring displaced tissue to its original position, isolating the core lesion area representing the initial damage. Finally, we inpaint the core lesion area to arrive at an estimation of the pre-lesion healthy brain. This proposed framework reverses a forward lesion growth process model that is well-established in biomechanical studies that model brain lesions. Our results demonstrate improved accuracy in lesion segmentation, characterization, and brain labeling compared to traditional methods, offering a robust tool for clinical and research applications in brain lesion analysis. Since pre-lesion healthy versions of abnormal brains are not available in any public dataset for validation of the reverse process, we simulate a forward model to synthesize multiple lesioned brain images."
      },
      {
        "id": "oai:arXiv.org:2507.05671v1",
        "title": "Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach",
        "link": "https://arxiv.org/abs/2507.05671",
        "author": "Netta Palez, L\\'eonie Stra{\\ss}, Sebastian Meller, Holger Volk, Anna Zamansky, Itzik Klein",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05671v1 Announce Type: new \nAbstract: Canine gait analysis using wearable inertial sensors is gaining attention in veterinary clinical settings, as it provides valuable insights into a range of mobility impairments. Neurological and orthopedic conditions cannot always be easily distinguished even by experienced clinicians. The current study explored and developed a deep learning approach using inertial sensor readings to assess whether neurological and orthopedic gait could facilitate gait analysis. Our investigation focused on optimizing both performance and generalizability in distinguishing between these gait abnormalities. Variations in sensor configurations, assessment protocols, and enhancements to deep learning model architectures were further suggested. Using a dataset of 29 dogs, our proposed approach achieved 96% accuracy in the multiclass classification task (healthy/orthopedic/neurological) and 82% accuracy in the binary classification task (healthy/non-healthy) when generalizing to unseen dogs. Our results demonstrate the potential of inertial-based deep learning models to serve as a practical and objective diagnostic and clinical aid to differentiate gait assessment in orthopedic and neurological conditions."
      },
      {
        "id": "oai:arXiv.org:2507.05673v1",
        "title": "R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding",
        "link": "https://arxiv.org/abs/2507.05673",
        "author": "Joonhyung Park, Peng Tang, Sagnik Das, Srikar Appalaraju, Kunwar Yashraj Singh, R. Manmatha, Shabnam Ghadar",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05673v1 Announce Type: new \nAbstract: Visual agent models for automating human activities on Graphical User Interfaces (GUIs) have emerged as a promising research direction, driven by advances in large Vision Language Models (VLMs). A critical challenge in GUI automation is the precise grounding of interface elements across diverse platforms. Existing vision-only GUI agents directly ground elements from large and cluttered screenshots, requiring them to process substantial irrelevant information that compromises their accuracy. In addition, these approaches typically employ basic cross-entropy loss for learning grounding objectives, which fails to effectively capture grounding quality compared to established object detection metrics like Intersection-over-Union (IoU). To address these issues, we introduce R-VLM, a novel GUI grounding approach that leverages zoomed-in region proposals for precise element localization. We also propose an IoU-aware objective function that facilitates model convergence toward high IoU predictions. Our approach bridges the gap between VLMs and conventional object detection techniques, improving the state-of-the-art grounding accuracy by 13% across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks."
      },
      {
        "id": "oai:arXiv.org:2507.05675v1",
        "title": "MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos",
        "link": "https://arxiv.org/abs/2507.05675",
        "author": "Rongsheng Wang, Junying Chen, Ke Ji, Zhenyang Cai, Shunian Chen, Yunjin Yang, Benyou Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05675v1 Announce Type: new \nAbstract: Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at https://github.com/FreedomIntelligence/MedGen"
      },
      {
        "id": "oai:arXiv.org:2507.05677v1",
        "title": "Integrated Structural Prompt Learning for Vision-Language Models",
        "link": "https://arxiv.org/abs/2507.05677",
        "author": "Jiahui Wang, Qin Xu, Bo Jiang, Bin Luo",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05677v1 Announce Type: new \nAbstract: Prompt learning methods have significantly extended the transferability of pre-trained Vision-Language Models (VLMs) like CLIP for various downstream tasks. These methods adopt handcraft templates or learnable vectors to provide text or image instructions in fine-tuning VLMs. However, most existing works ignore the structural relationships between learnable prompts and tokens within and between modalities. Moreover, balancing the performance of base and new classes remains a significant challenge. In this paper, we propose an Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of information representations between the text and image branches. ISP introduces self-structural and cross-structural prompt modules to model the structural relationships between learnable prompts and frozen tokens within and across modalities. This enables efficient information transfer while preserving feature stability. Additionally, we propose a sample probing module that dynamically adjusts loss coefficients based on sample difficulty, preventing the mode from overfitting to simple samples and improving generalization ability to new classes. Extensive experiments on three widely used settings: base-to-new generalization, cross-dataset evaluation, and domain generalization demonstrate that the proposed ISP achieves competitive performance against state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2507.05678v1",
        "title": "LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion",
        "link": "https://arxiv.org/abs/2507.05678",
        "author": "Yisu Zhang, Chenjie Cao, Chaohui Yu, Jianke Zhu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05678v1 Announce Type: new \nAbstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in synthesizing realistic videos by learning from large-scale data. Although vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal movement to driven VDMs with constrained data, achieving precise control over both camera trajectories and object motion remains challenging due to the unstable fusion and non-linear scalability. To address these issues, we propose LiON-LoRA, a novel framework that rethinks LoRA fusion through three core principles: Linear scalability, Orthogonality, and Norm consistency. First, we analyze the orthogonality of LoRA features in shallow VDM layers, enabling decoupled low-level controllability. Second, norm consistency is enforced across layers to stabilize fusion during complex camera motion combinations. Third, a controllable token is integrated into the diffusion transformer (DiT) to linearly adjust motion amplitudes for both cameras and objects with a modified self-attention mechanism to ensure decoupled control. Additionally, we extend LiON-LoRA to temporal generation by leveraging static-camera videos, unifying spatial and temporal controllability. Experiments demonstrate that LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy and motion strength adjustment, achieving superior generalization with minimal training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/"
      },
      {
        "id": "oai:arXiv.org:2507.05685v1",
        "title": "Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach",
        "link": "https://arxiv.org/abs/2507.05685",
        "author": "Xiaobing Chen, Boyang Zhang, Xiangwei Zhou, Mingxuan Sun, Shuai Zhang, Songyang Zhang, Geoffrey Ye Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05685v1 Announce Type: new \nAbstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE) presents a compelling pathway for training more powerful, large-scale artificial intelligence models (LAMs) on decentralized data while preserving privacy. However, efficient federated training of these complex MoE-structured LAMs is hindered by significant system-level challenges, particularly in managing the interplay between heterogeneous client resources and the sophisticated coordination required for numerous specialized experts. This article highlights a critical, yet underexplored concept: the absence of robust quantitative strategies for dynamic client-expert alignment that holistically considers varying client capacities and the imperative for system-wise load balancing. Specifically, we propose a conceptual system design for intelligent client-expert alignment that incorporates dynamic fitness scoring, global expert load monitoring, and client capacity profiling. By tackling these systemic issues, we can unlock more scalable, efficient, and robust training mechanisms {with fewer communication rounds for convergence}, paving the way for the widespread deployment of large-scale federated MoE-structured LAMs in edge computing with ultra-high communication efficiency."
      },
      {
        "id": "oai:arXiv.org:2507.05686v1",
        "title": "Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs",
        "link": "https://arxiv.org/abs/2507.05686",
        "author": "SeungWon Ji, Jungyup Lee, Jemin Kim, Sang Park, SeungJae Lee",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05686v1 Announce Type: new \nAbstract: Multilingual large language models (LLMs) often exhibit language confusion, a tendency to generate responses in a dominant language irrespective of the prompt's language. To address this, we propose Smoothie-Qwen, a lightweight, post-hoc method that mitigates language bias without retraining. This technique selectively adjusts token-level output probabilities to effectively suppress undesired language generation. Applied to the Qwen model, our method reduces unintended Chinese output by over 95% while preserving task accuracy on multilingual benchmarks. This work provides a practical and efficient solution for enhancing the language controllability of LLMs, making them more reliable for global applications."
      },
      {
        "id": "oai:arXiv.org:2507.05687v1",
        "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs",
        "link": "https://arxiv.org/abs/2507.05687",
        "author": "Shangzhan Li, Zefan Wang, Ye He, Yuxuan Li, Qi Shi, Jianling Li, Yonggang Hu, Wanxiang Che, Xu Han, Zhiyuan Liu, Maosong Sun",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05687v1 Announce Type: new \nAbstract: Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AutoTriton, the first model dedicated to Triton programming powered by reinforcement learning (RL). AutoTriton performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using a high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AutoTriton, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at https://github.com/AI9Stars/AutoTriton."
      },
      {
        "id": "oai:arXiv.org:2507.05698v1",
        "title": "Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting",
        "link": "https://arxiv.org/abs/2507.05698",
        "author": "Mohsi Jawaid, Marcus M\\\"artens, Tat-Jun Chin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05698v1 Announce Type: new \nAbstract: Spacecraft pose estimation is crucial for autonomous in-space operations, such as rendezvous, docking and on-orbit servicing. Vision-based pose estimation methods, which typically employ RGB imaging sensors, is a compelling solution for spacecraft pose estimation, but are challenged by harsh lighting conditions, which produce imaging artifacts such as glare, over-exposure, blooming and lens flare. Due to their much higher dynamic range, neuromorphic or event sensors are more resilient to extreme lighting conditions. However, event sensors generally have lower spatial resolution and suffer from reduced signal-to-noise ratio during periods of low relative motion. This work addresses these individual sensor limitations by introducing a sensor fusion approach combining RGB and event sensors. A beam-splitter prism was employed to achieve precise optical and temporal alignment. Then, a RANSAC-based technique was developed to fuse the information from the RGB and event channels to achieve pose estimation that leveraged the strengths of the two modalities. The pipeline was complemented by dropout uncertainty estimation to detect extreme conditions that affect either channel. To benchmark the performance of the proposed event-RGB fusion method, we collected a comprehensive real dataset of RGB and event data for satellite pose estimation in a laboratory setting under a variety of challenging illumination conditions. Encouraging results on the dataset demonstrate the efficacy of our event-RGB fusion approach and further supports the usage of event sensors for spacecraft pose estimation. To support community research on this topic, our dataset will be released publicly."
      },
      {
        "id": "oai:arXiv.org:2507.05707v1",
        "title": "Agentic-R1: Distilled Dual-Strategy Reasoning",
        "link": "https://arxiv.org/abs/2507.05707",
        "author": "Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05707v1 Announce Type: new \nAbstract: Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill"
      },
      {
        "id": "oai:arXiv.org:2507.05713v1",
        "title": "DRAGON: Dynamic RAG Benchmark On News",
        "link": "https://arxiv.org/abs/2507.05713",
        "author": "Fedor Chernogorskii, Sergei Averkiev, Liliya Kudraleeva, Zaven Martirosian, Maria Tikhonova, Valentin Malykh, Alena Fenogenova",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05713v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for improving the factuality of large language models (LLMs) by incorporating external knowledge at inference time. Although there exist multiple RAG benchmarks for English, evaluation resources for other languages, including Russian, remain scarce and static, failing to capture the dynamic nature of real-world deployments.\n  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first dynamic benchmark for evaluating RAG systems in Russian on a changing news corpora. DRAGON is built upon a regularly updated corpus of Russian news and public documents and supports comprehensive evaluation of both the retriever and generator components. Question generation is performed automatically with the use of Knowledge Graph constructed from the corpus and enables the extraction of four core question types aligned with distinct subgraph patterns. We release a complete evaluation framework comprising the pipeline for automatic question generation, evaluation scripts, which are potentially reusable for other languages and multilingual settings, and benchmark data. We also launch a public leaderboard to encourage community participation and comparison."
      },
      {
        "id": "oai:arXiv.org:2507.05714v1",
        "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2507.05714",
        "author": "YiHan Jiao, ZheHao Tan, Dan Yang, DuoLin Sun, Jie Feng, Jian Wang, Peng Wei",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05714v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \\textit{lack a granular focus on RAG task} or \\textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA."
      },
      {
        "id": "oai:arXiv.org:2507.05720v1",
        "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment",
        "link": "https://arxiv.org/abs/2507.05720",
        "author": "Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, Dong Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05720v1 Announce Type: new \nAbstract: Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach."
      },
      {
        "id": "oai:arXiv.org:2507.05722v1",
        "title": "Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2507.05722",
        "author": "Hongbao Li, Ziye Jia, Sijie He, Kun Guo, Qihui Wu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05722v1 Announce Type: new \nAbstract: With the emergence of compute-intensive and delay-sensitive applications in vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising complement for vehicular edge computing due to the high mobility and flexible deployment. However, the existing UAV-assisted offloading strategies are insufficient in coordinating heterogeneous computing resources and adapting to dynamic network conditions. Hence, this paper proposes a dual-layer UAV-assisted edge computing architecture based on partial offloading, composed of the relay capability of high-altitude UAVs and the computing support of low-altitude UAVs. The proposed architecture enables efficient integration and coordination of heterogeneous resources. A joint optimization problem is formulated to minimize the system delay and energy consumption while ensuring the task completion rate. To solve the high-dimensional decision problem, we reformulate the problem as a Markov decision process and propose a hierarchical offloading scheme based on the soft actor-critic algorithm. The method decouples global and local decisions, where the global decisions integrate offloading ratios and trajectory planning into continuous actions, while the local scheduling is handled via designing a priority-based mechanism. Simulations are conducted and demonstrate that the proposed approach outperforms several baselines in task completion rate, system efficiency, and convergence speed, showing strong robustness and applicability in dynamic vehicular environments."
      },
      {
        "id": "oai:arXiv.org:2507.05724v1",
        "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition",
        "link": "https://arxiv.org/abs/2507.05724",
        "author": "Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05724v1 Announce Type: new \nAbstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model \\emph{Omni-router Transformer}. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data."
      },
      {
        "id": "oai:arXiv.org:2507.05730v1",
        "title": "Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study",
        "link": "https://arxiv.org/abs/2507.05730",
        "author": "Aayushma Pant, Arbind Agrahari Baniya, Tsz-Kwan Lee, Sunil Aryal",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05730v1 Announce Type: new \nAbstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of contiguous spectral bands, enabling detailed material and surface analysis. Hyperspectral anomaly detection (HAD) refers to the technique of identifying and locating anomalous targets in such data without prior information about a hyperspectral scene or target spectrum. This technology has seen rapid advancements in recent years, with applications in agriculture, defence, military surveillance, and environmental monitoring. Despite this significant progress, existing HAD methods continue to face challenges such as high computational complexity, sensitivity to noise, and limited generalisation across diverse datasets. This study presents a comprehensive comparison of various HAD techniques, categorising them into statistical models, representation-based methods, classical machine learning approaches, and deep learning models. We evaluated these methods across 17 benchmarking datasets using different performance metrics, such as ROC, AUC, and separability map to analyse detection accuracy, computational efficiency, their strengths, limitations, and directions for future research.The research shows that deep learning models achieved the highest detection accuracy, while statistical models demonstrated exceptional speed across all datasets. This study aims to provide valuable insights for researchers and practitioners working to advance the field of hyperspectral anomaly detection methods."
      },
      {
        "id": "oai:arXiv.org:2507.05740v1",
        "title": "GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge",
        "link": "https://arxiv.org/abs/2507.05740",
        "author": "Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Moritz M\\\"uller, Simon Razniewski",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05740v1 Announce Type: new \nAbstract: Language models are powerful tools, yet their factual knowledge is still poorly understood, and inaccessible to ad-hoc browsing and scalable statistical analysis. This demonstration introduces GPTKB v1.5, a densely interlinked 100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu et al., ACL 2025). The demonstration experience focuses on three use cases: (1) link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM knowledge querying, (3) comparative exploration of the strengths and weaknesses of LLM knowledge. Massive-recursive LLM knowledge materialization is a groundbreaking opportunity both for the research area of systematic analysis of LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator is accessible at https://gptkb.org."
      },
      {
        "id": "oai:arXiv.org:2507.05750v1",
        "title": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities",
        "link": "https://arxiv.org/abs/2507.05750",
        "author": "Jing Yang Lee, Hamed Bonab, Nasser Zalmout, Ming Zeng, Sanket Lokegaonkar, Colin Lockard, Binxuan Huang, Ritesh Sarkhel, Haodong Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05750v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly employed in multi-turn conversational tasks, yet their pre-training data predominantly consists of continuous prose, creating a potential mismatch between required capabilities and training paradigms. We introduce a novel approach to address this discrepancy by synthesizing conversational data from existing text corpora. We present a pipeline that transforms a cluster of multiple related documents into an extended multi-turn, multi-topic information-seeking dialogue. Applying our pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training dialogue corpus consisting of over 730k long conversations. We hypothesize that exposure to such synthesized conversational structures during pre-training can enhance the fundamental multi-turn capabilities of LLMs, such as context memory and understanding. Empirically, we show that incorporating DocTalk during pre-training results in up to 40% gain in context memory and understanding, without compromising base performance. DocTalk is available at https://huggingface.co/datasets/AmazonScience/DocTalk."
      },
      {
        "id": "oai:arXiv.org:2507.05751v1",
        "title": "SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations",
        "link": "https://arxiv.org/abs/2507.05751",
        "author": "Yegyu Han, Taegyoon Yoon, Dayeon Woo, Sojeong Kim, Hyung-Sin Kim",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05751v1 Announce Type: new \nAbstract: Recent advances on 6D object-pose estimation has achieved high performance on representative benchmarks such as LM-O, YCB-V, and T-Less. However, these datasets were captured under fixed illumination and camera settings, leaving the impact of real-world variations in illumination, exposure, gain or depth-sensor mode - and the potential of test-time sensor control to mitigate such variations - largely unexplored. To bridge this gap, we introduce SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures, 9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels. For three common household objects (spray, pringles, and tincase), we acquire 101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting permutations per object pose. Experiments with state-of-the-art models on our dataset show that applying sensor control during test-time induces greater performance improvement over digital data augmentation, achieving performance comparable to or better than costly increases in real-world training data quantity and diversity. Adapting either RGB or depth sensors individually is effective, while jointly adapting multimodal RGB-D configurations yields even greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from data-centered to sensor-aware robustness, laying a foundation for adaptive, self-tuning perception systems capable of operating robustly in uncertain real-world environments. Our dataset is available at: huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at: github.com/yegyu-han/SenseShift6D"
      },
      {
        "id": "oai:arXiv.org:2507.05753v1",
        "title": "Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism",
        "link": "https://arxiv.org/abs/2507.05753",
        "author": "Deifilia Kieckhefen, Markus G\\\"otz, Lars H. Heyen, Achim Streit, Charlotte Debus",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05753v1 Announce Type: new \nAbstract: AI-based methods have revolutionized atmospheric forecasting, with recent successes in medium-range forecasting spurring the development of climate foundation models. Accurate modeling of complex atmospheric dynamics at high spatial resolutions and longer lead times requires large neural networks and gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the bottlenecks for model training. We introduce WeatherMixer, a multi-layer-perceptron-based architecture whose workload scales linearly with input size, allowing the model to learn global weather phenomena at accuracies similar to numerical weather prediction. To cope with the computational demand, we propose Jigsaw, a novel model parallelization scheme that employs both domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds state-of-the-art performance in strong scaling in compute-communication-limited systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems. We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs, 23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency versus 51% without model parallelism."
      },
      {
        "id": "oai:arXiv.org:2507.05757v1",
        "title": "Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy",
        "link": "https://arxiv.org/abs/2507.05757",
        "author": "Radoslaw Roszczyk, Artur Krupa, Izabella Antoniuk",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05757v1 Announce Type: new \nAbstract: The acquisition of accurately coloured, balanced images in an optical microscope can be a challenge even for experienced microscope operators. This article presents an entirely automatic mechanism for balancing the white level that allows the correction of the microscopic colour images adequately. The results of the algorithm have been confirmed experimentally on a set of two hundred microscopic images. The images contained scans of three microscopic specimens commonly used in pathomorphology. Also, the results achieved were compared with other commonly used white balance algorithms in digital photography. The algorithm applied in this work is more effective than the classical algorithms used in colour photography for microscopic images stained with hematoxylin-phloxine-saffron and for immunohistochemical staining images."
      },
      {
        "id": "oai:arXiv.org:2507.05763v1",
        "title": "DreamArt: Generating Interactable Articulated Objects from a Single Image",
        "link": "https://arxiv.org/abs/2507.05763",
        "author": "Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan, Gang Zeng, Yixin Chen, Siyuan Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05763v1 Announce Type: new \nAbstract: Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/."
      },
      {
        "id": "oai:arXiv.org:2507.05783v1",
        "title": "From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification",
        "link": "https://arxiv.org/abs/2507.05783",
        "author": "Comte Valentin, Gemma Piella, Mario Ceresa, Miguel A. Gonzalez Ballester",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05783v1 Announce Type: new \nAbstract: Cardiac diseases are among the leading causes of morbidity and mortality worldwide, which requires accurate and timely diagnostic strategies. In this study, we introduce an innovative approach that combines deep learning image registration with physics-informed regularization to predict the biomechanical properties of moving cardiac tissues and extract features for disease classification. We utilize the energy strain formulation of Neo-Hookean material to model cardiac tissue deformations, optimizing the deformation field while ensuring its physical and biomechanical coherence. This explainable approach not only improves image registration accuracy, but also provides insights into the underlying biomechanical processes of the cardiac tissues. Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the local strains within the moving heart and extract a detailed set of features used for cardiovascular disease classification. We evaluated five classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support Vector Classifier, Random Forest, and Nearest Neighbour, and identified the most relevant features using a feature selection algorithm. The best performing classifier obtained a classification accuracy of 98% in the training set and 100% in the test set of the ACDC dataset. By integrating explainable artificial intelligence, this method empowers clinicians with a transparent understanding of the model's predictions based on cardiac mechanics, while also significantly improving the accuracy and reliability of cardiac disease diagnosis, paving the way for more personalized and effective patient care."
      },
      {
        "id": "oai:arXiv.org:2507.05788v1",
        "title": "Flippi: End To End GenAI Assistant for E-Commerce",
        "link": "https://arxiv.org/abs/2507.05788",
        "author": "Anand A. Rajasekar, Praveen Tangarajan, Anjali Nainani, Amogh Batwal, Vinay Rao Dandin, Anusua Trivedi, Ozan Ersoy",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05788v1 Announce Type: new \nAbstract: The emergence of conversational assistants has fundamentally reshaped user interactions with digital platforms. This paper introduces Flippi-a cutting-edge, end-to-end conversational assistant powered by large language models (LLMs) and tailored for the e-commerce sector. Flippi addresses the challenges posed by the vast and often overwhelming product landscape, enabling customers to discover products more efficiently through natural language dialogue. By accommodating both objective and subjective user requirements, Flippi delivers a personalized shopping experience that surpasses traditional search methods. This paper details how Flippi interprets customer queries to provide precise product information, leveraging advanced NLP techniques such as Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG), Named Entity Recognition (NER), and Context Reduction. Flippi's unique capability to identify and present the most attractive offers on an e-commerce site is also explored, demonstrating how it empowers users to make cost-effective decisions. Additionally, the paper discusses Flippi's comparative analysis features, which help users make informed choices by contrasting product features, prices, and other relevant attributes. The system's robust architecture is outlined, emphasizing its adaptability for integration across various e-commerce platforms and the technological choices underpinning its performance and accuracy. Finally, a comprehensive evaluation framework is presented, covering performance metrics, user satisfaction, and the impact on customer engagement and conversion rates. By bridging the convenience of online shopping with the personalized assistance traditionally found in physical stores, Flippi sets a new standard for customer satisfaction and engagement in the digital marketplace."
      },
      {
        "id": "oai:arXiv.org:2507.05790v1",
        "title": "TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model",
        "link": "https://arxiv.org/abs/2507.05790",
        "author": "Yujie Hu, Xuanyu Zhang, Weiqi Li, Jian Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05790v1 Announce Type: new \nAbstract: Virtual try-on has made significant progress in recent years. This paper addresses how to achieve multifunctional virtual try-on guided solely by text instructions, including full outfit change and local editing. Previous methods primarily relied on end-to-end networks to perform single try-on tasks, lacking versatility and flexibility. We propose TalkFashion, an intelligent try-on assistant that leverages the powerful comprehension capabilities of large language models to analyze user instructions and determine which task to execute, thereby activating different processing pipelines accordingly. Additionally, we introduce an instruction-based local repainting model that eliminates the need for users to manually provide masks. With the help of multi-modal models, this approach achieves fully automated local editings, enhancing the flexibility of editing tasks. The experimental results demonstrate better semantic consistency and visual quality compared to the current methods."
      },
      {
        "id": "oai:arXiv.org:2507.05798v1",
        "title": "SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning",
        "link": "https://arxiv.org/abs/2507.05798",
        "author": "Xin Hu, Ke Qin, Guiduo Duan, Ming Li, Yuan-Fang Li, Tao He",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05798v1 Announce Type: new \nAbstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with relation understanding to capture pixel-level structural relationships in complex scenes. Although recent approaches leveraging pre-trained vision-language models (VLMs) have significantly improved performance in the open-vocabulary setting, they commonly ignore the inherent limitations of VLMs in spatial relation reasoning, such as difficulty in distinguishing object relative positions, which results in suboptimal relation prediction. Motivated by the denoising diffusion model's inversion process in preserving the spatial structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork) framework -- a novel approach for open-vocabulary PSG. SPADE consists of two key steps: (1) inversion-guided calibration for the UNet adaptation, and (2) spatial-aware context reasoning. In the first step, we calibrate a general pre-trained teacher diffusion model into a PSG-specific denoising network with cross-attention maps derived during inversion through a lightweight LoRA-based fine-tuning strategy. In the second step, we develop a spatial-aware relation graph transformer that captures both local and long-range contextual information, facilitating the generation of high-quality relation queries. Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate that SPADE outperforms state-of-the-art methods in both closed- and open-set scenarios, particularly for spatial relationship prediction."
      },
      {
        "id": "oai:arXiv.org:2507.05799v1",
        "title": "Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports",
        "link": "https://arxiv.org/abs/2507.05799",
        "author": "Amane Watahiki, Tomoki Doi, Taiga Shinozaki, Satoshi Nishida, Takuya Niikawa, Katsunori Miyahara, Hitomi Yanaka",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05799v1 Announce Type: new \nAbstract: One of the main objectives in developing large vision-language models (LVLMs) is to engineer systems that can assist humans with multimodal tasks, including interpreting descriptions of perceptual experiences. A central phenomenon in this context is amodal completion, in which people perceive objects even when parts of those objects are hidden. Although numerous studies have assessed whether computer-vision algorithms can detect or reconstruct occluded regions, the inferential abilities of LVLMs on texts related to amodal completion remain unexplored. To address this gap, we constructed a benchmark grounded in Basic Formal Ontology to achieve a systematic classification of amodal completion. Our results indicate that while many LVLMs achieve human-comparable performance overall, their accuracy diverges for certain types of objects being completed. Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet exhibit lower accuracy on original images compared to blank stimuli lacking visual content. Intriguingly, this disparity emerges only under Japanese prompting, suggesting a deficiency in Japanese-specific linguistic competence among these models."
      },
      {
        "id": "oai:arXiv.org:2507.05805v1",
        "title": "DREAM: Document Reconstruction via End-to-end Autoregressive Model",
        "link": "https://arxiv.org/abs/2507.05805",
        "author": "Xin Li, Mingming Gong, Yunfei Wu, Jianxin Dai, Antai Guo, Xinghua Jiang, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05805v1 Announce Type: new \nAbstract: Document reconstruction constitutes a significant facet of document analysis and recognition, a field that has been progressively accruing interest within the scholarly community. A multitude of these researchers employ an array of document understanding models to generate predictions on distinct subtasks, subsequently integrating their results into a holistic document reconstruction format via heuristic principles. Nevertheless, these multi-stage methodologies are hindered by the phenomenon of error propagation, resulting in suboptimal performance. Furthermore, contemporary studies utilize generative models to extract the logical sequence of plain text, tables and mathematical expressions in an end-to-end process. However, this approach is deficient in preserving the information related to element layouts, which are vital for document reconstruction. To surmount these aforementioned limitations, we in this paper present an innovative autoregressive model specifically designed for document reconstruction, referred to as Document Reconstruction via End-to-end Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence of document reconstruction in a comprehensive, end-to-end process, encapsulating a broader spectrum of document element information. In addition, we establish a standardized definition of the document reconstruction task, and introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for assessing the performance of the task. Empirical results substantiate that our methodology attains unparalleled performance in the realm of document reconstruction. Furthermore, the results on a variety of subtasks, encompassing document layout analysis, text recognition, table structure recognition, formula recognition and reading order detection, indicate that our model is competitive and compatible with various tasks."
      },
      {
        "id": "oai:arXiv.org:2507.05806v1",
        "title": "Predicting Graph Structure via Adapted Flux Balance Analysis",
        "link": "https://arxiv.org/abs/2507.05806",
        "author": "Sevvandi Kandanaarachchi, Ziqi Xu, Stefan Westerlund, Conrad Sanderson",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05806v1 Announce Type: new \nAbstract: Many dynamic processes such as telecommunication and transport networks can be described through discrete time series of graphs. Modelling the dynamics of such time series enables prediction of graph structure at future time steps, which can be used in applications such as detection of anomalies. Existing approaches for graph prediction have limitations such as assuming that the vertices do not to change between consecutive graphs. To address this, we propose to exploit time series prediction methods in combination with an adapted form of flux balance analysis (FBA), a linear programming method originating from biochemistry. FBA is adapted to incorporate various constraints applicable to the scenario of growing graphs. Empirical evaluations on synthetic datasets (constructed via Preferential Attachment model) and real datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the proposed approach."
      },
      {
        "id": "oai:arXiv.org:2507.05807v1",
        "title": "Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters",
        "link": "https://arxiv.org/abs/2507.05807",
        "author": "Marco Roschkowski",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05807v1 Announce Type: new \nAbstract: In this paper, we tackle two fundamental problems in few-shot domain adaptation of foundation models. First, hyperparameter tuning is often impractical due to the lack of large validation datasets. Second, model robustness under distribution shifts where test time data deviates slightly from training distributions, remains a concern. We show that by training multiple independent adapters and averaging their outputs, the new model has a higher performance and is more robust to distribution shifts compared to any individual adapter. This improvement holds even when the adapters are trained with diverse hyperparameters sampled from a wide range, resulting in varied individual performance. Consequently, our method addresses both of the problems described above. The ensemble is also significantly less sensitive to the residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble can be reparameterized to a single adapter again using a principled concatenation of the parameters, we refer to our method as Soup-Adapter. This is also the first study to explore CLIP adapter-style techniques for DINOv2 and to directly compare them with CLIP in this setting."
      },
      {
        "id": "oai:arXiv.org:2507.05810v1",
        "title": "Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs",
        "link": "https://arxiv.org/abs/2507.05810",
        "author": "Sofiia Chorna, Kateryna Tarelkina, Elo\\\"ise Berthier, Gianni Franchi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05810v1 Announce Type: new \nAbstract: While concept-based interpretability methods have traditionally focused on local explanations of neural network predictions, we propose a novel framework and interactive tool that extends these methods into the domain of mechanistic interpretability. Our approach enables a global dissection of model behavior by analyzing how high-level semantic attributes (referred to as concepts) emerge, interact, and propagate through internal model components. Unlike prior work that isolates individual neurons or predictions, our framework systematically quantifies how semantic concepts are represented across layers, revealing latent circuits and information flow that underlie model decision-making. A key innovation is our visualization platform that we named BAGEL (for Bias Analysis with a Graph for global Explanation Layers), which presents these insights in a structured knowledge graph, allowing users to explore concept-class relationships, identify spurious correlations, and enhance model trustworthiness. Our framework is model-agnostic, scalable, and contributes to a deeper understanding of how deep learning models generalize (or fail to) in the presence of dataset biases. The demonstration is available at https://knowledge-graph-ui-4a7cb5.gitlab.io/."
      },
      {
        "id": "oai:arXiv.org:2507.05812v1",
        "title": "Towards Solar Altitude Guided Scene Illumination",
        "link": "https://arxiv.org/abs/2507.05812",
        "author": "Samed Do\\u{g}an, Maximilian Hoh, Nico Leuze, Nicolas R. -Pe\\~na, Alfred Sch\\\"ottl",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05812v1 Announce Type: new \nAbstract: The development of safe and robust autonomous driving functions is heavily dependent on large-scale, high-quality sensor data. However, real-word data acquisition demands intensive human labor and is strongly limited by factors such as labeling cost, driver safety protocols and diverse scenario coverage. Thus, multiple lines of work focus on the conditional generation of synthetic camera sensor data. We identify a significant gap in research regarding daytime variation, presumably caused by the scarcity of available labels. Consequently, we present the solar altitude as global conditioning variable. It is readily computable from latitude-longitude coordinates and local time, eliminating the need for extensive manual labeling. Our work is complemented by a tailored normalization approach, targeting the sensitivity of daylight towards small numeric changes in altitude. We demonstrate its ability to accurately capture lighting characteristics and illumination-dependent image noise in the context of diffusion models."
      },
      {
        "id": "oai:arXiv.org:2507.05814v1",
        "title": "Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework",
        "link": "https://arxiv.org/abs/2507.05814",
        "author": "Wang Wang, Mingyu Shi, Jun Jiang, Wenqian Ma, Chong Liu, Yasutaka Narazaki, Xuguang Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05814v1 Announce Type: new \nAbstract: As critical transportation infrastructure, bridges face escalating challenges from aging and deterioration, while traditional manual inspection methods suffer from low efficiency. Although 3D point cloud technology provides a new data-driven paradigm, its application potential is often constrained by the incompleteness of real-world data, which results from missing labels and scanning occlusions. To overcome the bottleneck of insufficient generalization in existing synthetic data methods, this paper proposes a systematic framework for generating 3D bridge data.\n  This framework can automatically generate complete point clouds featuring component-level instance annotations, high-fidelity color, and precise normal vectors. It can be further extended to simulate the creation of diverse and physically realistic incomplete point clouds, designed to support the training of segmentation and completion networks, respectively. Experiments demonstrate that a PointNet++ model trained with our synthetic data achieves a mean Intersection over Union (mIoU) of 84.2% in real-world bridge semantic segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance on the component completion task.\n  This research offers an innovative methodology and a foundational dataset for the 3D visual analysis of bridge structures, holding significant implications for advancing the automated management and maintenance of infrastructure."
      },
      {
        "id": "oai:arXiv.org:2507.05819v1",
        "title": "2D Instance Editing in 3D Space",
        "link": "https://arxiv.org/abs/2507.05819",
        "author": "Yuhuan Xie, Aoxuan Pan, Ming-Xian Lin, Wei Huang, Yi-Hua Huang, Xiaojuan Qi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05819v1 Announce Type: new \nAbstract: Generative models have achieved significant progress in advancing 2D image editing, demonstrating exceptional precision and realism. However, they often struggle with consistency and object identity preservation due to their inherent pixel-manipulation nature. To address this limitation, we introduce a novel \"2D-3D-2D\" framework. Our approach begins by lifting 2D objects into 3D representation, enabling edits within a physically plausible, rigidity-constrained 3D environment. The edited 3D objects are then reprojected and seamlessly inpainted back into the original 2D image. In contrast to existing 2D editing methods, such as DragGAN and DragDiffusion, our method directly manipulates objects in a 3D environment. Extensive experiments highlight that our framework surpasses previous methods in general performance, delivering highly consistent edits while robustly preserving object identity."
      },
      {
        "id": "oai:arXiv.org:2507.05822v1",
        "title": "Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models",
        "link": "https://arxiv.org/abs/2507.05822",
        "author": "L'ea Dubois, Klaus Schmidt, Chengyu Wang, Ji-Hoon Park, Lin Wang, Santiago Munoz",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05822v1 Announce Type: new \nAbstract: Current video understanding models excel at recognizing \"what\" is happening but fall short in high-level cognitive tasks like causal reasoning and future prediction, a limitation rooted in their lack of commonsense world knowledge. To bridge this cognitive gap, we propose a novel framework that synergistically fuses a powerful Vision Foundation Model (VFM) for deep visual perception with a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our key technical innovation is a sophisticated fusion module, inspired by the Q-Former architecture, which distills complex spatiotemporal and object-centric visual features into a concise, language-aligned representation. This enables the LLM to effectively ground its inferential processes in direct visual evidence. The model is trained via a two-stage strategy, beginning with large-scale alignment pre-training on video-text data, followed by targeted instruction fine-tuning on a curated dataset designed to elicit advanced reasoning and prediction skills. Extensive experiments demonstrate that our model achieves state-of-the-art performance on multiple challenging benchmarks. Notably, it exhibits remarkable zero-shot generalization to unseen reasoning tasks, and our in-depth ablation studies validate the critical contribution of each architectural component. This work pushes the boundary of machine perception from simple recognition towards genuine cognitive understanding, paving the way for more intelligent and capable AI systems in robotics, human-computer interaction, and beyond."
      },
      {
        "id": "oai:arXiv.org:2507.05823v1",
        "title": "Fair Domain Generalization: An Information-Theoretic View",
        "link": "https://arxiv.org/abs/2507.05823",
        "author": "Tangzheng Lian, Guanyu Hu, Dimitrios Kollias, Xinyu Yang, Oya Celiktutan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05823v1 Announce Type: new \nAbstract: Domain generalization (DG) and algorithmic fairness are two critical challenges in machine learning. However, most DG methods focus only on minimizing expected risk in the unseen target domain without considering algorithmic fairness. Conversely, fairness methods typically do not account for domain shifts, so the fairness achieved during training may not generalize to unseen test domains. In this work, we bridge these gaps by studying the problem of Fair Domain Generalization (FairDG), which aims to minimize both expected risk and fairness violations in unseen target domains. We derive novel mutual information-based upper bounds for expected risk and fairness violations in multi-class classification tasks with multi-group sensitive attributes. These bounds provide key insights for algorithm design from an information-theoretic perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal Fairness for Domain Generalization), a practical framework that solves the FairDG problem and models the utility-fairness trade-off through Pareto optimization. Experiments on real-world vision and language datasets show that PAFDG achieves superior utility-fairness trade-offs compared to existing methods."
      },
      {
        "id": "oai:arXiv.org:2507.05838v1",
        "title": "I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation",
        "link": "https://arxiv.org/abs/2507.05838",
        "author": "Ourui Fu, Hangzhou He, Xinliang Zhang, Lei Zhu, Shuang Zeng, ZhaoHeng Xie, Yanye Lu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05838v1 Announce Type: new \nAbstract: The annotation bottleneck in semantic segmentation has driven significant interest in few-shot segmentation, which aims to develop segmentation models capable of generalizing rapidly to novel classes using minimal exemplars. Conventional training paradigms typically generate query prior maps by extracting masked-area features from support images, followed by making predictions guided by these prior maps. However, current approaches remain constrained by two critical limitations stemming from inter- and intra-image discrepancies, both of which significantly degrade segmentation performance: 1) The semantic gap between support and query images results in mismatched features and inaccurate prior maps; 2) Visually similar yet semantically distinct regions within support or query images lead to false negative or false positive predictions. We propose a novel FSS method called \\textbf{I$^2$R}: 1) Using category-specific high level representations which aggregate global semantic cues from support and query images, enabling more precise inter-image region localization and address the first limitation. 2) Directional masking strategy that suppresses inconsistent support-query pixel pairs, which exhibit high feature similarity but conflicting mask, to mitigate the second issue. Experiments demonstrate that our method outperforms state-of-the-art approaches, achieving improvements of 1.9\\% and 2.1\\% in mIoU under the 1-shot setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively."
      },
      {
        "id": "oai:arXiv.org:2507.05843v1",
        "title": "USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining",
        "link": "https://arxiv.org/abs/2507.05843",
        "author": "Yue Peng, Bing Xiong, Fuqiang Chen, De Eybo, RanRan Zhang, Wanming Hu, Jing Cai, Wenjian Qin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05843v1 Announce Type: new \nAbstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual IHC images from H\\&amp;E images while maintaining pathological semantic consistency with adjacent slices. This task aims to achieve cross-domain mapping between morphological structures and staining patterns through generative models, providing an efficient and cost-effective solution for pathological analysis. However, under weakly paired conditions, spatial heterogeneity between adjacent slices presents significant challenges. This can lead to inaccurate one-to-many mappings and generate results that are inconsistent with the pathological semantics of adjacent slices. To address this issue, we propose a novel unbalanced self-information feature transport for IHC virtual staining, named USIGAN, which extracts global morphological semantics without relying on positional correspondence.By removing weakly paired terms in the joint marginal distribution, we effectively mitigate the impact of weak pairing on joint distributions, thereby significantly improving the content consistency and pathological semantic consistency of the generated results. Moreover, we design the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation matrices between H\\&amp;E and generated IHC in image-level and real IHC and generated IHC image sets in intra-group level.. Experiments conducted on two publicly available datasets demonstrate that our method achieves superior performance across multiple clinically significant metrics, such as IoD and Pearson-R correlation, demonstrating better clinical relevance."
      },
      {
        "id": "oai:arXiv.org:2507.05849v1",
        "title": "DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction",
        "link": "https://arxiv.org/abs/2507.05849",
        "author": "Juli Zhang, Zeyu Yan, Jing Zhang, Qiguang Miao, Quan Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05849v1 Announce Type: new \nAbstract: Accurate remote sensing-based crop yield prediction remains a fundamental challenging task due to complex spatial patterns, heterogeneous spectral characteristics, and dynamic agricultural conditions. Existing methods often suffer from limited spatial modeling capacity, weak generalization across crop types and years. To address these challenges, we propose DFYP, a novel Dynamic Fusion framework for crop Yield Prediction, which combines spectral channel attention, edge-adaptive spatial modeling and a learnable fusion mechanism to improve robustness across diverse agricultural scenarios. Specifically, DFYP introduces three key components: (1) a Resolution-aware Channel Attention (RCA) module that enhances spectral representation by adaptively reweighting input channels based on resolution-specific characteristics; (2) an Adaptive Operator Learning Network (AOL-Net) that dynamically selects operators for convolutional kernels to improve edge-sensitive spatial feature extraction under varying crop and temporal conditions; and (3) a dual-branch architecture with a learnable fusion mechanism, which jointly models local spatial details and global contextual information to support cross-resolution and cross-crop generalization. Extensive experiments on multi-year datasets MODIS and multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms current state-of-the-art baselines in RMSE, MAE, and R2 across different spatial resolutions, crop types, and time periods, showcasing its effectiveness and robustness for real-world agricultural monitoring."
      },
      {
        "id": "oai:arXiv.org:2507.05852v1",
        "title": "Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning",
        "link": "https://arxiv.org/abs/2507.05852",
        "author": "Samuel Ofosu Mensah, Kerol Djoumessi, Philipp Berens",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05852v1 Announce Type: new \nAbstract: Federated learning (FL) provides a promising paradigm for collaboratively training machine learning models across distributed data sources while maintaining privacy. Nevertheless, real-world FL often faces major challenges including communication overhead during the transfer of large model parameters and statistical heterogeneity, arising from non-identical independent data distributions across clients. In this work, we propose an FL framework that 1) provides inherent interpretations using prototypes, and 2) tackles statistical heterogeneity by utilising lightweight adapter modules to act as compressed surrogates of local models and guide clients to achieve generalisation despite varying client distribution. Each client locally refines its model by aligning class embeddings toward prototype representations and simultaneously adjust the lightweight adapter. Our approach replaces the need to communicate entire model weights with prototypes and lightweight adapters. This design ensures that each client's model aligns with a globally shared structure while minimising communication load and providing inherent interpretations. Moreover, we conducted our experiments on a real-world retinal fundus image dataset, which provides clinical-site information. We demonstrate inherent interpretable capabilities and perform a classification task, which shows improvements in accuracy over baseline algorithms."
      },
      {
        "id": "oai:arXiv.org:2507.05859v1",
        "title": "D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos",
        "link": "https://arxiv.org/abs/2507.05859",
        "author": "Wenkang Zhang, Yan Zhao, Qiang Wang, Li Song, Zhengxue Cheng",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05859v1 Announce Type: new \nAbstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representations remains a major challenge. Recent advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have enabled high-fidelity scene modeling. However, existing methods often couple scene reconstruction with optimization-dependent coding, which limits generalizability. This paper presents Feedforward Compression of Dynamic Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing temporally correlated Gaussian point cloud sequences. Our approach introduces a Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame motions are extracted via sparse control points. The resulting motion tensors are compressed in a feedforward manner using a dual prior-aware entropy model that combines hyperprior and spatial-temporal priors for accurate rate estimation. For reconstruction, we perform control-point-guided motion compensation and employ a refinement network to enhance view-consistent fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS generalizes across scenes without per-scene optimization. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression in under 2 seconds while preserving visual quality across viewpoints. This work advances feedforward compression for dynamic 3DGS, paving the way for scalable FVV transmission and storage in immersive applications."
      },
      {
        "id": "oai:arXiv.org:2507.05874v1",
        "title": "Robust Power System State Estimation using Physics-Informed Neural Networks",
        "link": "https://arxiv.org/abs/2507.05874",
        "author": "Solon Falas, Markos Asprou, Charalambos Konstantinou, Maria K. Michael",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05874v1 Announce Type: new \nAbstract: Modern power systems face significant challenges in state estimation and real-time monitoring, particularly regarding response speed and accuracy under faulty conditions or cyber-attacks. This paper proposes a hybrid approach using physics-informed neural networks (PINNs) to enhance the accuracy and robustness, of power system state estimation. By embedding physical laws into the neural network architecture, PINNs improve estimation accuracy for transmission grid applications under both normal and faulty conditions, while also showing potential in addressing security concerns such as data manipulation attacks. Experimental results show that the proposed approach outperforms traditional machine learning models, achieving up to 83% higher accuracy on unseen subsets of the training dataset and 65% better performance on entirely new, unrelated datasets. Experiments also show that during a data manipulation attack against a critical bus in a system, the PINN can be up to 93% more accurate than an equivalent neural network."
      },
      {
        "id": "oai:arXiv.org:2507.05885v1",
        "title": "How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures",
        "link": "https://arxiv.org/abs/2507.05885",
        "author": "Tanvina Patel, Wiebke Hutiri, Aaron Yi Ding, Odette Scharenborg",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05885v1 Announce Type: new \nAbstract: There is increasingly more evidence that automatic speech recognition (ASR) systems are biased against different speakers and speaker groups, e.g., due to gender, age, or accent. Research on bias in ASR has so far primarily focused on detecting and quantifying bias, and developing mitigation approaches. Despite this progress, the open question is how to measure the performance and bias of a system. In this study, we compare different performance and bias measures, from literature and proposed, to evaluate state-of-the-art end-to-end ASR systems for Dutch. Our experiments use several bias mitigation strategies to address bias against different speaker groups. The findings reveal that averaged error rates, a standard in ASR research, alone is not sufficient and should be supplemented by other measures. The paper ends with recommendations for reporting ASR performance and bias to better represent a system's performance for diverse speaker groups, and overall system bias."
      },
      {
        "id": "oai:arXiv.org:2507.05887v1",
        "title": "GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing",
        "link": "https://arxiv.org/abs/2507.05887",
        "author": "Xianzhi Ma, Jianhui Li, Changhua Pei, Hao Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05887v1 Announce Type: new \nAbstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image understanding has achieved notable progress, demonstrating the basic ability to recognize and describe geographical entities. However, existing RS-VLMs are mostly limited to image-level and region-level tasks, lacking the capability to handle pixel-level tasks and performing poorly in small-object recognition scenarios. Moreover, RS-VLMs consume significant computational resources when processing high-resolution RS images, further restricting their practical applicability. In this context, we propose GeoMag (Geographical Magnifier), an end-to-end general-purpose large model framework for RS. GeoMag dynamically focuses the attention scope based on prompt semantics to effectively perform remote sensing image parsing across multiple levels of granularity. This method introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the spatial resolution of task-irrelevant regions while enhancing the visual representation of task-relevant areas. This approach improves the model's perception of critical target regions, suppresses background redundancy, and reduces the computational cost of interpreting high-resolution RS imagery. Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not only excels in handling pixel-level tasks but also maintains competitive performance across tasks of other granularities compared to existing RS-VLMs."
      },
      {
        "id": "oai:arXiv.org:2507.05890v1",
        "title": "Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators",
        "link": "https://arxiv.org/abs/2507.05890",
        "author": "Sungjib Lim, Woojung Song, Eun-Ju Lee, Yohan Jo",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05890v1 Announce Type: new \nAbstract: As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs replicate human-like behavior. We will publicly release our dataset and code to support future work."
      },
      {
        "id": "oai:arXiv.org:2507.05899v1",
        "title": "What You Have is What You Track: Adaptive and Robust Multimodal Tracking",
        "link": "https://arxiv.org/abs/2507.05899",
        "author": "Yuedong Tan, Jiawei Shao, Eduard Zamfir, Ruanjun Li, Zhaochong An, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte, Zongwei Wu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05899v1 Announce Type: new \nAbstract: Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities. To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness which is critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be publicly available at https://github.com/supertyd/FlexTrack/tree/main."
      },
      {
        "id": "oai:arXiv.org:2507.05904v1",
        "title": "Universal Embeddings of Tabular Data",
        "link": "https://arxiv.org/abs/2507.05904",
        "author": "Astrid Franz, Frederik Hoppe, Marianne Michaelis, Udo G\\\"obel",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05904v1 Announce Type: new \nAbstract: Tabular data in relational databases represents a significant portion of industrial data. Hence, analyzing and interpreting tabular data is of utmost importance. Application tasks on tabular data are manifold and are often not specified when setting up an industrial database. To address this, we present a novel framework for generating universal, i.e., task-independent embeddings of tabular data for performing downstream tasks without predefined targets. Our method transforms tabular data into a graph structure, leverages Graph Auto-Encoders to create entity embeddings, which are subsequently aggregated to obtain embeddings for each table row, i.e., each data sample. This two-step approach has the advantage that unseen samples, consisting of similar entities, can be embedded without additional training. Downstream tasks such as regression, classification or outlier detection, can then be performed by applying a distance-based similarity measure in the embedding space. Experiments on real-world datasets demonstrate that our method achieves superior performance compared to existing universal tabular data embedding techniques."
      },
      {
        "id": "oai:arXiv.org:2507.05906v1",
        "title": "Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why",
        "link": "https://arxiv.org/abs/2507.05906",
        "author": "Chenhao Li, Marco Hutter, Andreas Krause",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05906v1 Announce Type: new \nAbstract: This survey provides a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning. Feature-based methods offer dense, interpretable rewards that excel at high-fidelity motion imitation, yet often require sophisticated representations of references and struggle with generalization in unstructured settings. GAN-based methods, in contrast, use implicit, distributional supervision that enables scalability and adaptation flexibility, but are prone to training instability and coarse reward signals. Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration. We argue that the dichotomy between feature-based and GAN-based methods is increasingly nuanced: rather than one paradigm dominating the other, the choice should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability. This work outlines the algorithmic trade-offs and design considerations that underlie method selection, offering a framework for principled decision-making in learning from demonstrations."
      },
      {
        "id": "oai:arXiv.org:2507.05914v1",
        "title": "Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data",
        "link": "https://arxiv.org/abs/2507.05914",
        "author": "Rui Huang, Shitong Shao, Zikai Zhou, Pukun Zhao, Hangyu Guo, Tian Ye, Lichen Bai, Shuo Yang, Zeke Xie",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05914v1 Announce Type: new \nAbstract: Diffusion models have achieved remarkable success in various generative tasks, but training them remains highly resource-intensive, often requiring millions of images and many days of GPU computation. From a data-centric perspective addressing this limitation, we study diffusion dataset condensation as a new and challenging problem setting. The goal is to construct a \"synthetic\" sub-dataset with significantly fewer samples than the original dataset, enabling high-quality diffusion model training with greatly reduced cost. To the best of our knowledge, we are the first to formally investigate dataset condensation for diffusion models, whereas prior work focused on training discriminative models. To tackle this new challenge, we propose a novel Diffusion Dataset Condensation (D2C) framework, which consists of two phases: Select and Attach. The Select phase identifies a compact and diverse subset using a diffusion difficulty score and interval sampling. The Attach phase enhances the selected subset by attaching rich semantic and visual representations to strengthen the conditional signals. Extensive experiments across various dataset sizes, model architectures, and resolutions show that our D2C framework enables significantly faster diffusion model training with dramatically fewer data, while preserving high visual quality. Notably, for the SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID score of 4.3 in just 40k steps using only 0.8% of the training data."
      },
      {
        "id": "oai:arXiv.org:2507.05916v1",
        "title": "On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification",
        "link": "https://arxiv.org/abs/2507.05916",
        "author": "Jonas Klotz, Tom Burgert, Beg\\\"um Demir",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05916v1 Announce Type: new \nAbstract: The development of explainable artificial intelligence (xAI) methods for scene classification problems has attracted great attention in remote sensing (RS). Most xAI methods and the related evaluation metrics in RS are initially developed for natural images considered in computer vision (CV), and their direct usage in RS may not be suitable. To address this issue, in this paper, we investigate the effectiveness of explanation methods and metrics in the context of RS image scene classification. In detail, we methodologically and experimentally analyze ten explanation metrics spanning five categories (faithfulness, robustness, localization, complexity, randomization), applied to five established feature attribution methods (Occlusion, LIME, GradCAM, LRP, and DeepLIFT) across three RS datasets. Our methodological analysis identifies key limitations in both explanation methods and metrics. The performance of perturbation-based methods, such as Occlusion and LIME, heavily depends on perturbation baselines and spatial characteristics of RS scenes. Gradient-based approaches like GradCAM struggle when multiple labels are present in the same image, while some relevance propagation methods (LRP) can distribute relevance disproportionately relative to the spatial extent of classes. Analogously, we find limitations in evaluation metrics. Faithfulness metrics share the same problems as perturbation-based methods. Localization metrics and complexity metrics are unreliable for classes with a large spatial extent. In contrast, robustness metrics and randomization metrics consistently exhibit greater stability. Our experimental results support these methodological findings. Based on our analysis, we provide guidelines for selecting explanation methods, metrics, and hyperparameters in the context of RS image scene classification."
      },
      {
        "id": "oai:arXiv.org:2507.05918v1",
        "title": "Few-shot text-based emotion detection",
        "link": "https://arxiv.org/abs/2507.05918",
        "author": "Teodor-George Marchitan, Claudiu Creanga, Liviu P. Dinu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05918v1 Announce Type: new \nAbstract: This paper describes the approach of the Unibuc - NLP team in tackling the SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion Detection. We mainly focused on experiments using large language models (Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With our final system, for the multi-label emotion detection track (track A), we got an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36 teams) for the Portuguese (Mozambican) subset and $0.325$ (\\textbf{1}/31 teams) for the Emakhuwa subset."
      },
      {
        "id": "oai:arXiv.org:2507.05920v1",
        "title": "High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning",
        "link": "https://arxiv.org/abs/2507.05920",
        "author": "Xinyu Huang, Yuhao Dong, Weiwei Tian, Bo Li, Rui Feng, Ziwei Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05920v1 Announce Type: new \nAbstract: State-of-the-art large multi-modal models (LMMs) face challenges when processing high-resolution images, as these inputs are converted into enormous visual tokens, many of which are irrelevant to the downstream task. In this paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an end-to-end reinforcement learning (RL) framework that enables LMMs to iteratively focus on key visual regions by automatically cropping sub-images, based on model-predicted grounding coordinates within a multi-turn conversation framework. Compared to supervised fine-tuning (SFT), which requires costly additional grounding annotations, our approach highlights that LMMs can emerge robust grounding abilities during the RL training process, leveraging only a binary reward function derived from the correctness of the final answer. Additionally, we observe that LMMs struggle to autonomously trigger visual grounding during the rollout process. To address this cold start problem, we design a multi-turn conversational template and restrict policy loss computation to model outputs generated across multiple dialogue rounds, thereby promoting stable optimization. Extensive experiments demonstrate that, when trained on standard visual-question-short answering data without grounding annotations, MGPO effectively elicits stronger grounding capabilities compared to GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and 5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench. Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at https://github.com/EvolvingLMMs-Lab/MGPO."
      },
      {
        "id": "oai:arXiv.org:2507.05937v1",
        "title": "Towards a Principled Evaluation of Knowledge Editors",
        "link": "https://arxiv.org/abs/2507.05937",
        "author": "Sebastian Pohl, Max Ploner, Alan Akbik",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05937v1 Announce Type: new \nAbstract: Model editing has been gaining increasing attention over the past few years. For Knowledge Editing in particular, more challenging evaluation datasets have recently been released. These datasets use different methodologies to score the success of editors. Yet, it remains under-explored how robust these methodologies are and whether they unfairly favor some editors. Moreover, the disruptive impact of these editors on overall model capabilities remains a constant blind spot.\n  We address both of these problems and show that choosing different metrics and evaluation methodologies as well as different edit batch sizes can lead to a different ranking of knowledge editors. Crucially we demonstrate this effect also on general language understanding tasks evaluated alongside the knowledge editing tasks. Further we include a manual assessment of the string matching based evaluation method for knowledge editing that is favored by recently released datasets, revealing a tendency to produce false positive matches."
      },
      {
        "id": "oai:arXiv.org:2507.05939v1",
        "title": "Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors",
        "link": "https://arxiv.org/abs/2507.05939",
        "author": "Bing Wang, Ximing Li, Mengzhe Ye, Changchun Li, Bo Fu, Jianfeng Qu, Lin Yuanbo Wu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05939v1 Announce Type: new \nAbstract: Nowadays, misinformation articles, especially multimodal ones, are widely spread on social media platforms and cause serious negative effects. To control their propagation, Multimodal Misinformation Detection (MMD) becomes an active topic in the community to automatically identify misinformation. Previous MMD methods focus on supervising detectors by collecting offline data. However, in real-world scenarios, new events always continually emerge, making MMD models trained on offline data consistently outdated and ineffective. To address this issue, training MMD models under online data streams is an alternative, inducing an emerging task named continual MMD. Unfortunately, it is hindered by two major challenges. First, training on new data consistently decreases the detection performance on past data, named past knowledge forgetting. Second, the social environment constantly evolves over time, affecting the generalization on future data. To alleviate these challenges, we propose to remember past knowledge by isolating interference between event-specific parameters with a Dirichlet process-based mixture-of-expert structure, and anticipate future environmental distributions by learning a continuous-time dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD. Extensive experiments demonstrate that DAEDCMD can consistently and significantly outperform the compared methods, including six MMD baselines and three continual learning methods."
      },
      {
        "id": "oai:arXiv.org:2507.05940v1",
        "title": "Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems",
        "link": "https://arxiv.org/abs/2507.05940",
        "author": "Sandeep Mishra, Anubhab Mandal, Bishal Santra, Tushar Abhishek, Pawan Goyal, Manish Gupta",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05940v1 Announce Type: new \nAbstract: Ghosting, the ability to predict a user's intended text input for inline query auto-completion, is an invaluable feature for modern search engines and chat interfaces, greatly enhancing user experience. By suggesting completions to incomplete queries (or prefixes), ghosting aids users with slow typing speeds, disabilities, or limited language proficiency. Ghosting is a challenging problem and has become more important with the ubiquitousness of chat-based systems like ChatGPT, Copilot, etc. Despite the increasing prominence of chat-based systems utilizing ghosting, this challenging problem of Chat-Ghosting has received little attention from the NLP/ML research community. There is a lack of standardized benchmarks and relative performance analysis of deep learning and non-deep learning methods. We address this through an open and thorough study of this problem using four publicly available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and two human-bot (Open Assistant and ShareGPT). We experiment with various existing query auto-completion methods (using tries), n-gram methods and deep learning methods, with and without dialog context. We also propose a novel entropy-based dynamic early stopping strategy. Our analysis finds that statistical n-gram models and tries outperform deep learning based models in terms of both model performance and inference efficiency for seen prefixes. For unseen queries, neural models like T5 and Phi-2 lead to better results. Adding conversational context leads to significant improvements in ghosting quality, especially for Open-Assistant and ShareGPT. We make code and data publicly available"
      },
      {
        "id": "oai:arXiv.org:2507.05948v1",
        "title": "Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation",
        "link": "https://arxiv.org/abs/2507.05948",
        "author": "Quanzhu Niu, Yikang Zhou, Shihao Chen, Tao Zhang, Shunping Ji",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05948v1 Announce Type: new \nAbstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive challenges including object occlusions, motion blur, and appearance variations during temporal association. To overcome these limitations, this work introduces geometric awareness to enhance VIS robustness by strategically leveraging monocular depth estimation. We systematically investigate three distinct integration paradigms. Expanding Depth Channel (EDC) method concatenates the depth map as input channel to segmentation networks; Sharing ViT (SV) designs a uniform ViT backbone, shared between depth estimation and segmentation branches; Depth Supervision (DS) makes use of depth prediction as an auxiliary training guide for feature learning. Though DS exhibits limited effectiveness, benchmark evaluations demonstrate that EDC and SV significantly enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets 56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work conclusively establishes depth cues as critical enablers for robust video understanding."
      },
      {
        "id": "oai:arXiv.org:2507.05950v1",
        "title": "Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling",
        "link": "https://arxiv.org/abs/2507.05950",
        "author": "Pinar Bisgin, Tom Strube, Niklas Tschorn, Michael Pantf\\\"order, Maximilian Fecke, Ingrid Ljungvall, Jens H\\\"aggstr\\\"om, Gerhard Wess, Christoph Schummer, Sven Meister, Falk M. Howar",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05950v1 Announce Type: new \nAbstract: Noisy labels pose significant challenges for AI model training in veterinary medicine. This study examines expert assessment ambiguity in canine auscultation data, highlights the negative impact of label noise on classification performance, and introduces methods for label noise reduction. To evaluate whether label noise can be minimized by incorporating multiple expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated regarding the intensity of holosystolic heart murmurs caused by Myxomatous Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of 70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging individual heart cycles, the training data was expanded and classification robustness was enhanced. The investigation encompassed training and evaluating three classification algorithms: AdaBoost, XGBoost, and Random Forest. While AdaBoost and Random Forest exhibited reasonable performances, XGBoost demonstrated notable improvements in classification accuracy. All algorithms showed significant improvements in classification accuracy due to the applied label noise reduction, most notably XGBoost. Specifically, for the detection of mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23% to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling category, sensitivity and specificity increased from 58.28% to 95.09% and from 84.84% to 89.69%, respectively. These results highlight the importance of minimizing label noise to improve classification algorithms for the detection of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart sound classification, label noise reduction, machine learning, XGBoost, veterinary cardiology, MMVD."
      },
      {
        "id": "oai:arXiv.org:2507.05952v1",
        "title": "High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes",
        "link": "https://arxiv.org/abs/2507.05952",
        "author": "Aoxiang Fan, Corentin Dumery, Nicolas Talabot, Hieu Le, Pascal Fua",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05952v1 Announce Type: new \nAbstract: Generalizable neural surface reconstruction has become a compelling technique to reconstruct from few images without per-scene optimization, where dense 3D feature volume has proven effective as a global representation of scenes. However, the dense representation does not scale well to increasing voxel resolutions, severely limiting the reconstruction quality. We thus present a sparse representation method, that maximizes memory efficiency and enables significantly higher resolution reconstructions on standard hardware. We implement this through a two-stage approach: First training a network to predict voxel occupancies from posed images and associated depth maps, then computing features and performing volume rendering only in voxels with sufficiently high occupancy estimates. To support this sparse representation, we developed custom algorithms for efficient sampling, feature aggregation, and querying from sparse volumes-overcoming the dense-volume assumptions inherent in existing works. Experiments on public datasets demonstrate that our approach reduces storage requirements by more than 50 times without performance degradation, enabling reconstructions at $512^3$ resolution compared to the typical $128^3$ on similar hardware, and achieving superior reconstruction accuracy over current state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2507.05963v1",
        "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation",
        "link": "https://arxiv.org/abs/2507.05963",
        "author": "Zhenghao Zhang, Junchao Liao, Xiangyu Meng, Long Qin, Weizhi Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05963v1 Announce Type: new \nAbstract: Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: https://github.com/alibaba/Tora ."
      },
      {
        "id": "oai:arXiv.org:2507.05964v1",
        "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
        "link": "https://arxiv.org/abs/2507.05964",
        "author": "Vera Soboleva, Aibek Alanov, Andrey Kuznetsov, Konstantin Sobolev",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05964v1 Announce Type: new \nAbstract: While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA."
      },
      {
        "id": "oai:arXiv.org:2507.05965v1",
        "title": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation",
        "link": "https://arxiv.org/abs/2507.05965",
        "author": "Lucas Fonseca Lage, Simon Ostermann",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05965v1 Announce Type: new \nAbstract: We introduce OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models (LLMs). FActScore evaluates the factual accuracy of long-form text by using Atomic Fact Generation (AFG) to extract individual factual claims and Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge source. While the original FActScore relies on closed-source and commercial models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. We provide a detailed technical overview of our implementation, highlighting design choices and modifications made to support open models. We evaluate multiple open-source LLMs on both AFG and AFV using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments. OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation, and is available at: https://github.com/lflage/OpenFActScore."
      },
      {
        "id": "oai:arXiv.org:2507.05966v1",
        "title": "Simple Convergence Proof of Adam From a Sign-like Descent Perspective",
        "link": "https://arxiv.org/abs/2507.05966",
        "author": "Hanyang Peng, Shuang Qin, Yue Yu, Fangqing Jiang, Hui Wang, Zhouchen Lin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05966v1 Announce Type: new \nAbstract: Adam is widely recognized as one of the most effective optimizers for training deep neural networks (DNNs). Despite its remarkable empirical success, its theoretical convergence analysis remains unsatisfactory. Existing works predominantly interpret Adam as a preconditioned stochastic gradient descent with momentum (SGDM), formulated as $\\bm{x}_{t+1} = \\bm{x}_t - \\frac{\\gamma_t}{{\\sqrt{\\bm{v}_t}+\\epsilon}} \\circ \\bm{m}_t$. This perspective necessitates strong assumptions and intricate techniques, resulting in lengthy and opaque convergence proofs that are difficult to verify and extend. In contrast, we propose a novel interpretation by treating Adam as a sign-like optimizer, expressed as $\\bm{x}_{t+1} = \\bm{x}_t - \\gamma_t \\frac{|\\bm{m}_t|}{{\\sqrt{\\bm{v}_t}+\\epsilon}} \\circ {\\rm Sign}(\\bm{m}_t)$. This reformulation significantly simplifies the convergence analysis. For the first time, with some mild conditions, we prove that Adam achieves the optimal rate of ${\\cal O}(\\frac{1}{T^{\\sfrac{1}{4}}})$ rather than the previous ${\\cal O} \\left(\\frac{\\ln T}{T^{\\sfrac{1}{4}}}\\right)$ under weak assumptions of the generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without dependence on the model dimensionality or the numerical stability parameter $\\epsilon$. Additionally, our theoretical analysis provides new insights into the role of momentum as a key factor ensuring convergence and offers practical guidelines for tuning learning rates in Adam, further bridging the gap between theory and practice."
      },
      {
        "id": "oai:arXiv.org:2507.05970v1",
        "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval",
        "link": "https://arxiv.org/abs/2507.05970",
        "author": "Haiwen Li, Delong Liu, Zhaohui Hou, Zhicheng Zhao, Fei Su",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05970v1 Announce Type: new \nAbstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR) aims to retrieve target images using multimodal (image+text) queries. Although many existing CIR methods have attained promising performance, their reliance on costly, manually labeled triplets hinders scalability and zero-shot capability. To address this issue, we propose a scalable pipeline for automatic triplet generation, along with a fully synthetic dataset named Composed Image Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a large language model (LLM) to generate diverse prompts, controlling a text-to-image generative model to produce image pairs with identical elements in each pair, which are then filtered and reorganized to form the CIRHS dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a novel CIR framework, which can accomplish global alignment and local reasoning within a broader context, enabling the model to learn more robust and informative representations. By utilizing the synthetic CIRHS dataset, CoAlign achieves outstanding zero-shot performance on three commonly used benchmarks, demonstrating for the first time the feasibility of training CIR models on a fully synthetic dataset. Furthermore, under supervised training, our method outperforms all the state-of-the-art supervised CIR approaches, validating the effectiveness of our proposed retrieval framework. The code and the CIRHS dataset will be released soon."
      },
      {
        "id": "oai:arXiv.org:2507.05973v1",
        "title": "We Should Evaluate Real-World Impact",
        "link": "https://arxiv.org/abs/2507.05973",
        "author": "Ehud Reiter",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05973v1 Announce Type: new \nAbstract: The ACL community has very little interest in evaluating the real-world impact of NLP systems. A structured survey of the ACL Anthology shows that perhaps 0.1% of its papers contain such evaluations; furthermore most papers which include impact evaluations present them very sketchily and instead focus on metric evaluations. NLP technology would be more useful and more quickly adopted if we seriously tried to understand and evaluate its real-world impact."
      },
      {
        "id": "oai:arXiv.org:2507.05980v1",
        "title": "RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages",
        "link": "https://arxiv.org/abs/2507.05980",
        "author": "Gabriel Chua, Leanne Tan, Ziyu Ge, Roy Ka-Wei Lee",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05980v1 Announce Type: new \nAbstract: Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages. The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation. RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available."
      },
      {
        "id": "oai:arXiv.org:2507.05991v1",
        "title": "Evolution without Large Models: Training Language Model with Task Principles",
        "link": "https://arxiv.org/abs/2507.05991",
        "author": "Minghang Zhu, Shen Gao, Zhengliang Shi, Jiabao Fang, Pengjie Ren, Zhaochun Ren, Zhumin Chen, Shuo Shang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05991v1 Announce Type: new \nAbstract: A common training approach for language models involves using a large-scale language model to expand a human-provided dataset, which is subsequently used for model training.This method significantly reduces training costs by eliminating the need for extensive human data annotation. However, it still faces challenges such as high carbon emissions during data augmentation and the risk of data leakage when we use closed-source LLMs. To address these issues, we propose a self-evolution method for language models. First, we introduce the Multi-level Principle Generation, which enables a large-scale model to summarize task-completion principles based on a small amount of task data. Then, we propose the Principle-based Instance Generation, in which a smaller-scale language model uses these task principles to generate a large amount of data. This data is then used for model training. Experimental results show that our proposed method significantly improves model performance compared to directly using a smaller-scale language model to generate data. Additionally, since we only use the large-scale language model to generate the task-completion principles, the carbon emissions associated with training the model are greatly reduced."
      },
      {
        "id": "oai:arXiv.org:2507.05992v1",
        "title": "Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge",
        "link": "https://arxiv.org/abs/2507.05992",
        "author": "Xin Wu, Fei Teng, Yue Feng, Kaibo Shi, Zhuosheng Lin, Ji Zhang, James Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05992v1 Announce Type: new \nAbstract: Partial multi-label learning aims to extract knowledge from incompletely annotated data, which includes known correct labels, known incorrect labels, and unknown labels. The core challenge lies in accurately identifying the ambiguous relationships between labels and instances. In this paper, we emphasize that matching co-occurrence patterns between labels and instances is key to addressing this challenge. To this end, we propose Semantic Co-occurrence Insight Network (SCINet), a novel and effective framework for partial multi-label learning. Specifically, SCINet introduces a bi-dominant prompter module, which leverages an off-the-shelf multimodal model to capture text-image correlations and enhance semantic alignment. To reinforce instance-label interdependencies, we develop a cross-modality fusion module that jointly models inter-label correlations, inter-instance relationships, and co-occurrence patterns across instance-label assignments. Moreover, we propose an intrinsic semantic augmentation strategy that enhances the model's understanding of intrinsic data semantics by applying diverse image transformations, thereby fostering a synergistic relationship between label confidence and sample difficulty. Extensive experiments on four widely-used benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2507.05996v1",
        "title": "Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation",
        "link": "https://arxiv.org/abs/2507.05996",
        "author": "Haroon Wahab, Hassan Ugail, Lujain Jaleel",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05996v1 Announce Type: new \nAbstract: Machine learning-based Deepfake detection models have achieved impressive results on benchmark datasets, yet their performance often deteriorates significantly when evaluated on out-of-distribution data. In this work, we investigate an ensemble-based approach for improving the generalization of deepfake detection systems across diverse datasets. Building on a recent open-source benchmark, we combine prediction probabilities from several state-of-the-art asymmetric models proposed at top venues. Our experiments span two distinct out-of-domain datasets and demonstrate that no single model consistently outperforms others across settings. In contrast, ensemble-based predictions provide more stable and reliable performance in all scenarios. Our results suggest that asymmetric ensembling offers a robust and scalable solution for real-world deepfake detection where prior knowledge of forgery type or quality is often unavailable."
      },
      {
        "id": "oai:arXiv.org:2507.05997v1",
        "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations",
        "link": "https://arxiv.org/abs/2507.05997",
        "author": "Nicholas Popovi\\v{c}, Ashish Kangen, Tim Schopf, Michael F\\\"arber",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05997v1 Announce Type: new \nAbstract: Large, high-quality annotated corpora remain scarce in document-level entity and relation extraction in zero-shot or few-shot settings. In this paper, we present a fully automatic, LLM-based pipeline for synthetic data generation and in-context learning for document-level entity and relation extraction. In contrast to existing approaches that rely on manually annotated demonstrations or direct zero-shot inference, our method combines synthetic data generation with retrieval-based in-context learning, using a reasoning-optimized language model. This allows us to build a high-quality demonstration database without manual annotation and to dynamically retrieve relevant examples at inference time. Based on our approach we produce a synthetic dataset of over $5k$ Wikipedia abstracts with approximately $59k$ entities and $30k$ relation triples. Finally, we evaluate in-context learning performance on the DocIE shared task, extracting entities and relations from long documents in a zero-shot setting. We find that in-context joint entity and relation extraction at document-level remains a challenging task, even for state-of-the-art large language models."
      },
      {
        "id": "oai:arXiv.org:2507.05999v1",
        "title": "Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS",
        "link": "https://arxiv.org/abs/2507.05999",
        "author": "Xinyu Wang, Muhammad Ibrahim, Atif Mansoor, Ajmal Mian",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05999v1 Announce Type: new \nAbstract: Accurate geo-registration of LiDAR point clouds presents significant challenges in GNSS signal denied urban areas with high-rise buildings and bridges. Existing methods typically rely on real-time GNSS and IMU data, that require pre-calibration and assume stable positioning during data collection. However, this assumption often fails in dense urban areas, resulting in localization errors. To address this, we propose a structured geo-registration and spatial correction method that aligns 3D point clouds with satellite images, enabling frame-wise recovery of GNSS information and reconstruction of city scale 3D maps without relying on prior localization. The proposed approach employs a pre-trained Point Transformer model to segment the road points and then extracts the road skeleton and intersection points from the point cloud as well as the target map for alignment. Global rigid alignment of the two is performed using the intersection points, followed by local refinement using radial basis function (RBF) interpolation. Elevation correction is then applied to the point cloud based on terrain information from SRTM dataset to resolve vertical discrepancies. The proposed method was tested on the popular KITTI benchmark and a locally collected Perth (Western Australia) CBD dataset. On the KITTI dataset, our method achieved an average planimetric alignment standard deviation (STD) of 0.84~m across sequences with intersections, representing a 55.3\\% improvement over the original dataset. On the Perth dataset, which lacks GNSS information, our method achieved an average STD of 0.96~m compared to the GPS data extracted from Google Maps API. This corresponds to a 77.4\\% improvement from the initial alignment. Our method also resulted in elevation correlation gains of 30.5\\% on the KITTI dataset and 50.4\\% on the Perth dataset."
      },
      {
        "id": "oai:arXiv.org:2507.06009v1",
        "title": "KnowIt: Deep Time Series Modeling and Interpretation",
        "link": "https://arxiv.org/abs/2507.06009",
        "author": "M. W. Theunissen, R. Rabe, M. H. Davel",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06009v1 Announce Type: new \nAbstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for building deep time series models and interpreting them. It is implemented as a Python toolkit, with source code and documentation available from https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions about task specifications and decouples the definition of dataset, deep neural network architecture, and interpretability technique through well defined interfaces. This ensures the ease of importing new datasets, custom architectures, and the definition of different interpretability paradigms while maintaining on-the-fly modeling and interpretation of different aspects of a user's own time series data. KnowIt aims to provide an environment where users can perform knowledge discovery on their own complex time series data through building powerful deep learning models and explaining their behavior. With ongoing development, collaboration and application our goal is to make this a platform to progress this underexplored field and produce a trusted tool for deep time series modeling."
      },
      {
        "id": "oai:arXiv.org:2507.06016v1",
        "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents",
        "link": "https://arxiv.org/abs/2507.06016",
        "author": "Youmna Farag, Svetlana Stoyanchev, Mohan Li, Simon Keizer, Rama Doddipatla",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06016v1 Announce Type: new \nAbstract: Embodied agents performing complex tasks are susceptible to execution failures, motivating the need for effective failure recovery mechanisms. In this work, we introduce a conditional multistage failure recovery framework that employs zero-shot chain prompting. The framework is structured into four error-handling stages, with three operating during task execution and one functioning as a post-execution reflection phase. Our approach utilises the reasoning capabilities of LLMs to analyse execution challenges within their environmental context and devise strategic solutions. We evaluate our method on the TfD benchmark of the TEACH dataset and achieve state-of-the-art performance, outperforming a baseline without error recovery by 11.5% and surpassing the strongest existing model by 19%."
      },
      {
        "id": "oai:arXiv.org:2507.06021v1",
        "title": "Kamae: Bridging Spark and Keras for Seamless ML Preprocessing",
        "link": "https://arxiv.org/abs/2507.06021",
        "author": "George Barrowclough, Marian Andrecki, James Shinner, Daniele Donghi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06021v1 Announce Type: new \nAbstract: In production recommender systems, feature preprocessing must be faithfully replicated across training and inference environments. This often requires duplicating logic between offline and online environments, increasing engineering effort and introducing risks of dataset shift. We present Kamae, an open-source Python library that bridges this gap by translating PySpark preprocessing pipelines into equivalent Keras models. Kamae provides a suite of configurable Spark transformers and estimators, each mapped to a corresponding Keras layer, enabling consistent, end-to-end preprocessing across the ML lifecycle. Framework's utility is illustrated on real-world use cases, including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code is available at https://github.com/ExpediaGroup/kamae."
      },
      {
        "id": "oai:arXiv.org:2507.06026v1",
        "title": "Multi-view mid fusion: a universal approach for learning in an HDLSS setting",
        "link": "https://arxiv.org/abs/2507.06026",
        "author": "Lynn Houthuys",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06026v1 Announce Type: new \nAbstract: The high-dimensional low-sample-size (HDLSS) setting presents significant challenges in various applications where the feature dimension far exceeds the number of available samples. This paper introduces a universal approach for learning in HDLSS setting using multi-view mid fusion techniques. It shows how existing mid fusion multi-view methods perform well in an HDLSS setting even if no inherent views are provided. Three view construction methods are proposed that split the high-dimensional feature vectors into smaller subsets, each representing a different view. Extensive experimental validation across model-types and learning tasks confirm the effectiveness and generalization of the approach. We believe the work in this paper lays the foundation for further research into the universal benefits of multi-view mid fusion learning."
      },
      {
        "id": "oai:arXiv.org:2507.06033v1",
        "title": "TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision",
        "link": "https://arxiv.org/abs/2507.06033",
        "author": "Syeda Anshrah Gillani, Mirza Samad Ahmed Baig, Osama Ahmed Khan, Shahid Munir Shah, Umema Mujeeb, Maheen Ali",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06033v1 Announce Type: new \nAbstract: The modern text-to-image diffusion models boom has opened a new era in digital content production as it has proven the previously unseen ability to produce photorealistic and stylistically diverse imagery based on the semantics of natural-language descriptions. However, the consistent disadvantage of these models is that they cannot generate readable, meaningful, and correctly spelled text in generated images, which significantly limits the use of practical purposes like advertising, learning, and creative design. This paper introduces a new framework, namely Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), using which a typical diffusion backbone is extended by three well-designed modules. To begin with, the model has a dual-stream text encoder that encodes both semantic contextual information and explicit glyph representations, resulting in a character-aware representation of the input text that is rich in nature. Second, an attention mechanism that is aware of the character is proposed with a new attention segregation loss that aims to limit the attention distribution of each character independently in order to avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning phase, where a full text perceptual loss, directly optimises models to be legible and accurately spell. Large scale experiments to benchmark datasets, such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new state-of-the-art on all metrics, with better character based metrics on text rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality on high-fidelity (FID: 14.3)."
      },
      {
        "id": "oai:arXiv.org:2507.06034v1",
        "title": "The most influential philosophers in Wikipedia: a multicultural analysis",
        "link": "https://arxiv.org/abs/2507.06034",
        "author": "Guillaume Rollin, Jos\\'e Lages",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06034v1 Announce Type: new \nAbstract: We explore the influence and interconnectivity of philosophical thinkers within the Wikipedia knowledge network. Using a dataset of 237 articles dedicated to philosophers across nine different language editions (Arabic, Chinese, English, French, German, Japanese, Portuguese, Russian, and Spanish), we apply the PageRank and CheiRank algorithms to analyze their relative ranking and influence in each linguistic context. Furthermore, we compare our results with entries from the Stanford Encyclopedia of Philosophy and the Internet Encyclopedia of Philosophy, providing insight into the differences between general knowledge networks like Wikipedia and specialized philosophical databases. A key focus of our analysis is the sub-network of 21 presocratic philosophers, grouped into four traditional schools: Italic (Pythagorean + Eleatic), Ionian, Abderian (Atomist), and Sophist. Using the reduced Google matrix method, we uncover both direct and hidden links between these early thinkers, offering new perspectives on their intellectual relationships and influence within the Western philosophical tradition."
      },
      {
        "id": "oai:arXiv.org:2507.06040v1",
        "title": "EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization",
        "link": "https://arxiv.org/abs/2507.06040",
        "author": "Benjamin Hodo (D-ITET, ETH Z\\\"urich, Switzerland), Tommaso Polonelli (D-ITET, ETH Z\\\"urich, Switzerland), Amirhossein Moallemi (RTDT Laboratories, Switzerland), Luca Benini (D-ITET, ETH Z\\\"urich, Switzerland), Michele Magno (D-ITET, ETH Z\\\"urich, Switzerland)",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06040v1 Announce Type: new \nAbstract: We present EdgeCodec, an end-to-end neural compressor for barometric data collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric autoencoder architecture, trained with a discriminator and enhanced by a Residual Vector Quantizer to maximize compression efficiency. It achieves compression rates between 2'560:1 and 10'240:1 while maintaining a reconstruction error below 3%, and operates in real time on the GAP9 microcontroller with bitrates ranging from 11.25 to 45 bits per second. Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly adaptation to varying network conditions. In its highest compression mode, EdgeCodec reduces the energy consumption of wireless data transmission by up to 2.9x, significantly extending the operational lifetime of deployed sensor units."
      },
      {
        "id": "oai:arXiv.org:2507.06056v1",
        "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs",
        "link": "https://arxiv.org/abs/2507.06056",
        "author": "Yizhan Huang, Zhe Yang, Meifang Chen, Jianping Zhang, Michael R. Lyu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06056v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or \"gibberish\", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI)."
      },
      {
        "id": "oai:arXiv.org:2507.06060v1",
        "title": "VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis",
        "link": "https://arxiv.org/abs/2507.06060",
        "author": "Alexandre Symeonidis-Herzig, \\\"Ozge Mercano\\u{g}lu Sincan, Richard Bowden",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06060v1 Announce Type: new \nAbstract: Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars."
      },
      {
        "id": "oai:arXiv.org:2507.06062v1",
        "title": "Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport",
        "link": "https://arxiv.org/abs/2507.06062",
        "author": "Julia Pelzer, Corn\\'e Verburg, Alexander Heinlein, Miriam Schulte",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06062v1 Announce Type: new \nAbstract: Machine learning methods often struggle with real-world applications in science and engineering due to limited or low-quality training data. In this work, the example of groundwater flow with heat transport is considered; this corresponds to an advection-diffusion process under heterogeneous flow conditions, that is, spatially distributed material parameters and heat sources. Classical numerical simulations are costly and challenging due to high spatio-temporal resolution requirements and large domains. While often computationally more efficient, purely data-driven surrogate models face difficulties, particularly in predicting the advection process, which is highly sensitive to input variations and involves long-range spatial interactions. Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN) approach is introduced. It combines a lightweight numerical surrogate for the transport process (global) with convolutional neural networks for the groundwater velocity and heat diffusion processes (local). With the LGCNN, a city-wide subsurface temperature field is modeled, involving a heterogeneous groundwater flow field and one hundred groundwater heat pump injection points forming interacting heat plumes over long distances. The model is first systematically analyzed based on random subsurface input fields. Then, the model is trained on a handful of cut-outs from a real-world subsurface map of the Munich region in Germany, and it scales to larger cut-outs without retraining. All datasets, our code, and trained models are published for reproducibility."
      },
      {
        "id": "oai:arXiv.org:2507.06071v1",
        "title": "MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding",
        "link": "https://arxiv.org/abs/2507.06071",
        "author": "Chang Liu, Ye Pan, Chenyang Ding, Susanto Rahardja, Xiaokang Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06071v1 Announce Type: new \nAbstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip movements and vivid facial expressions. However, most existing approaches focus on static and predefined emotion labels, limiting their diversity and naturalness. To address these challenges, we propose MEDTalk, a novel framework for fine-grained and dynamic emotional talking head generation. Our approach first disentangles content and emotion embedding spaces from motion sequences using a carefully designed cross-reconstruction process, enabling independent control over lip movements and facial expressions. Beyond conventional audio-driven lip synchronization, we integrate audio and speech text, predicting frame-wise intensity variations and dynamically adjusting static emotion features to generate realistic emotional expressions. Furthermore, to enhance control and personalization, we incorporate multimodal inputs-including text descriptions and reference expression images-to guide the generation of user-specified facial expressions. With MetaHuman as the priority, our generated results can be conveniently integrated into the industrial production pipeline."
      },
      {
        "id": "oai:arXiv.org:2507.06072v1",
        "title": "MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding",
        "link": "https://arxiv.org/abs/2507.06072",
        "author": "Tongtong Cheng, Rongzhen Li, Yixin Xiong, Tao Zhang, Jing Wang, Kai Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06072v1 Announce Type: new \nAbstract: Accurate driving behavior recognition and reasoning are critical for autonomous driving video understanding. However, existing methods often tend to dig out the shallow causal, fail to address spurious correlations across modalities, and ignore the ego-vehicle level causality modeling. To overcome these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM) that constructs latent causal structures between visual and language modalities. Firstly, we design a multi-level feature extractor to capture long-range dependencies. Secondly, we design a causal analysis module that dynamically models driving scenarios using a directed acyclic graph (DAG) of driving states. Thirdly, we utilize a vision-language transformer to align critical visual features with their corresponding linguistic expressions. Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM achieves SOTA performance in visual-language causal relationship learning. Furthermore, the model exhibits superior capability in capturing causal characteristics within video sequences, showcasing its effectiveness for autonomous driving applications. The code is available at https://github.com/SixCorePeach/MCAM."
      },
      {
        "id": "oai:arXiv.org:2507.06075v1",
        "title": "Discontinuity-aware Normal Integration for Generic Central Camera Models",
        "link": "https://arxiv.org/abs/2507.06075",
        "author": "Francesco Milano, Manuel L\\'opez-Antequera, Naina Dhingra, Roland Siegwart, Robert Thiel",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06075v1 Announce Type: new \nAbstract: Recovering a 3D surface from its surface normal map, a problem known as normal integration, is a key component for photometric shape reconstruction techniques such as shape-from-shading and photometric stereo. The vast majority of existing approaches for normal integration handle only implicitly the presence of depth discontinuities and are limited to orthographic or ideal pinhole cameras. In this paper, we propose a novel formulation that allows modeling discontinuities explicitly and handling generic central cameras. Our key idea is based on a local planarity assumption, that we model through constraints between surface normals and ray directions. Compared to existing methods, our approach more accurately approximates the relation between depth and surface normals, achieves state-of-the-art results on the standard normal integration benchmark, and is the first to directly handle generic central camera models."
      },
      {
        "id": "oai:arXiv.org:2507.06078v1",
        "title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models",
        "link": "https://arxiv.org/abs/2507.06078",
        "author": "Chihan Huang, Hao Tang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06078v1 Announce Type: new \nAbstract: Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures."
      },
      {
        "id": "oai:arXiv.org:2507.06079v1",
        "title": "QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models",
        "link": "https://arxiv.org/abs/2507.06079",
        "author": "Sebastian Siegel, Ming-Jay Yang, Younes Bouhadjar, Maxime Fabre, Emre Neftci, John Paul Strachan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06079v1 Announce Type: new \nAbstract: Structured State Space models (SSM) have recently emerged as a new class of deep learning models, particularly well-suited for processing long sequences. Their constant memory footprint, in contrast to the linearly scaling memory demands of Transformers, makes them attractive candidates for deployment on resource-constrained edge-computing devices. While recent works have explored the effect of quantization-aware training (QAT) on SSMs, they typically do not address its implications for specialized edge hardware, for example, analog in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can significantly reduce the complexity of SSMs by up to two orders of magnitude across various performance metrics. We analyze the relation between model size and numerical precision, and show that QAT enhances robustness to analog noise and enables structural pruning. Finally, we integrate these techniques to deploy SSMs on a memristive analog in-memory computing substrate and highlight the resulting benefits in terms of computational efficiency."
      },
      {
        "id": "oai:arXiv.org:2507.06080v1",
        "title": "CAST-Phys: Contactless Affective States Through Physiological signals Database",
        "link": "https://arxiv.org/abs/2507.06080",
        "author": "Joaquim Comas, Alexander Joel Vera, Xavier Vives, Eleonora De Filippi, Alexandre Pereda, Federico Sukno",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06080v1 Announce Type: new \nAbstract: In recent years, affective computing and its applications have become a fast-growing research topic. Despite significant advancements, the lack of affective multi-modal datasets remains a major bottleneck in developing accurate emotion recognition systems. Furthermore, the use of contact-based devices during emotion elicitation often unintentionally influences the emotional experience, reducing or altering the genuine spontaneous emotional response. This limitation highlights the need for methods capable of extracting affective cues from multiple modalities without physical contact, such as remote physiological emotion recognition. To address this, we present the Contactless Affective States Through Physiological Signals Database (CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal remote physiological emotion recognition using facial and physiological cues. The dataset includes diverse physiological signals, such as photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate (RR), alongside high-resolution uncompressed facial video recordings, enabling the potential for remote signal recovery. Our analysis highlights the crucial role of physiological signals in realistic scenarios where facial expressions alone may not provide sufficient emotional information. Furthermore, we demonstrate the potential of remote multi-modal emotion recognition by evaluating the impact of individual and fused modalities, showcasing its effectiveness in advancing contactless emotion recognition technologies."
      },
      {
        "id": "oai:arXiv.org:2507.06085v1",
        "title": "A Survey on Prompt Tuning",
        "link": "https://arxiv.org/abs/2507.06085",
        "author": "Zongqian Li, Yixuan Su, Nigel Collier",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06085v1 Announce Type: new \nAbstract: This survey reviews prompt tuning, a parameter-efficient approach for adapting language models by prepending trainable continuous vectors while keeping the model frozen. We classify existing approaches into two categories: direct prompt learning and transfer learning. Direct prompt learning methods include: general optimization approaches, encoder-based methods, decomposition strategies, and mixture-of-experts frameworks. Transfer learning methods consist of: general transfer approaches, encoder-based methods, and decomposition strategies. For each method, we analyze method designs, innovations, insights, advantages, and disadvantages, with illustrative visualizations comparing different frameworks. We identify challenges in computational efficiency and training stability, and discuss future directions in improving training robustness and broadening application scope."
      },
      {
        "id": "oai:arXiv.org:2507.06086v1",
        "title": "QuHE: Optimizing Utility-Cost in Quantum Key Distribution and Homomorphic Encryption Enabled Secure Edge Computing Networks",
        "link": "https://arxiv.org/abs/2507.06086",
        "author": "Liangxin Qian, Yang Li, Jun Zhao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06086v1 Announce Type: new \nAbstract: Ensuring secure and efficient data processing in mobile edge computing (MEC) systems is a critical challenge. While quantum key distribution (QKD) offers unconditionally secure key exchange and homomorphic encryption (HE) enables privacy-preserving data processing, existing research fails to address the comprehensive trade-offs among QKD utility, HE security, and system costs. This paper proposes a novel framework integrating QKD, transciphering, and HE for secure and efficient MEC. QKD distributes symmetric keys, transciphering bridges symmetric encryption, and HE processes encrypted data at the server. We formulate an optimization problem balancing QKD utility, HE security, processing and wireless transmission costs. However, the formulated optimization is non-convex and NPhard. To solve it efficiently, we propose the Quantum-enhanced Homomorphic Encryption resource allocation (QuHE) algorithm. Theoretical analysis proves the proposed QuHE algorithm's convergence and optimality, and simulations demonstrate its effectiveness across multiple performance metrics."
      },
      {
        "id": "oai:arXiv.org:2507.06087v1",
        "title": "CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs",
        "link": "https://arxiv.org/abs/2507.06087",
        "author": "Haoxi Li, Sikai Bai, Jie Zhang, Song Guo",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06087v1 Announce Type: new \nAbstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in domains like mathematics and program synthesis. Despite their strong performance, LRMs often exhibit overthinking -- excessive and redundant reasoning steps that introduce inefficiencies during inference. This phenomenon raises an important question for LRM self-evaluation: How can a model autonomously assess the correctness of its own reasoning trajectory without external labels? To address this, we propose Chain-of-Reasoning Embedding (CoRE), a series of hidden states in latent space to enable label-free self-evaluation on intermediate reasoning steps of LRMs, so as to enhance metacognition abilities for improved reasoning efficiency. By analyzing the geometric properties of the CoRE trajectories, we reveal that redundant reasoning usually presents cyclical fluctuations, which correspond to repetitive and unconscious reflection/exploration. Leveraging this insight, we further introduce a training-free, label-free self-evaluation framework, CoRE-Eval, to detect such patterns and dynamically determine whether to terminate reasoning early. Extensive experiments on mathematical reasoning benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2% while improving answer accuracy by around 10%, achieving 70.0% accuracy on the challenging AIME benchmark with the 32B model."
      },
      {
        "id": "oai:arXiv.org:2507.06093v1",
        "title": "Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification",
        "link": "https://arxiv.org/abs/2507.06093",
        "author": "Murilo Gustineli, Anthony Miyaguchi, Adrian Cheung, Divyansh Khattak",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06093v1 Announce Type: new \nAbstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on multi-species plant identification in vegetation quadrat images. Our pipeline combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's 518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP + K-Means visual clustering and geolocation filtering. Tile predictions are aggregated by majority vote and re-weighted with cluster-specific Bayesian priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while requiring no additional training. All code, configuration files, and reproducibility scripts are publicly available at https://github.com/dsgt-arc/plantclef-2025."
      },
      {
        "id": "oai:arXiv.org:2507.06103v1",
        "title": "Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering",
        "link": "https://arxiv.org/abs/2507.06103",
        "author": "Jiayi Song, Zihan Ye, Qingyuan Zhou, Weidong Yang, Ben Fei, Jingyi Xu, Ying He, Wanli Ouyang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06103v1 Announce Type: new \nAbstract: Accurately rendering scenes with reflective surfaces remains a significant challenge in novel view synthesis, as existing methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections as physical geometry, resulting in degraded reconstructions. Previous methods rely on incomplete and non-generalizable geometric constraints, leading to misalignment between the positions of Gaussian splats and the actual scene geometry. When dealing with real-world scenes containing complex geometry, the accumulation of Gaussians further exacerbates surface artifacts and results in blurred reconstructions. To address these limitations, in this work, we propose Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D Gaussian Splatting, which explicitly disentangles transmitted and reflected components to better capture complex reflections and enhance geometric consistency in real-world scenes. Our approach employs a dual-branch representation with high-order spherical harmonics to capture high-frequency reflective details, alongside a reflection removal module providing pseudo reflection-free supervision to guide clean decomposition. Additionally, we incorporate pseudo-depth maps and a geometry-aware bilateral smoothness constraint to enhance 3D geometric consistency and stability in decomposition. Extensive experiments demonstrate that Ref-Unlock significantly outperforms classical GS-based reflection methods and achieves competitive results with NeRF-based models, while enabling flexible vision foundation models (VFMs) driven reflection editing. Our method thus offers an efficient and generalizable solution for realistic rendering of reflective scenes. Our code is available at https://ref-unlock.github.io/."
      },
      {
        "id": "oai:arXiv.org:2507.06111v1",
        "title": "Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation",
        "link": "https://arxiv.org/abs/2507.06111",
        "author": "Mohamad H. Danesh, Maxime Wabartha, Stanley Wu, Joelle Pineau, Hsiu-Chin Lin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06111v1 Announce Type: new \nAbstract: Deploying reinforcement learning (RL) policies in real-world involves significant challenges, including distribution shifts, safety concerns, and the impracticality of direct interactions during policy refinement. Existing methods, such as domain randomization (DR) and off-dynamics RL, enhance policy robustness by direct interaction with the target domain, an inherently unsafe practice. We propose Uncertainty-Aware RL (UARL), a novel framework that prioritizes safety during training by addressing Out-Of-Distribution (OOD) detection and policy adaptation without requiring direct interactions in target domain. UARL employs an ensemble of critics to quantify policy uncertainty and incorporates progressive environmental randomization to prepare the policy for diverse real-world conditions. By iteratively refining over high-uncertainty regions of the state space in simulated environments, UARL enhances robust generalization to the target domain without explicitly training on it. We evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its effectiveness in reliable OOD detection, improved performance, and enhanced sample efficiency compared to baselines."
      },
      {
        "id": "oai:arXiv.org:2507.06119v1",
        "title": "Omni-Video: Democratizing Unified Video Understanding and Generation",
        "link": "https://arxiv.org/abs/2507.06119",
        "author": "Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, Hao Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06119v1 Announce Type: new \nAbstract: Notable breakthroughs in unified understanding and generation modeling have led to remarkable advancements in image understanding, reasoning, production and editing, yet current foundational models predominantly focus on processing images, creating a gap in the development of unified models for video understanding and generation. This report presents Omni-Video, an efficient and effective unified framework for video understanding, generation, as well as instruction-based editing. Our key insight is to teach existing multimodal large language models (MLLMs) to produce continuous visual clues that are used as the input of diffusion decoders, which produce high-quality videos conditioned on these visual clues. To fully unlock the potential of our system for unified video modeling, we integrate several technical improvements: 1) a lightweight architectural design that respectively attaches a vision head on the top of MLLMs and a adapter before the input of diffusion decoders, the former produce visual tokens for the latter, which adapts these visual tokens to the conditional space of diffusion decoders; and 2) an efficient multi-stage training scheme that facilitates a fast connection between MLLMs and diffusion decoders with limited data and computational resources. We empirically demonstrate that our model exhibits satisfactory generalization abilities across video generation, editing and understanding tasks."
      },
      {
        "id": "oai:arXiv.org:2507.06125v1",
        "title": "Subspace-based Approximate Hessian Method for Zeroth-Order Optimization",
        "link": "https://arxiv.org/abs/2507.06125",
        "author": "Dongyoon Kim, Sungjae Lee, Wonjin Lee, Kwang In Kim",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06125v1 Announce Type: new \nAbstract: Zeroth-order optimization addresses problems where gradient information is inaccessible or impractical to compute. While most existing methods rely on first-order approximations, incorporating second-order (curvature) information can, in principle, significantly accelerate convergence. However, the high cost of function evaluations required to estimate Hessian matrices often limits practical applicability. We present the subspace-based approximate Hessian (ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these costs by focusing on randomly selected two-dimensional subspaces. Within each subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the objective function and extracting its second-order coefficients. To further reduce function-query costs, ZO-SAH employs a periodic subspace-switching strategy that reuses function evaluations across optimization steps. Experiments on eight benchmark datasets, including logistic regression and deep neural network training tasks, demonstrate that ZO-SAH achieves significantly faster convergence than existing zeroth-order methods."
      },
      {
        "id": "oai:arXiv.org:2507.06137v1",
        "title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
        "link": "https://arxiv.org/abs/2507.06137",
        "author": "Mohammad Mahdi Derakhshani, Dheeraj Varghese, Marzieh Fadaee, Cees G. M. Snoek",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06137v1 Announce Type: new \nAbstract: Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI."
      },
      {
        "id": "oai:arXiv.org:2507.06138v1",
        "title": "Coding Triangle: How Does Large Language Model Understand Code?",
        "link": "https://arxiv.org/abs/2507.06138",
        "author": "Taolin Zhang, Zihan Ma, Maosong Cao, Junnan Liu, Songyang Zhang, Kai Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06138v1 Announce Type: new \nAbstract: Large language models (LLMs) have achieved remarkable progress in code generation, yet their true programming competence remains underexplored. We introduce the Code Triangle framework, which systematically evaluates LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation. Through extensive experiments on competitive programming benchmarks, we reveal that while LLMs can form a self-consistent system across these dimensions, their solutions often lack the diversity and robustness of human programmers. We identify a significant distribution shift between model cognition and human expertise, with model errors tending to cluster due to training data biases and limited reasoning transfer. Our study demonstrates that incorporating human-generated editorials, solutions, and diverse test cases, as well as leveraging model mixtures, can substantially enhance both the performance and robustness of LLMs. Furthermore, we reveal both the consistency and inconsistency in the cognition of LLMs that may facilitate self-reflection and self-improvement, providing a potential direction for developing more powerful coding models."
      },
      {
        "id": "oai:arXiv.org:2507.06139v1",
        "title": "Topic Modeling and Link-Prediction for Material Property Discovery",
        "link": "https://arxiv.org/abs/2507.06139",
        "author": "Ryan C. Barron, Maksim E. Eren, Valentin Stanev, Cynthia Matuszek, Boian S. Alexandrov",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06139v1 Announce Type: new \nAbstract: Link prediction infers missing or future relations between graph nodes, based on connection patterns. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links between entities. We present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization (BNMFk) with automatic model selection, as well as Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). These materials are studied in a variety of physics fields with many current and potential applications.\n  An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent topics like superconductivity, energy storage, and tribology. Also, missing or weakly connected links are highlight between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and show the model predicts associations with the superconducting TMD clusters. This shows the method finds hidden connections in a graph of material to latent topic associations built from scientific literature, especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for human-in-the-loop scientific discovery."
      },
      {
        "id": "oai:arXiv.org:2507.06146v1",
        "title": "Prompt-Free Conditional Diffusion for Multi-object Image Augmentation",
        "link": "https://arxiv.org/abs/2507.06146",
        "author": "Haoyu Wang, Lei Zhang, Wei Wei, Chen Ding, Yanning Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06146v1 Announce Type: new \nAbstract: Diffusion models has underpinned much recent advances of dataset augmentation in various computer vision tasks. However, when involving generating multi-object images as real scenarios, most existing methods either rely entirely on text condition, resulting in a deviation between the generated objects and the original data, or rely too much on the original images, resulting in a lack of diversity in the generated images, which is of limited help to downstream tasks. To mitigate both problems with one stone, we propose a prompt-free conditional diffusion framework for multi-object image augmentation. Specifically, we introduce a local-global semantic fusion strategy to extract semantics from images to replace text, and inject knowledge into the diffusion model through LoRA to alleviate the category deviation between the original model and the target dataset. In addition, we design a reward model based counting loss to assist the traditional reconstruction loss for model training. By constraining the object counts of each category instead of pixel-by-pixel constraints, bridging the quantity deviation between the generated data and the original data while improving the diversity of the generated data. Experimental results demonstrate the superiority of the proposed method over several representative state-of-the-art baselines and showcase strong downstream task gain and out-of-domain generalization capabilities. Code is available at \\href{https://github.com/00why00/PFCD}{here}."
      },
      {
        "id": "oai:arXiv.org:2507.06148v1",
        "title": "SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance",
        "link": "https://arxiv.org/abs/2507.06148",
        "author": "Mustafa Bayram G\\\"ucen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06148v1 Announce Type: new \nAbstract: In this study, SoftReMish, a new activation function designed to improve the performance of convolutional neural networks (CNNs) in image classification tasks, is proposed. Using the MNIST dataset, a standard CNN architecture consisting of two convolutional layers, max pooling, and fully connected layers was implemented. SoftReMish was evaluated against popular activation functions including ReLU, Tanh, and Mish by replacing the activation function in all trainable layers. The model performance was assessed in terms of minimum training loss and maximum validation accuracy. Results showed that SoftReMish achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%), outperforming all other functions tested. These findings demonstrate that SoftReMish offers better convergence behavior and generalization capability, making it a promising candidate for visual recognition tasks."
      },
      {
        "id": "oai:arXiv.org:2507.06152v1",
        "title": "Aliasing in Convnets: A Frame-Theoretic Perspective",
        "link": "https://arxiv.org/abs/2507.06152",
        "author": "Daniel Haider, Vincent Lostanlen, Martin Ehler, Nicki Holighaus, Peter Balazs",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06152v1 Announce Type: new \nAbstract: Using a stride in a convolutional layer inherently introduces aliasing, which has implications for numerical stability and statistical generalization. While techniques such as the parametrizations via paraunitary systems have been used to promote orthogonal convolution and thus ensure Parseval stability, a general analysis of aliasing and its effects on the stability has not been done in this context. In this article, we adapt a frame-theoretic approach to describe aliasing in convolutional layers with 1D kernels, leading to practical estimates for stability bounds and characterizations of Parseval stability, that are tailored to take short kernel sizes into account. From this, we derive two computationally very efficient optimization objectives that promote Parseval stability via systematically suppressing aliasing. Finally, for layers with random kernels, we derive closed-form expressions for the expected value and variance of the terms that describe the aliasing effects, revealing fundamental insights into the aliasing behavior at initialization."
      },
      {
        "id": "oai:arXiv.org:2507.06161v1",
        "title": "Normalizing Diffusion Kernels with Optimal Transport",
        "link": "https://arxiv.org/abs/2507.06161",
        "author": "Nathan Kessler, Robin Magnet, Jean Feydy",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06161v1 Announce Type: new \nAbstract: Smoothing a signal based on local neighborhoods is a core operation in machine learning and geometry processing. On well-structured domains such as vector spaces and manifolds, the Laplace operator derived from differential geometry offers a principled approach to smoothing via heat diffusion, with strong theoretical guarantees. However, constructing such Laplacians requires a carefully defined domain structure, which is not always available. Most practitioners thus rely on simple convolution kernels and message-passing layers, which are biased against the boundaries of the domain. We bridge this gap by introducing a broad class of smoothing operators, derived from general similarity or adjacency matrices, and demonstrate that they can be normalized into diffusion-like operators that inherit desirable properties from Laplacians. Our approach relies on a symmetric variant of the Sinkhorn algorithm, which rescales positive smoothing operators to match the structural behavior of heat diffusion. This construction enables Laplacian-like smoothing and processing of irregular data such as point clouds, sparse voxel grids or mixture of Gaussians. We show that the resulting operators not only approximate heat diffusion but also retain spectral information from the Laplacian itself, with applications to shape analysis and matching."
      },
      {
        "id": "oai:arXiv.org:2507.06164v1",
        "title": "Critical Nodes Identification in Complex Networks: A Survey",
        "link": "https://arxiv.org/abs/2507.06164",
        "author": "Duxin Chen, Jiawen Chen, Xiaoyu Zhang, Qinghan Jia, Xiaolu Liu, Ye Sun, Linyuan Lv, Wenwu Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06164v1 Announce Type: new \nAbstract: Complex networks have become essential tools for understanding diverse phenomena in social systems, traffic systems, biomolecular systems, and financial systems. Identifying critical nodes is a central theme in contemporary research, serving as a vital bridge between theoretical foundations and practical applications. Nevertheless, the intrinsic complexity and structural heterogeneity characterizing real-world networks, with particular emphasis on dynamic and higher-order networks, present substantial obstacles to the development of universal frameworks for critical node identification. This paper provides a comprehensive review of critical node identification techniques, categorizing them into seven main classes: centrality, critical nodes deletion problem, influence maximization, network control, artificial intelligence, higher-order and dynamic methods. Our review bridges the gaps in existing surveys by systematically classifying methods based on their methodological foundations and practical implications, and by highlighting their strengths, limitations, and applicability across different network types. Our work enhances the understanding of critical node research by identifying key challenges, such as algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and highlights open questions, particularly in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning approaches, and developing scalable and interpretable metrics for complex systems."
      },
      {
        "id": "oai:arXiv.org:2507.06165v1",
        "title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion",
        "link": "https://arxiv.org/abs/2507.06165",
        "author": "Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, Xihui Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06165v1 Announce Type: new \nAbstract: The creation of 3D assets with explicit, editable part structures is crucial for advancing interactive applications, yet most generative methods produce only monolithic shapes, limiting their utility. We introduce OmniPart, a novel framework for part-aware 3D object generation designed to achieve high semantic decoupling among components while maintaining robust structural cohesion. OmniPart uniquely decouples this complex task into two synergistic stages: (1) an autoregressive structure planning module generates a controllable, variable-length sequence of 3D part bounding boxes, critically guided by flexible 2D part masks that allow for intuitive control over part decomposition without requiring direct correspondences or semantic labels; and (2) a spatially-conditioned rectified flow model, efficiently adapted from a pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and consistently within the planned layout. Our approach supports user-defined part granularity, precise localization, and enables diverse downstream applications. Extensive experiments demonstrate that OmniPart achieves state-of-the-art performance, paving the way for more interpretable, editable, and versatile 3D content."
      },
      {
        "id": "oai:arXiv.org:2507.06167v1",
        "title": "Skywork-R1V3 Technical Report",
        "link": "https://arxiv.org/abs/2507.06167",
        "author": "Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Yahui Zhou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06167v1 Announce Type: new \nAbstract: We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the model's reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities."
      },
      {
        "id": "oai:arXiv.org:2507.06173v1",
        "title": "A Method for Optimizing Connections in Differentiable Logic Gate Networks",
        "link": "https://arxiv.org/abs/2507.06173",
        "author": "Wout Mommen, Lars Keuninckx, Matthias Hartmann, Piet Wambacq",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06173v1 Announce Type: new \nAbstract: We introduce a novel method for partial optimization of the connections in Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a probability distribution over a subset of connections per gate input, selecting the connection with highest merit, after which the gate-types are selected. We show that the connection-optimized LGNs outperform standard fixed-connection LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only a fraction of the number of logic gates. When training all connections, we demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on the MNIST data set. Additionally, we show that our network has 24 times fewer gates, while performing better on the MNIST data set compared to standard fully connected LGNs. As such, our work shows a pathway towards fully trainable Boolean logic."
      },
      {
        "id": "oai:arXiv.org:2507.06181v1",
        "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization",
        "link": "https://arxiv.org/abs/2507.06181",
        "author": "Zhongyuan Peng, Yifan Yao, Kaijing Ma, Shuyue Guo, Yizhe Li, Yichi Zhang, Chenchen Zhang, Yifan Zhang, Zhouliang Yu, Luming Li, Minghao Liu, Yihang Xia, Jiawei Shen, Yuchen Wu, Yixin Cao, Zhaoxiang Zhang, Wenhao Huang, Jiaheng Liu, Ge Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06181v1 Announce Type: new \nAbstract: Translating natural language mathematical statements into formal, executable code is a fundamental challenge in automated theorem proving. While prior work has focused on generation and compilation success, little attention has been paid to the critic phase-the evaluation of whether generated formalizations truly capture the semantic intent of the original problem. In this paper, we introduce CriticLean, a novel critic-guided reinforcement learning framework that elevates the role of the critic from a passive validator to an active learning component. Specifically, first, we propose the CriticLeanGPT, trained via supervised fine-tuning and reinforcement learning, to rigorously assess the semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench, a benchmark designed to measure models' ability to distinguish semantically correct from incorrect formalizations, and demonstrate that our trained CriticLeanGPT models can significantly outperform strong open- and closed-source baselines. Building on the CriticLean framework, we construct FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich domain diversity, broad difficulty coverage, and high correctness based on human evaluation. Overall, our findings highlight that optimizing the critic phase is essential for producing reliable formalizations, and we hope our CriticLean will provide valuable insights for future advances in formal mathematical reasoning."
      },
      {
        "id": "oai:arXiv.org:2507.06183v1",
        "title": "Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling",
        "link": "https://arxiv.org/abs/2507.06183",
        "author": "Prahitha Movva, Naga Harshita Marupaka",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06183v1 Announce Type: new \nAbstract: Technical reports and articles often contain valuable information in the form of semi-structured data like charts, and figures. Interpreting these and using the information from them is essential for downstream tasks such as question answering (QA). Current approaches to visual question answering often struggle with the precision required for scientific data interpretation, particularly in handling numerical values, multi-step reasoning over visual elements, and maintaining consistency between visual observation and textual reasoning. We present our approach to the SciVQA 2025 shared task, focusing on answering visual and non-visual questions grounded in scientific figures from scholarly articles.\n  We conducted a series of experiments using models with 5B to 8B parameters. Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1 scores of \\textbf{0.740} and a BERTScore of \\textbf{0.983} on the SciVQA test split. We also developed an ensemble model with multiple vision language models (VLMs). Through error analysis on the validation split, our ensemble approach improved performance compared to most individual models, though InternVL3 remained the strongest standalone performer. Our findings underscore the effectiveness of prompt optimization, chain-of-thought reasoning and ensemble modeling in improving the model's ability in visual question answering."
      },
      {
        "id": "oai:arXiv.org:2507.06189v1",
        "title": "DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation",
        "link": "https://arxiv.org/abs/2507.06189",
        "author": "Maximilian Heil, Dionne Bang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06189v1 Announce Type: new \nAbstract: This paper presents our submission to Task 1, Subjectivity Detection, of the CheckThat! Lab at CLEF 2025. We investigate the effectiveness of transfer-learning and stylistic data augmentation to improve classification of subjective and objective sentences in English news text. Our approach contrasts fine-tuning of pre-trained encoders and transfer-learning of fine-tuned transformer on related tasks. We also introduce a controlled augmentation pipeline using GPT-4o to generate paraphrases in predefined subjectivity styles. To ensure label and style consistency, we employ the same model to correct and refine the generated samples. Results show that transfer-learning of specified encoders outperforms fine-tuning general-purpose ones, and that carefully curated augmentation significantly enhances model robustness, especially in detecting subjective content. Our official submission placed us $16^{th}$ of 24 participants. Overall, our findings underscore the value of combining encoder specialization with label-consistent augmentation for improved subjectivity detection. Our code is available at https://github.com/dsgt-arc/checkthat-2025-subject."
      },
      {
        "id": "oai:arXiv.org:2507.06195v1",
        "title": "DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification",
        "link": "https://arxiv.org/abs/2507.06195",
        "author": "Maximilian Heil, Aleksandar Pramov",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06195v1 Announce Type: new \nAbstract: Numerical claims, statements involving quantities, comparisons, and temporal references, pose unique challenges for automated fact-checking systems. In this study, we evaluate modeling strategies for veracity prediction of such claims using the QuanTemp dataset and building our own evidence retrieval pipeline. We investigate three key factors: (1) the impact of more evidences with longer input context windows using ModernBERT, (2) the effect of right-to-left (R2L) tokenization, and (3) their combined influence on classification performance. Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does not boost natural language inference (NLI) of numerical tasks. A longer context window does also not enhance veracity performance either, highlighting evidence quality as the dominant bottleneck. Our best-performing system achieves competitive macro-average F1 score of 0.57 and places us among the Top-4 submissions in Task 3 of CheckThat! 2025. Our code is available at https://github.com/dsgt-arc/checkthat-2025-numerical."
      },
      {
        "id": "oai:arXiv.org:2507.06196v1",
        "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language Models",
        "link": "https://arxiv.org/abs/2507.06196",
        "author": "Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06196v1 Announce Type: new \nAbstract: Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs."
      },
      {
        "id": "oai:arXiv.org:2507.06203v1",
        "title": "A Survey on Latent Reasoning",
        "link": "https://arxiv.org/abs/2507.06203",
        "author": "Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06203v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: https://github.com/multimodal-art-projection/LatentCoT-Horizon/."
      },
      {
        "id": "oai:arXiv.org:2507.06204v1",
        "title": "Differential Mamba",
        "link": "https://arxiv.org/abs/2507.06204",
        "author": "Nadav Schneider, Itamar Zimerman, Eliya Nachmani",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06204v1 Announce Type: new \nAbstract: Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available."
      },
      {
        "id": "oai:arXiv.org:2507.06205v1",
        "title": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media",
        "link": "https://arxiv.org/abs/2507.06205",
        "author": "Ayush Parikh, Hoang Thanh Thanh Truong, Jeanette Schofield, Maximilian Heil",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06205v1 Announce Type: new \nAbstract: In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a Scientific Web Discourse Detection, present the methods we explored for this task. For this multiclass classification task, we determined if a tweet contained a scientific claim, a reference to a scientific study or publication, and/or mentions of scientific entities, such as a university or a scientist. We present 3 modeling approaches for this task: transformer finetuning, few-shot prompting of LLMs, and a combined ensemble model whose design was informed by earlier experiments. Our team placed 7th in the competition, achieving a macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline of 0.8375. Our code is available on Github at https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a."
      },
      {
        "id": "oai:arXiv.org:2507.06210v1",
        "title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions",
        "link": "https://arxiv.org/abs/2507.06210",
        "author": "Yuchen Huang, Zhiyuan Fan, Zhitao He, Sandeep Polisetty, Wenyan Li, Yi R. Fung",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06210v1 Announce Type: new \nAbstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal understanding but struggle with contextually relevant fine-grained visual features, making it difficult to distinguish visually similar yet culturally distinct concepts. This limitation stems from the scarcity of high-quality culture-specific datasets, the lack of integrated contextual knowledge, and the absence of hard negatives highlighting subtle distinctions. To address these challenges, we first design a data curation pipeline that leverages open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a synthetic cultural dataset. This dataset consists of paired concept-caption-image triplets, where concepts visually resemble each other but represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to create CultureCLIP, which aligns cultural concepts with contextually enhanced captions and synthetic images through customized contrastive learning, enabling finer cultural differentiation while preserving generalization capabilities. Experiments on culturally relevant benchmarks show that CultureCLIP outperforms the base CLIP, achieving up to a notable 5.49% improvement in fine-grained concept recognition on certain tasks, while preserving CLIP's original generalization ability, validating the effectiveness of our data synthesis and VLM backbone training paradigm in capturing subtle cultural distinctions."
      },
      {
        "id": "oai:arXiv.org:2507.06211v1",
        "title": "Modern Methods in Associative Memory",
        "link": "https://arxiv.org/abs/2507.06211",
        "author": "Dmitry Krotov, Benjamin Hoover, Parikshit Ram, Bao Pham",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06211v1 Announce Type: new \nAbstract: Associative Memories like the famous Hopfield Networks are elegant models for describing fully recurrent neural networks whose fundamental job is to store and retrieve information. In the past few years they experienced a surge of interest due to novel theoretical results pertaining to their information storage capabilities, and their relationship with SOTA AI architectures, such as Transformers and Diffusion Models. These connections open up possibilities for interpreting the computation of traditional AI networks through the theoretical lens of Associative Memories. Additionally, novel Lagrangian formulations of these networks make it possible to design powerful distributed models that learn useful representations and inform the design of novel architectures. This tutorial provides an approachable introduction to Associative Memories, emphasizing the modern language and methods used in this area of research, with practical hands-on mathematical derivations and coding notebooks."
      },
      {
        "id": "oai:arXiv.org:2507.06222v1",
        "title": "Deep Learning Optimization of Two-State Pinching Antennas Systems",
        "link": "https://arxiv.org/abs/2507.06222",
        "author": "Odysseas G. Karagiannidis, Victoria E. Galanopoulou, Panagiotis D. Diamantoulakis, Zhiguo Ding, Octavia Dobre",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06222v1 Announce Type: new \nAbstract: The evolution of wireless communication systems requires flexible, energy-efficient, and cost-effective antenna technologies. Pinching antennas (PAs), which can dynamically control electromagnetic wave propagation through binary activation states, have recently emerged as a promising candidate. In this work, we investigate the problem of optimally selecting a subset of fixed-position PAs to activate in a waveguide, when the aim is to maximize the communication rate at a user terminal. Due to the complex interplay between antenna activation, waveguide-induced phase shifts, and power division, this problem is formulated as a combinatorial fractional 0-1 quadratic program. To efficiently solve this challenging problem, we use neural network architectures of varying complexity to learn activation policies directly from data, leveraging spatial features and signal structure. Furthermore, we incorporate user location uncertainty into our training and evaluation pipeline to simulate realistic deployment conditions. Simulation results demonstrate the effectiveness and robustness of the proposed models."
      },
      {
        "id": "oai:arXiv.org:2507.06223v1",
        "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
        "link": "https://arxiv.org/abs/2507.06223",
        "author": "Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao, Yi Fang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06223v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community."
      },
      {
        "id": "oai:arXiv.org:2507.06229v1",
        "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving",
        "link": "https://arxiv.org/abs/2507.06229",
        "author": "Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06229v1 Announce Type: new \nAbstract: As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other's experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks."
      },
      {
        "id": "oai:arXiv.org:2507.06230v1",
        "title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion",
        "link": "https://arxiv.org/abs/2507.06230",
        "author": "Aleksandar Jevti\\'c, Christoph Reich, Felix Wimbauer, Oliver Hahn, Christian Rupprecht, Stefan Roth, Daniel Cremers",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06230v1 Announce Type: new \nAbstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding."
      },
      {
        "id": "oai:arXiv.org:2507.06231v1",
        "title": "RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models",
        "link": "https://arxiv.org/abs/2507.06231",
        "author": "Keyan Chen, Chenyang Liu, Bowen Chen, Jiafan Zhang, Zhengxia Zou, Zhenwei Shi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06231v1 Announce Type: new \nAbstract: Referring Remote Sensing Image Segmentation provides a flexible and fine-grained framework for remote sensing scene analysis via vision-language collaborative interpretation. Current approaches predominantly utilize a three-stage pipeline encompassing dual-modal encoding, cross-modal interaction, and pixel decoding. These methods demonstrate significant limitations in managing complex semantic relationships and achieving precise cross-modal alignment, largely due to their coupled processing mechanism that conflates target localization with boundary delineation. This architectural coupling amplifies error propagation under semantic ambiguity while restricting model generalizability and interpretability. To address these issues, we propose RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow into a collaborative dual-stage framework: coarse localization followed by fine segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with SAM's segmentation generalizability through strategic foundation model collaboration. Specifically, CLIP is employed as the dual-modal encoder to activate target features within its pre-aligned semantic space and generate localization prompts. To mitigate CLIP's misactivation challenges in multi-entity scenarios described by referring texts, a cascaded second-order prompter is devised, which enhances precision through implicit reasoning via decomposition of text embeddings into complementary semantic subspaces. These optimized semantic prompts subsequently direct the SAM to generate pixel-level refined masks, thereby completing the semantic transmission pipeline. Extensive experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2 surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex semantic interpretation. Code is available at: https://github.com/KyanChen/RSRefSeg2."
      },
      {
        "id": "oai:arXiv.org:2507.06233v1",
        "title": "Learning to Track Any Points from Human Motion",
        "link": "https://arxiv.org/abs/2507.06233",
        "author": "In\\`es Hyeonsu Kim, Seokju Cho, Jahyeok Koo, Junghyun Park, Jiahui Huang, Joon-Young Lee, Seungryong Kim",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06233v1 Announce Type: new \nAbstract: Human motion, with its inherent complexities, such as non-rigid deformations, articulated movements, clothing distortions, and frequent occlusions caused by limbs or other individuals, provides a rich and challenging source of supervision that is crucial for training robust and generalizable point trackers. Despite the suitability of human motion, acquiring extensive training data for point tracking remains difficult due to laborious manual annotation. Our proposed pipeline, AnthroTAP, addresses this by proposing an automated pipeline to generate pseudo-labeled training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected humans in video frames, project the resulting 3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle occlusions using ray-casting, and filter out unreliable tracks based on optical flow consistency. A point tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other models trained on real videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to 256 GPUs used in recent state-of-the-art."
      },
      {
        "id": "oai:arXiv.org:2506.22521v1",
        "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
        "link": "https://arxiv.org/abs/2506.22521",
        "author": "Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, Yushun Dong",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.22521v1 Announce Type: cross \nAbstract: Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments."
      },
      {
        "id": "oai:arXiv.org:2507.05265v1",
        "title": "BMFM-DNA: A SNP-aware DNA foundation model to capture variant effects",
        "link": "https://arxiv.org/abs/2507.05265",
        "author": "Hongyang Li, Sanjoy Dey, Bum Chul Kwon, Michael Danziger, Michal Rosen-Tzvi, Jianying Hu, James Kozloski, Ching-Huei Tsou, Bharath Dandala, Pablo Meyer",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05265v1 Announce Type: cross \nAbstract: Large language models (LLMs) trained on text demonstrated remarkable results on natural language processing (NLP) tasks. These models have been adapted to decipher the language of DNA, where sequences of nucleotides act as \"words\" that encode genomic functions. However, the genome differs fundamentally from natural language, as it lacks clearly defined words or a consistent grammar. Although DNA language models (DNALMs) such as DNABERT, GENA-LM have achieved high level of performance on genome-related biological tasks, these models do not encode biological functions in the presence of sequence variations. To address this problem, we pre-train foundation models that effectively integrate sequence variations, in particular Single Nucleotide Polymorphisms (SNPs), as they underlie important biological functions. Specifically, we use ModernBERT to pre-train two different Biomedical Foundation Models (BMFM), namely, BMFM-DNA-REF in which the model is trained with sequences of varying lengths along with their reverse complements derived from the reference genome and BMFM-DNA-SNP in which the model is trained with sequences created using a novel representation scheme that encodes sequence variations. Our findings indicate that integrating sequence variations into DNALMs helps capture the biological functions as seen in improvements on all fine-tuning tasks. To explore the model's practical utility, we experimented with various strategies for SNP imputation on promoter detection task introduced in DNABERT-2. However, we acknowledge that the current benchmarks are limited in their ability to fully evaluate these models. To enable more comprehensive assessment in the future and encourage community contributions, we release our models through HuggingFace and the code to reproduce the results at https://github.com/BiomedSciAI/biomed-multi-omic"
      },
      {
        "id": "oai:arXiv.org:2507.05268v1",
        "title": "Cross-Subject DD: A Cross-Subject Brain-Computer Interface Algorithm",
        "link": "https://arxiv.org/abs/2507.05268",
        "author": "Xiaoyuan Li, Xinru Xue, Bohan Zhang, Ye Sun, Shoushuo Xi, Gang Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05268v1 Announce Type: cross \nAbstract: Brain-computer interface (BCI) based on motor imagery (MI) enables direct control of external devices by decoding the electroencephalogram (EEG) generated in the brain during imagined movements. However, due to inter-individual variability in brain activity, existing BCI models exhibit poor adaptability across subjects, thereby limiting their generalizability and widespread application. To address this issue, this paper proposes a cross-subject BCI algorithm named Cross-Subject DD (CSDD), which constructs a universal BCI model by extracting common features across subjects. The specific methods include: 1) training personalized models for each subject; 2) transforming personalized models into relation spectrums; 3) identifying common features through statistical analysis; and 4) constructing a cross-subject universal model based on common features. The experiments utilized the BCIC IV 2a dataset, involving nine subjects. Eight of these subjects were selected for training and extracing the common features, and the cross-subject decoding performance of the model was validated on the remaining subject. The results demonstrate that, compared with existing similar methods, our approach achieves a 3.28% improvement in performance. This paper introduces for the first time a novel method for extracting pure common features and constructing a universal cross-subject BCI model, thereby facilitating broader applications of BCI technology."
      },
      {
        "id": "oai:arXiv.org:2507.05279v1",
        "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy",
        "link": "https://arxiv.org/abs/2507.05279",
        "author": "Virgile Boraud (Mnemosyne), Yannis Bendi-Ouis (Mnemosyne), Paul Bernard (Mnemosyne), Xavier Hinaut (Mnemosyne)",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05279v1 Announce Type: cross \nAbstract: We introduce a tool designed to improve the capabilities of Large Language Models (LLMs) in assisting with code development using the ReservoirPy library, as well as in answering complex questions in the field of Reservoir Computing. By incorporating external knowledge through Retrieval-Augmented Generation (RAG) and knowledge graphs, our approach aims to reduce hallucinations and increase the factual accuracy of generated responses. The system provides an interactive experience similar to ChatGPT, tailored specifically for ReservoirPy, enabling users to write, debug, and understand Python code while accessing reliable domain-specific insights. In our evaluation, while proprietary models such as ChatGPT-4o and NotebookLM performed slightly better on general knowledge questions, our model outperformed them on coding tasks and showed a significant improvement over its base model, Codestral-22B."
      },
      {
        "id": "oai:arXiv.org:2507.05281v1",
        "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark",
        "link": "https://arxiv.org/abs/2507.05281",
        "author": "Lingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu, Zongyu Wang, Lin Qiu, Xunliang Cai, Xuezhi Cao, Weiwen Liu, Weinan Zhang, Yong Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05281v1 Announce Type: cross \nAbstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code processing capabilities, evaluating their performance on engineering-level code remains challenging. Existing repository-level benchmarks primarily focus on single scenarios, such as code generation or bug fixing, without adequately capturing the diversity and complexity of real-world software or project engineering workflows. Furthermore, these benchmarks suffer from limited controllability in question positioning and reliability issues in their generated test cases. To address these limitations, we present CorePipe, a fully automated pipeline that converts repositories into comprehensive test cases, and introduce CoreCodeBench, a configurable multi-scenario repository-level benchmark. To simulate real engineering scenarios, CorePipe generates three types of atomic questions (Development, BugFix, and Test-Driven Development) specifically targeting core code segments. These atomic questions are further combined into three types of composite questions, with difficulty levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects. Experiments with 16 LLMs across diverse scenarios reveal varying capabilities and offer multi-dimensional insights into LLM performance in engineering contexts. The code for CorePipe is available at https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for CoreCodeBench can be accessed at https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa."
      },
      {
        "id": "oai:arXiv.org:2507.05283v1",
        "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management",
        "link": "https://arxiv.org/abs/2507.05283",
        "author": "Yue Wang, Miao Zhou, Guijing Huang, Rui Zhuo, Chao Yi, Zhenliang Ma",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05283v1 Announce Type: cross \nAbstract: Pre-timed traffic signal control, commonly used for operating signalized intersections and coordinated arterials, requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting. To enable a user-friendly traffic signal control plan management process, this study proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could further be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers. With curated prompts, Chat2SPaT first leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulate the plan as a combination of phase sequence and phase attribute results in the json format. Based on LLM outputs, python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and finally assemble the complete traffic signal control plan. Within a chat, the pipeline can be utilized iteratively to conduct further plan editing. Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. As the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions, Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS. The source codes, prompts and test dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT."
      },
      {
        "id": "oai:arXiv.org:2507.05288v1",
        "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models",
        "link": "https://arxiv.org/abs/2507.05288",
        "author": "Shuliang Liu, Hongyi Liu, Aiwei Liu, Bingchen Duan, Qi Zheng, Yibo Yan, He Geng, Peijie Jiang, Jia Liu, Xuming Hu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05288v1 Announce Type: cross \nAbstract: The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63\\% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains."
      },
      {
        "id": "oai:arXiv.org:2507.05295v1",
        "title": "Enhancing Learning Path Recommendation via Multi-task Learning",
        "link": "https://arxiv.org/abs/2507.05295",
        "author": "Afsana Nasrin, Lijun Qian, Pamela Obiomon, Xishuang Dong",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05295v1 Announce Type: cross \nAbstract: Personalized learning is a student-centered educational approach that adapts content, pace, and assessment to meet each learner's unique needs. As the key technique to implement the personalized learning, learning path recommendation sequentially recommends personalized learning items such as lectures and exercises. Advances in deep learning, particularly deep reinforcement learning, have made modeling such recommendations more practical and effective. This paper proposes a multi-task LSTM model that enhances learning path recommendation by leveraging shared information across tasks. The approach reframes learning path recommendation as a sequence-to-sequence (Seq2Seq) prediction problem, generating personalized learning paths from a learner's historical interactions. The model uses a shared LSTM layer to capture common features for both learning path recommendation and deep knowledge tracing, along with task-specific LSTM layers for each objective. To avoid redundant recommendations, a non-repeat loss penalizes repeated items within the recommended learning path. Experiments on the ASSIST09 dataset show that the proposed model significantly outperforms baseline methods for the learning path recommendation."
      },
      {
        "id": "oai:arXiv.org:2507.05301v1",
        "title": "News Source Citing Patterns in AI Search Systems",
        "link": "https://arxiv.org/abs/2507.05301",
        "author": "Kai-Cheng Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05301v1 Announce Type: cross \nAbstract: AI-powered search systems are emerging as new information gatekeepers, fundamentally transforming how users access news and information. Despite their growing influence, the citation patterns of these systems remain poorly understood. We address this gap by analyzing data from the AI Search Arena, a head-to-head evaluation platform for AI search systems. The dataset comprises over 24,000 conversations and 65,000 responses from models across three major providers: OpenAI, Perplexity, and Google. Among the over 366,000 citations embedded in these responses, 9% reference news sources. We find that while models from different providers cite distinct news sources, they exhibit shared patterns in citation behavior. News citations concentrate heavily among a small number of outlets and display a pronounced liberal bias, though low-credibility sources are rarely cited. User preference analysis reveals that neither the political leaning nor the quality of cited news sources significantly influences user satisfaction. These findings reveal significant challenges in current AI search systems and have important implications for their design and governance."
      },
      {
        "id": "oai:arXiv.org:2507.05303v1",
        "title": "The Neural Networks with Tensor Weights and the Corresponding Fermionic Quantum Field Theory",
        "link": "https://arxiv.org/abs/2507.05303",
        "author": "Guojun Huang, Kai Zhou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05303v1 Announce Type: cross \nAbstract: In this paper, we establish a theoretical connection between complex-valued neural networks (CVNNs) and fermionic quantum field theory (QFT), bridging a fundamental gap in the emerging framework of neural network quantum field theory (NN-QFT). While prior NN-QFT works have linked real-valued architectures to bosonic fields, we demonstrate that CVNNs equipped with tensor-valued weights intrinsically generate fermionic quantum fields. By promoting hidden-to-output weights to Clifford algebra-valued tensors, we induce anticommutation relations essential for fermionic statistics. Through analytical study of the generating functional, we obtain the exact quantum state in the infinite-width limit, revealing that the parameters between the input layer and the last hidden layer correspond to the eigenvalues of the quantum system, and the tensor weighting parameters in the hidden-to-output layer map to dynamical fermionic fields. The continuum limit reproduces free fermion correlators, with diagrammatic expansions confirming anticommutation. The work provides the first explicit mapping from neural architectures to fermionic QFT at the level of correlation functions and generating functional. It extends NN-QFT beyond bosonic theories and opens avenues for encoding fermionic symmetries into machine learning models, with potential applications in quantum simulation and lattice field theory."
      },
      {
        "id": "oai:arXiv.org:2507.05304v1",
        "title": "Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes",
        "link": "https://arxiv.org/abs/2507.05304",
        "author": "Saqib Nazir (UNICAEN), Olivier L\\'ezoray (UNICAEN), S\\'ebastien Bougleux (UNICAEN)",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05304v1 Announce Type: cross \nAbstract: 3D meshes are fundamental data representations for capturing complex geometric shapes in computer vision and graphics applications. While Convolutional Neural Networks (CNNs) have excelled in structured data like images, extending them to irregular 3D meshes is challenging due to the non-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a solution by applying convolutions to graph-structured data, but many existing methods rely on isotropic filters or spectral decomposition, limiting their ability to capture both local and global mesh features. In this paper, we introduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework that uses anisotropic convolution layers to effectively learn both global and local features directly in the spatial domain. Unlike previous approaches that convert meshes into intermediate representations like voxel grids or point clouds, our method preserves the original polygonal mesh format throughout the reconstruction process, enabling more accurate shape reconstruction. Our architecture features a multi-scale encoder-decoder structure, where separate global and local pathways capture both large-scale geometric structures and fine-grained local details. Extensive experiments on the COMA dataset containing human faces demonstrate the efficiency of 3DGeoMeshNet in terms of reconstruction accuracy."
      },
      {
        "id": "oai:arXiv.org:2507.05305v1",
        "title": "Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools",
        "link": "https://arxiv.org/abs/2507.05305",
        "author": "Lorenzo Lee Solano, Charles Koutcheme, Juho Leinonen, Alexandra Vassar, Jake Renzella",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05305v1 Announce Type: cross \nAbstract: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher cryptic compiler errors for novice programmers, but their computational scale, cost, and tendency to over-assist make them problematic for widespread pedagogical adoption. This work demonstrates that smaller, specialised language models, enhanced via Supervised Fine-Tuning (SFT), present a more viable alternative for educational tools. We utilise a new dataset of 40,000 C compiler error explanations, derived from real introductory programming (CS1/2) student-generated programming errors, which we used to fine-tune three open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual evaluation, combining expert human reviews with a large-scale automated analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our results show that SFT significantly boosts the pedagogical quality of smaller models, achieving performance comparable to much larger models. We analyse the trade-offs between model size and quality, confirming that fine-tuning compact, efficient models on high-quality, domain-specific data is a potent strategy for creating specialised models to drive educational tools. We provide a replicable methodology to foster broader access to generative AI capabilities in educational contexts."
      },
      {
        "id": "oai:arXiv.org:2507.05306v1",
        "title": "Enjoying Non-linearity in Multinomial Logistic Bandits",
        "link": "https://arxiv.org/abs/2507.05306",
        "author": "Pierre Boudart (PSL, DI-ENS, Inria), Pierre Gaillard (PSL, DI-ENS, Inria), Alessandro Rudi (PSL, DI-ENS, Inria)",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05306v1 Announce Type: cross \nAbstract: We consider the multinomial logistic bandit problem, a variant of generalized linear bandits where a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $\\kappa_*$, that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\\smash{O(d\\sqrt{T})}$ to $\\smash{O(d\\sqrt{T/\\kappa_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of $\\kappa_*$ to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \\smash{\\widetilde{\\mathcal{O}}( Kd \\sqrt{{T}/{\\kappa_*}})} $, where $K$ is the number of actions and $\\kappa_* \\ge 1$. This improves upon the best existing guarantees of order $ \\smash{\\widetilde{\\mathcal{O}}( Kd \\sqrt{T} )} $. Moreover, we provide a $\\smash{ \\Omega(d\\sqrt{T/\\kappa_*})}$ lower-bound, showing that our dependence on $\\kappa_*$ is optimal."
      },
      {
        "id": "oai:arXiv.org:2507.05308v1",
        "title": "High Order Collaboration-Oriented Federated Graph Neural Network for Accurate QoS Prediction",
        "link": "https://arxiv.org/abs/2507.05308",
        "author": "Zehuan Chen, Xiangwei Lai",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05308v1 Announce Type: cross \nAbstract: Predicting Quality of Service (QoS) data crucial for cloud service selection, where user privacy is a critical concern. Federated Graph Neural Networks (FGNNs) can perform QoS data prediction as well as maintaining user privacy. However, existing FGNN-based QoS predictors commonly implement on-device training on scattered explicit user-service graphs, thereby failing to utilize the implicit user-user interactions. To address this issue, this study proposes a high order collaboration-oriented federated graph neural network (HC-FGNN) to obtain accurate QoS prediction with privacy preservation. Concretely, it magnifies the explicit user-service graphs following the principle of attention mechanism to obtain the high order collaboration, which reflects the implicit user-user interactions. Moreover, it utilizes a lightweight-based message aggregation way to improve the computational efficiency. The extensive experiments on two QoS datasets from real application indicate that the proposed HC-FGNN possesses the advantages of high prediction accurate and privacy protection."
      },
      {
        "id": "oai:arXiv.org:2507.05313v1",
        "title": "Solar Flare Prediction Using LSTM and DLSTM with Sliding Window Pattern Recognition",
        "link": "https://arxiv.org/abs/2507.05313",
        "author": "Zeinab Hassani, Davud Mohammadpur, Hossein Safari",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05313v1 Announce Type: cross \nAbstract: We investigate the use of Long Short-Term Memory (LSTM) and Decomposition-LSTM (DLSTM) networks, combined with an ensemble algorithm, to predict solar flare occurrences using time-series data from the GOES catalog. The dataset spans from 2003 to 2023 and includes 151,071 flare events. Among approximately possible patterns, 7,552 yearly pattern windows are identified, highlighting the challenge of long-term forecasting due to the Sun's complex, self-organized criticality-driven behavior. A sliding window technique is employed to detect temporal quasi-patterns in both irregular and regularized flare time series. Regularization reduces complexity, enhances large flare activity, and captures active days more effectively. To address class imbalance, resampling methods are applied. LSTM and DLSTM models are trained on sequences of peak fluxes and waiting times from irregular time series, while LSTM and DLSTM, integrated with an ensemble approach, are applied to sliding windows of regularized time series with a 3-hour interval. Performance metrics, particularly TSS (0.74), recall (0.95) and the area under the curve (AUC=0.87) in the receiver operating characteristic (ROC), indicate that DLSTM with an ensemble approach on regularized time series outperforms other models, offering more accurate large-flare forecasts with fewer false errors compared to models trained on irregular time series. The superior performance of DLSTM is attributed to its ability to decompose time series into trend and seasonal components, effectively isolating random noise. This study underscores the potential of advanced machine learning techniques for solar flare prediction and highlights the importance of incorporating various solar cycle phases and resampling strategies to enhance forecasting reliability."
      },
      {
        "id": "oai:arXiv.org:2507.05314v1",
        "title": "Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation",
        "link": "https://arxiv.org/abs/2507.05314",
        "author": "Daniel Cie\\'slak, Miriam Reca, Olena Onyshchenko, Jacek Rumi\\'nski",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05314v1 Announce Type: cross \nAbstract: Accurate segmentation of wounds and scale markers in clinical images remainsa significant challenge, crucial for effective wound management and automatedassessment. In this study, we propose a novel dual-attention U-Net++ archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms toaddress severe class imbalance and variability in medical images effectively.Initially, extensive benchmarking across diverse architectures and encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal encoder backbone.Subsequently, we independently trained two class-specific models with tailoredpreprocessing, extensive data augmentation, and Bayesian hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test Time Augmentationto further enhance prediction reliability. Our approach was evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition. Segmentationperformance was quantified using a weighted F1-score (75% wounds, 25% scalemarkers), calculated externally by competition organizers on undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640, underscoring itseffectiveness for complex medical segmentation tasks."
      },
      {
        "id": "oai:arXiv.org:2507.05317v1",
        "title": "PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT",
        "link": "https://arxiv.org/abs/2507.05317",
        "author": "Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05317v1 Announce Type: cross \nAbstract: Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM."
      },
      {
        "id": "oai:arXiv.org:2507.05447v1",
        "title": "NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones",
        "link": "https://arxiv.org/abs/2507.05447",
        "author": "Aiur Nanzatov, Lourdes Pe\\~na-Castillo, Oscar Meruvia-Pastor",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05447v1 Announce Type: cross \nAbstract: Two-factor authentication (2FA) has become widely adopted as an efficient and secure way to validate someone's identity online. Two-factor authentication is difficult in virtual reality (VR) because users are usually wearing a head-mounted display (HMD) which does not allow them to see their real-world surroundings. We present NRXR-ID, a technique to implement two-factor authentication while using extended reality systems and smartphones. The proposed method allows users to complete an authentication challenge using their smartphones without removing their HMD. We performed a user study where we explored four types of challenges for users, including a novel checkers-style challenge. Users responded to these challenges under three different configurations, including a technique that uses the smartphone to support gaze-based selection without the use of VR controllers. A 4X3 within-subjects design allowed us to study all the variations proposed. We collected performance metrics and performed user experience questionnaires to collect subjective impressions from 30 participants. Results suggest that the checkers-style visual matching challenge was the most appropriate option, followed by entering a digital PIN challenge submitted via the smartphone and answered within the VR environment."
      },
      {
        "id": "oai:arXiv.org:2507.05451v1",
        "title": "Self-supervised Deep Learning for Denoising in Ultrasound Microvascular Imaging",
        "link": "https://arxiv.org/abs/2507.05451",
        "author": "Lijie Huang, Jingyi Yin, Jingke Zhang, U-Wai Lok, Ryan M. DeRuiter, Jieyang Jin, Kate M. Knoll, Kendra E. Petersen, James D. Krier, Xiang-yang Zhu, Gina K. Hesley, Kathryn A. Robinson, Andrew J. Bentall, Thomas D. Atwell, Andrew D. Rule, Lilach O. Lerman, Shigao Chen, Chengwu Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05451v1 Announce Type: cross \nAbstract: Ultrasound microvascular imaging (UMI) is often hindered by low signal-to-noise ratio (SNR), especially in contrast-free or deep tissue scenarios, which impairs subsequent vascular quantification and reliable disease diagnosis. To address this challenge, we propose Half-Angle-to-Half-Angle (HA2HA), a self-supervised denoising framework specifically designed for UMI. HA2HA constructs training pairs from complementary angular subsets of beamformed radio-frequency (RF) blood flow data, across which vascular signals remain consistent while noise varies. HA2HA was trained using in-vivo contrast-free pig kidney data and validated across diverse datasets, including contrast-free and contrast-enhanced data from pig kidneys, as well as human liver and kidney. An improvement exceeding 15 dB in both contrast-to-noise ratio (CNR) and SNR was observed, indicating a substantial enhancement in image quality. In addition to power Doppler imaging, denoising directly in the RF domain is also beneficial for other downstream processing such as color Doppler imaging (CDI). CDI results of human liver derived from the HA2HA-denoised signals exhibited improved microvascular flow visualization, with a suppressed noisy background. HA2HA offers a label-free, generalizable, and clinically applicable solution for robust vascular imaging in both contrast-free and contrast-enhanced UMI."
      },
      {
        "id": "oai:arXiv.org:2507.05470v1",
        "title": "Temporal Conformal Prediction (TCP): A Distribution-Free Statistical and Machine Learning Framework for Adaptive Risk Forecasting",
        "link": "https://arxiv.org/abs/2507.05470",
        "author": "Agnideep Aich, Ashit Baran Aich, Dipak C. Jain",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05470v1 Announce Type: cross \nAbstract: We propose Temporal Conformal Prediction (TCP), a novel framework for constructing prediction intervals in financial time-series with guaranteed finite-sample validity. TCP integrates quantile regression with a conformal calibration layer that adapts online via a decaying learning rate. This hybrid design bridges statistical and machine learning paradigms, enabling TCP to accommodate non-stationarity, volatility clustering, and regime shifts which are hallmarks of real-world asset returns, without relying on rigid parametric assumptions. We benchmark TCP against established methods including GARCH, Historical Simulation, and static Quantile Regression across equities (S&amp;P 500), cryptocurrency (Bitcoin), and commodities (Gold). Empirical results show that TCP consistently delivers sharper intervals with competitive or superior coverage, particularly in high-volatility regimes. Our study underscores TCP's strength in navigating the coverage-sharpness tradeoff, a central challenge in modern risk forecasting. Overall, TCP offers a distribution-free, adaptive, and interpretable alternative for financial uncertainty quantification, advancing the interface between statistical inference and machine learning in finance."
      },
      {
        "id": "oai:arXiv.org:2507.05502v1",
        "title": "Predicting mutational effects on protein binding from folding energy",
        "link": "https://arxiv.org/abs/2507.05502",
        "author": "Arthur Deng, Karsten Householder, Fang Wu, Sebastian Thrun, K. Christopher Garcia, Brian Trippe",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05502v1 Announce Type: cross \nAbstract: Accurate estimation of mutational effects on protein-protein binding energies is an open problem with applications in structural biology and therapeutic design. Several deep learning predictors for this task have been proposed, but, presumably due to the scarcity of binding data, these methods underperform computationally expensive estimates based on empirical force fields. In response, we propose a transfer-learning approach that leverages advances in protein sequence modeling and folding stability prediction for this task. The key idea is to parameterize the binding energy as the difference between the folding energy of the protein complex and the sum of the folding energies of its binding partners. We show that using a pre-trained inverse-folding model as a proxy for folding energy provides strong zero-shot performance, and can be fine-tuned with (1) copious folding energy measurements and (2) more limited binding energy measurements. The resulting predictor, StaB-ddG, is the first deep learning predictor to match the accuracy of the state-of-the-art empirical force-field method FoldX, while offering an over 1,000x speed-up."
      },
      {
        "id": "oai:arXiv.org:2507.05515v1",
        "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality",
        "link": "https://arxiv.org/abs/2507.05515",
        "author": "Haochen Huang, Jiahuan Pei, Mohammad Aliannejadi, Xin Sun, Moonisa Ahsan, Pablo Cesar, Chuang Yu, Zhaochun Ren, Junxiao Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05515v1 Announce Type: cross \nAbstract: Vision-language models (VLMs) are essential for enabling AI-powered smart assistants to interpret and reason in multimodal environments. However, their application in augmented reality (AR) training remains largely unexplored. In this work, we introduce a comprehensive dataset tailored for AR training, featuring systematized vision-language tasks, and evaluate nine state-of-the-art VLMs on it. Our results reveal that even advanced models, including GPT-4o, struggle with fine-grained assembly tasks, achieving a maximum F1 score of just 40.54% on state detection. These findings highlight the demand for enhanced datasets, benchmarks, and further research to improve fine-grained vision-language alignment. Beyond technical contributions, our work has broader social implications, particularly in empowering blind and visually impaired users with equitable access to AI-driven learning opportunities. We provide all related resources, including the dataset, source code, and evaluation results, to support the research community."
      },
      {
        "id": "oai:arXiv.org:2507.05528v1",
        "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment",
        "link": "https://arxiv.org/abs/2507.05528",
        "author": "Jiahuan Pei, Fanghua Ye, Xin Sun, Wentao Deng, Koen Hindriks, Junxiao Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05528v1 Announce Type: cross \nAbstract: Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced."
      },
      {
        "id": "oai:arXiv.org:2507.05535v1",
        "title": "Special-Unitary Parameterization for Trainable Variational Quantum Circuits",
        "link": "https://arxiv.org/abs/2507.05535",
        "author": "Kuan-Cheng Chen, Huan-Hsin Tseng, Samuel Yen-Chi Chen, Chen-Yu Liu, Kin K. Leung",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05535v1 Announce Type: cross \nAbstract: We propose SUN-VQC, a variational-circuit architecture whose elementary layers are single exponentials of a symmetry-restricted Lie subgroup, $\\mathrm{SU}(2^{k}) \\subset \\mathrm{SU}(2^{n})$ with $k \\ll n$. Confining the evolution to this compact subspace reduces the dynamical Lie-algebra dimension from $\\mathcal{O}(4^{n})$ to $\\mathcal{O}(4^{k})$, ensuring only polynomial suppression of gradient variance and circumventing barren plateaus that plague hardware-efficient ans\\\"atze. Exact, hardware-compatible gradients are obtained using a generalized parameter-shift rule, avoiding ancillary qubits and finite-difference bias. Numerical experiments on quantum auto-encoding and classification show that SUN-VQCs sustain order-of-magnitude larger gradient signals, converge 2--3$\\times$ faster, and reach higher final fidelities than depth-matched Pauli-rotation or hardware-efficient circuits. These results demonstrate that Lie-subalgebra engineering provides a principled, scalable route to barren-plateau-resilient VQAs compatible with near-term quantum processors."
      },
      {
        "id": "oai:arXiv.org:2507.05550v1",
        "title": "A Malliavin calculus approach to score functions in diffusion generative models",
        "link": "https://arxiv.org/abs/2507.05550",
        "author": "Ehsan Mirafzali, Frank Proske, Utkarsh Gupta, Daniele Venturi, Razvan Marinescu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05550v1 Announce Type: cross \nAbstract: Score-based diffusion generative models have recently emerged as a powerful tool for modelling complex data distributions. These models aim at learning the score function, which defines a map from a known probability distribution to the target data distribution via deterministic or stochastic differential equations (SDEs). The score function is typically estimated from data using a variety of approximation techniques, such as denoising or sliced score matching, Hyv\\\"arien's method, or Schr\\\"odinger bridges. In this paper, we derive an exact, closed form, expression for the score function for a broad class of nonlinear diffusion generative models. Our approach combines modern stochastic analysis tools such as Malliavin derivatives and their adjoint operators (Skorokhod integrals or Malliavin Divergence) with a new Bismut-type formula. The resulting expression for the score function can be written entirely in terms of the first and second variation processes, with all Malliavin derivatives systematically eliminated, thereby enhancing its practical applicability. The theoretical framework presented in this work offers a principled foundation for advancing score estimation methods in generative modelling, enabling the design of new sampling algorithms for complex probability distributions. Our results can be extended to broader classes of stochastic differential equations, opening new directions for the development of score-based diffusion generative models."
      },
      {
        "id": "oai:arXiv.org:2507.05562v1",
        "title": "Exact and efficient basis pursuit denoising via differential inclusions and a selection principle",
        "link": "https://arxiv.org/abs/2507.05562",
        "author": "Gabriel P. Langlois, J\\'er\\^ome Darbon",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05562v1 Announce Type: cross \nAbstract: Basis pursuit denoising (BPDN) is a cornerstone of compressive sensing, statistics and machine learning. While various algorithms for BPDN have been proposed, they invariably suffer from drawbacks and must either favor efficiency at the expense of accuracy or vice versa. As such, state-of-the-art algorithms remain ineffective for high-dimensional applications that require accurate solutions within a reasonable amount of computational time. In this work, we address this issue and propose an exact and efficient algorithm for BPDN using differential inclusions. Specifically, we prove that a selection principle from the theory of differential inclusions turns the dual problem of BPDN into calculating the trajectory of an \\emph{integrable} projected dynamical system, that is, whose trajectory and asymptotic limit can be computed exactly. Our analysis naturally yields an exact algorithm, numerically up to machine precision, that is amenable to computing regularization paths and very fast. Numerical experiments confirm that our algorithm outperforms the state-of-the-art algorithms in both accuracy and efficiency. Moreover, we show that the global continuation of solutions (in terms of the hyperparameter and data) of the projected dynamical system yields a rigorous homotopy algorithm for BPDN, as well as a novel greedy algorithm for computing feasible solutions to basis pursuit in strongly polynomial time. Beyond this work, we expect that our results and analysis can be adapted to compute exact or approximate solutions to a broader class of polyhedral-constrained optimization problems."
      },
      {
        "id": "oai:arXiv.org:2507.05577v1",
        "title": "Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA",
        "link": "https://arxiv.org/abs/2507.05577",
        "author": "Shashank Verma, Fengyi Jiang, Xiangning Xue",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05577v1 Announce Type: cross \nAbstract: Biomedical semantic question answering rooted in information retrieval can play a crucial role in keeping up to date with vast, rapidly evolving and ever-growing biomedical literature. A robust system can help researchers, healthcare professionals and even layman users access relevant knowledge grounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important benchmark, offering a competitive platform for advancement of this space. This paper presents the methodologies and results from our participation in this challenge where we built a Retrieval-Augmented Generation (RAG) system that can answer biomedical questions by retrieving relevant PubMed documents and snippets to generate answers. For the retrieval task, we generated dense embeddings from biomedical articles for initial retrieval, and applied an ensemble of finetuned cross-encoders and large language models (LLMs) for re-ranking to identify top relevant documents. Our solution achieved an MAP@10 of 0.1581, placing 10th on the leaderboard for the retrieval task. For answer generation, we employed few-shot prompting of instruction-tuned LLMs. Our system achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean Reciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of 0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal answers (rank 11)."
      },
      {
        "id": "oai:arXiv.org:2507.05582v1",
        "title": "Learning Segmentation from Radiology Reports",
        "link": "https://arxiv.org/abs/2507.05582",
        "author": "Pedro R. A. S. Bassi, Wenxuan Li, Jieneng Chen, Zheren Zhu, Tianyu Lin, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05582v1 Announce Type: cross \nAbstract: Tumor segmentation in CT scans is key for diagnosis, surgery, and prognosis, yet segmentation masks are scarce because their creation requires time and expertise. Public abdominal CT datasets have from dozens to a couple thousand tumor masks, but hospitals have hundreds of thousands of tumor CTs with radiology reports. Thus, leveraging reports to improve segmentation is key for scaling. In this paper, we propose a report-supervision loss (R-Super) that converts radiology reports into voxel-wise supervision for tumor segmentation AI. We created a dataset with 6,718 CT-Report pairs (from the UCSF Hospital), and merged it with public CT-Mask datasets (from AbdomenAtlas 2.0). We used our R-Super to train with these masks and reports, and strongly improved tumor segmentation in internal and external validation--F1 Score increased by up to 16% with respect to training with masks only. By leveraging readily available radiology reports to supplement scarce segmentation masks, R-Super strongly improves AI performance both when very few training masks are available (e.g., 50), and when many masks were available (e.g., 1.7K).\n  Project: https://github.com/MrGiovanni/R-Super"
      },
      {
        "id": "oai:arXiv.org:2507.05610v1",
        "title": "On the Inherent Privacy of Zeroth Order Projected Gradient Descent",
        "link": "https://arxiv.org/abs/2507.05610",
        "author": "Devansh Gupta, Meisam Razaviyayn, Vatsal Sharan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05610v1 Announce Type: cross \nAbstract: Differentially private zeroth-order optimization methods have recently gained popularity in private fine tuning of machine learning models due to their reduced memory requirements. Current approaches for privatizing zeroth-order methods rely on adding Gaussian noise to the estimated zeroth-order gradients. However, since the search direction in the zeroth-order methods is inherently random, researchers including Tang et al. (2024) and Zhang et al. (2024a) have raised an important question: is the inherent noise in zeroth-order estimators sufficient to ensure the overall differential privacy of the algorithm? This work settles this question for a class of oracle-based optimization algorithms where the oracle returns zeroth-order gradient estimates. In particular, we show that for a fixed initialization, there exist strongly convex objective functions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD) is not differentially private. Furthermore, we show that even with random initialization and without revealing (initial and) intermediate iterates, the privacy loss in ZO-GD can grow superlinearly with the number of iterations when minimizing convex objective functions."
      },
      {
        "id": "oai:arXiv.org:2507.05622v1",
        "title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective",
        "link": "https://arxiv.org/abs/2507.05622",
        "author": "Shuo Shao, Yiming Li, Mengren Zheng, Zhiyang Hu, Yukun Chen, Boheng Li, Yu He, Junfeng Guo, Tianwei Zhang, Dacheng Tao, Zhan Qin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05622v1 Announce Type: cross \nAbstract: The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available at https://github.com/shaoshuo-ss/DATABench."
      },
      {
        "id": "oai:arXiv.org:2507.05627v1",
        "title": "DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation",
        "link": "https://arxiv.org/abs/2507.05627",
        "author": "Young Hun Kim, Seungyeon Kim, Yonghyeon Lee, Frank Chongwoo Park",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05627v1 Announce Type: cross \nAbstract: Partial-view 3D recognition -- reconstructing 3D geometry and identifying object instances from a few sparse RGB images -- is an exceptionally challenging yet practically essential task, particularly in cluttered, occluded real-world settings where full-view or reliable depth data are often unavailable. Existing methods, whether based on strong symmetry priors or supervised learning on curated datasets, fail to generalize to such scenarios. In this work, we introduce DreamGrasp, a framework that leverages the imagination capability of large-scale pre-trained image generative models to infer the unobserved parts of a scene. By combining coarse 3D reconstruction, instance segmentation via contrastive learning, and text-guided instance-wise refinement, DreamGrasp circumvents limitations of prior methods and enables robust 3D reconstruction in complex, multi-object environments. Our experiments show that DreamGrasp not only recovers accurate object geometry but also supports downstream tasks like sequential decluttering and target retrieval with high success rates."
      },
      {
        "id": "oai:arXiv.org:2507.05630v1",
        "title": "How Not to Detect Prompt Injections with an LLM",
        "link": "https://arxiv.org/abs/2507.05630",
        "author": "Sarthak Choudhary, Divyam Anshumaan, Nils Palumbo, Somesh Jha",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05630v1 Announce Type: cross \nAbstract: LLM-integrated applications and agents are vulnerable to prompt injection attacks, in which adversaries embed malicious instructions within seemingly benign user inputs to manipulate the LLM's intended behavior. Recent defenses based on $\\textit{known-answer detection}$ (KAD) have achieved near-perfect performance by using an LLM to classify inputs as clean or contaminated. In this work, we formally characterize the KAD framework and uncover a structural vulnerability in its design that invalidates its core security premise. We design a methodical adaptive attack, $\\textit{DataFlip}$, to exploit this fundamental weakness. It consistently evades KAD defenses with detection rates as low as $1.5\\%$ while reliably inducing malicious behavior with success rates of up to $88\\%$, without needing white-box access to the LLM or any optimization procedures."
      },
      {
        "id": "oai:arXiv.org:2507.05638v1",
        "title": "LLMs are Introvert",
        "link": "https://arxiv.org/abs/2507.05638",
        "author": "Litian Zhang, Xiaoming Zhang, Bingyu Yan, Ziyi Zhou, Bo Zhang, Zhenyu Guan, Xi Zhang, Chaozhuo Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05638v1 Announce Type: cross \nAbstract: The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents."
      },
      {
        "id": "oai:arXiv.org:2507.05640v1",
        "title": "Learnable quantum spectral filters for hybrid graph neural networks",
        "link": "https://arxiv.org/abs/2507.05640",
        "author": "Ammar Daskin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05640v1 Announce Type: cross \nAbstract: In this paper, we describe a parameterized quantum circuit that can be considered as convolutional and pooling layers for graph neural networks. The circuit incorporates the parameterized quantum Fourier circuit where the qubit connections for the controlled gates derived from the Laplacian operator. Specifically, we show that the eigenspace of the Laplacian operator of a graph can be approximated by using QFT based circuit whose connections are determined from the adjacency matrix. For an $N\\times N$ Laplacian, this approach yields an approximate polynomial-depth circuit requiring only $n=log(N)$ qubits. These types of circuits can eliminate the expensive classical computations for approximating the learnable functions of the Laplacian through Chebyshev polynomial or Taylor expansions.\n  Using this circuit as a convolutional layer provides an $n-$ dimensional probability vector that can be considered as the filtered and compressed graph signal. Therefore, the circuit along with the measurement can be considered a very efficient convolution plus pooling layer that transforms an $N$-dimensional signal input into $n-$dimensional signal with an exponential compression. We then apply a classical neural network prediction head to the output of the circuit to construct a complete graph neural network. Since the circuit incorporates geometric structure through its graph connection-based approach, we present graph classification results for the benchmark datasets listed in TUDataset library. Using only [1-100] learnable parameters for the quantum circuit and minimal classical layers (1000-5000 parameters) in a generic setting, the obtained results are comparable to and in some cases better than many of the baseline results, particularly for the cases when geometric structure plays a significant role."
      },
      {
        "id": "oai:arXiv.org:2507.05647v1",
        "title": "Diffusion-Based Limited-Angle CT Reconstruction under Noisy Conditions",
        "link": "https://arxiv.org/abs/2507.05647",
        "author": "Jiaqi Guo, Santiago L\\'opez-Tapia",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05647v1 Announce Type: cross \nAbstract: Limited-Angle Computed Tomography (LACT) is a challenging inverse problem where missing angular projections lead to incomplete sinograms and severe artifacts in the reconstructed images. While recent learning-based methods have demonstrated effectiveness, most of them assume ideal, noise-free measurements and fail to address the impact of measurement noise. To overcome this limitation, we treat LACT as a sinogram inpainting task and propose a diffusion-based framework that completes missing angular views using a Mean-Reverting Stochastic Differential Equation (MR-SDE) formulation. To improve robustness under realistic noise, we propose RNSD$^+$, a novel noise-aware rectification mechanism that explicitly models inference-time uncertainty, enabling reliable and robust reconstruction. Extensive experiments demonstrate that our method consistently surpasses baseline models in data consistency and perceptual quality, and generalizes well across varying noise intensity and acquisition scenarios."
      },
      {
        "id": "oai:arXiv.org:2507.05649v1",
        "title": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning",
        "link": "https://arxiv.org/abs/2507.05649",
        "author": "Kaixiang Zhao, Joseph Yousry Attalla, Qian Lou, Yushun Dong",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05649v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph-based learning tasks. However, enabling privacy-preserving GNNs in encrypted domains, such as under Fully Homomorphic Encryption (FHE), typically incurs substantial computational overhead, rendering real-time and privacy-preserving inference impractical. In this work, we propose DESIGN (EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel framework for efficient encrypted GNN inference. DESIGN tackles the critical efficiency limitations of existing FHE GNN approaches, which often overlook input data redundancy and apply uniform computational strategies. Our framework achieves significant performance gains through a hierarchical optimization strategy executed entirely on the server: first, FHE-compatible node importance scores (based on encrypted degree statistics) are computed from the encrypted graph. These scores then guide a homomorphic partitioning process, generating multi-level importance masks directly under FHE. This dynamically generated mask facilitates both input graph pruning (by logically removing unimportant elements) and a novel adaptive polynomial activation scheme, where activation complexity is tailored to node importance levels. Empirical evaluations demonstrate that DESIGN substantially accelerates FHE GNN inference compared to state-of-the-art methods while maintaining competitive model accuracy, presenting a robust solution for secure graph analytics."
      },
      {
        "id": "oai:arXiv.org:2507.05656v1",
        "title": "ADPv2: A Hierarchical Histological Tissue Type-Annotated Dataset for Potential Biomarker Discovery of Colorectal Disease",
        "link": "https://arxiv.org/abs/2507.05656",
        "author": "Zhiyuan Yang, Kai Li, Sophia Ghamoshi Ramandi, Patricia Brassard, Hakim Khellaf, Vincent Quoc-Huy Trinh, Jennifer Zhang, Lina Chen, Corwyn Rowsell, Sonal Varma, Kostas Plataniotis, Mahdi S. Hosseini",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05656v1 Announce Type: cross \nAbstract: Computational pathology (CoPath) leverages histopathology images to enhance diagnostic precision and reproducibility in clinical pathology. However, publicly available datasets for CoPath that are annotated with extensive histological tissue type (HTT) taxonomies at a granular level remain scarce due to the significant expertise and high annotation costs required. Existing datasets, such as the Atlas of Digital Pathology (ADP), address this by offering diverse HTT annotations generalized to multiple organs, but limit the capability for in-depth studies on specific organ diseases. Building upon this foundation, we introduce ADPv2, a novel dataset focused on gastrointestinal histopathology. Our dataset comprises 20,004 image patches derived from healthy colon biopsy slides, annotated according to a hierarchical taxonomy of 32 distinct HTTs of 3 levels. Furthermore, we train a multilabel representation learning model following a two-stage training procedure on our ADPv2 dataset. We leverage the VMamba architecture and achieving a mean average precision (mAP) of 0.88 in multilabel classification of colon HTTs. Finally, we show that our dataset is capable of an organ-specific in-depth study for potential biomarker discovery by analyzing the model's prediction behavior on tissues affected by different colon diseases, which reveals statistical patterns that confirm the two pathological pathways of colon cancer development. Our dataset is publicly available here: Part 1 at https://zenodo.org/records/15307021, Part 2 at https://zenodo.org/records/15312384 and Part 3 at https://zenodo.org/records/15312792"
      },
      {
        "id": "oai:arXiv.org:2507.05658v1",
        "title": "HRRRCast: a data-driven emulator for regional weather forecasting at convection allowing scales",
        "link": "https://arxiv.org/abs/2507.05658",
        "author": "Daniel Abdi, Isidora Jankov, Paul Madden, Vanderlei Vargas, Timothy A. Smith, Sergey Frolov, Montgomery Flora, Corey Potvin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05658v1 Announce Type: cross \nAbstract: The High-Resolution Rapid Refresh (HRRR) model is a convection-allowing model used in operational weather forecasting across the contiguous United States (CONUS). To provide a computationally efficient alternative, we introduce HRRRCast, a data-driven emulator built with advanced machine learning techniques. HRRRCast includes two architectures: a ResNet-based model (ResHRRR) and a Graph Neural Network-based model (GraphHRRR). ResHRRR uses convolutional neural networks enhanced with squeeze-and-excitation blocks and Feature-wise Linear Modulation, and supports probabilistic forecasting via the Denoising Diffusion Implicit Model (DDIM). To better handle longer lead times, we train a single model to predict multiple lead times (1h, 3h, and 6h), then use a greedy rollout strategy during inference. When evaluated on composite reflectivity over the full CONUS domain using ensembles of 3 to 10 members, ResHRRR outperforms HRRR forecast at light rainfall threshold (20 dBZ) and achieves competitive performance at moderate thresholds (30 dBZ). Our work advances the StormCast model of Pathak et al. [21] by: a) training on the full CONUS domain, b) using multiple lead times to improve long-range skill, c) training on analysis data instead of the +1h post-analysis data inadvertently used in StormCast, and d) incorporating future GFS states as inputs, enabling downscaling that improves long-lead accuracy. Grid-, neighborhood-, and object-based metrics confirm better storm placement, lower frequency bias, and higher success ratios than HRRR. HRRRCast ensemble forecasts also maintain sharper spatial detail, with power spectra more closely matching HRRR analysis. While GraphHRRR underperforms in its current form, it lays groundwork for future graph-based forecasting. HRRRCast represents a step toward efficient, data-driven regional weather prediction with competitive accuracy and ensemble capability."
      },
      {
        "id": "oai:arXiv.org:2507.05660v1",
        "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data",
        "link": "https://arxiv.org/abs/2507.05660",
        "author": "Aravind Cheruvu, Shravya Kanchi, Sifat Muhammad Abdullah, Nicholas Kong, Daphne Yao, Murtuza Jadliwala, Bimal Viswanath",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05660v1 Announce Type: cross \nAbstract: Recent advances in foundation models, such as LLMs, have revolutionized conversational AI. Chatbots are increasingly being developed by customizing LLMs on specific conversational datasets. However, mitigating toxicity during this customization, especially when dealing with untrusted training data, remains a significant challenge. To address this, we introduce TuneShield, a defense framework designed to mitigate toxicity during chatbot fine-tuning while preserving conversational quality. TuneShield leverages LLM-based toxicity classification, utilizing the instruction-following capabilities and safety alignment of LLMs to effectively identify toxic samples, outperforming industry API services. TuneShield generates synthetic conversation samples, termed 'healing data', based on the identified toxic samples, using them to mitigate toxicity while reinforcing desirable behavior during fine-tuning. It performs an alignment process to further nudge the chatbot towards producing desired responses. Our findings show that TuneShield effectively mitigates toxicity injection attacks while preserving conversational quality, even when the toxicity classifiers are imperfect or biased. TuneShield proves to be resilient against adaptive adversarial and jailbreak attacks. Additionally, TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning (DBL)."
      },
      {
        "id": "oai:arXiv.org:2507.05661v1",
        "title": "3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2507.05661",
        "author": "Haitao Lu, Haijier Chen, Haoze Liu, Shoujian Zhang, Bo Xu, Ziao Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05661v1 Announce Type: cross \nAbstract: In autonomous robotic systems, precise localization is a prerequisite for safe navigation. However, in complex urban environments, GNSS positioning often suffers from signal occlusion and multipath effects, leading to unreliable absolute positioning. Traditional mapping approaches are constrained by storage requirements and computational inefficiency, limiting their applicability to resource-constrained robotic platforms. To address these challenges, we propose 3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian Splatting (3DGS), enabling centimeter-level positioning using only a single monocular RGB image on the client side. We combine multi-sensor data to construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side localization requires just a standard camera input. Using SuperPoint and SuperGlue for feature extraction and matching, our core innovation is an iterative optimization strategy that refines localization results through step-by-step rendering, making it suitable for real-time autonomous navigation. Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads, boulevard roads, and traffic-dense highways respectively, significantly outperforming other representative methods while requiring only monocular RGB input. This approach provides autonomous robots with reliable localization capabilities even in challenging urban environments where GNSS fails."
      },
      {
        "id": "oai:arXiv.org:2507.05681v1",
        "title": "GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks",
        "link": "https://arxiv.org/abs/2507.05681",
        "author": "Muhammad Hadir Khan, Matthew Guthaus",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05681v1 Announce Type: cross \nAbstract: Clock meshes are essential in high-performance VLSI systems for minimizing skew and handling PVT variations, but analyzing them is difficult due to reconvergent paths, multi-source driving, and input mesh buffer skew. SPICE simulations are accurate but slow; yet simplified models miss key effects like slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based framework that models the clock mesh as a graph with augmented structural and physical features. Trained on SPICE data, GATMesh achieves high accuracy with average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups of 47146x over multi-threaded SPICE simulation."
      },
      {
        "id": "oai:arXiv.org:2507.05715v1",
        "title": "From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal Collaborative Filtering Recommendation",
        "link": "https://arxiv.org/abs/2507.05715",
        "author": "Guohao Li, Li Jing, Jia Wu, Xuefei Li, Kai Zhu, Yue He",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05715v1 Announce Type: cross \nAbstract: Most existing multimodal collaborative filtering recommendation (MCFRec) methods rely heavily on ID features and multimodal content to enhance recommendation performance. However, this paper reveals that ID features are effective but have limited benefits in multimodal collaborative filtering recommendation. Therefore, this paper systematically deconstruct the pros and cons of ID features: (i) they provide initial embedding but lack semantic richness, (ii) they provide a unique identifier for each user and item but hinder generalization to untrained data, and (iii) they assist in aligning and fusing multimodal features but may lead to representation shift. Based on these insights, this paper proposes IDFREE, an ID-free multimodal collaborative Filtering REcommEndation baseline. IDFREE replaces ID features with multimodal features and positional encodings to generate semantically meaningful ID-free embeddings. For ID-free multimodal collaborative filtering, it further proposes an adaptive similarity graph module to construct dynamic user-user and item-item graphs based on multimodal features. Then, an augmented user-item graph encoder is proposed to construct more effective user and item encoding. Finally, IDFREE achieves inter-multimodal alignment based on the contrastive learning and uses Softmax loss as recommendation loss. Basic experiments on three public datasets demonstrate that IDFREE outperforms existing ID-based MCFRec methods, achieving an average performance gain of 72.24% across standard metrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended experiments further validate our findings on the limitations of ID features in MCFRec. The code is released at https://github.com/G-H-Li/IDFREE."
      },
      {
        "id": "oai:arXiv.org:2507.05727v1",
        "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark",
        "link": "https://arxiv.org/abs/2507.05727",
        "author": "He Wang, Linhan Ma, Dake Guo, Xiong Wang, Lei Xie, Jin Xu, Junyang Lin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05727v1 Announce Type: cross \nAbstract: Automatic Speech Recognition (ASR) has been extensively investigated, yet prior evaluative efforts have largely been restricted to contextless paradigms. This constraint stems from the limited proficiency of conventional ASR models in context modeling and their deficiency in memory and reasoning based on world knowledge. Recent breakthroughs in the development of Large Language Models (LLMs) and corresponding Large Audio Language Models (LALMs) have markedly enhanced the visibility of general artificial intelligence capabilities. Consequently, there exists a compelling need for a benchmark that can evaluate both the generality and intelligence of ASR systems. To address this gap, we propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to assess contextual speech recognition. This benchmark encompasses up to 40,000 data entries across over 10 domains, enabling a thorough evaluation of model performance in scenarios that omit or incorporate coarse-grained or fine-grained contextual information. Moreover, diverging from conventional ASR evaluations, our benchmark includes an analysis of model efficacy in recognizing named entities mentioned within the auditory input. Our extensive evaluation highlights that LALMs, with strong world knowledge and context learning capabilities, outperform conventional ASR models by a large margin. The dataset and evaluation code have been released at https://github.com/MrSupW/ContextASR-Bench."
      },
      {
        "id": "oai:arXiv.org:2507.05731v1",
        "title": "A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation",
        "link": "https://arxiv.org/abs/2507.05731",
        "author": "Yuxin Zhang, Jiahao Yang, Zhe Chen, Wenjun Zhu, Jin Zhao, Yue Gao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05731v1 Announce Type: cross \nAbstract: Recently, large vision-language models (LVLMs) unleash powerful analysis capabilities for low Earth orbit (LEO) satellite Earth observation images in the data center. However, fast satellite motion, brief satellite-ground station (GS) contact windows, and large size of the images pose a data download challenge. To enable near real-time Earth observation applications (e.g., disaster and extreme weather monitoring), we should explore how to deploy LVLM in LEO satellite networks, and design SpaceVerse, an efficient satellite-ground synergistic LVLM inference system. To this end, firstly, we deploy compact LVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs to handle computationally intensive tasks. Then, we propose a computing and communication co-design framework comprised of a progressive confidence network and an attention-based multi-scale preprocessing, used to identify on-satellite inferring data, and reduce data redundancy before satellite-GS transmission, separately. We implement and evaluate SpaceVerse on real-world LEO satellite constellations and datasets, achieving a 31.2% average gain in accuracy and a 51.2% reduction in latency compared to state-of-the-art baselines."
      },
      {
        "id": "oai:arXiv.org:2507.05742v1",
        "title": "Tissue Concepts v2: a Supervised Foundation Model for whole slide images",
        "link": "https://arxiv.org/abs/2507.05742",
        "author": "Till Nicke, Daniela Scharcherer, Jan Raphael Sch\\\"afer, Natalia Artysh, Antje Prasse, Andr\\'e Homeyer, Andrea Schenk, Henning H\\\"ofener, Johannes Lotz",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05742v1 Announce Type: cross \nAbstract: Foundation models (FMs) are transforming the field of computational pathology by offering new approaches to analyzing histopathology images. Typically relying on weeks of training on large databases, the creation of FMs is a resource-intensive process in many ways. In this paper, we introduce the extension of our supervised foundation model, Tissue Concepts, to whole slide images, called Tissue Concepts v2 (TCv2), a supervised foundation model for whole slide images to address the issue above. TCv2 uses supervised, end-to-end multitask learning on slide-level labels. Training TCv2 uses a fraction of the training resources compared to self-supervised training. The presented model shows superior performance compared to SSL-trained models in cancer subtyping benchmarks and is fully trained on freely available data. Furthermore, a shared trained attention module provides an additional layer of explainability across different tasks."
      },
      {
        "id": "oai:arXiv.org:2507.05764v1",
        "title": "PSAT: Pediatric Segmentation Approaches via Adult Augmentations and Transfer Learning",
        "link": "https://arxiv.org/abs/2507.05764",
        "author": "Tristan Kirscher (ICube, ICANS), Sylvain Faisan (ICube), Xavier Coubez (ICANS), Loris Barrier (ICANS), Philippe Meyer (ICube, ICANS)",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05764v1 Announce Type: cross \nAbstract: Pediatric medical imaging presents unique challenges due to significant anatomical and developmental differences compared to adults. Direct application of segmentation models trained on adult data often yields suboptimal performance, particularly for small or rapidly evolving structures. To address these challenges, several strategies leveraging the nnU-Net framework have been proposed, differing along four key axes: (i) the fingerprint dataset (adult, pediatric, or a combination thereof) from which the Training Plan -including the network architecture-is derived; (ii) the Learning Set (adult, pediatric, or mixed), (iii) Data Augmentation parameters, and (iv) the Transfer learning method (finetuning versus continual learning). In this work, we introduce PSAT (Pediatric Segmentation Approaches via Adult Augmentations and Transfer learning), a systematic study that investigates the impact of these axes on segmentation performance. We benchmark the derived strategies on two pediatric CT datasets and compare them with state-of-theart methods, including a commercial radiotherapy solution. PSAT highlights key pitfalls and provides actionable insights for improving pediatric segmentation. Our experiments reveal that a training plan based on an adult fingerprint dataset is misaligned with pediatric anatomy-resulting in significant performance degradation, especially when segmenting fine structures-and that continual learning strategies mitigate institutional shifts, thus enhancing generalization across diverse pediatric datasets. The code is available at https://github.com/ICANS-Strasbourg/PSAT."
      },
      {
        "id": "oai:arXiv.org:2507.05785v1",
        "title": "Robust Bandwidth Estimation for Real-Time Communication with Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2507.05785",
        "author": "Jian Kai, Tianwei Zhang, Zihan Ling, Yang Cao, Can Shen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05785v1 Announce Type: cross \nAbstract: Accurate bandwidth estimation (BWE) is critical for real-time communication (RTC) systems. Traditional heuristic approaches offer limited adaptability under dynamic networks, while online reinforcement learning (RL) suffers from high exploration costs and potential service disruptions. Offline RL, which leverages high-quality data collected from real-world environments, offers a promising alternative. However, challenges such as out-of-distribution (OOD) actions, policy extraction from behaviorally diverse datasets, and reliable deployment in production systems remain unsolved. We propose RBWE, a robust bandwidth estimation framework based on offline RL that integrates Q-ensemble (an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD risks and enhance policy learning. A fallback mechanism ensures deployment stability by switching to heuristic methods under high uncertainty. Experimental results show that RBWE reduces overestimation errors by 18% and improves the 10th percentile Quality of Experience (QoE) by 18.6%, demonstrating its practical effectiveness in real-world RTC applications."
      },
      {
        "id": "oai:arXiv.org:2507.05815v1",
        "title": "Just Say Better or Worse: A Human-AI Collaborative Framework for Medical Image Segmentation Without Manual Annotations",
        "link": "https://arxiv.org/abs/2507.05815",
        "author": "Yizhe Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05815v1 Announce Type: cross \nAbstract: Manual annotation of medical images is a labor-intensive and time-consuming process, posing a significant bottleneck in the development and deployment of robust medical imaging AI systems. This paper introduces a novel Human-AI collaborative framework for medical image segmentation that substantially reduces the annotation burden by eliminating the need for explicit manual pixel-level labeling. The core innovation lies in a preference learning paradigm, where human experts provide minimal, intuitive feedback -- simply indicating whether an AI-generated segmentation is better or worse than a previous version. The framework comprises four key components: (1) an adaptable foundation model (FM) for feature extraction, (2) label propagation based on feature similarity, (3) a clicking agent that learns from human better-or-worse feedback to decide where to click and with which label, and (4) a multi-round segmentation learning procedure that trains a state-of-the-art segmentation network using pseudo-labels generated by the clicking agent and FM-based label propagation. Experiments on three public datasets demonstrate that the proposed approach achieves competitive segmentation performance using only binary preference feedback, without requiring experts to directly manually annotate the images."
      },
      {
        "id": "oai:arXiv.org:2507.05816v1",
        "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity",
        "link": "https://arxiv.org/abs/2507.05816",
        "author": "Shuai Zhao, Yulin Zhang, Luwei Xiao, Xinyi Wu, Yanhao Jia, Zhongliang Guo, Xiaobao Wu, Cong-Duy Nguyen, Guoming Zhang, Anh Tuan Luu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05816v1 Announce Type: cross \nAbstract: Despite the remarkable progress of large language models (LLMs) across various domains, their capacity to predict retinopathy of prematurity (ROP) risk remains largely unexplored. To address this gap, we introduce a novel Chinese benchmark dataset, termed CROP, comprising 993 admission records annotated with low, medium, and high-risk labels. To systematically examine the predictive capabilities and affective biases of LLMs in ROP risk stratification, we propose Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and ICL schemes leverage external medical knowledge to enhance predictive accuracy. Crucially, we integrate emotional elements at the prompt level to investigate how different affective framings influence the model's ability to predict ROP and its bias patterns. Empirical results derived from the CROP dataset yield two principal observations. First, LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, yet exhibit marked performance gains when augmented with structured external inputs. Second, affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Third, compared to negative emotions, positive emotional framing contributes to mitigating predictive bias in model outputs. These findings highlight the critical role of affect-sensitive prompt engineering in enhancing diagnostic reliability and emphasize the utility of Affective-ROPTester as a framework for evaluating and mitigating affective bias in clinical language modeling systems."
      },
      {
        "id": "oai:arXiv.org:2507.05829v1",
        "title": "Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing",
        "link": "https://arxiv.org/abs/2507.05829",
        "author": "Zekai Sun, Xiuxian Guan, Zheng Lin, Zihan Fang, Xiangming Cai, Zhe Chen, Fangming Liu, Heming Cui, Jie Xiong, Wei Ni, Chau Yuen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05829v1 Announce Type: cross \nAbstract: Deploying deep neural networks (DNNs) on resource-constrained mobile devices presents significant challenges, particularly in achieving real-time performance while simultaneously coping with limited computational resources and battery life. While Mobile Edge Computing (MEC) offers collaborative inference with GPU servers as a promising solution, existing approaches primarily rely on layer-wise model partitioning and undergo significant transmission bottlenecks caused by the sequential execution of DNN operations. To address this challenge, we present Intra-DP, a high-performance collaborative inference system optimized for DNN inference on MEC. Intra DP employs a novel parallel computing technique based on local operators (i.e., operators whose minimum unit input is not the entire input tensor, such as the convolution kernel). By decomposing their computations (operations) into several independent sub-operations and overlapping the computation and transmission of different sub-operations through parallel execution, Intra-DP mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient inference. The evaluation demonstrates that Intra-DP reduces per-inference latency by up to 50% and energy consumption by up to 75% compared to state-of-the-art baselines, without sacrificing accuracy."
      },
      {
        "id": "oai:arXiv.org:2507.05857v1",
        "title": "Property Elicitation on Imprecise Probabilities",
        "link": "https://arxiv.org/abs/2507.05857",
        "author": "James Bailie, Rabanus Derr",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05857v1 Announce Type: cross \nAbstract: Property elicitation studies which attributes of a probability distribution can be determined by minimising a risk. We investigate a generalisation of property elicitation to imprecise probabilities (IP). This investigation is motivated by multi-distribution learning, which takes the classical machine learning paradigm of minimising a single risk over a (precise) probability and replaces it with $\\Gamma$-maximin risk minimization over an IP. We provide necessary conditions for elicitability of a IP-property. Furthermore, we explain what an elicitable IP-property actually elicits through Bayes pairs -- the elicited IP-property is the corresponding standard property of the maximum Bayes risk distribution."
      },
      {
        "id": "oai:arXiv.org:2507.05861v1",
        "title": "Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments",
        "link": "https://arxiv.org/abs/2507.05861",
        "author": "Woonsang Kang, Joohyung Lee, Seungjun Kim, Jungchan Cho, Yoonseon Oh",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05861v1 Announce Type: cross \nAbstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy, but its reliance on large, diverse datasets creates significant data privacy and centralization challenges. Federated Learning (FL) offers a privacy-preserving solution, but its application to GPD is hindered by the substantial communication overhead of large models, a key issue for resource-constrained robots. To address this, we propose a novel module-wise FL framework that begins by analyzing the learning dynamics of the GPD model's functional components. This analysis identifies slower-converging modules, to which our framework then allocates additional communication effort. This is realized through a two-phase process: a standard full-model training phase is followed by a communication-efficient phase where only the identified subset of slower-converging modules is trained and their partial updates are aggregated. Extensive experiments on the GraspNet-1B dataset demonstrate that our method outperforms standard FedAvg and other baselines, achieving higher accuracy for a given communication budget. Furthermore, real-world experiments on a physical robot validate our approach, showing a superior grasp success rate compared to baseline methods in cluttered scenes. Our work presents a communication-efficient framework for training robust, generalized GPD models in a decentralized manner, effectively improving the trade-off between communication cost and model performance."
      },
      {
        "id": "oai:arXiv.org:2507.05883v1",
        "title": "A novel framework for fully-automated co-registration of intravascular ultrasound and optical coherence tomography imaging data",
        "link": "https://arxiv.org/abs/2507.05883",
        "author": "Xingwei He, Kit Mills Bransby, Ahmet Emir Ulutas, Thamil Kumaran, Nathan Angelo Lecaros Yap, Gonul Zeren, Hesong Zeng, Yaojun Zhang, Andreas Baumbach, James Moon, Anthony Mathur, Jouke Dijkstra, Qianni Zhang, Lorenz Raber, Christos V Bourantas",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05883v1 Announce Type: cross \nAbstract: Aims: To develop a deep-learning (DL) framework that will allow fully automated longitudinal and circumferential co-registration of intravascular ultrasound (IVUS) and optical coherence tomography (OCT) images. Methods and results: Data from 230 patients (714 vessels) with acute coronary syndrome that underwent near-infrared spectroscopy (NIRS)-IVUS and OCT imaging in their non-culprit vessels were included in the present analysis. The lumen borders annotated by expert analysts in 61,655 NIRS-IVUS and 62,334 OCT frames, and the side branches and calcific tissue identified in 10,000 NIRS-IVUS frames and 10,000 OCT frames, were used to train DL solutions for the automated extraction of these features. The trained DL solutions were used to process NIRS-IVUS and OCT images and their output was used by a dynamic time warping algorithm to co-register longitudinally the NIRS-IVUS and OCT images, while the circumferential registration of the IVUS and OCT was optimized through dynamic programming. On a test set of 77 vessels from 22 patients, the DL method showed high concordance with the expert analysts for the longitudinal and circumferential co-registration of the two imaging sets (concordance correlation coefficient >0.99 for the longitudinal and >0.90 for the circumferential co-registration). The Williams Index was 0.96 for longitudinal and 0.97 for circumferential co-registration, indicating a comparable performance to the analysts. The time needed for the DL pipeline to process imaging data from a vessel was <90s. Conclusion: The fully automated, DL-based framework introduced in this study for the co-registration of IVUS and OCT is fast and provides estimations that compare favorably to the expert analysts. These features renders it useful in research in the analysis of large-scale data collected in studies that incorporate multimodality imaging to characterize plaque composition."
      },
      {
        "id": "oai:arXiv.org:2507.05894v1",
        "title": "MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation",
        "link": "https://arxiv.org/abs/2507.05894",
        "author": "Fathinah Izzati, Xinyue Li, Yuxuan Wu, Gus Xia",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05894v1 Announce Type: cross \nAbstract: Humans can imagine various atmospheres and settings when listening to music, envisioning movie scenes that complement each piece. For example, slow, melancholic music might evoke scenes of heartbreak, while upbeat melodies suggest celebration. This paper explores whether a Music Language Model, e.g. MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI), which requires cross-modal information from video and music to train. To improve upon existing music captioning models which focusing solely on musical elements, we introduce MusiScene, a music captioning model designed to imagine scenes that complement each music. In this paper, (1) we construct a large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct comprehensive evaluations and prove that our MusiScene is more capable of generating contextually relevant captions compared to MU-LLaMA. We leverage the generated MSI captions to enhance Video Background Music Generation (VBMG) from text."
      },
      {
        "id": "oai:arXiv.org:2507.05900v1",
        "title": "Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning",
        "link": "https://arxiv.org/abs/2507.05900",
        "author": "Zengjing Chen, Lu Wang, Chengzhi Xing",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05900v1 Announce Type: cross \nAbstract: This study addresses the problem of stable acoustic relay assignment in an underwater acoustic network. Unlike the objectives of most existing literature, two distinct objectives, namely classical stable arrangement and ambiguous stable arrangement, are considered. To achieve these stable arrangements, a laser chaos-based multi-processing learning (LC-ML) method is introduced to efficiently obtain high throughput and rapidly attain stability. In order to sufficiently explore the relay's decision-making, this method uses random numbers generated by laser chaos to learn the assignment of relays to multiple source nodes. This study finds that the laser chaos-based random number and multi-processing in the exchange process have a positive effect on higher throughput and strong adaptability with environmental changing over time. Meanwhile, ambiguous cognitions result in the stable configuration with less volatility compared to accurate ones. This provides a practical and useful method and can be the basis for relay selection in complex underwater environments."
      },
      {
        "id": "oai:arXiv.org:2507.05903v1",
        "title": "AI-Reporter: A Path to a New Genre of Scientific Communication",
        "link": "https://arxiv.org/abs/2507.05903",
        "author": "Gerd Gra{\\ss}hoff",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05903v1 Announce Type: cross \nAbstract: The AI-Reporter represents a paradigmatic shift in scientific publication practice. This document demonstrates through a concrete case study how our system transforms academic presentations into publication-ready chapters -- in less than three minutes. Using Arno Simons' lecture on Large Language Models from the ``Large Language Models for the History, Philosophy, and Sociology of Science'' workshop (NEPI) as an example, we show how technological innovation bridges the gap between ephemeral presentation and permanent scientific documentation."
      },
      {
        "id": "oai:arXiv.org:2507.05913v1",
        "title": "Best-of-N through the Smoothing Lens: KL Divergence and Regret Analysis",
        "link": "https://arxiv.org/abs/2507.05913",
        "author": "Gholamali Aminian, Idan Shenfeld, Amir R. Asadi, Ahmad Beirami, Youssef Mroueh",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05913v1 Announce Type: cross \nAbstract: A simple yet effective method for inference-time alignment of generative models is Best-of-$N$ (BoN), where $N$ outcomes are sampled from a reference policy, evaluated using a proxy reward model, and the highest-scoring one is selected. While prior work argues that BoN is almost optimal in reward vs KL tradeoffs, the effectiveness of BoN depends critically on the quality of the proxy reward model used for selection. For this purpose, we study BoN through a smooth version known as Soft Best-of-N (SBoN) and develop a theoretical framework to address this gap. We analyze the scaling behaviour of BoN by providing bounds on the KL divergence between the SBoN policy and the reference policy, offering insights into how performance varies with the number of samples. We also study the regret gap, i.e., the gap between the expected true reward under the optimal policy and the SBoN policy. Our theoretical and empirical findings show that smoothing helps SBoN mitigate reward overoptimization, especially when the quality of the proxy reward is low."
      },
      {
        "id": "oai:arXiv.org:2507.05929v1",
        "title": "Online Regularized Learning Algorithms in RKHS with $\\beta$- and $\\phi$-Mixing Sequences",
        "link": "https://arxiv.org/abs/2507.05929",
        "author": "Priyanka Roy, Susanne Saminger-Platz",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05929v1 Announce Type: cross \nAbstract: In this paper, we study an online regularized learning algorithm in a reproducing kernel Hilbert spaces (RKHS) based on a class of dependent processes. We choose such a process where the degree of dependence is measured by mixing coefficients. As a representative example, we analyze a strictly stationary Markov chain, where the dependence structure is characterized by the \\(\\phi\\)- and \\(\\beta\\)-mixing coefficients. Under these assumptions, we derive probabilistic upper bounds as well as convergence rates for both the exponential and polynomial decay of the mixing coefficients."
      },
      {
        "id": "oai:arXiv.org:2507.05932v1",
        "title": "TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems",
        "link": "https://arxiv.org/abs/2507.05932",
        "author": "You Lu, Dingji Wang, Kaifeng Huang, Bihuan Chen, Xin Peng",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05932v1 Announce Type: cross \nAbstract: Autonomous vehicle technology has been developed in the last decades with recent advances in sensing and computing technology. There is an urgent need to ensure the reliability and robustness of autonomous driving systems (ADSs). Despite the recent achievements in testing various ADS modules, little attention has been paid on the automated testing of traffic light detection models in ADSs. A common practice is to manually collect and label traffic light data. However, it is labor-intensive, and even impossible to collect diverse data under different driving environments.\n  To address these problems, we propose and implement TigAug to automatically augment labeled traffic light images for testing traffic light detection models in ADSs. We construct two families of metamorphic relations and three families of transformations based on a systematic understanding of weather environments, camera properties, and traffic light properties. We use augmented images to detect erroneous behaviors of traffic light detection models by transformation-specific metamorphic relations, and to improve the performance of traffic light detection models by retraining. Large-scale experiments with four state-of-the-art traffic light detection models and two traffic light datasets have demonstrated that i) TigAug is effective in testing traffic light detection models, ii) TigAug is efficient in synthesizing traffic light images, and iii) TigAug generates traffic light images with acceptable naturalness."
      },
      {
        "id": "oai:arXiv.org:2507.05933v1",
        "title": "Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation",
        "link": "https://arxiv.org/abs/2507.05933",
        "author": "Y. Du",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05933v1 Announce Type: cross \nAbstract: Vector retrieval systems exhibit significant performance variance across queries due to heterogeneous embedding quality. We propose a lightweight framework for predicting retrieval performance at the query level by combining quantization robustness and neighborhood density metrics. Our approach is motivated by the observation that high-quality embeddings occupy geometrically stable regions in the embedding space and exhibit consistent neighborhood structures. We evaluate our method on 4 standard retrieval datasets, showing consistent improvements of 9.4$\\pm$1.2\\% in Recall@10 over competitive baselines. The framework requires minimal computational overhead (less than 5\\% of retrieval time) and enables adaptive retrieval strategies. Our analysis reveals systematic patterns in embedding quality across different query types, providing insights for targeted training data augmentation."
      },
      {
        "id": "oai:arXiv.org:2507.05938v1",
        "title": "A Wireless Foundation Model for Multi-Task Prediction",
        "link": "https://arxiv.org/abs/2507.05938",
        "author": "Yucheng Sheng, Jiacheng Wang, Xingyu Zhou, Le Liang, Hao Ye, Shi Jin, Geoffrey Ye Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05938v1 Announce Type: cross \nAbstract: With the growing complexity and dynamics of the mobile communication networks, accurately predicting key system parameters, such as channel state information (CSI), user location, and network traffic, has become essential for a wide range of physical (PHY)-layer and medium access control (MAC)-layer tasks. Although traditional deep learning (DL)-based methods have been widely applied to such prediction tasks, they often struggle to generalize across different scenarios and tasks. In response, we propose a unified foundation model for multi-task prediction in wireless networks that supports diverse prediction intervals. The proposed model enforces univariate decomposition to unify heterogeneous tasks, encodes granularity for interval awareness, and uses a causal Transformer backbone for accurate predictions. Additionally, we introduce a patch masking strategy during training to support arbitrary input lengths. After trained on large-scale datasets, the proposed foundation model demonstrates strong generalization to unseen scenarios and achieves zero-shot performance on new tasks that surpass traditional full-shot baselines."
      },
      {
        "id": "oai:arXiv.org:2507.05972v1",
        "title": "Generalized and Unified Equivalences between Hardness and Pseudoentropy",
        "link": "https://arxiv.org/abs/2507.05972",
        "author": "Lunjia Hu, Salil Vadhan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05972v1 Announce Type: cross \nAbstract: Pseudoentropy characterizations provide a quantitatively precise demonstration of the close relationship between computational hardness and computational randomness. We prove a unified pseudoentropy characterization that generalizes and strengthens previous results for both uniform and non-uniform models of computation. Our characterization holds for a general family of entropy notions that encompasses the common notions of Shannon entropy and min entropy as special cases. Moreover, we show that the characterizations for different entropy notions can be simultaneously achieved by a single, universal function that simultaneously witnesses computational hardness and computational randomness. A key technical insight of our work is that the notion of weight-restricted calibration from the recent literature on algorithm fairness, along with standard computational indistinguishability (known as multiaccuracy in the fairness literature), suffices for proving pseudoentropy characterizations for general entropy notions. This demonstrates the power of weight-restricted calibration to enhance the classic Complexity-Theoretic Regularity Lemma (Trevisan, Tulsiani, and Vadhan, 2009) and Leakage Simulation Lemma (Jetchev and Pietrzak, 2014) and allows us to achieve an exponential improvement in the complexity dependency on the alphabet size compared to the pseudoentropy characterizations by Casacuberta, Dwork, and Vadhan (2024) based on the much stronger notion of multicalibration. We show that the exponential dependency on the alphabet size is inevitable for multicalibration as well as for the weaker notion of calibrated multiaccuracy."
      },
      {
        "id": "oai:arXiv.org:2507.05984v1",
        "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening",
        "link": "https://arxiv.org/abs/2507.05984",
        "author": "Zhijun Guo, Alvina Lai, Julia Ive, Alexandru Petcu, Yutong Wang, Luyuan Qi, Johan H Thygesen, Kezhi Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05984v1 Announce Type: cross \nAbstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively screen depression but lack interactivity and adaptability. We developed HopeBot, a chatbot powered by a large language model (LLM) that administers the PHQ-9 using retrieval-augmented generation and real-time clarification. In a within-subject study, 132 adults in the United Kingdom and China completed both self-administered and chatbot versions. Scores demonstrated strong agreement (ICC = 0.91; 45% identical). Among 75 participants providing comparative feedback, 71% reported greater trust in the chatbot, highlighting clearer structure, interpretive guidance, and a supportive tone. Mean ratings (0-10) were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics, and 7.4 for recommendation helpfulness; the latter varied significantly by employment status and prior mental-health service use (p < 0.05). Overall, 87.1% expressed willingness to reuse or recommend HopeBot. These findings demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden adjuncts for routine depression screening."
      },
      {
        "id": "oai:arXiv.org:2507.05985v1",
        "title": "Robust Speech-Workload Estimation for Intelligent Human-Robot Systems",
        "link": "https://arxiv.org/abs/2507.05985",
        "author": "Julian Fortune, Julie A. Adams, Jamison Heard",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05985v1 Announce Type: cross \nAbstract: Demanding task environments (e.g., supervising a remotely piloted aircraft) require performing tasks quickly and accurately; however, periods of low and high operator workload can decrease task performance. Intelligent modulation of the system's demands and interaction modality in response to changes in operator workload state may increase performance by avoiding undesirable workload states. This system requires real-time estimation of each workload component (i.e., cognitive, physical, visual, speech, and auditory) to adapt the correct modality. Existing workload systems estimate multiple workload components post-hoc, but few estimate speech workload, or function in real-time. An algorithm to estimate speech workload and mitigate undesirable workload states in real-time is presented. An analysis of the algorithm's accuracy is presented, along with the results demonstrating the algorithm's generalizability across individuals and human-machine teaming paradigms. Real-time speech workload estimation is a crucial element towards developing adaptive human-machine systems."
      },
      {
        "id": "oai:arXiv.org:2507.05994v1",
        "title": "Beating the Best Constant Rebalancing Portfolio in Long-Term Investment: A Generalization of the Kelly Criterion and Universal Learning Algorithm for Markets with Serial Dependence",
        "link": "https://arxiv.org/abs/2507.05994",
        "author": "Duy Khanh Lam",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05994v1 Announce Type: cross \nAbstract: In the online portfolio optimization framework, existing learning algorithms generate strategies that yield significantly poorer cumulative wealth compared to the best constant rebalancing portfolio in hindsight, despite being consistent in asymptotic growth rate. While this unappealing performance can be improved by incorporating more side information, it raises difficulties in feature selection and high-dimensional settings. Instead, the inherent serial dependence of assets' returns, such as day-of-the-week and other calendar effects, can be leveraged. Although latent serial dependence patterns are commonly detected using large training datasets, this paper proposes an algorithm that learns such dependence using only gradually revealed data, without any assumption on their distribution, to form a strategy that eventually exceeds the cumulative wealth of the best constant rebalancing portfolio.\n  Moreover, the classical Kelly criterion, which requires independent assets' returns, is generalized to accommodate serial dependence in a market modeled as an independent and identically distributed process of random matrices. In such a stochastic market, where existing learning algorithms designed for stationary processes fail to apply, the proposed learning algorithm still generates a strategy that asymptotically grows to the highest rate among all strategies, matching that of the optimal strategy constructed under the generalized Kelly criterion. The experimental results with real market data demonstrate the theoretical guarantees of the algorithm and its performance as expected, as long as serial dependence is significant, regardless of the validity of the generalized Kelly criterion in the experimental market. This further affirms the broad applicability of the algorithm in general contexts."
      },
      {
        "id": "oai:arXiv.org:2507.06010v1",
        "title": "Instance-Optimal Quantum State Certification with Entangled Measurements",
        "link": "https://arxiv.org/abs/2507.06010",
        "author": "Ryan O'Donnell, Chirag Wadhwa",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06010v1 Announce Type: cross \nAbstract: We consider the task of quantum state certification: given a description of a hypothesis state $\\sigma$ and multiple copies of an unknown state $\\rho$, a tester aims to determine whether the two states are equal or $\\epsilon$-far in trace distance. It is known that $\\Theta(d/\\epsilon^2)$ copies of $\\rho$ are necessary and sufficient for this task, assuming the tester can make entangled measurements over all copies [CHW07,OW15,BOW19]. However, these bounds are for a worst-case $\\sigma$, and it is not known what the optimal copy complexity is for this problem on an instance-by-instance basis. While such instance-optimal bounds have previously been shown for quantum state certification when the tester is limited to measurements unentangled across copies [CLO22,CLHL22], they remained open when testers are unrestricted in the kind of measurements they can perform.\n  We address this open question by proving nearly instance-optimal bounds for quantum state certification when the tester can perform fully entangled measurements. Analogously to the unentangled setting, we show that the optimal copy complexity for certifying $\\sigma$ is given by the worst-case complexity times the fidelity between $\\sigma$ and the maximally mixed state. We prove our lower bounds using a novel quantum analogue of the Ingster-Suslina method, which is likely to be of independent interest. This method also allows us to recover the $\\Omega(d/\\epsilon^2)$ lower bound for mixedness testing [OW15], i.e., certification of the maximally mixed state, with a surprisingly simple proof."
      },
      {
        "id": "oai:arXiv.org:2507.06011v1",
        "title": "ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge",
        "link": "https://arxiv.org/abs/2507.06011",
        "author": "Daghash K. Alqahtani, Maria A. Rodriguez, Muhammad Aamir Cheema, Hamid Rezatofighi, Adel N. Toosi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06011v1 Announce Type: cross \nAbstract: Edge computing enables data processing closer to the source, significantly reducing latency an essential requirement for real-time vision-based analytics such as object detection in surveillance and smart city environments. However, these tasks place substantial demands on resource constrained edge devices, making the joint optimization of energy consumption and detection accuracy critical. To address this challenge, we propose ECORE, a framework that integrates multiple dynamic routing strategies including estimation based techniques and a greedy selection algorithm to direct image processing requests to the most suitable edge device-model pair. ECORE dynamically balances energy efficiency and detection performance based on object characteristics. We evaluate our approach through extensive experiments on real-world datasets, comparing the proposed routers against widely used baseline techniques. The evaluation leverages established object detection models (YOLO, SSD, EfficientDet) and diverse edge platforms, including Jetson Orin Nano, Raspberry Pi 4 and 5, and TPU accelerators. Results demonstrate that our proposed context-aware routing strategies can reduce energy consumption and latency by 45% and 49%, respectively, while incurring only a 2% loss in detection accuracy compared to accuracy-centric methods."
      },
      {
        "id": "oai:arXiv.org:2507.06031v1",
        "title": "Efficient Federated Learning with Timely Update Dissemination",
        "link": "https://arxiv.org/abs/2507.06031",
        "author": "Juncheng Jia, Ji Liu, Chao Huo, Yihui Shen, Yang Zhou, Huaiyu Dai, Dejing Dou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06031v1 Announce Type: cross \nAbstract: Federated Learning (FL) has emerged as a compelling methodology for the management of distributed data, marked by significant advancements in recent years. In this paper, we propose an efficient FL approach that capitalizes on additional downlink bandwidth resources to ensure timely update dissemination. Initially, we implement this strategy within an asynchronous framework, introducing the Asynchronous Staleness-aware Model Update (FedASMU), which integrates both server-side and device-side methodologies. On the server side, we present an asynchronous FL system model that employs a dynamic model aggregation technique, which harmonizes local model updates with the global model to enhance both accuracy and efficiency. Concurrently, on the device side, we propose an adaptive model adjustment mechanism that integrates the latest global model with local models during training to further elevate accuracy. Subsequently, we extend this approach to a synchronous context, referred to as FedSSMU. Theoretical analyses substantiate the convergence of our proposed methodologies. Extensive experiments, encompassing six models and five public datasets, demonstrate that FedASMU and FedSSMU significantly surpass baseline methods in terms of both accuracy (up to 145.87%) and efficiency (up to 97.59%)."
      },
      {
        "id": "oai:arXiv.org:2507.06038v1",
        "title": "Fredholm Neural Networks for forward and inverse problems in elliptic PDEs",
        "link": "https://arxiv.org/abs/2507.06038",
        "author": "Kyriakos Georgiou, Constantinos Siettos, Athanasios N. Yannacopoulos",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06038v1 Announce Type: cross \nAbstract: Building on our previous work introducing Fredholm Neural Networks (Fredholm NNs/ FNNs) for solving integral equations, we extend the framework to tackle forward and inverse problems for linear and semi-linear elliptic partial differential equations. The proposed scheme consists of a deep neural network (DNN) which is designed to represent the iterative process of fixed-point iterations for the solution of elliptic PDEs using the boundary integral method within the framework of potential theory. The number of layers, weights, biases and hyperparameters are computed in an explainable manner based on the iterative scheme, and we therefore refer to this as the Potential Fredholm Neural Network (PFNN). We show that this approach ensures both accuracy and explainability, achieving small errors in the interior of the domain, and near machine-precision on the boundary. We provide a constructive proof for the consistency of the scheme and provide explicit error bounds for both the interior and boundary of the domain, reflected in the layers of the PFNN. These error bounds depend on the approximation of the boundary function and the integral discretization scheme, both of which directly correspond to components of the Fredholm NN architecture. In this way, we provide an explainable scheme that explicitly respects the boundary conditions. We assess the performance of the proposed scheme for the solution of both the forward and inverse problem for linear and semi-linear elliptic PDEs in two dimensions."
      },
      {
        "id": "oai:arXiv.org:2507.06050v1",
        "title": "Minimal Deterministic Echo State Networks Outperform Random Reservoirs in Learning Chaotic Dynamics",
        "link": "https://arxiv.org/abs/2507.06050",
        "author": "Francesco Martinuzzi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06050v1 Announce Type: cross \nAbstract: Machine learning (ML) is widely used to model chaotic systems. Among ML approaches, echo state networks (ESNs) have received considerable attention due to their simple construction and fast training. However, ESN performance is highly sensitive to hyperparameter choices and to its random initialization. In this work, we demonstrate that ESNs constructed using deterministic rules and simple topologies (MESNs) outperform standard ESNs in the task of chaotic attractor reconstruction. We use a dataset of more than 90 chaotic systems to benchmark 10 different minimal deterministic reservoir initializations. We find that MESNs obtain up to a 41% reduction in error compared to standard ESNs. Furthermore, we show that the MESNs are more robust, exhibiting less inter-run variation, and have the ability to reuse hyperparameters across different systems. Our results illustrate how structured simplicity in ESN design can outperform stochastic complexity in learning chaotic dynamics."
      },
      {
        "id": "oai:arXiv.org:2507.06055v1",
        "title": "Kernel Trace Distance: Quantum Statistical Metric between Measures through RKHS Density Operators",
        "link": "https://arxiv.org/abs/2507.06055",
        "author": "Arturo Castellanos, Anna Korba, Pavlo Mozharovskyi, Hicham Janati",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06055v1 Announce Type: cross \nAbstract: Distances between probability distributions are a key component of many statistical machine learning tasks, from two-sample testing to generative modeling, among others. We introduce a novel distance between measures that compares them through a Schatten norm of their kernel covariance operators. We show that this new distance is an integral probability metric that can be framed between a Maximum Mean Discrepancy (MMD) and a Wasserstein distance. In particular, we show that it avoids some pitfalls of MMD, by being more discriminative and robust to the choice of hyperparameters. Moreover, it benefits from some compelling properties of kernel methods, that can avoid the curse of dimensionality for their sample complexity. We provide an algorithm to compute the distance in practice by introducing an extension of kernel matrix for difference of distributions that could be of independent interest. Those advantages are illustrated by robust approximate Bayesian computation under contamination as well as particle flow simulations."
      },
      {
        "id": "oai:arXiv.org:2507.06057v1",
        "title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models",
        "link": "https://arxiv.org/abs/2507.06057",
        "author": "Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06057v1 Announce Type: cross \nAbstract: Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models -- C32B, S32B, R32B -- from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation"
      },
      {
        "id": "oai:arXiv.org:2507.06061v1",
        "title": "Estimating prevalence with precision and accuracy",
        "link": "https://arxiv.org/abs/2507.06061",
        "author": "Aime Bienfait Igiraneza, Christophe Fraser, Robert Hinch",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06061v1 Announce Type: cross \nAbstract: Unlike classification, whose goal is to estimate the class of each data point in a dataset, prevalence estimation or quantification is a task that aims to estimate the distribution of classes in a dataset. The two main tasks in prevalence estimation are to adjust for bias, due to the prevalence in the training dataset, and to quantify the uncertainty in the estimate. The standard methods used to quantify uncertainty in prevalence estimates are bootstrapping and Bayesian quantification methods. It is not clear which approach is ideal in terms of precision (i.e. the width of confidence intervals) and coverage (i.e. the confidence intervals being well-calibrated). Here, we propose Precise Quantifier (PQ), a Bayesian quantifier that is more precise than existing quantifiers and with well-calibrated coverage. We discuss the theory behind PQ and present experiments based on simulated and real-world datasets. Through these experiments, we establish the factors which influence quantification precision: the discriminatory power of the underlying classifier; the size of the labeled dataset used to train the quantifier; and the size of the unlabeled dataset for which prevalence is estimated. Our analysis provides deep insights into uncertainty quantification for quantification learning."
      },
      {
        "id": "oai:arXiv.org:2507.06067v1",
        "title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End Registration",
        "link": "https://arxiv.org/abs/2507.06067",
        "author": "Maximilian Tschuchnig, Lukas Lamminger, Philipp Steininger, Michael Gadermayr",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06067v1 Announce Type: cross \nAbstract: Cone-Beam Computed Tomography (CBCT) is widely used for intraoperative imaging due to its rapid acquisition and low radiation dose. However, CBCT images typically suffer from artifacts and lower visual quality compared to conventional Computed Tomography (CT). A promising solution is synthetic CT (sCT) generation, where CBCT volumes are translated into the CT domain. In this work, we enhance sCT generation through multimodal learning by jointly leveraging intraoperative CBCT and preoperative CT data. To overcome the inherent misalignment between modalities, we introduce an end-to-end learnable registration module within the sCT pipeline. This model is evaluated on a controlled synthetic dataset, allowing precise manipulation of data quality and alignment parameters. Further, we validate its robustness and generalizability on two real-world clinical datasets. Experimental results demonstrate that integrating registration in multimodal sCT generation improves sCT quality, outperforming baseline multimodal methods in 79 out of 90 evaluation settings. Notably, the improvement is most significant in cases where CBCT quality is low and the preoperative CT is moderately misaligned."
      },
      {
        "id": "oai:arXiv.org:2507.06070v1",
        "title": "Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol",
        "link": "https://arxiv.org/abs/2507.06070",
        "author": "Christos Nikou, Theodoros Giannakopoulos",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06070v1 Announce Type: cross \nAbstract: Recent advances in song identification leverage deep neural networks to learn compact audio fingerprints directly from raw waveforms. While these methods perform well under controlled conditions, their accuracy drops significantly in real-world scenarios where the audio is captured via mobile devices in noisy environments. In this paper, we introduce a novel evaluation protocol designed to better reflect such real-world conditions. We generate three recordings of the same audio, each with increasing levels of noise, captured using a mobile device's microphone. Our results reveal a substantial performance drop for two state-of-the-art CNN-based models under this protocol, compared to previously reported benchmarks. Additionally, we highlight the critical role of the augmentation pipeline during training with contrastive loss. By introduction low pass and high pass filters in the augmentation pipeline we significantly increase the performance of both systems in our proposed evaluation. Furthermore, we develop a transformer-based model with a tailored projection module and demonstrate that transferring knowledge from a semantically relevant domain yields a more robust solution. The transformer architecture outperforms CNN-based models across all noise levels, and query durations. In low noise conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in finding the correct song, surpassing by 14%, and by 18.5% the second-best performing model, respectively, Under heavy noise levels, we achieve a detection rate 56.5% for 15-second query duration. All experiments are conducted on public large-scale dataset of over 100K songs, with queries matched against a database of 56 million vectors."
      },
      {
        "id": "oai:arXiv.org:2507.06090v1",
        "title": "Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India",
        "link": "https://arxiv.org/abs/2507.06090",
        "author": "Swapnil Bhattacharyya, Shrey Ganatra, Harshvivek Kashid, Spandan Anaokar, Shruti Nair, Reshma Sekhar, Siddharth Manohar, Rahul Hemrajani, Pushpak Bhattacharyya",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06090v1 Announce Type: cross \nAbstract: AI-based judicial assistance and case prediction have been extensively studied in criminal and civil domains, but remain largely unexplored in consumer law, especially in India. In this paper, we present Nyay-Darpan, a novel two-in-one framework that (i) summarizes consumer case files and (ii) retrieves similar case judgements to aid decision-making in consumer dispute resolution. Our methodology not only addresses the gap in consumer law AI tools but also introduces an innovative approach to evaluate the quality of the summary. The term 'Nyay-Darpan' translates into 'Mirror of Justice', symbolizing the ability of our tool to reflect the core of consumer disputes through precise summarization and intelligent case retrieval. Our system achieves over 75 percent accuracy in similar case prediction and approximately 70 percent accuracy across material summary evaluation metrics, demonstrating its practical effectiveness. We will publicly release the Nyay-Darpan framework and dataset to promote reproducibility and facilitate further research in this underexplored yet impactful domain."
      },
      {
        "id": "oai:arXiv.org:2507.06092v1",
        "title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI",
        "link": "https://arxiv.org/abs/2507.06092",
        "author": "Shravya Kanchi, Neal Mangaokar, Aravind Cheruvu, Sifat Muhammad Abdullah, Shirin Nilizadeh, Atul Prakash, Bimal Viswanath",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06092v1 Announce Type: cross \nAbstract: Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks."
      },
      {
        "id": "oai:arXiv.org:2507.06109v1",
        "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures",
        "link": "https://arxiv.org/abs/2507.06109",
        "author": "Seungoh Han, Jaehoon Jang, Hyunsu Kim, Jaeheung Surh, Junhyung Kwak, Hyowon Ha, Kyungdon Joo",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06109v1 Announce Type: cross \nAbstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement."
      },
      {
        "id": "oai:arXiv.org:2507.06140v1",
        "title": "LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models",
        "link": "https://arxiv.org/abs/2507.06140",
        "author": "Zhihao Chen, Tao Chen, Chenhui Wang, Qi Gao, Huidong Xie, Chuang Niu, Ge Wang, Hongming Shan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06140v1 Announce Type: cross \nAbstract: Low-dose computed tomography (LDCT) reduces radiation exposure but often degrades image quality, potentially compromising diagnostic accuracy. Existing deep learning-based denoising methods focus primarily on pixel-level mappings, overlooking the potential benefits of high-level semantic guidance. Recent advances in vision-language models (VLMs) suggest that language can serve as a powerful tool for capturing structured semantic information, offering new opportunities to improve LDCT reconstruction. In this paper, we introduce LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages VLM-derived representations to enhance supervision from normal-dose CT (NDCT). LangMamba follows a two-stage learning strategy. First, we pre-train a Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT images into a semantic space enriched with anatomical information. Second, we synergize LangAE with two key components to guide LDCT denoising: Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local semantic while capturing global features with efficient Mamba mechanism, and Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that denoised images align with NDCT in both perceptual and semantic spaces. Extensive experiments on two public datasets demonstrate that LangMamba outperforms conventional state-of-the-art methods, significantly improving detail preservation and visual fidelity. Remarkably, LangAE exhibits strong generalizability to unseen datasets, thereby reducing training costs. Furthermore, LangDA loss improves explainability by integrating language-guided insights into image reconstruction and offers a plug-and-play fashion. Our findings shed new light on the potential of language as a supervisory signal to advance LDCT denoising. The code is publicly available on https://github.com/hao1635/LangMamba."
      },
      {
        "id": "oai:arXiv.org:2507.06157v1",
        "title": "Evaluation of Habitat Robotics using Large Language Models",
        "link": "https://arxiv.org/abs/2507.06157",
        "author": "William Li, Lei Hamilton, Kaise Al-natour, Sanjeev Mohindra",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06157v1 Announce Type: cross \nAbstract: This paper focuses on evaluating the effectiveness of Large Language Models at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR provides simplified environments and robotic interactions within randomized indoor kitchen scenes. Each randomized kitchen scene is given a task where two robotic agents cooperatively work together to solve the task. We evaluated multiple frontier models on Meta PARTNER environments. Our results indicate that reasoning models like OpenAI o3-mini outperform non-reasoning models like OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied environments. o3-mini displayed outperform across centralized, decentralized, full observability, and partial observability configurations. This provides a promising avenue of research for embodied robotic development."
      },
      {
        "id": "oai:arXiv.org:2507.06185v1",
        "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
        "link": "https://arxiv.org/abs/2507.06185",
        "author": "Zhicheng Lin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06185v1 Announce Type: cross \nAbstract: In July 2025, 18 academic manuscripts on the preprint website arXiv were found to contain hidden instructions known as prompts designed to manipulate AI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\" were concealed using techniques like white-colored text. Author responses varied: one planned to withdraw the affected paper, while another defended the practice as legitimate testing of reviewer compliance. This commentary analyzes this practice as a novel form of research misconduct. We examine the technique of prompt injection in large language models (LLMs), revealing four types of hidden prompts, ranging from simple positive review commands to detailed evaluation frameworks. The defense that prompts served as \"honeypots\" to detect reviewers improperly using AI fails under examination--the consistently self-serving nature of prompt instructions indicates intent to manipulate. Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer review entirely, while Springer Nature permits limited use with disclosure requirements. The incident exposes systematic vulnerabilities extending beyond peer review to any automated system processing scholarly texts, including plagiarism detection and citation indexing. Our analysis underscores the need for coordinated technical screening at submission portals and harmonized policies governing generative AI (GenAI) use in academic evaluation."
      },
      {
        "id": "oai:arXiv.org:2507.06190v1",
        "title": "Conservative approximation-based feedforward neural network for WENO schemes",
        "link": "https://arxiv.org/abs/2507.06190",
        "author": "Kwanghyuk Park, Jiaxi Gu, Jae-Hun Jung",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06190v1 Announce Type: cross \nAbstract: In this work, we present the feedforward neural network based on the conservative approximation to the derivative from point values, for the weighted essentially non-oscillatory (WENO) schemes in solving hyperbolic conservation laws. The feedforward neural network, whose inputs are point values from the three-point stencil and outputs are two nonlinear weights, takes the place of the classical WENO weighting procedure. For the training phase, we employ the supervised learning and create a new labeled dataset for one-dimensional conservative approximation, where we construct a numerical flux function from the given point values such that the flux difference approximates the derivative to high-order accuracy. The symmetric-balancing term is introduced for the loss function so that it propels the neural network to match the conservative approximation to the derivative and satisfy the symmetric property that WENO3-JS and WENO3-Z have in common. The consequent WENO schemes, WENO3-CADNNs, demonstrate robust generalization across various benchmark scenarios and resolutions, where they outperform WENO3-Z and achieve accuracy comparable to WENO5-JS."
      },
      {
        "id": "oai:arXiv.org:2507.06192v1",
        "title": "SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads",
        "link": "https://arxiv.org/abs/2507.06192",
        "author": "Jiale Lao, Immanuel Trummer",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06192v1 Announce Type: cross \nAbstract: Database research and development often require a large number of SQL queries for benchmarking purposes. However, acquiring real-world SQL queries is challenging due to privacy concerns, and existing SQL generation methods are limited in customization and in satisfying realistic constraints. To address this issue, we present SQLBarber, a system based on Large Language Models (LLMs) to generate customized and realistic SQL workloads. SQLBarber (i) eliminates the need for users to manually craft SQL templates in advance, while providing the flexibility to accept natural language specifications to constrain SQL templates, (ii) scales efficiently to generate large volumes of queries matching any user-defined cost distribution (e.g., cardinality and execution plan cost), and (iii) uses execution statistics from Amazon Redshift and Snowflake to derive SQL template specifications and query cost distributions that reflect real-world query characteristics. SQLBarber introduces (i) a declarative interface for users to effortlessly generate customized SQL templates, (ii) an LLM-powered pipeline augmented with a self-correction module that profiles, refines, and prunes SQL templates based on query costs, and (iii) a Bayesian Optimizer to efficiently explore different predicate values and identify a set of queries that satisfy the target cost distribution. We construct and open-source ten benchmarks of varying difficulty levels and target query cost distributions based on real-world statistics from Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show that SQLBarber is the only system that can generate customized SQL templates. It reduces query generation time by one to three orders of magnitude, and significantly improves alignment with the target cost distribution, compared with existing methods."
      },
      {
        "id": "oai:arXiv.org:2507.06193v1",
        "title": "Identity isn't everything -- how far do demographics take us towards self-identified party ID?",
        "link": "https://arxiv.org/abs/2507.06193",
        "author": "Sabina Tomkins, David Rothschild, Alex Liu, Alexander Thompson",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06193v1 Announce Type: cross \nAbstract: How well do demographics explain party identification? Demographics are related to party identification in political polls, news articles, and academic publications. Yet, there is a diversity of party identification even within demographic groups which have historically been attached to one party. And some groups lack a clear connection to either party. It may be that demographics on their own fail to account for the fact that people generally belong to a variety of groups. They must select the groups which are most important to them when shaping a political identity, and may choose to construct an identity relatively unattached to any specific demographic group to which they belong. This prompts the question, do we need to consider measures of identity strength when using demographics to explain party identification? We utilize a predictive framework to address these questions and find that demographics are highly predictive for some groups (e.g., Black Democrats), while others benefit from the inclusion of identity strength (e.g., Hispanic Republicans)."
      },
      {
        "id": "oai:arXiv.org:2507.06217v1",
        "title": "What ZTF Saw Where Rubin Looked: Anomaly Hunting in DR23",
        "link": "https://arxiv.org/abs/2507.06217",
        "author": "Maria V. Pruzhinskaya, Anastasia D. Lavrukhina, Timofey A. Semenikhi, Alina A. Volnova, Sreevarsha Sreejith, Vadim V. Krushinsky, Emmanuel Gangler, Emille E. O. Ishida, Matwey V. Kornilov, Konstantin L. Malanchev",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06217v1 Announce Type: cross \nAbstract: We present results from the SNAD VIII Workshop, during which we conducted the first systematic anomaly search in the ZTF fields also observed by LSSTComCam during Rubin Scientific Pipeline commissioning. Using the PineForest active anomaly detection algorithm, we analysed four selected fields (two galactic and two extragalactic) and visually inspected 400 candidates. As a result, we discovered six previously uncatalogued variable stars, including RS~CVn, BY Draconis, ellipsoidal, and solar-type variables, and refined classifications and periods for six known objects. These results demonstrate the effectiveness of the SNAD anomaly detection pipeline and provide a preview of the discovery potential in the upcoming LSST data."
      },
      {
        "id": "oai:arXiv.org:2507.06219v1",
        "title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
        "link": "https://arxiv.org/abs/2507.06219",
        "author": "Modi Shi, Li Chen, Jin Chen, Yuxiang Lu, Chiming Liu, Guanghui Ren, Ping Luo, Di Huang, Maoqing Yao, Hongyang Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06219v1 Announce Type: cross \nAbstract: Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of \"more diverse is better\". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively."
      },
      {
        "id": "oai:arXiv.org:2211.14620v2",
        "title": "The distribution of syntactic dependency distances",
        "link": "https://arxiv.org/abs/2211.14620",
        "author": "Sonia Petrini, Ramon Ferrer-i-Cancho",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2211.14620v2 Announce Type: replace \nAbstract: The syntactic structure of a sentence can be represented as a graph, where vertices are words and edges indicate syntactic dependencies between them. In this setting, the distance between two linked words is defined as the difference between their positions. Here we wish to contribute to the characterization of the actual distribution of syntactic dependency distances, which has previously been argued to follow a power-law distribution. Here we propose a new model with two exponential regimes in which the probability decay is allowed to change after a break-point. This transition could mirror the transition from the processing of word chunks to higher-level structures. We find that a two-regime model - where the first regime follows either an exponential or a power-law decay - is the most likely one in all 20 languages we considered, independently of sentence length and annotation style. Moreover, the break-point exhibits low variation across languages and averages values of 4-5 words, suggesting that the amount of words that can be simultaneously processed abstracts from the specific language to a high degree. The probability decay slows down after the breakpoint, consistently with a universal chunk-and-pass mechanism. Finally, we give an account of the relation between the best estimated model and the closeness of syntactic dependencies as function of sentence length, according to a recently introduced optimality score."
      },
      {
        "id": "oai:arXiv.org:2211.15353v3",
        "title": "Copula Density Neural Estimation",
        "link": "https://arxiv.org/abs/2211.15353",
        "author": "Nunzio A. Letizia, Nicola Novello, Andrea M. Tonello",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2211.15353v3 Announce Type: replace \nAbstract: Probability density estimation from observed data constitutes a central task in statistics. In this brief, we focus on the problem of estimating the copula density associated to any observed data, as it fully describes the dependence between random variables. We separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and can be applied for mutual information estimation and data generation."
      },
      {
        "id": "oai:arXiv.org:2304.06670v2",
        "title": "Deep neural networks have an inbuilt Occam's razor",
        "link": "https://arxiv.org/abs/2304.06670",
        "author": "Chris Mingard, Henry Rees, Guillermo Valle-P\\'erez, Ard A. Louis",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2304.06670v2 Announce Type: replace \nAbstract: The remarkable performance of overparameterized deep neural networks (DNNs) must arise from an interplay between network architecture, training algorithms, and structure in the data. To disentangle these three components, we apply a Bayesian picture, based on the functions expressed by a DNN, to supervised learning. The prior over functions is determined by the network, and is varied by exploiting a transition between ordered and chaotic regimes. For Boolean function classification, we approximate the likelihood using the error spectrum of functions on data. When combined with the prior, this accurately predicts the posterior, measured for DNNs trained with stochastic gradient descent. This analysis reveals that structured data, combined with an intrinsic Occam's razor-like inductive bias towards (Kolmogorov) simple functions that is strong enough to counteract the exponential growth of the number of functions with complexity, is a key to the success of DNNs."
      },
      {
        "id": "oai:arXiv.org:2305.12659v3",
        "title": "UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model",
        "link": "https://arxiv.org/abs/2305.12659",
        "author": "Zhenghao Zhang, Shengfan Zhang, Zhichao Wei, Zuozhuo Dai, Siyu Zhu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2305.12659v3 Announce Type: replace \nAbstract: The current state-of-the-art methods for unsupervised video object segmentation (UVOS) require extensive training on video datasets with mask annotations, limiting their effectiveness in handling challenging scenarios. However, the Segment Anything Model (SAM) introduces a new prompt-driven paradigm for image segmentation, offering new possibilities. In this study, we investigate SAM's potential for UVOS through different prompt strategies. We then propose UVOSAM, a mask-free paradigm for UVOS that utilizes the STD-Net tracker. STD-Net incorporates a spatial-temporal decoupled deformable attention mechanism to establish an effective correlation between intra- and inter-frame features, remarkably enhancing the quality of box prompts in complex video scenes. Extensive experiments on the DAVIS2017-unsupervised and YoutubeVIS19\\&21 datasets demonstrate the superior performance of UVOSAM without mask supervision compared to existing mask-supervised methods, as well as its ability to generalize to weakly-annotated video datasets. Code can be found at https://github.com/alibaba/UVOSAM."
      },
      {
        "id": "oai:arXiv.org:2312.08968v2",
        "title": "Detecting value-expressive text posts in Russian social media",
        "link": "https://arxiv.org/abs/2312.08968",
        "author": "Maria Milkova, Maksim Rudnev, Lidia Okolskaya",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2312.08968v2 Announce Type: replace \nAbstract: Basic values are concepts or beliefs which pertain to desirable end-states and transcend specific situations. Studying personal values in social media can illuminate how and why societal values evolve especially when the stimuli-based methods, such as surveys, are inefficient, for instance, in hard-to-reach populations. On the other hand, user-generated content is driven by the massive use of stereotyped, culturally defined speech constructions rather than authentic expressions of personal values. We aimed to find a model that can accurately detect value-expressive posts in Russian social media VKontakte. A training dataset of 5,035 posts was annotated by three experts, 304 crowd-workers and ChatGPT. Crowd-workers and experts showed only moderate agreement in categorizing posts. ChatGPT was more consistent but struggled with spam detection. We applied an ensemble of human- and AI-assisted annotation involving active learning approach, subsequently trained several classification models using embeddings from various pre-trained transformer-based language models. The best performance was achieved with embeddings from a fine-tuned rubert-tiny2 model, yielding high value detection quality (F1 = 0.75, F1-macro = 0.80). This model provides a crucial step to a study of values within and between Russian social media users."
      },
      {
        "id": "oai:arXiv.org:2312.12491v2",
        "title": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation",
        "link": "https://arxiv.org/abs/2312.12491",
        "author": "Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu, Masayoshi Tomizuka, Kurt Keutzer",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2312.12491v2 Announce Type: replace \nAbstract: We introduce StreamDiffusion, a real-time diffusion pipeline designed for interactive image generation. Existing diffusion models are adept at creating images from text or image prompts, yet they often fall short in real-time interaction. This limitation becomes particularly evident in scenarios involving continuous input, such as Metaverse, live video streaming, and broadcasting, where high throughput is imperative. To address this, we present a novel approach that transforms the original sequential denoising into the batching denoising process. Stream Batch eliminates the conventional wait-and-interact approach and enables fluid and high throughput streams. To handle the frequency disparity between data input and model throughput, we design a novel input-output queue for parallelizing the streaming process. Moreover, the existing diffusion pipeline uses classifier-free guidance(CFG), which requires additional U-Net computation. To mitigate the redundant computations, we propose a novel residual classifier-free guidance (RCFG) algorithm that reduces the number of negative conditional denoising steps to only one or even zero. Besides, we introduce a stochastic similarity filter(SSF) to optimize power consumption. Our Stream Batch achieves around 1.5x speedup compared to the sequential denoising method at different denoising levels. The proposed RCFG leads to speeds up to 2.05x higher than the conventional CFG. Combining the proposed strategies and existing mature acceleration tools makes the image-to-image generation achieve up-to 91.07fps on one RTX4090, improving the throughputs of AutoPipline developed by Diffusers over 59.56x. Furthermore, our proposed StreamDiffusion also significantly reduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one RTX4090, respectively."
      },
      {
        "id": "oai:arXiv.org:2312.17670v4",
        "title": "Benchmarking the CoW with the TopCoW Challenge: Topology-Aware Anatomical Segmentation of the Circle of Willis for CTA and MRA",
        "link": "https://arxiv.org/abs/2312.17670",
        "author": "Kaiyuan Yang, Fabio Musio, Yihui Ma, Norman Juchler, Johannes C. Paetzold, Rami Al-Maskari, Luciano H\\\"oher, Hongwei Bran Li, Ibrahim Ethem Hamamci, Anjany Sekuboyina, Suprosanna Shit, Houjing Huang, Chinmay Prabhakar, Ezequiel de la Rosa, Bastian Wittmann, Diana Waldmannstetter, Florian Kofler, Fernando Navarro, Martin Menten, Ivan Ezhov, Daniel Rueckert, Iris N. Vos, Ynte M. Ruigrok, Birgitta K. Velthuis, Hugo J. Kuijf, Pengcheng Shi, Wei Liu, Ting Ma, Maximilian R. Rokuss, Yannick Kirchhoff, Fabian Isensee, Klaus Maier-Hein, Chengcheng Zhu, Huilin Zhao, Philippe Bijlenga, Julien H\\\"ammerli, Catherine Wurster, Laura Westphal, Jeroen Bisschop, Elisa Colombo, Hakim Baazaoui, Hannah-Lea Handelsmann, Andrew Makmur, James Hallinan, Amrish Soundararajan, Bene Wiestler, Jan S. Kirschke, Roland Wiest, Emmanuel Montagnon, Laurent Letourneau-Guillon, Kwanseok Oh, Dahye Lee, Adam Hilbert, Orhun Utku Aydin, Dimitrios Rallios, Jana Rieger, Satoru Tanioka, Alexander Koch, Dietmar Frey, Abdul Qayyum, Moona Mazher, Steven Niederer, Nico Disch, Julius Holzschuh, Dominic LaBella, Francesco Galati, Daniele Falcetta, Maria A. Zuluaga, Chaolong Lin, Haoran Zhao, Zehan Zhang, Minghui Zhang, Xin You, Hanxiao Zhang, Guang-Zhong Yang, Yun Gu, Sinyoung Ra, Jongyun Hwang, Hyunjin Park, Junqiang Chen, Marek Wodzinski, Henning M\\\"uller, Nesrin Mansouri, Florent Autrusseau, Cansu Yal\\c{c}in, Rachika E. Hamadache, Clara Lisazo, Joaquim Salvi, Adri\\`a Casamitjana, Xavier Llad\\'o, Uma Maria Lal-Trehan Estrada, Valeriia Abramova, Luca Giancardo, Arnau Oliver, Paula Casademunt, Adrian Galdran, Matteo Delucchi, Jialu Liu, Haibin Huang, Yue Cui, Zehang Lin, Yusheng Liu, Shunzhi Zhu, Tatsat R. Patel, Adnan H. Siddiqui, Vincent M. Tutino, Maysam Orouskhani, Huayu Wang, Mahmud Mossa-Basha, Yuki Sato, Sven Hirsch, Susanne Wegener, Bjoern Menze",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2312.17670v4 Announce Type: replace \nAbstract: The Circle of Willis (CoW) is an important network of arteries connecting major circulations of the brain. Its vascular architecture is believed to affect the risk, severity, and clinical outcome of serious neurovascular diseases. However, characterizing the highly variable CoW anatomy is still a manual and time-consuming expert task. The CoW is usually imaged by two non-invasive angiographic imaging modalities, magnetic resonance angiography (MRA) and computed tomography angiography (CTA), but there exist limited datasets with annotations on CoW anatomy, especially for CTA. Therefore, we organized the TopCoW challenge with the release of an annotated CoW dataset. The TopCoW dataset is the first public dataset with voxel-level annotations for 13 CoW vessel components, enabled by virtual reality technology. It is also the first large dataset using 200 pairs of MRA and CTA from the same patients. As part of the benchmark, we invited submissions worldwide and attracted over 250 registered participants from six continents. The submissions were evaluated on both internal and external test datasets of 226 scans from over five centers. The top performing teams achieved over 90% Dice scores at segmenting the CoW components, over 80% F1 scores at detecting key CoW components, and over 70% balanced accuracy at classifying CoW variants for nearly all test sets. The best algorithms also showed clinical potential in classifying fetal-type posterior cerebral artery and locating aneurysms with CoW anatomy. TopCoW demonstrated the utility and versatility of CoW segmentation algorithms for a wide range of downstream clinical applications with explainability. The annotated datasets and best performing algorithms have been released as public Zenodo records to foster further methodological development and clinical tool building."
      },
      {
        "id": "oai:arXiv.org:2402.14609v4",
        "title": "Learning Federated Neural Graph Databases for Answering Complex Queries from Distributed Knowledge Graphs",
        "link": "https://arxiv.org/abs/2402.14609",
        "author": "Qi Hu, Weifeng Jiang, Haoran Li, Zihao Wang, Jiaxin Bai, Qianren Mao, Yangqiu Song, Lixin Fan, Jianxin Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2402.14609v4 Announce Type: replace \nAbstract: The increasing demand for deep learning-based foundation models has highlighted the importance of efficient data retrieval mechanisms. Neural graph databases (NGDBs) offer a compelling solution, leveraging neural spaces to store and query graph-structured data, thereby enabling LLMs to access precise and contextually relevant information. However, current NGDBs are constrained to single-graph operation, limiting their capacity to reason across multiple, distributed graphs. Furthermore, the lack of support for multi-source graph data in existing NGDBs hinders their ability to capture the complexity and diversity of real-world data. In many applications, data is distributed across multiple sources, and the ability to reason across these sources is crucial for making informed decisions. This limitation is particularly problematic when dealing with sensitive graph data, as directly sharing and aggregating such data poses significant privacy risks. As a result, many applications that rely on NGDBs are forced to choose between compromising data privacy or sacrificing the ability to reason across multiple graphs. To address these limitations, we propose to learn Federated Neural Graph DataBase (FedNGDB), a pioneering systematic framework that empowers privacy-preserving reasoning over multi-source graph data. FedNGDB leverages federated learning to collaboratively learn graph representations across multiple sources, enriching relationships between entities, and improving the overall quality of graph data."
      },
      {
        "id": "oai:arXiv.org:2403.01671v4",
        "title": "Approximating invariant functions with the sorting trick is theoretically justified",
        "link": "https://arxiv.org/abs/2403.01671",
        "author": "Wee Chaimanowong, Ying Zhu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2403.01671v4 Announce Type: replace \nAbstract: Many machine learning models leverage group invariance which is enjoyed with a wide-range of applications. For exploiting an invariance structure, one common approach is known as \\emph{frame averaging}. One popular example of frame averaging is the \\emph{group averaging}, where the entire group is used to symmetrize a function. The other end of the spectrum is the \\emph{canonicalization}, where a frame at each point consists of a single group element which transforms the point to its orbit representative. Compared to group averaging, canonicalization is more efficient computationally. However, it results in non-differentiablity or discontinuity of the canonicalized function. As a result, the theoretical performance of canonicalization has not been given much attention. In this work, we establish an approximation theory for canonicalization. Specifically, we bound the point-wise and $L^2(\\mathbb{P})$ approximation errors as well as the kernel's eigenvalue decay rates associated with a canonicalization trick."
      },
      {
        "id": "oai:arXiv.org:2403.04945v4",
        "title": "MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation",
        "link": "https://arxiv.org/abs/2403.04945",
        "author": "Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Jing Xiong, Rossella Arcucci, Huaxiu Yao, Mi Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2403.04945v4 Announce Type: replace \nAbstract: Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for monitoring cardiac conditions and is crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is time-consuming and requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the first attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open-source LLMs using more than 800,000 ECG reports. MEIT's results underscore the superior performance of instruction-tuned LLMs, showcasing their proficiency in quality report generation, zero-shot capabilities, resilience to signal perturbation, and alignment with human expert evaluation. These findings emphasize the efficacy of MEIT and its potential for real-world clinical application."
      },
      {
        "id": "oai:arXiv.org:2403.13847v3",
        "title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models",
        "link": "https://arxiv.org/abs/2403.13847",
        "author": "Eduardo Fernandes Montesuma, Fred Maurice Ngol\\`e Mboula, Antoine Souloumiac",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2403.13847v3 Announce Type: replace \nAbstract: Machine learning systems operate under the assumption that training and test data are sampled from a fixed probability distribution. However, this assumptions is rarely verified in practice, as the conditions upon which data was acquired are likely to change. In this context, the adaptation of the unsupervised domain requires minimal access to the data of the new conditions for learning models robust to changes in the data distribution. Optimal transport is a theoretically grounded tool for analyzing changes in distribution, especially as it allows the mapping between domains. However, these methods are usually computationally expensive as their complexity scales cubically with the number of samples. In this work, we explore optimal transport between Gaussian Mixture Models (GMMs), which is conveniently written in terms of the components of source and target GMMs. We experiment with 9 benchmarks, with a total of $85$ adaptation tasks, showing that our methods are more efficient than previous shallow domain adaptation methods, and they scale well with number of samples $n$ and dimensions $d$."
      },
      {
        "id": "oai:arXiv.org:2403.16846v2",
        "title": "CoDy: Counterfactual Explainers for Dynamic Graphs",
        "link": "https://arxiv.org/abs/2403.16846",
        "author": "Zhan Qu, Daniel Gomm, Michael F\\\"arber",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2403.16846v2 Announce Type: replace \nAbstract: Temporal Graph Neural Networks (TGNNs) are widely used to model dynamic systems where relationships and features evolve over time. Although TGNNs demonstrate strong predictive capabilities in these domains, their complex architectures pose significant challenges for explainability. Counterfactual explanation methods provide a promising solution by illustrating how modifications to input graphs can influence model predictions. To address this challenge, we present CoDy, Counterfactual Explainer for Dynamic Graphs, a model-agnostic, instance-level explanation approach that identifies counterfactual subgraphs to interpret TGNN predictions. CoDy employs a search algorithm that combines Monte Carlo Tree Search with heuristic selection policies, efficiently exploring a vast search space of potential explanatory subgraphs by leveraging spatial, temporal, and local event impact information. Extensive experiments against state-of-the-art factual and counterfactual baselines demonstrate CoDy's effectiveness, with improvements of 16% in AUFSC+ over the strongest baseline."
      },
      {
        "id": "oai:arXiv.org:2404.16302v2",
        "title": "CFMW: Cross-modality Fusion Mamba for Robust Object Detection under Adverse Weather",
        "link": "https://arxiv.org/abs/2404.16302",
        "author": "Haoyuan Li, Qi Hu, Binjia Zhou, You Yao, Jiacheng Lin, Kailun Yang, Peng Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2404.16302v2 Announce Type: replace \nAbstract: Visible-infrared image pairs provide complementary information, enhancing the reliability and robustness of object detection applications in real-world scenarios. However, most existing methods face challenges in maintaining robustness under complex weather conditions, which limits their applicability. Meanwhile, the reliance on attention mechanisms in modality fusion introduces significant computational complexity and storage overhead, particularly when dealing with high-resolution images. To address these challenges, we propose the Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment stability and cost-effectiveness under adverse weather conditions. Leveraging the proposed Perturbation-Adaptive Diffusion Model (PADM) and Cross-modality Fusion Mamba (CFM) modules, CFMW is able to reconstruct visual features affected by adverse weather, enriching the representation of image details. With efficient architecture design, CFMW is 3 times faster than Transformer-style fusion (e.g., CFT). To bridge the gap in relevant datasets, we construct a new Severe Weather Visible-Infrared (SWVI) dataset, encompassing diverse adverse weather scenarios such as rain, haze, and snow. The dataset contains 64,281 paired visible-infrared images, providing a valuable resource for future research. Extensive experiments on public datasets (i.e., M3FD and LLVIP) and the newly constructed SWVI dataset conclusively demonstrate that CFMW achieves state-of-the-art detection performance. Both the dataset and source code will be made publicly available at https://github.com/lhy-zjut/CFMW."
      },
      {
        "id": "oai:arXiv.org:2404.19725v5",
        "title": "Curvature-Aligned Federated Learning (CAFe): Harmonizing Loss Landscapes for Fairness Without Demographics",
        "link": "https://arxiv.org/abs/2404.19725",
        "author": "Shaily Roy, Harshit Sharma, Asif Salekin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2404.19725v5 Announce Type: replace \nAbstract: Federated Learning (FL) enables privacy-preserving collaborative training, making it well-suited for decentralized human-sensing applications. Ensuring fairness in FL is challenging, as current methods rely on sensitive attribute knowledge, which conflicts with FL's privacy principles. Additionally, sensitive attributes in human-sensing data may be unknown or latent. To address this, we introduce Curvature-Aligned Federated Learning (CAFe), a theoretically grounded approach that achieves fairness in FL without requiring sensitive attribute knowledge, a concept termed \"Fairness without Demographics\" (FWD). CAFe introduces loss-landscape curvature regularization during local training and clients' loss-landscape sharpness-aware aggregation to align curvature both within and across clients, enabling a strong balance between higher fairness and performance. CAFe is especially suitable for real-world human-sensing FL scenarios involving single or multi-user edge devices with unknown or multiple bias factors. We validated CAFe through theoretical and empirical justifications, and comprehensive evaluations using three real-world datasets and a live real-world FL deployment with a heterogeneous testbed of resource-constrained devices. Additionally, we conduct sensitivity analyses on local training data volume, client sampling, communication overhead, resource costs, and runtime performance to demonstrate its feasibility for practical FL edge device deployment."
      },
      {
        "id": "oai:arXiv.org:2405.15660v3",
        "title": "Low-Light Video Enhancement via Spatial-Temporal Consistent Decomposition",
        "link": "https://arxiv.org/abs/2405.15660",
        "author": "Xiaogang Xu, Kun Zhou, Tao Hu, Jiafei Wu, Ruixing Wang, Hao Peng, Bei Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2405.15660v3 Announce Type: replace \nAbstract: Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Extensive experiments are conducted on widely recognized LLVE benchmarks, covering diverse scenarios. Our framework consistently outperforms existing methods, establishing a new SOTA performance."
      },
      {
        "id": "oai:arXiv.org:2406.00826v3",
        "title": "Policy Verification in Stochastic Dynamical Systems Using Logarithmic Neural Certificates",
        "link": "https://arxiv.org/abs/2406.00826",
        "author": "Thom Badings, Wietze Koops, Sebastian Junges, Nils Jansen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2406.00826v3 Announce Type: replace \nAbstract: We consider the verification of neural network policies for discrete-time stochastic systems with respect to reach-avoid specifications. We use a learner-verifier procedure that learns a certificate for the specification, represented as a neural network. Verifying that this neural network certificate is a so-called reach-avoid supermartingale (RASM) proves the satisfaction of a reach-avoid specification. Existing approaches for such a verification task rely on computed Lipschitz constants of neural networks. These approaches struggle with large Lipschitz constants, especially for reach-avoid specifications with high threshold probabilities. We present two key contributions to obtain smaller Lipschitz constants than existing approaches. First, we introduce logarithmic RASMs (logRASMs), which take exponentially smaller values than RASMs and hence have lower theoretical Lipschitz constants. Second, we present a fast method to compute tighter upper bounds on Lipschitz constants based on weighted norms. Our empirical evaluation shows we can consistently verify the satisfaction of reach-avoid specifications with probabilities as high as 99.9999%."
      },
      {
        "id": "oai:arXiv.org:2406.06641v2",
        "title": "News and Load: Social and Economic Drivers of Regional Multi-horizon Electricity Demand Forecasting",
        "link": "https://arxiv.org/abs/2406.06641",
        "author": "Yun Bai, Simon Camal, Andrea Michiorri",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2406.06641v2 Announce Type: replace \nAbstract: The relationship between electricity demand and variables such as economic activity and weather patterns is well established. However, this paper explores the connection between electricity demand and social aspects. It further embeds dynamic information about the state of society into energy demand modelling and forecasting approaches. Through the use of natural language processing on a large news corpus, we highlight this important link. This study is conducted in five regions of the UK and Ireland and considers multiple time horizons from 1 to 30 days. It also considers economic variables such as GDP, unemployment and inflation. The textual features used in this study represent central constructs from the word frequencies, topics, word embeddings extracted from the news. The findings indicate that: 1) the textual features are related to various contents, such as military conflicts, transportation, the global pandemic, regional economics, and the international energy market. They exhibit causal relationships with regional electricity demand, which are validated using Granger causality and Double Machine Learning methods. 2) Economic indicators play a more important role in the East Midlands and Northern Ireland, while social indicators are more influential in the West Midlands and the South West of England. 3) The use of these factors improves deterministic forecasting by around 6%."
      },
      {
        "id": "oai:arXiv.org:2406.14459v2",
        "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models",
        "link": "https://arxiv.org/abs/2406.14459",
        "author": "Shijie Han, Zhenyu Zhang, Andrei Arsene Simion",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2406.14459v2 Announce Type: replace \nAbstract: Language models like BERT excel at sentence classification tasks due to extensive pre-training on general data, but their robustness to parameter corruption is unexplored. To understand this better, we look at what happens if a language model is \"broken\", in the sense that some of its parameters are corrupted and then recovered by fine-tuning. Strategically corrupting BERT variants at different levels, we find corrupted models struggle to fully recover their original performance, with higher corruption causing more severe degradation. Notably, bottom-layer corruption affecting fundamental linguistic features is more detrimental than top-layer corruption. Our insights contribute to understanding language model robustness and adaptability under adverse conditions, informing strategies for developing resilient NLP systems against parameter perturbations."
      },
      {
        "id": "oai:arXiv.org:2406.15753v3",
        "title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
        "link": "https://arxiv.org/abs/2406.15753",
        "author": "Lukas Fluri, Leon Lang, Alessandro Abate, Patrick Forr\\'e, David Krueger, Joar Skalse",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2406.15753v3 Announce Type: replace \nAbstract: In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by learning the reward function. However, a learned reward model may have a low error on the data distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an error-regret mismatch. The main source of an error-regret mismatch is the distributional shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any fixed expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as RLHF. We hope our results stimulate the theoretical and empirical study of improved methods to learn reward models, and better ways to measure their quality reliably."
      },
      {
        "id": "oai:arXiv.org:2407.09718v2",
        "title": "CLOVER: Context-aware Long-term Object Viewpoint- and Environment- Invariant Representation Learning",
        "link": "https://arxiv.org/abs/2407.09718",
        "author": "Dongmyeong Lee, Amanda Adkins, Joydeep Biswas",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2407.09718v2 Announce Type: replace \nAbstract: Mobile service robots can benefit from object-level understanding of their environments, including the ability to distinguish object instances and re-identify previously seen instances. Object re-identification is challenging across different viewpoints and in scenes with significant appearance variation arising from weather or lighting changes. Existing works on object re-identification either focus on specific classes or require foreground segmentation. Further, these methods, along with object re-identification datasets, have limited consideration of challenges such as outdoor scenes and illumination changes. To address this problem, we introduce CODa Re-ID: an in-the-wild object re-identification dataset containing 1,037,814 observations of 557 objects across 8 classes under diverse lighting conditions and viewpoints. Further, we propose CLOVER, a representation learning method for object observations that can distinguish between static object instances without requiring foreground segmentation. We also introduce MapCLOVER, a method for scalably summarizing CLOVER descriptors for use in object maps and matching new observations to summarized descriptors. Our results show that CLOVER achieves superior performance in static object re-identification under varying lighting conditions and viewpoint changes and can generalize to unseen instances and classes."
      },
      {
        "id": "oai:arXiv.org:2407.16697v2",
        "title": "AbdomenAtlas: A Large-Scale, Detailed-Annotated, & Multi-Center Dataset for Efficient Transfer Learning and Open Algorithmic Benchmarking",
        "link": "https://arxiv.org/abs/2407.16697",
        "author": "Wenxuan Li, Chongyu Qu, Xiaoxi Chen, Pedro R. A. S. Bassi, Yijia Shi, Yuxiang Lai, Qian Yu, Huimin Xue, Yixiong Chen, Xiaorui Lin, Yutong Tang, Yining Cao, Haoqi Han, Zheyuan Zhang, Jiawei Liu, Tiezheng Zhang, Yujiu Ma, Jincheng Wang, Guang Zhang, Alan Yuille, Zongwei Zhou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2407.16697v2 Announce Type: replace \nAbstract: We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460 three-dimensional CT volumes sourced from 112 hospitals across diverse populations, geographies, and facilities. AbdomenAtlas provides 673K high-quality masks of anatomical structures in the abdominal region annotated by a team of 10 radiologists with the help of AI algorithms. We start by having expert radiologists manually annotate 22 anatomical structures in 5,246 CT volumes. Following this, a semi-automatic annotation procedure is performed for the remaining CT volumes, where radiologists revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from revised annotations. Such a large-scale, detailed-annotated, and multi-center dataset is needed for two reasons. Firstly, AbdomenAtlas provides important resources for AI development at scale, branded as large pre-trained models, which can alleviate the annotation workload of expert radiologists to transfer to broader clinical applications. Secondly, AbdomenAtlas establishes a large-scale benchmark for evaluating AI algorithms -- the more data we use to test the algorithms, the better we can guarantee reliable performance in complex clinical scenarios. An ISBI & MICCAI challenge named BodyMaps: Towards 3D Atlas of Human Body was launched using a subset of our AbdomenAtlas, aiming to stimulate AI innovation and to benchmark segmentation accuracy, inference efficiency, and domain generalizability. We hope our AbdomenAtlas can set the stage for larger-scale clinical trials and offer exceptional opportunities to practitioners in the medical imaging community. Codes, models, and datasets are available at https://www.zongweiz.com/dataset"
      },
      {
        "id": "oai:arXiv.org:2408.01701v4",
        "title": "Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics",
        "link": "https://arxiv.org/abs/2408.01701",
        "author": "Naichuan Zheng, Yuchen Du, Hailun Xia, Zeyu Liang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2408.01701v4 Announce Type: replace \nAbstract: For multimodal skeleton-based action recognition, Graph Convolutional Networks (GCNs) are effective models. Still, their reliance on floating-point computations leads to high energy consumption, limiting their applicability in battery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs) struggle to model skeleton dynamics, leading to suboptimal solutions. We propose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the temporal dimension of skeleton sequences as the spike time steps and represents features as multi-dimensional discrete stochastic signals for temporal-frequency domain feature extraction. It combines the 1D Spiking Graph Convolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module to extract features from the skeleton represented as spiking form. Additionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is proposed to extract dynamic spiking features and capture frequency-specific characteristics, enhancing classification performance. Experiments across three large-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based methods in accuracy and computational efficiency while attaining comparable performance with GCN methods and significantly reducing theoretical energy consumption."
      },
      {
        "id": "oai:arXiv.org:2408.03559v2",
        "title": "Enhanced hermit crabs detection using super-resolution reconstruction and improved YOLOv8 on UAV-captured imagery",
        "link": "https://arxiv.org/abs/2408.03559",
        "author": "Fan Zhao, Yijia Chen, Dianhan Xi, Yongying Liu, Jiaqi Wang, Shigeru Tabeta, Katsunori Mizuno",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2408.03559v2 Announce Type: replace \nAbstract: Hermit crabs play a crucial role in coastal ecosystems by dispersing seeds, cleaning up debris, and disturbing soil. They serve as vital indicators of marine environmental health, responding to climate change and pollution. Traditional survey methods, like quadrat sampling, are labor-intensive, time-consuming, and environmentally dependent. This study presents an innovative approach combining UAV-based remote sensing with Super-Resolution Reconstruction (SRR) and the CRAB-YOLO detection network, a modification of YOLOv8s, to monitor hermit crabs. SRR enhances image quality by addressing issues such as motion blur and insufficient resolution, significantly improving detection accuracy over conventional low-resolution fuzzy images. The CRAB-YOLO network integrates three improvements for detection accuracy, hermit crab characteristics, and computational efficiency, achieving state-of-the-art (SOTA) performance compared to other mainstream detection models. The RDN networks demonstrated the best image reconstruction performance, and CRAB-YOLO achieved a mean average precision (mAP) of 69.5% on the SRR test set, a 40% improvement over the conventional Bicubic method with a magnification factor of 4. These results indicate that the proposed method is effective in detecting hermit crabs, offering a cost-effective and automated solution for extensive hermit crab monitoring, thereby aiding coastal benthos conservation."
      },
      {
        "id": "oai:arXiv.org:2408.03564v3",
        "title": "Riverbed litter monitoring using consumer-grade aerial-aquatic speedy scanner (AASS) and deep learning based super-resolution reconstruction and detection network",
        "link": "https://arxiv.org/abs/2408.03564",
        "author": "Fan Zhao, Yongying Liu, Jiaqi Wang, Yijia Chen, Dianhan Xi, Xinlei Shao, Shigeru Tabeta, Katsunori Mizuno",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2408.03564v3 Announce Type: replace \nAbstract: Underwater litter is widely spread across aquatic environments such as lakes, rivers, and oceans, significantly impacting natural ecosystems. Current monitoring technologies for detecting underwater litter face limitations in survey efficiency, cost, and environmental conditions, highlighting the need for efficient, consumer-grade technologies for automatic detection. This research introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with Super-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network. AASS enhances data acquisition efficiency over traditional methods, capturing high-quality images that accurately identify underwater waste. SRR improves image-resolution by mitigating motion blur and insufficient resolution, thereby enhancing detection tasks. Specifically, the RCAN model achieved the highest mean average precision (mAP) of 78.6% for detection accuracy on reconstructed images among the tested SRR models. With a magnification factor of 4, the SRR test set shows an improved mAP compared to the conventional bicubic set. These results demonstrate the effectiveness of the proposed method in detecting underwater litter."
      },
      {
        "id": "oai:arXiv.org:2408.08971v3",
        "title": "A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation Recognition",
        "link": "https://arxiv.org/abs/2408.08971",
        "author": "Nelson Filipe Costa, Leila Kosseim",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2408.08971v3 Announce Type: replace \nAbstract: We propose a novel multi-label classification approach to implicit discourse relation recognition (IDRR). Our approach features a multi-task model that jointly learns multi-label representations of implicit discourse relations across all three sense levels in the PDTB 3.0 framework. The model can also be adapted to the traditional single-label IDRR setting by selecting the sense with the highest probability in the multi-label representation. We conduct extensive experiments to identify optimal model configurations and loss functions in both settings. Our approach establishes the first benchmark for multi-label IDRR and achieves SOTA results on single-label IDRR using DiscoGeM. Finally, we evaluate our model on the PDTB 3.0 corpus in the single-label setting, presenting the first analysis of transfer learning between the DiscoGeM and PDTB 3.0 corpora for IDRR."
      },
      {
        "id": "oai:arXiv.org:2408.13002v4",
        "title": "Measuring Variable Importance in Heterogeneous Treatment Effects with Confidence",
        "link": "https://arxiv.org/abs/2408.13002",
        "author": "Joseph Paillard, Angel Reyero Lobo, Vitaliy Kolodyazhniy, Bertrand Thirion, Denis A. Engemann",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2408.13002v4 Announce Type: replace \nAbstract: Causal machine learning holds promise for estimating individual treatment effects from complex data. For successful real-world applications of machine learning methods, it is of paramount importance to obtain reliable insights into which variables drive heterogeneity in the response to treatment. We propose PermuCATE, an algorithm based on the Conditional Permutation Importance (CPI) method, for statistically rigorous global variable importance assessment in the estimation of the Conditional Average Treatment Effect (CATE). Theoretical analysis of the finite sample regime and empirical studies show that PermuCATE has lower variance than the Leave-One-Covariate-Out (LOCO) reference method and provides a reliable measure of variable importance. This property increases statistical power, which is crucial for causal inference in the limited-data regime common to biomedical applications. We empirically demonstrate the benefits of PermuCATE in simulated and real-world health datasets, including settings with up to hundreds of correlated variables."
      },
      {
        "id": "oai:arXiv.org:2409.17172v2",
        "title": "What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning",
        "link": "https://arxiv.org/abs/2409.17172",
        "author": "Shashidhar Reddy Javaji, Zining Zhu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.17172v2 Announce Type: replace \nAbstract: Large language models (LLMs) can store a massive amount of knowledge, yet their potential to acquire new knowledge remains unknown. We propose a novel evaluation framework that evaluates this capability. This framework prompts LLMs to generate questions about a statement introducing scientific knowledge, simulating a curious person when facing the statement for the first time. We score the qualities of the generated questions, thereby evaluating the knowledge acquisition potential of the LLM. We apply controlled ablation studies to validate our scoring procedures. Additionally, we created a synthetic dataset consisting of 1101 statements in physics, chemistry, and maths with distinct levels of difficulties, 300 general knowledge statements, and 567 incorrect statements. Human evaluations were conducted to validate our model assessments, achieving an approximate weighted Cohen's kappa of 0.7 on all three metrics considered. We find that while large models like GPT-4 and Mistral 8x7b are adept at generating coherent and relevant questions, the smaller Phi-2 model is equally or more effective. This indicates that size does not solely determine a model's knowledge acquisition potential. The proposed framework quantifies a critical model capability that was commonly overlooked and opens up research opportunities for developing more knowledgeable AI systems"
      },
      {
        "id": "oai:arXiv.org:2409.18486v2",
        "title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI",
        "link": "https://arxiv.org/abs/2409.18486",
        "author": "Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yiheng Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, Tuo Zhang, Tianming Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.18486v2 Announce Type: replace \nAbstract: This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:\n  -83.3% success rate in solving complex competitive programming problems, surpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and specialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis and emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence."
      },
      {
        "id": "oai:arXiv.org:2409.18932v3",
        "title": "ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse Weather Conditions",
        "link": "https://arxiv.org/abs/2409.18932",
        "author": "Wenfeng Huang, Guoan Xu, Wenjing Jia, Stuart Perry, Guangwei Gao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.18932v3 Announce Type: replace \nAbstract: Images captured in challenging environments--such as nighttime, smoke, rainy weather, and underwater--often suffer from significant degradation, resulting in a substantial loss of visual quality. The effective restoration of these degraded images is critical for the subsequent vision tasks. While many existing approaches have successfully incorporated specific priors for individual tasks, these tailored solutions limit their applicability to other degradations. In this work, we propose a universal network architecture, dubbed ``ReviveDiff'', which can address various degradations and bring images back to life by enhancing and restoring their quality. Our approach is inspired by the observation that, unlike degradation caused by movement or electronic issues, quality degradation under adverse conditions primarily stems from natural media (such as fog, water, and low luminance), which generally preserves the original structures of objects. To restore the quality of such images, we leveraged the latest advancements in diffusion models and developed ReviveDiff to restore image quality from both macro and micro levels across some key factors determining image quality, such as sharpness, distortion, noise level, dynamic range, and color accuracy. We rigorously evaluated ReviveDiff on seven benchmark datasets covering five types of degrading conditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Our experimental results demonstrate that ReviveDiff outperforms the state-of-the-art methods both quantitatively and visually."
      },
      {
        "id": "oai:arXiv.org:2410.16658v4",
        "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent",
        "link": "https://arxiv.org/abs/2410.16658",
        "author": "Janghoon Ock, Radheesh Sharma Meda, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2410.16658v4 Announce Type: replace \nAbstract: Adsorption energy is a key reactivity descriptor in catalysis. Determining adsorption energy requires evaluating numerous adsorbate-catalyst configurations, making it computationally intensive. Current methods rely on exhaustive sampling, which does not guarantee the identification of the global minimum energy. To address this, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify stable adsorption configurations corresponding to the global minimum energy. Adsorb-Agent leverages its built-in knowledge and reasoning to strategically explore configurations, significantly reducing the number of initial setups required while improving energy prediction accuracy. In this study, we also evaluated the performance of different LLMs, including GPT-4o, GPT-4o-mini, Claude-3.7-Sonnet, and DeepSeek-Chat, as the reasoning engine for Adsorb-Agent, with GPT-4o showing the strongest overall performance. Tested on twenty diverse systems, Adsorb-Agent identifies comparable adsorption energies for 84% of cases and achieves lower energies for 35%, particularly excelling in complex systems. It identifies lower energies in 47% of intermetallic systems and 67% of systems with large adsorbates. These findings demonstrate Adsorb-Agent's potential to accelerate catalyst discovery by reducing computational costs and enhancing prediction reliability compared to exhaustive search methods."
      },
      {
        "id": "oai:arXiv.org:2410.21849v2",
        "title": "Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone Meeting Transcription",
        "link": "https://arxiv.org/abs/2410.21849",
        "author": "Can Cui (MULTISPEECH), Imran Ahamad Sheikh (MULTISPEECH), Mostafa Sadeghi (MULTISPEECH), Emmanuel Vincent (MULTISPEECH)",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2410.21849v2 Announce Type: replace \nAbstract: Distant-microphone meeting transcription is a challenging task. State-of-the-art end-to-end speaker-attributed automatic speech recognition (SA-ASR) architectures lack a multichannel noise and reverberation reduction front-end, which limits their performance. In this paper, we introduce a joint beamforming and SA-ASR approach for real meeting transcription. We first describe a data alignment and augmentation method to pretrain a neural beamformer on real meeting data. We then compare fixed, hybrid, and fully neural beamformers as front-ends to the SA-ASR model. Finally, we jointly optimize the fully neural beamformer and the SA-ASR model. Experiments on the real AMI corpus show that, while state-of-the-art multi-frame cross-channel attention based channel fusion fails to improve ASR performance, fine-tuning SA-ASR on the fixed beamformer's output and jointly fine-tuning SA-ASR with the neural beamformer reduce the word error rate by 8% and 9% relative, respectively."
      },
      {
        "id": "oai:arXiv.org:2411.00278v3",
        "title": "KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks",
        "link": "https://arxiv.org/abs/2411.00278",
        "author": "Quan Zhou, Changhua Pei, Fei Sun, Jing Han, Zhengwei Gao, Dan Pei, Haiming Zhang, Gaogang Xie, Jianhui Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00278v3 Announce Type: replace \nAbstract: Time series anomaly detection (TSAD) underpins real-time monitoring in cloud services and web systems, allowing rapid identification of anomalies to prevent costly failures. Most TSAD methods driven by forecasting models tend to overfit by emphasizing minor fluctuations. Our analysis reveals that effective TSAD should focus on modeling \"normal\" behavior through smooth local patterns. To achieve this, we reformulate time series modeling as approximating the series with smooth univariate functions. The local smoothness of each univariate function ensures that the fitted time series remains resilient against local disturbances. However, a direct KAN implementation proves susceptible to these disturbances due to the inherently localized characteristics of B-spline functions. We thus propose KAN-AD, replacing B-splines with truncated Fourier expansions and introducing a novel lightweight learning mechanism that emphasizes global patterns while staying robust to local disturbances. On four popular TSAD benchmarks, KAN-AD achieves an average 15% improvement in detection accuracy (with peaks exceeding 27%) over state-of-the-art baselines. Remarkably, it requires fewer than 1,000 trainable parameters, resulting in a 50% faster inference speed compared to the original KAN, demonstrating the approach's efficiency and practical viability."
      },
      {
        "id": "oai:arXiv.org:2411.01376v2",
        "title": "Multi-Channel Hypergraph Contrastive Learning for Matrix Completion",
        "link": "https://arxiv.org/abs/2411.01376",
        "author": "Xiang Li, Changsheng Shui, Zhongying Zhao, Junyu Dong, Yanwei Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.01376v2 Announce Type: replace \nAbstract: Rating is a typical user explicit feedback that visually reflects how much a user likes a related item. The (rating) matrix completion is essentially a rating prediction process, which is also a significant problem in recommender systems. Recently, graph neural networks (GNNs) have been widely used in matrix completion, which captures users' preferences over items by formulating a rating matrix as a bipartite graph. However, existing methods are susceptible due to data sparsity and long-tail distribution in real-world scenarios. Moreover, the messaging mechanism of GNNs makes it difficult to capture high-order correlations and constraints between nodes, which are essentially useful in recommendation tasks. To tackle these challenges, we propose a Multi-Channel Hypergraph Contrastive Learning framework for matrix completion, named MHCL. Specifically, MHCL adaptively learns hypergraph structures to capture high-order correlations between nodes and jointly captures local and global collaborative relationships through attention-based cross-view aggregation. Additionally, to consider the magnitude and order information of ratings, we treat different rating subgraphs as different channels, encourage alignment between adjacent ratings, and further achieve the mutual enhancement between different ratings through multi-channel cross-rating contrastive learning. Extensive experiments on five public datasets demonstrate that the proposed method significantly outperforms the current state-of-the-art approaches."
      },
      {
        "id": "oai:arXiv.org:2411.04427v3",
        "title": "One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity",
        "link": "https://arxiv.org/abs/2411.04427",
        "author": "Sonia K. Murthy, Tomer Ullman, Jennifer Hu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.04427v3 Announce Type: replace \nAbstract: Researchers in social science and psychology have recently proposed using large language models (LLMs) as replacements for humans in behavioral research. In addition to arguments about whether LLMs accurately capture population-level patterns, this has raised questions about whether LLMs capture human-like conceptual diversity. Separately, it is debated whether post-training alignment (RLHF or RLAIF) affects models' internal diversity. Inspired by human studies, we use a new way of measuring the conceptual diversity of synthetically-generated LLM \"populations\" by relating the internal variability of simulated individuals to the population-level variability. We use this approach to evaluate non-aligned and aligned LLMs on two domains with rich human behavioral data. While no model reaches human-like diversity, aligned models generally display less diversity than their instruction fine-tuned counterparts. Our findings highlight potential trade-offs between increasing models' value alignment and decreasing the diversity of their conceptual representations."
      },
      {
        "id": "oai:arXiv.org:2411.05983v2",
        "title": "Longitudinal Ensemble Integration for sequential classification with multimodal data",
        "link": "https://arxiv.org/abs/2411.05983",
        "author": "Aviad Susman, Rupak Krishnamurthy, Yan Chak Li, Mohammad Olaimat, Serdar Bozdag, Bino Varghese, Nasim Sheikh-Bahaei, Gaurav Pandey",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.05983v2 Announce Type: replace \nAbstract: Effectively modeling multimodal longitudinal data is a pressing need in various application areas, especially biomedicine. Despite this, few approaches exist in the literature for this problem, with most not adequately taking into account the multimodality of the data. In this study, we developed multiple configurations of a novel multimodal and longitudinal learning framework, Longitudinal Ensemble Integration (LEI), for sequential classification. We evaluated LEI's performance, and compared it against existing approaches, for the early detection of dementia, which is among the most studied multimodal sequential classification tasks. LEI outperformed these approaches due to its use of intermediate base predictions arising from the individual data modalities, which enabled their better integration over time. LEI's design also enabled the identification of features that were consistently important across time for the effective prediction of dementia-related diagnoses. Overall, our work demonstrates the potential of LEI for sequential classification from longitudinal multimodal data."
      },
      {
        "id": "oai:arXiv.org:2411.08324v2",
        "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle",
        "link": "https://arxiv.org/abs/2411.08324",
        "author": "Hui Dai, Ryan Teehan, Mengye Ren",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08324v2 Announce Type: replace \nAbstract: Many existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to the emergence of new models and training data. These benchmarks also fall short in assessing how LLM performance changes over time, as they consist of a static set of questions without a temporal dimension. To address these limitations, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict \"future\" event outcomes. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists, highlighting the need for continuous model updates. Code and data are available at https://agenticlearning.ai/daily-oracle."
      },
      {
        "id": "oai:arXiv.org:2411.09822v2",
        "title": "Advancing Stroke Risk Prediction Using a Multi-modal Foundation Model",
        "link": "https://arxiv.org/abs/2411.09822",
        "author": "Camille Delgrange, Olga Demler, Samia Mora, Bjoern Menze, Ezequiel de la Rosa, Neda Davoudi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.09822v2 Announce Type: replace \nAbstract: Predicting stroke risk is a complex challenge that can be enhanced by integrating diverse clinically available data modalities. This study introduces a self-supervised multimodal framework that combines 3D brain imaging, clinical data, and image-derived features to improve stroke risk prediction prior to onset. By leveraging large unannotated clinical datasets, the framework captures complementary and synergistic information across image and tabular data modalities. Our approach is based on a contrastive learning framework that couples contrastive language-image pretraining with an image-tabular matching module, to better align multimodal data representations in a shared latent space. The model is trained on the UK Biobank, which includes structural brain MRI and clinical data. We benchmark its performance against state-of-the-art unimodal and multimodal methods using tabular, image, and image-tabular combinations under diverse frozen and trainable model settings. The proposed model outperformed self-supervised tabular (image) methods by 2.6% (2.6%) in ROC-AUC and by 3.3% (5.6%) in balanced accuracy. Additionally, it showed a 7.6% increase in balanced accuracy compared to the best multimodal supervised model. Through interpretable tools, our approach demonstrated better integration of tabular and image data, providing richer and more aligned embeddings. Gradient-weighted Class Activation Mapping heatmaps further revealed activated brain regions commonly associated in the literature with brain aging, stroke risk, and clinical outcomes. This robust self-supervised multimodal framework surpasses state-of-the-art methods for stroke risk prediction and offers a strong foundation for future studies integrating diverse data modalities to advance clinical predictive modelling."
      },
      {
        "id": "oai:arXiv.org:2411.12155v5",
        "title": "Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning",
        "link": "https://arxiv.org/abs/2411.12155",
        "author": "Younggyo Seo, Pieter Abbeel",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.12155v5 Announce Type: replace \nAbstract: Predicting a sequence of actions has been crucial in the success of recent behavior cloning algorithms in robotics. Can similar ideas improve reinforcement learning (RL)? We answer affirmatively by observing that incorporating action sequences when predicting ground-truth return-to-go leads to lower validation loss. Motivated by this, we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that learns a critic network that outputs Q-values over a sequence of actions, i.e., explicitly training the value function to learn the consequence of executing action sequences. Our experiments show that CQN-AS outperforms several baselines on a variety of sparse-reward humanoid control and tabletop manipulation tasks from BiGym and RLBench."
      },
      {
        "id": "oai:arXiv.org:2411.12665v2",
        "title": "Regression for the Mean: Auto-Evaluation and Inference with Few Labels through Post-hoc Regression",
        "link": "https://arxiv.org/abs/2411.12665",
        "author": "Benjamin Eyre, David Madras",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.12665v2 Announce Type: replace \nAbstract: The availability of machine learning systems that can effectively perform arbitrary tasks has led to synthetic labels from these systems being used in applications of statistical inference, such as data analysis or model evaluation. The Prediction Powered Inference (PPI) framework provides a way of leveraging both a large pool of pseudo-labelled data and a small sample with real, high-quality labels to produce a low-variance, unbiased estimate of the quantity being evaluated for. Most work on PPI considers a relatively sizable set of labelled samples, which can be resource intensive to obtain. However, we find that when labelled data is scarce, the PPI++ method can perform even worse than classical inference. We analyze this phenomenon by relating PPI++ to ordinary least squares regression, which also experiences high variance with small sample sizes, and use this regression framework to better understand the efficacy of PPI. Motivated by this, we present two new PPI-based techniques that leverage robust regressors to produce even lower variance estimators in the few-label regime."
      },
      {
        "id": "oai:arXiv.org:2411.13918v4",
        "title": "Quantization without Tears",
        "link": "https://arxiv.org/abs/2411.13918",
        "author": "Minghao Fu, Hao Yu, Jie Shao, Junjie Zhou, Ke Zhu, Jianxin Wu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.13918v4 Announce Type: replace \nAbstract: Deep neural networks, while achieving remarkable success across diverse tasks, demand significant resources, including computation, GPU memory, bandwidth, storage, and energy. Network quantization, as a standard compression and acceleration technique, reduces storage costs and enables potential inference acceleration by discretizing network weights and activations into a finite set of integer values. However, current quantization methods are often complex and sensitive, requiring extensive task-specific hyperparameters, where even a single misconfiguration can impair model performance, limiting generality across different models and tasks. In this paper, we propose Quantization without Tears (QwT), a method that simultaneously achieves quantization speed, accuracy, simplicity, and generality. The key insight of QwT is to incorporate a lightweight additional structure into the quantized network to mitigate information loss during quantization. This structure consists solely of a small set of linear layers, keeping the method simple and efficient. More importantly, it provides a closed-form solution, allowing us to improve accuracy effortlessly under 2 minutes. Extensive experiments across various vision, language, and multimodal tasks demonstrate that QwT is both highly effective and versatile. In fact, our approach offers a robust solution for network quantization that combines simplicity, accuracy, and adaptability, which provides new insights for the design of novel quantization paradigms. The code is publicly available at https://github.com/wujx2001/QwT"
      },
      {
        "id": "oai:arXiv.org:2411.16167v3",
        "title": "Mind the Cost of Scaffold! Benign Clients May Even Become Accomplices of Backdoor Attack",
        "link": "https://arxiv.org/abs/2411.16167",
        "author": "Xingshuo Han, Xuanye Zhang, Xiang Lan, Haozhao Wang, Shengmin Xu, Shen Ren, Jason Zeng, Ming Wu, Michael Heinrich, Tianwei Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16167v3 Announce Type: replace \nAbstract: By using a control variate to calibrate the local gradient of each client, Scaffold has been widely known as a powerful solution to mitigate the impact of data heterogeneity in Federated Learning. Although Scaffold achieves significant performance improvements, we show that this superiority is at the cost of increased security vulnerabilities. Specifically, this paper presents BadSFL, the first backdoor attack targeting Scaffold, which turns benign clients into accomplices to amplify the attack effect. The core idea of BadSFL is to uniquely tamper with the control variate to subtly steer benign clients' local gradient updates towards the attacker's poisoned direction, effectively turning them into unwitting accomplices and significantly enhancing the backdoor persistence. Additionally, BadSFL leverages a GAN-enhanced poisoning strategy to enrich the attacker's dataset, maintaining high accuracy on both benign and backdoored samples while remaining stealthy. Extensive experiments demonstrate that BadSFL achieves superior attack durability, maintaining effectiveness for over 60 global rounds, lasting up to three times longer than existing baselines even after ceasing malicious model injections."
      },
      {
        "id": "oai:arXiv.org:2411.17616v4",
        "title": "Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints",
        "link": "https://arxiv.org/abs/2411.17616",
        "author": "Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.17616v4 Announce Type: replace \nAbstract: Diffusion Transformers (DiT) have emerged as a powerful architecture for image and video generation, offering superior quality and scalability. However, their practical application suffers from inherent dynamic feature instability, leading to error amplification during cached inference. Through systematic analysis, we identify the absence of long-range feature preservation mechanisms as the root cause of unstable feature propagation and perturbation sensitivity. To this end, we propose Skip-DiT, an image and video generative DiT variant enhanced with Long-Skip-Connections (LSCs) - the key efficiency component in U-Nets. Theoretical spectral norm and visualization analysis demonstrate how LSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic feature enable an efficient statical caching mechanism that reuses deep features across timesteps while updating shallow components. Extensive experiments across the image and video generation tasks demonstrate that Skip-DiT achieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2 times inference acceleration with negligible quality loss and high fidelity to the original output, outperforming existing DiT caching methods across various quantitative metrics. Our findings establish Long-Skip-Connections as critical architectural components for stable and efficient diffusion transformers. Codes are provided in the https://github.com/OpenSparseLLMs/Skip-DiT."
      },
      {
        "id": "oai:arXiv.org:2411.18702v2",
        "title": "Random Walks with Tweedie: A Unified View of Score-Based Diffusion Models",
        "link": "https://arxiv.org/abs/2411.18702",
        "author": "Chicago Y. Park, Michael T. McCann, Cristina Garcia-Cardona, Brendt Wohlberg, Ulugbek S. Kamilov",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.18702v2 Announce Type: replace \nAbstract: We present a concise derivation for several influential score-based diffusion models that relies on only a few textbook results. Diffusion models have recently emerged as powerful tools for generating realistic, synthetic signals -- particularly natural images -- and often play a role in state-of-the-art algorithms for inverse problems in image processing. While these algorithms are often surprisingly simple, the theory behind them is not, and multiple complex theoretical justifications exist in the literature. Here, we provide a simple and largely self-contained theoretical justification for score-based diffusion models that is targeted towards the signal processing community. This approach leads to generic algorithmic templates for training and generating samples with diffusion models. We show that several influential diffusion models correspond to particular choices within these templates and demonstrate that alternative, more straightforward algorithmic choices can provide comparable results. This approach has the added benefit of enabling conditional sampling without any likelihood approximation."
      },
      {
        "id": "oai:arXiv.org:2411.19230v2",
        "title": "Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG",
        "link": "https://arxiv.org/abs/2411.19230",
        "author": "Xinxu Wei, Kanhao Zhao, Yong Jiao, Hua Xie, Lifang He, Yu Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.19230v2 Announce Type: replace \nAbstract: Effectively utilizing extensive unlabeled high-density EEG data to improve performance in scenarios with limited labeled low-density EEG data presents a significant challenge. In this paper, we address this challenge by formulating it as a graph transfer learning and knowledge distillation problem. We propose a Unified Pre-trained Graph Contrastive Masked Autoencoder Distiller, named EEG-DisGCMAE, to bridge the gap between unlabeled and labeled as well as high- and low-density EEG data. Our approach introduces a novel unified graph self-supervised pre-training paradigm, which seamlessly integrates the graph contrastive pre-training with the graph masked autoencoder pre-training. Furthermore, we propose a graph topology distillation loss function, allowing a lightweight student model trained on low-density data to learn from a teacher model trained on high-density data during pre-training and fine-tuning. This method effectively handles missing electrodes through contrastive distillation. We validate the effectiveness of EEG-DisGCMAE across four classification tasks using two clinical EEG datasets with abundant data. The source code is available at https://github.com/weixinxu666/EEG_DisGCMAE."
      },
      {
        "id": "oai:arXiv.org:2412.01717v3",
        "title": "Driving View Synthesis on Free-form Trajectories with Generative Prior",
        "link": "https://arxiv.org/abs/2412.01717",
        "author": "Zeyu Yang, Zijie Pan, Yuankun Yang, Xiatian Zhu, Li Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01717v3 Announce Type: replace \nAbstract: Driving view synthesis along free-form trajectories is essential for realistic driving simulations, enabling closed-loop evaluation of end-to-end driving policies. Existing methods excel at view interpolation along recorded paths but struggle to generalize to novel trajectories due to limited viewpoints in driving videos. To tackle this challenge, we propose DriveX, a novel free-form driving view synthesis framework, that progressively distills generative prior into the 3D Gaussian model during its optimization. Within this framework, we utilize a video diffusion model to refine the degraded novel trajectory renderings from the in-training Gaussian model, while the restored videos in turn serve as additional supervision for optimizing the 3D Gaussian. Concretely, we craft an inpainting-based video restoration task, which can disentangle the identification of degraded regions from the generative capability of the diffusion model and remove the need of simulating specific degraded pattern in the training of the diffusion model. To further enhance the consistency and fidelity of generated contents, the pseudo ground truth is progressively updated with gradually improved novel trajectory rendering, allowing both components to co-adapt and reinforce each other while minimizing the disruption on the optimization. By tightly integrating 3D scene representation with generative prior, DriveX achieves high-quality view synthesis beyond recorded trajectories in real time--unlocking new possibilities for flexible and realistic driving simulations on free-form trajectories."
      },
      {
        "id": "oai:arXiv.org:2412.01787v5",
        "title": "Pretrained Reversible Generation as Unsupervised Visual Representation Learning",
        "link": "https://arxiv.org/abs/2412.01787",
        "author": "Rongkun Xue, Jinouwen Zhang, Yazhe Niu, Dazhong Shen, Bingqi Ma, Yu Liu, Jing Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01787v5 Announce Type: replace \nAbstract: Recent generative models based on score matching and flow matching have significantly advanced generation tasks, but their potential in discriminative tasks remains underexplored. Previous approaches, such as generative classifiers, have not fully leveraged the capabilities of these models for discriminative tasks due to their intricate designs. We propose Pretrained Reversible Generation (PRG), which extracts unsupervised representations by reversing the generative process of a pretrained continuous generation model. PRG effectively reuses unsupervised generative models, leveraging their high capacity to serve as robust and generalizable feature extractors for downstream tasks. This framework enables the flexible selection of feature hierarchies tailored to specific downstream tasks. Our method consistently outperforms prior approaches across multiple benchmarks, achieving state-of-the-art performance among generative model based methods, including 78% top-1 accuracy on ImageNet at a resolution of 64*64. Extensive ablation studies, including out-of-distribution evaluations, further validate the effectiveness of our approach.PRG is available at https://github.com/opendilab/PRG."
      },
      {
        "id": "oai:arXiv.org:2412.01827v2",
        "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
        "link": "https://arxiv.org/abs/2412.01827",
        "author": "Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01827v2 Announce Type: replace \nAbstract: We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generating images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enables random order by inserting a \"position instruction token\" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to its conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner. We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Our project page is at https://rand-ar.github.io/."
      },
      {
        "id": "oai:arXiv.org:2412.02287v2",
        "title": "Viewpoint Consistency in 3D Generation via Attention and CLIP Guidance",
        "link": "https://arxiv.org/abs/2412.02287",
        "author": "Qing Zhang, Zehao Chen, Jinguang Tong, Jing Zhang, Jie Hong, Xuesong Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.02287v2 Announce Type: replace \nAbstract: Despite recent advances in text-to-3D generation techniques, current methods often suffer from geometric inconsistencies, commonly referred to as the Janus Problem. This paper identifies the root cause of the Janus Problem: viewpoint generation bias in diffusion models, which creates a significant gap between the actual generated viewpoint and the expected one required for optimizing the 3D model. To address this issue, we propose a tuning-free approach called the Attention and CLIP Guidance (ACG) mechanism. ACG enhances desired viewpoints by adaptively controlling cross-attention maps, employs CLIP-based view-text similarities to filter out erroneous viewpoints, and uses a coarse-to-fine optimization strategy with staged prompts to progressively refine 3D generation. Extensive experiments demonstrate that our method significantly reduces the Janus Problem without compromising generation speed, establishing ACG as an efficient, plug-and-play component for existing text-to-3D frameworks."
      },
      {
        "id": "oai:arXiv.org:2412.11459v2",
        "title": "Rethinking Associative Memory Mechanism in Induction Head",
        "link": "https://arxiv.org/abs/2412.11459",
        "author": "Shuo Wang, Issei Sato",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.11459v2 Announce Type: replace \nAbstract: Induction head mechanism is a part of the computational circuits for in-context learning (ICL) that enable large language models (LLMs) to adapt to new tasks without fine-tuning. Most existing work explains the training dynamics behind acquiring such a powerful mechanism. However, the model's ability to coordinate in-context information over long contexts and global knowledge acquired during pretraining remains poorly understood. This paper investigates how a two-layer transformer thoroughly captures in-context information and balances it with pretrained bigram knowledge in next token prediction, from the viewpoint of associative memory. We theoretically analyze the representation of weight matrices in attention layers and the resulting logits when a transformer is given prompts generated by a bigram model. In the experiments, we design specific prompts to evaluate whether the outputs of the trained transformer align with the theoretical results."
      },
      {
        "id": "oai:arXiv.org:2412.13875v2",
        "title": "Enhancing Visual Re-ranking through Denoising Nearest Neighbor Graph via Continuous CRF",
        "link": "https://arxiv.org/abs/2412.13875",
        "author": "Jaeyoon Kim, Yoonki Cho, Taeyoung Kim, Sung-Eui Yoon",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.13875v2 Announce Type: replace \nAbstract: Nearest neighbor (NN) graph based visual re-ranking has emerged as a powerful approach for improving retrieval accuracy, offering the advantages of effectively exploring high-dimensional manifolds without requiring additional fine-tuning. However, the effectiveness of NN graph-based re-ranking is fundamentally constrained by the quality of its edge connectivity, as incorrect connections between dissimilar (negative) images frequently occur. This is known as a noisy edge problem, which hinders the re-ranking performance of existing techniques and limits their potential. To remedy this issue, we propose a complementary denoising method based on Continuous Conditional Random Fields (C-CRF) that leverages statistical distances derived from similarity-based distributions. As a pre-processing step for enhancing NN graph-based retrieval, our approach constructs fully connected cliques around each anchor image and employs a novel statistical distance metric to robustly alleviate noisy edges before re-ranking while achieving efficient processing through offline computation. Extensive experimental results demonstrate that our method consistently improves three different NN graph-based re-ranking approaches, yielding significant gains in retrieval accuracy."
      },
      {
        "id": "oai:arXiv.org:2501.01366v2",
        "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding",
        "link": "https://arxiv.org/abs/2501.01366",
        "author": "Austin T. Wang, ZeMing Gong, Angel X. Chang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.01366v2 Announce Type: replace \nAbstract: 3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications."
      },
      {
        "id": "oai:arXiv.org:2501.01370v3",
        "title": "Embedding-Based Approaches to Hyperpartisan News Detection",
        "link": "https://arxiv.org/abs/2501.01370",
        "author": "Karthik Mohan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.01370v3 Announce Type: replace \nAbstract: In this report, I describe the systems in which the objective is to determine whether a given news article could be considered as hyperpartisan. Hyperpartisan news takes an extremely polarized political standpoint with an intention of creating political divide among the public. Several approaches, including n-grams, sentiment analysis, as well as sentence and document representations using pre-tained ELMo models were used. The best system is using LLMs for embedding generation achieving an accuracy of around 92% over the previously best system using pre-trained ELMo with Bidirectional LSTM which achieved an accuracy of around 83% through 10-fold cross-validation."
      },
      {
        "id": "oai:arXiv.org:2501.03113v2",
        "title": "Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality",
        "link": "https://arxiv.org/abs/2501.03113",
        "author": "Joshua Southern, Yam Eitan, Guy Bar-Shalom, Michael Bronstein, Haggai Maron, Fabrizio Frasca",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.03113v2 Announce Type: replace \nAbstract: Subgraph GNNs have emerged as promising architectures that overcome the expressiveness limitations of Graph Neural Networks (GNNs) by processing bags of subgraphs. Despite their compelling empirical performance, these methods are afflicted by a high computational complexity: they process bags whose size grows linearly in the number of nodes, hindering their applicability to larger graphs. In this work, we propose an effective and easy-to-implement approach to dramatically alleviate the computational cost of Subgraph GNNs and unleash broader applications thereof. Our method, dubbed HyMN, leverages walk-based centrality measures to sample a small number of relevant subgraphs and drastically reduce the bag size. By drawing a connection to perturbation analysis, we highlight the strength of the proposed centrality-based subgraph sampling, and further prove that these walk-based centralities can be additionally used as Structural Encodings for improved discriminative power. A comprehensive set of experimental results demonstrates that HyMN provides an effective synthesis of expressiveness, efficiency, and downstream performance, unlocking the application of Subgraph GNNs to dramatically larger graphs. Not only does our method outperform more sophisticated subgraph sampling approaches, it is also competitive, and sometimes better, than other state-of-the-art approaches for a fraction of their runtime."
      },
      {
        "id": "oai:arXiv.org:2501.06035v3",
        "title": "Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction",
        "link": "https://arxiv.org/abs/2501.06035",
        "author": "Cecilia Curreli, Dominik Muhle, Abhishek Saroha, Zhenzhang Ye, Riccardo Marin, Daniel Cremers",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06035v3 Announce Type: replace \nAbstract: Probabilistic human motion prediction aims to forecast multiple possible future movements from past observations. While current approaches report high diversity and realism, they often generate motions with undetected limb stretching and jitter. To address this, we introduce SkeletonDiffusion, a latent diffusion model that embeds an explicit inductive bias on the human body within its architecture and training. Our model is trained with a novel nonisotropic Gaussian diffusion formulation that aligns with the natural kinematic structure of the human skeleton. Results show that our approach outperforms conventional isotropic alternatives, consistently generating realistic predictions while avoiding artifacts such as limb distortion. Additionally, we identify a limitation in commonly used diversity metrics, which may inadvertently favor models that produce inconsistent limb lengths within the same sequence. SkeletonDiffusion sets a new benchmark on real-world datasets, outperforming various baselines across multiple evaluation metrics. Visit our project page at https://ceveloper.github.io/publications/skeletondiffusion/ ."
      },
      {
        "id": "oai:arXiv.org:2502.04468v2",
        "title": "Iterative Importance Fine-tuning of Diffusion Models",
        "link": "https://arxiv.org/abs/2502.04468",
        "author": "Alexander Denker, Shreyas Padhy, Francisco Vargas, Johannes Hertrich",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04468v2 Announce Type: replace \nAbstract: Diffusion models are an important tool for generative modelling, serving as effective priors in applications such as imaging and protein design. A key challenge in applying diffusion models for downstream tasks is efficiently sampling from resulting posterior distributions, which can be addressed using the $h$-transform. This work introduces a self-supervised algorithm for fine-tuning diffusion models by estimating the $h$-transform, enabling amortised conditional sampling. Our method iteratively refines the $h$-transform using a synthetic dataset resampled with path-based importance weights. We demonstrate the effectiveness of this framework on class-conditional sampling, inverse problems and reward fine-tuning for text-to-image diffusion models."
      },
      {
        "id": "oai:arXiv.org:2502.04557v3",
        "title": "Speeding up Speculative Decoding via Sequential Approximate Verification",
        "link": "https://arxiv.org/abs/2502.04557",
        "author": "Meiyu Zhong, Noel Teku, Ravi Tandon",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.04557v3 Announce Type: replace \nAbstract: Speculative Decoding (SD) is a recently proposed technique for faster inference using Large Language Models (LLMs). SD operates by using a smaller draft LLM for autoregressively generating a sequence of tokens and a larger target LLM for parallel verification to ensure statistical consistency. However, periodic parallel calls to the target LLM for verification prevent SD from achieving even lower latencies. We propose SPRINTER, which utilizes a low-complexity verifier trained to predict if tokens generated from a draft LLM would be accepted by the target LLM. By performing sequential approximate verification, SPRINTER does not require verification by the target LLM and is only invoked when a token is deemed unacceptable. This reduces the number of calls to the larger LLM, achieving further speedups and lower computation cost. We present a theoretical analysis of SPRINTER, examining the statistical properties of the generated tokens, as well as the expected reduction in latency as a function of the verifier. We evaluate SPRINTER on several datasets and model pairs, demonstrating that approximate verification can still maintain high quality generation while further reducing latency."
      },
      {
        "id": "oai:arXiv.org:2502.05307v2",
        "title": "Training Set Reconstruction from Differentially Private Forests: How Effective is DP?",
        "link": "https://arxiv.org/abs/2502.05307",
        "author": "Alice Gorg\\'e, Julien Ferry, S\\'ebastien Gambs, Thibaut Vidal",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05307v2 Announce Type: replace \nAbstract: Recent research has shown that machine learning models are vulnerable to privacy attacks targeting their training data. To mitigate these risks, differential privacy (DP) has become a widely adopted countermeasure, as it offers rigorous privacy protection. In this paper, we introduce a reconstruction attack targeting state-of-the-art $\\varepsilon$-DP random forests. By leveraging a constraint programming model that incorporates knowledge of the forest's structure and DP mechanism characteristics, our approach formally reconstructs the most likely dataset that could have produced a given forest. Through extensive computational experiments, we examine the interplay between model utility, privacy guarantees and reconstruction accuracy across various configurations. Our results reveal that random forests trained with meaningful DP guarantees can still leak portions of their training data. Specifically, while DP reduces the success of reconstruction attacks, the only forests fully robust to our attack exhibit predictive performance no better than a constant classifier. Building on these insights, we provide practical recommendations for the construction of DP random forests that are more resilient to reconstruction attacks and maintain non-trivial predictive performance."
      },
      {
        "id": "oai:arXiv.org:2502.05325v2",
        "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
        "link": "https://arxiv.org/abs/2502.05325",
        "author": "Awa Khouna, Julien Ferry, Thibaut Vidal",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.05325v2 Announce Type: replace \nAbstract: The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the \"oracle'' queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment."
      },
      {
        "id": "oai:arXiv.org:2502.06816v2",
        "title": "DeepCell: Self-Supervised Multiview Fusion for Circuit Representation Learning",
        "link": "https://arxiv.org/abs/2502.06816",
        "author": "Zhengyuan Shi, Chengyu Ma, Ziyang Zheng, Lingfeng Zhou, Hongyang Pan, Wentao Jiang, Fan Yang, Xiaoyan Yang, Zhufei Chu, Qiang Xu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06816v2 Announce Type: replace \nAbstract: We introduce DeepCell, a novel circuit representation learning framework that effectively integrates multiview information from both And-Inverter Graphs (AIGs) and Post-Mapping (PM) netlists. At its core, DeepCell employs a self-supervised Mask Circuit Modeling (MCM) strategy, inspired by masked language modeling, to fuse complementary circuit representations from different design stages into unified and rich embeddings. To our knowledge, DeepCell is the first framework explicitly designed for PM netlist representation learning, setting new benchmarks in both predictive accuracy and reconstruction quality. We demonstrate the practical efficacy of DeepCell by applying it to critical EDA tasks such as functional Engineering Change Orders (ECO) and technology mapping. Extensive experimental results show that DeepCell significantly surpasses state-of-the-art open-source EDA tools in efficiency and performance."
      },
      {
        "id": "oai:arXiv.org:2502.07616v2",
        "title": "Tractable Transformers for Flexible Conditional Generation",
        "link": "https://arxiv.org/abs/2502.07616",
        "author": "Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07616v2 Announce Type: replace \nAbstract: Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries (i.e., the set of unknown variables) unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines."
      },
      {
        "id": "oai:arXiv.org:2502.08696v3",
        "title": "Scalable Discrete Diffusion Samplers: Combinatorial Optimization and Statistical Physics",
        "link": "https://arxiv.org/abs/2502.08696",
        "author": "Sebastian Sanokowski, Wilhelm Berghammer, Martin Ennemoser, Haoyu Peter Wang, Sepp Hochreiter, Sebastian Lehner",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.08696v3 Announce Type: replace \nAbstract: Learning to sample from complex unnormalized distributions over discrete domains emerged as a promising research direction with applications in statistical physics, variational inference, and combinatorial optimization. Recent work has demonstrated the potential of diffusion models in this domain. However, existing methods face limitations in memory scaling and thus the number of attainable diffusion steps since they require backpropagation through the entire generative process. To overcome these limitations we introduce two novel training methods for discrete diffusion samplers, one grounded in the policy gradient theorem and the other one leveraging Self-Normalized Neural Importance Sampling (SN-NIS). These methods yield memory-efficient training and achieve state-of-the-art results in unsupervised combinatorial optimization. Numerous scientific applications additionally require the ability of unbiased sampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte Carlo that enable for the first time the application of discrete diffusion models to this problem. We validate our methods on Ising model benchmarks and find that they outperform popular autoregressive approaches. Our work opens new avenues for applying diffusion models to a wide range of scientific applications in discrete domains that were hitherto restricted to exact likelihood models."
      },
      {
        "id": "oai:arXiv.org:2502.12086v2",
        "title": "Unifying Explainable Anomaly Detection and Root Cause Analysis in Dynamical Systems",
        "link": "https://arxiv.org/abs/2502.12086",
        "author": "Yue Sun, Rick S. Blum, Parv Venkitasubramaniam",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12086v2 Announce Type: replace \nAbstract: Dynamical systems, prevalent in various scientific and engineering domains, are susceptible to anomalies that can significantly impact their performance and reliability. This paper addresses the critical challenges of anomaly detection, root cause localization, and anomaly type classification in dynamical systems governed by ordinary differential equations (ODEs). We define two categories of anomalies: cyber anomalies, which propagate through interconnected variables, and measurement anomalies, which remain localized to individual variables. To address these challenges, we propose the Interpretable Causality Ordinary Differential Equation (ICODE) Networks, a model-intrinsic explainable learning framework. ICODE leverages Neural ODEs for anomaly detection while employing causality inference through an explanation channel to perform root cause analysis (RCA), elucidating why specific time periods are flagged as anomalous. ICODE is designed to simultaneously perform anomaly detection, RCA, and anomaly type classification within a single, interpretable framework. Our approach is grounded in the hypothesis that anomalies alter the underlying ODEs of the system, manifesting as changes in causal relationships between variables. We provide a theoretical analysis of how perturbations in learned model parameters can be utilized to identify anomalies and their root causes in time series data. Comprehensive experimental evaluations demonstrate the efficacy of ICODE across various dynamical systems, showcasing its ability to accurately detect anomalies, classify their types, and pinpoint their origins."
      },
      {
        "id": "oai:arXiv.org:2502.12632v3",
        "title": "MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation",
        "link": "https://arxiv.org/abs/2502.12632",
        "author": "Sihyun Yu, Meera Hahn, Dan Kondratyuk, Jinwoo Shin, Agrim Gupta, Jos\\'e Lezama, Irfan Essa, David Ross, Jonathan Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12632v3 Announce Type: replace \nAbstract: Diffusion models are successful for synthesizing high-quality videos but are limited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained footage (e.g. over minutes) still remains an open research question. In this paper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers), a new diffusion model specialized for long video generation. MALT Diffusion (or just MALT) handles long videos by subdividing them into short segments and doing segment-level autoregressive generation. To achieve this, we first propose recurrent attention layers that encode multiple segments into a compact memory latent vector; by maintaining this memory vector over time, MALT is able to condition on it and continuously generate new footage based on a long temporal context. We also present several training techniques that enable the model to generate frames over a long horizon with consistent quality and minimal degradation. We validate the effectiveness of MALT through experiments on long video benchmarks. We first perform extensive analysis of MALT in long-contextual understanding capability and stability using popular long video benchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video generation on UCF-101, outperforming the previous state-of-the-art of 648.4. Finally, we explore MALT's capabilities in a text-to-video generation setting and show that it can produce long videos compared with recent techniques for long text-to-video generation."
      },
      {
        "id": "oai:arXiv.org:2502.14429v2",
        "title": "Early-Exit and Instant Confidence Translation Quality Estimation",
        "link": "https://arxiv.org/abs/2502.14429",
        "author": "Vil\\'em Zouhar, Maike Z\\\"ufle, Beni Egressy, Julius Cheng, Mrinmaya Sachan, Jan Niehues",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14429v2 Announce Type: replace \nAbstract: Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) reducing the cost of quality estimation at scale, and (2) developing an inexpensive uncertainty estimation method for quality estimation. To address the latter, we introduce Instant Confidence COMET, an uncertainty-aware quality estimation model that matches the performance of previous approaches at a fraction of their costs. We extend this to Early-Exit COMET, a quality estimation model that can compute quality scores and associated confidences already at early model layers, allowing us to early-exit computations and reduce evaluation costs. We also apply our model to machine translation reranking. We combine Early-Exit COMET with an upper confidence bound bandit algorithm to find the best candidate from a large pool without having to run the full evaluation model on all candidates. In both cases (evaluation and reranking) our methods reduce the required compute by 50% with very little degradation in performance. Finally, we show how Instant Confidence COMET can be used to decide which translations a human evaluator should score rather than relying on the COMET score."
      },
      {
        "id": "oai:arXiv.org:2502.14583v2",
        "title": "A Theory for Conditional Generative Modeling on Multiple Data Sources",
        "link": "https://arxiv.org/abs/2502.14583",
        "author": "Rongzhen Wang, Yan Zhang, Chenyu Zheng, Chongxuan Li, Guoqiang Wu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14583v2 Announce Type: replace \nAbstract: The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source. Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number. Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training. We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. Simulations and real-world experiments are conducted to validate the theory, with code available at: https://github.com/ML-GSAI/Multi-Source-GM."
      },
      {
        "id": "oai:arXiv.org:2502.16548v2",
        "title": "Composable Strategy Framework with Integrated Video-Text based Large Language Models for Heart Failure Assessment",
        "link": "https://arxiv.org/abs/2502.16548",
        "author": "Jianzhou Chen, Jinyang Sun, Xiumei Wang, Xi Chen, Heyu Chu, Guo Song, Yuji Luo, Xingping Zhou, Rong Gu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16548v2 Announce Type: replace \nAbstract: Heart failure is one of the leading causes of death worldwide, with millons of deaths each year, according to data from the World Health Organization (WHO) and other public health agencies. While significant progress has been made in the field of heart failure, leading to improved survival rates and improvement of ejection fraction, there remains substantial unmet needs, due to the complexity and multifactorial characteristics. Therefore, we propose a composable strategy framework for assessment and treatment optimization in heart failure. This framework simulates the doctor-patient consultation process and leverages multi-modal algorithms to analyze a range of data, including video, physical examination, text results as well as medical history. By integrating these various data sources, our framework offers a more holistic evaluation and optimized treatment plan for patients. Our results demonstrate that this multi-modal approach outperforms single-modal artificial intelligence (AI) algorithms in terms of accuracy in heart failure (HF) prognosis prediction. Through this method, we can further evaluate the impact of various pathological indicators on HF prognosis,providing a more comprehensive evaluation."
      },
      {
        "id": "oai:arXiv.org:2502.18116v3",
        "title": "Bayesian Optimization for Controlled Image Editing via LLMs",
        "link": "https://arxiv.org/abs/2502.18116",
        "author": "Chengkun Cai, Haoliang Liu, Xu Zhao, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, John Lee, Jenq-Neng Hwang, Lei Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.18116v3 Announce Type: replace \nAbstract: In the rapidly evolving field of image generation, achieving precise control over generated content and maintaining semantic consistency remain significant limitations, particularly concerning grounding techniques and the necessity for model fine-tuning. To address these challenges, we propose BayesGenie, an off-the-shelf approach that integrates Large Language Models (LLMs) with Bayesian Optimization to facilitate precise and user-friendly image editing. Our method enables users to modify images through natural language descriptions without manual area marking, while preserving the original image's semantic integrity. Unlike existing techniques that require extensive pre-training or fine-tuning, our approach demonstrates remarkable adaptability across various LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian optimization strategy to automatically refine the inference process parameters, achieving high-precision image editing with minimal user intervention. Through extensive experiments across diverse scenarios, we demonstrate that our framework significantly outperforms existing methods in both editing accuracy and semantic preservation, as validated using different LLMs including Claude3 and GPT-4."
      },
      {
        "id": "oai:arXiv.org:2502.20292v5",
        "title": "Visual Adaptive Prompting for Compositional Zero-Shot Learning",
        "link": "https://arxiv.org/abs/2502.20292",
        "author": "Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20292v5 Announce Type: replace \nAbstract: Vision-Language Models (VLMs) have demonstrated impressive multimodal capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives--such as attributes and objects--that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose a Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2502.20855v2",
        "title": "MAMUT: A Novel Framework for Modifying Mathematical Formulas for the Generation of Specialized Datasets for Language Model Training",
        "link": "https://arxiv.org/abs/2502.20855",
        "author": "Jonathan Drechsel, Anja Reusch, Steffen Herbold",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20855v2 Announce Type: replace \nAbstract: Mathematical formulas are a fundamental and widely used component in various scientific fields, serving as a universal language for expressing complex concepts and relationships. While state-of-the-art transformer models excel in processing and understanding natural language, they encounter challenges with mathematical notation, which involves a complex structure and diverse representations. This study focuses on the development of specialized training datasets to enhance the encoding of mathematical content. We introduce Math Mutator (MAMUT), a framework capable of generating equivalent and falsified versions of a given mathematical formula in LaTeX notation, effectively capturing the mathematical variety in notation of the same concept. Based on MAMUT, we have generated four large mathematical datasets containing diverse notation. Experiments show that models trained on these datasets exhibit new SoTA performance on mathematical retrieval tasks. We publish our code, generated datasets, and pretrained mathematical models: https://github.com/aieng-lab/math-mutator."
      },
      {
        "id": "oai:arXiv.org:2502.20964v3",
        "title": "Fine-Grained Knowledge Structuring and Retrieval for Visual Question Answering",
        "link": "https://arxiv.org/abs/2502.20964",
        "author": "Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20964v3 Announce Type: replace \nAbstract: Visual Question Answering (VQA) focuses on providing answers to natural language questions by utilizing information from images. Although cutting-edge multimodal large language models (MLLMs) such as GPT-4o achieve strong performance on VQA tasks, they frequently fall short in accessing domain-specific or the latest knowledge. To mitigate this issue, retrieval-augmented generation (RAG) leveraging external knowledge bases (KBs), referred to as KB-VQA, emerges as a promising approach. Nevertheless, conventional unimodal retrieval techniques, which translate images into textual descriptions, often result in the loss of critical visual details. To address these challenges, this study presents two key innovations. First, we introduce fine-grained knowledge units that consist of multimodal data fragments (e.g. text fragments, entity images, and so on) in a structured manner. Rather than merely refining retrieval mechanisms, we prioritize the systematic organization and management of these knowledge units, ensuring that the structuring process itself enhances retrieval quality. Second, we propose a knowledge unit retrieval-augmented generation framework (KU-RAG) that seamlessly integrates fine-grained retrieval with MLLMs. Our KU-RAG framework not only ensures precise retrieval of relevant knowledge but also enhances reasoning capabilities through a knowledge correction chain. Experimental results demonstrate that our approach consistently outperforms existing KB-VQA methods across four benchmarks, achieving an average improvement of approximately 3% and up to 11% in the best case."
      },
      {
        "id": "oai:arXiv.org:2503.00030v2",
        "title": "RSPO: Regularized Self-Play Alignment of Large Language Models",
        "link": "https://arxiv.org/abs/2503.00030",
        "author": "Xiaohang Tang, Sangwoong Yoon, Seongho Son, Huizhuo Yuan, Quanquan Gu, Ilija Bogunovic",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00030v2 Announce Type: replace \nAbstract: Self-play alignment has emerged as an effective approach for fine-tuning large language models (LLMs), formulating preference optimization as a two-player game. However, the regularization with respect to the reference policy, which is crucial for mitigating over-optimization, has been insufficiently investigated in self-play alignment. To study the impact of different regularization strategies, we propose \\textbf{Regularized Self-Play Policy Optimization (RSPO)}, a general and modular framework that unifies prior methods and enables simple plug-and-play integration of various regularizers, meanwhile preserving convergence to Nash equilibrium of the corresponding regularized game.Our empirical study involving over $120$ fine-tuned Mistral-7B-Instruct models reveals that forward KL divergence regularization reduces response length, whereas reverse KL divergence markedly improves raw win rates. Crucially, RSPO regularized with a linear combination of forward and reverse KL divergence significantly boosts the length-controlled win rate on AlpacaEval-2 from $28.5\\%$ (unregularized self-play, SPPO) to $35.4\\%$, and consistently demonstrates superior performance on Arena-Hard, MT-Bench, ArmoRM scores, and response diversity. Combining simplicity, convergence guarantees, and significant empirical gains, RSPO offers a strong foundation for exploring regularized self-play in language model alignment."
      },
      {
        "id": "oai:arXiv.org:2503.00301v3",
        "title": "Differential Coding for Training-Free ANN-to-SNN Conversion",
        "link": "https://arxiv.org/abs/2503.00301",
        "author": "Zihan Huang, Wei Fang, Tong Bu, Peng Xue, Zecheng Hao, Wenxuan Liu, Yuanhong Tang, Zhaofei Yu, Tiejun Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00301v3 Announce Type: replace \nAbstract: Spiking Neural Networks (SNNs) exhibit significant potential due to their low energy consumption. Converting Artificial Neural Networks (ANNs) to SNNs is an efficient way to achieve high-performance SNNs. However, many conversion methods are based on rate coding, which requires numerous spikes and longer time-steps compared to directly trained SNNs, leading to increased energy consumption and latency. This article introduces differential coding for ANN-to-SNN conversion, a novel coding scheme that reduces spike counts and energy consumption by transmitting changes in rate information rather than rates directly, and explores its application across various layers. Additionally, the threshold iteration method is proposed to optimize thresholds based on activation distribution when converting Rectified Linear Units (ReLUs) to spiking neurons. Experimental results on various Convolutional Neural Networks (CNNs) and Transformers demonstrate that the proposed differential coding significantly improves accuracy while reducing energy consumption, particularly when combined with the threshold iteration method, achieving state-of-the-art performance. The source codes of the proposed method are available at https://github.com/h-z-h-cell/ANN-to-SNN-DCGS."
      },
      {
        "id": "oai:arXiv.org:2503.02233v3",
        "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling",
        "link": "https://arxiv.org/abs/2503.02233",
        "author": "Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.02233v3 Announce Type: replace \nAbstract: Large language models (LLMs) are prone to hallucination stemming from misaligned self-awareness, particularly when processing queries exceeding their knowledge boundaries. While existing mitigation strategies employ uncertainty estimation or query rejection mechanisms, they suffer from computational efficiency and sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate utilization of high-confidence outputs, whereas uncertain predictions trigger a slow refinement model for accuracy improvement. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. The framework establishes a scalable paradigm for deploying reliable LLMs in error-sensitive applications, effectively balancing accuracy and practical utility."
      },
      {
        "id": "oai:arXiv.org:2503.04504v2",
        "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
        "link": "https://arxiv.org/abs/2503.04504",
        "author": "Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sein Kwon, Inpyo Hong, Sanghyun Park",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04504v2 Announce Type: replace \nAbstract: Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly."
      },
      {
        "id": "oai:arXiv.org:2503.05763v4",
        "title": "GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification",
        "link": "https://arxiv.org/abs/2503.05763",
        "author": "Aarush Sinha",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.05763v4 Announce Type: replace \nAbstract: Integrating structured graph data with rich textual information from nodes poses a significant challenge, particularly for heterophilic node classification. Current approaches often struggle with computational costs or effective fusion of disparate modalities. We propose \\textbf{Graph Masked Language Model (GMLM)}, a novel architecture efficiently combining Graph Neural Networks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three key innovations: (i) a \\textbf{dynamic active node selection} strategy for scalable PLM text processing; (ii) a GNN-specific \\textbf{contrastive pretraining stage} using soft masking with a learnable graph \\texttt{[MASK]} token for robust structural representations; and (iii) a \\textbf{dedicated fusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \\& DistilBERT) embeddings. Extensive experiments on heterophilic benchmarks (Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably, GMLM(DistilBERT) achieves significant performance gains, improving accuracy by over \\textbf{4.7\\%} on Cornell and over \\textbf{2.0\\%} on Texas compared to the previous best-performing baselines. This work underscores the benefits of targeted PLM engagement and modality-specific pretraining for improved, efficient learning on text-rich graphs."
      },
      {
        "id": "oai:arXiv.org:2503.05893v2",
        "title": "Zero-shot Medical Event Prediction Using a Generative Pre-trained Transformer on Electronic Health Records",
        "link": "https://arxiv.org/abs/2503.05893",
        "author": "Ekaterina Redekop, Zichen Wang, Rushikesh Kulkarni, Mara Pleasure, Aaron Chin, Hamid Reza Hassanzadeh, Brian L. Hill, Melika Emami, William Speier, Corey W. Arnold",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.05893v2 Announce Type: replace \nAbstract: Longitudinal data in electronic health records (EHRs) represent an individual`s clinical history through a sequence of codified concepts, including diagnoses, procedures, medications, and laboratory tests. Generative pre-trained transformers (GPT) can leverage this data to predict future events. While fine-tuning of these models can enhance task-specific performance, it becomes costly when applied to many clinical prediction tasks. In contrast, a pretrained foundation model can be used in zero-shot forecasting setting, offering a scalable alternative to fine-tuning separate models for each outcome.\n  This study presents the first comprehensive analysis of zero-shot forecasting with GPT-based foundational models in EHRs, introducing a novel pipeline that formulates medical concept prediction as a generative modeling task. Unlike supervised approaches requiring extensive labeled data, our method enables the model to forecast a next medical event purely from a pretraining knowledge. We evaluate performance across multiple time horizons and clinical categories, demonstrating model`s ability to capture latent temporal dependencies and complex patient trajectories without task supervision.\n  Model performance for predicting the next medical concept was evaluated using precision and recall metrics, achieving an average top1 precision of 0.614 and recall of 0.524. For 12 major diagnostic conditions, the model demonstrated strong zero-shot performance, achieving high true positive rates while maintaining low false positives.\n  We demonstrate the power of a foundational EHR GPT model in capturing diverse phenotypes and enabling robust, zero-shot forecasting of clinical outcomes. This capability enhances the versatility of predictive healthcare models and reduces the need for task-specific training, enabling more scalable applications in clinical settings."
      },
      {
        "id": "oai:arXiv.org:2503.08199v2",
        "title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging Control Integrating Large Language Models",
        "link": "https://arxiv.org/abs/2503.08199",
        "author": "Miao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang, Shuai Lu, Junfeng Jiao, Tianyu Shi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08199v2 Announce Type: replace \nAbstract: Traditional Reinforcement Learning (RL) suffers from replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues.These tasks are compounded when deep environment understanding, agent coordination and dynamic optimization are required. While Large Language Model (LLM) enhanced methods have shown promise in generalization and interoperability, they often neglect necessary multi-agent coordination. Therefore, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, integrating RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that the CCMA outperforms existing RL methods, demonstrating significant improvements in both micro and macro-level performance in complex driving environments."
      },
      {
        "id": "oai:arXiv.org:2503.08805v2",
        "title": "Filter Like You Test: Data-Driven Data Filtering for CLIP Pretraining",
        "link": "https://arxiv.org/abs/2503.08805",
        "author": "Mikey Shechter, Yair Carmon",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08805v2 Announce Type: replace \nAbstract: We introduce Filter Like You Test (FLYT), an algorithm for curating large-scale vision-language datasets that learns the usefulness of each data point as a pretraining example. FLYT trains a scoring model that learns to weigh each example's features using gradient signals from downstream tasks training sets. Based on FLYT, we implement Mixing-FLYT (M-FLYT), which takes the per-example scores generated by different scoring methods as features, and learns to unify them into a single score. FLYT naturally produces a distribution over the training examples, which we leverage through Soft Cap Sampling (SCS), a strategy for obtaining a filtered pretraining dataset from per-example probabilities that samples examples while preventing over-representation through a repetition penalty. Using these methods, we achieve 40.1% ImageNet zero-shot accuracy on the DataComp medium scale filtering benchmark, a 2% absolute accuracy increase over all previous results and a 5.5% increase over results that - like us - use only public resources. Our approach also yields 37.7\\% on the average of 38 DataComp evaluation tasks, outperforming previous public-resource approaches by 0.4\\%."
      },
      {
        "id": "oai:arXiv.org:2503.09277v2",
        "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
        "link": "https://arxiv.org/abs/2503.09277",
        "author": "Haoxuan Wang, Jinlong Peng, Qingdong He, Hao Yang, Ying Jin, Jiafu Wu, Xiaobin Hu, Yanjie Pan, Zhenye Gan, Mingmin Chi, Bo Peng, Yabiao Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.09277v2 Announce Type: replace \nAbstract: With the rapid development of diffusion models in image generation, the demand for more powerful and flexible controllable frameworks is increasing. Although existing methods can guide generation beyond text prompts, the challenge of effectively combining multiple conditional inputs while maintaining consistency with all of them remains unsolved. To address this, we introduce UniCombine, a DiT-based multi-conditional controllable generative framework capable of handling any combination of conditions, including but not limited to text prompts, spatial maps, and subject images. Specifically, we introduce a novel Conditional MMDiT Attention mechanism and incorporate a trainable LoRA module to build both the training-free and training-based versions. Additionally, we propose a new pipeline to construct SubjectSpatial200K, the first dataset designed for multi-conditional generative tasks covering both the subject-driven and spatially-aligned conditions. Extensive experimental results on multi-conditional generation demonstrate the outstanding universality and powerful capability of our approach with state-of-the-art performance."
      },
      {
        "id": "oai:arXiv.org:2503.12149v2",
        "title": "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2503.12149",
        "author": "Junjie Chen, Xuyang Liu, Subin Huang, Linfeng Zhang, Hang Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.12149v2 Announce Type: replace \nAbstract: With the advent of large vision-language models (LVLMs) demonstrating increasingly human-like abilities, a pivotal question emerges: do different LVLMs interpret multimodal sarcasm differently, and can a single model grasp sarcasm from multiple perspectives like humans? To explore this, we introduce an analytical framework using systematically designed prompts on existing multimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409 samples, we examine interpretive variations within and across models, focusing on confidence levels, alignment with dataset labels, and recognition of ambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across LVLMs and within the same model under varied prompts. While classification-oriented prompts yield higher internal consistency, models diverge markedly when tasked with interpretive reasoning. These results challenge binary labeling paradigms by highlighting sarcasm's subjectivity. We advocate moving beyond rigid annotation schemes toward multi-perspective, uncertainty-aware modeling, offering deeper insights into multimodal sarcasm comprehension. Our code and data are available at: https://github.com/CoderChen01/LVLMSarcasmAnalysis"
      },
      {
        "id": "oai:arXiv.org:2503.13299v2",
        "title": "A Survey on Transformer Context Extension: Approaches and Evaluation",
        "link": "https://arxiv.org/abs/2503.13299",
        "author": "Yijun Liu, Jinzheng Yu, Yang Xu, Zhongyang Li, Qingfu Zhu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13299v2 Announce Type: replace \nAbstract: Large language models (LLMs) based on Transformer have been widely applied in the filed of natural language processing (NLP), demonstrating strong performance, particularly in handling short text tasks. However, when it comes to long context scenarios, the performance of LLMs degrades due to some challenges. To alleviate this phenomenon, there is a number of work proposed recently. In this survey, we first list the challenges of applying pre-trained LLMs to process long contexts. Then systematically review the approaches related to long context and propose our taxonomy categorizing them into four main types: positional encoding, context compression, retrieval augmented, and attention pattern. In addition to the approaches, we focus on the evaluation of long context, organizing relevant data, tasks, and metrics based on existing long context benchmarks. Finally, we summarize unresolved issues in the long context domain and put forward our views on future developments."
      },
      {
        "id": "oai:arXiv.org:2503.13575v2",
        "title": "Analytic Subspace Routing: How Recursive Least Squares Works in Continual Learning of Large Language Model",
        "link": "https://arxiv.org/abs/2503.13575",
        "author": "Kai Tong, Kang Pan, Xiao Zhang, Erli Meng, Run He, Yawen Cui, Nuoyan Guo, Huiping Zhuang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.13575v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) possess encompassing capabilities that can process diverse language-related tasks. However, finetuning on LLMs will diminish this general skills and continual finetuning will further cause severe degradation on accumulated knowledge. Recently, Continual Learning (CL) in Large Language Models (LLMs) arises which aims to continually adapt the LLMs to new tasks while maintaining previously learned knowledge and inheriting general skills. Existing techniques either leverage previous data to replay, leading to extra computational costs, or utilize a single parameter-efficient module to learn the downstream task, constraining new knowledge absorption with interference between different tasks. Toward these issues, this paper proposes Analytic Subspace Routing(ASR) to address these challenges. For each task, we isolate the learning within a subspace of deep layers' features via low-rank adaptation, eliminating knowledge interference between different tasks. Additionally, we propose an analytic routing mechanism to properly utilize knowledge learned in different subspaces. Our approach employs Recursive Least Squares to train a multi-task router model, allowing the router to dynamically adapt to incoming data without requiring access to historical data. Also, the router effectively assigns the current task to an appropriate subspace and has a non-forgetting property of previously learned tasks with a solid theoretical guarantee. Experimental results demonstrate that our method achieves near-perfect retention of prior knowledge while seamlessly integrating new information, effectively overcoming the core limitations of existing methods. Our code will be released after acceptance."
      },
      {
        "id": "oai:arXiv.org:2503.14552v2",
        "title": "Eyes on the Environment: AI-Driven Analysis for Fire and Smoke Classification, Segmentation, and Detection",
        "link": "https://arxiv.org/abs/2503.14552",
        "author": "Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Fatemeh Afghah, Connor Peter McGrath, Danish Bhatkar, Mithilesh Anil Biradar, Abolfazl Razi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.14552v2 Announce Type: replace \nAbstract: Fire and smoke phenomena pose a significant threat to the natural environment, ecosystems, and global economy, as well as human lives and wildlife. In this particular circumstance, there is a demand for more sophisticated and advanced technologies to implement an effective strategy for early detection, real-time monitoring, and minimizing the overall impacts of fires on ecological balance and public safety. Recently, the rapid advancement of Artificial Intelligence (AI) and Computer Vision (CV) frameworks has substantially revolutionized the momentum for developing efficient fire management systems. However, these systems extensively rely on the availability of adequate and high-quality fire and smoke data to create proficient Machine Learning (ML) methods for various tasks, such as detection and monitoring. Although fire and smoke datasets play a critical role in training, evaluating, and testing advanced Deep Learning (DL) models, a comprehensive review of the existing datasets is still unexplored. For this purpose, we provide an in-depth review to systematically analyze and evaluate fire and smoke datasets collected over the past 20 years. We investigate the characteristics of each dataset, including type, size, format, collection methods, and geographical diversities. We also review and highlight the unique features of each dataset, such as imaging modalities (RGB, thermal, infrared) and their applicability for different fire management tasks (classification, segmentation, detection). Furthermore, we summarize the strengths and weaknesses of each dataset and discuss their potential for advancing research and technology in fire management. Ultimately, we conduct extensive experimental analyses across different datasets using several state-of-the-art algorithms, such as ResNet-50, DeepLab-V3, and YoloV8."
      },
      {
        "id": "oai:arXiv.org:2503.15275v3",
        "title": "Challenges and Trends in Egocentric Vision: A Survey",
        "link": "https://arxiv.org/abs/2503.15275",
        "author": "Xiang Li, Heqian Qiu, Lanxiao Wang, Hanwen Zhang, Chenghao Qi, Linfeng Han, Huiyu Xiong, Hongliang Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15275v3 Announce Type: replace \nAbstract: With the rapid development of artificial intelligence technologies and wearable devices, egocentric vision understanding has emerged as a new and challenging research direction, gradually attracting widespread attention from both academia and industry. Egocentric vision captures visual and multimodal data through cameras or sensors worn on the human body, offering a unique perspective that simulates human visual experiences. This paper provides a comprehensive survey of the research on egocentric vision understanding, systematically analyzing the components of egocentric scenes and categorizing the tasks into four main areas: subject understanding, object understanding, environment understanding, and hybrid understanding. We explore in detail the sub-tasks within each category. We also summarize the main challenges and trends currently existing in the field. Furthermore, this paper presents an overview of high-quality egocentric vision datasets, offering valuable resources for future research. By summarizing the latest advancements, we anticipate the broad applications of egocentric vision technologies in fields such as augmented reality, virtual reality, and embodied intelligence, and propose future research directions based on the latest developments in the field."
      },
      {
        "id": "oai:arXiv.org:2503.17069v3",
        "title": "PVChat: Personalized Video Chat with One-Shot Learning",
        "link": "https://arxiv.org/abs/2503.17069",
        "author": "Yufei Shi, Weilong Yan, Gang Xu, Yumeng Li, Yucheng Chen, Zhenxi Li, Fei Richard Yu, Ming Li, Si Yong Yeo",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17069v3 Announce Type: replace \nAbstract: Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or \"Tom is discussing with Sarah\", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs."
      },
      {
        "id": "oai:arXiv.org:2503.17660v3",
        "title": "OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Intent Understanding",
        "link": "https://arxiv.org/abs/2503.17660",
        "author": "Kun Li, Jianhui Wang, Miao Zhang, Xueqian Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17660v3 Announce Type: replace \nAbstract: Generative AI has significantly advanced text-driven image generation, but it still faces challenges in producing outputs that consistently align with evolving user preferences and intents, particularly in multi-turn dialogue scenarios. In this research, We present a Visual Co-Adaptation (VCA) framework that incorporates human-in-the-loop feedback, utilizing a well-trained reward model specifically designed to closely align with human preferences. Using a diverse multi-turn dialogue dataset, the framework applies multiple reward functions (such as diversity, consistency, and preference feedback) to refine the diffusion model through LoRA, effectively optimizing image generation based on user input. We also constructed multi-round dialogue datasets with prompts and image pairs that well-fit user intent. Experiments show the model achieves 508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It also achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and excels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments demonstrate the effectiveness of the proposed method over state-of-the-art baselines, with significant improvements in image consistency and alignment with user intent."
      },
      {
        "id": "oai:arXiv.org:2503.17669v3",
        "title": "TDRI: Two-Phase Dialogue Refinement and Co-Adaptation for Interactive Image Generation",
        "link": "https://arxiv.org/abs/2503.17669",
        "author": "Yuheng Feng, Jianhui Wang, Kun Li, Sida Li, Tianyu Shi, Haoyue Han, Miao Zhang, Xueqian Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.17669v3 Announce Type: replace \nAbstract: Although text-to-image generation technologies have made significant advancements, they still face challenges when dealing with ambiguous prompts and aligning outputs with user intent.Our proposed framework, TDRI (Two-Phase Dialogue Refinement and Co-Adaptation), addresses these issues by enhancing image generation through iterative user interaction. It consists of two phases: the Initial Generation Phase, which creates base images based on user prompts, and the Interactive Refinement Phase, which integrates user feedback through three key modules. The Dialogue-to-Prompt (D2P) module ensures that user feedback is effectively transformed into actionable prompts, which improves the alignment between user intent and model input. By evaluating generated outputs against user expectations, the Feedback-Reflection (FR) module identifies discrepancies and facilitates improvements. In an effort to ensure consistently high-quality results, the Adaptive Optimization (AO) module fine-tunes the generation process by balancing user preferences and maintaining prompt fidelity. Experimental results show that TDRI outperforms existing methods by achieving 33.6% human preference, compared to 6.2% for GPT-4 augmentation, and the highest CLIP and BLIP alignment scores (0.338 and 0.336, respectively). In iterative feedback tasks, user satisfaction increased to 88% after 8 rounds, with diminishing returns beyond 6 rounds. Furthermore, TDRI has been found to reduce the number of iterations and improve personalization in the creation of fashion products. TDRI exhibits a strong potential for a wide range of applications in the creative and industrial domains, as it streamlines the creative process and improves alignment with user preferences"
      },
      {
        "id": "oai:arXiv.org:2503.18364v2",
        "title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark",
        "link": "https://arxiv.org/abs/2503.18364",
        "author": "Chenxi Xie, Minghan Li, Hui Zeng, Jun Luo, Lei Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18364v2 Announce Type: replace \nAbstract: High-resolution semantic segmentation is essential for applications such as image editing, bokeh imaging, AR/VR, etc. Unfortunately, existing datasets often have limited resolution and lack precise mask details and boundaries. In this work, we build a large-scale, matting-level semantic segmentation dataset, named MaSS13K, which consists of 13,348 real-world images, all at 4K resolution. MaSS13K provides high-quality mask annotations of a number of objects, which are categorized into seven categories: human, vegetation, ground, sky, water, building, and others. MaSS13K features precise masks, with an average mask complexity 20-50 times higher than existing semantic segmentation datasets. We consequently present a method specifically designed for high-resolution semantic segmentation, namely MaSSFormer, which employs an efficient pixel decoder that aggregates high-level semantic features and low-level texture features across three stages, aiming to produce high-resolution masks with minimal computational cost. Finally, we propose a new learning paradigm, which integrates the high-quality masks of the seven given categories with pseudo labels from new classes, enabling MaSSFormer to transfer its accurate segmentation capability to other classes of objects. Our proposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark together with 14 representative segmentation models. We expect that our meticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate the research of high-resolution and high-quality semantic segmentation. Datasets and codes can be found at https://github.com/xiechenxi99/MaSS13K."
      },
      {
        "id": "oai:arXiv.org:2503.23367v3",
        "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
        "link": "https://arxiv.org/abs/2503.23367",
        "author": "Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, Luca Benini",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23367v3 Announce Type: replace \nAbstract: Visual Autoregressive (VAR) modeling has gained popularity for its shift towards next-scale prediction. However, existing VAR paradigms process the entire token map at each scale step, leading to the complexity and runtime scaling dramatically with image resolution. To address this challenge, we propose FastVAR, a post-training acceleration method for efficient resolution scaling with VARs. Our key finding is that the majority of latency arises from the large-scale step where most tokens have already converged. Leveraging this observation, we develop the cached token pruning strategy that only forwards pivotal tokens for scale-specific modeling while using cached tokens from previous scale steps to restore the pruned slots. This significantly reduces the number of forwarded tokens and improves the efficiency at larger resolutions. Experiments show the proposed FastVAR can further speedup FlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop of <1%. We further extend FastVAR to zero-shot generation of higher resolution images. In particular, FastVAR can generate one 2K image with 15GB memory footprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at https://github.com/csguoh/FastVAR."
      },
      {
        "id": "oai:arXiv.org:2503.23519v2",
        "title": "BoundMatch: Boundary detection applied to semi-supervised segmentation for urban-driving scenes",
        "link": "https://arxiv.org/abs/2503.23519",
        "author": "Haruya Ishikawa, Yoshimitsu Aoki",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23519v2 Announce Type: replace \nAbstract: Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy annotation burden of dense pixel labeling by leveraging abundant unlabeled images alongside a small labeled set. While current consistency regularization methods achieve strong results, they often overlook a critical challenge: the precise delineation of object boundaries. In this paper, we propose BoundMatch, a novel multi-task SS-SS framework that explicitly integrates semantic boundary detection into a teacher-student consistency regularization pipeline. Our core mechanism, Boundary Consistency Regularized Multi-Task Learning (BCRM), enforces prediction agreement between teacher and student models on both segmentation masks and detailed semantic boundaries. To further enhance performance and sharpen boundaries, BoundMatch incorporates two lightweight fusion modules: Boundary-Semantic Fusion (BSF) injects learned boundary cues into the segmentation decoder, while Spatial Gradient Fusion (SGF) refines boundary predictions using mask gradients, leading to higher-quality boundary pseudo-labels. This framework is built upon SAMTH, a strong teacher-student baseline featuring a Harmonious Batch Normalization (HBN) update strategy for improved stability. Extensive experiments on diverse urban-driving scene datasets including Cityscapes, BDD100K, and SYNTHIA show that BoundMatch achieves competitive performance against current state-of-the-art methods. Our approach achieves state-of-the-art results on the new benchmark with DINOv2 foundation model. We further validate our approach's generalizability on Pascal VOC and ADE20K datasets. Ablation studies highlight BoundMatch's ability to improve boundary-specific evaluation metrics, its effectiveness in realistic large-scale unlabeled data scenarios, and applicability to lightweight architectures for mobile deployment."
      },
      {
        "id": "oai:arXiv.org:2504.01531v2",
        "title": "DRAN: A Distribution and Relation Adaptive Network for Spatio-temporal Forecasting",
        "link": "https://arxiv.org/abs/2504.01531",
        "author": "Xiaobei Zou, Luolin Xiong, Kexuan Zhang, Cesare Alippi, Yang Tang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01531v2 Announce Type: replace \nAbstract: Accurate predictions of spatio-temporal systems are crucial for tasks such as system management, control, and crisis prevention. However, the inherent time variance of many spatio-temporal systems poses challenges to achieving accurate predictions whenever stationarity is not granted. In order to address non-stationarity, we propose a Distribution and Relation Adaptive Network (DRAN) capable of dynamically adapting to relation and distribution changes over time. While temporal normalization and de-normalization are frequently used techniques to adapt to distribution shifts, this operation is not suitable for the spatio-temporal context as temporal normalization scales the time series of nodes and possibly disrupts the spatial relations among nodes. In order to address this problem, a Spatial Factor Learner (SFL) module is developed that enables the normalization and de-normalization process. To adapt to dynamic changes in spatial relationships among sensors, we propose a Dynamic-Static Fusion Learner (DSFL) module that effectively integrates features learned from both dynamic and static relations through an adaptive fusion ratio mechanism. Furthermore, we introduce a Stochastic Learner to capture the noisy components of spatio-temporal representations. Our approach outperforms state-of-the-art methods on weather prediction and traffic flow forecasting tasks.Experimental results show that our SFL efficiently preserves spatial relationships across various temporal normalization operations. Visualizations of the learned dynamic and static relations demonstrate that DSFL can capture both local and distant relationships between nodes."
      },
      {
        "id": "oai:arXiv.org:2504.01931v4",
        "title": "On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows",
        "link": "https://arxiv.org/abs/2504.01931",
        "author": "Souradip Chakraborty, Mohammadreza Pourreza, Ruoxi Sun, Yiwen Song, Nino Scherrer, Furong Huang, Amrit Singh Bedi, Ahmad Beirami, Jindong Gu, Hamid Palangi, Tomas Pfister",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01931v4 Announce Type: replace \nAbstract: Agentic AI workflows (systems that autonomously plan and act) are becoming widespread, yet their task success rate on complex tasks remains low. A promising solution is inference-time alignment, which uses extra compute at test time to improve performance. Inference-time alignment relies on three components: sampling, evaluation, and feedback. While most prior work studies sampling and automatic evaluation, feedback remains underexplored. To study the role of feedback, we introduce Iterative Agent Decoding (IAD), a procedure that repeatedly inserts feedback extracted from different forms of critiques (reward models or AI-generated textual feedback) between decoding steps. Through IAD, we analyze feedback along four dimensions: (1) its role in the accuracy-compute trade-offs with limited inference budget, (2) quantifying the gains over diversity-only baselines such as best-of-N sampling, (3) effectiveness of composing feedback from reward models versus textual critique, and (4) robustness to noisy or low-quality feedback. Across Sketch2Code, Text2SQL, Intercode, and WebShop, we show that IAD with proper integration of high fidelity feedback leads to consistent gains up to 10 percent absolute performance improvement over various baselines such as best-of-N. Our findings underscore feedback as a crucial knob for inference-time alignment of agentic AI workflows with limited inference budget."
      },
      {
        "id": "oai:arXiv.org:2504.06048v4",
        "title": "Trust-Region Twisted Policy Improvement",
        "link": "https://arxiv.org/abs/2504.06048",
        "author": "Joery A. de Vries, Jinke He, Yaniv Oren, Matthijs T. J. Spaan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.06048v4 Announce Type: replace \nAbstract: Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep reinforcement learning (RL). However, scaling MCTS to parallel compute has proven challenging in practice which has motivated alternative planners like sequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters for smoothing through a reformulation of RL as a policy inference problem. Yet, persisting design choices of these particle filters often conflict with the aim of online planning in RL, which is to obtain a policy improvement at the start of planning. Drawing inspiration from MCTS, we tailor SMC planners specifically for RL by improving data generation within the planner through constrained action sampling and explicit terminal state handling, as well as improving policy and value target estimation. This leads to our Trust-Region Twisted SMC (TRT-SMC), which shows improved runtime and sample-efficiency over baseline MCTS and SMC methods in both discrete and continuous domains."
      },
      {
        "id": "oai:arXiv.org:2504.07096v2",
        "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens",
        "link": "https://arxiv.org/abs/2504.07096",
        "author": "Jiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen, Arnavi Chheda-Kothary, Huy Tran, Byron Bischoff, Eric Marsh, Michael Schmitz, Cassidy Trier, Aaron Sarnat, Jenna James, Jon Borchardt, Bailey Kuehl, Evie Cheng, Karen Farley, Sruthi Sreeram, Taira Anderson, David Albright, Carissa Schoenick, Luca Soldaini, Dirk Groeneveld, Rock Yuren Pang, Pang Wei Koh, Noah A. Smith, Sophie Lebrecht, Yejin Choi, Hannaneh Hajishirzi, Ali Farhadi, Jesse Dodge",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.07096v2 Announce Type: replace \nAbstract: We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source."
      },
      {
        "id": "oai:arXiv.org:2504.11150v2",
        "title": "GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention",
        "link": "https://arxiv.org/abs/2504.11150",
        "author": "Mahir Gulzar, Yar Muhammad, Naveed Muhammad",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.11150v2 Announce Type: replace \nAbstract: Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results."
      },
      {
        "id": "oai:arXiv.org:2504.11364v3",
        "title": "Offline Learning and Forgetting for Reasoning with Large Language Models",
        "link": "https://arxiv.org/abs/2504.11364",
        "author": "Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.11364v3 Announce Type: replace \nAbstract: Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it on unpaired successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. A key challenge we identify is that naive fine-tuning can degrade the model's search capability; we show this can be mitigated with a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown reasoning benchmarks show that, replacing CoT-generated data with search-generated data for offline fine-tuning improves success rates by around 23% over inference-time search baselines, while reducing inference time by 180$\\times$. On top of this, our learning and forgetting objective consistently outperforms both supervised fine-tuning and preference-based methods."
      },
      {
        "id": "oai:arXiv.org:2504.14569v2",
        "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models",
        "link": "https://arxiv.org/abs/2504.14569",
        "author": "Lawrence Liu, Inesh Chakrabarti, Yixiao Li, Mengdi Wang, Tuo Zhao, Lin F. Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.14569v2 Announce Type: replace \nAbstract: Large language models (LLMs) exhibit remarkable performance across various natural language processing tasks but suffer from immense computational and memory demands, limiting their deployment in resource-constrained environments. To address this challenge, we propose NoWag: (Normalized Weight and Activation Guided Compression), a unified framework for zero-shot shape preserving compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB models, using two popular forms of shape-preserving compression, vector quantization NoWag-VQ (NoWag for Vector Quantization), and unstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that NoWag-P performs competitively against state-of-the-art methods. These results suggest commonalities between these compression paradigms that could inspire future work. Our code is available at https://github.com/LawrenceRLiu/NoWag"
      },
      {
        "id": "oai:arXiv.org:2504.19600v2",
        "title": "Heat Diffusion Models -- Interpixel Attention Mechanism",
        "link": "https://arxiv.org/abs/2504.19600",
        "author": "Pengfei Zhang, Shouqing Jia",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.19600v2 Announce Type: replace \nAbstract: Denoising Diffusion Probabilistic Models (DDPM) process images as a whole. Since adjacent pixels are highly likely to belong to the same object, we propose the Heat Diffusion Model (HDM) to further preserve image details and generate more realistic images. HDM essentially is a DDPM that incorporates an attention mechanism between pixels. In HDM, the discrete form of the two-dimensional heat equation is integrated into the diffusion and generation formulas of DDPM, enabling the model to compute relationships between neighboring pixels during image processing. Our experiments demonstrate that HDM can generate higher-quality samples compared to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN)."
      },
      {
        "id": "oai:arXiv.org:2504.21771v2",
        "title": "Anatomical Similarity as a New Metric to Evaluate Brain Generative Models",
        "link": "https://arxiv.org/abs/2504.21771",
        "author": "Bahram Jafrasteh, Wei Peng, Cheng Wan, Yimin Luo, Ehsan Adeli, Qingyu Zhao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.21771v2 Announce Type: replace \nAbstract: Generative models enhance neuroimaging through data augmentation, quality improvement, and rare condition studies. Despite advances in realistic synthetic MRIs, evaluations focus on texture and perception, lacking sensitivity to crucial anatomical fidelity. This study proposes a new metric, called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the anatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg}, a deep learning-based brain parcellation tool, to derive volumetric measures of brain regions in each MRI and uses the multivariate Wasserstein distance to compare distributions between real and synthetic anatomies. Based on controlled experiments on two real datasets and synthetic MRIs from five generative models, WASABI demonstrates higher sensitivity in quantifying anatomical discrepancies compared to traditional image-level metrics, even when synthetic images achieve near-perfect visual quality. Our findings advocate for shifting the evaluation paradigm beyond visual inspection and conventional metrics, emphasizing anatomical fidelity as a crucial benchmark for clinically meaningful brain MRI synthesis. Our code is available at https://github.com/BahramJafrasteh/wasabi-mri."
      },
      {
        "id": "oai:arXiv.org:2505.00503v3",
        "title": "Variational OOD State Correction for Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2505.00503",
        "author": "Ke Jiang, Wen Jiang, Xiaoyang Tan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.00503v3 Announce Type: replace \nAbstract: The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites."
      },
      {
        "id": "oai:arXiv.org:2505.04531v2",
        "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review",
        "link": "https://arxiv.org/abs/2505.04531",
        "author": "Josh McGiff, Nikola S. Nikolov",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.04531v2 Announce Type: replace \nAbstract: Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini. While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English. This has amplified concerns over linguistic inequality in natural language processing (NLP). This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL). Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks. We also analyse trends in architecture choices, language family representation, and evaluation methods. Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies. We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems. Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies."
      },
      {
        "id": "oai:arXiv.org:2505.04649v2",
        "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights",
        "link": "https://arxiv.org/abs/2505.04649",
        "author": "Chengzhang Yu, Yiming Zhang, Zhixin Liu, Zenghui Ding, Yining Sun, Zhanpeng Jin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.04649v2 Announce Type: replace \nAbstract: The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards."
      },
      {
        "id": "oai:arXiv.org:2505.05599v3",
        "title": "Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling",
        "link": "https://arxiv.org/abs/2505.05599",
        "author": "Seraj Al Mahmud Mostafa, Chenxi Wang, Jia Yue, Yuta Hozumi, Jianwu Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.05599v3 Announce Type: replace \nAbstract: Object localization in satellite imagery is particularly challenging due to the high variability of objects, low spatial resolution, and interference from noise and dominant features such as clouds and city lights. In this research, we focus on three satellite datasets: upper atmospheric Gravity Waves (GW), mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique challenges. These challenges include the variability in the scale and appearance of the main object patterns, where the size, shape, and feature extent of objects of interest can differ significantly. To address these challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed to improve object localization in these complex scenarios. YOLO-DCAP incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture multi-scale features at scale with varying dilation rates, and an Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant spatial regions, enhancing feature selection. These structural improvements help to better localize objects in satellite imagery. Experimental results demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model and state-of-the-art approaches, achieving an average improvement of 20.95% in mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively over state-of-the-art alternatives, consistently across all three satellite datasets. These consistent gains across all three satellite datasets highlight the robustness and generalizability of the proposed approach. Our code is open sourced at https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization."
      },
      {
        "id": "oai:arXiv.org:2505.10482v3",
        "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps",
        "link": "https://arxiv.org/abs/2505.10482",
        "author": "Ningyuan Yang, Jiaxuan Gao, Feng Gao, Yi Wu, Chao Yu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.10482v3 Announce Type: replace \nAbstract: Diffusion policies, widely adopted in decision-making scenarios such as robotics, gaming and autonomous driving, are capable of learning diverse skills from demonstration data due to their high representation power. However, the sub-optimal and limited coverage of demonstration data could lead to diffusion policies that generate sub-optimal trajectories and even catastrophic failures. While reinforcement learning (RL)-based fine-tuning has emerged as a promising solution to address these limitations, existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This challenge stems from the computational intractability of action likelihood estimation during the denoising process, which leads to complicated optimization objectives. In our experiments starting from randomly initialized policies, we find that online tuning of Diffusion Policies demonstrates much lower sample efficiency compared to directly applying PPO on MLP policies (MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps. Our experiments demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy."
      },
      {
        "id": "oai:arXiv.org:2505.11211v2",
        "title": "Bayesian Hierarchical Invariant Prediction",
        "link": "https://arxiv.org/abs/2505.11211",
        "author": "Francisco Madaleno, Pernille Julie Viuff Sand, Francisco C. Pereira, Sergio Hernan Garrido Mejia",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.11211v2 Announce Type: replace \nAbstract: We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We leverage the hierarchical structure to explicitly test invariance of causal mechanisms under heterogeneous data, resulting in improved computational scalability for a larger number of predictors compared to ICP. Moreover, given its Bayesian nature BHIP enables the use of prior information. In this paper, we test two sparsity inducing priors: horseshoe and spike-and-slab, both of which allow us a more reliable identification of causal features. We test BHIP in synthetic and real-world data showing its potential as an alternative inference method to ICP."
      },
      {
        "id": "oai:arXiv.org:2505.12182v3",
        "title": "Truth Neurons",
        "link": "https://arxiv.org/abs/2505.12182",
        "author": "Haohang Li, Yupeng Cao, Yangyang Yu, Jordan W. Suchow, Zining Zhu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.12182v3 Announce Type: replace \nAbstract: Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability."
      },
      {
        "id": "oai:arXiv.org:2505.15634v3",
        "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models",
        "link": "https://arxiv.org/abs/2505.15634",
        "author": "Zihao Li, Xu Wang, Yuzhe Yang, Ziyu Yao, Haoyi Xiong, Mengnan Du",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.15634v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs."
      },
      {
        "id": "oai:arXiv.org:2505.17670v2",
        "title": "Towards General Continuous Memory for Vision-Language Models",
        "link": "https://arxiv.org/abs/2505.17670",
        "author": "Wenyi Wu, Zixuan Song, Kun Zhou, Yifei Shao, Zhiting Hu, Biwei Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.17670v2 Announce Type: replace \nAbstract: Language models (LMs) and their extension, vision-language models (VLMs), have achieved remarkable performance across various tasks. However, they still struggle with complex reasoning tasks that require multimodal or multilingual real-world knowledge. To support such capabilities, an external memory system that can efficiently provide relevant multimodal information is essential. Existing approaches generally concatenate image and text tokens into a long sequence as memory, which, however, may drastically increase context length and even degrade performance. In contrast, we propose using continuous memory, a compact set of dense embeddings to more effectively and efficiently represent multimodal and multilingual knowledge. Our key insight is that a VLM can serve as its own continuous memory encoder. We empirically show that this design improves performance on complex multimodal reasoning tasks. Building on this, we introduce a data-efficient and parameter-efficient method to fine-tune the VLM into a memory encoder, requiring only 1.2% of the model's parameters and a small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes VLM's original capabilities to encode arbitrary multimodal and multilingual knowledge into just 8 continuous embeddings. Since the inference-time VLM remains frozen, our memory module is plug-and-play and can be flexibly integrated as needed. Extensive experiments across eight multimodal reasoning benchmarks demonstrate the effectiveness of our approach."
      },
      {
        "id": "oai:arXiv.org:2505.21381v5",
        "title": "ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding",
        "link": "https://arxiv.org/abs/2505.21381",
        "author": "Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.21381v5 Announce Type: replace \nAbstract: State Space models (SSMs) such as PointMamba enable efficient feature extraction for point cloud self-supervised learning with linear complexity, outperforming Transformers in computational efficiency. However, existing PointMamba-based methods depend on complex token ordering and random masking, which disrupt spatial continuity and local semantic correlations. We propose ZigzagPointMamba to tackle these challenges. The core of our approach is a simple zigzag scan path that globally sequences point cloud tokens, enhancing spatial continuity by preserving the proximity of spatially adjacent point tokens. Nevertheless, random masking undermines local semantic modeling in self-supervised learning. To address this, we introduce a Semantic-Siamese Masking Strategy (SMS), which masks semantically similar tokens to facilitate reconstruction by integrating local features of original and similar tokens. This overcomes the dependence on isolated local features and enables robust global semantic modeling. Our pre-trained ZigzagPointMamba weights significantly improve downstream tasks, achieving a 1.59% mIoU gain on ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of ScanObjectNN."
      },
      {
        "id": "oai:arXiv.org:2505.23102v2",
        "title": "CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing",
        "link": "https://arxiv.org/abs/2505.23102",
        "author": "Yuka Ogino, Takahiro Toizumi, Atsushi Ito",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23102v2 Announce Type: replace \nAbstract: Low-Light Image Enhancement (LLIE) is crucial for improving both human perception and computer vision tasks. This paper addresses two challenges in zero-reference LLIE: obtaining perceptually 'good' images using the Contrastive Language-Image Pre-Training (CLIP) model and maintaining computational efficiency for high-resolution images. We propose CLIP-Utilized Reinforcement learning-based Visual image Enhancement (CURVE). CURVE employs a simple image processing module which adjusts global image tone based on B\\'ezier curve and estimates its processing parameters iteratively. The estimator is trained by reinforcement learning with rewards designed using CLIP text embeddings. Experiments on low-light and multi-exposure datasets demonstrate the performance of CURVE in terms of enhancement quality and processing speed compared to conventional methods."
      },
      {
        "id": "oai:arXiv.org:2505.23404v3",
        "title": "MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models",
        "link": "https://arxiv.org/abs/2505.23404",
        "author": "Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin, Fei Gao, Wenmin Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.23404v3 Announce Type: replace \nAbstract: Recent advancements in adversarial jailbreak attacks have revealed significant vulnerabilities in Large Language Models (LLMs), facilitating the evasion of alignment safeguards through increasingly sophisticated prompt manipulations. In this paper, we propose MEF, a capability-aware multi-encryption framework for evaluating vulnerabilities in black-box LLMs. Our key insight is that the effectiveness of jailbreak strategies can be significantly enhanced by tailoring them to the semantic comprehension capabilities of the target model. We present a typology that classifies LLMs into Type I and Type II based on their comprehension levels, and design adaptive attack strategies for each. MEF combines layered semantic mutations and dual-ended encryption techniques, enabling circumvention of input, inference, and output-level defenses. Experimental results demonstrate the superiority of our approach. Remarkably, it achieves a jailbreak success rate of 98.9\\% on GPT-4o (29 May 2025 release). Our findings reveal vulnerabilities in current LLMs' alignment defenses."
      },
      {
        "id": "oai:arXiv.org:2506.00854v3",
        "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG",
        "link": "https://arxiv.org/abs/2506.00854",
        "author": "Jacky Tai-Yu Lu, Jung Chiang, Chi-Sheng Chen, Anna Nai-Yun Tung, Hsiang Wei Hu, Yuan Chiao Cheng",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00854v3 Announce Type: replace \nAbstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese."
      },
      {
        "id": "oai:arXiv.org:2506.01975v2",
        "title": "An empirical study of task and feature correlations in the reuse of pre-trained models",
        "link": "https://arxiv.org/abs/2506.01975",
        "author": "Jama Hussein Mohamud, Willie Brink",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.01975v2 Announce Type: replace \nAbstract: Pre-trained neural networks are commonly used and reused in the machine learning community. Alice trains a model for a particular task, and a part of her neural network is reused by Bob for a different task, often to great effect. To what can we ascribe Bob's success? This paper introduces an experimental setup through which factors contributing to Bob's empirical success could be studied in silico. As a result, we demonstrate that Bob might just be lucky: his task accuracy increases monotonically with the correlation between his task and Alice's. Even when Bob has provably uncorrelated tasks and input features from Alice's pre-trained network, he can achieve significantly better than random performance due to Alice's choice of network and optimizer. When there is little correlation between tasks, only reusing lower pre-trained layers is preferable, and we hypothesize the converse: that the optimal number of retrained layers is indicative of task and feature correlation. Finally, we show in controlled real-world scenarios that Bob can effectively reuse Alice's pre-trained network if there are semantic correlations between his and Alice's task."
      },
      {
        "id": "oai:arXiv.org:2506.03861v2",
        "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading",
        "link": "https://arxiv.org/abs/2506.03861",
        "author": "Qiuhan Han, Qian Wang, Atsushi Yoshikawa, Masayuki Yamamura",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.03861v2 Announce Type: replace \nAbstract: High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding rapid decision-making. Social media platforms like Reddit offer valuable, yet underexplored, information for such high-frequency, short-term trading. This paper introduces \\textbf{PulseReddit}, a novel dataset that is the first to align large-scale Reddit discussion data with high-frequency cryptocurrency market statistics for short-term trading analysis. We conduct an extensive empirical study using Large Language Model (LLM)-based Multi-Agent Systems (MAS) to investigate the impact of social sentiment from PulseReddit on trading performance. Our experiments conclude that MAS augmented with PulseReddit data achieve superior trading outcomes compared to traditional baselines, particularly in bull markets, and demonstrate robust adaptability across different market regimes. Furthermore, our research provides conclusive insights into the performance-efficiency trade-offs of different LLMs, detailing significant considerations for practical model selection in HFT applications. PulseReddit and our findings establish a foundation for advanced MAS research in HFT, demonstrating the tangible benefits of integrating social media."
      },
      {
        "id": "oai:arXiv.org:2506.04682v3",
        "title": "MARS: Radio Map Super-resolution and Reconstruction Method under Sparse Channel Measurements",
        "link": "https://arxiv.org/abs/2506.04682",
        "author": "Chuyun Deng, Na Liu, Wei Xie, Lianming Xu, Li Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.04682v3 Announce Type: replace \nAbstract: Radio maps reflect the spatial distribution of signal strength and are essential for applications like smart cities, IoT, and wireless network planning. However, reconstructing accurate radio maps from sparse measurements remains challenging. Traditional interpolation and inpainting methods lack environmental awareness, while many deep learning approaches depend on detailed scene data, limiting generalization. To address this, we propose MARS, a Multi-scale Aware Radiomap Super-resolution method that combines CNNs and Transformers with multi-scale feature fusion and residual connections. MARS focuses on both global and local feature extraction, enhancing feature representation across different receptive fields and improving reconstruction accuracy. Experiments across different scenes and antenna locations show that MARS outperforms baseline models in both MSE and SSIM, while maintaining low computational cost, demonstrating strong practical potential."
      },
      {
        "id": "oai:arXiv.org:2506.05752v2",
        "title": "Integrating Spatiotemporal Features in LSTM for Spatially Informed COVID-19 Hospitalization Forecasting",
        "link": "https://arxiv.org/abs/2506.05752",
        "author": "Zhongying Wang, Thoai D. Ngo, Hamidreza Zoraghein, Benjamin Lucas, Morteza Karimzadeh",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.05752v2 Announce Type: replace \nAbstract: The COVID-19 pandemic's severe impact highlighted the need for accurate and timely hospitalization forecasting to support effective healthcare planning. However, most forecasting models struggled, particularly during variant surges, when they were most needed. This study introduces a novel parallel-stream Long Short-Term Memory (LSTM) framework to forecast daily state-level incident hospitalizations in the United States. Our framework incorporates a spatiotemporal feature, Social Proximity to Hospitalizations (SPH), derived from Meta's Social Connectedness Index, to improve forecasts. SPH serves as a proxy for interstate population interaction, capturing transmission dynamics across space and time. Our architecture captures both short- and long-term temporal dependencies, and a multi-horizon ensembling strategy balances forecasting consistency and error. An evaluation against the COVID-19 Forecast Hub ensemble models during the Delta and Omicron surges reveals the superiority of our model. On average, our model surpasses the ensemble by 27, 42, 54, and 69 hospitalizations per state at the 7-, 14-, 21-, and 28-day horizons, respectively, during the Omicron surge. Data-ablation experiments confirm SPH's predictive power, highlighting its effectiveness in enhancing forecasting models. This research not only advances hospitalization forecasting but also underscores the significance of spatiotemporal features, such as SPH, in modeling the complex dynamics of infectious disease spread."
      },
      {
        "id": "oai:arXiv.org:2506.06232v2",
        "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study",
        "link": "https://arxiv.org/abs/2506.06232",
        "author": "Leon Mayer, Tim R\\\"adsch, Dominik Michael, Lucas Luttner, Amine Yamlahi, Evangelia Christodoulou, Patrick Godau, Marcel Knopp, Annika Reinke, Fiona Kolbinger, Lena Maier-Hein",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06232v2 Announce Type: replace \nAbstract: While traditional computer vision models have historically struggled to generalize to endoscopic domains, the emergence of foundation models has shown promising cross-domain performance. In this work, we present the first large-scale study assessing the capabilities of Vision Language Models (VLMs) for endoscopic tasks with a specific focus on laparoscopic surgery. Using a diverse set of state-of-the-art models, multiple surgical datasets, and extensive human reference annotations, we address three key research questions: (1) Can current VLMs solve basic perception tasks on surgical images? (2) Can they handle advanced frame-based endoscopic scene understanding tasks? and (3) How do specialized medical VLMs compare to generalist models in this context? Our results reveal that VLMs can effectively perform basic surgical perception tasks, such as object counting and localization, with performance levels comparable to general domain tasks. However, their performance deteriorates significantly when the tasks require medical knowledge. Notably, we find that specialized medical VLMs currently underperform compared to generalist models across both basic and advanced surgical tasks, suggesting that they are not yet optimized for the complexity of surgical environments. These findings highlight the need for further advancements to enable VLMs to handle the unique challenges posed by surgery. Overall, our work provides important insights for the development of next-generation endoscopic AI systems and identifies key areas for improvement in medical visual language models."
      },
      {
        "id": "oai:arXiv.org:2506.08938v2",
        "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2506.08938",
        "author": "Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, Jinsong Su",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.08938v2 Announce Type: replace \nAbstract: Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM`s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model`s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model`s parametric knowledge, which undermines the model`s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model`s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https://github.com/DeepLearnXMU/Faithful-RAG"
      },
      {
        "id": "oai:arXiv.org:2506.09016v2",
        "title": "SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning",
        "link": "https://arxiv.org/abs/2506.09016",
        "author": "Ruiqi Zhang, Daman Arora, Song Mei, Andrea Zanette",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09016v2 Announce Type: replace \nAbstract: Training large language models with reinforcement learning (RL) against verifiable rewards significantly enhances their reasoning abilities, yet remains computationally expensive due to inefficient uniform prompt sampling. We introduce Selective Prompting with Efficient Estimation of Difficulty (SPEED), an adaptive online RL curriculum that selectively chooses training examples of intermediate difficulty to maximize learning efficiency. Theoretically, we establish that intermediate-difficulty prompts improve the gradient estimator's signal-to-noise ratio, accelerating convergence. Empirically, our efficient implementation leads to 2x to 6x faster training without degrading accuracy, requires no manual tuning, and integrates seamlessly into standard RL algorithms."
      },
      {
        "id": "oai:arXiv.org:2506.09594v2",
        "title": "Accelerating Large-Scale Regularized High-Order Tensor Recovery",
        "link": "https://arxiv.org/abs/2506.09594",
        "author": "Wenjin Qin, Hailin Wang, Jingyao Hou, Jianjun Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.09594v2 Announce Type: replace \nAbstract: Currently, existing tensor recovery methods fail to recognize the impact of tensor scale variations on their structural characteristics. Furthermore, existing studies face prohibitive computational costs when dealing with large-scale high-order tensor data. To alleviate these issue, assisted by the Krylov subspace iteration, block Lanczos bidiagonalization process, and random projection strategies, this article first devises two fast and accurate randomized algorithms for low-rank tensor approximation (LRTA) problem. Theoretical bounds on the accuracy of the approximation error estimate are established. Next, we develop a novel generalized nonconvex modeling framework tailored to large-scale tensor recovery, in which a new regularization paradigm is exploited to achieve insightful prior representation for large-scale tensors. On the basis of the above, we further investigate new unified nonconvex models and efficient optimization algorithms, respectively, for several typical high-order tensor recovery tasks in unquantized and quantized situations. To render the proposed algorithms practical and efficient for large-scale tensor data, the proposed randomized LRTA schemes are integrated into their central and time-intensive computations. Finally, we conduct extensive experiments on various large-scale tensors, whose results demonstrate the practicability, effectiveness and superiority of the proposed method in comparison with some state-of-the-art approaches."
      },
      {
        "id": "oai:arXiv.org:2506.12025v3",
        "title": "Unsupervised Learning for Optimal Transport plan prediction between unbalanced graphs",
        "link": "https://arxiv.org/abs/2506.12025",
        "author": "Sonia Mazelet, R\\'emi Flamary, Bertrand Thirion",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.12025v3 Announce Type: replace \nAbstract: Optimal transport between graphs, based on Gromov-Wasserstein and other extensions, is a powerful tool for comparing and aligning graph structures. However, solving the associated non-convex optimization problems is computationally expensive, which limits the scalability of these methods to large graphs. In this work, we present Unbalanced Learning of Optimal Transport (ULOT), a deep learning method that predicts optimal transport plans between two graphs. Our method is trained by minimizing the fused unbalanced Gromov-Wasserstein (FUGW) loss. We propose a novel neural architecture with cross-attention that is conditioned on the FUGW tradeoff hyperparameters. We evaluate ULOT on synthetic stochastic block model (SBM) graphs and on real cortical surface data obtained from fMRI. ULOT predicts transport plans with competitive loss up to two orders of magnitude faster than classical solvers. Furthermore, the predicted plan can be used as a warm start for classical solvers to accelerate their convergence. Finally, the predicted transport plan is fully differentiable with respect to the graph inputs and FUGW hyperparameters, enabling the optimization of functionals of the ULOT plan."
      },
      {
        "id": "oai:arXiv.org:2506.12229v2",
        "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index",
        "link": "https://arxiv.org/abs/2506.12229",
        "author": "Hao Xu, Jiacheng Liu, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.12229v2 Announce Type: replace \nAbstract: Language models are trained mainly on massive text data from the Internet, and it becomes increasingly important to understand this data source. Exact-match search engines enable searching in large text corpora -- counting string appearances and retrieving the enclosing documents -- yet the high storage overhead hinders their application on Internet-scale data. We present Infini-gram mini, an efficient and scalable system that can make petabyte-level text corpora searchable. Based on the FM-index data structure (Ferragina and Manzini, 2000), which simultaneously indexes and compresses text, our system creates indexes with size only 44% of the corpus. Infini-gram mini greatly improves upon the best existing implementation of FM-index in terms of indexing speed (18$\\times$) and memory use during both indexing (3.2$\\times$ reduction) and querying (down to a negligible amount). We index 46TB of Internet text in 50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes). We show one important use case of Infini-gram mini in a large-scale analysis of benchmark contamination. We find several core LM evaluation benchmarks to be heavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead to overestimating the capabilities of language models if trained on such data. We host a benchmark contamination bulletin to share the contamination rate of many core and community-contributed benchmarks. We also release a web interface and an API endpoint to serve general search queries on Infini-gram mini indexes."
      },
      {
        "id": "oai:arXiv.org:2506.13734v2",
        "title": "Instruction Following by Boosting Attention of Large Language Models",
        "link": "https://arxiv.org/abs/2506.13734",
        "author": "Vitoria Guardieiro, Adam Stein, Avishree Khare, Eric Wong",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.13734v2 Announce Type: replace \nAbstract: Controlling the generation of large language models (LLMs) remains a central challenge to ensure their safe and reliable deployment. While prompt engineering and finetuning are common approaches, recent work has explored latent steering, a lightweight technique that alters LLM internal activations to guide generation. However, subsequent studies revealed latent steering's effectiveness to be limited, often underperforming simple instruction prompting. To address this limitation, we first establish a benchmark across diverse behaviors for standardized evaluation of steering techniques. Building on insights from this benchmark, we introduce Instruction Attention Boosting (InstABoost), a latent steering method that boosts the strength of instruction prompting by altering the model's attention during generation. InstABoost combines the strengths of existing approaches and is theoretically supported by prior work that suggests that in-context rule following in transformer-based models can be controlled by manipulating attention on instructions. Empirically, InstABoost demonstrates superior control success compared to both traditional prompting and latent steering."
      },
      {
        "id": "oai:arXiv.org:2506.15817v2",
        "title": "Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions",
        "link": "https://arxiv.org/abs/2506.15817",
        "author": "Jason Tandiary",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.15817v2 Announce Type: replace \nAbstract: This paper studies Vickrey first-price auctions under binary feedback. Leveraging the enhanced performance of machine learning algorithms, the new algorithm uses past information to improve the regret bounds of the BROAD-OMD algorithm. Motivated by the growing relevance of first-price auctions and the predictive capabilities of machine learning models, this paper proposes a new algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages predictions of the highest competing bid. This paper's main contribution is an algorithm that achieves zero regret under accurate predictions. Additionally, a bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain normality conditions."
      },
      {
        "id": "oai:arXiv.org:2506.19652v2",
        "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager",
        "link": "https://arxiv.org/abs/2506.19652",
        "author": "Lucie Galland, Catherine Pelachaud, Florian Pecune",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.19652v2 Announce Type: replace \nAbstract: In this work, we propose a novel framework that integrates large language models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a specific goal. By leveraging hierarchical reinforcement learning to model the structured phases of dialogue and employ meta-learning to enhance adaptability across diverse user profiles, our approach enhances adaptability and efficiency, enabling the system to learn from limited data, transition fluidly between dialogue phases, and personalize responses to heterogeneous patient needs. We apply our framework to Motivational Interviews, aiming to foster behavior change, and demonstrate that the proposed dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward, showing a potential benefit of conditioning LLMs to create open-ended dialogue systems with specific goals."
      },
      {
        "id": "oai:arXiv.org:2506.20214v2",
        "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation",
        "link": "https://arxiv.org/abs/2506.20214",
        "author": "Yanzhe Chen (Yen-chieh Chan), Huasong Zhong, Yan Li, Zhenheng Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.20214v2 Announce Type: replace \nAbstract: Unified multimodal large language models (MLLMs) have shown promise in jointly advancing multimodal understanding and generation, with visual codebooks discretizing images into tokens for autoregressive modeling. Existing codebook-based methods either rely on small vocabularies (~16K entries) that lack fine-grained semantics or naively scale up, resulting in low token utilization and unstable training. We propose UniCode$^2$, a cascaded codebook framework enabling large-scale, semantically aligned, and stable visual tokenization. By clustering millions of SigLIP sequence embeddings, we build a 500K-entry codebook that preserves vision-language alignment while expanding capacity. Stability is ensured via a cascaded design: a frozen codebook anchors the embedding space, and a trainable codebook refines task-specific semantics. This decoupling promotes high utilization and robust learning. Moreover, the alignment of our visual tokens with textual semantics enables seamless integration with pretrained diffusion decoders, supporting high-quality visual synthesis with minimal adaptation. UniCode^2 delivers strong performance across diverse benchmarks, demonstrating the viability of scaling visual token spaces without sacrificing stability, semantics, or modularity."
      },
      {
        "id": "oai:arXiv.org:2506.21116v2",
        "title": "IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes",
        "link": "https://arxiv.org/abs/2506.21116",
        "author": "Yujia Liang, Jile Jiao, Xuetao Feng, Zixuan Ye, Yuan Wang, Zhicheng Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.21116v2 Announce Type: replace \nAbstract: Video Large Language Models (VideoLLMs) have demonstrated remarkable understanding capabilities, but are found struggling to tackle multi-shot scenarios,e.g., video clips with varying camera angles or scene changes. This challenge can render failures such as instance identity forgetting and key frame negligence. In this work, we first attribute the challenge to the lack of multi-shot annotations among existing datasets and therefore we introduce a new dataset termed MultiClip-Bench, featuring dense descriptions and instruction-based question-answering pairs tailored for multi-shot scenarios. We empirically find that the training set significantly boosts the multi-shot performance, while the testing benchmark provides a reliable measure of the model capability in multi-shot scenarios. By further analyzing and discovering that current models only encode instance features in a discrete or lossy manner, at the risk of missing identity information, we then contribute a new model IPFormer-VideoLLM. Its key idea is the injection of instance-level features as instance prompts through an efficient attention-based connector. This allows for the aggregation of instance-specific information across scenes. Experiments demonstrate that our proposed dataset and model not only enhance the multi-scene video understanding significantly, but also offer distinct advantages across various video benchmarks."
      },
      {
        "id": "oai:arXiv.org:2506.21940v2",
        "title": "GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus",
        "link": "https://arxiv.org/abs/2506.21940",
        "author": "Marwan Ait Haddou, Mohamed Bennai",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.21940v2 Announce Type: replace \nAbstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum advantage but face challenges from barren plateaus, where gradients vanish, and poorly conditioned optimization landscapes. We introduce GuiderNet, a meta-learning framework that conditions Parameterized Quantum Circuits (PQCs) using data-dependent parameter shifts aimed at minimizing the log condition number of the Fubini-Study metric tensor. Implemented as a classical neural network, GuiderNet is meta-trained to guide PQC parameters into geometrically favorable regions and is embedded within hybrid quantum-classical pipelines to steer both initialization and adaptive modulation during training.\n  Applied to the Kaggle Diabetes classification task, GuiderNet reduces cumulative training loss by over 5x, improves test accuracy from 75.3% to 98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also suppresses gradient explosion and stabilizes parameter updates, enabling smoother and more robust optimization. These results demonstrate that geometric meta-conditioning can mitigate barren plateaus and ill-conditioning, providing a scalable approach to enhance trainability and generalization in quantum machine learning."
      },
      {
        "id": "oai:arXiv.org:2506.22099v3",
        "title": "B\\'ezierGS: Dynamic Urban Scene Reconstruction with B\\'ezier Curve Gaussian Splatting",
        "link": "https://arxiv.org/abs/2506.22099",
        "author": "Zipei Ma, Junzhe Jiang, Yurui Chen, Li Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.22099v3 Announce Type: replace \nAbstract: The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B\\'ezier curve Gaussian splatting (B\\'ezierGS), which represents the motion trajectories of dynamic objects using learnable B\\'ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis."
      },
      {
        "id": "oai:arXiv.org:2506.23782v2",
        "title": "WATS: Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling",
        "link": "https://arxiv.org/abs/2506.23782",
        "author": "Xiaoyang Li, Linwei Tao, Haohui Lu, Minjing Dong, Junbin Gao, Chang Xu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.23782v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance on relational data; however, their confidence estimates often misalign with actual predictive correctness, posing significant limitations for deployment in safety-critical settings. While existing graph-aware calibration methods seek to mitigate this limitation, they primarily depend on coarse one-hop statistics, such as neighbor-predicted confidence, or latent node embeddings, thereby neglecting the fine-grained structural heterogeneity inherent in graph topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a post-hoc calibration framework that assigns node-specific temperatures based on tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the scalability and topology sensitivity of graph wavelets to refine confidence estimates, all without necessitating model retraining or access to neighboring logits or predictions. Extensive evaluations across seven benchmark datasets with varying graph structures and two GNN backbones demonstrate that WATS achieves the lowest Expected Calibration Error (ECE) among all compared methods, outperforming both classical and graph-specific baselines by up to 42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared with graph-specific methods. Moreover, WATS remains computationally efficient, scaling well across graphs of diverse sizes and densities. Code will be released based on publication."
      },
      {
        "id": "oai:arXiv.org:2507.00825v2",
        "title": "High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery",
        "link": "https://arxiv.org/abs/2507.00825",
        "author": "Hongxing Peng, Lide Chen, Hui Zhu, Yan Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.00825v2 Announce Type: replace \nAbstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial challenges, including small target sizes, high-density distributions, and cluttered backgrounds in UAV imagery. Current algorithms often depend on hand-crafted components like anchor boxes, which demand fine-tuning and exhibit limited generalization, and Non-Maximum Suppression (NMS), which is threshold-sensitive and prone to misclassifying dense objects. These generic architectures thus struggle to adapt to aerial imaging characteristics, resulting in performance limitations. Moreover, emerging end-to-end frameworks have yet to effectively mitigate these aerial-specific challenges.To address these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time Detection Transformer framework tailored for UAVs. First, we introduce the High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone. HFESNet preserves critical high-frequency spatial details to extract robust semantic features, thereby improving discriminative capability for small and occluded targets in complex backgrounds. Second, our Efficient Small Object Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with minimal computational overhead, significantly boosting small object detection. Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware Positional Encoding (GAPE) modules enhance the detector's decoder stability and localization accuracy, effectively optimizing bounding boxes and providing explicit spatial priors for dense scenes. Experiments on the VisDrone dataset demonstrate that HEGS-DETR achieves a 5.1% AP50 and 3.8% AP increase over the baseline, while maintaining real-time speed and reducing parameter count by 4M."
      },
      {
        "id": "oai:arXiv.org:2507.01201v3",
        "title": "Escaping Plato's Cave: JAM for Aligning Independently Trained Vision and Language Models",
        "link": "https://arxiv.org/abs/2507.01201",
        "author": "Hyoseo (Lauren),  Yoon, Yisong Yue, Been Kim",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.01201v3 Announce Type: replace \nAbstract: Independently trained vision and language models inhabit disjoint representational spaces, shaped by their respective modalities, objectives, and architectures. Yet an emerging hypothesis - the Platonic Representation Hypothesis - suggests that such models may nonetheless converge toward a shared statistical model of reality. This compatibility, if it exists, raises a fundamental question: can we move beyond post-hoc statistical detection of alignment and explicitly optimize for it between such disjoint representations? We cast this Platonic alignment problem as a multi-objective optimization task - preserve each modality's native structure while aligning for mutual coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that jointly trains modality-specific autoencoders on the latent representations of pre-trained single modality models, encouraging alignment through both reconstruction and cross-modal objectives. By analogy, this framework serves as a method to escape Plato's Cave, enabling the emergence of shared structure from disjoint inputs. We evaluate this framework across three critical design axes: (i) the alignment objective - comparing contrastive loss (Con), its hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at which alignment is most effective, and (iii) the impact of foundation model scale on representational convergence. Our findings show that our lightweight Pareto-efficient framework reliably induces alignment, even across frozen, independently trained representations, offering both theoretical insight and practical pathways for transforming generalist unimodal foundations into specialist multimodal models."
      },
      {
        "id": "oai:arXiv.org:2507.01722v3",
        "title": "When Does Pruning Benefit Vision Representations?",
        "link": "https://arxiv.org/abs/2507.01722",
        "author": "Enrico Cassano, Riccardo Renzulli, Andrea Bragagnolo, Marco Grangetto",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.01722v3 Announce Type: replace \nAbstract: Pruning is widely used to reduce the complexity of deep learning models, but its effects on interpretability and representation learning remain poorly understood. This paper investigates how pruning influences vision models across three key dimensions: (i) interpretability, (ii) unsupervised object discovery, and (iii) alignment with human perception. We first analyze different vision network architectures to examine how varying sparsity levels affect feature attribution interpretability methods. Additionally, we explore whether pruning promotes more succinct and structured representations, potentially improving unsupervised object discovery by discarding redundant information while preserving essential features. Finally, we assess whether pruning enhances the alignment between model representations and human perception, investigating whether sparser models focus on more discriminative features similarly to humans. Our findings also reveal the presence of sweet spots, where sparse models exhibit higher interpretability, downstream generalization and human alignment. However, these spots highly depend on the network architectures and their size in terms of trainable parameters. Our results suggest a complex interplay between these three dimensions, highlighting the importance of investigating when and how pruning benefits vision representations."
      },
      {
        "id": "oai:arXiv.org:2507.01882v2",
        "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video",
        "link": "https://arxiv.org/abs/2507.01882",
        "author": "Guiqiu Liao, Matjaz Jogan, Marcel Hussing, Edward Zhang, Eric Eaton, Daniel A. Hashimoto",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.01882v2 Announce Type: replace \nAbstract: Object-centric slot attention is an emerging paradigm for unsupervised learning of structured, interpretable object-centric representations (slots). This enables effective reasoning about objects and events at a low computational cost and is thus applicable to critical healthcare applications, such as real-time interpretation of surgical video. The heterogeneous scenes in real-world applications like surgery are, however, difficult to parse into a meaningful set of slots. Current approaches with an adaptive slot count perform well on images, but their performance on surgical videos is low. To address this challenge, we propose a dynamic temporal slot transformer (DTST) module that is trained both for temporal reasoning and for predicting the optimal future slot initialization. The model achieves state-of-the-art performance on multiple surgical databases, demonstrating that unsupervised object-centric methods can be applied to real-world data and become part of the common arsenal in healthcare applications."
      },
      {
        "id": "oai:arXiv.org:2507.02358v3",
        "title": "Hita: Holistic Tokenizer for Autoregressive Image Generation",
        "link": "https://arxiv.org/abs/2507.02358",
        "author": "Anlin Zheng, Haochen Wang, Yucheng Zhao, Weipeng Deng, Tiancai Wang, Xiangyu Zhang, Xiaojuan Qi",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02358v3 Announce Type: replace \nAbstract: Vanilla autoregressive image generation models generate visual tokens step-by-step, limiting their ability to capture holistic relationships among token sequences. Moreover, because most visual tokenizers map local image patches into latent tokens, global information is limited. To address this, we introduce \\textit{Hita}, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Hita incorporates two key strategies to better align with the AR generation process: 1) {arranging} a sequential structure with holistic tokens at the beginning, followed by patch-level tokens, and using causal attention to maintain awareness of previous tokens; and 2) adopting a lightweight fusion module before feeding the de-quantized tokens into the decoder to control information flow and prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID} and \\textbf{281.9 IS} on the ImageNet benchmark. Detailed analysis of the holistic representation highlights its ability to capture global image properties, such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at \\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}."
      },
      {
        "id": "oai:arXiv.org:2507.02395v2",
        "title": "Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis",
        "link": "https://arxiv.org/abs/2507.02395",
        "author": "Byung Hyun Lee, Wongi Jeong, Woojae Han, Kyoungbun Lee, Se Young Chun",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02395v2 Announce Type: replace \nAbstract: Multiple instance learning (MIL) significantly reduced annotation costs via bag-level weak labels for large-scale images, such as histopathological whole slide images (WSIs). However, its adaptability to continual tasks with minimal forgetting has been rarely explored, especially on instance classification for localization. Weakly incremental learning for semantic segmentation has been studied for continual localization, but it focused on natural images, leveraging global relationships among hundreds of small patches (e.g., $16 \\times 16$) using pre-trained models. This approach seems infeasible for MIL localization due to enormous amounts ($\\sim 10^5$) of large patches (e.g., $256 \\times 256$) and no available global relationships such as cancer cells. To address these challenges, we propose Continual Multiple Instance Learning with Enhanced Localization (CoMEL), an MIL framework for both localization and adaptability with minimal forgetting. CoMEL consists of (1) Grouped Double Attention Transformer (GDAT) for efficient instance encoding, (2) Bag Prototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling, and (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting in both bag and instance classification. Extensive experiments on three public WSI datasets demonstrate superior performance of CoMEL, outperforming the prior arts by up to $11.00\\%$ in bag-level accuracy and up to $23.4\\%$ in localization accuracy under the continual MIL setup."
      },
      {
        "id": "oai:arXiv.org:2507.02792v2",
        "title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2507.02792",
        "author": "Liheng Zhang, Lexi Pang, Hang Ye, Xiaoxuan Ma, Yizhou Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02792v2 Announce Type: replace \nAbstract: Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., depth or pose maps) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. By revisiting existing methods, we identify a core limitation: the synchronous injection of condition features fails to account for the trade-off between domain alignment and structural preservation during denoising. Inspired by this observation, we propose a flexible feature injection framework that decouples the injection timestep from the denoising process. At its core is a structure-rich injection module, which enables the model to better adapt to the evolving interplay between alignment and structure preservation throughout the diffusion steps, resulting in more faithful structural generation. In addition, we introduce appearance-rich prompting and a restart refinement strategy to further enhance appearance control and visual quality. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art performance across diverse zero-shot conditioning scenarios."
      },
      {
        "id": "oai:arXiv.org:2507.02803v2",
        "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars",
        "link": "https://arxiv.org/abs/2507.02803",
        "author": "Gent Serifi, Marcel C. B\\\"uhler",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02803v2 Announce Type: replace \nAbstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections."
      },
      {
        "id": "oai:arXiv.org:2507.02850v2",
        "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users",
        "link": "https://arxiv.org/abs/2507.02850",
        "author": "Almog Hilel, Idan Shenfeld, Jacob Andreas, Leshem Choshen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02850v2 Announce Type: replace \nAbstract: We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a \"poisoned\" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection)."
      },
      {
        "id": "oai:arXiv.org:2507.02950v2",
        "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria",
        "link": "https://arxiv.org/abs/2507.02950",
        "author": "Keita Kiuchi, Yoshikazu Fujimoto, Hideyuki Goto, Tomonori Hosokawa, Makoto Nishimura, Yosuke Sato, Izumi Sezai",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02950v2 Announce Type: replace \nAbstract: This study provides the first comprehensive evaluation of large language model (LLM) performance across three counseling roles in Japanese-language therapeutic contexts. We simultaneously assessed counselor artificial intelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured Multi-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations, and evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human experts (n = 15) with extensive counseling experience evaluated AI-generated dialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding Manual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance across all MITI global ratings compared with zeroshot prompting, with no significant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed comparable performance to human raters for Cultivating Change Talk but systematically overestimated Softening Sustain Talk and the overall quality metrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3 focused on technical proficiency, and Sonnet prioritized emotional expression. Client AI simulations exhibited a limited emotional range and unnaturally high compliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English contexts and identify critical areas for improvement through advanced prompt engineering, retrieval-augmented generation, and targeted fine-tuning, with important implications for developing culturally sensitive AI mental health tools."
      },
      {
        "id": "oai:arXiv.org:2507.02962v2",
        "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism",
        "link": "https://arxiv.org/abs/2507.02962",
        "author": "Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02962v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while they remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have explored enhancing models' search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to the single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, aimed at reducing inference time and enhancing the model's capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%."
      },
      {
        "id": "oai:arXiv.org:2507.02986v2",
        "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models",
        "link": "https://arxiv.org/abs/2507.02986",
        "author": "Seshu Tirupathi, Dhaval Salwala, Elizabeth Daly, Inge Vejsbjerg",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02986v2 Announce Type: replace \nAbstract: As Large Language Models (LLMs) continue to be increasingly applied across various domains, their widespread adoption necessitates rigorous monitoring to prevent unintended negative consequences and ensure robustness. Furthermore, LLMs must be designed to align with human values, like preventing harmful content and ensuring responsible usage. The current automated systems and solutions for monitoring LLMs in production are primarily centered on LLM-specific concerns like hallucination etc, with little consideration given to the requirements of specific use-cases and user preferences. This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center. The framework is designed to detect and monitor risks associated with the deployment of LLM based applications. The approach models autonomous agents that identify risks, activate risk detection tools, within specific use-cases and facilitate continuous monitoring and reporting to enhance AI safety, and user expectations. The code is available at https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard."
      },
      {
        "id": "oai:arXiv.org:2507.02995v2",
        "title": "FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images",
        "link": "https://arxiv.org/abs/2507.02995",
        "author": "Guang Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.02995v2 Announce Type: replace \nAbstract: The rapid advancement of diffusion models, particularly Stable Diffusion 3.5, has enabled the generation of highly photorealistic synthetic images that pose significant challenges to existing detection methods. This paper presents FreqCross, a novel multi-modal fusion network that combines spatial RGB features, frequency domain artifacts, and radial energy distribution patterns to achieve robust detection of AI-generated images. Our approach leverages a three-branch architecture: (1) a ResNet-18 backbone for spatial feature extraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and (3) a multi-layer perceptron for analyzing radial energy profiles. We introduce a novel radial energy distribution analysis that captures characteristic frequency artifacts inherent in diffusion-generated images, and fuse it with spatial and spectral cues via simple feature concatenation followed by a compact classification head. Extensive experiments on a dataset of 10,000 paired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate that FreqCross achieves 97.8\\% accuracy, outperforming state-of-the-art baselines by 5.2\\%. The frequency analysis further reveals that synthetic images exhibit distinct spectral signatures in the 0.1--0.4 normalised frequency range, providing theoretical foundation for our approach. Code and pre-trained models are publicly available to facilitate reproducible research."
      },
      {
        "id": "oai:arXiv.org:2507.03009v2",
        "title": "PDFMathTranslate: Scientific Document Translation Preserving Layouts",
        "link": "https://arxiv.org/abs/2507.03009",
        "author": "Rongxin Ouyang, Chang Chu, Zhikuang Xin, Xiangyao Ma",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03009v2 Announce Type: replace \nAbstract: Language barriers in scientific documents hinder the diffusion and development of science and technologies. However, prior efforts in translating such documents largely overlooked the information in layouts. To bridge the gap, we introduce PDFMathTranslate, the world's first open-source software for translating scientific documents while preserving layouts. Leveraging the most recent advances in large language models and precise layout detection, we contribute to the community with key improvements in precision, flexibility, and efficiency. The work has been open-sourced at https://github.com/byaidu/pdfmathtranslate with more than 222k downloads."
      },
      {
        "id": "oai:arXiv.org:2507.03119v2",
        "title": "Neural-Network solver of ideal MHD equilibria",
        "link": "https://arxiv.org/abs/2507.03119",
        "author": "Timo Thun, Andrea Merlo, Rory Conlin, Dario Panici, Daniel B\\\"ockenhoff",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03119v2 Announce Type: replace \nAbstract: We present a novel approach to compute three-dimensional Magnetohydrodynamic equilibria by parametrizing Fourier modes with artificial neural networks and compare it to equilibria computed by conventional solvers. The full nonlinear global force residual across the volume in real space is then minimized with first order optimizers. Already,we observe competitive computational cost to arrive at the same minimum residuals computed by existing codes. With increased computational cost,lower minima of the residual are achieved by the neural networks,establishing a new lower bound for the force residual. We use minimally complex neural networks,and we expect significant improvements for solving not only single equilibria with neural networks,but also for computing neural network models valid over continuous distributions of equilibria."
      },
      {
        "id": "oai:arXiv.org:2507.03312v2",
        "title": "MPX: Mixed Precision Training for JAX",
        "link": "https://arxiv.org/abs/2507.03312",
        "author": "Alexander Gr\\\"afe, Sebastian Trimpe",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03312v2 Announce Type: replace \nAbstract: Mixed-precision training has emerged as an indispensable tool for enhancing the efficiency of neural network training in recent years. Concurrently, JAX has grown in popularity as a versatile machine learning toolbox. However, it currently lacks robust support for mixed-precision training. We propose MPX, a mixed-precision training toolbox for JAX that simplifies and accelerates the training of large-scale neural networks while preserving model accuracy. MPX seamlessly integrates with popular toolboxes such as Equinox and Flax, allowing users to convert full-precision pipelines to mixed-precision versions with minimal modifications. By casting both inputs and outputs to half precision, and introducing a dynamic loss-scaling mechanism, MPX alleviates issues like gradient underflow and overflow that commonly arise in half precision computations. Its design inherits critical features from JAX's type-promotion behavior, ensuring that operations take place in the correct precision and allowing for selective enforcement of full precision where needed (e.g., sums, means, or softmax). MPX further provides wrappers for automatic creation and management of mixed-precision gradients and optimizers, enabling straightforward integration into existing JAX training pipelines. MPX's source code, documentation, and usage examples are available at github.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX ."
      },
      {
        "id": "oai:arXiv.org:2507.03343v2",
        "title": "SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge",
        "link": "https://arxiv.org/abs/2507.03343",
        "author": "Yuxiang Mei, Yuang Zheng, Dongxing Xu, Yanhua Long",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03343v2 Announce Type: replace \nAbstract: This paper describes SHNU multilingual conversational speech recognition system (SHNU-mASR, team name-\"maybe\"), submitted to Track 1 of the INTERSPEECH 2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder architecture with a large language model (LLM) to form a unified multilingual ASR framework. The parallel-speech-encoder consists of two pre-trained encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output embeddings are concatenated and fed into the LLM, enabling the model to leverage complementary acoustic and linguistic knowledge and achieve competitive performance. Moreover, we adopt a tri-stage training strategy to jointly update the low-rank adaptation modules and projector parameters of both the speech encoders and the LLM. In addition, we incorporate an additional language-aware prompt at the LLM input to enhance language-specific text generation. The SHNU-mASR system achieves an overall character/word error rate (CER/WER) of 11.76% on the blind evaluation set of the challenge, outperforming the official MLC-SLM baseline by 8.41 absolute CER/WER, without increasing the baseline training data."
      },
      {
        "id": "oai:arXiv.org:2507.03483v2",
        "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset",
        "link": "https://arxiv.org/abs/2507.03483",
        "author": "Zhiheng Xi, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng Zuo, Zhenfei Yin, Lei Bai, Tao Ji, Tao Gui, Qi Zhang, Philip Torr, Xuanjing Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03483v2 Announce Type: replace \nAbstract: In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMs' knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community."
      },
      {
        "id": "oai:arXiv.org:2507.03532v2",
        "title": "PhenoBench: A Comprehensive Benchmark for Cell Phenotyping",
        "link": "https://arxiv.org/abs/2507.03532",
        "author": "Jerome Luescher, Nora Koreuber, Jannik Franzen, Fabian H. Reith, Claudia Winklmayr, Elias Baumann, Christian M. Schuerch, Dagmar Kainmueller, Josef Lorenz Rumberger",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03532v2 Announce Type: replace \nAbstract: Digital pathology has seen the advent of a wealth of foundational models (FM), yet to date their performance on cell phenotyping has not been benchmarked in a unified manner. We therefore propose PhenoBench: A comprehensive benchmark for cell phenotyping on Hematoxylin and Eosin (H&amp;E) stained histopathology images. We provide both PhenoCell, a new H&amp;E dataset featuring 14 granular cell types identified by using multiplexed imaging, and ready-to-use fine-tuning and benchmarking code that allows the systematic evaluation of multiple prominent pathology FMs in terms of dense cell phenotype predictions in different generalization scenarios. We perform extensive benchmarking of existing FMs, providing insights into their generalization behavior under technical vs. medical domain shifts. Furthermore, while FMs achieve macro F1 scores > 0.70 on previously established benchmarks such as Lizard and PanNuke, on PhenoCell, we observe scores as low as 0.20. This indicates a much more challenging task not captured by previous benchmarks, establishing PhenoCell as a prime asset for future benchmarking of FMs and supervised models alike. Code and data are available on GitHub."
      },
      {
        "id": "oai:arXiv.org:2507.03633v2",
        "title": "From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis",
        "link": "https://arxiv.org/abs/2507.03633",
        "author": "Amirabbas Hojjati, Lu Li, Ibrahim Hameed, Anis Yazidi, Pedro G. Lind, Rabindra Khadka",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03633v2 Announce Type: replace \nAbstract: EEG signals capture brain activity with high temporal and low spatial resolution, supporting applications such as neurological diagnosis, cognitive monitoring, and brain-computer interfaces. However, effective analysis is hindered by limited labeled data, high dimensionality, and the absence of scalable models that fully capture spatiotemporal dependencies. Existing self-supervised learning (SSL) methods often focus on either spatial or temporal features, leading to suboptimal representations. To this end, we propose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive Architecture (V-JEPA) for EEG classification. By treating EEG as video-like sequences, EEG-VJEPA learns semantically meaningful spatiotemporal representations using joint embeddings and adaptive masking. To our knowledge, this is the first work that exploits V-JEPA for EEG classification and explores the visual concepts learned by the model. Evaluations on the publicly available Temple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA outperforms existing state-of-the-art models in classification accuracy.Beyond classification accuracy, EEG-VJEPA captures physiologically relevant spatial and temporal signal patterns, offering interpretable embeddings that may support human-AI collaboration in diagnostic workflows. These findings position EEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in real-world clinical settings."
      },
      {
        "id": "oai:arXiv.org:2507.03711v2",
        "title": "Can LLMs Play \\^O \\u{A}n Quan Game? A Study of Multi-Step Planning and Decision Making",
        "link": "https://arxiv.org/abs/2507.03711",
        "author": "Sang Quang Nguyen, Kiet Van Nguyen, Vinh-Tiep Nguyen, Thanh Duc Ngo, Ngan Luu-Thuy Nguyen, Dinh-Duy Le",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03711v2 Announce Type: replace \nAbstract: In this paper, we explore the ability of large language models (LLMs) to plan and make decisions through the lens of the traditional Vietnamese board game, \\^O \\u{A}n Quan. This game, which involves a series of strategic token movements and captures, offers a unique environment for evaluating the decision-making and strategic capabilities of LLMs. Specifically, we develop various agent personas, ranging from aggressive to defensive, and employ the \\^O \\u{A}n Quan game as a testbed for assessing LLM performance across different strategies. Through experimentation with models like Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we aim to understand how these models execute strategic decision-making, plan moves, and manage dynamic game states. The results will offer insights into the strengths and weaknesses of LLMs in terms of reasoning and strategy, contributing to a deeper understanding of their general capabilities."
      },
      {
        "id": "oai:arXiv.org:2507.03724v2",
        "title": "MemOS: A Memory OS for AI System",
        "link": "https://arxiv.org/abs/2507.03724",
        "author": "Zhiyu Li, Shichao Song, Chenyang Xi, Hanyu Wang, Chen Tang, Simin Niu, Ding Chen, Jiawei Yang, Chunyu Li, Qingchen Yu, Jihao Zhao, Yezhaohui Wang, Peng Liu, Zehao Lin, Pengyuan Wang, Jiahao Huo, Tianyi Chen, Kai Chen, Kehang Li, Zhen Tao, Junpeng Ren, Huayi Lai, Hao Wu, Bo Tang, Zhenren Wang, Zhaoxin Fan, Ningyu Zhang, Linfeng Zhang, Junchi Yan, Mingchuan Yang, Tong Xu, Wei Xu, Huajun Chen, Haofeng Wang, Hongkang Yang, Wentao Zhang, Zhi-Qin John Xu, Siheng Chen, Feiyu Xiong",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03724v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling."
      },
      {
        "id": "oai:arXiv.org:2507.03745v2",
        "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation",
        "link": "https://arxiv.org/abs/2507.03745",
        "author": "Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, Yue Zhao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03745v2 Announce Type: replace \nAbstract: Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: https://cumulo-autumn.github.io/StreamDiT/"
      },
      {
        "id": "oai:arXiv.org:2507.03990v2",
        "title": "LEHA-CVQAD: Dataset To Enable Generalized Video Quality Assessment of Compression Artifacts",
        "link": "https://arxiv.org/abs/2507.03990",
        "author": "Aleksandr Gushchin, Maksim Smirnov, Dmitriy Vatolin, Anastasia Antsiferova",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03990v2 Announce Type: replace \nAbstract: We propose the LEHA-CVQAD (Large-scale Enriched Human-Annotated Compressed Video Quality Assessment) dataset, which comprises 6,240 clips for compression-oriented video quality assessment. 59 source videos are encoded with 186 codec-preset variants, 1.8M pairwise, and 1.5k MOS ratings are fused into a single quality scale; part of the videos remains hidden for blind evaluation. We also propose Rate-Distortion Alignment Error (RDAE), a novel evaluation metric that quantifies how well VQA models preserve bitrate-quality ordering, directly supporting codec parameter tuning. Testing IQA/VQA methods reveals that popular VQA metrics exhibit high RDAE and lower correlations, underscoring the dataset challenges and utility. The open part and the results of LEHA-CVQAD are available at https://aleksandrgushchin.github.io/lcvqad/"
      },
      {
        "id": "oai:arXiv.org:2507.04176v2",
        "title": "skfolio: Portfolio Optimization in Python",
        "link": "https://arxiv.org/abs/2507.04176",
        "author": "Carlo Nicolini, Matteo Manzi, Hugo Delatte",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04176v2 Announce Type: replace \nAbstract: Portfolio optimization is a fundamental challenge in quantitative finance, requiring robust computational tools that integrate statistical rigor with practical implementation. We present skfolio, an open-source Python library for portfolio construction and risk management that seamlessly integrates with the scikit-learn ecosystem. skfolio provides a unified framework for diverse allocation strategies, from classical mean-variance optimization to modern clustering-based methods, state-of-the-art financial estimators with native interfaces, and advanced cross-validation techniques tailored for financial time series. By adhering to scikit-learn's fit-predict-transform paradigm, the library enables researchers and practitioners to leverage machine learning workflows for portfolio optimization, promoting reproducibility and transparency in quantitative finance."
      },
      {
        "id": "oai:arXiv.org:2507.04243v2",
        "title": "Domain Generalizable Portrait Style Transfer",
        "link": "https://arxiv.org/abs/2507.04243",
        "author": "Xinbo Wang, Wenju Xu, Qing Zhang, Wei-Shi Zheng",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04243v2 Announce Type: replace \nAbstract: This paper presents a portrait style transfer method that generalizes well to various different domains while enabling high-quality semantic-aligned stylization on regions including hair, eyes, eyelashes, skins, lips, and background. To this end, we propose to establish dense semantic correspondence between the given input and reference portraits based on a pre-trained model and a semantic adapter, with which we obtain a warped reference semantically aligned with the input. To ensure effective yet controllable style transfer, we devise an AdaIN-Wavelet transform to balance content preservation and stylization by blending low-frequency information of the warped reference with high-frequency information of the input in the latent space. A style adapter is also designed to provide style guidance from the warped reference. With the stylized latent from AdaIN-Wavelet transform, we employ a dual-conditional diffusion model that integrates a ControlNet recording high-frequency information and the style guidance to generate the final result. Extensive experiments demonstrate the superiority of our method. Our code and trained model are available at https://github.com/wangxb29/DGPST."
      },
      {
        "id": "oai:arXiv.org:2507.04487v2",
        "title": "LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization",
        "link": "https://arxiv.org/abs/2507.04487",
        "author": "Xujia Wang, Yunjia Qi, Bin Xu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04487v2 Announce Type: replace \nAbstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources Subnet Integration Adaptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about $27\\%$ compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training."
      },
      {
        "id": "oai:arXiv.org:2507.04511v2",
        "title": "FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection",
        "link": "https://arxiv.org/abs/2507.04511",
        "author": "Xinhua Lu, Runhe Lai, Yanqi Wu, Kanghao Chen, Wei-Shi Zheng, Ruixuan Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04511v2 Announce Type: replace \nAbstract: Pre-trained vision-language models (VLMs) have advanced out-of-distribution (OOD) detection recently. However, existing CLIP-based methods often focus on learning OOD-related knowledge to improve OOD detection, showing limited generalization or reliance on external large-scale auxiliary datasets. In this study, instead of delving into the intricate OOD-related knowledge, we propose an innovative CLIP-based framework based on Forced prompt leArning (FA), designed to make full use of the In-Distribution (ID) knowledge and ultimately boost the effectiveness of OOD detection. Our key insight is to learn a prompt (i.e., forced prompt) that contains more diversified and richer descriptions of the ID classes beyond the textual semantics of class labels. Specifically, it promotes better discernment for ID images, by forcing more notable semantic similarity between ID images and the learnable forced prompt. Moreover, we introduce a forced coefficient, encouraging the forced prompt to learn more comprehensive and nuanced descriptions of the ID classes. In this way, FA is capable of achieving notable improvements in OOD detection, even when trained without any external auxiliary datasets, while maintaining an identical number of trainable parameters as CoOp. Extensive empirical evaluations confirm our method consistently outperforms current state-of-the-art methods. Code is available at https://github.com/0xFAFA/FA."
      },
      {
        "id": "oai:arXiv.org:2507.04638v2",
        "title": "UGG-ReID: Uncertainty-Guided Graph Model for Multi-Modal Object Re-Identification",
        "link": "https://arxiv.org/abs/2507.04638",
        "author": "Xixi Wan, Aihua Zheng, Bo Jiang, Beibei Wang, Chenglong Li, Jin Tang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04638v2 Announce Type: replace \nAbstract: Multi-modal object Re-IDentification (ReID) has gained considerable attention with the goal of retrieving specific targets across cameras using heterogeneous visual data sources. Existing methods primarily aim to improve identification performance, but often overlook the uncertainty arising from inherent defects, such as intra-modal noise and inter-modal conflicts. This uncertainty is particularly significant in the case of fine-grained local occlusion and frame loss, which becomes a challenge in multi-modal learning. To address the above challenge, we propose a robust approach named Uncertainty-Guided Graph model for multi-modal object ReID (UGG-ReID). UGG-ReID is designed to mitigate noise interference and facilitate effective multi-modal fusion by estimating both local and sample-level aleatoric uncertainty and explicitly modeling their dependencies. Specifically, we first propose the Gaussian patch-graph representation model that leverages uncertainty to quantify fine-grained local cues and capture their structural relationships. This process boosts the expressiveness of modal-specific information, ensuring that the generated embeddings are both more informative and robust. Subsequently, we design an uncertainty-guided mixture of experts strategy that dynamically routes samples to experts exhibiting low uncertainty. This strategy effectively suppresses noise-induced instability, leading to enhanced robustness. Meanwhile, we design an uncertainty-guided routing to strengthen the multi-modal interaction, improving the performance. UGG-ReID is comprehensively evaluated on five representative multi-modal object ReID datasets, encompassing diverse spectral modalities. Experimental results show that the proposed method achieves excellent performance on all datasets and is significantly better than current methods in terms of noise immunity. Our code will be made public upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2507.04667v2",
        "title": "What's Making That Sound Right Now? Video-centric Audio-Visual Localization",
        "link": "https://arxiv.org/abs/2507.04667",
        "author": "Hahyeon Choi, Junhoo Lee, Nojun Kwak",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04667v2 Announce Type: replace \nAbstract: Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization."
      },
      {
        "id": "oai:arXiv.org:2507.04801v2",
        "title": "PointGAC: Geometric-Aware Codebook for Masked Point Cloud Modeling",
        "link": "https://arxiv.org/abs/2507.04801",
        "author": "Abiao Li, Chenlei Lv, Yuming Fang, Yifan Zuo, Jian Zhang, Guofeng Mei",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04801v2 Announce Type: replace \nAbstract: Most masked point cloud modeling (MPM) methods follow a regression paradigm to reconstruct the coordinate or feature of masked regions. However, they tend to over-constrain the model to learn the details of the masked region, resulting in failure to capture generalized features. To address this limitation, we propose \\textbf{\\textit{PointGAC}}, a novel clustering-based MPM method that aims to align the feature distribution of masked regions. Specially, it features an online codebook-guided teacher-student framework. Firstly, it presents a geometry-aware partitioning strategy to extract initial patches. Then, the teacher model updates a codebook via online k-means based on features extracted from the complete patches. This procedure facilitates codebook vectors to become cluster centers. Afterward, we assigns the unmasked features to their corresponding cluster centers, and the student model aligns the assignment for the reconstructed masked features. This strategy focuses on identifying the cluster centers to which the masked features belong, enabling the model to learn more generalized feature representations. Benefiting from a proposed codebook maintenance mechanism, codebook vectors are actively updated, which further increases the efficiency of semantic feature learning. Experiments validate the effectiveness of the proposed method on various downstream tasks. Code is available at https://github.com/LAB123-tech/PointGAC"
      },
      {
        "id": "oai:arXiv.org:2507.04942v2",
        "title": "SIGIR 2025 -- LiveRAG Challenge Report",
        "link": "https://arxiv.org/abs/2507.04942",
        "author": "David Carmel, Simone Filice, Guy Horowitz, Yoelle Maarek, Oren Somekh, Ran Tavory, Mehdi Ghissassi, Edo Liberty, Roy Miara",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04942v2 Announce Type: replace \nAbstract: The LiveRAG Challenge at SIGIR 2025, held between March and May 2025, provided a competitive platform for advancing Retrieval-Augmented Generation (RAG) technologies. Participants from academia and industry were invited to develop a RAG-based question-answering system using a fixed corpus (Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal was to facilitate challenging comparisons of retrieval and prompting strategies. During the Live Challenge Day, 70 teams from 27 different countries provided answers and supportive information to 500 unseen questions within a strict two-hour time window. Evaluation was conducted in two stages: first an automated LLM-as-a-judge approach was used to compute correctness and faithfulness score, then a manual review of top ranked submissions was conducted. The finalists were announced on June 12, 2025, with prizes awarded during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy."
      },
      {
        "id": "oai:arXiv.org:2507.04981v2",
        "title": "Classification of autoimmune diseases from Peripheral blood TCR repertoires by multimodal multi-instance learning",
        "link": "https://arxiv.org/abs/2507.04981",
        "author": "Ruihao Zhang, Fei Ye, Dandan Meng, Yixuan Huang,  Maochen, Xiao Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04981v2 Announce Type: replace \nAbstract: T cell receptor (TCR) repertoires encode critical immunological signatures for autoimmune diseases, yet their clinical application remains limited by sequence sparsity and low witness rates. We developed EAMil, a multi-instance deep learning framework that leverages TCR sequencing data to diagnose systemic lupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional accuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding and enhanced gate attention mechanisms, our model achieved state-of-the-art performance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully identified disease-associated genes with over 90% concordance with established differential analyses and effectively distinguished disease-specific TCR genes. The model demonstrated robustness in classifying multiple disease categories, utilizing the SLEDAI score to stratify SLE patients by disease severity as well as to diagnose the site of damage in SLE patients, and effectively controlling for confounding factors such as age and gender. This interpretable framework for immune receptor analysis provides new insights for autoimmune disease detection and classification with broad potential clinical applications across immune-mediated conditions."
      },
      {
        "id": "oai:arXiv.org:2507.05177v2",
        "title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model",
        "link": "https://arxiv.org/abs/2507.05177",
        "author": "Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05177v2 Announce Type: replace \nAbstract: Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at https://casia-lm.github.io/OpenS2S"
      },
      {
        "id": "oai:arXiv.org:2507.05221v2",
        "title": "CTA: Cross-Task Alignment for Better Test Time Training",
        "link": "https://arxiv.org/abs/2507.05221",
        "author": "Samuel Barbeau, Pedram Fekri, David Osowiechi, Ali Bahri, Moslem Yazdanpanah, Masih Aminbeidokhti, Christian Desrosiers",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05221v2 Announce Type: replace \nAbstract: Deep learning models have demonstrated exceptional performance across a wide range of computer vision tasks. However, their performance often degrades significantly when faced with distribution shifts, such as domain or dataset changes. Test-Time Training (TTT) has emerged as an effective method to enhance model robustness by incorporating an auxiliary unsupervised task during training and leveraging it for model updates at test time. In this work, we introduce CTA (Cross-Task Alignment), a novel approach for improving TTT. Unlike existing TTT methods, CTA does not require a specialized model architecture and instead takes inspiration from the success of multi-modal contrastive learning to align a supervised encoder with a self-supervised one. This process enforces alignment between the learned representations of both models, thereby mitigating the risk of gradient interference, preserving the intrinsic robustness of self-supervised learning and enabling more semantically meaningful updates at test-time. Experimental results demonstrate substantial improvements in robustness and generalization over the state-of-the-art on several benchmark datasets."
      },
      {
        "id": "oai:arXiv.org:2202.01116v3",
        "title": "An Optimal Transport Perspective on Unpaired Image Super-Resolution",
        "link": "https://arxiv.org/abs/2202.01116",
        "author": "Milena Gazdieva, Petr Mokrov, Litu Rout, Alexander Korotin, Andrey Kravchenko, Alexander Filippov, Evgeny Burnaev",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2202.01116v3 Announce Type: replace-cross \nAbstract: Real-world image super-resolution (SR) tasks often do not have paired datasets, which limits the application of supervised techniques. As a result, the tasks are usually approached by unpaired techniques based on Generative Adversarial Networks (GANs), which yield complex training losses with several regularization terms, e.g., content or identity losses. While GANs usually provide good practical performance, they are used heuristically, i.e., theoretical understanding of their behaviour is yet rather limited. We theoretically investigate optimization problems which arise in such models and find two surprising observations. First, the learned SR map is always an optimal transport (OT) map. Second, we theoretically prove and empirically show that the learned map is biased, i.e., it does not actually transform the distribution of low-resolution images to high-resolution ones. Inspired by these findings, we investigate recent advances in neural OT field to resolve the bias issue. We establish an intriguing connection between regularized GANs and neural OT approaches. We show that unlike the existing GAN-based alternatives, these algorithms aim to learn an unbiased OT map. We empirically demonstrate our findings via a series of synthetic and real-world unpaired SR experiments. Our source code is publicly available at https://github.com/milenagazdieva/OT-Super-Resolution."
      },
      {
        "id": "oai:arXiv.org:2305.09666v3",
        "title": "AbdomenAtlas-8K: Annotating 8,000 CT Volumes for Multi-Organ Segmentation in Three Weeks",
        "link": "https://arxiv.org/abs/2305.09666",
        "author": "Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Jie Liu, Yucheng Tang, Alan Yuille, Zongwei Zhou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2305.09666v3 Announce Type: replace-cross \nAbstract: Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes an active learning method to expedite the annotation process for organ segmentation and creates the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintaining a similar or even better annotation quality. This achievement is attributed to three unique properties of our method: (1) label bias reduction using multiple pre-trained segmentation models, (2) effective error detection in the model predictions, and (3) attention guidance for annotators to make corrections on the most salient errors. Furthermore, we summarize the taxonomy of common errors made by AI algorithms and annotators. This allows for continuous revision of both AI and annotations and significantly reduces the annotation costs required to create large-scale datasets for a wider variety of medical imaging tasks."
      },
      {
        "id": "oai:arXiv.org:2306.01190v2",
        "title": "Identifying visible tissue in intraoperative ultrasound: a method and application",
        "link": "https://arxiv.org/abs/2306.01190",
        "author": "Alistair Weld, Luke Dixon, Giulio Anichini, Michael Dyck, Alex Ranne, Sophie Camp, Stamatia Giannarou",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2306.01190v2 Announce Type: replace-cross \nAbstract: Purpose: Intraoperative ultrasound scanning is a demanding visuotactile task. It requires operators to simultaneously localise the ultrasound perspective and manually perform slight adjustments to the pose of the probe, making sure not to apply excessive force or breaking contact with the tissue, whilst also characterising the visible tissue. Method: To analyse the probe-tissue contact, an iterative filtering and topological method is proposed to identify the underlying visible tissue, which can be used to detect acoustic shadow and construct confidence maps of perceptual salience. Results: For evaluation, datasets containing both in vivo and medical phantom data are created. A suite of evaluations is performed, including an evaluation of acoustic shadow classification. Compared to an ablation, deep learning, and statistical method, the proposed approach achieves superior classification on in vivo data, achieving an F_beta score of 0.864, in comparison to 0.838, 0.808, 0.808. A novel framework for evaluating the confidence estimation of probe tissue contact is created. The phantom data is captured specifically for this, and comparison is made against two established methods. The proposed method produced the superior response, achieving an average normalised root mean square error of 0.168, in comparison to 1.836 and 4.542. Evaluation is also extended to determine the algorithm's robustness to parameter perturbation, speckle noise, data distribution shift, and capability for guiding a robotic scan. Conclusion: The results of this comprehensive set of experiments justify the potential clinical value of the proposed algorithm, which can be used to support clinical training and robotic ultrasound automation."
      },
      {
        "id": "oai:arXiv.org:2406.07072v3",
        "title": "On the relation between trainability and dequantization of variational quantum learning models",
        "link": "https://arxiv.org/abs/2406.07072",
        "author": "Elies Gil-Fuster, Casper Gyurik, Adri\\'an P\\'erez-Salinas, Vedran Dunjko",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2406.07072v3 Announce Type: replace-cross \nAbstract: The quest for successful variational quantum machine learning (QML) relies on the design of suitable parametrized quantum circuits (PQCs), as analogues to neural networks in classical machine learning. Successful QML models must fulfill the properties of trainability and non-dequantization, among others. Recent works have highlighted an intricate interplay between trainability and dequantization of such models, which is still unresolved. In this work we contribute to this debate from the perspective of machine learning, proving a number of results identifying, among others when trainability and non-dequantization are not mutually exclusive. We begin by providing a number of new somewhat broader definitions of the relevant concepts, compared to what is found in other literature, which are operationally motivated, and consistent with prior art. With these precise definitions given and motivated, we then study the relation between trainability and dequantization of variational QML. Next, we also discuss the degrees of \"variationalness\" of QML models, where we distinguish between models like the hardware efficient ansatz and quantum kernel methods. Finally, we introduce recipes for building PQC-based QML models which are both trainable and nondequantizable, and corresponding to different degrees of variationalness. We do not address the practical utility for such models. Our work however does point toward a way forward for finding more general constructions, for which finding applications may become feasible."
      },
      {
        "id": "oai:arXiv.org:2406.08321v2",
        "title": "Deep learning from strongly mixing observations: Sparse-penalized regularization and minimax optimality",
        "link": "https://arxiv.org/abs/2406.08321",
        "author": "William Kengne, Modou Wade",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2406.08321v2 Announce Type: replace-cross \nAbstract: The explicit regularization and optimality of deep neural networks estimators from independent data have made considerable progress recently. The study of such properties on dependent data is still a challenge. In this paper, we carry out deep learning from strongly mixing observations, and deal with the squared and a broad class of loss functions. We consider sparse-penalized regularization for deep neural network predictor. For a general framework that includes, regression estimation, classification, time series prediction,$\\cdots$, oracle inequality for the expected excess risk is established and a bound on the class of H\\\"older smooth functions is provided. For nonparametric regression from strong mixing data and sub-exponentially error, we provide an oracle inequality for the $L_2$ error and investigate an upper bound of this error on a class of H\\\"older composition functions. For the specific case of nonparametric autoregression with Gaussian and Laplace errors, a lower bound of the $L_2$ error on this H\\\"older composition class is established. Up to logarithmic factor, this bound matches its upper bound; so, the deep neural network estimator attains the minimax optimal rate."
      },
      {
        "id": "oai:arXiv.org:2406.16993v3",
        "title": "Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image Segmentation?",
        "link": "https://arxiv.org/abs/2406.16993",
        "author": "Pallabi Dutta, Soham Bose, Swalpa Kumar Roy, Sushmita Mitra",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2406.16993v3 Announce Type: replace-cross \nAbstract: The development of efficient segmentation strategies for medical images has evolved from its initial dependence on Convolutional Neural Networks (CNNs) to the current investigation of hybrid models that combine CNNs with Vision Transformers (ViTs). There is an increasing focus on creating architectures that are both high-performing and computationally efficient, capable of being deployed on remote systems with limited resources. Although transformers can capture global dependencies in the input space, they face challenges from the corresponding high computational and storage expenses involved. This research investigates the integration of CNNs with Vision Extended Long Short-Term Memory (Vision-xLSTM)s by introducing the novel U-VixLSTM.\n  The Vision-xLSTM blocks capture the temporal and global relationships within the patches extracted from the CNN feature maps. The convolutional feature reconstruction path upsamples the output volume from the Vision-xLSTM blocks to produce the segmentation output. Our primary objective is to propose that Vision-xLSTM forms an appropriate backbone for medical image segmentation, offering excellent performance with reduced computational costs. The U-VixLSTM exhibits superior performance compared to the state-of-the-art networks in the publicly available Synapse, ISIC and ACDC datasets. Code provided: https://github.com/duttapallabi2907/U-VixLSTM"
      },
      {
        "id": "oai:arXiv.org:2409.01754v3",
        "title": "Empirical evidence of Large Language Model's influence on human spoken communication",
        "link": "https://arxiv.org/abs/2409.01754",
        "author": "Hiromu Yakura, Ezequiel Lopez-Lopez, Levin Brinkmann, Ignacio Serna, Prateek Gupta, Ivan Soraperra, Iyad Rahwan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.01754v3 Announce Type: replace-cross \nAbstract: From the invention of writing and the printing press, to television and social media, human history is punctuated by major innovations in communication technology, which fundamentally altered how ideas spread and reshaped our culture. Recent chatbots powered by generative artificial intelligence constitute a novel medium that encodes cultural patterns in their neural representations and disseminates them in conversations with hundreds of millions of people. Understanding whether these patterns transmit into human language, and ultimately shape human culture, is a fundamental question. While fully quantifying the causal impact of a chatbot like ChatGPT on human culture is very challenging, lexicographic shift in human spoken communication may offer an early indicator of such broad phenomenon. Here, we apply econometric causal inference techniques to 740,249 hours of human discourse from 360,445 YouTube academic talks and 771,591 conversational podcast episodes across multiple disciplines. We detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT, such as delve, comprehend, boast, swift, and meticulous, after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture. This marks the beginning of a closed cultural feedback loop in which cultural traits circulate bidirectionally between humans and machines. Our results motivate further research into the evolution of human-machine culture, and raise concerns over the erosion of linguistic and cultural diversity, and the risks of scalable manipulation."
      },
      {
        "id": "oai:arXiv.org:2409.06672v2",
        "title": "Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort",
        "link": "https://arxiv.org/abs/2409.06672",
        "author": "Cristian Trout",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.06672v2 Announce Type: replace-cross \nAbstract: Many experts believe that AI systems will sooner or later pose uninsurable risks, including existential risks. This creates an extreme judgment-proof problem: few if any parties can be held accountable ex post in the event of such a catastrophe. This paper proposes a novel solution: a government-provided, mandatory indemnification program for AI developers. The program uses risk-priced indemnity fees to induce socially optimal levels of care. Risk-estimates are determined by surveying experts, including indemnified developers. The Bayesian Truth Serum mechanism is employed to incent honest and effortful responses. Compared to alternatives, this approach arguably better leverages all private information, and provides a clearer signal to indemnified developers regarding what risks they must mitigate to lower their fees. It's recommended that collected fees be used to help fund the safety research developers need, employing a fund matching mechanism (Quadratic Financing) to induce an optimal supply of this public good. Under Quadratic Financing, safety research projects would compete for private contributions from developers, signaling how much each is to be supplemented with public funds."
      },
      {
        "id": "oai:arXiv.org:2409.06673v2",
        "title": "Liability and Insurance for Catastrophic Losses: the Nuclear Power Precedent and Lessons for AI",
        "link": "https://arxiv.org/abs/2409.06673",
        "author": "Cristian Trout",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.06673v2 Announce Type: replace-cross \nAbstract: As AI systems become more autonomous and capable, experts warn of them potentially causing catastrophic losses. Drawing on the successful precedent set by the nuclear power industry, this paper argues that developers of frontier AI models should be assigned limited, strict, and exclusive third party liability for harms resulting from Critical AI Occurrences (CAIOs) - events that cause or easily could have caused catastrophic losses. Mandatory insurance for CAIO liability is recommended to overcome developers' judgment-proofness, mitigate winner's curse dynamics, and leverage insurers' quasi-regulatory abilities. Based on theoretical arguments and observations from the analogous nuclear power context, insurers are expected to engage in a mix of causal risk-modeling, monitoring, lobbying for stricter regulation, and providing loss prevention guidance in the context of insuring against heavy-tail risks from AI. While not a substitute for regulation, clear liability assignment and mandatory insurance can help efficiently allocate resources to risk-modeling and safe design, facilitating future regulatory efforts."
      },
      {
        "id": "oai:arXiv.org:2409.09309v2",
        "title": "Real-Time Stochastic Terrain Mapping and Processing for Autonomous Safe Landing",
        "link": "https://arxiv.org/abs/2409.09309",
        "author": "Kento Tomita, Koki Ho",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.09309v2 Announce Type: replace-cross \nAbstract: Onboard terrain sensing and mapping for safe planetary landings often suffer from missed hazardous features, e.g., small rocks, due to the large observational range and the limited resolution of the obtained terrain data. To this end, this paper develops a novel real-time stochastic terrain mapping algorithm that accounts for topographic uncertainty between the sampled points, or the uncertainty due to the sparse 3D terrain measurements. We introduce a Gaussian digital elevation map that is efficiently constructed using the combination of Delauney triangulation and local Gaussian process regression. The geometric investigation of the lander-terrain interaction is exploited to efficiently evaluate the marginally conservative local slope and roughness while avoiding the costly computation of the local plane. The conservativeness is proved in the paper. The developed real-time uncertainty quantification pipeline enables stochastic landing safety evaluation under challenging operational conditions, such as a large observational range or limited sensor capability, which is a critical stepping stone for the development of predictive guidance algorithms for safe autonomous planetary landing. Detailed reviews on background and related works are also presented."
      },
      {
        "id": "oai:arXiv.org:2409.15866v4",
        "title": "Online Planning for Multi-UAV Pursuit-Evasion in Unknown Environments Using Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2409.15866",
        "author": "Jiayu Chen, Chao Yu, Guosheng Li, Wenhao Tang, Shilong Ji, Xinyi Yang, Botian Xu, Huazhong Yang, Yu Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.15866v4 Announce Type: replace-cross \nAbstract: Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL) has demonstrated potential in modeling cooperative behaviors, but most RL-based approaches remain constrained to simplified simulations with limited dynamics or fixed scenarios. Previous attempts to deploy RL policy to real-world pursuit-evasion are largely restricted to two-dimensional scenarios, such as ground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV pursuit-evasion by considering UAV dynamics and physical constraints. We introduce an evader prediction-enhanced network to tackle partial observability in cooperative strategy learning. Additionally, we propose an adaptive environment generator within MARL training, enabling higher exploration efficiency and better policy generalization across diverse scenarios. Simulations show our method significantly outperforms all baselines in challenging scenarios, generalizing to unseen scenarios with a 100% capture rate. Finally, we derive a feasible policy via a two-stage reward refinement and deploy the policy on real quadrotors in a zero-shot manner. To our knowledge, this is the first work to derive and deploy an RL-based policy using collective thrust and body rates control commands for multi-UAV pursuit-evasion in unknown environments. The open-source code and videos are available at https://sites.google.com/view/pursuit-evasion-rl."
      },
      {
        "id": "oai:arXiv.org:2409.18162v3",
        "title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review",
        "link": "https://arxiv.org/abs/2409.18162",
        "author": "Biplov Paneru",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2409.18162v3 Announce Type: replace-cross \nAbstract: The emergence of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children, especially with disorders like autism spectrum disorder (ASD), is studied in detail in this review study. 150 publications were collected by a thorough literature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 60 of them were chosen based on their methodological rigor and relevance to the focus area. Three of the primary areas are studied and covered in this review: how AR can improve social and learning results, how LLMs can support communication, and how UI/UX design affects how effective these technologies can be. Results show that while LLMs can provide individualized learning and communication support, AR has shown promise in enhancing social skills, motivation, and attention. For children with ASD, accessible and engaging interventions rely heavily on effective UI/UX design, but there is still a significant lack of robotics-based education and therapeutic programs specifically tailored for autistic children. To optimize the benefits of these technologies in ASD therapies and immersive education, the study emphasizes the need for additional research to address difficulties related to customization, accessibility, and integration."
      },
      {
        "id": "oai:arXiv.org:2410.02892v3",
        "title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
        "link": "https://arxiv.org/abs/2410.02892",
        "author": "Chengkun Cai, Xu Zhao, Haoliang Liu, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Lei Li",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02892v3 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning tasks, yet their reliance on static prompt structures and limited adaptability to complex scenarios remains a significant challenge. In this paper, we propose the Deductive and InDuctive(DID) method, a novel framework that enhances LLM reasoning by dynamically integrating both deductive and inductive reasoning approaches. Drawing from cognitive science principles, DID implements a dual-metric complexity evaluation system that combines Littlestone dimension and information entropy to precisely assess task difficulty and guide decomposition strategies. DID enables the model to progressively adapt its reasoning pathways based on problem complexity, mirroring human cognitive processes. We evaluate DID's effectiveness across multiple benchmarks, including the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset for temporal reasoning. Our results demonstrate significant improvements in reasoning quality and solution accuracy - achieving 70.3% accuracy on AIW (compared to 62.2% for Tree of Thought) while maintaining lower computational costs. The success of DID in improving LLM performance while preserving computational efficiency suggests promising directions for developing more cognitively aligned and capable language models. Our work contributes a theoretically grounded, input-centric approach to enhancing LLM reasoning capabilities, offering an efficient alternative to traditional output-exploration methods."
      },
      {
        "id": "oai:arXiv.org:2410.06949v3",
        "title": "Towards Exception Safety Code Generation with Intermediate Representation Agents Framework",
        "link": "https://arxiv.org/abs/2410.06949",
        "author": "Xuanming Zhang, Yuxuan Chen, Yuan Yuan, Minlie Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2410.06949v3 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) often struggle with robust exception handling in generated code, leading to fragile programs that are prone to runtime errors. We propose Seeker, a novel multi-agent framework that enforces exception safety in LLM generated code through an Intermediate Representation (IR) approach. Seeker decomposes exception handling into five specialized agents: Scanner, Detector, Predator, Ranker, and Handler that collaboratively analyze code, detect fragile segments, retrieve best practice exception strategies, and inject robust handling code. We also introduce Common Exception Enumeration (CEE), a comprehensive knowledge base derived from official documentation, technical practices, and real world code, to standardize exception handling strategies. Seeker also incorporates a Deep Retrieval-Augmented Generation (Deep RAG) algorithm to efficiently navigate the exception inheritance hierarchy, cutting down search overhead by 93% while improving accuracy in identifying relevant exceptions. We evaluate Seeker on 15 open source Java projects and multiple benchmarks. Seeker outperforms state of the art baselines, improving exception handling precision by up to 37% and overall code robustness by 38% as measured by expert code review. It significantly closes the gap between LLM and human developers in exception management, achieving a 28% success rate on real world issue fixes (SWE bench) versus 19% by prior methods. Our framework preserves functional correctness of code while proactively handling errors, demonstrating a practical, generalizable solution for safer code generation. In this paper, we discuss the novelty of using intermediate representation and multi-agent collaboration for exception handling, and outline how Seeker can be extended to other programming languages and complex software engineering tasks, aligning LLM-generated code with industrial standard."
      },
      {
        "id": "oai:arXiv.org:2410.08194v2",
        "title": "Features are fate: a theory of transfer learning in high-dimensional regression",
        "link": "https://arxiv.org/abs/2410.08194",
        "author": "Javan Tahir, Surya Ganguli, Grant M. Rotskoff",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08194v2 Announce Type: replace-cross \nAbstract: With the emergence of large-scale pre-trained neural networks, methods to adapt such \"foundation\" models to data-limited downstream tasks have become a necessity. Fine-tuning, preference optimization, and transfer learning have all been successfully employed for these purposes when the target task closely resembles the source task, but a precise theoretical understanding of \"task similarity\" is still lacking. While conventional wisdom suggests that simple measures of similarity between source and target distributions, such as $\\phi$-divergences or integral probability metrics, can directly predict the success of transfer, we prove the surprising fact that, in general, this is not the case. We adopt, instead, a feature-centric viewpoint on transfer learning and establish a number of theoretical results that demonstrate that when the target task is well represented by the feature space of the pre-trained model, transfer learning outperforms training from scratch. We study deep linear networks as a minimal model of transfer learning in which we can analytically characterize the transferability phase diagram as a function of the target dataset size and the feature space overlap. For this model, we establish rigorously that when the feature space overlap between the source and target tasks is sufficiently strong, both linear transfer and fine-tuning improve performance, especially in the low data limit. These results build on an emerging understanding of feature learning dynamics in deep linear networks, and we demonstrate numerically that the rigorous results we derive for the linear case also apply to nonlinear networks."
      },
      {
        "id": "oai:arXiv.org:2410.16327v2",
        "title": "Feint and Attack: Attention-Based Strategies for Jailbreaking and Protecting LLMs",
        "link": "https://arxiv.org/abs/2410.16327",
        "author": "Rui Pu, Chaozhuo Li, Rui Ha, Zejian Chen, Litian Zhang, Zheng Liu, Lirong Qiu, Zaisheng Ye",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2410.16327v2 Announce Type: replace-cross \nAbstract: Jailbreak attack can be used to access the vulnerabilities of Large Language Models (LLMs) by inducing LLMs to generate the harmful content. And the most common method of the attack is to construct semantically ambiguous prompts to confuse and mislead the LLMs. To access the security and reveal the intrinsic relation between the input prompt and the output for LLMs, the distribution of attention weight is introduced to analyze the underlying reasons. By using statistical analysis methods, some novel metrics are defined to better describe the distribution of attention weight, such as the Attention Intensity on Sensitive Words (Attn_SensWords), the Attention-based Contextual Dependency Score (Attn_DepScore) and Attention Dispersion Entropy (Attn_Entropy). By leveraging the distinct characteristics of these metrics, the beam search algorithm and inspired by the military strategy \"Feint and Attack\", an effective jailbreak attack strategy named as Attention-Based Attack (ABA) is proposed. In the ABA, nested attack prompts are employed to divert the attention distribution of the LLMs. In this manner, more harmless parts of the input can be used to attract the attention of the LLMs. In addition, motivated by ABA, an effective defense strategy called as Attention-Based Defense (ABD) is also put forward. Compared with ABA, the ABD can be used to enhance the robustness of LLMs by calibrating the attention distribution of the input prompt. Some comparative experiments have been given to demonstrate the effectiveness of ABA and ABD. Therefore, both ABA and ABD can be used to access the security of the LLMs. The comparative experiment results also give a logical explanation that the distribution of attention weight can bring great influence on the output for LLMs."
      },
      {
        "id": "oai:arXiv.org:2411.01746v2",
        "title": "Entropy stable conservative flux form neural networks",
        "link": "https://arxiv.org/abs/2411.01746",
        "author": "Lizuo Liu, Tongtong Li, Anne Gelb, Yoonsang Lee",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.01746v2 Announce Type: replace-cross \nAbstract: We propose an entropy-stable conservative flux form neural network (CFN) that integrates classical numerical conservation laws into a data-driven framework using the entropy-stable, second-order, and non-oscillatory Kurganov-Tadmor (KT) scheme. The proposed entropy-stable CFN uses slope limiting as a denoising mechanism, ensuring accurate predictions in both noisy and sparse observation environments, as well as in both smooth and discontinuous regions. Numerical experiments demonstrate that the entropy-stable CFN achieves both stability and conservation while maintaining accuracy over extended time domains. Furthermore, it successfully predicts shock propagation speeds in long-term simulations, {\\it without} oracle knowledge of later-time profiles in the training data."
      },
      {
        "id": "oai:arXiv.org:2411.01866v2",
        "title": "Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales",
        "link": "https://arxiv.org/abs/2411.01866",
        "author": "Resul Dagdanov, Milan Andrejevic, Dikai Liu, Chin-Teng Lin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.01866v2 Announce Type: replace-cross \nAbstract: When interacting with each other, humans adjust their behavior based on perceived trust. To achieve similar adaptability, robots must accurately estimate human trust at sufficiently granular timescales while collaborating with humans. Beta reputation is a popular way to formalize a mathematical estimation of human trust. However, it relies on binary performance, which updates trust estimations only after each task concludes. Additionally, manually crafting a reward function is the usual method of building a performance indicator, which is labor-intensive and time-consuming. These limitations prevent efficient capture of continuous trust changes at more granular timescales throughout the collaboration task. Therefore, this paper presents a new framework for the estimation of human trust using beta reputation at fine-grained timescales. To achieve granularity in beta reputation, we utilize continuous reward values to update trust estimates at each timestep of a task. We construct a continuous reward function using maximum entropy optimization to eliminate the need for the laborious specification of a performance indicator. The proposed framework improves trust estimations by increasing accuracy, eliminating the need to manually craft a reward function, and advancing toward the development of more intelligent robots."
      },
      {
        "id": "oai:arXiv.org:2411.18148v3",
        "title": "A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs",
        "link": "https://arxiv.org/abs/2411.18148",
        "author": "Ehsan Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2411.18148v3 Announce Type: replace-cross \nAbstract: Transformer neural networks (TNN) excel in natural language processing (NLP), machine translation, and computer vision (CV) without relying on recurrent or convolutional layers. However, they have high computational and memory demands, particularly on resource-constrained devices like FPGAs. Moreover, transformer models vary in processing time across applications, requiring custom models with specific parameters. Designing custom accelerators for each model is complex and time-intensive. Some custom accelerators exist with no runtime adaptability, and they often rely on sparse matrices to reduce latency. However, hardware designs become more challenging due to the need for application-specific sparsity patterns. This paper introduces ADAPTOR, a runtime-adaptive accelerator for dense matrix computations in transformer encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing elements and on-chip memory, enhancing parallelism and reducing latency. It incorporates efficient matrix tiling to distribute resources across FPGA platforms and is fully quantized for computational efficiency and portability. Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like VC707 and ZCU102 show that our design is 1.2$\\times$ and 2.87$\\times$ more power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively. Additionally, it achieves a speedup of 1.7 to 2.25$\\times$ compared to some state-of-the-art FPGA-based accelerators."
      },
      {
        "id": "oai:arXiv.org:2412.09331v2",
        "title": "Physics-Driven Autoregressive State Space Models for Medical Image Reconstruction",
        "link": "https://arxiv.org/abs/2412.09331",
        "author": "Bilal Kabas, Fuat Arslan, Valiyeh A. Nezhad, Saban Ozturk, Emine U. Saritas, Tolga \\c{C}ukur",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.09331v2 Announce Type: replace-cross \nAbstract: Medical image reconstruction from undersampled acquisitions is an ill-posed problem involving inversion of the imaging operator linking measurement and image domains. Physics-driven (PD) models have gained prominence in reconstruction tasks due to their desirable performance and generalization. These models jointly promote data fidelity and artifact suppression, typically by combining data-consistency mechanisms with learned network modules. Artifact suppression depends on the network's ability to disentangle artifacts from true tissue signals, both of which can exhibit contextual structure across diverse spatial scales. Convolutional neural networks (CNNs) are strong in capturing local correlations, albeit relatively insensitive to non-local context. While transformers promise to alleviate this limitation, practical implementations frequently involve design compromises to reduce computational cost by balancing local and non-local sensitivity, occasionally resulting in performance comparable to or trailing that of CNNs. To enhance contextual sensitivity without incurring high complexity, we introduce a novel physics-driven autoregressive state-space model (MambaRoll) for medical image reconstruction. In each cascade of its unrolled architecture, MambaRoll employs a physics-driven state-space module (PD-SSM) to aggregate contextual features efficiently at a given spatial scale, and autoregressively predicts finer-scale feature maps conditioned on coarser-scale features to capture multi-scale context. Learning across scales is further enhanced via a deep multi-scale decoding (DMSD) loss tailored to the autoregressive prediction task. Demonstrations on accelerated MRI and sparse-view CT reconstructions show that MambaRoll consistently outperforms state-of-the-art data-driven and physics-driven methods based on CNN, transformer, and SSM backbones."
      },
      {
        "id": "oai:arXiv.org:2412.20545v2",
        "title": "The Impact of Prompt Programming on Function-Level Code Generation",
        "link": "https://arxiv.org/abs/2412.20545",
        "author": "Ranim Khojah, Francisco Gomes de Oliveira Neto, Mazen Mohamad, Philipp Leitner",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2412.20545v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are increasingly used by software engineers for code generation. However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code. While some prompt techniques have been studied, the impact of different techniques -- and their interactions -- on code generation is still not fully understood. In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome. Additionally, we observed a trade-off between correctness and quality when using prompt techniques. Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques."
      },
      {
        "id": "oai:arXiv.org:2501.02547v2",
        "title": "Transformers Simulate MLE for Sequence Generation in Bayesian Networks",
        "link": "https://arxiv.org/abs/2501.02547",
        "author": "Yuan Cao, Yihan He, Dennis Wu, Hong-Yu Chen, Jianqing Fan, Han Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.02547v2 Announce Type: replace-cross \nAbstract: Transformers have achieved significant success in various fields, notably excelling in tasks involving sequential data like natural language processing. Despite these achievements, the theoretical understanding of transformers' capabilities remains limited. In this paper, we investigate the theoretical capabilities of transformers to autoregressively generate sequences in Bayesian networks based on in-context maximum likelihood estimation (MLE). Specifically, we consider a setting where a context is formed by a set of independent sequences generated according to a Bayesian network. We demonstrate that there exists a simple transformer model that can (i) estimate the conditional probabilities of the Bayesian network according to the context, and (ii) autoregressively generate a new sample according to the Bayesian network with estimated conditional probabilities. We further demonstrate in extensive experiments that such a transformer does not only exist in theory, but can also be effectively obtained through training. Our analysis highlights the potential of transformers to learn complex probabilistic models and contributes to a better understanding of large language models as a powerful class of sequence generators."
      },
      {
        "id": "oai:arXiv.org:2501.06108v4",
        "title": "Inferring Higher-Order Couplings with Neural Networks",
        "link": "https://arxiv.org/abs/2501.06108",
        "author": "Aur\\'elien Decelle, Alfonso de Jes\\'us Navas G\\'omez, Beatriz Seoane",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06108v4 Announce Type: replace-cross \nAbstract: Maximum entropy methods, rooted in the inverse Ising/Potts problem from statistical physics, are widely used to model pairwise interactions in complex systems across disciplines such as bioinformatics and neuroscience. While successful, these approaches often fail to capture higher-order interactions that are critical for understanding collective behavior. In contrast, modern machine learning methods can model such interactions, but their interpretability often comes at a prohibitive computational cost. Restricted Boltzmann Machines (RBMs) provide a computationally efficient alternative by encoding statistical correlations through hidden units in a bipartite architecture. In this work, we introduce a method that maps RBMs onto generalized Potts models, enabling the systematic extraction of interactions up to arbitrary order. Leveraging large-$N$ approximations, made tractable by the RBM's structure, we extract effective many-body couplings with minimal computational effort. We further propose a robust framework for recovering higher-order interactions in more complex generative models, and introduce a simple gauge-fixing scheme for the effective Potts representation. Validation on synthetic data demonstrates accurate recovery of two- and three-body interactions. Applied to protein sequence data, our method reconstructs contact maps with high fidelity and outperforms state-of-the-art inverse Potts models. These results establish RBMs as a powerful and efficient tool for modeling higher-order structure in high-dimensional categorical data."
      },
      {
        "id": "oai:arXiv.org:2501.08667v2",
        "title": "TimeFlow: Longitudinal Brain Image Registration and Aging Progression Analysis",
        "link": "https://arxiv.org/abs/2501.08667",
        "author": "Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.08667v2 Announce Type: replace-cross \nAbstract: Predicting future brain states is crucial for understanding healthy aging and neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone for such analyses, has long been limited by its inability to forecast future developments, reliance on extensive dense longitudinal data, and the need to balance registration accuracy with temporal smoothness. In this work, we present \\emph{TimeFlow}, a novel framework for longitudinal brain MRI registration that overcomes all these challenges. TimeFlow leverages a U-Net architecture with temporal conditioning inspired by diffusion models, enabling accurate registration using only two images as input and facilitating prospective analyses through future image prediction. Unlike traditional methods, TimeFlow eliminates the demand for explicit smoothness regularizers and dense sequential data while maintaining temporal consistency and continuity. Experimental results highlight its superior performance in both future timepoint prediction and registration accuracy compared to state-of-the-art methods. Additionally, TimeFlow supports novel biological brain aging analyses, effectively differentiating neurodegenerative conditions from healthy aging, all without requiring segmentation, thus avoiding non-trivial annotation and inconsistent segmentation flaws. This framework paves the way for accurate, data-efficient, and annotation-free prospective analyses of brain aging and chronic diseases."
      },
      {
        "id": "oai:arXiv.org:2501.18099v2",
        "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge",
        "link": "https://arxiv.org/abs/2501.18099",
        "author": "Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2501.18099v2 Announce Type: replace-cross \nAbstract: LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-bystep reasoning process that underlies the final evaluation of a response. However, due to the lack of human annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench (with a score of 93.9), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models."
      },
      {
        "id": "oai:arXiv.org:2502.00406v2",
        "title": "Agents Are All You Need for LLM Unlearning",
        "link": "https://arxiv.org/abs/2502.00406",
        "author": "Debdeep Sanyal, Murari Mandal",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00406v2 Announce Type: replace-cross \nAbstract: Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. In this work we show that \\textit{agents might be all we need for effective and practical inference-time LLM unlearning}. We present the first agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and \\texttt{ALU} seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that \\texttt{ALU} consistently stands out as the most robust inference-time LLM unlearning framework among current state-of-the-art methods while incurring time cost that remains effectively constant regardless of the number of unlearning targets. We further highlight \\texttt{ALU}'s superior performance compared to existing methods when evaluated at scale. Specifically, \\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods."
      },
      {
        "id": "oai:arXiv.org:2502.01932v4",
        "title": "VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play",
        "link": "https://arxiv.org/abs/2502.01932",
        "author": "Zelai Xu, Ruize Zhang, Chao Yu, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Feng Gao, Wenbo Ding, Xinlei Chen, Yu Wang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01932v4 Announce Type: replace-cross \nAbstract: Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. Competitive and cooperative gameplay challenges each drone to coordinate with its teammates while anticipating and countering opposing teams' tactics. Turn-based interaction demands precise timing, accurate state prediction, and management of long-horizon temporal dependencies. Agile 3D maneuvering requires rapid accelerations, sharp turns, and precise 3D positioning despite the quadrotor's underactuated dynamics. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy reinforcement learning (RL) methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves a 69.5% percent win rate against the strongest baseline in the 3 vs 3 task, underscoring its potential as an effective solution for tackling the complex interplay between low-level control and high-level strategy. The project page is at https://sites.google.com/view/thu-volleybots."
      },
      {
        "id": "oai:arXiv.org:2502.12793v2",
        "title": "Unsupervised Anomaly Detection through Mass Repulsing Optimal Transport",
        "link": "https://arxiv.org/abs/2502.12793",
        "author": "Eduardo Fernandes Montesuma, Adel El Habazi, Fred Ngole Mboula",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12793v2 Announce Type: replace-cross \nAbstract: Detecting anomalies in datasets is a longstanding problem in machine learning. In this context, anomalies are defined as a sample that significantly deviates from the remaining data. Meanwhile, optimal transport (OT) is a field of mathematics concerned with the transportation, between two probability measures, at least effort. In classical OT, the optimal transportation strategy of a measure to itself is the identity. In this paper, we tackle anomaly detection by forcing samples to displace its mass, while keeping the least effort objective. We call this new transportation problem Mass Repulsing Optimal Transport (MROT). Naturally, samples lying in low density regions of space will be forced to displace mass very far, incurring a higher transportation cost. We use these concepts to design a new anomaly score. Through a series of experiments in existing benchmarks, and fault detection problems, we show that our algorithm improves over existing methods."
      },
      {
        "id": "oai:arXiv.org:2502.12961v2",
        "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
        "link": "https://arxiv.org/abs/2502.12961",
        "author": "Wenjun Li, Dexun Li, Kuicai Dong, Cong Zhang, Hao Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Liu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.12961v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or up-to-date data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, calculators), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues: increased latency due to unnecessary tool calls, and potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, reflecting the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Experiments across multiple backbone models and benchmarks show that MeCo reliably detects LLMs' internal cognitive signals and significantly improves tool-use decision-making."
      },
      {
        "id": "oai:arXiv.org:2502.17380v3",
        "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
        "link": "https://arxiv.org/abs/2502.17380",
        "author": "Qiuming Zhao, Guangzhi Sun, Chao Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17380v3 Announce Type: replace-cross \nAbstract: Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-lingual multi-task training approaches aim to address this by jointly optimising multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language interference, and enhancing extensibility. Experimental results across 10 languages demonstrate that LoRS-Merging significantly outperforms multi-lingual multi-task training, sequential training, and other merging methods, achieving over 20% improvement in normalised performance. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications."
      },
      {
        "id": "oai:arXiv.org:2502.20423v2",
        "title": "Efficient Risk-sensitive Planning via Entropic Risk Measures",
        "link": "https://arxiv.org/abs/2502.20423",
        "author": "Alexandre Marthe (ENS de Lyon, UMPA-ENSL), Samuel Bounan (UMPA-ENSL, MC2), Aur\\'elien Garivier (UMPA-ENSL, MC2), Claire Vernade",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20423v2 Announce Type: replace-cross \nAbstract: Risk-sensitive planning aims to identify policies maximizing some tail-focused metrics in Markov Decision Processes (MDPs). Such an optimization task can be very costly for the most widely used and interpretable metrics such as threshold probabilities or (Conditional) Values at Risk. Indeed, previous work showed that only Entropic Risk Measures (EntRM) can be efficiently optimized through dynamic programming, leaving a hard-to-interpret parameter to choose. We show that the computation of the full set of optimal policies for EntRM across parameter values leads to tight approximations for the metrics of interest. We prove that this optimality front can be computed effectively thanks to a novel structural analysis and smoothness properties of entropic risks. Empirical results demonstrate that our approach achieves strong performance in a variety of decision-making scenarios."
      },
      {
        "id": "oai:arXiv.org:2503.11947v2",
        "title": "Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance",
        "link": "https://arxiv.org/abs/2503.11947",
        "author": "Austin Shouli, Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.11947v2 Announce Type: replace-cross \nAbstract: The rapid expansion of Artificial Intelligence (AI) in digital platforms used by youth has created significant challenges related to privacy, autonomy, and data protection. While AI-driven personalization offers enhanced user experiences, it often operates without clear ethical boundaries, leaving young users vulnerable to data exploitation and algorithmic biases. This paper presents a call to action for ethical AI governance, advocating for a structured framework that ensures youth-centred privacy protections, transparent data practices, and regulatory oversight. We outline key areas requiring urgent intervention, including algorithmic transparency, privacy education, parental data-sharing ethics, and accountability measures. Through this approach, we seek to empower youth with greater control over their digital identities and propose actionable strategies for policymakers, AI developers, and educators to build a fairer and more accountable AI ecosystem."
      },
      {
        "id": "oai:arXiv.org:2503.22968v4",
        "title": "Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models",
        "link": "https://arxiv.org/abs/2503.22968",
        "author": "Hanwool Lee, Dasol Choi, Sooyong Kim, Ilgyun Jung, Sangwon Baek, Guijin Son, Inseon Hwang, Naeun Lee, Seunghyeok Hong",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22968v4 Announce Type: replace-cross \nAbstract: Recent advancements in Korean large language models (LLMs) have driven numerous benchmarks and evaluation methods, yet inconsistent protocols cause up to 10 p.p performance gaps across institutions. Overcoming these reproducibility gaps does not mean enforcing a one-size-fits-all evaluation. Rather, effective benchmarking requires diverse experimental approaches and a framework robust enough to support them. To this end, we introduce HRET (Haerae Evaluation Toolkit), an open-source, registry-based framework that unifies Korean LLM assessment. HRET integrates major Korean benchmarks, multiple inference backends, and multi-method evaluation, with language consistency enforcement to ensure genuine Korean outputs. Its modular registry design also enables rapid incorporation of new datasets, methods, and backends, ensuring the toolkit adapts to evolving research needs. Beyond standard accuracy metrics, HRET incorporates Korean-focused output analyses-morphology-aware Type-Token Ratio (TTR) for evaluating lexical diversity and systematic keyword-omission detection for identifying missing concepts-to provide diagnostic insights into language-specific behaviors. These targeted analyses help researchers pinpoint morphological and semantic shortcomings in model outputs, guiding focused improvements in Korean LLM development."
      },
      {
        "id": "oai:arXiv.org:2504.01757v2",
        "title": "KD$^{2}$M: A unifying framework for feature knowledge distillation",
        "link": "https://arxiv.org/abs/2504.01757",
        "author": "Eduardo Fernandes Montesuma",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01757v2 Announce Type: replace-cross \nAbstract: Knowledge Distillation (KD) seeks to transfer the knowledge of a teacher, towards a student neural net. This process is often done by matching the networks' predictions (i.e., their output), but, recently several works have proposed to match the distributions of neural nets' activations (i.e., their features), a process known as \\emph{distribution matching}. In this paper, we propose an unifying framework, Knowledge Distillation through Distribution Matching (KD$^{2}$M), which formalizes this strategy. Our contributions are threefold. We i) provide an overview of distribution metrics used in distribution matching, ii) benchmark on computer vision datasets, and iii) derive new theoretical results for KD."
      },
      {
        "id": "oai:arXiv.org:2504.08773v2",
        "title": "Counterfactual Inference under Thompson Sampling",
        "link": "https://arxiv.org/abs/2504.08773",
        "author": "Olivier Jeunen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08773v2 Announce Type: replace-cross \nAbstract: Recommender systems exemplify sequential decision-making under uncertainty, strategically deciding what content to serve to users, to optimise a range of potential objectives. To balance the explore-exploit trade-off successfully, Thompson sampling provides a natural and widespread paradigm to probabilistically select which action to take. Questions of causal and counterfactual inference, which underpin use-cases like offline evaluation, are not straightforward to answer in these contexts. Specifically, whilst most existing estimators rely on action propensities, these are not readily available under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action propensities under a variety of parameter and outcome distributions, enabling the use of off-policy estimators in Thompson sampling scenarios. This opens up a range of practical use-cases where counterfactual inference is crucial, including unbiased offline evaluation of recommender systems, as well as general applications of causal inference in online advertising, personalisation, and beyond."
      },
      {
        "id": "oai:arXiv.org:2504.08814v2",
        "title": "When Federated Learning Meets Quantum Computing: Survey and Research Opportunities",
        "link": "https://arxiv.org/abs/2504.08814",
        "author": "Aakar Mathur, Ashish Gupta, Sajal K. Das",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.08814v2 Announce Type: replace-cross \nAbstract: Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. This work explores key developments and integration strategies, along with the impact of quantum computing on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use."
      },
      {
        "id": "oai:arXiv.org:2504.11286v2",
        "title": "Lightweight Medical Image Restoration via Integrating Reliable Lesion-Semantic Driven Prior",
        "link": "https://arxiv.org/abs/2504.11286",
        "author": "Pengcheng Zheng, Kecheng Chen, Jiaxin Huang, Bohao Chen, Ju Liu, Yazhou Ren, Xiaorong Pu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2504.11286v2 Announce Type: replace-cross \nAbstract: Medical image restoration tasks aim to recover high-quality images from degraded observations, exhibiting emergent desires in many clinical scenarios, such as low-dose CT image denoising, MRI super-resolution, and MRI artifact removal. Despite the success achieved by existing deep learning-based restoration methods with sophisticated modules, they struggle with rendering computationally-efficient reconstruction results. Moreover, they usually ignore the reliability of the restoration results, which is much more urgent in medical systems. To alleviate these issues, we present LRformer, a Lightweight Transformer-based method via Reliability-guided learning in the frequency domain. Specifically, inspired by the uncertainty quantification in Bayesian neural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer (RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling operations to generate sufficiently-reliable priors by performing multiple inferences on the foundational medical image segmentation model, MedSAM. Additionally, instead of directly incorporating the priors in the spatial domain, we decompose the cross-attention (CA) mechanism into real symmetric and imaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in the design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging the conjugated symmetric property of FFT, GFCA reduces the computational complexity of naive CA by nearly half. Extensive experimental results in various tasks demonstrate the superiority of the proposed LRformer in both effectiveness and efficiency."
      },
      {
        "id": "oai:arXiv.org:2505.03729v4",
        "title": "Visual Imitation Enables Contextual Humanoid Control",
        "link": "https://arxiv.org/abs/2505.03729",
        "author": "Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.03729v4 Announce Type: replace-cross \nAbstract: How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments."
      },
      {
        "id": "oai:arXiv.org:2505.04648v2",
        "title": "Quantum QSAR for drug discovery",
        "link": "https://arxiv.org/abs/2505.04648",
        "author": "Alejandro Giraldo, Daniel Ruiz, Mariano Caruso, Guido Bellomo",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.04648v2 Announce Type: replace-cross \nAbstract: Quantitative Structure-Activity Relationship (QSAR) modeling is key in drug discovery, but classical methods face limitations when handling high-dimensional data and capturing complex molecular interactions. This research proposes enhancing QSAR techniques through Quantum Support Vector Machines (QSVMs), which leverage quantum computing principles to process information Hilbert spaces. By using quantum data encoding and quantum kernel functions, we aim to develop more accurate and efficient predictive models."
      },
      {
        "id": "oai:arXiv.org:2505.06386v2",
        "title": "Embedding Atlas: Low-Friction, Interactive Embedding Visualization",
        "link": "https://arxiv.org/abs/2505.06386",
        "author": "Donghao Ren, Fred Hohman, Halden Lin, Dominik Moritz",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.06386v2 Announce Type: replace-cross \nAbstract: Embedding projections are popular for visualizing large datasets and models. However, people often encounter \"friction\" when using embedding visualization tools: (1) barriers to adoption, e.g., tedious data wrangling and loading, scalability limits, no integration of results into existing workflows, and (2) limitations in possible analyses, without integration with external tools to additionally show coordinated views of metadata. In this paper, we present Embedding Atlas, a scalable, interactive visualization tool designed to make interacting with large embeddings as easy as possible. Embedding Atlas uses modern web technologies and advanced algorithms -- including density-based clustering, and automated labeling -- to provide a fast and rich data analysis experience at scale. We evaluate Embedding Atlas with a competitive analysis against other popular embedding tools, showing that Embedding Atlas's feature set specifically helps reduce friction, and report a benchmark on its real-time rendering performance with millions of points. Embedding Atlas is available as open source to support future work in embedding-based analysis."
      },
      {
        "id": "oai:arXiv.org:2505.11079v2",
        "title": "ALLM4ADD: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection",
        "link": "https://arxiv.org/abs/2505.11079",
        "author": "Hao Gu, Jiangyan Yi, Chenglong Wang, Jianhua Tao, Zheng Lian, Jiayi He, Yong Ren, Yujie Chen, Zhengqi Wen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.11079v2 Announce Type: replace-cross \nAbstract: Audio deepfake detection (ADD) has grown increasingly important due to the rise of high-fidelity audio generative models and their potential for misuse. Given that audio large language models (ALLMs) have made significant progress in various audio processing tasks, a heuristic question arises: \\textit{Can ALLMs be leveraged to solve ADD?}. In this paper, we first conduct a comprehensive zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness. To this end, we propose ALLM4ADD, an ALLM-driven framework for ADD. Specifically, we reformulate ADD task as an audio question answering problem, prompting the model with the question: ``Is this audio fake or real?''. We then perform supervised fine-tuning to enable the ALLM to assess the authenticity of query audio. Extensive experiments are conducted to demonstrate that our ALLM-based method can achieve superior performance in fake audio detection, particularly in data-scarce scenarios. As a pioneering study, we anticipate that this work will inspire the research community to leverage ALLMs to develop more effective ADD systems. Code is available at https://github.com/ucas-hao/qwen_audio_for_add.git"
      },
      {
        "id": "oai:arXiv.org:2505.12519v2",
        "title": "Efficient Implementation of Gaussian Process Regression Accelerated Saddle Point Searches with Application to Molecular Reactions",
        "link": "https://arxiv.org/abs/2505.12519",
        "author": "Rohit Goswami (Science Institute and Faculty of Physical Sciences, University of Iceland, Reykjav\\'ik, Iceland), Maxim Masterov (SURF, Amsterdam, The Netherlands), Satish Kamath (SURF, Amsterdam, The Netherlands), Alejandro Pe\\~na-Torres (Science Institute and Faculty of Physical Sciences, University of Iceland, Reykjav\\'ik, Iceland), Hannes J\\'onsson (Science Institute and Faculty of Physical Sciences, University of Iceland, Reykjav\\'ik, Iceland)",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.12519v2 Announce Type: replace-cross \nAbstract: The task of locating first order saddle points on high-dimensional surfaces describing the variation of energy as a function of atomic coordinates is an essential step for identifying the mechanism and estimating the rate of thermally activated events within the harmonic approximation of transition state theory. When combined directly with electronic structure calculations, the number of energy and atomic force evaluations needed for convergence is a primary issue. Here, we describe an efficient implementation of Gaussian process regression (GPR) acceleration of the minimum mode following method where a dimer is used to estimate the lowest eigenmode of the Hessian. A surrogate energy surface is constructed and updated after each electronic structure calculation. The method is applied to a test set of 500 molecular reactions previously generated by Hermez and coworkers [J. Chem. Theory Comput. 18, 6974 (2022)]. An order of magnitude reduction in the number of electronic structure calculations needed to reach the saddle point configurations is obtained by using the GPR compared to the dimer method. Despite the wide range in stiffness of the molecular degrees of freedom, the calculations are carried out using Cartesian coordinates and are found to require similar number of electronic structure calculations as an elaborate internal coordinate method implemented in the Sella software package. The present implementation of the GPR surrogate model in C++ is efficient enough for the wall time of the saddle point searches to be reduced in 3 out of 4 cases even though the calculations are carried out at a low Hartree-Fock level."
      },
      {
        "id": "oai:arXiv.org:2505.12578v3",
        "title": "Stacked conformal prediction",
        "link": "https://arxiv.org/abs/2505.12578",
        "author": "Paulo C. Marques F",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.12578v3 Announce Type: replace-cross \nAbstract: We consider a method for conformalizing a stacked ensemble of predictive models, showing that the potentially simple form of the meta-learner at the top of the stack enables a procedure with manageable computational cost that achieves approximate marginal validity without requiring the use of a separate calibration sample. Empirical results indicate that the method compares favorably to a standard inductive alternative."
      },
      {
        "id": "oai:arXiv.org:2505.19840v2",
        "title": "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP",
        "link": "https://arxiv.org/abs/2505.19840",
        "author": "Binyan Xu, Xilin Dai, Di Tang, Kehuan Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.19840v2 Announce Type: replace-cross \nAbstract: Deep Neural Networks (DNNs) have achieved widespread success yet remain prone to adversarial attacks. Typically, such attacks either involve frequent queries to the target model or rely on surrogate models closely mirroring the target model -- often trained with subsets of the target model's training data -- to achieve high attack success rates through transferability. However, in realistic scenarios where training data is inaccessible and excessive queries can raise alarms, crafting adversarial examples becomes more challenging. In this paper, we present UnivIntruder, a novel attack framework that relies solely on a single, publicly available CLIP model and publicly available datasets. By using textual concepts, UnivIntruder generates universal, transferable, and targeted adversarial perturbations that mislead DNNs into misclassifying inputs into adversary-specified classes defined by textual concepts.\n  Our extensive experiments show that our approach achieves an Attack Success Rate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly outperforming existing transfer-based methods. Additionally, we reveal real-world vulnerabilities, showing that even without querying target models, UnivIntruder compromises image search engines like Google and Baidu with ASR rates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR rates up to 80%. These findings underscore the practicality of our attack in scenarios where traditional avenues are blocked, highlighting the need to reevaluate security paradigms in AI applications."
      },
      {
        "id": "oai:arXiv.org:2505.24132v2",
        "title": "Information-theoretic machine learning for time-varying mode decomposition of separated aerodynamic flows",
        "link": "https://arxiv.org/abs/2505.24132",
        "author": "Kai Fukami, Ryo Araki",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.24132v2 Announce Type: replace-cross \nAbstract: We perform an information-theoretic mode decomposition for separated aerodynamic flows. The current data-driven approach based on a neural network referred to as deep sigmoidal flow enables the extraction of an informative component from a given flow field snapshot with respect to a target variable at a future time stamp, thereby capturing the causality as a time-varying modal structure. We consider four examples of separated flows around a wing, namely, 1. laminar periodic wake at post-stall angles of attack, strong gust-wing interactions of 2. numerical and 3. experimental measurements, and 4. a turbulent wake in a spanwise-periodic domain. The present approach reveals informative vortical structures associated with a time-varying lift response. For the periodic shedding cases, the informative structures vary in time corresponding to the fluctuation level from their mean values. With the examples of gust-wing interactions, how the effect of gust on a wing emerges in the lift response over time is identified in an interpretable manner. Furthermore, for the case of turbulent wake, the present model highlights structures near the wing and vortex cores as informative components based solely on the information metric without any prior knowledge of aerodynamics and length scales. This study provides causality-based insights into a range of unsteady aerodynamic problems."
      },
      {
        "id": "oai:arXiv.org:2506.00180v2",
        "title": "Empirical Validation of the Independent Chip Model",
        "link": "https://arxiv.org/abs/2506.00180",
        "author": "Juho Kim",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.00180v2 Announce Type: replace-cross \nAbstract: The independent chip model (ICM) forms a cornerstone of all modern poker tournament strategy. However, despite its prominence, the ICM's performance in the real world has not been sufficiently scrutinized, especially at a large scale. In this paper, we introduce our new dataset of poker tournaments, consisting of results of over ten thousand events. Then, using this dataset, we perform two experiments as part of a large-scale empirical validation of the ICM. First, we verify that the ICM performs more accurately than a baseline we propose. Second, we obtain empirical evidence of the ICM underestimating the performances of players with larger stacks while overestimating those who are short-stacked. Our contributions may be useful to future researchers developing new algorithms for estimating a player's value in poker tournaments."
      },
      {
        "id": "oai:arXiv.org:2506.06382v3",
        "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models",
        "link": "https://arxiv.org/abs/2506.06382",
        "author": "Micha{\\l} P. Karpowicz",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.06382v3 Announce Type: replace-cross \nAbstract: We prove that perfect hallucination control in large language models is mathematically impossible. No LLM inference mechanism can simultaneously achieve truthful response generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality. This impossibility is fundamental, arising from the mathematical structure of information aggregation itself rather than engineering limitations. The proof spans three mathematical frameworks: auction theory, proper scoring theory for probabilistic predictions, and log-sum-exp analysis for transformer architectures. In each setting, we demonstrate that information aggregation creates unavoidable violations of conservation principles. The Jensen gap in transformer probability aggregation provides a direct measure of this impossibility. These results reframe hallucination from an engineering bug to an inevitable mathematical feature of distributed intelligence. There are fundamental trade-offs between truthfulness, knowledge utilization, and response completeness, providing principled foundations for managing rather than eliminating hallucination. This work reveals deep connections between neural network inference, philosophy of knowledge and reasoning, and classical results in game theory and information theory, opening new research directions for developing beneficial AI systems within mathematical constraints."
      },
      {
        "id": "oai:arXiv.org:2506.13477v2",
        "title": "Multimodal Integration Challenges in Emotionally Expressive Child Avatars for Training Applications",
        "link": "https://arxiv.org/abs/2506.13477",
        "author": "Pegah Salehi, Sajad Amouei Sheshkal, Vajira Thambawita, Michael A. Riegler, P{\\aa}l Halvorsen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.13477v2 Announce Type: replace-cross \nAbstract: Dynamic facial emotion is essential for believable AI-generated avatars, yet most systems remain visually static, limiting their use in simulations like virtual training for investigative interviews with abused children. We present a real-time architecture combining Unreal Engine 5 MetaHuman rendering with NVIDIA Omniverse Audio2Face to generate facial expressions from vocal prosody in photorealistic child avatars. Due to limited TTS options, both avatars were voiced using young adult female models from two systems to better fit character profiles, introducing a voice-age mismatch. This confound may affect audiovisual alignment. We used a two-PC setup to decouple speech generation from GPU-intensive rendering, enabling low-latency interaction in desktop and VR. A between-subjects study (N=70) compared audio+visual vs. visual-only conditions as participants rated emotional clarity, facial realism, and empathy for avatars expressing joy, sadness, and anger. While emotions were generally recognized - especially sadness and joy - anger was harder to detect without audio, highlighting the role of voice in high-arousal expressions. Interestingly, silencing clips improved perceived realism by removing mismatches between voice and animation, especially when tone or age felt incongruent. These results emphasize the importance of audiovisual congruence: mismatched voice undermines expression, while a good match can enhance weaker visuals - posing challenges for emotionally coherent avatars in sensitive contexts."
      },
      {
        "id": "oai:arXiv.org:2506.15011v2",
        "title": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC",
        "link": "https://arxiv.org/abs/2506.15011",
        "author": "Eman Alqudah, Ashfaq Khokhar",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.15011v2 Announce Type: replace-cross \nAbstract: Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph Convolutional Network (GCN) integrated with a Deep Q-Network (DQN) reinforcement learning framework for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach dynamically learns link priorities based on real-time traffic demand, network topology, remaining transmission opportunities, and interference patterns. The GCN captures spatial dependencies, while the DQN enables adaptive scheduling decisions through reward-guided exploration. Simulation results show that our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and 175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN model demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over our previous CNN-based approach across the same configurations. These results underscore the effectiveness of our GCN-DQN model in addressing complex URLLC requirements with minimal overhead and superior network performance."
      },
      {
        "id": "oai:arXiv.org:2507.00209v2",
        "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures",
        "link": "https://arxiv.org/abs/2507.00209",
        "author": "Fengyi Jiang, Xiaorui Zhang, Lingbo Jin, Ruixing Liang, Yuxin Chen, Adi Chola Venkatesh, Jason Culman, Tiantian Wu, Lirong Shao, Wenqing Sun, Cong Gao, Hallie McNamara, Jingpei Lu, Omid Mohareri",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.00209v2 Announce Type: replace-cross \nAbstract: High-resolution imaging is crucial for enhancing visual clarity and enabling precise computer-assisted guidance in minimally invasive surgery (MIS). Despite the increasing adoption of 4K endoscopic systems, there remains a significant gap in publicly available native 4K datasets tailored specifically for robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible surgical imaging and video dataset captured at a native 4K resolution, representing realistic conditions of robotic-assisted procedures. SurgiSR4K comprises diverse visual scenarios including specular reflections, tool occlusions, bleeding, and soft tissue deformations, meticulously designed to reflect common challenges faced during laparoscopic and robotic surgeries. This dataset opens up possibilities for a broad range of computer vision tasks that might benefit from high resolution data, such as super resolution (SR), smoke removal, surgical instrument detection, 3D tissue reconstruction, monocular depth estimation, instance segmentation, novel view synthesis, and vision-language model (VLM) development. SurgiSR4K provides a robust foundation for advancing research in high-resolution surgical imaging and fosters the development of intelligent imaging technologies aimed at enhancing performance, safety, and usability in image-guided robotic surgeries."
      },
      {
        "id": "oai:arXiv.org:2507.00582v2",
        "title": "Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models",
        "link": "https://arxiv.org/abs/2507.00582",
        "author": "Yi Zhang, Yidong Zhao, Qian Tao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.00582v2 Announce Type: replace-cross \nAbstract: Deformable medical image registration is traditionally formulated as an optimization problem. While classical methods solve this problem iteratively, recent learning-based approaches use recurrent neural networks (RNNs) to mimic this process by unrolling the prediction of deformation fields in a fixed number of steps. However, classical methods typically converge after sufficient iterations, but learning-based unrolling methods lack a theoretical convergence guarantee and show instability empirically. In addition, unrolling methods have a practical bottleneck at training time: GPU memory usage grows linearly with the unrolling steps due to backpropagation through time (BPTT). To address both theoretical and practical challenges, we propose DEQReg, a novel registration framework based on Deep Equilibrium Models (DEQ), which formulates registration as an equilibrium-seeking problem, establishing a natural connection between classical optimization and learning-based unrolling methods. DEQReg maintains constant memory usage, enabling theoretically unlimited iteration steps. Through extensive evaluation on the public brain MRI and lung CT datasets, we show that DEQReg can achieve competitive registration performance, while substantially reducing memory consumption compared to state-of-the-art unrolling methods. We also reveal an intriguing phenomenon: the performance of existing unrolling methods first increases slightly then degrades irreversibly when the inference steps go beyond the training configuration. In contrast, DEQReg achieves stable convergence with its inbuilt equilibrium-seeking mechanism, bridging the gap between classical optimization-based and modern learning-based registration methods."
      },
      {
        "id": "oai:arXiv.org:2507.04173v2",
        "title": "Efficient Detection of Intermittent Job Failures Using Few-Shot Learning",
        "link": "https://arxiv.org/abs/2507.04173",
        "author": "Henri A\\\"idasso, Francis Bordeleau, Ali Tizghadam",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04173v2 Announce Type: replace-cross \nAbstract: One of the main challenges developers face in the use of continuous integration (CI) and deployment pipelines is the occurrence of intermittent job failures, which result from unexpected non-deterministic issues (e.g., flaky tests or infrastructure problems) rather than regular code-related errors such as bugs. Prior studies developed machine learning (ML) models trained on large datasets of job logs to classify job failures as either intermittent or regular. As an alternative to costly manual labeling of large datasets, the state-of-the-art (SOTA) approach leveraged a heuristic based on non-deterministic job reruns. However, this method mislabels intermittent job failures as regular in contexts where rerunning suspicious job failures is not an explicit policy, and therefore limits the SOTA's performance in practice. In fact, our manual analysis of 2,125 job failures from 5 industrial and 1 open-source projects reveals that, on average, 32% of intermittent job failures are mislabeled as regular. To address these limitations, this paper introduces a novel approach to intermittent job failure detection using few-shot learning (FSL). Specifically, we fine-tune a small language model using a few number of manually labeled log examples to generate rich embeddings, which are then used to train an ML classifier. Our FSL-based approach achieves 70-88% F1-score with only 12 shots in all projects, outperforming the SOTA, which proved ineffective (34-52% F1-score) in 4 projects. Overall, this study underlines the importance of data quality over quantity and provides a more efficient and practical framework for the detection of intermittent job failures in organizations."
      },
      {
        "id": "oai:arXiv.org:2507.04554v2",
        "title": "Self-supervised learning of speech representations with Dutch archival data",
        "link": "https://arxiv.org/abs/2507.04554",
        "author": "Nik Vaessen, Roeland Ordelman, David A. van Leeuwen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04554v2 Announce Type: replace-cross \nAbstract: This paper explores the use of Dutch archival television broadcast data for self-supervised learning of speech foundation models, specifically wav2vec 2.0. We first study data quality assumptions for pre-training, and show how music, noise and speaker overlap affect SSL convergence and downstream fine-tuning performance. Secondly, we explore effectively pre-processing strategies to convert the noisy broadcast dataset into a qualitative dataset for pre-training, by using Whisper and WhisperX. Thirdly, we compare mono-lingual and multi-lingual pre-training with equivalent amounts of data, and show that mono-lingual pre-training is more robust to out-of-domain data. Lastly, we achieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a continuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k hour archival dataset."
      },
      {
        "id": "oai:arXiv.org:2507.04742v2",
        "title": "Activation Steering for Chain-of-Thought Compression",
        "link": "https://arxiv.org/abs/2507.04742",
        "author": "Seyedarmin Azizi, Erfan Baghaei Potraghloo, Massoud Pedram",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04742v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) excel at complex reasoning when they include intermediate steps, known as \"chains of thought\" (CoTs). However, these rationales are often overly verbose, even for simple problems, leading to wasted context, increased latency, and higher energy consumption. We observe that verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct regions in the model's residual-stream activation space. By extracting and injecting a \"steering vector\" to transition between these modes, we can reliably shift generation toward more concise reasoning, effectively compressing CoTs without retraining. We formalize this approach as Activation-Steered Compression (ASC), an inference-time technique that shortens reasoning traces by directly modifying hidden representations. In addition, we provide a theoretical analysis of the impact of ASC on the output distribution, derived from a closed-form KL-divergence-bounded constraint to regulate steering strength. Using only 100 paired verbose and concise examples, ASC achieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets, while maintaining accuracy across 7B, 8B, and 32B parameter models. As a training-free method, ASC introduces negligible runtime overhead and, on MATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock time on an 8B model. This makes ASC a practical and efficient tool for streamlining the deployment of reasoning-capable LLMs in latency- or cost-sensitive settings. The code is available at: https://github.com/ArminAzizi98/ASC"
      },
      {
        "id": "oai:arXiv.org:2507.05006v2",
        "title": "Do We Really Need Specialization? Evaluating Generalist Text Embeddings for Zero-Shot Recommendation and Search",
        "link": "https://arxiv.org/abs/2507.05006",
        "author": "Matteo Attimonelli, Alessandro De Bellis, Claudio Pomo, Dietmar Jannach, Eugenio Di Sciascio, Tommaso Di Noia",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05006v2 Announce Type: replace-cross \nAbstract: Pre-trained language models (PLMs) are widely used to derive semantic representations from item metadata in recommendation and search. In sequential recommendation, PLMs enhance ID-based embeddings through textual metadata, while in product search, they align item characteristics with user intent. Recent studies suggest task and domain-specific fine-tuning are needed to improve representational power. This paper challenges this assumption, showing that Generalist Text Embedding Models (GTEs), pre-trained on large-scale corpora, can guarantee strong zero-shot performance without specialized adaptation. Our experiments demonstrate that GTEs outperform traditional and fine-tuned models in both sequential recommendation and product search. We attribute this to a superior representational power, as they distribute features more evenly across the embedding space. Finally, we show that compressing embedding dimensions by focusing on the most informative directions (e.g., via PCA) effectively reduces noise and improves the performance of specialized models. To ensure reproducibility, we provide our repository at https://split.to/gte4ps."
      },
      {
        "id": "oai:arXiv.org:2507.05060v2",
        "title": "A COMPASS to Model Comparison and Simulation-Based Inference in Galactic Chemical Evolution",
        "link": "https://arxiv.org/abs/2507.05060",
        "author": "Berkay Gunes, Sven Buder, Tobias Buck",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05060v2 Announce Type: replace-cross \nAbstract: We present COMPASS, a novel simulation-based inference framework that combines score-based diffusion models with transformer architectures to jointly perform parameter estimation and Bayesian model comparison across competing Galactic Chemical Evolution (GCE) models. COMPASS handles high-dimensional, incomplete, and variable-size stellar abundance datasets. Applied to high-precision elemental abundance measurements, COMPASS evaluates 40 combinations of nucleosynthetic yield tables. The model strongly favours Asymptotic Giant Branch yields from NuGrid and core-collapse SN yields used in the IllustrisTNG simulation, achieving near-unity cumulative posterior probability. Using the preferred model, we infer a steep high-mass IMF slope and an elevated Supernova Ia normalization, consistent with prior solar neighbourhood studies but now derived from fully amortized Bayesian inference. Our results demonstrate that modern SBI methods can robustly constrain uncertain physics in astrophysical simulators and enable principled model selection when analysing complex, simulation-based data."
      },
      {
        "id": "oai:arXiv.org:2507.05201v2",
        "title": "MedGemma Technical Report",
        "link": "https://arxiv.org/abs/2507.05201",
        "author": "Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, C\\'ian Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram\\'e, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry Lepikhin, Vlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, L\\'eonard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, Lin Yang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05201v2 Announce Type: replace-cross \nAbstract: Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma."
      },
      {
        "id": "oai:arXiv.org:2507.05241v2",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "link": "https://arxiv.org/abs/2507.05241",
        "author": "Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Weinan E, Yuzhi Zhang, Linfeng Zhang, Siheng Chen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05241v2 Announce Type: replace-cross \nAbstract: The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Wed, 09 Jul 2025 04:02:02 +0000",
      "published": "Wed, 09 Jul 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2507.05396v1",
        "title": "Comparative Analysis of Finite Difference and Finite Element Method for Audio Waveform Simulation",
        "link": "https://arxiv.org/abs/2507.05396",
        "author": "Juliette Florin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05396v1 Announce Type: new \nAbstract: In many industries, including aerospace and defense, waveform analysis is commonly conducted to compute the resonance of physical objects, with the Finite Element Method (FEM) being the standard approach. The Finite Difference Method (FDM) is seldom used, and this preference is often stated without formal justification in the literature. In this work, the accuracy, feasibility, and time of simulation of FEM and FDM are compared by simulating the vibration of a guitar string. Python simulations for both methods are implemented, and their results are compared against analytical solutions and experimental data. Additionally, FDM is applied to analyze the sound of a cycling bell to assess its reliability compared to a real cycling bell. Final results show that both FEM and FDM yield similar error margins and accurately predict the system's behavior. Moreover, the errors from FEM and FDM follow the same periodicity with a phase shift when varying the assumed analytical tension and without a phase shift when changing the time interval. However, FEM converges faster with increasing mesh complexity, whereas FDM demonstrates quicker computational performance and achieves stable solutions even with bigger time intervals. Despite this FDM is limited to simpler configurations and often demands extensive mathematical formulation, which can become cumbersome for intricate shapes. For example, modeling a hemispherical object using FDM results in significant simulation times and big calculations. In conclusion, while FDM may offer faster convergence and computation time in certain cases, FEM remains the preferred method in industrial contexts due to its flexibility, scalability, and ease of implementation for complex geometries."
      },
      {
        "id": "oai:arXiv.org:2507.05399v1",
        "title": "Sample Rate Offset Compensated Acoustic Echo Cancellation For Multi-Device Scenarios",
        "link": "https://arxiv.org/abs/2507.05399",
        "author": "Srikanth Korse, Oliver Thiergart, Emanuel A. P. Habets",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05399v1 Announce Type: new \nAbstract: Acoustic echo cancellation (AEC) in multi-device scenarios is a challenging problem due to sample rate offset (SRO) between devices. The SRO hinders the convergence of the AEC filter, diminishing its performance. To address this , we approach the multi-device AEC scenario as a multi-channel AEC problem involving a multi-channel Kalman filter, SRO estimation, and resampling of far-end signals. Experiments in a two-device scenario show that our system mitigates the divergence of the multi-channel Kalman filter in the presence of SRO for both correlated and uncorrelated playback signals during echo-only and double-talk. Additionally, for devices with correlated playback signals, an independent single-channel AEC filter is crucial to ensure fast convergence of SRO estimation."
      },
      {
        "id": "oai:arXiv.org:2507.05402v1",
        "title": "Stereo Reproduction in the Presence of Sample Rate Offsets",
        "link": "https://arxiv.org/abs/2507.05402",
        "author": "Srikanth Korse, Andreas Walther, Emanuel A. P. Habets",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05402v1 Announce Type: new \nAbstract: One of the main challenges in synchronizing wirelessly connected loudspeakers for spatial audio reproduction is clock skew. Clock skew arises from sample rate offsets ( SROs) between the loudspeakers, caused by the use of independent device clocks. While network-based protocols like Precision Time Protocol (PTP) and Network Time Protocol (NTP) are explored, the impact of SROs on spatial audio reproduction and its perceptual consequences remains underexplored. We propose an audio-domain SRO compensation method using spatial filtering to isolate loudspeaker contributions. These filtered signals, along with the original playback signal, are used to estimate the SROs, and their influence is compensated for prior to spatial audio reproduction. We evaluate the effect of the compensation method in a subjective listening test. The results of these tests as well as objective metrics demonstrate that the proposed method mitigates the perceptual degradation introduced by SROs by preserving the spatial cues."
      },
      {
        "id": "oai:arXiv.org:2507.05409v1",
        "title": "Parametric Object Coding in IVAS: Efficient Coding of Multiple Audio Objects at Low Bit Rates",
        "link": "https://arxiv.org/abs/2507.05409",
        "author": "Andrea Eichenseer, Srikanth Korse, Guillaume Fuchs, Markus Multrus",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05409v1 Announce Type: new \nAbstract: The recently standardized 3GPP codec for Immersive Voice and Audio Services (IVAS) includes a parametric mode for efficiently coding multiple audio objects at low bit rates. In this mode, parametric side information is obtained from both the object metadata and the input audio objects. The side information comprises directional information, indices of two dominant objects, and the power ratio between these two dominant objects. It is transmitted to the decoder along with a stereo downmix. In IVAS, parametric object coding allows for transmitting three or four arbitrarily placed objects at bit rates of 24.4 or 32 kbit/s and faithfully reconstructing the spatial image of the original audio scene. Subjective listening tests confirm that IVAS provides a comparable immersive experience at lower bit rate and complexity compared to coding the audio objects independently using Enhanced Voice Services (EVS)."
      },
      {
        "id": "oai:arXiv.org:2507.05609v1",
        "title": "MMW: Side Talk Rejection Multi-Microphone Whisper on Smart Glasses",
        "link": "https://arxiv.org/abs/2507.05609",
        "author": "Yang Liu, Li Wan, Yiteng Huang, Yong Xu, yangyang shi, Saurabh Adya, ming sun, Florian Metze",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05609v1 Announce Type: new \nAbstract: Smart glasses are increasingly positioned as the next-generation interface for ubiquitous access to large language models (LLMs). Nevertheless, achieving reliable interaction in real-world noisy environments remains a major challenge, particularly due to interference from side speech. In this work, we introduce a novel side-talk rejection multi-microphone Whisper (MMW) framework for smart glasses, incorporating three key innovations. First, we propose a Mix Block based on a Tri-Mamba architecture to effectively fuse multi-channel audio at the raw waveform level, while maintaining compatibility with streaming processing. Second, we design a Frame Diarization Mamba Layer to enhance frame-level side-talk suppression, facilitating more efficient fine-tuning of Whisper models. Third, we employ a Multi-Scale Group Relative Policy Optimization (GRPO) strategy to jointly optimize frame-level and utterance-level side speech suppression. Experimental evaluations demonstrate that the proposed MMW system can reduce the word error rate (WER) by 4.95\\% in noisy conditions."
      },
      {
        "id": "oai:arXiv.org:2507.05635v1",
        "title": "Frequency-Specific Neural Response and Cross-Correlation Analysis of Envelope Following Responses to Native Speech and Music Using Multichannel EEG Signals: A Case Study",
        "link": "https://arxiv.org/abs/2507.05635",
        "author": "Md. Mahbub Hasan, Md Rakibul Hasan, Md Zakir Hossain, Tom Gedeon",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05635v1 Announce Type: new \nAbstract: Although native speech and music envelope following responses (EFRs) play a crucial role in auditory processing and cognition, their frequency profile, such as the dominating frequency and spectral coherence, is largely unknown. We have assumed that the auditory pathway - which transmits envelope components of speech and music to the scalp through time-varying neurophysiological processes - is a linear time-varying system, with the envelope and the multi-channel EEG responses as excitation and response, respectively. This paper investigates the transfer function of this system through two analytical techniques - time-averaged spectral responses and cross-spectral density - in the frequency domain at four different positions of the human scalp. Our findings suggest that alpha (8-11 Hz), lower gamma (53-56 Hz), and higher gamma (78-81 Hz) bands are the peak responses of the system. These frequently appearing dominant frequency responses may be the key components of familiar speech perception, maintaining attention, binding acoustic features, and memory processing. The cross-spectral density, which reflects the spatial neural coherence of the human brain, shows that 10-13 Hz, 27-29 Hz, and 62-64 Hz are common for all channel pairs. As neural coherences are frequently observed in these frequencies among native participants, we suggest that these distributed neural processes are also dominant in native speech and music perception."
      },
      {
        "id": "oai:arXiv.org:2507.05657v1",
        "title": "Adaptive Linearly Constrained Minimum Variance Volumetric Active Noise Control",
        "link": "https://arxiv.org/abs/2507.05657",
        "author": "Manan Mittal, Ryan M. Corey, Andrew C. Singer",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05657v1 Announce Type: new \nAbstract: Traditional volumetric noise control typically relies on multipoint error minimization to suppress sound energy across a region, but offers limited flexibility in shaping spatial responses. This paper introduces a time-domain formulation for linearly constrained minimum variance active noise control (LCMV ANC) for spatial control filter design. We demonstrate how the LCMV ANC optimization framework allows system designers to prioritize noise reduction at specific spatial locations through strategically defined linear constraints, providing a more flexible alternative to uniformly weighted multipoint error minimization. An adaptive algorithm based on filtered-X least mean squares (FxLMS) is derived for online adaptation of filter coefficients. Simulation and experimental results validate the proposed method's noise reduction and constraint adherence, demonstrating effective, spatially selective, and broadband noise control compared to multipoint volumetric noise control."
      },
      {
        "id": "oai:arXiv.org:2507.05662v1",
        "title": "Beamforming with Random Projections: Upper and Lower Bounds",
        "link": "https://arxiv.org/abs/2507.05662",
        "author": "Manan Mittal, Ryan M. Corey, Andrew C. Singer",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05662v1 Announce Type: new \nAbstract: Beamformers often trade off white noise gain against the ability to suppress interferers. With distributed microphone arrays, this trade-off becomes crucial as different arrays capture vastly different magnitude and phase differences for each source. We propose the use of multiple random projections as a first-stage preprocessing scheme in a data-driven approach to dimensionality reduction and beamforming. We show that a mixture beamformer derived from the use of multiple such random projections can effectively outperform the minimum variance distortionless response (MVDR) beamformer in terms of signal-to-noise ratio (SNR) and signal-to-interferer-and-noise ratio (SINR) gain. Moreover, our method introduces computational complexity as a trade-off in the design of adaptive beamformers, alongside noise gain and interferer suppression. This added degree of freedom allows the algorithm to better exploit the inherent structure of the received signal and achieve better real-time performance while requiring fewer computations. Finally, we derive upper and lower bounds for the output power of the compressed beamformer when compared to the full complexity MVDR beamformer."
      },
      {
        "id": "oai:arXiv.org:2507.05688v1",
        "title": "Robust One-step Speech Enhancement via Consistency Distillation",
        "link": "https://arxiv.org/abs/2507.05688",
        "author": "Liang Xu, Longfei Felix Yan, W. Bastiaan Kleijn",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05688v1 Announce Type: new \nAbstract: Diffusion models have shown strong performance in speech enhancement, but their real-time applicability has been limited by multi-step iterative sampling. Consistency distillation has recently emerged as a promising alternative by distilling a one-step consistency model from a multi-step diffusion-based teacher model. However, distilled consistency models are inherently biased towards the sampling trajectory of the teacher model, making them less robust to noise and prone to inheriting inaccuracies from the teacher model. To address this limitation, we propose ROSE-CD: Robust One-step Speech Enhancement via Consistency Distillation, a novel approach for distilling a one-step consistency model. Specifically, we introduce a randomized learning trajectory to improve the model's robustness to noise. Furthermore, we jointly optimize the one-step model with two time-domain auxiliary losses, enabling it to recover from teacher-induced errors and surpass the teacher model in overall performance. This is the first pure one-step consistency distillation model for diffusion-based speech enhancement, achieving 54 times faster inference speed and superior performance compared to its 30-step teacher model. Experiments on the VoiceBank-DEMAND dataset demonstrate that the proposed model achieves state-of-the-art performance in terms of speech quality. Moreover, its generalization ability is validated on both an out-of-domain dataset and real-world noisy recordings."
      },
      {
        "id": "oai:arXiv.org:2507.05727v1",
        "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark",
        "link": "https://arxiv.org/abs/2507.05727",
        "author": "He Wang, Linhan Ma, Dake Guo, Xiong Wang, Lei Xie, Jin Xu, Junyang Lin",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05727v1 Announce Type: new \nAbstract: Automatic Speech Recognition (ASR) has been extensively investigated, yet prior evaluative efforts have largely been restricted to contextless paradigms. This constraint stems from the limited proficiency of conventional ASR models in context modeling and their deficiency in memory and reasoning based on world knowledge. Recent breakthroughs in the development of Large Language Models (LLMs) and corresponding Large Audio Language Models (LALMs) have markedly enhanced the visibility of general artificial intelligence capabilities. Consequently, there exists a compelling need for a benchmark that can evaluate both the generality and intelligence of ASR systems. To address this gap, we propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to assess contextual speech recognition. This benchmark encompasses up to 40,000 data entries across over 10 domains, enabling a thorough evaluation of model performance in scenarios that omit or incorporate coarse-grained or fine-grained contextual information. Moreover, diverging from conventional ASR evaluations, our benchmark includes an analysis of model efficacy in recognizing named entities mentioned within the auditory input. Our extensive evaluation highlights that LALMs, with strong world knowledge and context learning capabilities, outperform conventional ASR models by a large margin. The dataset and evaluation code have been released at https://github.com/MrSupW/ContextASR-Bench."
      },
      {
        "id": "oai:arXiv.org:2507.05729v1",
        "title": "Non-Intrusive Binaural Speech Intelligibility Prediction Using Mamba for Hearing-Impaired Listeners",
        "link": "https://arxiv.org/abs/2507.05729",
        "author": "Katsuhiko Yamamoto, Koichi Miyazaki",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05729v1 Announce Type: new \nAbstract: Speech intelligibility prediction (SIP) models have been used as objective metrics to assess intelligibility for hearing-impaired (HI) listeners. In the Clarity Prediction Challenge 2 (CPC2), non-intrusive binaural SIP models based on transformers showed high prediction accuracy. However, the self-attention mechanism theoretically incurs high computational and memory costs, making it a bottleneck for low-latency, power-efficient devices. This may also degrade the temporal processing of binaural SIPs. Therefore, we propose Mamba-based SIP models instead of transformers for the temporal processing blocks. Experimental results show that our proposed SIP model achieves competitive performance compared to the baseline while maintaining a relatively small number of parameters. Our analysis suggests that the SIP model based on bidirectional Mamba effectively captures contextual and spatial speech information from binaural signals."
      },
      {
        "id": "oai:arXiv.org:2507.05900v1",
        "title": "Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning",
        "link": "https://arxiv.org/abs/2507.05900",
        "author": "Zengjing Chen, Lu Wang, Chengzhi Xing",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05900v1 Announce Type: new \nAbstract: This study addresses the problem of stable acoustic relay assignment in an underwater acoustic network. Unlike the objectives of most existing literature, two distinct objectives, namely classical stable arrangement and ambiguous stable arrangement, are considered. To achieve these stable arrangements, a laser chaos-based multi-processing learning (LC-ML) method is introduced to efficiently obtain high throughput and rapidly attain stability. In order to sufficiently explore the relay's decision-making, this method uses random numbers generated by laser chaos to learn the assignment of relays to multiple source nodes. This study finds that the laser chaos-based random number and multi-processing in the exchange process have a positive effect on higher throughput and strong adaptability with environmental changing over time. Meanwhile, ambiguous cognitions result in the stable configuration with less volatility compared to accurate ones. This provides a practical and useful method and can be the basis for relay selection in complex underwater environments."
      },
      {
        "id": "oai:arXiv.org:2507.05911v1",
        "title": "Differentiable Reward Optimization for LLM based TTS system",
        "link": "https://arxiv.org/abs/2507.05911",
        "author": "Changfeng Gao, Zhihao Du, Shiliang Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05911v1 Announce Type: new \nAbstract: This paper proposes a novel Differentiable Reward Optimization (DiffRO) method aimed at enhancing the performance of neural codec language models based text-to-speech (TTS) systems. In contrast to conventional reinforcement learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly compute the rewards based on neural codec tokens, rather than relying on synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to render the reward function differentiable, thereby streamlining the RLHF training process. Additionally, we introduce a multi-task reward (MTR) model which can provide feedback from different perspectives and find that it can augment the system's capability to follow instructions effectively.Experimental results indicate that DiffRO significantly improves the pronunciation accuracy of the TTS system, achieving state-of-the-art (SOTA) WER results on the seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we demonstrate the ability to control emotional and quality attributes in a zero-shot manner."
      },
      {
        "id": "oai:arXiv.org:2507.06070v1",
        "title": "Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol",
        "link": "https://arxiv.org/abs/2507.06070",
        "author": "Christos Nikou, Theodoros Giannakopoulos",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06070v1 Announce Type: new \nAbstract: Recent advances in song identification leverage deep neural networks to learn compact audio fingerprints directly from raw waveforms. While these methods perform well under controlled conditions, their accuracy drops significantly in real-world scenarios where the audio is captured via mobile devices in noisy environments. In this paper, we introduce a novel evaluation protocol designed to better reflect such real-world conditions. We generate three recordings of the same audio, each with increasing levels of noise, captured using a mobile device's microphone. Our results reveal a substantial performance drop for two state-of-the-art CNN-based models under this protocol, compared to previously reported benchmarks. Additionally, we highlight the critical role of the augmentation pipeline during training with contrastive loss. By introduction low pass and high pass filters in the augmentation pipeline we significantly increase the performance of both systems in our proposed evaluation. Furthermore, we develop a transformer-based model with a tailored projection module and demonstrate that transferring knowledge from a semantically relevant domain yields a more robust solution. The transformer architecture outperforms CNN-based models across all noise levels, and query durations. In low noise conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in finding the correct song, surpassing by 14%, and by 18.5% the second-best performing model, respectively, Under heavy noise levels, we achieve a detection rate 56.5% for 15-second query duration. All experiments are conducted on public large-scale dataset of over 100K songs, with queries matched against a database of 56 million vectors."
      },
      {
        "id": "oai:arXiv.org:2507.06116v1",
        "title": "Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis",
        "link": "https://arxiv.org/abs/2507.06116",
        "author": "Xintong Hu, Yixuan Chen, Rui Yang, Wenxiang Guo, Changhao Pan",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06116v1 Announce Type: new \nAbstract: Automatic speech quality assessment plays a crucial role in the development of speech synthesis systems, but existing models exhibit significant performance variations across different granularity levels of prediction tasks. This paper proposes an enhanced MOS prediction system based on self-supervised learning speech models, incorporating a Mixture of Experts (MoE) classification head and utilizing synthetic data from multiple commercial generation models for data augmentation. Our method builds upon existing self-supervised models such as wav2vec2, designing a specialized MoE architecture to address different types of speech quality assessment tasks. We also collected a large-scale synthetic speech dataset encompassing the latest text-to-speech, speech conversion, and speech enhancement systems. However, despite the adoption of the MoE architecture and expanded dataset, the model's performance improvements in sentence-level prediction tasks remain limited. Our work reveals the limitations of current methods in handling sentence-level quality assessment, provides new technical pathways for the field of automatic speech quality assessment, and also delves into the fundamental causes of performance differences across different assessment granularities."
      },
      {
        "id": "oai:arXiv.org:2507.06179v1",
        "title": "Dynamic Slimmable Networks for Efficient Speech Separation",
        "link": "https://arxiv.org/abs/2507.06179",
        "author": "Mohamed Elminshawi, Srikanth Raj Chetupalli, Emanu\\\"el A. P. Habets",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.06179v1 Announce Type: new \nAbstract: Recent progress in speech separation has been largely driven by advances in deep neural networks, yet their high computational and memory requirements hinder deployment on resource-constrained devices. A significant inefficiency in conventional systems arises from using static network architectures that maintain constant computational complexity across all input segments, regardless of their characteristics. This approach is sub-optimal for simpler segments that do not require intensive processing, such as silence or non-overlapping speech. To address this limitation, we propose a dynamic slimmable network (DSN) for speech separation that adaptively adjusts its computational complexity based on the input signal. The DSN combines a slimmable network, which can operate at different network widths, with a lightweight gating module that dynamically determines the required width by analyzing the local input characteristics. To balance performance and efficiency, we introduce a signal-dependent complexity loss that penalizes unnecessary computation based on segmental reconstruction error. Experiments on clean and noisy two-speaker mixtures from the WSJ0-2mix and WHAM! datasets show that the DSN achieves a better performance-efficiency trade-off than individually trained static networks of different sizes."
      },
      {
        "id": "oai:arXiv.org:2507.05724v1",
        "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition",
        "link": "https://arxiv.org/abs/2507.05724",
        "author": "Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05724v1 Announce Type: cross \nAbstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model \\emph{Omni-router Transformer}. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data."
      },
      {
        "id": "oai:arXiv.org:2507.05885v1",
        "title": "How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures",
        "link": "https://arxiv.org/abs/2507.05885",
        "author": "Tanvina Patel, Wiebke Hutiri, Aaron Yi Ding, Odette Scharenborg",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05885v1 Announce Type: cross \nAbstract: There is increasingly more evidence that automatic speech recognition (ASR) systems are biased against different speakers and speaker groups, e.g., due to gender, age, or accent. Research on bias in ASR has so far primarily focused on detecting and quantifying bias, and developing mitigation approaches. Despite this progress, the open question is how to measure the performance and bias of a system. In this study, we compare different performance and bias measures, from literature and proposed, to evaluate state-of-the-art end-to-end ASR systems for Dutch. Our experiments use several bias mitigation strategies to address bias against different speaker groups. The findings reveal that averaged error rates, a standard in ASR research, alone is not sufficient and should be supplemented by other measures. The paper ends with recommendations for reporting ASR performance and bias to better represent a system's performance for diverse speaker groups, and overall system bias."
      },
      {
        "id": "oai:arXiv.org:2502.17380v3",
        "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
        "link": "https://arxiv.org/abs/2502.17380",
        "author": "Qiuming Zhao, Guangzhi Sun, Chao Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2502.17380v3 Announce Type: replace \nAbstract: Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-lingual multi-task training approaches aim to address this by jointly optimising multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language interference, and enhancing extensibility. Experimental results across 10 languages demonstrate that LoRS-Merging significantly outperforms multi-lingual multi-task training, sequential training, and other merging methods, achieving over 20% improvement in normalised performance. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications."
      },
      {
        "id": "oai:arXiv.org:2505.11079v2",
        "title": "ALLM4ADD: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection",
        "link": "https://arxiv.org/abs/2505.11079",
        "author": "Hao Gu, Jiangyan Yi, Chenglong Wang, Jianhua Tao, Zheng Lian, Jiayi He, Yong Ren, Yujie Chen, Zhengqi Wen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2505.11079v2 Announce Type: replace \nAbstract: Audio deepfake detection (ADD) has grown increasingly important due to the rise of high-fidelity audio generative models and their potential for misuse. Given that audio large language models (ALLMs) have made significant progress in various audio processing tasks, a heuristic question arises: \\textit{Can ALLMs be leveraged to solve ADD?}. In this paper, we first conduct a comprehensive zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness. To this end, we propose ALLM4ADD, an ALLM-driven framework for ADD. Specifically, we reformulate ADD task as an audio question answering problem, prompting the model with the question: ``Is this audio fake or real?''. We then perform supervised fine-tuning to enable the ALLM to assess the authenticity of query audio. Extensive experiments are conducted to demonstrate that our ALLM-based method can achieve superior performance in fake audio detection, particularly in data-scarce scenarios. As a pioneering study, we anticipate that this work will inspire the research community to leverage ALLMs to develop more effective ADD systems. Code is available at https://github.com/ucas-hao/qwen_audio_for_add.git"
      },
      {
        "id": "oai:arXiv.org:2506.02080v2",
        "title": "Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge",
        "link": "https://arxiv.org/abs/2506.02080",
        "author": "Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.02080v2 Announce Type: replace \nAbstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research."
      },
      {
        "id": "oai:arXiv.org:2506.11160v5",
        "title": "S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning",
        "link": "https://arxiv.org/abs/2506.11160",
        "author": "Yu Pan, Yuguang Yang, Yanni Hu, Jianhao Ye, Xiang Zhang, Hongbin Zhou, Lei Ma, Jianjun Zhao",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.11160v5 Announce Type: replace \nAbstract: Despite recent advances in multilingual speech-to-speech translation (S2ST), several critical challenges persist: 1) achieving high-quality translation remains a major hurdle, and 2) most existing methods heavily rely on large-scale parallel speech corpora, which are costly and difficult to obtain. To address these issues, we propose \\textit{S2ST-Omni}, an efficient and scalable framework for multilingual S2ST. Specifically, we decompose the S2ST task into speech-to-text translation (S2TT) and text-to-speech synthesis (TTS). For S2TT, we propose an effective speech language model that integrates the pretrained Whisper encoder for robust audio understanding and Qwen 3.0 for advanced text comprehension. A lightweight speech adapter is employed to bridge the modality gap between speech and text representations. To further facilitate the multimodal knowledge learning, a two-stage fine-tuning strategy is introduced. In the TTS stage, we adopt a streaming autoregressive generation approach to produce natural and fluent target speech. Experiments on the CVSS benchmark show that S2ST-Omni consistently outperforms existing state-of-the-art S2ST systems in translation quality, highlighting its effectiveness and superiority."
      },
      {
        "id": "oai:arXiv.org:2506.12067v2",
        "title": "Evaluating Logit-Based GOP Scores for Mispronunciation Detection",
        "link": "https://arxiv.org/abs/2506.12067",
        "author": "Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2506.12067v2 Announce Type: replace \nAbstract: Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment."
      },
      {
        "id": "oai:arXiv.org:2507.01348v2",
        "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech",
        "link": "https://arxiv.org/abs/2507.01348",
        "author": "Zhuangfei Cheng, Guangyan Zhang, Zehai Tu, Yangyang Song, Shuiyang Mao, Xiaoqi Jiao, Jingyu Li, Yiwen Guo, Jiasong Wu",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.01348v2 Announce Type: replace \nAbstract: Foreign accent conversion (FAC) in speech processing remains a challenging task. Building on the remarkable success of large language models (LLMs) in Text-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based techniques for FAC, which we term SpeechAccentLLM. At the core of this framework, we introduce SpeechCodeVAE, the first model to integrate connectionist temporal classification (CTC) directly into codebook discretization for speech content tokenization. This novel architecture generates tokens with a unique \"locality\" property, as validated by experiments demonstrating optimal trade-offs among content faithfulness, temporal coherence, and structural recoverability. Then, to address data scarcity for the FAC module, we adopted a multitask learning strategy that jointly trains the FAC and TTS modules. Beyond mitigating data limitations, this approach yielded accelerated convergence and superior speech quality compared to standalone FAC training. Moreover, leveraging the salient properties of our discrete speech representations, we introduce SpeechRestorer, a postprocessing architecture designed to refine LLM-generated outputs. This module effectively mitigates stochastic errors prevalent in LLM inference pipelines while enhancing prosodic continuity, as validated by ablation experiments."
      },
      {
        "id": "oai:arXiv.org:2507.04554v2",
        "title": "Self-supervised learning of speech representations with Dutch archival data",
        "link": "https://arxiv.org/abs/2507.04554",
        "author": "Nik Vaessen, Roeland Ordelman, David A. van Leeuwen",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04554v2 Announce Type: replace \nAbstract: This paper explores the use of Dutch archival television broadcast data for self-supervised learning of speech foundation models, specifically wav2vec 2.0. We first study data quality assumptions for pre-training, and show how music, noise and speaker overlap affect SSL convergence and downstream fine-tuning performance. Secondly, we explore effectively pre-processing strategies to convert the noisy broadcast dataset into a qualitative dataset for pre-training, by using Whisper and WhisperX. Thirdly, we compare mono-lingual and multi-lingual pre-training with equivalent amounts of data, and show that mono-lingual pre-training is more robust to out-of-domain data. Lastly, we achieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a continuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k hour archival dataset."
      },
      {
        "id": "oai:arXiv.org:2507.03343v2",
        "title": "SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge",
        "link": "https://arxiv.org/abs/2507.03343",
        "author": "Yuxiang Mei, Yuang Zheng, Dongxing Xu, Yanhua Long",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.03343v2 Announce Type: replace-cross \nAbstract: This paper describes SHNU multilingual conversational speech recognition system (SHNU-mASR, team name-\"maybe\"), submitted to Track 1 of the INTERSPEECH 2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder architecture with a large language model (LLM) to form a unified multilingual ASR framework. The parallel-speech-encoder consists of two pre-trained encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output embeddings are concatenated and fed into the LLM, enabling the model to leverage complementary acoustic and linguistic knowledge and achieve competitive performance. Moreover, we adopt a tri-stage training strategy to jointly update the low-rank adaptation modules and projector parameters of both the speech encoders and the LLM. In addition, we incorporate an additional language-aware prompt at the LLM input to enhance language-specific text generation. The SHNU-mASR system achieves an overall character/word error rate (CER/WER) of 11.76% on the blind evaluation set of the challenge, outperforming the official MLC-SLM baseline by 8.41 absolute CER/WER, without increasing the baseline training data."
      },
      {
        "id": "oai:arXiv.org:2507.04667v2",
        "title": "What's Making That Sound Right Now? Video-centric Audio-Visual Localization",
        "link": "https://arxiv.org/abs/2507.04667",
        "author": "Hahyeon Choi, Junhoo Lee, Nojun Kwak",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.04667v2 Announce Type: replace-cross \nAbstract: Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization."
      },
      {
        "id": "oai:arXiv.org:2507.05177v2",
        "title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model",
        "link": "https://arxiv.org/abs/2507.05177",
        "author": "Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang",
        "published": "Wed, 09 Jul 2025 00:00:00 -0400",
        "summary": "arXiv:2507.05177v2 Announce Type: replace-cross \nAbstract: Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at https://casia-lm.github.io/OpenS2S"
      }
    ]
  }
}