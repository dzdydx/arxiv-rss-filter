{
  "https://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI": {
    "feed": {
      "title": "cs.CL, cs.CV, cs.MM, cs.LG, cs.SI updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.CL+cs.CV+cs.MM+cs.LG+cs.SI",
      "updated": "Fri, 04 Apr 2025 11:46:37 +0000",
      "published": "Fri, 04 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.01979v1",
        "title": "Correlation-Attention Masked Temporal Transformer for User Identity Linkage Using Heterogeneous Mobility Data",
        "link": "https://arxiv.org/abs/2504.01979",
        "author": "Ziang Yan, Xingyu Zhao, Hanqing Ma, Wei Chen, Jianpeng Qi, Yanwei Yu, Junyu Dong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01979v1 Announce Type: new \nAbstract: With the rise of social media and Location-Based Social Networks (LBSN), check-in data across platforms has become crucial for User Identity Linkage (UIL). These data not only reveal users' spatio-temporal information but also provide insights into their behavior patterns and interests. However, cross-platform identity linkage faces challenges like poor data quality, high sparsity, and noise interference, which hinder existing methods from extracting cross-platform user information. To address these issues, we propose a Correlation-Attention Masked Transformer for User Identity Linkage Network (MT-Link), a transformer-based framework to enhance model performance by learning spatio-temporal co-occurrence patterns of cross-platform users. Our model effectively captures spatio-temporal co-occurrence in cross-platform user check-in sequences. It employs a correlation attention mechanism to detect the spatio-temporal co-occurrence between user check-in sequences. Guided by attention weight maps, the model focuses on co-occurrence points while filtering out noise, ultimately improving classification performance. Experimental results show that our model significantly outperforms state-of-the-art baselines by 12.92%~17.76% and 5.80%~8.38% improvements in terms of Macro-F1 and Area Under Curve (AUC)."
      },
      {
        "id": "oai:arXiv.org:2504.01982v1",
        "title": "Divine Social Networking in the Age of Lost Omens",
        "link": "https://arxiv.org/abs/2504.01982",
        "author": "W. Brian Lane",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01982v1 Announce Type: new \nAbstract: The last two years have seen significant changes in the divine pantheon of the Lost Omens campaign setting of the Pathfinder Tabletop Roleplaying Game. First, the Pathfinder Remaster, necessitated by the Open Game License debacle, prompted the removal of alignment and an enrichment of divine identities and relationships. Second, the War of Immortals, kicked off by the death of one of the core 20 deities, shook up the membership and relationships within the setting's primary pantheon. These two changes prompted the reprinting of deity information in Pathfinder: Lost Omens Divine Mysteries, which updates and replaces the pre-Remaster Pathfinder: Lost Omens Gods & Magic. Notably, Divine Mysteries features double the page count profiling the core 20 deities. In this paper, we use social network analysis to examine the impact of these changes (Remaster, War of Immortals, and page count) on the relationships among the core 20 deities. In this analysis, each deity features as a node, connected by edges that represent the number of times each pair of deities is mentioned in each other's profiles. The results reveal a much richer, more connected divine network in Divine Mysteries than in Gods & Magic. We conclude by discussing implications for the Lost Omens campaign setting and areas of future development."
      },
      {
        "id": "oai:arXiv.org:2504.01991v1",
        "title": "Disinformation about autism in Latin America and the Caribbean: Mapping 150 false causes and 150 false cures of ASD in conspiracy theory communities on Telegram",
        "link": "https://arxiv.org/abs/2504.01991",
        "author": "Ergon Cugler de Moraes Silva, Arthur Ataide Ferreira Garcia, Guilherme de Almeida, Julie Ricard",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01991v1 Announce Type: new \nAbstract: How do conspiracy theory communities in Latin America and the Caribbean structure, articulate, and sustain the dissemination of disinformation about autism? To answer this question, this research investigates the structuring, articulation, and promotion of autism-related disinformation in conspiracy theory communities in Latin America and the Caribbean. By analyzing publications from 1,659 Telegram communities over ten years (2015 - 2025) and examining more than 58 million pieces of shared content from approximately 5.3 million users, this study explores how false narratives about autism are promoted, including unfounded claims about its causes and promises of miraculous cures. The adopted methodology combines network analysis, time series analysis, thematic clustering, and content analysis, enabling the identification of dissemination patterns, key influencers, and interconnections with other conspiracy theories. Among the key findings, Brazilian communities stand out as the leading producers and distributors of these narratives in the region, accounting for 46% of the analyzed content. Additionally, there has been an exponential 15,000% (x151) increase in the volume of autism-related disinformation since the COVID-19 pandemic in Latin America and the Caribbean, highlighting the correlation between health crises and the rise of conspiracy beliefs. The research also reveals that false cures, such as chlorine dioxide (CDS), ozone therapy, and extreme diets, are widely promoted within these communities and commercially exploited, often preying on desperate families in exchange for money. By addressing the research question, this study aims to contribute to the understanding of the disinformation ecosystem and proposes critical reflections on how to confront these harmful narratives."
      },
      {
        "id": "oai:arXiv.org:2504.02010v1",
        "title": "When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks",
        "link": "https://arxiv.org/abs/2504.02010",
        "author": "Nan Zhang, Yusen Zhang, Prasenjit Mitra, Rui Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02010v1 Announce Type: new \nAbstract: Recent open-source large reasoning models (LRMs) exhibit strong performance on complex reasoning tasks, but their large parameter count makes them prohibitively expensive for individuals. The compression of large language models (LLMs) offers an effective solution to reduce cost of computational resources. However, systematic studies on the performance of compressed LLMs in complex reasoning tasks, especially for LRMs, are lacking. Most works on quantization and pruning focus on preserving language modeling performance, while existing distillation works do not comprehensively benchmark student models based on reasoning difficulty or compression impact on knowledge and reasoning. In this paper, we benchmark compressed DeepSeek-R1 models on four different reasoning datasets (AIME 2024, FOLIO, Temporal Sequences of BIG-Bench Hard, and MuSiQue), ranging from mathematical to multihop reasoning, using quantization, distillation, and pruning methods. We benchmark 2.51-, 1.73-, and 1.58-bit R1 models that adopt dynamic quantization. We also benchmark distilled R1 models that are based on LLaMA or Qwen and run SparseGPT on them to obtain various sparsity levels. Studying the performance and behavior of compressed LRMs, we report their performance scores and test-time compute (number of tokens spent on each question). Notably, using MuSiQue, we find that parameter count has a much greater impact on LRMs' knowledge memorization than on their reasoning capability, which can inform the choice of compression techniques. Through our empirical analysis of test-time compute, we find that shorter model outputs generally achieve better performance than longer ones across several benchmarks for both R1 and its compressed variants, highlighting the need for more concise reasoning chains."
      },
      {
        "id": "oai:arXiv.org:2504.02011v1",
        "title": "Random Conditioning with Distillation for Data-Efficient Diffusion Model Compression",
        "link": "https://arxiv.org/abs/2504.02011",
        "author": "Dohyun Kim, Sehwan Park, Geonhee Han, Seung Wook Kim, Paul Hongsuck Seo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02011v1 Announce Type: new \nAbstract: Diffusion models generate high-quality images through progressive denoising but are computationally intensive due to large model sizes and repeated sampling. Knowledge distillation, which transfers knowledge from a complex teacher to a simpler student model, has been widely studied in recognition tasks, particularly for transferring concepts unseen during student training. However, its application to diffusion models remains underexplored, especially in enabling student models to generate concepts not covered by the training images. In this work, we propose Random Conditioning, a novel approach that pairs noised images with randomly selected text conditions to enable efficient, image-free knowledge distillation. By leveraging this technique, we show that the student can generate concepts unseen in the training images. When applied to conditional diffusion model distillation, our method allows the student to explore the condition space without generating condition-specific images, resulting in notable improvements in both generation quality and efficiency. This promotes resource-efficient deployment of generative diffusion models, broadening their accessibility for both research and real-world applications. Code, models, and datasets are available at https://dohyun-as.github.io/Random-Conditioning ."
      },
      {
        "id": "oai:arXiv.org:2504.02012v1",
        "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
        "link": "https://arxiv.org/abs/2504.02012",
        "author": "Soro Bedionita, Bruno Andreis, Song Chong, Sung Ju Hwang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02012v1 Announce Type: new \nAbstract: Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning."
      },
      {
        "id": "oai:arXiv.org:2504.02013v1",
        "title": "Attention Mamba: Time Series Modeling with Adaptive Pooling Acceleration and Receptive Field Enhancements",
        "link": "https://arxiv.org/abs/2504.02013",
        "author": "Sijie Xiong, Shuqing Liu, Cheng Tang, Fumiya Okubo, Haoling Xiong, Atsushi Shimada",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02013v1 Announce Type: new \nAbstract: \"This work has been submitted to the lEEE for possible publication. Copyright may be transferred without noticeafter which this version may no longer be accessible.\" Time series modeling serves as the cornerstone of real-world applications, such as weather forecasting and transportation management. Recently, Mamba has become a promising model that combines near-linear computational complexity with high prediction accuracy in time series modeling, while facing challenges such as insufficient modeling of nonlinear dependencies in attention and restricted receptive fields caused by convolutions. To overcome these limitations, this paper introduces an innovative framework, Attention Mamba, featuring a novel Adaptive Pooling block that accelerates attention computation and incorporates global information, effectively overcoming the constraints of limited receptive fields. Furthermore, Attention Mamba integrates a bidirectional Mamba block, efficiently capturing long-short features and transforming inputs into the Value representations for attention mechanisms. Extensive experiments conducted on diverse datasets underscore the effectiveness of Attention Mamba in extracting nonlinear dependencies and enhancing receptive fields, establishing superior performance among leading counterparts. Our codes will be available on GitHub."
      },
      {
        "id": "oai:arXiv.org:2504.02015v1",
        "title": "Fault injection analysis of Real NVP normalising flow model for satellite anomaly detection",
        "link": "https://arxiv.org/abs/2504.02015",
        "author": "Gabriele Greco, Carlo Cena, Umberto Albertin, Mauro Martini, Marcello Chiaberge",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02015v1 Announce Type: new \nAbstract: Satellites are used for a multitude of applications, including communications, Earth observation, and space science. Neural networks and deep learning-based approaches now represent the state-of-the-art to enhance the performance and efficiency of these tasks. Given that satellites are susceptible to various faults, one critical application of Artificial Intelligence (AI) is fault detection. However, despite the advantages of neural networks, these systems are vulnerable to radiation errors, which can significantly impact their reliability. Ensuring the dependability of these solutions requires extensive testing and validation, particularly using fault injection methods. This study analyses a physics-informed (PI) real-valued non-volume preserving (Real NVP) normalizing flow model for fault detection in space systems, with a focus on resilience to Single-Event Upsets (SEUs). We present a customized fault injection framework in TensorFlow to assess neural network resilience. Fault injections are applied through two primary methods: Layer State injection, targeting internal network components such as weights and biases, and Layer Output injection, which modifies layer outputs across various activations. Fault types include zeros, random values, and bit-flip operations, applied at varying levels and across different network layers. Our findings reveal several critical insights, such as the significance of bit-flip errors in critical bits, that can lead to substantial performance degradation or even system failure. With this work, we aim to exhaustively study the resilience of Real NVP models against errors due to radiation, providing a means to guide the implementation of fault tolerance measures."
      },
      {
        "id": "oai:arXiv.org:2504.02016v1",
        "title": "Fourier Feature Attribution: A New Efficiency Attribution Method",
        "link": "https://arxiv.org/abs/2504.02016",
        "author": "Zechen Liu, Feiyang Zhang, Wei Song, Xiang Li, Wei Wei",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02016v1 Announce Type: new \nAbstract: The study of neural networks from the perspective of Fourier features has garnered significant attention. While existing analytical research suggests that neural networks tend to learn low-frequency features, a clear attribution method for identifying the specific learned Fourier features has remained elusive. To bridge this gap, we propose a novel Fourier feature attribution method grounded in signal decomposition theory. Additionally, we analyze the differences between game-theoretic attribution metrics for Fourier and spatial domain features, demonstrating that game-theoretic evaluation metrics are better suited for Fourier-based feature attribution.\n  Our experiments show that Fourier feature attribution exhibits superior feature selection capabilities compared to spatial domain attribution methods. For instance, in the case of Vision Transformers (ViTs) on the ImageNet dataset, only $8\\%$ of the Fourier features are required to maintain the original predictions for $80\\%$ of the samples. Furthermore, we compare the specificity of features identified by our method against traditional spatial domain attribution methods. Results reveal that Fourier features exhibit greater intra-class concentration and inter-class distinctiveness, indicating their potential for more efficient classification and explainable AI algorithms."
      },
      {
        "id": "oai:arXiv.org:2504.02018v1",
        "title": "Geometric Reasoning in the Embedding Space",
        "link": "https://arxiv.org/abs/2504.02018",
        "author": "Jan H\\r{u}la, David Moj\\v{z}\\'i\\v{s}ek, Ji\\v{r}\\'i Jane\\v{c}ek, David Herel, Mikol\\'a\\v{s} Janota",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02018v1 Announce Type: new \nAbstract: In this contribution, we demonstrate that Graph Neural Networks and Transformers can learn to reason about geometric constraints. We train them to predict spatial position of points in a discrete 2D grid from a set of constraints that uniquely describe hidden figures containing these points. Both models are able to predict the position of points and interestingly, they form the hidden figures described by the input constraints in the embedding space during the reasoning process. Our analysis shows that both models recover the grid structure during training so that the embeddings corresponding to the points within the grid organize themselves in a 2D subspace and reflect the neighborhood structure of the grid. We also show that the Graph Neural Network we design for the task performs significantly better than the Transformer and is also easier to scale."
      },
      {
        "id": "oai:arXiv.org:2504.02019v1",
        "title": "Antithetic Sampling for Top-k Shapley Identification",
        "link": "https://arxiv.org/abs/2504.02019",
        "author": "Patrick Kolpaczki, Tim Nielen, Eyke H\\\"ullermeier",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02019v1 Announce Type: new \nAbstract: Additive feature explanations rely primarily on game-theoretic notions such as the Shapley value by viewing features as cooperating players. The Shapley value's popularity in and outside of explainable AI stems from its axiomatic uniqueness. However, its computational complexity severely limits practicability. Most works investigate the uniform approximation of all features' Shapley values, needlessly consuming samples for insignificant features. In contrast, identifying the $k$ most important features can already be sufficiently insightful and yields the potential to leverage algorithmic opportunities connected to the field of multi-armed bandits. We propose Comparable Marginal Contributions Sampling (CMCS), a method for the top-$k$ identification problem utilizing a new sampling scheme taking advantage of correlated observations. We conduct experiments to showcase the efficacy of our method in compared to competitive baselines. Our empirical findings reveal that estimation quality for the approximate-all problem does not necessarily transfer to top-$k$ identification and vice versa."
      },
      {
        "id": "oai:arXiv.org:2504.02060v1",
        "title": "LSC-ADL: An Activity of Daily Living (ADL)-Annotated Lifelog Dataset Generated via Semi-Automatic Clustering",
        "link": "https://arxiv.org/abs/2504.02060",
        "author": "Minh-Quan Ho-Le, Duy-Khang Ho, Van-Tu Ninh, Cathal Gurrin, Minh-Triet Tran",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02060v1 Announce Type: new \nAbstract: Lifelogging involves continuously capturing personal data through wearable cameras, providing an egocentric view of daily activities. Lifelog retrieval aims to search and retrieve relevant moments from this data, yet existing methods largely overlook activity-level annotations, which capture temporal relationships and enrich semantic understanding. In this work, we introduce LSC-ADL, an ADL-annotated lifelog dataset derived from the LSC dataset, incorporating Activities of Daily Living (ADLs) as a structured semantic layer. Using a semi-automatic approach featuring the HDBSCAN algorithm for intra-class clustering and human-in-the-loop verification, we generate accurate ADL annotations to enhance retrieval explainability. By integrating action recognition into lifelog retrieval, LSC-ADL bridges a critical gap in existing research, offering a more context-aware representation of daily life. We believe this dataset will advance research in lifelog retrieval, activity recognition, and egocentric vision, ultimately improving the accuracy and interpretability of retrieved content. The ADL annotations can be downloaded at https://bit.ly/lsc-adl-annotations."
      },
      {
        "id": "oai:arXiv.org:2504.02061v1",
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "link": "https://arxiv.org/abs/2504.02061",
        "author": "Yuxin Guo, Shuailei Ma, Shijie Ma, Xiaoyi Bao, Chen-Wei Xie, Kecheng Zheng, Tingyu Weng, Siyang Sun, Yun Zheng, Wei Zou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02061v1 Announce Type: new \nAbstract: Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual Large Language Models (AV-LLMs) exhibit deficiencies in exploiting audio information, leading to weak understanding and hallucinations. To solve the issues, we delve into the model architecture and dataset. (1) From the architectural perspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent alignment of audio and visual modalities in both temporal and spatial dimensions ensures a comprehensive and accurate understanding of videos. Specifically, we devise an audio-visual multi-scale adapter for multi-scale information aggregation, which achieves spatial alignment. For temporal alignment, we propose audio-visual interleaved merging. (2) From the dataset perspective, we curate an audio-visual caption and instruction-tuning dataset, called AVU. It comprises 5.2 million diverse, open-ended data tuples (video, audio, question, answer) and introduces a novel data partitioning strategy. Extensive experiments show our model not only achieves remarkable performance in audio-visual understanding, but also mitigates potential hallucinations."
      },
      {
        "id": "oai:arXiv.org:2504.02064v1",
        "title": "From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP",
        "link": "https://arxiv.org/abs/2504.02064",
        "author": "Fabio Y\\'a\\~nez-Romero, Andr\\'es Montoyo, Armando Su\\'arez, Yoan Guti\\'errez, Ruslan Mitkov",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02064v1 Announce Type: new \nAbstract: Researchers have relegated natural language processing tasks to Transformer-type models, particularly generative models, because these models exhibit high versatility when performing generation and classification tasks. As the size of these models increases, they achieve outstanding results. Given their widespread use, many explainability techniques are developed based on these models. However, this process becomes computationally expensive due to the large size of the models. Additionally, transformers interpret input information through tokens that fragment input words into sequences lacking inherent semantic meaning, complicating the explanation of the model from the very beginning. This study proposes a novel methodology to achieve explainability in natural language processing tasks by automatically converting sentences into graphs and maintaining semantics through nodes and relations that express fundamental linguistic concepts. It also allows the subsequent exploitation of this knowledge in subsequent tasks, making it possible to obtain trends and understand how the model associates the different elements inside the text with the explained task. The experiments delivered promising results in determining the most critical components within the text structure for a given classification."
      },
      {
        "id": "oai:arXiv.org:2504.02067v1",
        "title": "A Truncated Newton Method for Optimal Transport",
        "link": "https://arxiv.org/abs/2504.02067",
        "author": "Mete Kemertas, Amir-massoud Farahmand, Allan D. Jepson",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02067v1 Announce Type: new \nAbstract: Developing a contemporary optimal transport (OT) solver requires navigating trade-offs among several critical requirements: GPU parallelization, scalability to high-dimensional problems, theoretical convergence guarantees, empirical performance in terms of precision versus runtime, and numerical stability in practice. With these challenges in mind, we introduce a specialized truncated Newton algorithm for entropic-regularized OT. In addition to proving that locally quadratic convergence is possible without assuming a Lipschitz Hessian, we provide strategies to maximally exploit the high rate of local convergence in practice. Our GPU-parallel algorithm exhibits exceptionally favorable runtime performance, achieving high precision orders of magnitude faster than many existing alternatives. This is evidenced by wall-clock time experiments on 24 problem sets (12 datasets $\\times$ 2 cost functions). The scalability of the algorithm is showcased on an extremely large OT problem with $n \\approx 10^6$, solved approximately under weak entopric regularization."
      },
      {
        "id": "oai:arXiv.org:2504.02083v1",
        "title": "Measuring the Data",
        "link": "https://arxiv.org/abs/2504.02083",
        "author": "Ido Cohen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02083v1 Announce Type: new \nAbstract: Measuring the Data analytically finds the intrinsic manifold in big data. First, Optimal Transport generates the tangent space at each data point from which the intrinsic dimension is revealed. Then, the Koopman Dimensionality Reduction procedure derives a nonlinear transformation from the data to the intrinsic manifold. Measuring the data procedure is presented here, backed up with encouraging results."
      },
      {
        "id": "oai:arXiv.org:2504.02087v1",
        "title": "An Introductory Survey to Autoencoder-based Deep Clustering -- Sandboxes for Combining Clustering with Deep Learning",
        "link": "https://arxiv.org/abs/2504.02087",
        "author": "Collin Leiber, Lukas Miklautz, Claudia Plant, Christian B\\\"ohm",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02087v1 Announce Type: new \nAbstract: Autoencoders offer a general way of learning low-dimensional, non-linear representations from data without labels. This is achieved without making any particular assumptions about the data type or other domain knowledge. The generality and domain agnosticism in combination with their simplicity make autoencoders a perfect sandbox for researching and developing novel (deep) clustering algorithms. Clustering methods group data based on similarity, a task that benefits from the lower-dimensional representation learned by an autoencoder, mitigating the curse of dimensionality. Specifically, the combination of deep learning with clustering, called Deep Clustering, enables to learn a representation tailored to specific clustering tasks, leading to high-quality results. This survey provides an introduction to fundamental autoencoder-based deep clustering algorithms that serve as building blocks for many modern approaches."
      },
      {
        "id": "oai:arXiv.org:2504.02091v1",
        "title": "Increasing happiness through conversations with artificial intelligence",
        "link": "https://arxiv.org/abs/2504.02091",
        "author": "Joseph Heffner, Chongyu Qin, Martin Chadwick, Chris Knutsen, Christopher Summerfield, Zeb Kurth-Nelson, Robb B. Rutledge",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02091v1 Announce Type: new \nAbstract: Chatbots powered by artificial intelligence (AI) have rapidly become a significant part of everyday life, with over a quarter of American adults using them multiple times per week. While these tools offer potential benefits and risks, a fundamental question remains largely unexplored: How do conversations with AI influence subjective well-being? To investigate this, we conducted a study where participants either engaged in conversations with an AI chatbot (N = 334) or wrote journal entires (N = 193) on the same randomly assigned topics and reported their momentary happiness afterward. We found that happiness after AI chatbot conversations was higher than after journaling, particularly when discussing negative topics such as depression or guilt. Leveraging large language models for sentiment analysis, we found that the AI chatbot mirrored participants' sentiment while maintaining a consistent positivity bias. When discussing negative topics, participants gradually aligned their sentiment with the AI's positivity, leading to an overall increase in happiness. We hypothesized that the history of participants' sentiment prediction errors, the difference between expected and actual emotional tone when responding to the AI chatbot, might explain this happiness effect. Using computational modeling, we find the history of these sentiment prediction errors over the course of a conversation predicts greater post-conversation happiness, demonstrating a central role of emotional expectations during dialogue. Our findings underscore the effect that AI interactions can have on human well-being."
      },
      {
        "id": "oai:arXiv.org:2504.02094v1",
        "title": "FlowDistill: Scalable Traffic Flow Prediction via Distillation from LLMs",
        "link": "https://arxiv.org/abs/2504.02094",
        "author": "Chenyang Yu, Xinpeng Xie, Yan Huang, Chenxi Qiu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02094v1 Announce Type: new \nAbstract: Accurate traffic flow prediction is vital for optimizing urban mobility, yet it remains difficult in many cities due to complex spatio-temporal dependencies and limited high-quality data. While deep graph-based models demonstrate strong predictive power, their performance often comes at the cost of high computational overhead and substantial training data requirements, making them impractical for deployment in resource-constrained or data-scarce environments. We propose the FlowDistill, a lightweight and scalable traffic prediction framework based on knowledge distillation from large language models (LLMs). In this teacher-student setup, a fine-tuned LLM guides a compact multi-layer perceptron (MLP) student model using a novel combination of the information bottleneck principle and teacher-bounded regression loss, ensuring the distilled model retains only essential and transferable knowledge. Spatial and temporal correlations are explicitly encoded to enhance the model's generalization across diverse urban settings. Despite its simplicity, FlowDistill consistently outperforms state-of-the-art models in prediction accuracy while requiring significantly less training data, and achieving lower memory usage and inference latency, highlighting its efficiency and suitability for real-world, scalable deployment."
      },
      {
        "id": "oai:arXiv.org:2504.02106v1",
        "title": "ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation",
        "link": "https://arxiv.org/abs/2504.02106",
        "author": "Xiao Wang, Daniil Larionov, Siwei Wu, Yiqi Liu, Steffen Eger, Nafise Sadat Moosavi, Chenghua Lin",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02106v1 Announce Type: new \nAbstract: Evaluating the quality of generated text automatically remains a significant challenge. Conventional reference-based metrics have been shown to exhibit relatively weak correlation with human evaluations. Recent research advocates the use of large language models (LLMs) as source-based metrics for natural language generation (NLG) assessment. While promising, LLM-based metrics, particularly those using smaller models, still fall short in aligning with human judgments. In this work, we introduce ContrastScore, a contrastive evaluation metric designed to enable higher-quality, less biased, and more efficient assessment of generated text. We evaluate ContrastScore on two NLG tasks: machine translation and summarization. Experimental results show that ContrastScore consistently achieves stronger correlation with human judgments than both single-model and ensemble-based baselines. Notably, ContrastScore based on Qwen 3B and 0.5B even outperforms Qwen 7B, despite having only half as many parameters, demonstrating its efficiency. Furthermore, it effectively mitigates common evaluation biases such as length and likelihood preferences, resulting in more robust automatic evaluation."
      },
      {
        "id": "oai:arXiv.org:2504.02107v1",
        "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining",
        "link": "https://arxiv.org/abs/2504.02107",
        "author": "Jeffrey Li, Mohammadreza Armandpour, Iman Mirzadeh, Sachin Mehta, Vaishaal Shankar, Raviteja Vemulapalli, Samy Bengio, Oncel Tuzel, Mehrdad Farajtabar, Hadi Pouransari, Fartash Faghri",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02107v1 Announce Type: new \nAbstract: Large Language Models (LLMs) trained on historical web data inevitably become outdated. We investigate evaluation strategies and update methods for LLMs as new data becomes available. We introduce a web-scale dataset for time-continual pretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of magnitude larger than previous continual language modeling benchmarks. We also design time-stratified evaluations across both general CC data and specific domains (Wikipedia, StackExchange, and code documentation) to assess how well various continual learning methods adapt to new data while retaining past knowledge. Our findings demonstrate that, on general CC data, autoregressive meta-schedules combined with a fixed-ratio replay of older data can achieve comparable held-out loss to re-training from scratch, while requiring significantly less computation (2.6x). However, the optimal balance between incorporating new data and replaying old data differs as replay is crucial to avoid forgetting on generic web data but less so on specific domains."
      },
      {
        "id": "oai:arXiv.org:2504.02112v1",
        "title": "PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal",
        "link": "https://arxiv.org/abs/2504.02112",
        "author": "Renjie Liu, Haitian Jiang, Xiao Yan, Bo Tang, Jinyang Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02112v1 Announce Type: new \nAbstract: GraphRAG enhances large language models (LLMs) to generate quality answers for user questions by retrieving related facts from external knowledge graphs. Existing GraphRAG methods adopt a fixed graph traversal strategy for fact retrieval but we observe that user questions come in different types and require different graph traversal strategies. As such, existing GraphRAG methods are limited in effectiveness (i.e., quality of the generated answers) and/or efficiency (i.e., response time or the number of used tokens). In this paper, we propose to classify the questions according to a complete four-class taxonomy and adaptively select the appropriate graph traversal strategy for each type of questions. Our system PolyG is essentially a query planner for GraphRAG and can handle diverse questions with an unified interface and execution engine. Compared with SOTA GraphRAG methods, PolyG achieves an overall win rate of 75% on generation quality and a speedup up to 4x on response time."
      },
      {
        "id": "oai:arXiv.org:2504.02116v1",
        "title": "Language Models at the Syntax-Semantics Interface: A Case Study of the Long-Distance Binding of Chinese Reflexive ziji",
        "link": "https://arxiv.org/abs/2504.02116",
        "author": "Xiulin Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02116v1 Announce Type: new \nAbstract: This paper explores whether language models can effectively resolve the complex binding patterns of the Mandarin Chinese reflexive ziji, which are constrained by both syntactic and semantic factors. We construct a dataset of 240 synthetic sentences using templates and examples from syntactic literature, along with 320 natural sentences from the BCC corpus. Evaluating 21 language models against this dataset and comparing their performance to judgments from native Mandarin speakers, we find that none of the models consistently replicates human-like judgments. The results indicate that existing language models tend to rely heavily on sequential cues, though not always favoring the closest strings, and often overlooking subtle semantic and syntactic constraints. They tend to be more sensitive to noun-related than verb-related semantics."
      },
      {
        "id": "oai:arXiv.org:2504.02118v1",
        "title": "LLMPi: Optimizing LLMs for High-Throughput on Raspberry Pi",
        "link": "https://arxiv.org/abs/2504.02118",
        "author": "Mahsa Ardakani, Jinendra Malekar, Ramtin Zand",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02118v1 Announce Type: new \nAbstract: Deploying Large Language Models (LLMs) on resource-constrained edge devices like the Raspberry Pi presents challenges in computational efficiency, power consumption, and response latency. This paper explores quantization-based optimization techniques to enable high-throughput, energy-efficient execution of LLMs on low-power embedded systems. Our approach leverages k-quantization, a Post-Training Quantization (PTQ) method designed for different bit-widths, enabling efficient 2-bit, 4-bit, 6-bit, and 8-bit weight quantization. Additionally, we employ ternary quantization using Quantization-Aware Training (QAT) for BitNet models, allowing for more effective adaptation to lower-bit representations while preserving accuracy.\n  Our findings highlight the potential of quantized LLMs for real-time conversational AI on edge devices, paving the way for low-power, high-efficiency AI deployment in mobile and embedded applications. This study demonstrates that aggressive quantization strategies can significantly reduce energy consumption while maintaining inference quality, making LLMs practical for resource-limited environments."
      },
      {
        "id": "oai:arXiv.org:2504.02119v1",
        "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
        "link": "https://arxiv.org/abs/2504.02119",
        "author": "Wang Wei, Tiankai Yang, Hongjie Chen, Ryan A. Rossi, Yue Zhao, Franck Dernoncourt, Hoda Eldardiry",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02119v1 Announce Type: new \nAbstract: Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting."
      },
      {
        "id": "oai:arXiv.org:2504.02122v1",
        "title": "Overcoming Vocabulary Constraints with Pixel-level Fallback",
        "link": "https://arxiv.org/abs/2504.02122",
        "author": "Jonas F. Lotz, Hendra Setiawan, Stephan Peitz, Yova Kementchedjhieva",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02122v1 Announce Type: new \nAbstract: Subword tokenization requires balancing computational efficiency and vocabulary coverage, which often leads to suboptimal performance on languages and scripts not prioritized during training. We propose to augment pretrained language models with a vocabulary-free encoder that generates input embeddings from text rendered as pixels. Through experiments on English-centric language models, we demonstrate that our approach substantially improves machine translation performance and facilitates effective cross-lingual transfer, outperforming tokenizer-based methods. Furthermore, we find that pixel-based representations outperform byte-level approaches and standard vocabulary expansion. Our approach enhances the multilingual capabilities of monolingual language models without extensive retraining and reduces decoding latency via input compression."
      },
      {
        "id": "oai:arXiv.org:2504.02130v1",
        "title": "Ordering-based Conditions for Global Convergence of Policy Gradient Methods",
        "link": "https://arxiv.org/abs/2504.02130",
        "author": "Jincheng Mei, Bo Dai, Alekh Agarwal, Mohammad Ghavamzadeh, Csaba Szepesvari, Dale Schuurmans",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02130v1 Announce Type: new \nAbstract: We prove that, for finite-arm bandits with linear function approximation, the global convergence of policy gradient (PG) methods depends on inter-related properties between the policy update and the representation. textcolor{blue}{First}, we establish a few key observations that frame the study: \\textbf{(i)} Global convergence can be achieved under linear function approximation without policy or reward realizability, both for the standard Softmax PG and natural policy gradient (NPG). \\textbf{(ii)} Approximation error is not a key quantity for characterizing global convergence in either algorithm. \\textbf{(iii)} The conditions on the representation that imply global convergence are different between these two algorithms. Overall, these observations call into question approximation error as an appropriate quantity for characterizing the global convergence of PG methods under linear function approximation. \\textcolor{blue}{Second}, motivated by these observations, we establish new general results: \\textbf{(i)} NPG with linear function approximation achieves global convergence \\emph{if and only if} the projection of the reward onto the representable space preserves the optimal action's rank, a quantity that is not strongly related to approximation error. \\textbf{(ii)} The global convergence of Softmax PG occurs if the representation satisfies a non-domination condition and can preserve the ranking of rewards, which goes well beyond policy or reward realizability. We provide experimental results to support these theoretical findings."
      },
      {
        "id": "oai:arXiv.org:2504.02132v1",
        "title": "One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image",
        "link": "https://arxiv.org/abs/2504.02132",
        "author": "Ezzeldin Shereen, Dan Ristea, Burak Hasircioglu, Shae McFadden, Vasilios Mavroudis, Chris Hicks",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02132v1 Announce Type: new \nAbstract: Multimodal retrieval augmented generation (M-RAG) has recently emerged as a method to inhibit hallucinations of large multimodal models (LMMs) through a factual knowledge base (KB). However, M-RAG also introduces new attack vectors for adversaries that aim to disrupt the system by injecting malicious entries into the KB. In this work, we present a poisoning attack against M-RAG targeting visual document retrieval applications, where the KB contains images of document pages. Our objective is to craft a single image that is retrieved for a variety of different user queries, and consistently influences the output produced by the generative model, thus creating a universal denial-of-service (DoS) attack against the M-RAG system. We demonstrate that while our attack is effective against a diverse range of widely-used, state-of-the-art retrievers (embedding models) and generators (LMMs), it can also be ineffective against robust embedding models. Our attack not only highlights the vulnerability of M-RAG pipelines to poisoning attacks, but also sheds light on a fundamental weakness that potentially hinders their performance even in benign settings."
      },
      {
        "id": "oai:arXiv.org:2504.02142v1",
        "title": "Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds",
        "link": "https://arxiv.org/abs/2504.02142",
        "author": "Michael-Andrei Panaitescu-Liess, Yigitcan Kaya, Sicheng Zhu, Furong Huang, Tudor Dumitras",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02142v1 Announce Type: new \nAbstract: Group robustness has become a major concern in machine learning (ML) as conventional training paradigms were found to produce high error on minority groups. Without explicit group annotations, proposed solutions rely on heuristics that aim to identify and then amplify the minority samples during training. In our work, we first uncover a critical shortcoming of these methods: an inability to distinguish legitimate minority samples from poison samples in the training set. By amplifying poison samples as well, group robustness methods inadvertently boost the success rate of an adversary -- e.g., from $0\\%$ without amplification to over $97\\%$ with it. Notably, we supplement our empirical evidence with an impossibility result proving this inability of a standard heuristic under some assumptions. Moreover, scrutinizing recent poisoning defenses both in centralized and federated learning, we observe that they rely on similar heuristics to identify which samples should be eliminated as poisons. In consequence, minority samples are eliminated along with poisons, which damages group robustness -- e.g., from $55\\%$ without the removal of the minority samples to $41\\%$ with it. Finally, as they pursue opposing goals using similar heuristics, our attempt to alleviate the trade-off by combining group robustness methods and poisoning defenses falls short. By exposing this tension, we also hope to highlight how benchmark-driven ML scholarship can obscure the trade-offs among different metrics with potentially detrimental consequences."
      },
      {
        "id": "oai:arXiv.org:2504.02144v1",
        "title": "Towards Interpretable Soft Prompts",
        "link": "https://arxiv.org/abs/2504.02144",
        "author": "Oam Patel, Jason Wang, Nikhil Shivakumar Nayak, Suraj Srinivas, Himabindu Lakkaraju",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02144v1 Announce Type: new \nAbstract: Soft prompts have been popularized as a cheap and easy way to improve task-specific LLM performance beyond few-shot prompts. Despite their origin as an automated prompting method, however, soft prompts and other trainable prompts remain a black-box method with no immediately interpretable connections to prompting. We create a novel theoretical framework for evaluating the interpretability of trainable prompts based on two desiderata: faithfulness and scrutability. We find that existing methods do not naturally satisfy our proposed interpretability criterion. Instead, our framework inspires a new direction of trainable prompting methods that explicitly optimizes for interpretability. To this end, we formulate and test new interpretability-oriented objective functions for two state-of-the-art prompt tuners: Hard Prompts Made Easy (PEZ) and RLPrompt. Our experiments with GPT-2 demonstrate a fundamental trade-off between interpretability and the task-performance of the trainable prompt, explicating the hardness of the soft prompt interpretability problem and revealing odd behavior that arises when one optimizes for an interpretability proxy."
      },
      {
        "id": "oai:arXiv.org:2504.02146v1",
        "title": "LL4G: Self-Supervised Dynamic Optimization for Graph-Based Personality Detection",
        "link": "https://arxiv.org/abs/2504.02146",
        "author": "Lingzhi Shen, Yunfei Long, Xiaohao Cai, Guanming Chen, Yuhan Wang, Imran Razzak, Shoaib Jameel",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02146v1 Announce Type: new \nAbstract: Graph-based personality detection constructs graph structures from textual data, particularly social media posts. Current methods often struggle with sparse or noisy data and rely on static graphs, limiting their ability to capture dynamic changes between nodes and relationships. This paper introduces LL4G, a self-supervised framework leveraging large language models (LLMs) to optimize graph neural networks (GNNs). LLMs extract rich semantic features to generate node representations and to infer explicit and implicit relationships. The graph structure adaptively adds nodes and edges based on input data, continuously optimizing itself. The GNN then uses these optimized representations for joint training on node reconstruction, edge prediction, and contrastive learning tasks. This integration of semantic and structural information generates robust personality profiles. Experimental results on Kaggle and Pandora datasets show LL4G outperforms state-of-the-art models."
      },
      {
        "id": "oai:arXiv.org:2504.02151v1",
        "title": "Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP",
        "link": "https://arxiv.org/abs/2504.02151",
        "author": "Jiztom Kavalakkatt Francis, Matthew J Darr",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02151v1 Announce Type: new \nAbstract: The rapid use of artificial intelligence (AI) in processes such as coding, image processing, and data prediction means it is crucial to understand and validate the data we are working with fully. This paper dives into the hurdles of analyzing high-dimensional data, especially when it gets too complex. Traditional methods in data analysis often look at direct connections between input variables, which can miss out on the more complicated relationships within the data.\n  To address these issues, we explore several tested techniques, such as removing specific variables to see their impact and using statistical analysis to find connections between multiple variables. We also consider the role of synthetic data and how information can sometimes be redundant across different sensors. These analyses are typically very computationally demanding and often require much human effort to make sense of the results.\n  A common approach is to treat the entire dataset as one unit and apply advanced models to handle it. However, this can become problematic with larger, noisier datasets and more complex models. So, we suggest methods to identify overall patterns that can help with tasks like classification or regression based on the idea that more straightforward approaches might be more understandable.\n  Our research looks at two datasets: a real-world dataset and a synthetic one. The goal is to create a methodology that highlights key features on a global scale that lead to predictions, making it easier to validate or quantify the data set. By reducing the dimensionality with this method, we can simplify the models used and thus clarify the insights we gain. Furthermore, our method can reveal unexplored relationships between specific inputs and outcomes, providing a way to validate these new connections further."
      },
      {
        "id": "oai:arXiv.org:2504.02153v1",
        "title": "Niche Dynamics in Complex Online Community Ecosystems",
        "link": "https://arxiv.org/abs/2504.02153",
        "author": "Nathan TeBlunthuis",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02153v1 Announce Type: new \nAbstract: Online communities are important organizational forms where members socialize and share information. Curiously, different online communities often overlap considerably in topic and membership. Recent research has investigated competition and mutualism among overlapping online communities through the lens of organizational ecology; however, it has not accounted for how the nonlinear dynamics of online attention may lead to episodic competition and mutualism. Neither has it explored the origins of competition and mutualism in the processes by which online communities select or adapt to their niches. This paper presents a large-scale study of 8,806 Reddit communities belonging to 1,919 clusters of high user overlap over a 5-year period. The method uses nonlinear time series methods to infer bursty, often short-lived ecological dynamics. Results reveal that mutualism episodes are longer lived and slightly more frequent than competition episodes. Next, it tests whether online communities find their niches by specializing to avoid competition using panel regression models. It finds that competitive ecological interactions lead to decreasing topic and user overlaps; however, changes that decrease such niche overlaps do not lead to mutualism. The discussion proposes that future designs may enable online community ecosystem management by informing online community leaders to organize \"spin-off\" communities or via feeds and recommendations."
      },
      {
        "id": "oai:arXiv.org:2504.02154v1",
        "title": "FreSca: Unveiling the Scaling Space in Diffusion Models",
        "link": "https://arxiv.org/abs/2504.02154",
        "author": "Chao Huang, Susan Liang, Yunlong Tang, Li Ma, Yapeng Tian, Chenliang Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02154v1 Announce Type: new \nAbstract: Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets."
      },
      {
        "id": "oai:arXiv.org:2504.02158v1",
        "title": "UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.02158",
        "author": "Jaehoon Choi, Dongki Jung, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02158v1 Announce Type: new \nAbstract: We present UAVTwin, a method for creating digital twins from real-world environments and facilitating data augmentation for training downstream models embedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses on synthesizing foreground components, such as various human instances in motion within complex scene backgrounds, from UAV perspectives. This is achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing backgrounds along with controllable synthetic human models that display diverse appearances and actions in multiple poses. To the best of our knowledge, UAVTwin is the first approach for UAV-based perception that is capable of generating high-fidelity digital twins based on 3DGS. The proposed work significantly enhances downstream models through data augmentation for real-world environments with multiple dynamic objects and significant appearance variations-both of which typically introduce artifacts in 3DGS-based modeling. To tackle these challenges, we propose a novel appearance modeling strategy and a mask refinement module to enhance the training of 3D Gaussian Splatting. We demonstrate the high quality of neural rendering by achieving a 1.23 dB improvement in PSNR compared to recent methods. Furthermore, we validate the effectiveness of data augmentation by showing a 2.5% to 13.7% improvement in mAP for the human detection task."
      },
      {
        "id": "oai:arXiv.org:2504.02160v1",
        "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation",
        "link": "https://arxiv.org/abs/2504.02160",
        "author": "Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02160v1 Announce Type: new \nAbstract: Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation."
      },
      {
        "id": "oai:arXiv.org:2504.02163v1",
        "title": "Neural Style Transfer for Synthesising a Dataset of Ancient Egyptian Hieroglyphs",
        "link": "https://arxiv.org/abs/2504.02163",
        "author": "Lewis Matheson Creed",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02163v1 Announce Type: new \nAbstract: The limited availability of training data for low-resource languages makes applying machine learning techniques challenging. Ancient Egyptian is one such language with few resources. However, innovative applications of data augmentation methods, such as Neural Style Transfer, could overcome these barriers. This paper presents a novel method for generating datasets of ancient Egyptian hieroglyphs by applying NST to a digital typeface. Experimental results found that image classification models trained on NST-generated examples and photographs demonstrate equal performance and transferability to real unseen images of hieroglyphs."
      },
      {
        "id": "oai:arXiv.org:2504.02168v1",
        "title": "MDP: Multidimensional Vision Model Pruning with Latency Constraint",
        "link": "https://arxiv.org/abs/2504.02168",
        "author": "Xinglong Sun, Barath Lakshmanan, Maying Shen, Shiyi Lan, Jingde Chen, Jose M. Alvarez",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02168v1 Announce Type: new \nAbstract: Current structural pruning methods face two significant limitations: (i) they often limit pruning to finer-grained levels like channels, making aggressive parameter reduction challenging, and (ii) they focus heavily on parameter and FLOP reduction, with existing latency-aware methods frequently relying on simplistic, suboptimal linear models that fail to generalize well to transformers, where multiple interacting dimensions impact latency. In this paper, we address both limitations by introducing Multi-Dimensional Pruning (MDP), a novel paradigm that jointly optimizes across a variety of pruning granularities-including channels, query, key, heads, embeddings, and blocks. MDP employs an advanced latency modeling technique to accurately capture latency variations across all prunable dimensions, achieving an optimal balance between latency and accuracy. By reformulating pruning as a Mixed-Integer Nonlinear Program (MINLP), MDP efficiently identifies the optimal pruned structure across all prunable dimensions while respecting latency constraints. This versatile framework supports both CNNs and transformers. Extensive experiments demonstrate that MDP significantly outperforms previous methods, especially at high pruning ratios. On ImageNet, MDP achieves a 28% speed increase with a +1.4 Top-1 accuracy improvement over prior work like HALP for ResNet50 pruning. Against the latest transformer pruning method, Isomorphic, MDP delivers an additional 37% acceleration with a +0.7 Top-1 accuracy improvement."
      },
      {
        "id": "oai:arXiv.org:2504.02169v1",
        "title": "On the Geometry of Receiver Operating Characteristic and Precision-Recall Curves",
        "link": "https://arxiv.org/abs/2504.02169",
        "author": "Reza Sameni",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02169v1 Announce Type: new \nAbstract: We study the geometry of Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves in binary classification problems. The key finding is that many of the most commonly used binary classification metrics are merely functions of the composition function $G := F_p \\circ F_n^{-1}$, where $F_p(\\cdot)$ and $F_n(\\cdot)$ are the class-conditional cumulative distribution functions of the classifier scores in the positive and negative classes, respectively. This geometric perspective facilitates the selection of operating points, understanding the effect of decision thresholds, and comparison between classifiers. It also helps explain how the shapes and geometry of ROC/PR curves reflect classifier behavior, providing objective tools for building classifiers optimized for specific applications with context-specific constraints. We further explore the conditions for classifier dominance, present analytical and numerical examples demonstrating the effects of class separability and variance on ROC and PR geometries, and derive a link between the positive-to-negative class leakage function $G(\\cdot)$ and the Kullback--Leibler divergence. The framework highlights practical considerations, such as model calibration, cost-sensitive optimization, and operating point selection under real-world capacity constraints, enabling more informed approaches to classifier deployment and decision-making."
      },
      {
        "id": "oai:arXiv.org:2504.02175v1",
        "title": "Who Should Set the Standards? Analysing Censored Arabic Content on Facebook during the Palestine-Israel Conflict",
        "link": "https://arxiv.org/abs/2504.02175",
        "author": "Walid Magdy, Hamdy Mubarak, Joni Salminen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02175v1 Announce Type: new \nAbstract: Nascent research on human-computer interaction concerns itself with fairness of content moderation systems. Designing globally applicable content moderation systems requires considering historical, cultural, and socio-technical factors. Inspired by this line of work, we investigate Arab users' perception of Facebook's moderation practices. We collect a set of 448 deleted Arabic posts, and we ask Arab annotators to evaluate these posts based on (a) Facebook Community Standards (FBCS) and (b) their personal opinion. Each post was judged by 10 annotators to account for subjectivity. Our analysis shows a clear gap between the Arabs' understanding of the FBCS and how Facebook implements these standards. The study highlights a need for discussion on the moderation guidelines on social media platforms about who decides the moderation guidelines, how these guidelines are interpreted, and how well they represent the views of marginalised user communities."
      },
      {
        "id": "oai:arXiv.org:2504.02178v1",
        "title": "Subasa -- Adapting Language Models for Low-resourced Offensive Language Detection in Sinhala",
        "link": "https://arxiv.org/abs/2504.02178",
        "author": "Shanilka Haturusinghe, Tharindu Cyril Weerasooriya, Marcos Zampieri, Christopher M. Homan, S. R. Liyanage",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02178v1 Announce Type: new \nAbstract: Accurate detection of offensive language is essential for a number of applications related to social media safety. There is a sharp contrast in performance in this task between low and high-resource languages. In this paper, we adapt fine-tuning strategies that have not been previously explored for Sinhala in the downstream task of offensive language detection. Using this approach, we introduce four models: \"Subasa-XLM-R\", which incorporates an intermediate Pre-Finetuning step using Masked Rationale Prediction. Two variants of \"Subasa-Llama\" and \"Subasa-Mistral\", are fine-tuned versions of Llama (3.2) and Mistral (v0.3), respectively, with a task-specific strategy. We evaluate our models on the SOLD benchmark dataset for Sinhala offensive language detection. All our models outperform existing baselines. Subasa-XLM-R achieves the highest Macro F1 score (0.84) surpassing state-of-the-art large language models like GPT-4o when evaluated on the same SOLD benchmark dataset under zero-shot settings. The models and code are publicly available."
      },
      {
        "id": "oai:arXiv.org:2504.02180v1",
        "title": "Foreground Focus: Enhancing Coherence and Fidelity in Camouflaged Image Generation",
        "link": "https://arxiv.org/abs/2504.02180",
        "author": "Pei-Chi Chen, Yi Yao, Chan-Feng Hsu, HongXia Xie, Hung-Jen Chen, Hong-Han Shuai, Wen-Huang Cheng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02180v1 Announce Type: new \nAbstract: Camouflaged image generation is emerging as a solution to data scarcity in camouflaged vision perception, offering a cost-effective alternative to data collection and labeling. Recently, the state-of-the-art approach successfully generates camouflaged images using only foreground objects. However, it faces two critical weaknesses: 1) the background knowledge does not integrate effectively with foreground features, resulting in a lack of foreground-background coherence (e.g., color discrepancy); 2) the generation process does not prioritize the fidelity of foreground objects, which leads to distortion, particularly for small objects. To address these issues, we propose a Foreground-Aware Camouflaged Image Generation (FACIG) model. Specifically, we introduce a Foreground-Aware Feature Integration Module (FAFIM) to strengthen the integration between foreground features and background knowledge. In addition, a Foreground-Aware Denoising Loss is designed to enhance foreground reconstruction supervision. Experiments on various datasets show our method outperforms previous methods in overall camouflaged image quality and foreground fidelity."
      },
      {
        "id": "oai:arXiv.org:2504.02199v1",
        "title": "ESC: Erasing Space Concept for Knowledge Deletion",
        "link": "https://arxiv.org/abs/2504.02199",
        "author": "Tae-Young Lee, Sundong Park, Minwoo Jeon, Hyoseok Hwang, Gyeong-Moon Park",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02199v1 Announce Type: new \nAbstract: As concerns regarding privacy in deep learning continue to grow, individuals are increasingly apprehensive about the potential exploitation of their personal knowledge in trained models. Despite several research efforts to address this, they often fail to consider the real-world demand from users for complete knowledge erasure. Furthermore, our investigation reveals that existing methods have a risk of leaking personal knowledge through embedding features. To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD. Our extensive experiments on various datasets and models demonstrate that our proposed methods achieve the fastest and state-of-the-art performance. Notably, our methods are applicable to diverse forgetting scenarios, such as facial domain setting, demonstrating the generalizability of our methods. The code is available at http://github.com/KU-VGI/ESC ."
      },
      {
        "id": "oai:arXiv.org:2504.02213v1",
        "title": "Secure Generalization through Stochastic Bidirectional Parameter Updates Using Dual-Gradient Mechanism",
        "link": "https://arxiv.org/abs/2504.02213",
        "author": "Shourya Goel, Himanshi Tibrewal, Anant Jain, Anshul Pundhir, Pravendra Singh",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02213v1 Announce Type: new \nAbstract: Federated learning (FL) has gained increasing attention due to privacy-preserving collaborative training on decentralized clients, mitigating the need to upload sensitive data to a central server directly. Nonetheless, recent research has underscored the risk of exposing private data to adversaries, even within FL frameworks. In general, existing methods sacrifice performance while ensuring resistance to privacy leakage in FL. We overcome these issues and generate diverse models at a global server through the proposed stochastic bidirectional parameter update mechanism. Using diverse models, we improved the generalization and feature representation in the FL setup, which also helped to improve the robustness of the model against privacy leakage without hurting the model's utility. We use global models from past FL rounds to follow systematic perturbation in parameter space at the server to ensure model generalization and resistance against privacy attacks. We generate diverse models (in close neighborhoods) for each client by using systematic perturbations in model parameters at a fine-grained level (i.e., altering each convolutional filter across the layers of the model) to improve the generalization and security perspective. We evaluated our proposed approach on four benchmark datasets to validate its superiority. We surpassed the state-of-the-art methods in terms of model utility and robustness towards privacy leakage. We have proven the effectiveness of our method by evaluating performance using several quantitative and qualitative results."
      },
      {
        "id": "oai:arXiv.org:2504.02214v1",
        "title": "Geospatial Artificial Intelligence for Satellite-based Flood Extent Mapping: Concepts, Advances, and Future Perspectives",
        "link": "https://arxiv.org/abs/2504.02214",
        "author": "Hyunho Lee, Wenwen Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02214v1 Announce Type: new \nAbstract: Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent mapping systematically integrates artificial intelligence techniques with satellite data to identify flood events and assess their impacts, for disaster management and spatial decision-making. The primary output often includes flood extent maps, which delineate the affected areas, along with additional analytical outputs such as uncertainty estimation and change detection."
      },
      {
        "id": "oai:arXiv.org:2504.02221v1",
        "title": "Learning and Improving Backgammon Strategy",
        "link": "https://arxiv.org/abs/2504.02221",
        "author": "Gregory R. Galperin",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02221v1 Announce Type: new \nAbstract: A novel approach to learning is presented, combining features of on-line and off-line methods to achieve considerable performance in the task of learning a backgammon value function in a process that exploits the processing power of parallel supercomputers. The off-line methods comprise a set of techniques for parallelizing neural network training and $TD(\\lambda)$ reinforcement learning; here Monte-Carlo ``Rollouts'' are introduced as a massively parallel on-line policy improvement technique which applies resources to the decision points encountered during the search of the game tree to further augment the learned value function estimate. A level of play roughly as good as, or possibly better than, the current champion human and computer backgammon players has been achieved in a short period of learning."
      },
      {
        "id": "oai:arXiv.org:2504.02231v1",
        "title": "AC-LoRA: Auto Component LoRA for Personalized Artistic Style Image Generation",
        "link": "https://arxiv.org/abs/2504.02231",
        "author": "Zhipu Cui, Andong Tian, Zhi Ying, Jialiang Lu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02231v1 Announce Type: new \nAbstract: Personalized image generation allows users to preserve styles or subjects of a provided small set of images for further image generation. With the advancement in large text-to-image models, many techniques have been developed to efficiently fine-tune those models for personalization, such as Low Rank Adaptation (LoRA). However, LoRA-based methods often face the challenge of adjusting the rank parameter to achieve satisfactory results. To address this challenge, AutoComponent-LoRA (AC-LoRA) is proposed, which is able to automatically separate the signal component and noise component of the LoRA matrices for fast and efficient personalized artistic style image generation. This method is based on Singular Value Decomposition (SVD) and dynamic heuristics to update the hyperparameters during training. Superior performance over existing methods in overcoming model underfitting or overfitting problems is demonstrated. The results were validated using FID, CLIP, DINO, and ImageReward, achieving an average of 9% improvement."
      },
      {
        "id": "oai:arXiv.org:2504.02244v1",
        "title": "SocialGesture: Delving into Multi-person Gesture Understanding",
        "link": "https://arxiv.org/abs/2504.02244",
        "author": "Xu Cao, Pranav Virupaksha, Wenqi Jia, Bolin Lai, Fiona Ryan, Sangmin Lee, James M. Rehg",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02244v1 Announce Type: new \nAbstract: Previous research in human gesture recognition has largely overlooked multi-person interactions, which are crucial for understanding the social context of naturally occurring gestures. This limitation in existing datasets presents a significant challenge in aligning human gestures with other modalities like language and speech. To address this issue, we introduce SocialGesture, the first large-scale dataset specifically designed for multi-person gesture analysis. SocialGesture features a diverse range of natural scenarios and supports multiple gesture analysis tasks, including video-based recognition and temporal localization, providing a valuable resource for advancing the study of gesture during complex social interactions. Furthermore, we propose a novel visual question answering (VQA) task to benchmark vision language models'(VLMs) performance on social gesture understanding. Our findings highlight several limitations of current gesture recognition models, offering insights into future directions for improvement in this field. SocialGesture is available at huggingface.co/datasets/IrohXu/SocialGesture."
      },
      {
        "id": "oai:arXiv.org:2504.02248v1",
        "title": "CRC-SGAD: Conformal Risk Control for Supervised Graph Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.02248",
        "author": "Songran Bai, Xiaolong Zheng, Daniel Dajun Zeng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02248v1 Announce Type: new \nAbstract: Graph Anomaly Detection (GAD) is critical in security-sensitive domains, yet faces reliability challenges: miscalibrated confidence estimation (underconfidence in normal nodes, overconfidence in anomalies), adversarial vulnerability of derived confidence score under structural perturbations, and limited efficacy of conventional calibration methods for sparse anomaly patterns. Thus we propose CRC-SGAD, a framework integrating statistical risk control into GAD via two innovations: (1) A Dual-Threshold Conformal Risk Control mechanism that provides theoretically guaranteed bounds for both False Negative Rate (FNR) and False Positive Rate (FPR) through providing prediction sets; (2) A Subgraph-aware Spectral Graph Neural Calibrator (SSGNC) that optimizes node representations through adaptive spectral filtering while reducing the size of prediction sets via hybrid loss optimization. Experiments on four datasets and five GAD models demonstrate statistically significant improvements in FNR and FPR control and prediction set size. CRC-SGAD establishes a paradigm for statistically rigorous anomaly detection in graph-structured security applications."
      },
      {
        "id": "oai:arXiv.org:2504.02251v1",
        "title": "Quantum Lipschitz Bandits",
        "link": "https://arxiv.org/abs/2504.02251",
        "author": "Bongsoo Yi, Yue Kang, Yao Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02251v1 Announce Type: new \nAbstract: The Lipschitz bandit is a key variant of stochastic bandit problems where the expected reward function satisfies a Lipschitz condition with respect to an arm metric space. With its wide-ranging practical applications, various Lipschitz bandit algorithms have been developed, achieving the cumulative regret lower bound of order $\\tilde O(T^{(d_z+1)/(d_z+2)})$ over time horizon $T$. Motivated by recent advancements in quantum computing and the demonstrated success of quantum Monte Carlo in simpler bandit settings, we introduce the first quantum Lipschitz bandit algorithms to address the challenges of continuous action spaces and non-linear reward functions. Specifically, we first leverage the elimination-based framework to propose an efficient quantum Lipschitz bandit algorithm named Q-LAE. Next, we present novel modifications to the classical Zooming algorithm, which results in a simple quantum Lipschitz bandit method, Q-Zooming. Both algorithms exploit the computational power of quantum methods to achieve an improved regret bound of $\\tilde O(T^{d_z/(d_z+1)})$. Comprehensive experiments further validate our improved theoretical findings, demonstrating superior empirical performance compared to existing Lipschitz bandit methods."
      },
      {
        "id": "oai:arXiv.org:2504.02252v1",
        "title": "Adapting World Models with Latent-State Dynamics Residuals",
        "link": "https://arxiv.org/abs/2504.02252",
        "author": "JB Lanier, Kyungmin Kim, Armin Karamzade, Yifei Liu, Ankita Sinha, Kat He, Davide Corsi, Roy Fox",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02252v1 Announce Type: new \nAbstract: Simulation-to-reality reinforcement learning (RL) faces the critical challenge of reconciling discrepancies between simulated and real-world dynamics, which can severely degrade agent performance. A promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images. To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to target environments through residual corrections of latent-state dynamics rather than of explicit observed states. Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world. In multiple vision-based MuJoCo domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail."
      },
      {
        "id": "oai:arXiv.org:2504.02254v1",
        "title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks",
        "link": "https://arxiv.org/abs/2504.02254",
        "author": "Seunghyun Yoo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02254v1 Announce Type: new \nAbstract: Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game \"Connections\", we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment."
      },
      {
        "id": "oai:arXiv.org:2504.02259v1",
        "title": "Re-thinking Temporal Search for Long-Form Video Understanding",
        "link": "https://arxiv.org/abs/2504.02259",
        "author": "Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, Manling Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02259v1 Announce Type: new \nAbstract: Efficient understanding of long-form videos remains a significant challenge in computer vision. In this work, we revisit temporal search paradigms for long-form video understanding, studying a fundamental issue pertaining to all state-of-the-art (SOTA) long-context vision-language models (VLMs). In particular, our contributions are two-fold: First, we formulate temporal search as a Long Video Haystack problem, i.e., finding a minimal set of relevant frames (typically one to five) among tens of thousands of frames from real-world long videos given specific queries. To validate our formulation, we create LV-Haystack, the first benchmark containing 3,874 human-annotated instances with fine-grained evaluation metrics for assessing keyframe search quality and computational efficiency. Experimental results on LV-Haystack highlight a significant research gap in temporal search capabilities, with SOTA keyframe selection methods achieving only 2.1% temporal F1 score on the LVBench subset.\n  Next, inspired by visual search in images, we re-think temporal searching and propose a lightweight keyframe searching framework, T*, which casts the expensive temporal search as a spatial search problem. T* leverages superior visual localization capabilities typically used in images and introduces an adaptive zooming-in mechanism that operates across both temporal and spatial dimensions. Our extensive experiments show that when integrated with existing methods, T* significantly improves SOTA long-form video understanding performance. Specifically, under an inference budget of 32 frames, T* improves GPT-4o's performance from 50.5% to 53.1% and LLaVA-OneVision-72B's performance from 56.5% to 62.4% on LongVideoBench XL subset. Our PyTorch code, benchmark dataset and models are included in the Supplementary material."
      },
      {
        "id": "oai:arXiv.org:2504.02260v1",
        "title": "Implicit Neural Differential Model for Spatiotemporal Dynamics",
        "link": "https://arxiv.org/abs/2504.02260",
        "author": "Deepak Akhare, Pan Du, Tengfei Luo, Jian-Xun Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02260v1 Announce Type: new \nAbstract: Hybrid neural-physics modeling frameworks through differentiable programming have emerged as powerful tools in scientific machine learning, enabling the integration of known physics with data-driven learning to improve prediction accuracy and generalizability. However, most existing hybrid frameworks rely on explicit recurrent formulations, which suffer from numerical instability and error accumulation during long-horizon forecasting. In this work, we introduce Im-PiNDiff, a novel implicit physics-integrated neural differentiable solver for stable and accurate modeling of spatiotemporal dynamics. Inspired by deep equilibrium models, Im-PiNDiff advances the state using implicit fixed-point layers, enabling robust long-term simulation while remaining fully end-to-end differentiable. To enable scalable training, we introduce a hybrid gradient propagation strategy that integrates adjoint-state methods with reverse-mode automatic differentiation. This approach eliminates the need to store intermediate solver states and decouples memory complexity from the number of solver iterations, significantly reducing training overhead. We further incorporate checkpointing techniques to manage memory in long-horizon rollouts. Numerical experiments on various spatiotemporal PDE systems, including advection-diffusion processes, Burgers' dynamics, and multi-physics chemical vapor infiltration processes, demonstrate that Im-PiNDiff achieves superior predictive performance, enhanced numerical stability, and substantial reductions in memory and runtime cost relative to explicit and naive implicit baselines. This work provides a principled, efficient, and scalable framework for hybrid neural-physics modeling."
      },
      {
        "id": "oai:arXiv.org:2504.02261v1",
        "title": "WonderTurbo: Generating Interactive 3D World in 0.72 Seconds",
        "link": "https://arxiv.org/abs/2504.02261",
        "author": "Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, Wenjun Mei",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02261v1 Announce Type: new \nAbstract: Interactive 3D generation is gaining momentum and capturing extensive attention for its potential to create immersive virtual experiences. However, a critical challenge in current 3D generation technologies lies in achieving real-time interactivity. To address this issue, we introduce WonderTurbo, the first real-time interactive 3D scene generation framework capable of generating novel perspectives of 3D scenes within 0.72 seconds. Specifically, WonderTurbo accelerates both geometric and appearance modeling in 3D scene generation. In terms of geometry, we propose StepSplat, an innovative method that constructs efficient 3D geometric representations through dynamic updates, each taking only 0.26 seconds. Additionally, we design QuickDepth, a lightweight depth completion module that provides consistent depth input for StepSplat, further enhancing geometric accuracy. For appearance modeling, we develop FastPaint, a 2-steps diffusion model tailored for instant inpainting, which focuses on maintaining spatial appearance consistency. Experimental results demonstrate that WonderTurbo achieves a remarkable 15X speedup compared to baseline methods, while preserving excellent spatial consistency and delivering high-quality output."
      },
      {
        "id": "oai:arXiv.org:2504.02264v1",
        "title": "MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception",
        "link": "https://arxiv.org/abs/2504.02264",
        "author": "Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo, Jiayin Zhu, Pengfei Li, Zilong Chen, Huiming Yang, Zhiwei Li, Lening Wang, Tiao Tan, Huaping Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02264v1 Announce Type: new \nAbstract: Advanced driver assistance systems require a comprehensive understanding of the driver's mental/physical state and traffic context but existing works often neglect the potential benefits of joint learning between these tasks. This paper proposes MMTL-UniAD, a unified multi-modal multi-task learning framework that simultaneously recognizes driver behavior (e.g., looking around, talking), driver emotion (e.g., anxiety, happiness), vehicle behavior (e.g., parking, turning), and traffic context (e.g., traffic jam, traffic smooth). A key challenge is avoiding negative transfer between tasks, which can impair learning performance. To address this, we introduce two key components into the framework: one is the multi-axis region attention network to extract global context-sensitive features, and the other is the dual-branch multimodal embedding to learn multimodal embeddings from both task-shared and task-specific features. The former uses a multi-attention mechanism to extract task-relevant features, mitigating negative transfer caused by task-unrelated features. The latter employs a dual-branch structure to adaptively adjust task-shared and task-specific parameters, enhancing cross-task knowledge transfer while reducing task conflicts. We assess MMTL-UniAD on the AIDE dataset, using a series of ablation studies, and show that it outperforms state-of-the-art methods across all four tasks. The code is available on https://github.com/Wenzhuo-Liu/MMTL-UniAD."
      },
      {
        "id": "oai:arXiv.org:2504.02268v1",
        "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data",
        "link": "https://arxiv.org/abs/2504.02268",
        "author": "Waris Gill (Redis, Virginia Tech), Justin Cechmanek (Redis), Tyler Hutcherson (Redis), Srijith Rajamohan (Redis), Jen Agarwal (Redis), Muhammad Ali Gulzar (Virginia Tech), Manvinder Singh (Redis), Benoit Dion",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02268v1 Announce Type: new \nAbstract: This report investigates enhancing semantic caching effectiveness by employing specialized, fine-tuned embedding models. Semantic caching relies on embedding similarity rather than exact key matching, presenting unique challenges in balancing precision, query latency, and computational efficiency. We propose leveraging smaller, domain-specific embedding models, fine-tuned with targeted real-world and synthetically generated datasets. Our empirical evaluations demonstrate that compact embedding models fine-tuned for just one epoch on specialized datasets significantly surpass both state-of-the-art open-source and proprietary alternatives in precision and recall. Moreover, we introduce a novel synthetic data generation pipeline for the semantic cache that mitigates the challenge of limited domain-specific annotated data, further boosting embedding performance. Our approach effectively balances computational overhead and accuracy, establishing a viable and efficient strategy for practical semantic caching implementations."
      },
      {
        "id": "oai:arXiv.org:2504.02270v1",
        "title": "MinkOcc: Towards real-time label-efficient semantic occupancy prediction",
        "link": "https://arxiv.org/abs/2504.02270",
        "author": "Samuel Sze, Daniele De Martini, Lars Kunze",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02270v1 Announce Type: new \nAbstract: Developing 3D semantic occupancy prediction models often relies on dense 3D annotations for supervised learning, a process that is both labor and resource-intensive, underscoring the need for label-efficient or even label-free approaches. To address this, we introduce MinkOcc, a multi-modal 3D semantic occupancy prediction framework for cameras and LiDARs that proposes a two-step semi-supervised training procedure. Here, a small dataset of explicitly 3D annotations warm-starts the training process; then, the supervision is continued by simpler-to-annotate accumulated LiDAR sweeps and images -- semantically labelled through vision foundational models. MinkOcc effectively utilizes these sensor-rich supervisory cues and reduces reliance on manual labeling by 90\\% while maintaining competitive accuracy. In addition, the proposed model incorporates information from LiDAR and camera data through early fusion and leverages sparse convolution networks for real-time prediction. With its efficiency in both supervision and computation, we aim to extend MinkOcc beyond curated datasets, enabling broader real-world deployment of 3D semantic occupancy prediction in autonomous driving."
      },
      {
        "id": "oai:arXiv.org:2504.02272v1",
        "title": "Generative Classifier for Domain Generalization",
        "link": "https://arxiv.org/abs/2504.02272",
        "author": "Shaocong Long, Qianyu Zhou, Xiangtai Li, Chenhao Ying, Yunhai Tong, Lizhuang Ma, Yuan Luo, Dacheng Tao",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02272v1 Announce Type: new \nAbstract: Domain generalization (DG) aims to improve the generalizability of computer vision models toward distribution shifts. The mainstream DG methods focus on learning domain invariance, however, such methods overlook the potential inherent in domain-specific information. While the prevailing practice of discriminative linear classifier has been tailored to domain-invariant features, it struggles when confronted with diverse domain-specific information, e.g., intra-class shifts, that exhibits multi-modality. To address these issues, we explore the theoretical implications of relying on domain invariance, revealing the crucial role of domain-specific information in mitigating the target risk for DG. Drawing from these insights, we propose Generative Classifier-driven Domain Generalization (GCDG), introducing a generative paradigm for the DG classifier based on Gaussian Mixture Models (GMMs) for each class across domains. GCDG consists of three key modules: Heterogeneity Learning Classifier~(HLC), Spurious Correlation Blocking~(SCB), and Diverse Component Balancing~(DCB). Concretely, HLC attempts to model the feature distributions and thereby capture valuable domain-specific information via GMMs. SCB identifies the neural units containing spurious correlations and perturbs them, mitigating the risk of HLC learning spurious patterns. Meanwhile, DCB ensures a balanced contribution of components in HLC, preventing the underestimation or neglect of critical components. In this way, GCDG excels in capturing the nuances of domain-specific information characterized by diverse distributions. GCDG demonstrates the potential to reduce the target risk and encourage flat minima, improving the generalizability. Extensive experiments show GCDG's comparable performance on five DG benchmarks and one face anti-spoofing dataset, seamlessly integrating into existing DG methods with consistent improvements."
      },
      {
        "id": "oai:arXiv.org:2504.02273v1",
        "title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models",
        "link": "https://arxiv.org/abs/2504.02273",
        "author": "Hung Le, Dai Do, Dung Nguyen, Svetha Venkatesh",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02273v1 Announce Type: new \nAbstract: Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these successes have been largely demonstrated on large-scale models with billions of parameters, where a strong pretraining foundation ensures effective initial exploration. In contrast, RL remains challenging for tiny LLMs with 1 billion parameters or fewer because they lack the necessary pretraining strength to explore effectively, often leading to suboptimal reasoning patterns. This work introduces a novel intrinsic motivation approach that leverages episodic memory to address this challenge, improving tiny LLMs in CoT reasoning tasks. Inspired by human memory-driven learning, our method leverages successful reasoning patterns stored in memory while allowing for controlled exploration to generate novel responses. Intrinsic rewards are computed efficiently using a kNN-based episodic memory, allowing the model to discover new reasoning strategies while quickly adapting to effective past solutions. Experiments on fine-tuning GSM8K and AI-MO datasets demonstrate that our approach significantly enhances smaller LLMs' sample efficiency and generalization capability, making RL-based reasoning improvements more accessible in low-resource settings."
      },
      {
        "id": "oai:arXiv.org:2504.02275v1",
        "title": "Enhancing Customer Contact Efficiency with Graph Neural Networks in Credit Card Fraud Detection Workflow",
        "link": "https://arxiv.org/abs/2504.02275",
        "author": "Menghao Huo, Kuan Lu, Qiang Zhu, Zhenrui Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02275v1 Announce Type: new \nAbstract: Credit card fraud has been a persistent issue since the last century, causing significant financial losses to the industry. The most effective way to prevent fraud is by contacting customers to verify suspicious transactions. However, while these systems are designed to detect fraudulent activity, they often mistakenly flag legitimate transactions, leading to unnecessary declines that disrupt the user experience and erode customer trust. Frequent false positives can frustrate customers, resulting in dissatisfaction, increased complaints, and a diminished sense of security. To address these limitations, we propose a fraud detection framework incorporating Relational Graph Convolutional Networks (RGCN) to enhance the accuracy and efficiency of identifying fraudulent transactions. By leveraging the relational structure of transaction data, our model reduces the need for direct customer confirmation while maintaining high detection performance. Our experiments are conducted using the IBM credit card transaction dataset to evaluate the effectiveness of this approach."
      },
      {
        "id": "oai:arXiv.org:2504.02277v1",
        "title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation",
        "link": "https://arxiv.org/abs/2504.02277",
        "author": "Amit Rand, Hadi Ibrahim",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02277v1 Announce Type: new \nAbstract: Medical imaging, particularly X-ray analysis, often involves detecting multiple conditions simultaneously within a single scan, making multi-label classification crucial for real-world clinical applications. We present the Medical X-ray Attention (MXA) block, a novel attention mechanism tailored specifically to address the unique challenges of X-ray abnormality detection. The MXA block enhances traditional Multi-Head Self Attention (MHSA) by integrating a specialized module that efficiently captures both detailed local information and broader global context. To the best of our knowledge, this is the first work to propose a task-specific attention mechanism for diagnosing chest X-rays, as well as to attempt multi-label classification using an Efficient Vision Transformer (EfficientViT). By embedding the MXA block within the EfficientViT architecture and employing knowledge distillation, our proposed model significantly improves performance on the CheXpert dataset, a widely used benchmark for multi-label chest X-ray abnormality detection. Our approach achieves an area under the curve (AUC) of 0.85, an absolute improvement of 0.19 compared to our baseline model's AUC of 0.66, corresponding to a substantial approximate 233% relative improvement over random guessing (AUC = 0.5)."
      },
      {
        "id": "oai:arXiv.org:2504.02279v1",
        "title": "MultiTSF: Transformer-based Sensor Fusion for Human-Centric Multi-view and Multi-modal Action Recognition",
        "link": "https://arxiv.org/abs/2504.02279",
        "author": "Trung Thanh Nguyen, Yasutomo Kawanishi, Vijay John, Takahiro Komamizu, Ichiro Ide",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02279v1 Announce Type: new \nAbstract: Action recognition from multi-modal and multi-view observations holds significant potential for applications in surveillance, robotics, and smart environments. However, existing methods often fall short of addressing real-world challenges such as diverse environmental conditions, strict sensor synchronization, and the need for fine-grained annotations. In this study, we propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF). The proposed method leverages a Transformer-based to dynamically model inter-view relationships and capture temporal dependencies across multiple views. Additionally, we introduce a Human Detection Module to generate pseudo-ground-truth labels, enabling the model to prioritize frames containing human activity and enhance spatial feature learning. Comprehensive experiments conducted on our in-house MultiSensor-Home dataset and the existing MM-Office dataset demonstrate that MultiTSF outperforms state-of-the-art methods in both video sequence-level and frame-level action recognition settings."
      },
      {
        "id": "oai:arXiv.org:2504.02283v1",
        "title": "Ga$_2$O$_3$ TCAD Mobility Parameter Calibration using Simulation Augmented Machine Learning with Physics Informed Neural Network",
        "link": "https://arxiv.org/abs/2504.02283",
        "author": "Le Minh Long Nguyen, Edric Ong, Matthew Eng, Yuhao Zhang, Hiu Yung Wong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02283v1 Announce Type: new \nAbstract: In this paper, we demonstrate the possibility of performing automatic Technology Computer-Aided-Design (TCAD) parameter calibration using machine learning, verified with experimental data. The machine only needs to be trained by TCAD data. Schottky Barrier Diode (SBD) fabricated with emerging ultra-wide-bandgap material, Gallium Oxide (Ga$_2$O$_3$), is measured and its current-voltage (IV) is used for Ga$_2$O$_3$ Philips Unified Mobility (PhuMob) model parameters, effective anode workfunction, and ambient temperature extraction (7 parameters). A machine comprised of an autoencoder (AE) and a neural network (NN) (AE-NN) is used. Ga$_2$O$_3$ PhuMob parameters are extracted from the noisy experimental curves. TCAD simulation with the extracted parameters shows that the quality of the parameters is as good as an expert's calibration at the pre-turned-on regime but not in the on-state regime. By using a simple physics-informed neural network (PINN) (AE-PINN), the machine performs as well as the human expert in all regimes."
      },
      {
        "id": "oai:arXiv.org:2504.02285v1",
        "title": "Tree-based Models for Vertical Federated Learning: A Survey",
        "link": "https://arxiv.org/abs/2504.02285",
        "author": "Bingchen Qian, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02285v1 Announce Type: new \nAbstract: Tree-based models have achieved great success in a wide range of real-world applications due to their effectiveness, robustness, and interpretability, which inspired people to apply them in vertical federated learning (VFL) scenarios in recent years. In this paper, we conduct a comprehensive study to give an overall picture of applying tree-based models in VFL, from the perspective of their communication and computation protocols. We categorize tree-based models in VFL into two types, i.e., feature-gathering models and label-scattering models, and provide a detailed discussion regarding their characteristics, advantages, privacy protection mechanisms, and applications. This study also focuses on the implementation of tree-based models in VFL, summarizing several design principles for better satisfying various requirements from both academic research and industrial deployment. We conduct a series of experiments to provide empirical observations on the differences and advances of different types of tree-based models."
      },
      {
        "id": "oai:arXiv.org:2504.02286v1",
        "title": "Moment Quantization for Video Temporal Grounding",
        "link": "https://arxiv.org/abs/2504.02286",
        "author": "Xiaolong Sun, Le Wang, Sanping Zhou, Liushuai Shi, Kun Xia, Mengnan Liu, Yabing Wang, Gang Hua",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02286v1 Announce Type: new \nAbstract: Video temporal grounding is a critical video understanding task, which aims to localize moments relevant to a language description. The challenge of this task lies in distinguishing relevant and irrelevant moments. Previous methods focused on learning continuous features exhibit weak differentiation between foreground and background features. In this paper, we propose a novel Moment-Quantization based Video Temporal Grounding method (MQVTG), which quantizes the input video into various discrete vectors to enhance the discrimination between relevant and irrelevant moments. Specifically, MQVTG maintains a learnable moment codebook, where each video moment matches a codeword. Considering the visual diversity, i.e., various visual expressions for the same moment, MQVTG treats moment-codeword matching as a clustering process without using discrete vectors, avoiding the loss of useful information from direct hard quantization. Additionally, we employ effective prior-initialization and joint-projection strategies to enhance the maintained moment codebook. With its simple implementation, the proposed method can be integrated into existing temporal grounding models as a plug-and-play component. Extensive experiments on six popular benchmarks demonstrate the effectiveness and generalizability of MQVTG, significantly outperforming state-of-the-art methods. Further qualitative analysis shows that our method effectively groups relevant features and separates irrelevant ones, aligning with our goal of enhancing discrimination."
      },
      {
        "id": "oai:arXiv.org:2504.02287v1",
        "title": "MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion",
        "link": "https://arxiv.org/abs/2504.02287",
        "author": "Trung Thanh Nguyen, Yasutomo Kawanishi, Vijay John, Takahiro Komamizu, Ichiro Ide",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02287v1 Announce Type: new \nAbstract: Multi-modal multi-view action recognition is a rapidly growing field in computer vision, offering significant potential for applications in surveillance. However, current datasets often fail to address real-world challenges such as wide-area environmental conditions, asynchronous data streams, and the lack of frame-level annotations. Furthermore, existing methods face difficulties in effectively modeling inter-view relationships and enhancing spatial feature learning. In this study, we propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF) method and introduce the MultiSensor-Home dataset, a novel benchmark designed for comprehensive action recognition in home environments. The MultiSensor-Home dataset features untrimmed videos captured by distributed sensors, providing high-resolution RGB and audio data along with detailed multi-view frame-level action labels. The proposed MultiTSF method leverages a Transformer-based fusion mechanism to dynamically model inter-view relationships. Furthermore, the method also integrates a external human detection module to enhance spatial feature learning. Experiments on MultiSensor-Home and MM-Office datasets demonstrate the superiority of MultiTSF over the state-of-the-art methods. The quantitative and qualitative results highlight the effectiveness of the proposed method in advancing real-world multi-modal multi-view action recognition."
      },
      {
        "id": "oai:arXiv.org:2504.02293v1",
        "title": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla",
        "link": "https://arxiv.org/abs/2504.02293",
        "author": "Sharif Md. Abdullah, Abhijit Paul, Shebuti Rayana, Ahmedul Kabir, Zarif Masud",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02293v1 Announce Type: new \nAbstract: Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language (BdSL) remains a understudied domain. Specifically, there are no works on Bangla text-to-gloss translation task. To address this gap, we begin by addressing the dataset problem. We take inspiration from grammatical rule based gloss generation used in Germany and American sign langauage (ASL) and adapt it for BdSL. We also leverage LLM to generate synthetic data and use back-translation, text generation for data augmentation. With dataset prepared, we started experimentation. We fine-tuned pretrained mBART-50 and mBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a novel seq-to-seq model with multi-head attention. We observe significant high performance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual model from Facebook. We then explored why we observe such high performance with mBART. We soon notice an interesting property of mBART -- it was trained on shuffled and masked text data. And as we know, gloss form has shuffling property. So we hypothesize that mBART is inherently good at text-to-gloss tasks. To find support against this hypothesis, we trained mBART-50 on PHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50 finetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark, far outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 = 55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on the results, this study proposes a new paradigm for text-to-gloss task using mBART models. Additionally, our results show that BdSL text-to-gloss task can greatly benefit from rule-based synthetic dataset."
      },
      {
        "id": "oai:arXiv.org:2504.02298v1",
        "title": "SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks",
        "link": "https://arxiv.org/abs/2504.02298",
        "author": "Xinyu Luo, Kecheng Chen, Pao-Sheng Vincent Sun, Chris Xing Tian, Arindam Basu, Haoliang Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02298v1 Announce Type: new \nAbstract: Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose $\\textbf{SP}$ike-$\\textbf{A}$ware $\\textbf{C}$onsistency $\\textbf{E}$nhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets, including CIFAR-10-C, CIFAR-100-C, Tiny-ImageNet-C and DVS Gesture-C. Furthermore, SPACE demonstrates strong generalization across different model architectures, achieving consistent performance improvements on both VGG9 and ResNet11. Experimental results show that SPACE outperforms state-of-the-art methods, highlighting its effectiveness and robustness in real-world settings."
      },
      {
        "id": "oai:arXiv.org:2504.02304v1",
        "title": "Measurement of LLM's Philosophies of Human Nature",
        "link": "https://arxiv.org/abs/2504.02304",
        "author": "Minheng Ni, Ennan Wu, Zidong Gong, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Lijuan Wang, Wangmeng Zuo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02304v1 Announce Type: new \nAbstract: The widespread application of artificial intelligence (AI) in various tasks, along with frequent reports of conflicts or violations involving AI, has sparked societal concerns about interactions with AI systems. Based on Wrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically validated over decades to effectively assess individuals' attitudes toward human nature, we design the standardized psychological scale specifically targeting large language models (LLM), named the Machine-based Philosophies of Human Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature across six dimensions, we reveal that current LLMs exhibit a systemic lack of trust in humans, and there is a significant negative correlation between the model's intelligence level and its trust in humans. Furthermore, we propose a mental loop learning framework, which enables LLM to continuously optimize its value system during virtual interactions by constructing moral scenarios, thereby improving its attitude toward human nature. Experiments demonstrate that mental loop learning significantly enhances their trust in humans compared to persona or instruction prompts. This finding highlights the potential of human-based psychological assessments for LLM, which can not only diagnose cognitive biases but also provide a potential solution for ethical learning in artificial intelligence. We release the M-PHNS evaluation code and data at https://github.com/kodenii/M-PHNS."
      },
      {
        "id": "oai:arXiv.org:2504.02310v1",
        "title": "Improving Harmful Text Detection with Joint Retrieval and External Knowledge",
        "link": "https://arxiv.org/abs/2504.02310",
        "author": "Zidong Yu, Shuo Wang, Nan Jiang, Weiqiang Huang, Xu Han, Junliang Du",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02310v1 Announce Type: new \nAbstract: Harmful text detection has become a crucial task in the development and deployment of large language models, especially as AI-generated content continues to expand across digital platforms. This study proposes a joint retrieval framework that integrates pre-trained language models with knowledge graphs to improve the accuracy and robustness of harmful text detection. Experimental results demonstrate that the joint retrieval approach significantly outperforms single-model baselines, particularly in low-resource training scenarios and multilingual environments. The proposed method effectively captures nuanced harmful content by leveraging external contextual information, addressing the limitations of traditional detection models. Future research should focus on optimizing computational efficiency, enhancing model interpretability, and expanding multimodal detection capabilities to better tackle evolving harmful content patterns. This work contributes to the advancement of AI safety, ensuring more trustworthy and reliable content moderation systems."
      },
      {
        "id": "oai:arXiv.org:2504.02312v1",
        "title": "OmniCam: Unified Multimodal Video Generation via Camera Control",
        "link": "https://arxiv.org/abs/2504.02312",
        "author": "Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, Cewu Lu, Lixin Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02312v1 Announce Type: new \nAbstract: Camera control, which achieves diverse visual effects by changing camera position and pose, has attracted widespread attention. However, existing methods face challenges such as complex interaction and limited control capabilities. To address these issues, we present OmniCam, a unified multimodal camera control framework. Leveraging large language models and video diffusion models, OmniCam generates spatio-temporally consistent videos. It supports various combinations of input modalities: the user can provide text or video with expected trajectory as camera path guidance, and image or video as content reference, enabling precise control over camera motion. To facilitate the training of OmniCam, we introduce the OmniTr dataset, which contains a large collection of high-quality long-sequence trajectories, videos, and corresponding descriptions. Experimental results demonstrate that our model achieves state-of-the-art performance in high-quality camera-controlled video generation across various metrics."
      },
      {
        "id": "oai:arXiv.org:2504.02316v1",
        "title": "ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation",
        "link": "https://arxiv.org/abs/2504.02316",
        "author": "Yuan Zhou, Shilong Jin, Litao Hua, Wanjun Lv, Haoran Duan, Jungong Han",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02316v1 Announce Type: new \nAbstract: Recent advances in zero-shot text-to-3D generation have revolutionized 3D content creation by enabling direct synthesis from textual descriptions. While state-of-the-art methods leverage 3D Gaussian Splatting with score distillation to enhance multi-view rendering through pre-trained text-to-image (T2I) models, they suffer from inherent view biases in T2I priors. These biases lead to inconsistent 3D generation, particularly manifesting as the multi-face Janus problem, where objects exhibit conflicting features across views. To address this fundamental challenge, we propose ConsDreamer, a novel framework that mitigates view bias by refining both the conditional and unconditional terms in the score distillation process: (1) a View Disentanglement Module (VDM) that eliminates viewpoint biases in conditional prompts by decoupling irrelevant view components and injecting precise camera parameters; and (2) a similarity-based partial order loss that enforces geometric consistency in the unconditional term by aligning cosine similarities with azimuth relationships. Extensive experiments demonstrate that ConsDreamer effectively mitigates the multi-face Janus problem in text-to-3D generation, outperforming existing methods in both visual quality and consistency."
      },
      {
        "id": "oai:arXiv.org:2504.02317v1",
        "title": "Temporal Gaussian Copula For Clinical Multivariate Time Series Data Imputation",
        "link": "https://arxiv.org/abs/2504.02317",
        "author": "Ye Su, Hezhe Qiao, Di Wu, Yuwen Chen, Lin Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02317v1 Announce Type: new \nAbstract: The imputation of the Multivariate time series (MTS) is particularly challenging since the MTS typically contains irregular patterns of missing values due to various factors such as instrument failures, interference from irrelevant data, and privacy regulations. Existing statistical methods and deep learning methods have shown promising results in time series imputation. In this paper, we propose a Temporal Gaussian Copula Model (TGC) for three-order MTS imputation. The key idea is to leverage the Gaussian Copula to explore the cross-variable and temporal relationships based on the latent Gaussian representation. Subsequently, we employ an Expectation-Maximization (EM) algorithm to improve robustness in managing data with varying missing rates. Comprehensive experiments were conducted on three real-world MTS datasets. The results demonstrate that our TGC substantially outperforms the state-of-the-art imputation methods. Additionally, the TGC model exhibits stronger robustness to the varying missing ratios in the test dataset. Our code is available at https://github.com/MVL-Lab/TGC-MTS."
      },
      {
        "id": "oai:arXiv.org:2504.02318v1",
        "title": "X-Capture: An Open-Source Portable Device for Multi-Sensory Learning",
        "link": "https://arxiv.org/abs/2504.02318",
        "author": "Samuel Clarke, Suzannah Wistreich, Yanjie Ze, Jiajun Wu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02318v1 Announce Type: new \nAbstract: Understanding objects through multiple sensory modalities is fundamental to human perception, enabling cross-sensory integration and richer comprehension. For AI and robotic systems to replicate this ability, access to diverse, high-quality multi-sensory data is critical. Existing datasets are often limited by their focus on controlled environments, simulated objects, or restricted modality pairings. We introduce X-Capture, an open-source, portable, and cost-effective device for real-world multi-sensory data collection, capable of capturing correlated RGBD images, tactile readings, and impact audio. With a build cost under $1,000, X-Capture democratizes the creation of multi-sensory datasets, requiring only consumer-grade tools for assembly. Using X-Capture, we curate a sample dataset of 3,000 total points on 500 everyday objects from diverse, real-world environments, offering both richness and variety. Our experiments demonstrate the value of both the quantity and the sensory breadth of our data for both pretraining and fine-tuning multi-modal representations for object-centric tasks such as cross-sensory retrieval and reconstruction. X-Capture lays the groundwork for advancing human-like sensory representations in AI, emphasizing scalability, accessibility, and real-world applicability."
      },
      {
        "id": "oai:arXiv.org:2504.02321v1",
        "title": "On shallow feedforward neural networks with inputs from a topological space",
        "link": "https://arxiv.org/abs/2504.02321",
        "author": "Vugar Ismailov",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02321v1 Announce Type: new \nAbstract: We study feedforward neural networks with inputs from a topological space (TFNNs). We prove a universal approximation theorem for shallow TFNNs, which demonstrates their capacity to approximate any continuous function defined on this topological space. As an application, we obtain an approximative version of Kolmogorov's superposition theorem for compact metric spaces."
      },
      {
        "id": "oai:arXiv.org:2504.02323v1",
        "title": "CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning, and Active Learning for Generalizable Formative Assessment Scoring",
        "link": "https://arxiv.org/abs/2504.02323",
        "author": "Clayton Cohn, Nicole Hutchins, Ashwin T S, Gautam Biswas",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02323v1 Announce Type: new \nAbstract: Large language models (LLMs) have created new opportunities to assist teachers and support student learning. Methods such as chain-of-thought (CoT) prompting enable LLMs to grade formative assessments in science, providing scores and relevant feedback to students. However, the extent to which these methods generalize across curricula in multiple domains (such as science, computing, and engineering) remains largely untested. In this paper, we introduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based approach to formative assessment scoring that (1) leverages Evidence-Centered Design (ECD) principles to develop curriculum-aligned formative assessments and rubrics, (2) applies human-in-the-loop prompt engineering to automate response scoring, and (3) incorporates teacher and student feedback to iteratively refine assessment questions, grading rubrics, and LLM prompts for automated grading. Our findings demonstrate that CoTAL improves GPT-4's scoring performance, achieving gains of up to 24.5% over a non-prompt-engineered baseline. Both teachers and students view CoTAL as effective in scoring and explaining student responses, each providing valuable refinements to enhance grading accuracy and explanation quality."
      },
      {
        "id": "oai:arXiv.org:2504.02327v1",
        "title": "LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large Language Models",
        "link": "https://arxiv.org/abs/2504.02327",
        "author": "Weibin Liao, Xin Gao, Tianyu Jia, Rihong Qiu, Yifan Zhu, Yang Lin, Xu Chu, Junfeng Zhao, Yasha Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02327v1 Announce Type: new \nAbstract: Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling seamless interaction with databases. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable performance in this domain. However, existing NL2SQL methods predominantly rely on closed-source LLMs leveraging prompt engineering, while open-source models typically require fine-tuning to acquire domain-specific knowledge. Despite these efforts, open-source LLMs struggle with complex NL2SQL tasks due to the indirect expression of user query objectives and the semantic gap between user queries and database schemas. Inspired by the application of reinforcement learning in mathematical problem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT (Learning NL2SQL with AST-guided Task Decomposition), a novel framework that improves the performance of open-source LLMs on complex NL2SQL tasks through task decomposition and reinforcement learning. LearNAT introduces three key components: (1) a Decomposition Synthesis Procedure that leverages Abstract Syntax Trees (ASTs) to guide efficient search and pruning strategies for task decomposition, (2) Margin-aware Reinforcement Learning, which employs fine-grained step-level optimization via DPO with AST margins, and (3) Adaptive Demonstration Reasoning, a mechanism for dynamically selecting relevant examples to enhance decomposition capabilities. Extensive experiments on two benchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a 7B-parameter open-source LLM to achieve performance comparable to GPT-4, while offering improved efficiency and accessibility."
      },
      {
        "id": "oai:arXiv.org:2504.02328v1",
        "title": "Refining CLIP's Spatial Awareness: A Visual-Centric Perspective",
        "link": "https://arxiv.org/abs/2504.02328",
        "author": "Congpei Qiu, Yanhao Wu, Wei Ke, Xiuxiu Bai, Tong Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02328v1 Announce Type: new \nAbstract: Contrastive Language-Image Pre-training (CLIP) excels in global alignment with language but exhibits limited sensitivity to spatial information, leading to strong performance in zero-shot classification tasks but underperformance in tasks requiring precise spatial understanding. Recent approaches have introduced Region-Language Alignment (RLA) to enhance CLIP's performance in dense multimodal tasks by aligning regional visual representations with corresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA suffer from notable loss in spatial awareness, which is crucial for dense prediction tasks. To address this, we propose the Spatial Correlation Distillation (SCD) framework, which preserves CLIP's inherent spatial structure and mitigates the above degradation. To further enhance spatial correlations, we introduce a lightweight Refiner that extracts refined correlations directly from CLIP before feeding them into SCD, based on an intriguing finding that CLIP naturally captures high-quality dense features. Together, these components form a robust distillation framework that enables CLIP ViTs to integrate both visual-language and visual-centric improvements, achieving state-of-the-art results across various open-vocabulary dense prediction benchmarks."
      },
      {
        "id": "oai:arXiv.org:2504.02329v1",
        "title": "Towards Assessing Deep Learning Test Input Generators",
        "link": "https://arxiv.org/abs/2504.02329",
        "author": "Seif Mzoughi, Ahmed Hajyahmed, Mohamed Elshafei, Foutse Khomh anb Diego Elias Costa",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02329v1 Announce Type: new \nAbstract: Deep Learning (DL) systems are increasingly deployed in safety-critical applications, yet they remain vulnerable to robustness issues that can lead to significant failures. While numerous Test Input Generators (TIGs) have been developed to evaluate DL robustness, a comprehensive assessment of their effectiveness across different dimensions is still lacking. This paper presents a comprehensive assessment of four state-of-the-art TIGs--DeepHunter, DeepFault, AdvGAN, and SinVAD--across multiple critical aspects: fault-revealing capability, naturalness, diversity, and efficiency. Our empirical study leverages three pre-trained models (LeNet-5, VGG16, and EfficientNetB3) on datasets of varying complexity (MNIST, CIFAR-10, and ImageNet-1K) to evaluate TIG performance. Our findings reveal important trade-offs in robustness revealing capability, variation in test case generation, and computational efficiency across TIGs. The results also show that TIG performance varies significantly with dataset complexity, as tools that perform well on simpler datasets may struggle with more complex ones. In contrast, others maintain steadier performance or better scalability. This paper offers practical guidance for selecting appropriate TIGs aligned with specific objectives and dataset characteristics. Nonetheless, more work is needed to address TIG limitations and advance TIGs for real-world, safety-critical systems."
      },
      {
        "id": "oai:arXiv.org:2504.02335v1",
        "title": "Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing",
        "link": "https://arxiv.org/abs/2504.02335",
        "author": "Seif Mzoughi, Mohamed Elshafeia, Foutse Khomh",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02335v1 Announce Type: new \nAbstract: Image segmentation is critical for applications such as medical imaging, augmented reality, and video surveillance. However, segmentation models often lack robustness, making them vulnerable to adversarial perturbations from subtle image distortions. In this work, we propose SegRMT, a metamorphic testing approach that leverages genetic algorithms (GA) to optimize sequences of spatial and spectral transformations while preserving image fidelity via a predefined PSNR threshold. Using the Cityscapes dataset, our method generates adversarial examples that effectively challenge the DeepLabV3 segmentation model. Our experiments show that SegRMT reduces DeepLabV3's mean Intersection over Union (mIoU) to 6.4%, outperforming other adversarial baselines that decrease mIoU to between 8.5% and 21.7%. Furthermore, when used for adversarial training, SegRMT boosts model performance, achieving mIoU improvements up to 73% on dedicated adversarial datasets and increasing cross-adversarial mIoU to 53.8%, compared to only 2%-10% for other methods. These findings demonstrate that SegRMT not only simulates realistic image distortions but also enhances the robustness of segmentation models, making it a valuable tool for ensuring reliable performance in safety-critical applications."
      },
      {
        "id": "oai:arXiv.org:2504.02337v1",
        "title": "LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images",
        "link": "https://arxiv.org/abs/2504.02337",
        "author": "Ming-Jia Yang, Yu-Xiao Guo, Yang Liu, Bin Zhou, Xin Tong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02337v1 Announce Type: new \nAbstract: Generating realistic, room-level indoor scenes with semantically plausible and detailed appearances from in-the-wild images is crucial for various applications in VR, AR, and robotics. The success of NeRF-based generative methods indicates a promising direction to address this challenge. However, unlike their success at the object level, existing scene-level generative methods require additional information, such as multiple views, depth images, or semantic guidance, rather than relying solely on RGB images. This is because NeRF-based methods necessitate prior knowledge of camera poses, which is challenging to approximate for indoor scenes due to the complexity of defining alignment and the difficulty of globally estimating poses from a single image, given the unseen parts behind the camera. To address this challenge, we redefine global poses within the framework of Local-Pose-Alignment (LPA) -- an anchor-based multi-local-coordinate system that uses a selected number of anchors as the roots of these coordinates. Building on this foundation, we introduce LPA-GAN, a novel NeRF-based generative approach that incorporates specific modifications to estimate the priors of camera poses under LPA. It also co-optimizes the pose predictor and scene generation processes. Our ablation study and comparisons with straightforward extensions of NeRF-based object generative methods demonstrate the effectiveness of our approach. Furthermore, visual comparisons with other techniques reveal that our method achieves superior view-to-view consistency and semantic normality."
      },
      {
        "id": "oai:arXiv.org:2504.02343v1",
        "title": "Toward General and Robust LLM-enhanced Text-attributed Graph Learning",
        "link": "https://arxiv.org/abs/2504.02343",
        "author": "Zihao Zhang, Xunkai Li, Rong-Hua Li, Bing Zhou, Zhenjun Li, Guoren Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02343v1 Announce Type: new \nAbstract: Recent advancements in Large Language Models (LLMs) and the proliferation of Text-Attributed Graphs (TAGs) across various domains have positioned LLM-enhanced TAG learning as a critical research area. By utilizing rich graph descriptions, this paradigm leverages LLMs to generate high-quality embeddings, thereby enhancing the representational capacity of Graph Neural Networks (GNNs). However, the field faces significant challenges: (1) the absence of a unified framework to systematize the diverse optimization perspectives arising from the complex interactions between LLMs and GNNs, and (2) the lack of a robust method capable of handling real-world TAGs, which often suffer from texts and edge sparsity, leading to suboptimal performance.\n  To address these challenges, we propose UltraTAG, a unified pipeline for LLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and domain-adaptive framework that not only organizes existing methodologies but also paves the way for future advancements in the field. Building on this framework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed to tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs LLM-based text propagation and text augmentation to mitigate text sparsity, while leveraging LLM-augmented node selection techniques based on PageRank and edge reconfiguration strategies to address edge sparsity. Our extensive experiments demonstrate that UltraTAG-S significantly outperforms existing baselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse settings, respectively. Moreover, as the data sparsity ratio increases, the performance improvement of UltraTAG-S also rises, which underscores the effectiveness and robustness of UltraTAG-S."
      },
      {
        "id": "oai:arXiv.org:2504.02345v1",
        "title": "SemiISP/SemiIE: Semi-Supervised Image Signal Processor and Image Enhancement Leveraging One-to-Many Mapping sRGB-to-RAW",
        "link": "https://arxiv.org/abs/2504.02345",
        "author": "Masakazu Yoshimura, Junji Otsuka, Radu Berdan, Takeshi Ohashi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02345v1 Announce Type: new \nAbstract: DNN-based methods have been successful in Image Signal Processor (ISP) and image enhancement (IE) tasks. However, the cost of creating training data for these tasks is considerably higher than for other tasks, making it difficult to prepare large-scale datasets. Also, creating personalized ISP and IE with minimal training data can lead to new value streams since preferred image quality varies depending on the person and use case. While semi-supervised learning could be a potential solution in such cases, it has rarely been utilized for these tasks. In this paper, we realize semi-supervised learning for ISP and IE leveraging a RAW image reconstruction (sRGB-to-RAW) method. Although existing sRGB-to-RAW methods can generate pseudo-RAW image datasets that improve the accuracy of RAW-based high-level computer vision tasks such as object detection, their quality is not sufficient for ISP and IE tasks that require precise image quality definition. Therefore, we also propose a sRGB-to-RAW method that can improve the image quality of these tasks. The proposed semi-supervised learning with the proposed sRGB-to-RAW method successfully improves the image quality of various models on various datasets."
      },
      {
        "id": "oai:arXiv.org:2504.02349v1",
        "title": "Large (Vision) Language Models are Unsupervised In-Context Learners",
        "link": "https://arxiv.org/abs/2504.02349",
        "author": "Artyom Gadetsky, Andrei Atanov, Yulun Jiang, Zhitong Gao, Ghazal Hosseini Mighan, Amir Zamir, Maria Brbic",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02349v1 Announce Type: new \nAbstract: Recent advances in large language and vision-language models have enabled zero-shot inference, allowing models to solve new tasks without task-specific training. Various adaptation techniques such as prompt engineering, In-Context Learning (ICL), and supervised fine-tuning can further enhance the model's performance on a downstream task, but they require substantial manual effort to construct effective prompts or labeled examples. In this work, we introduce a joint inference framework for fully unsupervised adaptation, eliminating the need for manual prompt engineering and labeled examples. Unlike zero-shot inference, which makes independent predictions, the joint inference makes predictions simultaneously for all inputs in a given task. Since direct joint inference involves computationally expensive optimization, we develop efficient approximation techniques, leading to two unsupervised adaptation methods: unsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness of our methods across diverse tasks and models, including language-only Llama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math on grade school math problems, vision-language OpenFlamingo on vision tasks, and the API-only access GPT-4o model on massive multi-discipline tasks. Our experiments demonstrate substantial improvements over the standard zero-shot approach, including 39% absolute improvement on the challenging GSM8K math reasoning dataset. Remarkably, despite being fully unsupervised, our framework often performs on par with supervised approaches that rely on ground truth labels."
      },
      {
        "id": "oai:arXiv.org:2504.02351v1",
        "title": "Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation",
        "link": "https://arxiv.org/abs/2504.02351",
        "author": "Chengxi Zeng, Yuxuan Jiang, Fan Zhang, Alberto Gambaruto, Tilo Burghardt",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02351v1 Announce Type: new \nAbstract: The deployment of foundation models for medical imaging has demonstrated considerable success. However, their training overheads associated with downstream tasks remain substantial due to the size of the image encoders employed, and the inference complexity is also significantly high. Although lightweight variants have been obtained for these foundation models, their performance is constrained by their limited model capacity and suboptimal training strategies. In order to achieve an improved tradeoff between complexity and performance, we propose a new framework to improve the performance of low complexity models via knowledge distillation from multiple large medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each specializing in different vision tasks, with the goal to effectively bridge the performance gap for medical image segmentation tasks. The agglomerated model demonstrates superior generalization across 12 segmentation tasks, whereas specialized models require explicit training for each task. Our approach achieved an average performance gain of 2\\% in Dice coefficient compared to simple distillation."
      },
      {
        "id": "oai:arXiv.org:2504.02356v1",
        "title": "All-day Depth Completion via Thermal-LiDAR Fusion",
        "link": "https://arxiv.org/abs/2504.02356",
        "author": "Janghyun Kim, Minseong Kweon, Jinsun Park, Ukcheol Shin",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02356v1 Announce Type: new \nAbstract: Depth completion, which estimates dense depth from sparse LiDAR and RGB images, has demonstrated outstanding performance in well-lit conditions. However, due to the limitations of RGB sensors, existing methods often struggle to achieve reliable performance in harsh environments, such as heavy rain and low-light conditions. Furthermore, we observe that ground truth depth maps often suffer from large missing measurements in adverse weather conditions such as heavy rain, leading to insufficient supervision. In contrast, thermal cameras are known for providing clear and reliable visibility in such conditions, yet research on thermal-LiDAR depth completion remains underexplored. Moreover, the characteristics of thermal images, such as blurriness, low contrast, and noise, bring unclear depth boundary problems. To address these challenges, we first evaluate the feasibility and robustness of thermal-LiDAR depth completion across diverse lighting (eg., well-lit, low-light), weather (eg., clear-sky, rainy), and environment (eg., indoor, outdoor) conditions, by conducting extensive benchmarks on the MS$^2$ and ViViD datasets. In addition, we propose a framework that utilizes COntrastive learning and Pseudo-Supervision (COPS) to enhance depth boundary clarity and improve completion accuracy by leveraging a depth foundation model in two key ways. First, COPS enforces a depth-aware contrastive loss between different depth points by mining positive and negative samples using a monocular depth foundation model to sharpen depth boundaries. Second, it mitigates the issue of incomplete supervision from ground truth depth maps by leveraging foundation model predictions as dense depth priors. We also provide in-depth analyses of the key challenges in thermal-LiDAR depth completion to aid in understanding the task and encourage future research."
      },
      {
        "id": "oai:arXiv.org:2504.02362v1",
        "title": "Brightness Perceiving for Recursive Low-Light Image Enhancement",
        "link": "https://arxiv.org/abs/2504.02362",
        "author": "Haodian Wang, Long Peng, Yuejin Sun, Zengyu Wan, Yang Wang, Yang Cao",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02362v1 Announce Type: new \nAbstract: Due to the wide dynamic range in real low-light scenes, there will be large differences in the degree of contrast degradation and detail blurring of captured images, making it difficult for existing end-to-end methods to enhance low-light images to normal exposure. To address the above issue, we decompose low-light image enhancement into a recursive enhancement task and propose a brightness-perceiving-based recursive enhancement framework for high dynamic range low-light image enhancement. Specifically, our recursive enhancement framework consists of two parallel sub-networks: Adaptive Contrast and Texture enhancement network (ACT-Net) and Brightness Perception network (BP-Net). The ACT-Net is proposed to adaptively enhance image contrast and details under the guidance of the brightness adjustment branch and gradient adjustment branch, which are proposed to perceive the degradation degree of contrast and details in low-light images. To adaptively enhance images captured under different brightness levels, BP-Net is proposed to control the recursive enhancement times of ACT-Net by exploring the image brightness distribution properties. Finally, in order to coordinate ACT-Net and BP-Net, we design a novel unsupervised training strategy to facilitate the training procedure. To further validate the effectiveness of the proposed method, we construct a new dataset with a broader brightness distribution by mixing three low-light datasets. Compared with eleven existing representative methods, the proposed method achieves new SOTA performance on six reference and no reference metrics. Specifically, the proposed method improves the PSNR by 0.9 dB compared to the existing SOTA method."
      },
      {
        "id": "oai:arXiv.org:2504.02383v1",
        "title": "Reinforcement Learning for Solving the Pricing Problem in Column Generation: Applications to Vehicle Routing",
        "link": "https://arxiv.org/abs/2504.02383",
        "author": "Abdo Abouelrous, Laurens Bliek, Adriana F. Gabor, Yaoxin Wu, Yingqian Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02383v1 Announce Type: new \nAbstract: In this paper, we address the problem of Column Generation (CG) using Reinforcement Learning (RL). Specifically, we use a RL model based on the attention-mechanism architecture to find the columns with most negative reduced cost in the Pricing Problem (PP). Unlike previous Machine Learning (ML) applications for CG, our model deploys an end-to-end mechanism as it independently solves the pricing problem without the help of any heuristic. We consider a variant of Vehicle Routing Problem (VRP) as a case study for our method. Through a set of experiments where our method is compared against a Dynamic Programming (DP)-based heuristic for solving the PP, we show that our method solves the linear relaxation up to a reasonable objective gap within 9% in significantly shorter running times, up to over 300 times faster for instances with 100 customers."
      },
      {
        "id": "oai:arXiv.org:2504.02386v1",
        "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
        "link": "https://arxiv.org/abs/2504.02386",
        "author": "Kim Sung-Bin, Jeongsoo Choi, Puyuan Peng, Joon Son Chung, Tae-Hyun Oh, David Harwath",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02386v1 Announce Type: new \nAbstract: We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications."
      },
      {
        "id": "oai:arXiv.org:2504.02391v1",
        "title": "Marine Saliency Segmenter: Object-Focused Conditional Diffusion with Region-Level Semantic Knowledge Distillation",
        "link": "https://arxiv.org/abs/2504.02391",
        "author": "Laibin Chang, Yunke Wang, JiaXing Huang, Longxiang Deng, Bo Du, Chang Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02391v1 Announce Type: new \nAbstract: Marine Saliency Segmentation (MSS) plays a pivotal role in various vision-based marine exploration tasks. However, existing marine segmentation techniques face the dilemma of object mislocalization and imprecise boundaries due to the complex underwater environment. Meanwhile, despite the impressive performance of diffusion models in visual segmentation, there remains potential to further leverage contextual semantics to enhance feature learning of region-level salient objects, thereby improving segmentation outcomes. Building on this insight, we propose DiffMSS, a novel marine saliency segmenter based on the diffusion model, which utilizes semantic knowledge distillation to guide the segmentation of marine salient objects. Specifically, we design a region-word similarity matching mechanism to identify salient terms at the word level from the text descriptions. These high-level semantic features guide the conditional feature learning network in generating salient and accurate diffusion conditions with semantic knowledge distillation. To further refine the segmentation of fine-grained structures in unique marine organisms, we develop the dedicated consensus deterministic sampling to suppress overconfident missegmentations. Comprehensive experiments demonstrate the superior performance of DiffMSS over state-of-the-art methods in both quantitative and qualitative evaluations."
      },
      {
        "id": "oai:arXiv.org:2504.02395v1",
        "title": "The quasi-semantic competence of LLMs: a case study on the part-whole relation",
        "link": "https://arxiv.org/abs/2504.02395",
        "author": "Mattia Proietti, Alessandro Lenci",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02395v1 Announce Type: new \nAbstract: Understanding the extent and depth of the semantic competence of \\emph{Large Language Models} (LLMs) is at the center of the current scientific agenda in Artificial Intelligence (AI) and Computational Linguistics (CL). We contribute to this endeavor by investigating their knowledge of the \\emph{part-whole} relation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical organization, but it is significantly understudied. We used data from ConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic feature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with \\textit{part-whole} relations. We employed several methods based on three levels of analysis: i.) \\textbf{behavioral} testing via prompting, where we directly queried the models on their knowledge of meronymy, ii.) sentence \\textbf{probability} scoring, where we tested models' abilities to discriminate correct (real) and incorrect (asymmetric counterfactual) \\textit{part-whole} relations, and iii.) \\textbf{concept representation} analysis in vector space, where we proved the linear organization of the \\textit{part-whole} concept in the embedding and unembedding spaces. These analyses present a complex picture that reveals that the LLMs' knowledge of this relation is only partial. They have just a ``\\emph{quasi}-semantic'' competence and still fall short of capturing deep inferential properties."
      },
      {
        "id": "oai:arXiv.org:2504.02397v1",
        "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval",
        "link": "https://arxiv.org/abs/2504.02397",
        "author": "Boseung Jeong, Jicheol Park, Sungyeon Kim, Suha Kwak",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02397v1 Announce Type: new \nAbstract: Video-text retrieval, the task of retrieving videos based on a textual query or vice versa, is of paramount importance for video understanding and multimodal information retrieval. Recent methods in this area rely primarily on visual and textual features and often ignore audio, although it helps enhance overall comprehension of video content. Moreover, traditional models that incorporate audio blindly utilize the audio input regardless of whether it is useful or not, resulting in suboptimal video representation. To address these limitations, we propose a novel video-text retrieval framework, Audio-guided VIdeo representation learning with GATEd attention (AVIGATE), that effectively leverages audio cues through a gated attention mechanism that selectively filters out uninformative audio signals. In addition, we propose an adaptive margin-based contrastive loss to deal with the inherently unclear positive-negative relationship between video and text, which facilitates learning better video-text alignment. Our extensive experiments demonstrate that AVIGATE achieves state-of-the-art performance on all the public benchmarks."
      },
      {
        "id": "oai:arXiv.org:2504.02398v1",
        "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
        "link": "https://arxiv.org/abs/2504.02398",
        "author": "Gallil Maimon, Michael Hassid, Amit Roth, Yossi Adi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02398v1 Announce Type: new \nAbstract: Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims."
      },
      {
        "id": "oai:arXiv.org:2504.02403v1",
        "title": "DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers",
        "link": "https://arxiv.org/abs/2504.02403",
        "author": "Max M\\\"uller-Eberstein, Mike Zhang, Elisa Bassignana, Peter Brunsgaard Trolle, Rob van der Goot",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02403v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have seen widespread societal adoption. However, while they are able to interact with users in languages beyond English, they have been shown to lack cultural awareness, providing anglocentric or inappropriate responses for underrepresented language communities. To investigate this gap and disentangle linguistic versus cultural proficiency, we conduct the first cultural evaluation study for the mid-resource language of Danish, in which native speakers prompt different models to solve tasks requiring cultural awareness. Our analysis of the resulting 1,038 interactions from 63 demographically diverse participants highlights open challenges to cultural adaptation: Particularly, how currently employed automatically translated data are insufficient to train or measure cultural adaptation, and how training on native-speaker data can more than double response acceptance rates. We release our study data as DaKultur - the first native Danish cultural awareness dataset."
      },
      {
        "id": "oai:arXiv.org:2504.02404v1",
        "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology",
        "link": "https://arxiv.org/abs/2504.02404",
        "author": "Xiang Feng, Wentao Jiang, Zengmao Wang, Yong Luo, Pingbo Xu, Baosheng Yu, Hua Jin, Bo Du, Jing Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02404v1 Announce Type: new \nAbstract: The application of large language models (LLMs) in the medical field has gained significant attention, yet their reasoning capabilities in more specialized domains like anesthesiology remain underexplored. In this paper, we systematically evaluate the reasoning capabilities of LLMs in anesthesiology and analyze key factors influencing their performance. To this end, we introduce AnesBench, a cross-lingual benchmark designed to assess anesthesiology-related reasoning across three levels: factual retrieval (System 1), hybrid reasoning (System 1.x), and complex decision-making (System 2). Through extensive experiments, we first explore how model characteristics, including model scale, Chain of Thought (CoT) length, and language transferability, affect reasoning performance. Then, we further evaluate the effectiveness of different training strategies, leveraging our curated anesthesiology-related dataset, including continuous pre-training (CPT) and supervised fine-tuning (SFT). Additionally, we also investigate how the test-time reasoning techniques, such as Best-of-N sampling and beam search, influence reasoning performance, and assess the impact of reasoning-enhanced model distillation, specifically DeepSeek-R1. We will publicly release AnesBench, along with our CPT and SFT training datasets and evaluation code at https://github.com/MiliLab/AnesBench."
      },
      {
        "id": "oai:arXiv.org:2504.02411v1",
        "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation",
        "link": "https://arxiv.org/abs/2504.02411",
        "author": "Alexandre Misrahi, Nadezhda Chirkova, Maxime Louis, Vassilina Nikoulina",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02411v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark comprising a variety of question-answering tasks from 8 sources and covering 13 domains. Our second contribution consists in systematically testing out-of-domain generalization for typical RAG tuning strategies. While our findings reveal that standard fine-tuning fails to generalize effectively, we show that sequence-level distillation with teacher-generated labels improves out-of-domain performance by providing more coherent supervision. Our findings highlight key strategies for improving multi-domain RAG robustness."
      },
      {
        "id": "oai:arXiv.org:2504.02412v1",
        "title": "Bridging the Theoretical Gap in Randomized Smoothing",
        "link": "https://arxiv.org/abs/2504.02412",
        "author": "Blaise Delattre, Paul Caillon, Quentin Barth\\'elemy, Erwan Fagnou, Alexandre Allauzen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02412v1 Announce Type: new \nAbstract: Randomized smoothing has become a leading approach for certifying adversarial robustness in machine learning models. However, a persistent gap remains between theoretical certified robustness and empirical robustness accuracy. This paper introduces a new framework that bridges this gap by leveraging Lipschitz continuity for certification and proposing a novel, less conservative method for computing confidence intervals in randomized smoothing. Our approach tightens the bounds of certified robustness, offering a more accurate reflection of model robustness in practice. Through rigorous experimentation we show that our method improves the robust accuracy, compressing the gap between empirical findings and previous theoretical results. We argue that investigating local Lipschitz constants and designing ad-hoc confidence intervals can further enhance the performance of randomized smoothing. These results pave the way for a deeper understanding of the relationship between Lipschitz continuity and certified robustness."
      },
      {
        "id": "oai:arXiv.org:2504.02416v1",
        "title": "Hyperspectral Remote Sensing Images Salient Object Detection: The First Benchmark Dataset and Baseline",
        "link": "https://arxiv.org/abs/2504.02416",
        "author": "Peifu Liu, Huiyan Bai, Tingfa Xu, Jihui Wang, Huan Chen, Jianan Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02416v1 Announce Type: new \nAbstract: The objective of hyperspectral remote sensing image salient object detection (HRSI-SOD) is to identify objects or regions that exhibit distinct spectrum contrasts with the background. This area holds significant promise for practical applications; however, progress has been limited by a notable scarcity of dedicated datasets and methodologies. To bridge this gap and stimulate further research, we introduce the first HRSI-SOD dataset, termed HRSSD, which includes 704 hyperspectral images and 5327 pixel-level annotated salient objects. The HRSSD dataset poses substantial challenges for salient object detection algorithms due to large scale variation, diverse foreground-background relations, and multi-salient objects. Additionally, we propose an innovative and efficient baseline model for HRSI-SOD, termed the Deep Spectral Saliency Network (DSSN). The core of DSSN is the Cross-level Saliency Assessment Block, which performs pixel-wise attention and evaluates the contributions of multi-scale similarity maps at each spatial location, effectively reducing erroneous responses in cluttered regions and emphasizes salient regions across scales. Additionally, the High-resolution Fusion Module combines bottom-up fusion strategy and learned spatial upsampling to leverage the strengths of multi-scale saliency maps, ensuring accurate localization of small objects. Experiments on the HRSSD dataset robustly validate the superiority of DSSN, underscoring the critical need for specialized datasets and methodologies in this domain. Further evaluations on the HSOD-BIT and HS-SOD datasets demonstrate the generalizability of the proposed method. The dataset and source code are publicly available at https://github.com/laprf/HRSSD."
      },
      {
        "id": "oai:arXiv.org:2504.02417v1",
        "title": "Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering",
        "link": "https://arxiv.org/abs/2504.02417",
        "author": "Lili Liang, Guanglu Sun",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02417v1 Announce Type: new \nAbstract: Video Question Answering (VideoQA) is an important research direction in the field of artificial intelligence, enabling machines to understand video content and perform reasoning and answering based on natural language questions. Although methods based on static relationship reasoning have made certain progress, there are still deficiencies in the accuracy of static relationship recognition and representation, and they have not fully utilized the static relationship information in videos for in-depth reasoning and analysis. Therefore, this paper proposes a reasoning method for intra-type and inter-type message passing based on static relationships. This method constructs a dual graph for intra-type message passing reasoning and builds a heterogeneous graph based on static relationships for inter-type message passing reasoning. The intra-type message passing reasoning model captures the neighborhood information of targets and relationships related to the question in the dual graph, updating the dual graph to obtain intra-type clues for answering the question. The inter-type message passing reasoning model captures the neighborhood information of targets and relationships from different categories related to the question in the heterogeneous graph, updating the heterogeneous graph to obtain inter-type clues for answering the question. Finally, the answers are inferred by combining the intra-type and inter-type clues based on static relationships. Experimental results on the ANetQA and Next-QA datasets demonstrate the effectiveness of this method."
      },
      {
        "id": "oai:arXiv.org:2504.02432v1",
        "title": "Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection",
        "link": "https://arxiv.org/abs/2504.02432",
        "author": "Aidan Tiruvan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02432v1 Announce Type: new \nAbstract: Robust low-rank approximation under row-wise adversarial corruption can be achieved with a single pass, randomized procedure that detects and removes outlier rows by thresholding their projected norms. We propose a scalable, non-iterative algorithm that efficiently recovers the underlying low-rank structure in the presence of row-wise adversarial corruption. By first compressing the data with a Johnson Lindenstrauss projection, our approach preserves the geometry of clean rows while dramatically reducing dimensionality. Robust statistical techniques based on the median and median absolute deviation then enable precise identification and removal of outlier rows with abnormally high norms. The subsequent rank-k approximation achieves near-optimal error bounds with a one pass procedure that scales linearly with the number of observations. Empirical results confirm that combining random sketches with robust statistics yields efficient, accurate decompositions even in the presence of large fractions of corrupted rows."
      },
      {
        "id": "oai:arXiv.org:2504.02433v1",
        "title": "OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication",
        "link": "https://arxiv.org/abs/2504.02433",
        "author": "Zhongjian Wang, Peng Zhang, Jinwei Qi, Guangyuan Wang Sheng Xu, Bang Zhang, Liefeng Bo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02433v1 Announce Type: new \nAbstract: Recent years have witnessed remarkable advances in talking head generation, owing to its potential to revolutionize the human-AI interaction from text interfaces into realistic video chats. However, research on text-driven talking heads remains underexplored, with existing methods predominantly adopting a cascaded pipeline that combines TTS systems with audio-driven talking head models. This conventional pipeline not only introduces system complexity and latency overhead but also fundamentally suffers from asynchronous audiovisual output and stylistic discrepancies between generated speech and visual expressions. To address these limitations, we introduce OmniTalker, an end-to-end unified framework that simultaneously generates synchronized speech and talking head videos from text and reference video in real-time zero-shot scenarios, while preserving both speech style and facial styles. The framework employs a dual-branch diffusion transformer architecture: the audio branch synthesizes mel-spectrograms from text, while the visual branch predicts fine-grained head poses and facial dynamics. To bridge modalities, we introduce a novel audio-visual fusion module that integrates cross-modal information to ensure temporal synchronization and stylistic coherence between audio and visual outputs. Furthermore, our in-context reference learning module effectively captures both speech and facial style characteristics from a single reference video without introducing an extra style extracting module. To the best of our knowledge, OmniTalker presents the first unified framework that jointly models speech style and facial style in a zero-shot setting, achieving real-time inference speed of 25 FPS. Extensive experiments demonstrate that our method surpasses existing approaches in generation quality, particularly excelling in style preservation and audio-video synchronization."
      },
      {
        "id": "oai:arXiv.org:2504.02436v1",
        "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
        "link": "https://arxiv.org/abs/2504.02436",
        "author": "Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, Yahui Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02436v1 Announce Type: new \nAbstract: This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation."
      },
      {
        "id": "oai:arXiv.org:2504.02437v1",
        "title": "MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM",
        "link": "https://arxiv.org/abs/2504.02437",
        "author": "Renwu Li, Wenjing Ke, Dong Li, Lu Tian, Emad Barsoum",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02437v1 Announce Type: new \nAbstract: We present MonoGS++, a novel fast and accurate Simultaneous Localization and Mapping (SLAM) method that leverages 3D Gaussian representations and operates solely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods largely depended on depth sensors, our approach reduces the hardware dependency and only requires RGB input, leveraging online visual odometry (VO) to generate sparse point clouds in real-time. To reduce redundancy and enhance the quality of 3D scene reconstruction, we implemented a series of methodological enhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian insertion to avoid adding redundant Gaussians in previously well-reconstructed areas. Secondly, we introduced clarity-enhancing Gaussian densification module and planar regularization to handle texture-less areas and flat surfaces better. We achieved precise camera tracking results both on the synthetic Replica and real-world TUM-RGBD datasets, comparable to those of the state-of-the-art. Additionally, our method realized a significant 5.57x improvement in frames per second (fps) over the previous state-of-the-art, MonoGS."
      },
      {
        "id": "oai:arXiv.org:2504.02438v1",
        "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation",
        "link": "https://arxiv.org/abs/2504.02438",
        "author": "Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02438v1 Announce Type: new \nAbstract: Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLaMP, a hierarchical video-language model that processes hour-long videos at ``mixed precision'' through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLaMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLaMP's superior performance across four video understanding benchmarks, particularly on long-form content. Notably, ViLaMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance."
      },
      {
        "id": "oai:arXiv.org:2504.02440v1",
        "title": "HGFormer: Topology-Aware Vision Transformer with HyperGraph Learning",
        "link": "https://arxiv.org/abs/2504.02440",
        "author": "Hao Wang, Shuo Zhang, Biao Leng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02440v1 Announce Type: new \nAbstract: The computer vision community has witnessed an extensive exploration of vision transformers in the past two years. Drawing inspiration from traditional schemes, numerous works focus on introducing vision-specific inductive biases. However, the implicit modeling of permutation invariance and fully-connected interaction with individual tokens disrupts the regional context and spatial topology, further hindering higher-order modeling. This deviates from the principle of perceptual organization that emphasizes the local groups and overall topology of visual elements. Thus, we introduce the concept of hypergraph for perceptual exploration. Specifically, we propose a topology-aware vision transformer called HyperGraph Transformer (HGFormer). Firstly, we present a Center Sampling K-Nearest Neighbors (CS-KNN) algorithm for semantic guidance during hypergraph construction. Secondly, we present a topology-aware HyperGraph Attention (HGA) mechanism that integrates hypergraph topology as perceptual indications to guide the aggregation of global and unbiased information during hypergraph messaging. Using HGFormer as visual backbone, we develop an effective and unitive representation, achieving distinct and detailed scene depictions. Empirical experiments show that the proposed HGFormer achieves competitive performance compared to the recent SoTA counterparts on various visual benchmarks. Extensive ablation and visualization studies provide comprehensive explanations of our ideas and contributions."
      },
      {
        "id": "oai:arXiv.org:2504.02441v1",
        "title": "Cognitive Memory in Large Language Models",
        "link": "https://arxiv.org/abs/2504.02441",
        "author": "Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02441v1 Announce Type: new \nAbstract: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions."
      },
      {
        "id": "oai:arXiv.org:2504.02451v1",
        "title": "ConMo: Controllable Motion Disentanglement and Recomposition for Zero-Shot Motion Transfer",
        "link": "https://arxiv.org/abs/2504.02451",
        "author": "Jiayi Gao, Zijin Yin, Changcheng Hua, Yuxin Peng, Kongming Liang, Zhanyu Ma, Jun Guo, Yang Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02451v1 Announce Type: new \nAbstract: The development of Text-to-Video (T2V) generation has made motion transfer possible, enabling the control of video motion based on existing footage. However, current methods have two limitations: 1) struggle to handle multi-subjects videos, failing to transfer specific subject motion; 2) struggle to preserve the diversity and accuracy of motion as transferring to subjects with varying shapes. To overcome these, we introduce \\textbf{ConMo}, a zero-shot framework that disentangle and recompose the motions of subjects and camera movements. ConMo isolates individual subject and background motion cues from complex trajectories in source videos using only subject masks, and reassembles them for target video generation. This approach enables more accurate motion control across diverse subjects and improves performance in multi-subject scenarios. Additionally, we propose soft guidance in the recomposition stage which controls the retention of original motion to adjust shape constraints, aiding subject shape adaptation and semantic transformation. Unlike previous methods, ConMo unlocks a wide range of applications, including subject size and position editing, subject removal, semantic modifications, and camera motion simulation. Extensive experiments demonstrate that ConMo significantly outperforms state-of-the-art methods in motion fidelity and semantic consistency. The code is available at https://github.com/Andyplus1/ConMo."
      },
      {
        "id": "oai:arXiv.org:2504.02454v1",
        "title": "Taylor Series-Inspired Local Structure Fitting Network for Few-shot Point Cloud Semantic Segmentation",
        "link": "https://arxiv.org/abs/2504.02454",
        "author": "Changshuo Wang, Shuting He, Xiang Fang, Meiqing Wu, Siew-Kei Lam, Prayag Tiwari",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02454v1 Announce Type: new \nAbstract: Few-shot point cloud semantic segmentation aims to accurately segment \"unseen\" new categories in point cloud scenes using limited labeled data. However, pretraining-based methods not only introduce excessive time overhead but also overlook the local structure representation among irregular point clouds. To address these issues, we propose a pretraining-free local structure fitting network for few-shot point cloud semantic segmentation, named TaylorSeg. Specifically, inspired by Taylor series, we treat the local structure representation of irregular point clouds as a polynomial fitting problem and propose a novel local structure fitting convolution, called TaylorConv. This convolution learns the low-order basic information and high-order refined information of point clouds from explicit encoding of local geometric structures. Then, using TaylorConv as the basic component, we construct two variants of TaylorSeg: a non-parametric TaylorSeg-NN and a parametric TaylorSeg-PN. The former can achieve performance comparable to existing parametric models without pretraining. For the latter, we equip it with an Adaptive Push-Pull (APP) module to mitigate the feature distribution differences between the query set and the support set. Extensive experiments validate the effectiveness of the proposed method. Notably, under the 2-way 1-shot setting, TaylorSeg-PN achieves improvements of +2.28% and +4.37% mIoU on the S3DIS and ScanNet datasets respectively, compared to the previous state-of-the-art methods. Our code is available at https://github.com/changshuowang/TaylorSeg."
      },
      {
        "id": "oai:arXiv.org:2504.02456v1",
        "title": "The Amenability Framework: Rethinking Causal Ordering Without Estimating Causal Effects",
        "link": "https://arxiv.org/abs/2504.02456",
        "author": "Carlos Fern\\'andez-Lor\\'ia, Jorge Lor\\'ia",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02456v1 Announce Type: new \nAbstract: Who should we prioritize for intervention when we cannot estimate intervention effects? In many applied domains (e.g., advertising, customer retention, and behavioral nudging) prioritization is guided by predictive models that estimate outcome probabilities rather than causal effects. This paper investigates when these predictions (scores) can effectively rank individuals by their intervention effects, particularly when direct effect estimation is infeasible or unreliable. We propose a conceptual framework based on amenability: an individual's latent proclivity to be influenced by an intervention. We then formalize conditions under which predictive scores serve as effective proxies for amenability. These conditions justify using non-causal scores for intervention prioritization, even when the scores do not directly estimate effects. We further show that, under plausible assumptions, predictive models can outperform causal effect estimators in ranking individuals by intervention effects. Empirical evidence from an advertising context supports our theoretical findings, demonstrating that predictive modeling can offer a more robust approach to targeting than effect estimation. Our framework suggests a shift in focus, from estimating effects to inferring who is amenable, as a practical and theoretically grounded strategy for prioritizing interventions in resource-constrained environments."
      },
      {
        "id": "oai:arXiv.org:2504.02459v1",
        "title": "A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries",
        "link": "https://arxiv.org/abs/2504.02459",
        "author": "Reza Najian Asl, Yusuke Yamazaki, Kianoosh Taghikhani, Mayu Muramatsu, Markus Apel, Shahed Rezaei",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02459v1 Announce Type: new \nAbstract: In this work, we introduce implicit Finite Operator Learning (iFOL) for the continuous and parametric solution of partial differential equations (PDEs) on arbitrary geometries. We propose a physics-informed encoder-decoder network to establish the mapping between continuous parameter and solution spaces. The decoder constructs the parametric solution field by leveraging an implicit neural field network conditioned on a latent or feature code. Instance-specific codes are derived through a PDE encoding process based on the second-order meta-learning technique. In training and inference, a physics-informed loss function is minimized during the PDE encoding and decoding. iFOL expresses the loss function in an energy or weighted residual form and evaluates it using discrete residuals derived from standard numerical PDE methods. This approach results in the backpropagation of discrete residuals during both training and inference.\n  iFOL features several key properties: (1) its unique loss formulation eliminates the need for the conventional encode-process-decode pipeline previously used in operator learning with conditional neural fields for PDEs; (2) it not only provides accurate parametric and continuous fields but also delivers solution-to-parameter gradients without requiring additional loss terms or sensitivity analysis; (3) it can effectively capture sharp discontinuities in the solution; and (4) it removes constraints on the geometry and mesh, making it applicable to arbitrary geometries and spatial sampling (zero-shot super-resolution capability). We critically assess these features and analyze the network's ability to generalize to unseen samples across both stationary and transient PDEs. The overall performance of the proposed method is promising, demonstrating its applicability to a range of challenging problems in computational mechanics."
      },
      {
        "id": "oai:arXiv.org:2504.02464v1",
        "title": "CornerPoint3D: Look at the Nearest Corner Instead of the Center",
        "link": "https://arxiv.org/abs/2504.02464",
        "author": "Ruixiao Zhang, Runwei Guan, Xiangyu Chen, Adam Prugel-Bennett, Xiaohao Cai",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02464v1 Announce Type: new \nAbstract: 3D object detection aims to predict object centers, dimensions, and rotations from LiDAR point clouds. Despite its simplicity, LiDAR captures only the near side of objects, making center-based detectors prone to poor localization accuracy in cross-domain tasks with varying point distributions. Meanwhile, existing evaluation metrics designed for single-domain assessment also suffer from overfitting due to dataset-specific size variations. A key question arises: Do we really need models to maintain excellent performance in the entire 3D bounding boxes after being applied across domains? Actually, one of our main focuses is on preventing collisions between vehicles and other obstacles, especially in cross-domain scenarios where correctly predicting the sizes is much more difficult. To address these issues, we rethink cross-domain 3D object detection from a practical perspective. We propose two new metrics that evaluate a model's ability to detect objects' closer-surfaces to the LiDAR sensor. Additionally, we introduce EdgeHead, a refinement head that guides models to focus more on learnable closer surfaces, significantly improving cross-domain performance under both our new and traditional BEV/3D metrics. Furthermore, we argue that predicting the nearest corner rather than the object center enhances robustness. We propose a novel 3D object detector, coined as CornerPoint3D, which is built upon CenterPoint and uses heatmaps to supervise the learning and detection of the nearest corner of each object. Our proposed methods realize a balanced trade-off between the detection quality of entire bounding boxes and the locating accuracy of closer surfaces to the LiDAR sensor, outperforming the traditional center-based detector CenterPoint in multiple cross-domain tasks and providing a more practically reasonable and robust cross-domain 3D object detection solution."
      },
      {
        "id": "oai:arXiv.org:2504.02471v1",
        "title": "Semantic segmentation of forest stands using deep learning",
        "link": "https://arxiv.org/abs/2504.02471",
        "author": "H{\\aa}kon N{\\ae}ss Sandum (Faculty of Environmental Sciences and Natural Resource Management, Norwegian University of Life Sciences, NMBU, {\\AA}s, Norway), Hans Ole {\\O}rka (Faculty of Environmental Sciences and Natural Resource Management, Norwegian University of Life Sciences, NMBU, {\\AA}s, Norway), Oliver Tomic (Faculty of Science and Technology, Norwegian University of Life Sciences, NMBU, {\\AA}s, Norway), Erik N{\\ae}sset (Faculty of Environmental Sciences and Natural Resource Management, Norwegian University of Life Sciences, NMBU, {\\AA}s, Norway), Terje Gobakken (Faculty of Environmental Sciences and Natural Resource Management, Norwegian University of Life Sciences, NMBU, {\\AA}s, Norway)",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02471v1 Announce Type: new \nAbstract: Forest stands are the fundamental units in forest management inventories, silviculture, and financial analysis within operational forestry. Over the past two decades, a common method for mapping stand borders has involved delineation through manual interpretation of stereographic aerial images. This is a time-consuming and subjective process, limiting operational efficiency and introducing inconsistencies. Substantial effort has been devoted to automating the process, using various algorithms together with aerial images and canopy height models constructed from airborne laser scanning (ALS) data, but manual interpretation remains the preferred method. Deep learning (DL) methods have demonstrated great potential in computer vision, yet their application to forest stand delineation remains unexplored in published research. This study presents a novel approach, framing stand delineation as a multiclass segmentation problem and applying a U-Net based DL framework. The model was trained and evaluated using multispectral images, ALS data, and an existing stand map created by an expert interpreter. Performance was assessed on independent data using overall accuracy, a standard metric for classification tasks that measures the proportions of correctly classified pixels. The model achieved an overall accuracy of 0.73. These results demonstrate strong potential for DL in automated stand delineation. However, a few key challenges were noted, especially for complex forest environments."
      },
      {
        "id": "oai:arXiv.org:2504.02478v1",
        "title": "MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities",
        "link": "https://arxiv.org/abs/2504.02478",
        "author": "Bizhu Wu, Jinheng Xie, Keming Shen, Zhe Kong, Jianfeng Ren, Ruibin Bai, Rong Qu, Linlin Shen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02478v1 Announce Type: new \nAbstract: Recent motion-aware large language models have demonstrated promising potential in unifying motion comprehension and generation. However, existing approaches primarily focus on coarse-grained motion-text modeling, where text describes the overall semantics of an entire motion sequence in just a few words. This limits their ability to handle fine-grained motion-relevant tasks, such as understanding and controlling the movements of specific body parts. To overcome this limitation, we pioneer MG-MotionLLM, a unified motion-language model for multi-granular motion comprehension and generation. We further introduce a comprehensive multi-granularity training scheme by incorporating a set of novel auxiliary tasks, such as localizing temporal boundaries of motion segments via detailed text as well as motion detailed captioning, to facilitate mutual reinforcement for motion-text modeling across various levels of granularity. Extensive experiments show that our MG-MotionLLM achieves superior performance on classical text-to-motion and motion-to-text tasks, and exhibits potential in novel fine-grained motion comprehension and editing tasks. Project page: CVI-SZU/MG-MotionLLM"
      },
      {
        "id": "oai:arXiv.org:2504.02479v1",
        "title": "Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets",
        "link": "https://arxiv.org/abs/2504.02479",
        "author": "Stefano Covone, Italo Napolitano, Francesco De Lellis, Mario di Bernardo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02479v1 Announce Type: new \nAbstract: We propose a decentralized reinforcement learning solution for multi-agent shepherding of non-cohesive targets using policy-gradient methods. Our architecture integrates target-selection with target-driving through Proximal Policy Optimization, overcoming discrete-action constraints of previous Deep Q-Network approaches and enabling smoother agent trajectories. This model-free framework effectively solves the shepherding problem without prior dynamics knowledge. Experiments demonstrate our method's effectiveness and scalability with increased target numbers and limited sensing capabilities."
      },
      {
        "id": "oai:arXiv.org:2504.02480v1",
        "title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging",
        "link": "https://arxiv.org/abs/2504.02480",
        "author": "Kyungmin Choi, JaKeoung Koo, Stephen McLaughlin, Abderrahim Halimi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02480v1 Announce Type: new \nAbstract: Single-photon Lidar imaging offers a significant advantage in 3D imaging due to its high resolution and long-range capabilities, however it is challenging to apply in noisy environments with multiple targets per pixel. To tackle these challenges, several methods have been proposed. Statistical methods demonstrate interpretability on the inferred parameters, but they are often limited in their ability to handle complex scenes. Deep learning-based methods have shown superior performance in terms of accuracy and robustness, but they lack interpretability or they are limited to a single-peak per pixel. In this paper, we propose a deep unrolling algorithm for dual-peak single-photon Lidar imaging. We introduce a hierarchical Bayesian model for multiple targets and propose a neural network that unrolls the underlying statistical method. To support multiple targets, we adopt a dual depth maps representation and exploit geometric deep learning to extract features from the point cloud. The proposed method takes advantages of statistical methods and learning-based methods in terms of accuracy and quantifying uncertainty. The experimental results on synthetic and real data demonstrate the competitive performance when compared to existing methods, while also providing uncertainty information."
      },
      {
        "id": "oai:arXiv.org:2504.02494v1",
        "title": "Semiconductor Wafer Map Defect Classification with Tiny Vision Transformers",
        "link": "https://arxiv.org/abs/2504.02494",
        "author": "Faisal Mohammad, Duksan Ryu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02494v1 Announce Type: new \nAbstract: Semiconductor wafer defect classification is critical for ensuring high precision and yield in manufacturing. Traditional CNN-based models often struggle with class imbalances and recognition of the multiple overlapping defect types in wafer maps. To address these challenges, we propose ViT-Tiny, a lightweight Vision Transformer (ViT) framework optimized for wafer defect classification. Trained on the WM-38k dataset. ViT-Tiny outperforms its ViT-Base counterpart and state-of-the-art (SOTA) models, such as MSF-Trans and CNN-based architectures. Through extensive ablation studies, we determine that a patch size of 16 provides optimal performance. ViT-Tiny achieves an F1-score of 98.4%, surpassing MSF-Trans by 2.94% in four-defect classification, improving recall by 2.86% in two-defect classification, and increasing precision by 3.13% in three-defect classification. Additionally, it demonstrates enhanced robustness under limited labeled data conditions, making it a computationally efficient and reliable solution for real-world semiconductor defect detection."
      },
      {
        "id": "oai:arXiv.org:2504.02495v1",
        "title": "Inference-Time Scaling for Generalist Reward Modeling",
        "link": "https://arxiv.org/abs/2504.02495",
        "author": "Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02495v1 Announce Type: new \nAbstract: Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced."
      },
      {
        "id": "oai:arXiv.org:2504.02496v1",
        "title": "Group-based Distinctive Image Captioning with Memory Difference Encoding and Attention",
        "link": "https://arxiv.org/abs/2504.02496",
        "author": "Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02496v1 Announce Type: new \nAbstract: Recent advances in image captioning have focused on enhancing accuracy by substantially increasing the dataset and model size. While conventional captioning models exhibit high performance on established metrics such as BLEU, CIDEr, and SPICE, the capability of captions to distinguish the target image from other similar images is under-explored. To generate distinctive captions, a few pioneers employed contrastive learning or re-weighted the ground-truth captions. However, these approaches often overlook the relationships among objects in a similar image group (e.g., items or properties within the same album or fine-grained events). In this paper, we introduce a novel approach to enhance the distinctiveness of image captions, namely Group-based Differential Distinctive Captioning Method, which visually compares each image with other images in one similar group and highlights the uniqueness of each image. In particular, we introduce a Group-based Differential Memory Attention (GDMA) module, designed to identify and emphasize object features in an image that are uniquely distinguishable within its image group, i.e., those exhibiting low similarity with objects in other images. This mechanism ensures that such unique object features are prioritized during caption generation for the image, thereby enhancing the distinctiveness of the resulting captions. To further refine this process, we select distinctive words from the ground-truth captions to guide both the language decoder and the GDMA module. Additionally, we propose a new evaluation metric, the Distinctive Word Rate (DisWordRate), to quantitatively assess caption distinctiveness. Quantitative results indicate that the proposed method significantly improves the distinctiveness of several baseline models, and achieves state-of-the-art performance on distinctiveness while not excessively sacrificing accuracy..."
      },
      {
        "id": "oai:arXiv.org:2504.02498v1",
        "title": "VISTA: Unsupervised 2D Temporal Dependency Representations for Time Series Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.02498",
        "author": "Sinchee Chin, Fan Zhang, Xiaochen Yang, Jing-Hao Xue, Wenming Yang, Peng Jia, Guijin Wang, Luo Yingqun",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02498v1 Announce Type: new \nAbstract: Time Series Anomaly Detection (TSAD) is essential for uncovering rare and potentially harmful events in unlabeled time series data. Existing methods are highly dependent on clean, high-quality inputs, making them susceptible to noise and real-world imperfections. Additionally, intricate temporal relationships in time series data are often inadequately captured in traditional 1D representations, leading to suboptimal modeling of dependencies. We introduce VISTA, a training-free, unsupervised TSAD algorithm designed to overcome these challenges. VISTA features three core modules: 1) Time Series Decomposition using Seasonal and Trend Decomposition via Loess (STL) to decompose noisy time series into trend, seasonal, and residual components; 2) Temporal Self-Attention, which transforms 1D time series into 2D temporal correlation matrices for richer dependency modeling and anomaly detection; and 3) Multivariate Temporal Aggregation, which uses a pretrained feature extractor to integrate cross-variable information into a unified, memory-efficient representation. VISTA's training-free approach enables rapid deployment and easy hyperparameter tuning, making it suitable for industrial applications. It achieves state-of-the-art performance on five multivariate TSAD benchmarks."
      },
      {
        "id": "oai:arXiv.org:2504.02507v1",
        "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
        "link": "https://arxiv.org/abs/2504.02507",
        "author": "Abhay Kumar, Louis Owen, Nilabhra Roy Chowdhury, Fabian G\\\"ura",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02507v1 Announce Type: new \nAbstract: Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip."
      },
      {
        "id": "oai:arXiv.org:2504.02508v1",
        "title": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers",
        "link": "https://arxiv.org/abs/2504.02508",
        "author": "Zhuguanyu Wu, Jiayi Zhang, Jiaxin Chen, Jinyang Guo, Di Huang, Yunhong Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02508v1 Announce Type: new \nAbstract: Vision Transformers (ViTs) have become one of the most commonly used backbones for vision tasks. Despite their remarkable performance, they often suffer significant accuracy drops when quantized for practical deployment, particularly by post-training quantization (PTQ) under ultra-low bits. Recently, reconstruction-based PTQ methods have shown promising performance in quantizing Convolutional Neural Networks (CNNs). However, they fail when applied to ViTs, primarily due to the inaccurate estimation of output importance and the substantial accuracy degradation in quantizing post-GELU activations. To address these issues, we propose \\textbf{APHQ-ViT}, a novel PTQ approach based on importance estimation with Average Perturbation Hessian (APH). Specifically, we first thoroughly analyze the current approximation approaches with Hessian loss, and propose an improved average perturbation Hessian loss. To deal with the quantization of the post-GELU activations, we design an MLP Reconstruction (MR) method by replacing the GELU function in MLP with ReLU and reconstructing it by the APH loss on a small unlabeled calibration set. Extensive experiments demonstrate that APHQ-ViT using linear quantizers outperforms existing PTQ methods by substantial margins in 3-bit and 4-bit across different vision tasks. The source code is available at https://github.com/GoatWu/APHQ-ViT."
      },
      {
        "id": "oai:arXiv.org:2504.02512v1",
        "title": "Towards Generalizing Temporal Action Segmentation to Unseen Views",
        "link": "https://arxiv.org/abs/2504.02512",
        "author": "Emad Bahrami, Olga Zatsarynna, Gianpiero Francesca, Juergen Gall",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02512v1 Announce Type: new \nAbstract: While there has been substantial progress in temporal action segmentation, the challenge to generalize to unseen views remains unaddressed. Hence, we define a protocol for unseen view action segmentation where camera views for evaluating the model are unavailable during training. This includes changing from top-frontal views to a side view or even more challenging from exocentric to egocentric views. Furthermore, we present an approach for temporal action segmentation that tackles this challenge. Our approach leverages a shared representation at both the sequence and segment levels to reduce the impact of view differences during training. We achieve this by introducing a sequence loss and an action loss, which together facilitate consistent video and action representations across different views. The evaluation on the Assembly101, IkeaASM, and EgoExoLearn datasets demonstrate significant improvements, with a 12.8% increase in F1@50 for unseen exocentric views and a substantial 54% improvement for unseen egocentric views."
      },
      {
        "id": "oai:arXiv.org:2504.02515v1",
        "title": "Exploration-Driven Generative Interactive Environments",
        "link": "https://arxiv.org/abs/2504.02515",
        "author": "Nedko Savov, Naser Kazemi, Mohammad Mahdi, Danda Pani Paudel, Xi Wang, Luc Van Gool",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02515v1 Announce Type: new \nAbstract: Modern world models require costly and time-consuming collection of large video datasets with action demonstrations by people or by environment-specific agents. To simplify training, we focus on using many virtual environments for inexpensive, automatically collected interaction data. Genie, a recent multi-environment world model, demonstrates simulation abilities of many environments with shared behavior. Unfortunately, training their model requires expensive demonstrations. Therefore, we propose a training framework merely using a random agent in virtual environments. While the model trained in this manner exhibits good controls, it is limited by the random exploration possibilities. To address this limitation, we propose AutoExplore Agent - an exploration agent that entirely relies on the uncertainty of the world model, delivering diverse data from which it can learn the best. Our agent is fully independent of environment-specific rewards and thus adapts easily to new environments. With this approach, the pretrained multi-environment model can quickly adapt to new environments achieving video fidelity and controllability improvement. In order to obtain automatically large-scale interaction datasets for pretraining, we group environments with similar behavior and controls. To this end, we annotate the behavior and controls of 974 virtual environments - a dataset that we name RetroAct. For building our model, we first create an open implementation of Genie - GenieRedux and apply enhancements and adaptations in our version GenieRedux-G. Our code and data are available at https://github.com/insait-institute/GenieRedux."
      },
      {
        "id": "oai:arXiv.org:2504.02517v1",
        "title": "MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields",
        "link": "https://arxiv.org/abs/2504.02517",
        "author": "Yash Kulthe, Andrew Gilbert, John Collomosse",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02517v1 Announce Type: new \nAbstract: We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely keyed watermarks within images rendered by a single Neural Radiance Field (NeRF) model, whilst maintaining high visual quality. Our approach extends the TensoRF NeRF model by incorporating a dedicated watermark grid alongside the existing geometry and appearance grids. This extension ensures higher watermark capacity without entangling watermark signals with scene content. We propose a FiLM-based conditional modulation mechanism that dynamically activates watermarks based on input identifiers, allowing multiple independent watermarks to be embedded and extracted without requiring model retraining. MultiNeRF is validated on the NeRF-Synthetic and LLFF datasets, with statistically significant improvements in robust capacity without compromising rendering quality. By generalizing single-watermark NeRF methods into a flexible multi-watermarking framework, MultiNeRF provides a scalable solution for 3D content. attribution."
      },
      {
        "id": "oai:arXiv.org:2504.02519v1",
        "title": "Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework",
        "link": "https://arxiv.org/abs/2504.02519",
        "author": "Christian Alexander Holz, Christian Bader, Markus Enzweiler, Matthias Dr\\\"uppel",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02519v1 Announce Type: new \nAbstract: This paper presents novel Machine Learning (ML) methodologies for Multi-Object Tracking (MOT), specifically designed to meet the increasing complexity and precision demands of Advanced Driver Assistance Systems (ADAS). We introduce three Neural Network (NN) models that address key challenges in MOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii) the Single-Association Network (SANT) for mapping individual Sensor Object (SO) to existing tracks, and (iii) the Multi-Association Network (MANTa) for associating multiple SOs to multiple tracks. These models are seamlessly integrated into a traditional Kalman Filter (KF) framework, maintaining the system's modularity by replacing relevant components without disrupting the overall architecture. Importantly, all three networks are designed to be run in a realtime, embedded environment. Each network contains less than 50k trainable parameters. Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance. SPENT reduces the Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT and MANTa achieve up to 95% accuracy in sensor object-to-track assignments. These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability."
      },
      {
        "id": "oai:arXiv.org:2504.02521v1",
        "title": "UNDO: Understanding Distillation as Optimization",
        "link": "https://arxiv.org/abs/2504.02521",
        "author": "Kushal Jain, Piyushi Goyal, Kumar Shridhar",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02521v1 Announce Type: new \nAbstract: Knowledge distillation has emerged as an effective strategy for compressing large language models' (LLMs) knowledge into smaller, more efficient student models. However, standard one-shot distillation methods often produce suboptimal results due to a mismatch between teacher-generated rationales and the student's specific learning requirements. In this paper, we introduce the UNDO: UNderstanding Distillation as Optimization framework, designed to bridge this gap by iteratively identifying the student's errors and prompting the teacher to refine its explanations accordingly. Each iteration directly targets the student's learning deficiencies, motivating the teacher to provide tailored and enhanced rationales that specifically address these weaknesses. Empirical evaluations on various challenging mathematical and commonsense reasoning tasks demonstrate that our iterative distillation method, UNDO, significantly outperforms standard one-step distillation methods, achieving performance gains of up to 20%. Additionally, we show that teacher-generated data refined through our iterative process remains effective even when applied to different student models, underscoring the broad applicability of our approach. Our work fundamentally reframes knowledge distillation as an iterative teacher-student interaction, effectively leveraging dynamic refinement by the teacher for better knowledge distillation."
      },
      {
        "id": "oai:arXiv.org:2504.02522v1",
        "title": "Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment",
        "link": "https://arxiv.org/abs/2504.02522",
        "author": "Fatemeh Behrad, Tinne Tuytelaars, Johan Wagemans",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02522v1 Announce Type: new \nAbstract: The capacity of Vision transformers (ViTs) to handle variable-sized inputs is often constrained by computational complexity and batch processing limitations. Consequently, ViTs are typically trained on small, fixed-size images obtained through downscaling or cropping. While reducing computational burden, these methods result in significant information loss, negatively affecting tasks like image aesthetic assessment. We introduce Charm, a novel tokenization approach that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale information simultaneously. Charm prioritizes high-resolution details in specific regions while downscaling others, enabling shorter fixed-size input sequences for ViTs while incorporating essential information. Charm is designed to be compatible with pre-trained ViTs and their learned positional embeddings. By providing multiscale input and introducing variety to input tokens, Charm improves ViT performance and generalizability for image aesthetic assessment. We avoid cropping or changing the aspect ratio to further preserve information. Extensive experiments demonstrate significant performance improvements on various image aesthetic and quality assessment datasets (up to 8.1 %) using a lightweight ViT backbone. Code and pre-trained models are available at https://github.com/FBehrad/Charm."
      },
      {
        "id": "oai:arXiv.org:2504.02524v1",
        "title": "SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation",
        "link": "https://arxiv.org/abs/2504.02524",
        "author": "Yunhao Lv, Lingyu Chen, Jian Wang, Yangxi Li, Fang Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02524v1 Announce Type: new \nAbstract: In recent years, deep learning methods such as convolutional neural network (CNN) and transformers have made significant progress in CT multi-organ segmentation. However, CT multi-organ segmentation methods based on masked image modeling (MIM) are very limited. There are already methods using MAE for CT multi-organ segmentation task, we believe that the existing methods do not identify the most difficult areas to reconstruct. To this end, we propose a MIM self-training framework with hard patches mining masked autoencoders for CT multi-organ segmentation tasks (selfMedHPM). The method performs ViT self-pretraining on the training set of the target data and introduces an auxiliary loss predictor, which first predicts the patch loss and determines the location of the next mask. SelfMedHPM implementation is better than various competitive methods in abdominal CT multi-organ segmentation and body CT multi-organ segmentation. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen mult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body multi-organ segmentation tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02534v1",
        "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery",
        "link": "https://arxiv.org/abs/2504.02534",
        "author": "Mykola Lavreniuk, Nataliia Kussul, Andrii Shelestov, Bohdan Yailymov, Yevhenii Salii, Volodymyr Kuzin, Zoltan Szantoi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02534v1 Announce Type: new \nAbstract: The accurate delineation of agricultural field boundaries from satellite imagery is vital for land management and crop monitoring. However, current methods face challenges due to limited dataset sizes, resolution discrepancies, and diverse environmental conditions. We address this by reformulating the task as instance segmentation and introducing the Field Boundary Instance Segmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset comprising 672,909 high-resolution satellite image patches (ranging from 0.25 m to 10 m) and 22,926,427 instance masks of individual fields, significantly narrowing the gap between agricultural datasets and those in other computer vision domains. We further propose Delineate Anything, an instance segmentation model trained on our new FBIS-22M dataset. Our proposed model sets a new state-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and 103% in mAP@0.5:0.95 over existing methods, while also demonstrating significantly faster inference and strong zero-shot generalization across diverse image resolutions and unseen geographic regions. Code, pre-trained models, and the FBIS-22M dataset are available at https://lavreniuk.github.io/Delineate-Anything."
      },
      {
        "id": "oai:arXiv.org:2504.02536v1",
        "title": "A Sensorimotor Vision Transformer",
        "link": "https://arxiv.org/abs/2504.02536",
        "author": "Konrad Gadzicki, Kerstin Schill, Christoph Zetzsche",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02536v1 Announce Type: new \nAbstract: This paper presents the Sensorimotor Transformer (SMT), a vision model inspired by human saccadic eye movements that prioritize high-saliency regions in visual input to enhance computational efficiency and reduce memory consumption. Unlike traditional models that process all image patches uniformly, SMT identifies and selects the most salient patches based on intrinsic two-dimensional (i2D) features, such as corners and occlusions, which are known to convey high-information content and align with human fixation patterns. The SMT architecture uses this biological principle to leverage vision transformers to process only the most informative patches, allowing for a substantial reduction in memory usage that scales with the sequence length of selected patches. This approach aligns with visual neuroscience findings, suggesting that the human visual system optimizes information gathering through selective, spatially dynamic focus. Experimental evaluations on Imagenet-1k demonstrate that SMT achieves competitive top-1 accuracy while significantly reducing memory consumption and computational complexity, particularly when a limited number of patches is used. This work introduces a saccade-like selection mechanism into transformer-based vision models, offering an efficient alternative for image analysis and providing new insights into biologically motivated architectures for resource-constrained applications."
      },
      {
        "id": "oai:arXiv.org:2504.02542v1",
        "title": "Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation",
        "link": "https://arxiv.org/abs/2504.02542",
        "author": "Fa-Ting Hong, Zunnan Xu, Zixiang Zhou, Jun Zhou, Xiu Li, Qin Lin, Qinglin Lu, Dan Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02542v1 Announce Type: new \nAbstract: Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce \\textbf{ACTalker}, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict."
      },
      {
        "id": "oai:arXiv.org:2504.02543v1",
        "title": "Probabilistic Pontryagin's Maximum Principle for Continuous-Time Model-Based Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.02543",
        "author": "David Leeftink, \\c{C}a\\u{g}atay Y{\\i}ld{\\i}z, Steffen Ridderbusch, Max Hinne, Marcel van Gerven",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02543v1 Announce Type: new \nAbstract: Without exact knowledge of the true system dynamics, optimal control of non-linear continuous-time systems requires careful treatment of epistemic uncertainty. In this work, we propose a probabilistic extension to Pontryagin's maximum principle by minimizing the mean Hamiltonian with respect to epistemic uncertainty. We show minimization of the mean Hamiltonian is a necessary optimality condition when optimizing the mean cost, and propose a multiple shooting numerical method scalable to large-scale probabilistic dynamical models, including ensemble neural ordinary differential equations. Comparisons against state-of-the-art methods in online and offline model-based reinforcement learning tasks show that our probabilistic Hamiltonian formulation leads to reduced trial costs in offline settings and achieves competitive performance in online scenarios. By bridging optimal control and reinforcement learning, our approach offers a principled and practical framework for controlling uncertain systems with learned dynamics."
      },
      {
        "id": "oai:arXiv.org:2504.02544v1",
        "title": "Fourier Sliced-Wasserstein Embedding for Multisets and Measures",
        "link": "https://arxiv.org/abs/2504.02544",
        "author": "Tal Amir, Nadav Dym",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02544v1 Announce Type: new \nAbstract: We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to embed multisets and measures over $\\mathbb{R}^d$ into Euclidean space.\n  Our proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful representations that better capture the structure of the input. Moreover, it is injective on measures and bi-Lipschitz on multisets - a significant advantage over prevalent methods based on sum- or max-pooling, which are provably not bi-Lipschitz, and, in many cases, not even injective. The required output dimension for these guarantees is near-optimal: roughly $2 N d$, where $N$ is the maximal input multiset size.\n  Furthermore, we prove that it is impossible to embed distributions over $\\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of our embedding are, in a sense, the best possible.\n  Through numerical experiments, we demonstrate that our method yields superior multiset representations that improve performance in practical learning tasks. Specifically, we show that (a) a simple combination of the FSW embedding with an MLP achieves state-of-the-art performance in learning the (non-sliced) Wasserstein distance; and (b) replacing max-pooling with the FSW embedding makes PointNet significantly more robust to parameter reduction, with only minor performance degradation even after a 40-fold reduction."
      },
      {
        "id": "oai:arXiv.org:2504.02545v1",
        "title": "MAD: Makeup All-in-One with Cross-Domain Diffusion Model",
        "link": "https://arxiv.org/abs/2504.02545",
        "author": "Bo-Kai Ruan, Hong-Han Shuai",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02545v1 Announce Type: new \nAbstract: Existing makeup techniques often require designing multiple models to handle different inputs and align features across domains for different makeup tasks, e.g., beauty filter, makeup transfer, and makeup removal, leading to increased complexity. Another limitation is the absence of text-guided makeup try-on, which is more user-friendly without needing reference images. In this study, we make the first attempt to use a single model for various makeup tasks. Specifically, we formulate different makeup tasks as cross-domain translations and leverage a cross-domain diffusion model to accomplish all tasks. Unlike existing methods that rely on separate encoder-decoder configurations or cycle-based mechanisms, we propose using different domain embeddings to facilitate domain control. This allows for seamless domain switching by merely changing embeddings with a single model, thereby reducing the reliance on additional modules for different tasks. Moreover, to support precise text-to-makeup applications, we introduce the MT-Text dataset by extending the MT dataset with textual annotations, advancing the practicality of makeup technologies."
      },
      {
        "id": "oai:arXiv.org:2504.02546v1",
        "title": "GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning",
        "link": "https://arxiv.org/abs/2504.02546",
        "author": "Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02546v1 Announce Type: new \nAbstract: Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. As illustrated in our paper, by eliminating both the critic and reference models, and avoiding KL divergence constraints, our approach significantly simplifies the training process when compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. Extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at https://github.com/AMAP-ML/GPG."
      },
      {
        "id": "oai:arXiv.org:2504.02555v1",
        "title": "Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement",
        "link": "https://arxiv.org/abs/2504.02555",
        "author": "Hesong Li, Ziqi Wu, Ruiwen Shao, Tao Zhang, Ying Fu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02555v1 Announce Type: new \nAbstract: Scanning Transmission Electron Microscopy (STEM) enables the observation of atomic arrangements at sub-angstrom resolution, allowing for atomically resolved analysis of the physical and chemical properties of materials. However, due to the effects of noise, electron beam damage, sample thickness, etc, obtaining satisfactory atomic-level images is often challenging. Enhancing STEM images can reveal clearer structural details of materials. Nonetheless, existing STEM image enhancement methods usually overlook unique features in the frequency domain, and existing datasets lack realism and generality. To resolve these issues, in this paper, we develop noise calibration, data synthesis, and enhancement methods for STEM images. We first present a STEM noise calibration method, which is used to synthesize more realistic STEM images. The parameters of background noise, scan noise, and pointwise noise are obtained by statistical analysis and fitting of real STEM images containing atoms. Then we use these parameters to develop a more general dataset that considers both regular and random atomic arrangements and includes both HAADF and BF mode images. Finally, we design a spatial-frequency interactive network for STEM image enhancement, which can explore the information in the frequency domain formed by the periodicity of atomic arrangement. Experimental results show that our data is closer to real STEM images and achieves better enhancement performances together with our network. Code will be available at https://github.com/HeasonLee/SFIN}{https://github.com/HeasonLee/SFIN."
      },
      {
        "id": "oai:arXiv.org:2504.02558v1",
        "title": "Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results",
        "link": "https://arxiv.org/abs/2504.02558",
        "author": "Andrei Dumitriu, Florin Tatui, Florin Miron, Radu Tudor Ionescu, Radu Timofte",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02558v1 Announce Type: new \nAbstract: Rip currents are the leading cause of fatal accidents and injuries on many beaches worldwide, emphasizing the importance of automatically detecting these hazardous surface water currents. In this paper, we address a novel task: rip current instance segmentation. We introduce a comprehensive dataset containing $2,466$ images with newly created polygonal annotations for instance segmentation, used for training and validation. Additionally, we present a novel dataset comprising $17$ drone videos (comprising about $24K$ frames) captured at $30 FPS$, annotated with both polygons for instance segmentation and bounding boxes for object detection, employed for testing purposes. We train various versions of YOLOv8 for instance segmentation on static images and assess their performance on the test dataset (videos). The best results were achieved by the YOLOv8-nano model (runnable on a portable device), with an mAP50 of $88.94%$ on the validation dataset and $81.21%$ macro average on the test dataset. The results provide a baseline for future research in rip current segmentation. Our work contributes to the existing literature by introducing a detailed, annotated dataset, and training a deep learning model for instance segmentation of rip currents. The code, training details and the annotated dataset are made publicly available at https://github.com/Irikos/rip_currents."
      },
      {
        "id": "oai:arXiv.org:2504.02559v1",
        "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables",
        "link": "https://arxiv.org/abs/2504.02559",
        "author": "Siddharth Khincha, Tushar Kataria, Ankita Anand, Dan Roth, Vivek Gupta",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02559v1 Announce Type: new \nAbstract: The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance. Our findings reveal that single-prompt approaches often produce suboptimal results, prompting us to introduce a task decomposition strategy that enhances coherence and accuracy. Our proposed method outperforms existing baselines, particularly in Information Updation (1.79%) and Information Addition (20.58%), highlighting the model strength in dynamically updating and enriching data across architectures"
      },
      {
        "id": "oai:arXiv.org:2504.02560v1",
        "title": "L-LBVC: Long-Term Motion Estimation and Prediction for Learned Bi-Directional Video Compression",
        "link": "https://arxiv.org/abs/2504.02560",
        "author": "Yongqi Zhai, Luyang Tang, Wei Jiang, Jiayu Yang, Ronggang Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02560v1 Announce Type: new \nAbstract: Recently, learned video compression (LVC) has shown superior performance under low-delay configuration. However, the performance of learned bi-directional video compression (LBVC) still lags behind traditional bi-directional coding. The performance gap mainly arises from inaccurate long-term motion estimation and prediction of distant frames, especially in large motion scenes. To solve these two critical problems, this paper proposes a novel LBVC framework, namely L-LBVC. Firstly, we propose an adaptive motion estimation module that can handle both short-term and long-term motions. Specifically, we directly estimate the optical flows for adjacent frames and non-adjacent frames with small motions. For non-adjacent frames with large motions, we recursively accumulate local flows between adjacent frames to estimate long-term flows. Secondly, we propose an adaptive motion prediction module that can largely reduce the bit cost for motion coding. To improve the accuracy of long-term motion prediction, we adaptively downsample reference frames during testing to match the motion ranges observed during training. Experiments show that our L-LBVC significantly outperforms previous state-of-the-art LVC methods and even surpasses VVC (VTM) on some test datasets under random access configuration."
      },
      {
        "id": "oai:arXiv.org:2504.02572v1",
        "title": "Language Models reach higher Agreement than Humans in Historical Interpretation",
        "link": "https://arxiv.org/abs/2504.02572",
        "author": "Fabio Celli, Georgios Spathulas",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02572v1 Announce Type: new \nAbstract: This paper compares historical annotations by humans and Large Language Models. The findings reveal that both exhibit some cultural bias, but Large Language Models achieve a higher consensus on the interpretation of historical facts from short texts. While humans tend to disagree on the basis of their personal biases, Large Models disagree when they skip information or produce hallucinations. These findings have significant implications for digital humanities, enabling large-scale annotation and quantitative analysis of historical data. This offers new educational and research opportunities to explore historical interpretations from different Language Models, fostering critical thinking about bias."
      },
      {
        "id": "oai:arXiv.org:2504.02587v1",
        "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme",
        "link": "https://arxiv.org/abs/2504.02587",
        "author": "Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, Pengfei Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02587v1 Announce Type: new \nAbstract: Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research."
      },
      {
        "id": "oai:arXiv.org:2504.02589v1",
        "title": "Knowledge Graph Completion with Mixed Geometry Tensor Factorization",
        "link": "https://arxiv.org/abs/2504.02589",
        "author": "Viacheslav Yusupov, Maxim Rakhuba, Evgeny Frolov",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02589v1 Announce Type: new \nAbstract: In this paper, we propose a new geometric approach for knowledge graph completion via low rank tensor approximation. We augment a pretrained and well-established Euclidean model based on a Tucker tensor decomposition with a novel hyperbolic interaction term. This correction enables more nuanced capturing of distributional properties in data better aligned with real-world knowledge graphs. By combining two geometries together, our approach improves expressivity of the resulting model achieving new state-of-the-art link prediction accuracy with a significantly lower number of parameters compared to the previous Euclidean and hyperbolic models."
      },
      {
        "id": "oai:arXiv.org:2504.02590v1",
        "title": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning",
        "link": "https://arxiv.org/abs/2504.02590",
        "author": "Kepu Zhang, Guofu Xie, Weijie Yu, Mingyue Xu, Xu Tang, Yaxin Li, Jun Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02590v1 Announce Type: new \nAbstract: The legal mathematical reasoning ability of LLMs is crucial when applying them to real-world scenarios, as it directly affects the credibility of the LLM. While existing legal LLMs can perform general judicial question answering, their legal mathematical reasoning capabilities have not been trained. Open-domain reasoning models, though able to generate detailed calculation steps, do not follow the reasoning logic required for legal scenarios. Additionally, there is currently a lack of legal mathematical reasoning datasets to help validate and enhance LLMs' reasoning abilities in legal contexts. To address these issues, we propose the first Chinese legal Mathematical Reasoning Dataset, LexNum, which includes three common legal mathematical reasoning scenarios: economic compensation, work injury compensation, and traffic accident compensation. Based on LexNum, we tested the performance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a reinforcement learning algorithm guided by legal procedural awareness to train LLMs, enhancing their mathematical reasoning abilities in legal scenarios. Experiments on tasks in the three legal scenarios show that the performance of existing legal LLMs and reasoning models in legal mathematical reasoning tasks is unsatisfactory. LexPam can enhance the LLM's ability in these tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02591v1",
        "title": "State-Space Model Inspired Multiple-Input Multiple-Output Spiking Neurons",
        "link": "https://arxiv.org/abs/2504.02591",
        "author": "Sanja Karilanova, Subhrakanti Dey, Ay\\c{c}a \\\"Oz\\c{c}elikkale",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02591v1 Announce Type: new \nAbstract: In spiking neural networks (SNNs), the main unit of information processing is the neuron with an internal state. The internal state generates an output spike based on its component associated with the membrane potential. This spike is then communicated to other neurons in the network. Here, we propose a general multiple-input multiple-output (MIMO) spiking neuron model that goes beyond this traditional single-input single-output (SISO) model in the SNN literature. Our proposed framework is based on interpreting the neurons as state-space models (SSMs) with linear state evolutions and non-linear spiking activation functions. We illustrate the trade-offs among various parameters of the proposed SSM-inspired neuron model, such as the number of hidden neuron states, the number of input and output channels, including single-input multiple-output (SIMO) and multiple-input single-output (MISO) models. We show that for SNNs with a small number of neurons with large internal state spaces, significant performance gains may be obtained by increasing the number of output channels of a neuron. In particular, a network with spiking neurons with multiple-output channels may achieve the same level of accuracy with the baseline with the continuous-valued communications on the same reference network architecture."
      },
      {
        "id": "oai:arXiv.org:2504.02602v1",
        "title": "Leveraging Sparse Annotations for Leukemia Diagnosis on the Large Leukemia Dataset",
        "link": "https://arxiv.org/abs/2504.02602",
        "author": "Abdul Rehman, Talha Meraj, Aiman Mahmood Minhas, Ayisha Imran, Mohsen Ali, Waqas Sultani, Mubarak Shah",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02602v1 Announce Type: new \nAbstract: Leukemia is 10th most frequently diagnosed cancer and one of the leading causes of cancer related deaths worldwide. Realistic analysis of Leukemia requires White Blook Cells (WBC) localization, classification, and morphological assessment. Despite deep learning advances in medical imaging, leukemia analysis lacks a large, diverse multi-task dataset, while existing small datasets lack domain diversity, limiting real world applicability. To overcome dataset challenges, we present a large scale WBC dataset named Large Leukemia Dataset (LLD) and novel methods for detecting WBC with their attributes. Our contribution here is threefold. First, we present a large-scale Leukemia dataset collected through Peripheral Blood Films (PBF) from several patients, through multiple microscopes, multi cameras, and multi magnification. To enhance diagnosis explainability and medical expert acceptance, each leukemia cell is annotated at 100x with 7 morphological attributes, ranging from Cell Size to Nuclear Shape. Secondly, we propose a multi task model that not only detects WBCs but also predicts their attributes, providing an interpretable and clinically meaningful solution. Third, we propose a method for WBC detection with attribute analysis using sparse annotations. This approach reduces the annotation burden on hematologists, requiring them to mark only a small area within the field of view. Our method enables the model to leverage the entire field of view rather than just the annotated regions, enhancing learning efficiency and diagnostic accuracy. From diagnosis explainability to overcoming domain shift challenges, presented datasets could be used for many challenging aspects of microscopic image analysis. The datasets, code, and demo are available at: https://im.itu.edu.pk/sparse-leukemiaattri/"
      },
      {
        "id": "oai:arXiv.org:2504.02604v1",
        "title": "LinTO Audio and Textual Datasets to Train and Evaluate Automatic Speech Recognition in Tunisian Arabic Dialect",
        "link": "https://arxiv.org/abs/2504.02604",
        "author": "Hedi Naouara, Jean-Pierre Lorr\\'e, J\\'er\\^ome Louradour",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02604v1 Announce Type: new \nAbstract: Developing Automatic Speech Recognition (ASR) systems for Tunisian Arabic Dialect is challenging due to the dialect's linguistic complexity and the scarcity of annotated speech datasets. To address these challenges, we propose the LinTO audio and textual datasets -- comprehensive resources that capture phonological and lexical features of Tunisian Arabic Dialect. These datasets include a variety of texts from numerous sources and real-world audio samples featuring diverse speakers and code-switching between Tunisian Arabic Dialect and English or French. By providing high-quality audio paired with precise transcriptions, the LinTO audio and textual datasets aim to provide qualitative material to build and benchmark ASR systems for the Tunisian Arabic Dialect.\n  Keywords -- Tunisian Arabic Dialect, Speech-to-Text, Low-Resource Languages, Audio Data Augmentation"
      },
      {
        "id": "oai:arXiv.org:2504.02606v1",
        "title": "Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification",
        "link": "https://arxiv.org/abs/2504.02606",
        "author": "Jonas Teufel, Annika Leinweber, Pascal Friederich",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02606v1 Announce Type: new \nAbstract: Explainable AI (xAI) interventions aim to improve interpretability for complex black-box models, not only to improve user trust but also as a means to extract scientific insights from high-performing predictive systems. In molecular property prediction, counterfactual explanations offer a way to understand predictive behavior by highlighting which minimal perturbations in the input molecular structure cause the greatest deviation in the predicted property. However, such explanations only allow for meaningful scientific insights if they reflect the distribution of the true underlying property -- a feature we define as counterfactual truthfulness. To increase this truthfulness, we propose the integration of uncertainty estimation techniques to filter counterfactual candidates with high predicted uncertainty. Through computational experiments with synthetic and real-world datasets, we demonstrate that traditional uncertainty estimation methods, such as ensembles and mean-variance estimation, can already substantially reduce the average prediction error and increase counterfactual truthfulness, especially for out-of-distribution settings. Our results highlight the importance and potential impact of incorporating uncertainty estimation into explainability methods, especially considering the relatively high effectiveness of low-effort interventions like model ensembles."
      },
      {
        "id": "oai:arXiv.org:2504.02607v1",
        "title": "Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks",
        "link": "https://arxiv.org/abs/2504.02607",
        "author": "Samuel Tesfazgi, Leonhard Sprandl, Sandra Hirche",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02607v1 Announce Type: new \nAbstract: The practical deployment of learning-based autonomous systems would greatly benefit from tools that flexibly obtain safety guarantees in the form of certificate functions from data. While the geometrical properties of such certificate functions are well understood, synthesizing them using machine learning techniques still remains a challenge. To mitigate this issue, we propose a diffeomorphic function learning framework where prior structural knowledge of the desired output is encoded in the geometry of a simple surrogate function, which is subsequently augmented through an expressive, topology-preserving state-space transformation. Thereby, we achieve an indirect function approximation framework that is guaranteed to remain in the desired hypothesis space. To this end, we introduce a novel approach to construct diffeomorphic maps based on RBF networks, which facilitate precise, local transformations around data. Finally, we demonstrate our approach by learning diffeomorphic Lyapunov functions from real-world data and apply our method to different attractor systems."
      },
      {
        "id": "oai:arXiv.org:2504.02612v1",
        "title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation",
        "link": "https://arxiv.org/abs/2504.02612",
        "author": "Jiwoo Chung, Sangeek Hyun, Hyunjun Kim, Eunseo Koh, MinKyu Lee, Jae-Pil Heo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02612v1 Announce Type: new \nAbstract: Recent advances in text-to-image generative models have enabled numerous practical applications, including subject-driven generation, which fine-tunes pretrained models to capture subject semantics from only a few examples. While diffusion-based models produce high-quality images, their extensive denoising steps result in significant computational overhead, limiting real-world applicability. Visual autoregressive~(VAR) models, which predict next-scale tokens rather than spatially adjacent ones, offer significantly faster inference suitable for practical deployment. In this paper, we propose the first VAR-based approach for subject-driven generation. However, na\\\"{\\i}ve fine-tuning VAR leads to computational overhead, language drift, and reduced diversity. To address these challenges, we introduce selective layer tuning to reduce complexity and prior distillation to mitigate language drift. Additionally, we found that the early stages have a greater influence on the generation of subject than the latter stages, which merely synthesize local details. Based on this finding, we propose scale-wise weighted tuning, which prioritizes coarser resolutions for promoting the model to focus on the subject-relevant information instead of local details. Extensive experiments validate that our method significantly outperforms diffusion-based baselines across various metrics and demonstrates its practical usage."
      },
      {
        "id": "oai:arXiv.org:2504.02615v1",
        "title": "A Hybrid Similarity-Aware Graph Neural Network with Transformer for Node Classification",
        "link": "https://arxiv.org/abs/2504.02615",
        "author": "Aman Singh, Shahid Shafi Dar, Ranveer Singh, Nagendra Kumar",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02615v1 Announce Type: new \nAbstract: Node classification has gained significant importance in graph deep learning with real-world applications such as recommendation systems, drug discovery, and citation networks. Graph Convolutional Networks and Graph Transformers have achieved superior performance in node classification tasks. However, the key concern with Graph Convolutional Networks is over-squashing, which limits their ability to capture long-range dependencies in the network. Additionally, Graph Transformers face scalability challenges, making it difficult to process large graphs efficiently. To address this, we propose a novel framework, A Hybrid SImilarity-Aware Graph Neural Network with Transformer for Node Classification (SIGNNet), which capitalizes on local and global structural information, enhances the model's capability to effectively capture fine-grained relationships and broader contextual patterns within the graph structure. The proposed method leverages Graph Convolutional Networks alongside a score-based mechanism to effectively capture local and global node interactions while addressing the limitations of over-squashing. Our proposed method employs a novel Personalized PageRank-based node sampling method to address scalability issues by generating subgraphs of nodes. Additionally, SIGNNet incorporates a novel attention mechanism, Structure-Aware Multi-Head Attention (SA-MHA), which integrates node structural information for informed attention weighting, enabling the model to prioritize nodes based on topological significance. Extensive experiments demonstrate the significant improvements achieved by the proposed method over existing state-of-the-art methods, with average accuracy gains of 6.03%, 5.47%, 4.78%, 19.10%, 19.61%, 7.22%, 19.54%, and 14.94% on Cora, Citeseer, CS, Wisconsin, Texas, Actor, Cornell and Chameleon datasets, respectively."
      },
      {
        "id": "oai:arXiv.org:2504.02617v1",
        "title": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation",
        "link": "https://arxiv.org/abs/2504.02617",
        "author": "Lihua Liu, Jiehong Lin, Zhenxin Liu, Kui Jia",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02617v1 Announce Type: new \nAbstract: Novel object pose estimation from RGB images presents a significant challenge for zero-shot generalization, as it involves estimating the relative 6D transformation between an RGB observation and a CAD model of an object that was not seen during training. In this paper, we introduce PicoPose, a novel framework designed to tackle this task using a three-stage pixel-to-pixel correspondence learning process. Firstly, PicoPose matches features from the RGB observation with those from rendered object templates, identifying the best-matched template and establishing coarse correspondences. Secondly, PicoPose smooths the correspondences by globally regressing a 2D affine transformation, including in-plane rotation, scale, and 2D translation, from the coarse correspondence map. Thirdly, PicoPose applies the affine transformation to the feature map of the best-matched template and learns correspondence offsets within local regions to achieve fine-grained correspondences. By progressively refining the correspondences, PicoPose significantly improves the accuracy of object poses computed via PnP/RANSAC. PicoPose achieves state-of-the-art performance on the seven core datasets of the BOP benchmark, demonstrating exceptional generalization to novel objects represented by CAD models or object reference images. Code and models are available at https://github.com/foollh/PicoPose."
      },
      {
        "id": "oai:arXiv.org:2504.02618v1",
        "title": "Variational Online Mirror Descent for Robust Learning in Schr\\\"odinger Bridge",
        "link": "https://arxiv.org/abs/2504.02618",
        "author": "Dong-Sig Han, Jaein Kim, Hee Bin Yoo, Byoung-Tak Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02618v1 Announce Type: new \nAbstract: Sch\\\"odinger bridge (SB) has evolved into a universal class of probabilistic generative models. In practice, however, estimated learning signals are often uncertain, and the reliability promised by existing methods is often based on speculative optimal-case scenarios. Recent studies regarding the Sinkhorn algorithm through mirror descent (MD) have gained attention, revealing geometric insights into solution acquisition of the SB problems. In this paper, we propose a variational online MD (OMD) framework for the SB problems, which provides further stability to SB solvers. We formally prove convergence and a regret bound for the novel OMD formulation of SB acquisition. As a result, we propose a simulation-free SB algorithm called Variational Mirrored Schr\\\"odinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of the Gaussian mixture parameterization for Schr\\\"odinger potentials. Based on the Wasserstein gradient flow theory, the algorithm offers tractable learning dynamics that precisely approximate each OMD step. In experiments, we validate the performance of the proposed VMSB algorithm across an extensive suite of benchmarks. VMSB consistently outperforms contemporary SB solvers on a range of SB problems, demonstrating the robustness predicted by our theory."
      },
      {
        "id": "oai:arXiv.org:2504.02620v1",
        "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
        "link": "https://arxiv.org/abs/2504.02620",
        "author": "Leonardo Iurada, Marco Ciccone, Tatiana Tommasi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02620v1 Announce Type: new \nAbstract: Task arithmetic has emerged as a promising approach for editing models by representing task-specific knowledge as composable task vectors. However, existing methods rely on network linearization to derive task vectors, leading to computational bottlenecks during training and inference. Moreover, linearization alone does not ensure weight disentanglement, the key property that enables conflict-free composition of task vectors. To address this, we propose TaLoS which allows to build sparse task vectors with minimal interference without requiring explicit linearization and sharing information across tasks. We find that pre-trained models contain a subset of parameters with consistently low gradient sensitivity across tasks, and that sparsely updating only these parameters allows for promoting weight disentanglement during fine-tuning. Our experiments prove that TaLoS improves training and inference efficiency while outperforming current methods in task addition and negation. By enabling modular parameter editing, our approach fosters practical deployment of adaptable foundation models in real-world applications."
      },
      {
        "id": "oai:arXiv.org:2504.02630v1",
        "title": "Grammar-based Ordinary Differential Equation Discovery",
        "link": "https://arxiv.org/abs/2504.02630",
        "author": "Karin L. Yu, Eleni Chatzi, Georgios Kissas",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02630v1 Announce Type: new \nAbstract: The understanding and modeling of complex physical phenomena through dynamical systems has historically driven scientific progress, as it provides the tools for predicting the behavior of different systems under diverse conditions through time. The discovery of dynamical systems has been indispensable in engineering, as it allows for the analysis and prediction of complex behaviors for computational modeling, diagnostics, prognostics, and control of engineered systems. Joining recent efforts that harness the power of symbolic regression in this domain, we propose a novel framework for the end-to-end discovery of ordinary differential equations (ODEs), termed Grammar-based ODE Discovery Engine (GODE). The proposed methodology combines formal grammars with dimensionality reduction and stochastic search for efficiently navigating high-dimensional combinatorial spaces. Grammars allow us to seed domain knowledge and structure for both constraining, as well as, exploring the space of candidate expressions. GODE proves to be more sample- and parameter-efficient than state-of-the-art transformer-based models and to discover more accurate and parsimonious ODE expressions than both genetic programming- and other grammar-based methods for more complex inference tasks, such as the discovery of structural dynamics. Thus, we introduce a tool that could play a catalytic role in dynamics discovery tasks, including modeling, system identification, and monitoring tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02639v1",
        "title": "Reservoir Computing: A New Paradigm for Neural Networks",
        "link": "https://arxiv.org/abs/2504.02639",
        "author": "Felix Grezes",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02639v1 Announce Type: new \nAbstract: A Literature Review of Reservoir Computing.\n  Even before Artificial Intelligence was its own field of computational science, humanity has tried to mimic the activity of the human brain. In the early 1940s the first artificial neuron models were created as purely mathematical concepts. Over the years, ideas from neuroscience and computer science were used to develop the modern Neural Network. The interest in these models rose quickly but fell when they failed to be successfully applied to practical applications, and rose again in the late 2000s with the drastic increase in computing power, notably in the field of natural language processing, for example with the state-of-the-art speech recognizer making heavy use of deep neural networks.\n  Recurrent Neural Networks (RNNs), a class of neural networks with cycles in the network, exacerbates the difficulties of traditional neural nets. Slow convergence limiting the use to small networks, and difficulty to train through gradient-descent methods because of the recurrent dynamics have hindered research on RNNs, yet their biological plausibility and their capability to model dynamical systems over simple functions makes then interesting for computational researchers.\n  Reservoir Computing emerges as a solution to these problems that RNNs traditionally face. Promising to be both theoretically sound and computationally fast, Reservoir Computing has already been applied successfully to numerous fields: natural language processing, computational biology and neuroscience, robotics, even physics. This survey will explore the history and appeal of both traditional feed-forward and recurrent neural networks, before describing the theory and models of this new reservoir computing paradigm. Finally recent papers using reservoir computing in a variety of scientific fields will be reviewed."
      },
      {
        "id": "oai:arXiv.org:2504.02640v1",
        "title": "RoSMM: A Robust and Secure Multi-Modal Watermarking Framework for Diffusion Models",
        "link": "https://arxiv.org/abs/2504.02640",
        "author": "ZhongLi Fang, Yu Xie, Ping Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02640v1 Announce Type: new \nAbstract: Current image watermarking technologies are predominantly categorized into text watermarking techniques and image steganography; however, few methods can simultaneously handle text and image-based watermark data, which limits their applicability in complex digital environments. This paper introduces an innovative multi-modal watermarking approach, drawing on the concept of vector discretization in encoder-based vector quantization. By constructing adjacency matrices, the proposed method enables the transformation of text watermarks into robust image-based representations, providing a novel multi-modal watermarking paradigm for image generation applications. Additionally, this study presents a newly designed image restoration module to mitigate image degradation caused by transmission losses and various noise interferences, thereby ensuring the reliability and integrity of the watermark. Experimental results validate the robustness of the method under multiple noise attacks, providing a secure, scalable, and efficient solution for digital image copyright protection."
      },
      {
        "id": "oai:arXiv.org:2504.02644v1",
        "title": "Solving the Paint Shop Problem with Flexible Management of Multi-Lane Buffers Using Reinforcement Learning and Action Masking",
        "link": "https://arxiv.org/abs/2504.02644",
        "author": "Mirko Stappert, Bernhard Lutz, Janis Brammer, Dirk Neumann",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02644v1 Announce Type: new \nAbstract: In the paint shop problem, an unordered incoming sequence of cars assigned to different colors has to be reshuffled with the objective of minimizing the number of color changes. To reshuffle the incoming sequence, manufacturers can employ a first-in-first-out multi-lane buffer system allowing store and retrieve operations. So far, prior studies primarily focused on simple decision heuristics like greedy or simplified problem variants that do not allow full flexibility when performing store and retrieve operations. In this study, we propose a reinforcement learning approach to minimize color changes for the flexible problem variant, where store and retrieve operations can be performed in an arbitrary order. After proving that greedy retrieval is optimal, we incorporate this finding into the model using action masking. Our evaluation, based on 170 problem instances with 2-8 buffer lanes and 5-15 colors, shows that our approach reduces color changes compared to existing methods by considerable margins depending on the problem size. Furthermore, we demonstrate the robustness of our approach towards different buffer sizes and imbalanced color distributions."
      },
      {
        "id": "oai:arXiv.org:2504.02646v1",
        "title": "Prompt Optimization with Logged Bandit Data",
        "link": "https://arxiv.org/abs/2504.02646",
        "author": "Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02646v1 Announce Type: new \nAbstract: We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts. Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions. To circumvent these challenges, we propose a novel kernel-based off-policy gradient method, which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias. Empirical results on our newly established suite of benchmarks demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large."
      },
      {
        "id": "oai:arXiv.org:2504.02658v1",
        "title": "MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank Compensators",
        "link": "https://arxiv.org/abs/2504.02658",
        "author": "Beichen Huang, Yueming Yuan, Zelei Shao, Minjia Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02658v1 Announce Type: new \nAbstract: A critical approach for efficiently deploying Mixture-of-Experts (MoE) models with massive parameters is quantization. However, state-of-the-art MoE models suffer from non-negligible accuracy loss with extreme quantization, such as under 4 bits. To address this, we introduce MiLo, a novel method that augments highly quantized MoEs with a mixture of low-rank compensators. These compensators consume only a small amount of additional memory but significantly recover accuracy loss from extreme quantization. MiLo also identifies that MoEmodels exhibit distinctive characteristics across weights due to their hybrid dense-sparse architectures, and employs adaptive rank selection policies along with iterative optimizations to close the accuracy gap. MiLo does not rely on calibration data, allowing it to generalize to different MoE models and datasets without overfitting to a calibration set. To avoid the hardware inefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor Core-friendly 3-bit kernels, enabling measured latency speedups on 3-bit quantized MoE models. Our evaluation shows that MiLo outperforms existing methods on SoTA MoE models across various tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02662v1",
        "title": "Integrating Human Knowledge Through Action Masking in Reinforcement Learning for Operations Research",
        "link": "https://arxiv.org/abs/2504.02662",
        "author": "Mirko Stappert, Bernhard Lutz, Niklas Goby, Dirk Neumann",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02662v1 Announce Type: new \nAbstract: Reinforcement learning (RL) provides a powerful method to address problems in operations research. However, its real-world application often fails due to a lack of user acceptance and trust. A possible remedy is to provide managers with the possibility of altering the RL policy by incorporating human expert knowledge. In this study, we analyze the benefits and caveats of including human knowledge via action masking. While action masking has so far been used to exclude invalid actions, its ability to integrate human expertise remains underexplored. Human knowledge is often encapsulated in heuristics, which suggest reasonable, near-optimal actions in certain situations. Enforcing such actions should hence increase trust among the human workforce to rely on the model's decisions. Yet, a strict enforcement of heuristic actions may also restrict the policy from exploring superior actions, thereby leading to overall lower performance. We analyze the effects of action masking based on three problems with different characteristics, namely, paint shop scheduling, peak load management, and inventory management. Our findings demonstrate that incorporating human knowledge through action masking can achieve substantial improvements over policies trained without action masking. In addition, we find that action masking is crucial for learning effective policies in constrained action spaces, where certain actions can only be performed a limited number of times. Finally, we highlight the potential for suboptimal outcomes when action masks are overly restrictive."
      },
      {
        "id": "oai:arXiv.org:2504.02666v1",
        "title": "BECAME: BayEsian Continual Learning with Adaptive Model MErging",
        "link": "https://arxiv.org/abs/2504.02666",
        "author": "Mei Li, Yuxiang Lu, Qinyan Dai, Suizhi Huang, Yue Ding, Hongtao Lu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02666v1 Announce Type: new \nAbstract: Continual Learning (CL) strives to learn incrementally across tasks while mitigating catastrophic forgetting. A key challenge in CL is balancing stability (retaining prior knowledge) and plasticity (learning new tasks). While representative gradient projection methods ensure stability, they often limit plasticity. Model merging techniques offer promising solutions, but prior methods typically rely on empirical assumptions and carefully selected hyperparameters. In this paper, we explore the potential of model merging to enhance the stability-plasticity trade-off, providing theoretical insights that underscore its benefits. Specifically, we reformulate the merging mechanism using Bayesian continual learning principles and derive a closed-form solution for the optimal merging coefficient that adapts to the diverse characteristics of tasks. To validate our approach, we introduce a two-stage framework named BECAME, which synergizes the expertise of gradient projection and adaptive merging. Extensive experiments show that our approach outperforms state-of-the-art CL methods and existing merging strategies."
      },
      {
        "id": "oai:arXiv.org:2504.02667v1",
        "title": "Compositionality Unlocks Deep Interpretable Models",
        "link": "https://arxiv.org/abs/2504.02667",
        "author": "Thomas Dooms, Ward Gauderis, Geraint A. Wiggins, Jose Oramas",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02667v1 Announce Type: new \nAbstract: We propose $\\chi$-net, an intrinsically interpretable architecture combining the compositional multilinear structure of tensor networks with the expressivity and efficiency of deep neural networks. $\\chi$-nets retain equal accuracy compared to their baseline counterparts. Our novel, efficient diagonalisation algorithm, ODT, reveals linear low-rank structure in a multilayer SVHN model. We leverage this toward formal weight-based interpretability and model compression."
      },
      {
        "id": "oai:arXiv.org:2504.02671v1",
        "title": "LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems",
        "link": "https://arxiv.org/abs/2504.02671",
        "author": "Zishuo Liu, Carlos Rabat Villarreal, Mostafa Rahgouy, Amit Das, Zheng Zhang, Chang Ren, Dongji Feng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02671v1 Announce Type: new \nAbstract: Fermi Problems (FPs) are mathematical reasoning tasks that require human-like logic and numerical reasoning. Unlike other reasoning questions, FPs often involve real-world impracticalities or ambiguous concepts, making them challenging even for humans to solve. Despite advancements in AI, particularly with large language models (LLMs) in various reasoning tasks, FPs remain relatively under-explored. This work conducted an exploratory study to examine the capabilities and limitations of LLMs in solving FPs. We first evaluated the overall performance of three advanced LLMs using a publicly available FP dataset. We designed prompts according to the recently proposed TELeR taxonomy, including a zero-shot scenario. Results indicated that all three LLMs achieved a fp_score (range between 0 - 1) below 0.5, underscoring the inherent difficulty of these reasoning tasks. To further investigate, we categorized FPs into standard and specific questions, hypothesizing that LLMs would perform better on standard questions, which are characterized by clarity and conciseness, than on specific ones. Comparative experiments confirmed this hypothesis, demonstrating that LLMs performed better on standard FPs in terms of both accuracy and efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.02674v1",
        "title": "Limitations of Religious Data and the Importance of the Target Domain: Towards Machine Translation for Guinea-Bissau Creole",
        "link": "https://arxiv.org/abs/2504.02674",
        "author": "Jacqueline Rowe, Edward Gow-Smith, Mark Hepple",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02674v1 Announce Type: new \nAbstract: We introduce a new dataset for machine translation of Guinea-Bissau Creole (Kiriol), comprising around 40 thousand parallel sentences to English and Portuguese. This dataset is made up of predominantly religious data (from the Bible and texts from the Jehovah's Witnesses), but also a small amount of general domain data (from a dictionary). This mirrors the typical resource availability of many low resource languages. We train a number of transformer-based models to investigate how to improve domain transfer from religious data to a more general domain. We find that adding even 300 sentences from the target domain when training substantially improves the translation performance, highlighting the importance and need for data collection for low-resource languages, even on a small-scale. We additionally find that Portuguese-to-Kiriol translation models perform better on average than other source and target language pairs, and investigate how this relates to the morphological complexity of the languages involved and the degree of lexical overlap between creoles and lexifiers. Overall, we hope our work will stimulate research into Kiriol and into how machine translation might better support creole languages in general."
      },
      {
        "id": "oai:arXiv.org:2504.02685v1",
        "title": "STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability",
        "link": "https://arxiv.org/abs/2504.02685",
        "author": "Iv\\'an Sevillano-Garc\\'ia, Juli\\'an Luengo, Francisco Herrera",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02685v1 Announce Type: new \nAbstract: Out-of-Distribution (OOD) detection is a critical task in machine learning, particularly in safety-sensitive applications where model failures can have serious consequences. However, current OOD detection methods often suffer from restrictive distributional assumptions, limited scalability, and a lack of interpretability. To address these challenges, we propose STOOD-X, a two-stage methodology that combines a Statistical nonparametric Test for OOD Detection with eXplainability enhancements. In the first stage, STOOD-X uses feature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD samples without assuming a specific feature distribution. In the second stage, it generates user-friendly, concept-based visual explanations that reveal the features driving each decision, aligning with the BLUE XAI paradigm. Through extensive experiments on benchmark datasets and multiple architectures, STOOD-X achieves competitive performance against state-of-the-art post hoc OOD detectors, particularly in high-dimensional and complex settings. In addition, its explainability framework enables human oversight, bias detection, and model debugging, fostering trust and collaboration between humans and AI systems. The STOOD-X methodology therefore offers a robust, explainable, and scalable solution for real-world OOD detection tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02692v1",
        "title": "GPTQv2: Efficient Finetuning-Free Quantization for Asymmetric Calibration",
        "link": "https://arxiv.org/abs/2504.02692",
        "author": "Yuhang Li, Ruokai Yin, Donghyun Lee, Shiting Xiao, Priyadarshini Panda",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02692v1 Announce Type: new \nAbstract: We introduce GPTQv2, a novel finetuning-free quantization method for compressing large-scale transformer architectures. Unlike the previous GPTQ method, which independently calibrates each layer, we always match the quantized layer's output to the exact output in the full-precision model, resulting in a scheme that we call asymmetric calibration. Such a scheme can effectively reduce the quantization error accumulated in previous layers. We analyze this problem using optimal brain compression to derive a close-formed solution. The new solution explicitly minimizes the quantization error as well as the accumulated asymmetry error. Furthermore, we utilize various techniques to parallelize the solution calculation, including channel parallelization, neuron decomposition, and Cholesky reformulation for matrix fusion. As a result, GPTQv2 is easy to implement, simply using 20 more lines of code than GPTQ but improving its performance under low-bit quantization. Remarkably, on a single GPU, we quantize a 405B language transformer as well as EVA-02 the rank first vision transformer that achieves 90% pretraining Imagenet accuracy. Code is available at github.com/Intelligent-Computing-Lab-Yale/GPTQv2."
      },
      {
        "id": "oai:arXiv.org:2504.02697v1",
        "title": "Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation",
        "link": "https://arxiv.org/abs/2504.02697",
        "author": "Xingguang Zhang, Nicholas Chimitt, Xijun Wang, Yu Yuan, Stanley H. Chan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02697v1 Announce Type: new \nAbstract: Atmospheric turbulence is a major source of image degradation in long-range imaging systems. Although numerous deep learning-based turbulence mitigation (TM) methods have been proposed, many are slow, memory-hungry, and do not generalize well. In the spatial domain, methods based on convolutional operators have a limited receptive field, so they cannot handle a large spatial dependency required by turbulence. In the temporal domain, methods relying on self-attention can, in theory, leverage the lucky effects of turbulence, but their quadratic complexity makes it difficult to scale to many frames. Traditional recurrent aggregation methods face parallelization challenges.\n  In this paper, we present a new TM method based on two concepts: (1) A turbulence mitigation network based on the Selective State Space Model (MambaTM). MambaTM provides a global receptive field in each layer across spatial and temporal dimensions while maintaining linear computational complexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state space model. Unlike classical Zernike-based representations of phase distortion, the new LPD map uniquely captures the actual effects of turbulence, significantly improving the model's capability to estimate degradation by reducing the ill-posedness. Our proposed method exceeds current state-of-the-art networks on various synthetic and real-world TM benchmarks with significantly faster inference speed. The code is available at http://github.com/xg416/MambaTM."
      },
      {
        "id": "oai:arXiv.org:2504.02698v1",
        "title": "SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions",
        "link": "https://arxiv.org/abs/2504.02698",
        "author": "Shengrui XU, Tianchi Lu, Zikun Wang, Jixiu Zhai, Jingwan Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02698v1 Announce Type: new \nAbstract: Protein-Protein Interaction (PPI) prediction is a key task in uncovering cellular functional networks and disease mechanisms. However, traditional experimental methods are time-consuming and costly, and existing computational models face challenges in cross-modal feature fusion, robustness, and false-negative suppression. In this paper, we propose a novel supervised contrastive multimodal framework, SCMPPI, for PPI prediction. By integrating protein sequence features (AAC, DPC, CKSAAP-ESMC) with PPI network topology information (Node2Vec graph embedding), and combining an improved supervised contrastive learning strategy, SCMPPI significantly enhances PPI prediction performance. For the PPI task, SCMPPI introduces a negative sample filtering mechanism and modifies the contrastive loss function, effectively optimizing multimodal features. Experiments on eight benchmark datasets, including yeast, human, and H.pylori, show that SCMPPI outperforms existing state-of-the-art methods (such as DF-PPI and TAGPPI) in key metrics such as accuracy ( 98.01%) and AUC (99.62%), and demonstrates strong generalization in cross-species prediction (AUC > 99% on multi-species datasets). Furthermore, SCMPPI has been successfully applied to CD9 networks, the Wnt pathway, and cancer-specific networks, providing a reliable tool for disease target discovery. This framework also offers a new paradigm for multimodal biological information fusion and contrastive learning in collaborative optimization for various combined predictions."
      },
      {
        "id": "oai:arXiv.org:2504.02708v1",
        "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context",
        "link": "https://arxiv.org/abs/2504.02708",
        "author": "Nikhil Verma, Manasa Bharadwaj",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02708v1 Announce Type: new \nAbstract: Alignment tuning has enabled large language models to excel in reasoning, instruction-following, and minimizing harmful generations. However, despite their widespread deployment, these models exhibit a monolingual bias, raising concerns about the effectiveness of alignment across languages. Current alignment methods predominantly focus on English, leaving it unclear how alignment mechanism generalize to multilingual settings. To address this, we conduct a systematic analysis of distributional shifts in the embedding space of LLMs before and after alignment, uncovering its impact on model behavior across diverse languages. We leverage the alignment-induced separation in safety space as a quantitative tool to measure how alignment enforces safety constraints. Our study evaluates seven LLMs using balanced toxicity datasets and parallel text-detoxification benchmarks, revealing substantial disparities in the latent representation space between high-resource and low-resource languages. These findings underscore the need for language-specific fine-tuning to ensure fair, reliable and robust multilingual alignment. Our insights provide a foundation for developing truly safe multilingual LLMs, emphasizing the urgency of addressing alignment gaps in underrepresented languages."
      },
      {
        "id": "oai:arXiv.org:2504.02725v1",
        "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization",
        "link": "https://arxiv.org/abs/2504.02725",
        "author": "Kehua Feng, Keyan Ding, Jing Yu, Menghan Li, Yuhao Wang, Tong Xu, Xinda Wang, Qiang Zhang, Huajun Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02725v1 Announce Type: new \nAbstract: Recent advancements in large language models (LLMs) have accelerated progress toward artificial general intelligence, yet their potential to generate harmful content poses critical safety challenges. Existing alignment methods often struggle to cover diverse safety scenarios and remain vulnerable to adversarial attacks. In this work, we propose Ex-Ante Reasoning Preference Optimization (ERPO), a novel safety alignment framework that equips LLMs with explicit preemptive reasoning through Chain-of-Thought and provides clear evidence for safety judgments by embedding predefined safety rules. Specifically, our approach consists of three stages: first, equipping the model with Ex-Ante reasoning through supervised fine-tuning (SFT) using a constructed reasoning module; second, enhancing safety, usefulness, and efficiency via Direct Preference Optimization (DPO); and third, mitigating inference latency with a length-controlled iterative preference optimization strategy. Experiments on multiple open-source LLMs demonstrate that ERPO significantly enhances safety performance while maintaining response efficiency."
      },
      {
        "id": "oai:arXiv.org:2504.02730v1",
        "title": "HQViT: Hybrid Quantum Vision Transformer for Image Classification",
        "link": "https://arxiv.org/abs/2504.02730",
        "author": "Hui Zhang, Qinglin Zhao, Mengchu Zhou, Li Feng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02730v1 Announce Type: new \nAbstract: Transformer-based architectures have revolutionized the landscape of deep learning. In computer vision domain, Vision Transformer demonstrates remarkable performance on par with or even surpassing that of convolutional neural networks. However, the quadratic computational complexity of its self-attention mechanism poses challenges for classical computing, making model training with high-dimensional input data, e.g., images, particularly expensive. To address such limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that leverages the principles of quantum computing to accelerate model training while enhancing model performance. HQViT introduces whole-image processing with amplitude encoding to better preserve global image information without additional positional encoding. By leveraging quantum computation on the most critical steps and selectively handling other components in a classical way, we lower the cost of quantum resources for HQViT. The qubit requirement is minimized to $O(log_2N)$ and the number of parameterized quantum gates is only $O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum devices. By offloading the computationally intensive attention coefficient matrix calculation to the quantum framework, HQViT reduces the classical computational load by $O(T^2d)$. Extensive experiments across various computer vision datasets demonstrate that HQViT outperforms existing models, achieving a maximum improvement of up to $10.9\\%$ (on the MNIST 10-classification task) over the state of the art. This work highlights the great potential to combine quantum and classical computing to cope with complex image classification tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02732v1",
        "title": "Why do LLMs attend to the first token?",
        "link": "https://arxiv.org/abs/2504.02732",
        "author": "Federico Barbero, \\'Alvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veli\\v{c}kovi \\'c, Razvan Pascanu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02732v1 Announce Type: new \nAbstract: Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training."
      },
      {
        "id": "oai:arXiv.org:2504.02733v1",
        "title": "Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study",
        "link": "https://arxiv.org/abs/2504.02733",
        "author": "Aryan Agrawal, Lisa Alazraki, Shahin Honarvar, Marek Rei",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02733v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are highly vulnerable to input perturbations, as even a small prompt change may result in a substantially different output. Existing methods to enhance LLM robustness are primarily focused on perturbed data samples, whereas improving resiliency to perturbations of task-level instructions has remained relatively underexplored. In this work, we focus on character- and word-level edits of task-specific instructions, which substantially degrade downstream performance. We experiment with a variety of techniques to enhance the robustness of LLMs, including self-denoising and representation alignment, testing different models (Llama 3 and Flan-T5), datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and role-oriented). We find that, on average, self-denoising -- whether performed by a frozen LLM or a fine-tuned model -- achieves substantially higher performance gains than alternative strategies, including more complex baselines such as ensembling and supervised methods."
      },
      {
        "id": "oai:arXiv.org:2504.02757v1",
        "title": "Echoes of the hidden: Uncovering coordination beyond network structure",
        "link": "https://arxiv.org/abs/2504.02757",
        "author": "Shahar Somin, Tom Cohen, Jeremy Kepner, Alex Pentland",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02757v1 Announce Type: new \nAbstract: The study of connectivity and coordination has drawn increasing attention in recent decades due to their central role in driving markets, shaping societal dynamics, and influencing biological systems. Traditionally, observable connections, such as phone calls, financial transactions, or social media connections, have been used to infer coordination and connectivity. However, incomplete, encrypted, or fragmented data, alongside the ubiquity of communication platforms and deliberate obfuscation, often leave many real-world connections hidden. In this study, we demonstrate that coordinating individuals exhibit shared bursty activity patterns, enabling their detection even when observable links between them are sparse or entirely absent. We further propose a generative model based on the network of networks formalism to account for the mechanisms driving this collaborative burstiness, attributing it to shock propagation across networks rather than isolated individual behavior. Model simulations demonstrate that when observable connection density is below 70\\%, burstiness significantly improves coordination detection compared to state-of-the-art temporal and structural methods. This work provides a new perspective on community and coordination dynamics, advancing both theoretical understanding and practical detection. By laying the foundation for identifying hidden connections beyond observable network structures, it enables detection across different platforms, alongside enhancing system behavior understanding, informed decision-making, and risk mitigation."
      },
      {
        "id": "oai:arXiv.org:2504.02762v1",
        "title": "MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection",
        "link": "https://arxiv.org/abs/2504.02762",
        "author": "Ahmet Burak Yildirim, Mustafa Utku Aydogdu, Duygu Ceylan, Aysegul Dundar",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02762v1 Announce Type: new \nAbstract: We introduce MD-ProjTex, a method for fast and consistent text-guided texture generation for 3D shapes using pretrained text-to-image diffusion models. At the core of our approach is a multi-view consistency mechanism in UV space, which ensures coherent textures across different viewpoints. Specifically, MD-ProjTex fuses noise predictions from multiple views at each diffusion step and jointly updates the per-view denoising directions to maintain 3D consistency. In contrast to existing state-of-the-art methods that rely on optimization or sequential view synthesis, MD-ProjTex is computationally more efficient and achieves better quantitative and qualitative results."
      },
      {
        "id": "oai:arXiv.org:2504.02763v1",
        "title": "CanonNet: Canonical Ordering and Curvature Learning for Point Cloud Analysis",
        "link": "https://arxiv.org/abs/2504.02763",
        "author": "Benjy Friedmann, Michael Werman",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02763v1 Announce Type: new \nAbstract: Point cloud processing poses two fundamental challenges: establishing consistent point ordering and effectively learning fine-grained geometric features. Current architectures rely on complex operations that limit expressivity while struggling to capture detailed surface geometry. We present CanonNet, a lightweight neural network composed of two complementary components: (1) a preprocessing pipeline that creates a canonical point ordering and orientation, and (2) a geometric learning framework where networks learn from synthetic surfaces with precise curvature values. This modular approach eliminates the need for complex transformation-invariant architectures while effectively capturing local geometric properties. Our experiments demonstrate state-of-the-art performance in curvature estimation and competitive results in geometric descriptor tasks with significantly fewer parameters (\\textbf{100X}) than comparable methods. CanonNet's efficiency makes it particularly suitable for real-world applications where computational resources are limited, demonstrating that mathematical preprocessing can effectively complement neural architectures for point cloud analysis. The code for the project is publicly available \\hyperlink{https://benjyfri.github.io/CanonNet/}{https://benjyfri.github.io/CanonNet/}."
      },
      {
        "id": "oai:arXiv.org:2504.02764v1",
        "title": "Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model",
        "link": "https://arxiv.org/abs/2504.02764",
        "author": "Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, Yueqi Duan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02764v1 Announce Type: new \nAbstract: In this paper, we propose Scene Splatter, a momentum-based paradigm for video diffusion to generate generic scenes from single image. Existing methods, which employ video generation models to synthesize novel views, suffer from limited video length and scene inconsistency, leading to artifacts and distortions during further reconstruction. To address this issue, we construct noisy samples from original features as momentum to enhance video details and maintain scene consistency. However, for latent features with the perception field that spans both known and unknown regions, such latent-level momentum restricts the generative ability of video diffusion in unknown regions. Therefore, we further introduce the aforementioned consistent video as a pixel-level momentum to a directly generated video without momentum for better recovery of unseen regions. Our cascaded momentum enables video diffusion models to generate both high-fidelity and consistent novel views. We further finetune the global Gaussian representations with enhanced frames and render new frames for momentum update in the next step. In this manner, we can iteratively recover a 3D scene, avoiding the limitation of video length. Extensive experiments demonstrate the generalization capability and superior performance of our method in high-fidelity and consistent scene generation."
      },
      {
        "id": "oai:arXiv.org:2504.02768v1",
        "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs",
        "link": "https://arxiv.org/abs/2504.02768",
        "author": "Jaap Jumelet, Leonie Weissweiler, Arianna Bisazza",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02768v1 Announce Type: new \nAbstract: We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages, 6 linguistic phenomena and containing more than 125,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages."
      },
      {
        "id": "oai:arXiv.org:2504.02775v1",
        "title": "TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly Detection",
        "link": "https://arxiv.org/abs/2504.02775",
        "author": "Yoon Gyo Jung, Jaewoo Park, Jaeho Yoon, Kuan-Chuan Peng, Wonchul Kim, Andrew Beng Jin Teoh, Octavia Camps",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02775v1 Announce Type: new \nAbstract: We aim to solve unsupervised anomaly detection in a practical challenging environment where the normal dataset is both contaminated with defective regions and its product class distribution is tailed but unknown. We observe that existing models suffer from tail-versus-noise trade-off where if a model is robust against pixel noise, then its performance deteriorates on tail class samples, and vice versa. To mitigate the issue, we handle the tail class and noise samples independently. To this end, we propose TailSampler, a novel class size predictor that estimates the class cardinality of samples based on a symmetric assumption on the class-wise distribution of embedding similarities. TailSampler can be utilized to sample the tail class samples exclusively, allowing to handle them separately. Based on these facets, we build a memory-based anomaly detection model TailedCore, whose memory both well captures tail class information and is noise-robust. We extensively validate the effectiveness of TailedCore on the unsupervised long-tail noisy anomaly detection setting, and show that TailedCore outperforms the state-of-the-art in most settings."
      },
      {
        "id": "oai:arXiv.org:2504.02778v1",
        "title": "Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition",
        "link": "https://arxiv.org/abs/2504.02778",
        "author": "Vincent Gbouna Zakka, Luis J. Manso, Zhuangzhuang Dai",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02778v1 Announce Type: new \nAbstract: Human activity recognition is increasingly vital for supporting independent living, particularly for the elderly and those in need of assistance. Domestic service robots with monitoring capabilities can enhance safety and provide essential support. Although image-based methods have advanced considerably in the past decade, their adoption remains limited by concerns over privacy and sensitivity to low-light or dark conditions. As an alternative, millimetre-wave (mmWave) radar can produce point cloud data which is privacy-preserving. However, processing the sparse and noisy point clouds remains a long-standing challenge. While graph-based methods and attention mechanisms show promise, they predominantly rely on \"fixed\" kernels; kernels that are applied uniformly across all neighbourhoods, highlighting the need for adaptive approaches that can dynamically adjust their kernels to the specific geometry of each local neighbourhood in point cloud data. To overcome this limitation, we introduce an adaptive approach within the graph convolutional framework. Instead of a single shared weight function, our Multi-Head Adaptive Kernel (MAK) module generates multiple dynamic kernels, each capturing different aspects of the local feature space. By progressively refining local features while maintaining global spatial context, our method enables convolution kernels to adapt to varying local features. Experimental results on benchmark datasets confirm the effectiveness of our approach, achieving state-of-the-art performance in human activity recognition. Our source code is made publicly available at: https://github.com/Gbouna/MAK-GCN"
      },
      {
        "id": "oai:arXiv.org:2504.02781v1",
        "title": "Towards Green AI-Native Networks: Evaluation of Neural Circuit Policy for Estimating Energy Consumption of Base Stations",
        "link": "https://arxiv.org/abs/2504.02781",
        "author": "Selim Ickin, Shruti Bothe, Aman Raparia, Nitin Khanna, Erik Sanders",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02781v1 Announce Type: new \nAbstract: Optimization of radio hardware and AI-based network management software yield significant energy savings in radio access networks. The execution of underlying Machine Learning (ML) models, which enable energy savings through recommended actions, may require additional compute and energy, highlighting the opportunity to explore and adopt accurate and energy-efficient ML technologies. This work evaluates the novel use of sparsely structured Neural Circuit Policies (NCPs) in a use case to estimate the energy consumption of base stations. Sparsity in ML models yields reduced memory, computation and energy demand, hence facilitating a low-cost and scalable solution. We also evaluate the generalization capability of NCPs in comparison to traditional and widely used ML models such as Long Short Term Memory (LSTM), via quantifying their sensitivity to varying model hyper-parameters (HPs). NCPs demonstrated a clear reduction in computational overhead and energy consumption. Moreover, results indicated that the NCPs are robust to varying HPs such as number of epochs and neurons in each layer, making them a suitable option to ease model management and to reduce energy consumption in Machine Learning Operations (MLOps) in telecommunications."
      },
      {
        "id": "oai:arXiv.org:2504.02782v1",
        "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation",
        "link": "https://arxiv.org/abs/2504.02782",
        "author": "Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, Li Yuan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02782v1 Announce Type: new \nAbstract: The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval."
      },
      {
        "id": "oai:arXiv.org:2504.02789v1",
        "title": "A Framework for Robust Cognitive Evaluation of LLMs",
        "link": "https://arxiv.org/abs/2504.02789",
        "author": "Karin de Langis, Jong Inn Park, Bin Hu, Khanh Chi Le, Andreas Schramm, Michael C. Mensink, Andrew Elfenbein, Dongyeop Kang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02789v1 Announce Type: new \nAbstract: Emergent cognitive abilities in large language models (LLMs) have been widely observed, but their nature and underlying mechanisms remain poorly understood. A growing body of research draws on cognitive science to investigate LLM cognition, but standard methodologies and experimen-tal pipelines have not yet been established. To address this gap we develop CognitivEval, a framework for systematically evaluating the artificial cognitive capabilities of LLMs, with a particular emphasis on robustness in response collection. The key features of CognitivEval include: (i) automatic prompt permutations, and (ii) testing that gathers both generations and model probability estimates. Our experiments demonstrate that these features lead to more robust experimental outcomes. Using CognitivEval, we replicate five classic experiments in cognitive science, illustrating the framework's generalizability across various experimental tasks and obtaining a cognitive profile of several state of the art LLMs. CognitivEval will be released publicly to foster broader collaboration within the cognitive science community."
      },
      {
        "id": "oai:arXiv.org:2504.02797v1",
        "title": "Spline-based Transformers",
        "link": "https://arxiv.org/abs/2504.02797",
        "author": "Prashanth Chandran, Agon Serifi, Markus Gross, Moritz B\\\"acher",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02797v1 Announce Type: new \nAbstract: We introduce Spline-based Transformers, a novel class of Transformer models that eliminate the need for positional encoding. Inspired by workflows using splines in computer animation, our Spline-based Transformers embed an input sequence of elements as a smooth trajectory in latent space. Overcoming drawbacks of positional encoding such as sequence length extrapolation, Spline-based Transformers also provide a novel way for users to interact with transformer latent spaces by directly manipulating the latent control points to create new latent trajectories and sequences. We demonstrate the superior performance of our approach in comparison to conventional positional encoding on a variety of datasets, ranging from synthetic 2D to large-scale real-world datasets of images, 3D shapes, and animations."
      },
      {
        "id": "oai:arXiv.org:2504.02799v1",
        "title": "Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence",
        "link": "https://arxiv.org/abs/2504.02799",
        "author": "Anita Rau, Mark Endo, Josiah Aklilu, Jaewoo Heo, Khaled Saab, Alberto Paderno, Jeffrey Jopling, F. Christopher Holsinger, Serena Yeung-Levy",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02799v1 Announce Type: new \nAbstract: Large Vision-Language Models offer a new paradigm for AI-driven image understanding, enabling models to perform tasks without task-specific training. This flexibility holds particular promise across medicine, where expert-annotated data is scarce. Yet, VLMs' practical utility in intervention-focused domains--especially surgery, where decision-making is subjective and clinical scenarios are variable--remains uncertain. Here, we present a comprehensive analysis of 11 state-of-the-art VLMs across 17 key visual understanding tasks in surgical AI--from anatomy recognition to skill assessment--using 13 datasets spanning laparoscopic, robotic, and open procedures. In our experiments, VLMs demonstrate promising generalizability, at times outperforming supervised models when deployed outside their training setting. In-context learning, incorporating examples during testing, boosted performance up to three-fold, suggesting adaptability as a key strength. Still, tasks requiring spatial or temporal reasoning remained difficult. Beyond surgery, our findings offer insights into VLMs' potential for tackling complex and dynamic scenarios in clinical and broader real-world applications."
      },
      {
        "id": "oai:arXiv.org:2504.02800v1",
        "title": "A Survey of Large Language Models in Mental Health Disorder Detection on Social Media",
        "link": "https://arxiv.org/abs/2504.02800",
        "author": "Zhuohan Ge (The Hong Kong Polytechnic University), Nicole Hu (The Chinese University of Hong Kong), Darian Li (The Hong Kong Polytechnic University), Yubo Wang (Hong Kong University of Science and Technology), Shihao Qi (The Hong Kong Polytechnic University), Yuming Xu (The Hong Kong Polytechnic University), Han Shi (Hong Kong University of Science and Technology), Jason Zhang (The Hong Kong Polytechnic University)",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02800v1 Announce Type: new \nAbstract: The detection and intervention of mental health issues represent a critical global research focus, and social media data has been recognized as an important resource for mental health research. However, how to utilize Large Language Models (LLMs) for mental health problem detection on social media poses significant challenges. Hence, this paper aims to explore the potential of LLM applications in social media data analysis, focusing not only on the most common psychological disorders such as depression and anxiety but also incorporating psychotic disorders and externalizing disorders, summarizing the application methods of LLM from different dimensions, such as text data analysis and detection of mental disorders, and revealing the major challenges and shortcomings of current research. In addition, the paper provides an overview of popular datasets, and evaluation metrics. The survey in this paper provides a comprehensive frame of reference for researchers in the field of mental health, while demonstrating the great potential of LLMs in mental health detection to facilitate the further application of LLMs in future mental health interventions."
      },
      {
        "id": "oai:arXiv.org:2504.02801v1",
        "title": "F-ViTA: Foundation Model Guided Visible to Thermal Translation",
        "link": "https://arxiv.org/abs/2504.02801",
        "author": "Jay N. Paranjape, Celso de Melo, Vishal M. Patel",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02801v1 Announce Type: new \nAbstract: Thermal imaging is crucial for scene understanding, particularly in low-light and nighttime conditions. However, collecting large thermal datasets is costly and labor-intensive due to the specialized equipment required for infrared image capture. To address this challenge, researchers have explored visible-to-thermal image translation. Most existing methods rely on Generative Adversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a style transfer problem. As a result, these approaches attempt to learn both the modality distribution shift and underlying physical principles from limited training data. In this paper, we propose F-ViTA, a novel approach that leverages the general world knowledge embedded in foundation models to guide the diffusion process for improved translation. Specifically, we condition an InstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation models such as SAM and Grounded DINO. This allows the model to learn meaningful correlations between scene objects and their thermal signatures in infrared imagery. Extensive experiments on five public datasets demonstrate that F-ViTA outperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes well to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared (LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the same visible image. Code: https://github.com/JayParanjape/F-ViTA/tree/master."
      },
      {
        "id": "oai:arXiv.org:2504.02807v1",
        "title": "MegaMath: Pushing the Limits of Open Math Corpora",
        "link": "https://arxiv.org/abs/2504.02807",
        "author": "Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, Eric P. Xing",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02807v1 Announce Type: new \nAbstract: Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets."
      },
      {
        "id": "oai:arXiv.org:2504.02810v1",
        "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
        "link": "https://arxiv.org/abs/2504.02810",
        "author": "Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02810v1 Announce Type: new \nAbstract: With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities."
      },
      {
        "id": "oai:arXiv.org:2504.02812v1",
        "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation",
        "link": "https://arxiv.org/abs/2504.02812",
        "author": "Van Nguyen Nguyen, Stephen Tyree, Andrew Guo, Mederic Fourmy, Anas Gouda, Taeyeop Lee, Sungphill Moon, Hyeontae Son, Lukas Ranftl, Jonathan Tremblay, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Stan Birchfield, Jiri Matas, Yann Labbe, Martin Sundermeyer, Tomas Hodan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02812v1 Announce Type: new \nAbstract: We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/"
      },
      {
        "id": "oai:arXiv.org:2504.02817v1",
        "title": "Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization",
        "link": "https://arxiv.org/abs/2504.02817",
        "author": "Kangle Deng, Hsueh-Ti Derek Liu, Yiheng Zhu, Xiaoxia Sun, Chong Shang, Kiran Bhat, Deva Ramanan, Jun-Yan Zhu, Maneesh Agrawala, Tinghui Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02817v1 Announce Type: new \nAbstract: Many 3D generative models rely on variational autoencoders (VAEs) to learn compact shape representations. However, existing methods encode all shapes into a fixed-size token, disregarding the inherent variations in scale and complexity across 3D data. This leads to inefficient latent representations that can compromise downstream generation. We address this challenge by introducing Octree-based Adaptive Tokenization, a novel framework that adjusts the dimension of latent representations according to shape complexity. Our approach constructs an adaptive octree structure guided by a quadric-error-based subdivision criterion and allocates a shape latent vector to each octree cell using a query-based transformer. Building upon this tokenization, we develop an octree-based autoregressive generative model that effectively leverages these variable-sized representations in shape generation. Extensive experiments demonstrate that our approach reduces token counts by 50% compared to fixed-size methods while maintaining comparable visual quality. When using a similar token length, our method produces significantly higher-quality shapes. When incorporated with our downstream generative model, our method creates more detailed and diverse 3D content than existing approaches."
      },
      {
        "id": "oai:arXiv.org:2504.02819v1",
        "title": "GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings",
        "link": "https://arxiv.org/abs/2504.02819",
        "author": "Yuexi Du, Jiazhen Zhang, Nicha C. Dvornek, John A. Onofrey",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02819v1 Announce Type: new \nAbstract: Symmetry, where certain features remain invariant under geometric transformations, can often serve as a powerful prior in designing convolutional neural networks (CNNs). While conventional CNNs inherently support translational equivariance, extending this property to rotation and reflection has proven challenging, often forcing a compromise between equivariance, efficiency, and information loss. In this work, we introduce Gaussian Mixture Ring Convolution (GMR-Conv), an efficient convolution kernel that smooths radial symmetry using a mixture of Gaussian-weighted rings. This design mitigates discretization errors of circular kernels, thereby preserving robust rotation and reflection equivariance without incurring computational overhead. We further optimize both the space and speed efficiency of GMR-Conv via a novel parameterization and computation strategy, allowing larger kernels at an acceptable cost. Extensive experiments on eight classification and one segmentation datasets demonstrate that GMR-Conv not only matches conventional CNNs' performance but can also surpass it in applications with orientation-less data. GMR-Conv is also proven to be more robust and efficient than the state-of-the-art equivariant learning methods. Our work provides inspiring empirical evidence that carefully applied radial symmetry can alleviate the challenges of information loss, marking a promising advance in equivariant network architectures. The code is available at https://github.com/XYPB/GMR-Conv."
      },
      {
        "id": "oai:arXiv.org:2504.02821v1",
        "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models",
        "link": "https://arxiv.org/abs/2504.02821",
        "author": "Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02821v1 Announce Type: new \nAbstract: Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs."
      },
      {
        "id": "oai:arXiv.org:2504.02823v1",
        "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection",
        "link": "https://arxiv.org/abs/2504.02823",
        "author": "Divya Velayudhan, Abdelfatah Ahmed, Mohamad Alansari, Neha Gour, Abderaouf Behouch, Taimur Hassan, Syed Talal Wasim, Nabil Maalej, Muzammal Naseer, Juergen Gall, Mohammed Bennamoun, Ernesto Damiani, Naoufel Werghi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02823v1 Announce Type: new \nAbstract: Advancements in Computer-Aided Screening (CAS) systems are essential for improving the detection of security threats in X-ray baggage scans. However, current datasets are limited in representing real-world, sophisticated threats and concealment tactics, and existing approaches are constrained by a closed-set paradigm with predefined labels. To address these challenges, we introduce STCray, the first multimodal X-ray baggage security dataset, comprising 46,642 image-caption paired scans across 21 threat categories, generated using an X-ray scanner for airport security. STCray is meticulously developed with our specialized protocol that ensures domain-aware, coherent captions, that lead to the multi-modal instruction following data in X-ray baggage security. This allows us to train a domain-aware visual AI assistant named STING-BEE that supports a range of vision-language tasks, including scene comprehension, referring threat localization, visual grounding, and visual question answering (VQA), establishing novel baselines for multi-modal learning in X-ray baggage security. Further, STING-BEE shows state-of-the-art generalization in cross-domain settings. Code, data, and models are available at https://divs1159.github.io/STING-BEE/."
      },
      {
        "id": "oai:arXiv.org:2504.02826v1",
        "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing",
        "link": "https://arxiv.org/abs/2504.02826",
        "author": "Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, Haodong Duan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02826v1 Announce Type: new \nAbstract: Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench."
      },
      {
        "id": "oai:arXiv.org:2504.02827v1",
        "title": "On Vanishing Variance in Transformer Length Generalization",
        "link": "https://arxiv.org/abs/2504.02827",
        "author": "Ruining Li (Jinghao), Gabrijel Boduljak (Jinghao),  Jensen (Jinghao),  Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02827v1 Announce Type: new \nAbstract: It is a widely known issue that Transformers, when trained on shorter sequences, fail to generalize robustly to longer ones at test time. This raises the question of whether Transformer models are real reasoning engines, despite their impressive abilities in mathematical problem solving and code synthesis. In this paper, we offer a vanishing variance perspective on this issue. To the best of our knowledge, we are the first to demonstrate that even for today's frontier models, a longer sequence length results in a decrease in variance in the output of the multi-head attention modules. On the argmax retrieval and dictionary lookup tasks, our experiments show that applying layer normalization after the attention outputs leads to significantly better length generalization. Our analyses attribute this improvement to a reduction-though not a complete elimination-of the distribution shift caused by vanishing variance."
      },
      {
        "id": "oai:arXiv.org:2504.02828v1",
        "title": "Concept Lancet: Image Editing with Compositional Representation Transplant",
        "link": "https://arxiv.org/abs/2504.02828",
        "author": "Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Hancheng Min, Chris Callison-Burch, Ren\\'e Vidal",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02828v1 Announce Type: new \nAbstract: Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation."
      },
      {
        "id": "oai:arXiv.org:2407.06951v1",
        "title": "RoboCAS: A Benchmark for Robotic Manipulation in Complex Object Arrangement Scenarios",
        "link": "https://arxiv.org/abs/2407.06951",
        "author": "Liming Zheng, Feng Yan, Fanfan Liu, Chengjian Feng, Zhuoliang Kang, Lin Ma",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.06951v1 Announce Type: cross \nAbstract: Foundation models hold significant potential for enabling robots to perform long-horizon general manipulation tasks. However, the simplicity of tasks and the uniformity of environments in existing benchmarks restrict their effective deployment in complex scenarios. To address this limitation, this paper introduces the \\textit{RoboCAS} benchmark, the first benchmark specifically designed for complex object arrangement scenarios in robotic manipulation. This benchmark employs flexible and concise scripted policies to efficiently collect a diverse array of demonstrations, showcasing scattered, orderly, and stacked object arrangements within a highly realistic physical simulation environment. It includes complex processes such as target retrieval, obstacle clearance, and robot manipulation, testing agents' abilities to perform long-horizon planning for spatial reasoning and predicting chain reactions under ambiguous instructions. Extensive experiments on multiple baseline models reveal their limitations in managing complex object arrangement scenarios, underscoring the urgent need for intelligent agents capable of performing long-horizon operations in practical deployments and providing valuable insights for future research directions. Project website: \\url{https://github.com/notFoundThisPerson/RoboCAS-v0}."
      },
      {
        "id": "oai:arXiv.org:2504.01963v1",
        "title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
        "link": "https://arxiv.org/abs/2504.01963",
        "author": "R. M. Aratchige, W. M. K. S. Ilmini",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01963v1 Announce Type: cross \nAbstract: This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience."
      },
      {
        "id": "oai:arXiv.org:2504.01964v1",
        "title": "What Can 240,000 New Credit Transactions Tell Us About the Impact of NGEU Funds?",
        "link": "https://arxiv.org/abs/2504.01964",
        "author": "Alvaro Ortiz, Tomasa Rodrigo, David Sarasa, Pedro Torinos, Sirenia Vazquez",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01964v1 Announce Type: cross \nAbstract: Using a panel data local projections model and controlling for firm characteristics, procurement bid attributes, and macroeconomic conditions, the study estimates the dynamic effects of procurement awards on new lending, a more precise measure than the change in the stock of credit. The analysis further examines heterogeneity in credit responses based on firm size, industry, credit maturity, and value chain position of the firms. The empirical evidence confirms that public procurement awards significantly increase new lending, with NGEU-funded contracts generating stronger credit expansion than traditional procurement during the recent period. The results show that the impact of NGEU procurement programs aligns closely with historical procurement impacts, with differences driven mainly by lower utilization rates. Moreover, integrating high-frequency financial data with procurement records highlights the potential of Big Data in refining public policy design."
      },
      {
        "id": "oai:arXiv.org:2504.01970v1",
        "title": "Differentiable Optimization for Deep Learning-Enhanced DC Approximation of AC Optimal Power Flow",
        "link": "https://arxiv.org/abs/2504.01970",
        "author": "Andrew Rosemberg, Michael Klamkin",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01970v1 Announce Type: cross \nAbstract: The growing scale of power systems and the increasing uncertainty introduced by renewable energy sources necessitates novel optimization techniques that are significantly faster and more accurate than existing methods. The AC Optimal Power Flow (AC-OPF) problem, a core component of power grid optimization, is often approximated using linearized DC Optimal Power Flow (DC-OPF) models for computational tractability, albeit at the cost of suboptimal and inefficient decisions. To address these limitations, we propose a novel deep learning-based framework for network equivalency that enhances DC-OPF to more closely mimic the behavior of AC-OPF. The approach utilizes recent advances in differentiable optimization, incorporating a neural network trained to predict adjusted nodal shunt conductances and branch susceptances in order to account for nonlinear power flow behavior. The model can be trained end-to-end using modern deep learning frameworks by leveraging the implicit function theorem. Results demonstrate the framework's ability to significantly improve prediction accuracy, paving the way for more reliable and efficient power systems."
      },
      {
        "id": "oai:arXiv.org:2504.01987v1",
        "title": "CaLiV: LiDAR-to-Vehicle Calibration of Arbitrary Sensor Setups via Object Reconstruction",
        "link": "https://arxiv.org/abs/2504.01987",
        "author": "Ilir Tahiraj, Markus Edinger, Dominik Kulmer, Markus Lienkamp",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01987v1 Announce Type: cross \nAbstract: In autonomous systems, sensor calibration is essential for a safe and efficient navigation in dynamic environments. Accurate calibration is a prerequisite for reliable perception and planning tasks such as object detection and obstacle avoidance. Many existing LiDAR calibration methods require overlapping fields of view, while others use external sensing devices or postulate a feature-rich environment. In addition, Sensor-to-Vehicle calibration is not supported by the vast majority of calibration algorithms. In this work, we propose a novel target-based technique for extrinsic Sensor-to-Sensor and Sensor-to-Vehicle calibration of multi-LiDAR systems called CaLiV. This algorithm works for non-overlapping FoVs, as well as arbitrary calibration targets, and does not require any external sensing devices. First, we apply motion to produce FoV overlaps and utilize a simple unscented Kalman filter to obtain vehicle poses. Then, we use the Gaussian mixture model-based registration framework GMMCalib to align the point clouds in a common calibration frame. Finally, we reduce the task of recovering the sensor extrinsics to a minimization problem. We show that both translational and rotational Sensor-to-Sensor errors can be solved accurately by our method. In addition, all Sensor-to-Vehicle rotation angles can also be calibrated with high accuracy. We validate the simulation results in real-world experiments. The code is open source and available on https://github.com/TUMFTM/CaLiV."
      },
      {
        "id": "oai:arXiv.org:2504.01988v1",
        "title": "Distance Estimation to Support Assistive Drones for the Visually Impaired using Robust Calibration",
        "link": "https://arxiv.org/abs/2504.01988",
        "author": "Suman Raj, Bhavani A Madhabhavi, Madhav Kumar, Prabhav Gupta, Yogesh Simmhan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01988v1 Announce Type: cross \nAbstract: Autonomous navigation by drones using onboard sensors, combined with deep learning and computer vision algorithms, is impacting a number of domains. We examine the use of drones to autonomously assist Visually Impaired People (VIPs) in navigating outdoor environments while avoiding obstacles. Here, we present NOVA, a robust calibration technique using depth maps to estimate absolute distances to obstacles in a campus environment. NOVA uses a dynamic-update method that can adapt to adversarial scenarios. We compare NOVA with SOTA depth map approaches, and with geometric and regression-based baseline models, for distance estimation to VIPs and other obstacles in diverse and dynamic conditions. We also provide exhaustive evaluations to validate the robustness and generalizability of our methods. NOVA predicts distances to VIP with an error <30cm and to different obstacles like cars and bicycles with a maximum of 60cm error, which are better than the baselines. NOVA also clearly out-performs SOTA depth map methods, by upto 5.3-14.6x."
      },
      {
        "id": "oai:arXiv.org:2504.01989v1",
        "title": "A Concise Survey on Lane Topology Reasoning for HD Mapping",
        "link": "https://arxiv.org/abs/2504.01989",
        "author": "Yi Yao, Miao Fan, Shengtong Xu, Haoyi Xiong, Xiangzeng Liu, Wenbo Hu, Wenbing Huang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01989v1 Announce Type: cross \nAbstract: Lane topology reasoning techniques play a crucial role in high-definition (HD) mapping and autonomous driving applications. While recent years have witnessed significant advances in this field, there has been limited effort to consolidate these works into a comprehensive overview. This survey systematically reviews the evolution and current state of lane topology reasoning methods, categorizing them into three major paradigms: procedural modeling-based methods, aerial imagery-based methods, and onboard sensors-based methods. We analyze the progression from early rule-based approaches to modern learning-based solutions utilizing transformers, graph neural networks (GNNs), and other deep learning architectures. The paper examines standardized evaluation metrics, including road-level measures (APLS and TLTS score), and lane-level metrics (DET and TOP score), along with performance comparisons on benchmark datasets such as OpenLane-V2. We identify key technical challenges, including dataset availability and model efficiency, and outline promising directions for future research. This comprehensive review provides researchers and practitioners with insights into the theoretical frameworks, practical implementations, and emerging trends in lane topology reasoning for HD mapping applications."
      },
      {
        "id": "oai:arXiv.org:2504.01995v1",
        "title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
        "link": "https://arxiv.org/abs/2504.01995",
        "author": "Hamed Mahdavi, Alireza Hashemi, Majid Daliri, Pegah Mohammadipour, Alireza Farhadi, Samira Malek, Yekta Yazdanifard, Amir Khasahmadi, Vasant Honavar",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01995v1 Announce Type: cross \nAbstract: Recent advancements in large language models (LLMs) have shown impressive progress in mathematical reasoning tasks. However, current evaluation benchmarks predominantly focus on the accuracy of final answers, often overlooking the logical rigor crucial for mathematical problem-solving. The claim that state-of-the-art LLMs can solve Math Olympiad-level problems requires closer examination. To explore this, we conducted both qualitative and quantitative human evaluations of proofs generated by LLMs, and developed a schema for automatically assessing their reasoning capabilities. Our study reveals that current LLMs fall significantly short of solving challenging Olympiad-level problems and frequently fail to distinguish correct mathematical reasoning from clearly flawed solutions. We also found that occasional correct final answers provided by LLMs often result from pattern recognition or heuristic shortcuts rather than genuine mathematical reasoning. These findings underscore the substantial gap between LLM performance and human expertise in advanced mathematical reasoning and highlight the importance of developing benchmarks that prioritize the rigor and coherence of mathematical arguments rather than merely the correctness of final answers."
      },
      {
        "id": "oai:arXiv.org:2504.01996v1",
        "title": "Real-Time Navigation for Autonomous Aerial Vehicles Using Video",
        "link": "https://arxiv.org/abs/2504.01996",
        "author": "Khizar Anjum, Parul Pandey, Vidyasagar Sadhu, Roberto Tron, Dario Pompili",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01996v1 Announce Type: cross \nAbstract: Most applications in autonomous navigation using mounted cameras rely on the construction and processing of geometric 3D point clouds, which is an expensive process. However, there is another simpler way to make a space navigable quickly: to use semantic information (e.g., traffic signs) to guide the agent. However, detecting and acting on semantic information involves Computer Vision~(CV) algorithms such as object detection, which themselves are demanding for agents such as aerial drones with limited onboard resources. To solve this problem, we introduce a novel Markov Decision Process~(MDP) framework to reduce the workload of these CV approaches. We apply our proposed framework to both feature-based and neural-network-based object-detection tasks, using open-loop and closed-loop simulations as well as hardware-in-the-loop emulations. These holistic tests show significant benefits in energy consumption and speed with only a limited loss in accuracy compared to models based on static features and neural networks."
      },
      {
        "id": "oai:arXiv.org:2504.02009v1",
        "title": "Urban Computing in the Era of Large Language Models",
        "link": "https://arxiv.org/abs/2504.02009",
        "author": "Zhonghang Li, Lianghao Xia, Xubin Ren, Jiabin Tang, Tianyi Chen, Yong Xu, Chao Huang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02009v1 Announce Type: cross \nAbstract: Urban computing has emerged as a multidisciplinary field that harnesses data-driven technologies to address challenges and improve urban living. Traditional approaches, while beneficial, often face challenges with generalization, scalability, and contextual understanding. The advent of Large Language Models (LLMs) offers transformative potential in this domain. This survey explores the intersection of LLMs and urban computing, emphasizing the impact of LLMs in processing and analyzing urban data, enhancing decision-making, and fostering citizen engagement. We provide a concise overview of the evolution and core technologies of LLMs. Additionally, we survey their applications across key urban domains, such as transportation, public safety, and environmental monitoring, summarizing essential tasks and prior works in various urban contexts, while highlighting LLMs' functional roles and implementation patterns. Building on this, we propose potential LLM-based solutions to address unresolved challenges. To facilitate in-depth research, we compile a list of available datasets and tools applicable to diverse urban scenarios. Finally, we discuss the limitations of current approaches and outline future directions for advancing LLMs in urban computing."
      },
      {
        "id": "oai:arXiv.org:2504.02014v1",
        "title": "HCAF-DTA: drug-target binding affinity prediction with cross-attention fused hypergraph neural networks",
        "link": "https://arxiv.org/abs/2504.02014",
        "author": "Jiannuo Li, Lan Yao",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02014v1 Announce Type: cross \nAbstract: Accurate prediction of the binding affinity between drugs and target proteins is a core task in computer-aided drug design. Existing deep learning methods tend to ignore the information of internal sub-structural features of drug molecules and drug-target interactions, resulting in limited prediction performance. In this paper, we propose a drug-target association prediction model HCAF-DTA based on cross-attention fusion hypergraph neural network. The model innovatively introduces hypergraph representation in the feature extraction stage: drug molecule hypergraphs are constructed based on the tree decomposition algorithm, and the sub-structural and global features extracted by fusing the hypergraph neural network with the graphical neural network through hopping connections, in which the hyper edges can efficiently characterise the functional functional groups and other key chemical features; for the protein feature extraction, a weighted graph is constructed based on the residues predicted by the ESM model contact maps to construct weighted graphs, and multilayer graph neural networks were used to capture spatial dependencies. In the prediction stage, a bidirectional multi-head cross-attention mechanism is designed to model intermolecular interactions from the dual viewpoints of atoms and amino acids, and cross-modal features with correlated information are fused by attention. Experiments on benchmark datasets such as Davis and KIBA show that HCAF-DTA outperforms state of the arts in all three performance evaluation metrics, with the MSE metrics reaching 0.198 and 0.122, respectively, with an improvement of up to 4% from the optimal baseline."
      },
      {
        "id": "oai:arXiv.org:2504.02045v1",
        "title": "WorldPrompter: Traversable Text-to-Scene Generation",
        "link": "https://arxiv.org/abs/2504.02045",
        "author": "Zhaoyang Zhang, Yannick Hold-Geoffroy, Milo\\v{s} Ha\\v{s}an, Chen Ziwen, Fujun Luan, Julie Dorsey, Yiwei Hu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02045v1 Announce Type: cross \nAbstract: Scene-level 3D generation is a challenging research topic, with most existing methods generating only partial scenes and offering limited navigational freedom. We introduce WorldPrompter, a novel generative pipeline for synthesizing traversable 3D scenes from text prompts. We leverage panoramic videos as an intermediate representation to model the 360{\\deg} details of a scene. WorldPrompter incorporates a conditional 360{\\deg} panoramic video generator, capable of producing a 128-frame video that simulates a person walking through and capturing a virtual environment. The resulting video is then reconstructed as Gaussian splats by a fast feedforward 3D reconstructor, enabling a true walkable experience within the 3D scene. Experiments demonstrate that our panoramic video generation model achieves convincing view consistency across frames, enabling high-quality panoramic Gaussian splat reconstruction and facilitating traversal over an area of the scene. Qualitative and quantitative results also show it outperforms the state-of-the-art 360{\\deg} video generators and 3D scene generation models."
      },
      {
        "id": "oai:arXiv.org:2504.02051v1",
        "title": "Self-Resource Allocation in Multi-Agent LLM Systems",
        "link": "https://arxiv.org/abs/2504.02051",
        "author": "Alfonso Amayuelas, Jingbo Yang, Saaket Agashe, Ashwin Nagarajan, Antonis Antoniades, Xin Eric Wang, William Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02051v1 Announce Type: cross \nAbstract: With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers."
      },
      {
        "id": "oai:arXiv.org:2504.02069v1",
        "title": "RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics",
        "link": "https://arxiv.org/abs/2504.02069",
        "author": "Zhiyuan Zhang, Yuxin He, Yong Sun, Junyu Shi, Lijiang Liu, Qiang Nie",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02069v1 Announce Type: cross \nAbstract: Visual Language Models (VLMs) have emerged as pivotal tools for robotic systems, enabling cross-task generalization, dynamic environmental interaction, and long-horizon planning through multimodal perception and semantic reasoning. However, existing open-source VLMs predominantly trained for generic vision-language alignment tasks fail to model temporally correlated action semantics that are crucial for robotic manipulation effectively. While current image-based fine-tuning methods partially adapt VLMs to robotic applications, they fundamentally disregard temporal evolution patterns in video sequences and suffer from visual feature entanglement between robotic agents, manipulated objects, and environmental contexts, thereby limiting semantic decoupling capability for atomic actions and compromising model generalizability.To overcome these challenges, this work presents RoboAct-CLIP with dual technical contributions: 1) A dataset reconstruction framework that performs semantic-constrained action unit segmentation and re-annotation on open-source robotic videos, constructing purified training sets containing singular atomic actions (e.g., \"grasp\"); 2) A temporal-decoupling fine-tuning strategy based on Contrastive Language-Image Pretraining (CLIP) architecture, which disentangles temporal action features across video frames from object-centric characteristics to achieve hierarchical representation learning of robotic atomic actions.Experimental results in simulated environments demonstrate that the RoboAct-CLIP pretrained model achieves a 12% higher success rate than baseline VLMs, along with superior generalization in multi-object manipulation tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02084v1",
        "title": "Evaluation of Flight Parameters in UAV-based 3D Reconstruction for Rooftop Infrastructure Assessment",
        "link": "https://arxiv.org/abs/2504.02084",
        "author": "Nick Chodura, Melissa Greeff, Joshua Woods",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02084v1 Announce Type: cross \nAbstract: Rooftop 3D reconstruction using UAV-based photogrammetry offers a promising solution for infrastructure assessment, but existing methods often require high percentages of image overlap and extended flight times to ensure model accuracy when using autonomous flight paths. This study systematically evaluates key flight parameters-ground sampling distance (GSD) and image overlap-to optimize the 3D reconstruction of complex rooftop infrastructure. Controlled UAV flights were conducted over a multi-segment rooftop at Queen's University using a DJI Phantom 4 Pro V2, with varied GSD and overlap settings. The collected data were processed using Reality Capture software and evaluated against ground truth models generated from UAV-based LiDAR and terrestrial laser scanning (TLS). Experimental results indicate that a GSD range of 0.75-1.26 cm combined with 85% image overlap achieves a high degree of model accuracy, while minimizing images collected and flight time. These findings provide guidance for planning autonomous UAV flight paths for efficient rooftop assessments."
      },
      {
        "id": "oai:arXiv.org:2504.02111v1",
        "title": "Exploring LLM Reasoning Through Controlled Prompt Variations",
        "link": "https://arxiv.org/abs/2504.02111",
        "author": "Giannis Chatziveroglou, Richard Yun, Maura Kelleher",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02111v1 Announce Type: cross \nAbstract: This study investigates the reasoning robustness of large language models (LLMs) on mathematical problem-solving tasks under systematically introduced input perturbations. Using the GSM8K dataset as a controlled testbed, we evaluate how well state-of-the-art models maintain logical consistency and correctness when confronted with four categories of prompt perturbations: irrelevant context, pathological instructions, factually relevant but non-essential context, and a combination of the latter two. Our experiments, conducted on thirteen open-source and closed-source LLMs, reveal that introducing irrelevant context within the model's context window significantly degrades performance, suggesting that distinguishing essential from extraneous details remains a pressing challenge. Surprisingly, performance regressions are relatively insensitive to the complexity of the reasoning task, as measured by the number of steps required, and are not strictly correlated with model size. Moreover, we observe that certain perturbations inadvertently trigger chain-of-thought-like reasoning behaviors, even without explicit prompting. Our findings highlight critical vulnerabilities in current LLMs and underscore the need for improved robustness against noisy, misleading, and contextually dense inputs, paving the way for more resilient and reliable reasoning in real-world applications."
      },
      {
        "id": "oai:arXiv.org:2504.02114v1",
        "title": "On Model Protection in Federated Learning against Eavesdropping Attacks",
        "link": "https://arxiv.org/abs/2504.02114",
        "author": "Dipankar Maity, Kushal Chakrabarti",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02114v1 Announce Type: cross \nAbstract: In this study, we investigate the protection offered by federated learning algorithms against eavesdropping adversaries. In our model, the adversary is capable of intercepting model updates transmitted from clients to the server, enabling it to create its own estimate of the model. Unlike previous research, which predominantly focuses on safeguarding client data, our work shifts attention protecting the client model itself. Through a theoretical analysis, we examine how various factors, such as the probability of client selection, the structure of local objective functions, global aggregation at the server, and the eavesdropper's capabilities, impact the overall level of protection. We further validate our findings through numerical experiments, assessing the protection by evaluating the model accuracy achieved by the adversary. Finally, we compare our results with methods based on differential privacy, underscoring their limitations in this specific context."
      },
      {
        "id": "oai:arXiv.org:2504.02128v1",
        "title": "Achieving Unanimous Consensus in Decision Making Using Multi-Agents",
        "link": "https://arxiv.org/abs/2504.02128",
        "author": "Apurba Pokharel, Ram Dantu, Shakila Zaman, Sirisha Talapuru, Vinh Quach",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02128v1 Announce Type: cross \nAbstract: Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agreement based on honest majority or weighted consensus. This paper introduces a novel deliberation-based consensus mechanism where Large Language Models (LLMs) act as rational agents engaging in structured discussions to reach a unanimous consensus. By leveraging graded consensus and a multi-round deliberation process, our approach ensures both unanimous consensus for definitive problems and graded confidence for prioritized decisions and policies. We provide a formalization of our system and use it to show that the properties of blockchains: consistency, agreement, liveness, and determinism are maintained. Moreover, experimental results demonstrate our system's feasibility, showcasing how our deliberation method's convergence, block properties, and accuracy enable decision-making on blockchain networks. We also address key challenges with this novel approach such as degeneration of thoughts, hallucinations, malicious models and nodes, resource consumption, and scalability."
      },
      {
        "id": "oai:arXiv.org:2504.02134v1",
        "title": "Robust Channel Estimation for Optical Wireless Communications Using Neural Network",
        "link": "https://arxiv.org/abs/2504.02134",
        "author": "Dianxin Luan, John Thompson",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02134v1 Announce Type: cross \nAbstract: Optical Wireless Communication (OWC) has gained significant attention due to its high-speed data transmission and throughput. Optical wireless channels are often assumed to be flat, but we evaluate frequency selective channels to consider high data rate optical wireless or very dispersive environments. To address this for optical scenarios, this paper presents a robust channel estimation framework with low-complexity to mitigate frequency-selective effects, then to improve system reliability and performance. This channel estimation framework contains a neural network that can estimate general optical wireless channels without prior channel information about the environment. Based on this estimate and the corresponding delay spread, one of several candidate offline-trained neural networks will be activated to predict this channel. Simulation results demonstrate that the proposed method has improved and robust normalized mean square error (NMSE) and bit error rate (BER) performance compared to conventional estimation methods while maintaining computational efficiency. These findings highlight the potential of neural network solutions in enhancing the performance of OWC systems under indoor channel conditions."
      },
      {
        "id": "oai:arXiv.org:2504.02148v1",
        "title": "OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Joint LLM and GNN Modeling",
        "link": "https://arxiv.org/abs/2504.02148",
        "author": "Heming Zhang, Tim Xu, Dekang Cao, Shunning Liang, Lars Schimmelpfennig, Levi Kaster, Di Huang, Carlos Cruchaga, Guangfu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02148v1 Announce Type: cross \nAbstract: Complex cell signaling systems -- governed by varying protein abundances and interactions -- generate diverse cell types across organs. These systems evolve under influences such as age, sex, diet, environmental exposures, and diseases, making them challenging to decode given the involvement of tens of thousands of genes and proteins. Recently, hundreds of millions of single-cell omics data have provided a robust foundation for understanding these signaling networks within various cell subpopulations and conditions. Inspired by the success of large foundation models (for example, large language models and large vision models) pre-trained on massive datasets, we introduce OmniCellTOSG, the first dataset of cell text-omic signaling graphs (TOSGs). Each TOSG represents the signaling network of an individual or meta-cell and is labeled with information such as organ, disease, sex, age, and cell subtype. OmniCellTOSG offers two key contributions. First, it introduces a novel graph model that integrates human-readable annotations -- such as biological functions, cellular locations, signaling pathways, related diseases, and drugs -- with quantitative gene and protein abundance data, enabling graph reasoning to decode cell signaling. This approach calls for new joint models combining large language models and graph neural networks. Second, the dataset is built from single-cell RNA sequencing data of approximately 120 million cells from diverse tissues and conditions (healthy and diseased) and is fully compatible with PyTorch. This facilitates the development of innovative cell signaling models that could transform research in life sciences, healthcare, and precision medicine. The OmniCellTOSG dataset is continuously expanding and will be updated regularly. The dataset and code are available at https://github.com/FuhaiLiAiLab/OmniCellTOSG."
      },
      {
        "id": "oai:arXiv.org:2504.02161v1",
        "title": "Preference-Driven Active 3D Scene Representation for Robotic Inspection in Nuclear Decommissioning",
        "link": "https://arxiv.org/abs/2504.02161",
        "author": "Zhen Meng, Kan Chen, Xiangmin Xu, Erwin Jose Lopez Pulgarin, Emma Li, Philip G. Zhao, David Flynn",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02161v1 Announce Type: cross \nAbstract: Active 3D scene representation is pivotal in modern robotics applications, including remote inspection, manipulation, and telepresence. Traditional methods primarily optimize geometric fidelity or rendering accuracy, but often overlook operator-specific objectives, such as safety-critical coverage or task-driven viewpoints. This limitation leads to suboptimal viewpoint selection, particularly in constrained environments such as nuclear decommissioning. To bridge this gap, we introduce a novel framework that integrates expert operator preferences into the active 3D scene representation pipeline. Specifically, we employ Reinforcement Learning from Human Feedback (RLHF) to guide robotic path planning, reshaping the reward function based on expert input. To capture operator-specific priorities, we conduct interactive choice experiments that evaluate user preferences in 3D scene representation. We validate our framework using a UR3e robotic arm for reactor tile inspection in a nuclear decommissioning scenario. Compared to baseline methods, our approach enhances scene representation while optimizing trajectory efficiency. The RLHF-based policy consistently outperforms random selection, prioritizing task-critical details. By unifying explicit 3D geometric modeling with implicit human-in-the-loop optimization, this work establishes a foundation for adaptive, safety-critical robotic perception systems, paving the way for enhanced automation in nuclear decommissioning, remote maintenance, and other high-risk environments."
      },
      {
        "id": "oai:arXiv.org:2504.02167v1",
        "title": "HQCC: A Hybrid Quantum-Classical Classifier with Adaptive Structure",
        "link": "https://arxiv.org/abs/2504.02167",
        "author": "Ren-Xin Zhao, Xinze Tong, Shi Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02167v1 Announce Type: cross \nAbstract: Parameterized Quantum Circuits (PQCs) with fixed structures severely degrade the performance of Quantum Machine Learning (QML). To address this, a Hybrid Quantum-Classical Classifier (HQCC) is proposed. It opens a practical way to advance QML in the Noisy Intermediate-Scale Quantum (NISQ) era by adaptively optimizing the PQC through a Long Short-Term Memory (LSTM) driven dynamic circuit generator, utilizing a local quantum filter for scalable feature extraction, and exploiting architectural plasticity to balance the entanglement depth and noise robustness. We realize the HQCC on the TensorCircuit platform and run simulations on the MNIST and Fashion MNIST datasets, achieving up to 97.12\\% accuracy on MNIST and outperforming several alternative methods."
      },
      {
        "id": "oai:arXiv.org:2504.02170v1",
        "title": "Example-Free Learning of Regular Languages with Prefix Queries",
        "link": "https://arxiv.org/abs/2504.02170",
        "author": "Eve Fernando, Sasha Rubin, Rahul Gopinath",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02170v1 Announce Type: cross \nAbstract: Language learning refers to the problem of inferring a mathematical model which accurately represents a formal language. Many language learning algorithms learn by asking certain types of queries about the language being modeled. Language learning is of practical interest in the field of cybersecurity, where it is used to model the language accepted by a program's input parser (also known as its input processor). In this setting, a learner can only query a string of its choice by executing the parser on it, which limits the language learning algorithms that can be used. Most practical parsers can indicate not only whether the string is valid or not, but also where the parsing failed. This extra information can be leveraged into producing a type of query we call the prefix query. Notably, no existing language learning algorithms make use of prefix queries, though some ask membership queries i.e., they ask whether or not a given string is valid. When these approaches are used to learn the language of a parser, the prefix information provided by the parser remains unused.\n  In this work, we present PL*, the first known language learning algorithm to make use of the prefix query, and a novel modification of the classical L* algorithm. We show both theoretically and empirically that PL* is able to learn more efficiently than L* due to its ability to exploit the additional information given by prefix queries over membership queries. Furthermore, we show how PL* can be used to learn the language of a parser, by adapting it to a more practical setting in which prefix queries are the only source of information available to it; that is, it does not have access to any labelled examples or any other types of queries. We demonstrate empirically that, even in this more constrained setting, PL* is still capable of accurately learning a range of languages of practical interest."
      },
      {
        "id": "oai:arXiv.org:2504.02174v1",
        "title": "FastFlow: Early Yet Robust Network Flow Classification using the Minimal Number of Time-Series Packets",
        "link": "https://arxiv.org/abs/2504.02174",
        "author": "Rushi Jayeshkumar Babaria, Minzhao Lyu, Gustavo Batista, Vijay Sivaraman",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02174v1 Announce Type: cross \nAbstract: Network traffic classification is of great importance for network operators in their daily routines, such as analyzing the usage patterns of multimedia applications and optimizing network configurations. Internet service providers (ISPs) that operate high-speed links expect network flow classifiers to accurately classify flows early, using the minimal number of necessary initial packets per flow. These classifiers must also be robust to packet sequence disorders in candidate flows and capable of detecting unseen flow types that are not within the existing classification scope, which are not well achieved by existing methods. In this paper, we develop FastFlow, a time-series flow classification method that accurately classifies network flows as one of the known types or the unknown type, which dynamically selects the minimal number of packets to balance accuracy and efficiency. Toward the objectives, we first develop a flow representation process that converts packet streams at both per-packet and per-slot granularity for precise packet statistics with robustness to packet sequence disorders. Second, we develop a sequential decision-based classification model that leverages LSTM architecture trained with reinforcement learning. Our model makes dynamic decisions on the minimal number of time-series data points per flow for the confident classification as one of the known flow types or an unknown one. We evaluated our method on public datasets and demonstrated its superior performance in early and accurate flow classification. Deployment insights on the classification of over 22.9 million flows across seven application types and 33 content providers in a campus network over one week are discussed, showing that FastFlow requires an average of only 8.37 packets and 0.5 seconds to classify the application type of a flow with over 91% accuracy and over 96% accuracy for the content providers."
      },
      {
        "id": "oai:arXiv.org:2504.02191v1",
        "title": "A User-Tunable Machine Learning Framework for Step-Wise Synthesis Planning",
        "link": "https://arxiv.org/abs/2504.02191",
        "author": "Shivesh Prakash, Viki Kumar Prasad, Hans-Arno Jacobsen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02191v1 Announce Type: cross \nAbstract: We introduce MHNpath, a machine learning-driven retrosynthetic tool designed for computer-aided synthesis planning. Leveraging modern Hopfield networks and novel comparative metrics, MHNpath efficiently prioritizes reaction templates, improving the scalability and accuracy of retrosynthetic predictions. The tool incorporates a tunable scoring system that allows users to prioritize pathways based on cost, reaction temperature, and toxicity, thereby facilitating the design of greener and cost-effective reaction routes. We demonstrate its effectiveness through case studies involving complex molecules from ChemByDesign, showcasing its ability to predict novel synthetic and enzymatic pathways. Furthermore, we benchmark MHNpath against existing frameworks, replicating experimentally validated \"gold-standard\" pathways from PaRoutes. Our case studies reveal that the tool can generate shorter, cheaper, moderate-temperature routes employing green solvents, as exemplified by compounds such as dronabinol, arformoterol, and lupinine."
      },
      {
        "id": "oai:arXiv.org:2504.02196v1",
        "title": "Orbit Determination through Cosmic Microwave Background Radiation",
        "link": "https://arxiv.org/abs/2504.02196",
        "author": "Pedro K de Albuquerque, Andre R Kuroswiski, Annie S. Wu, Willer G. dos Santos, Paulo Costa",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02196v1 Announce Type: cross \nAbstract: This research explores the use of Cosmic Microwave Background (CMB) radiation as a reference signal for Initial Orbit Determination (IOD). By leveraging the unique properties of CMB, this study introduces a novel method for estimating spacecraft velocity and position with minimal reliance on pre-existing environmental data, offering significant advantages for space missions independent of Earth-specific conditions. Using Machine Learning (ML) regression models, this approach demonstrates the capability to determine velocity from CMB signals and subsequently determine the satellite's position. The results indicate that CMB has the potential to enhance the autonomy and flexibility of spacecraft operations."
      },
      {
        "id": "oai:arXiv.org:2504.02211v1",
        "title": "FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention",
        "link": "https://arxiv.org/abs/2504.02211",
        "author": "Huangliang Dai, Shixun Wu, Hairui Zhao, Jiajun Huang, Zizhe Jian, Yue Zhu, Haiyang Hu, Zizhong Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02211v1 Announce Type: cross \nAbstract: Transformer models leverage self-attention mechanisms to capture complex dependencies, demonstrating exceptional performance in various applications. However, the long-duration high-load computations required for model inference impose stringent reliability demands on the computing platform, as soft errors that occur during execution can significantly degrade model performance. Existing fault tolerance methods protect each operation separately using decoupled kernels, incurring substantial computational and memory overhead. In this paper, we propose a novel error-resilient framework for Transformer models, integrating end-to-end fault tolerant attention (EFTA) to improve inference reliability against soft errors. Our approach enables error detection and correction within a fully fused attention kernel, reducing redundant data access and thereby mitigating memory faults. To further enhance error coverage and reduce overhead, we design a hybrid fault tolerance scheme tailored for the EFTA, introducing for the first time: 1) architecture-aware algorithm-based fault tolerance (ABFT) using tensor checksum, which minimizes inter-thread communication overhead on tensor cores during error detection; 2) selective neuron value restriction, which selectively applies adaptive fault tolerance constraints to neuron values, balancing error coverage and overhead; 3) unified verification, reusing checksums to streamline multiple computation steps into a single verification process. Experimental results show that EFTA achieves up to 7.56x speedup over traditional methods with an average fault tolerance overhead of 13.9%."
      },
      {
        "id": "oai:arXiv.org:2504.02216v1",
        "title": "Image Coding for Machines via Feature-Preserving Rate-Distortion Optimization",
        "link": "https://arxiv.org/abs/2504.02216",
        "author": "Samuel Fern\\'andez-Mendui\\~na, Eduardo Pavez, Antonio Ortega",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02216v1 Announce Type: cross \nAbstract: Many images and videos are primarily processed by computer vision algorithms, involving only occasional human inspection. When this content requires compression before processing, e.g., in distributed applications, coding methods must optimize for both visual quality and downstream task performance. We first show that, given the features obtained from the original and the decoded images, an approach to reduce the effect of compression on a task loss is to perform rate-distortion optimization (RDO) using the distance between features as a distortion metric. However, optimizing directly such a rate-distortion trade-off requires an iterative workflow of encoding, decoding, and feature evaluation for each coding parameter, which is computationally impractical. We address this problem by simplifying the RDO formulation to make the distortion term computable using block-based encoders. We first apply Taylor's expansion to the feature extractor, recasting the feature distance as a quadratic metric with the Jacobian matrix of the neural network. Then, we replace the linearized metric with a block-wise approximation, which we call input-dependent squared error (IDSE). To reduce computational complexity, we approximate IDSE using Jacobian sketches. The resulting loss can be evaluated block-wise in the transform domain and combined with the sum of squared errors (SSE) to address both visual quality and computer vision performance. Simulations with AVC across multiple feature extractors and downstream neural networks show up to 10% bit-rate savings for the same computer vision accuracy compared to RDO based on SSE, with no decoder complexity overhead and just a 7% encoder complexity increase."
      },
      {
        "id": "oai:arXiv.org:2504.02222v1",
        "title": "APSeg: Auto-Prompt Model with Acquired and Injected Knowledge for Nuclear Instance Segmentation and Classification",
        "link": "https://arxiv.org/abs/2504.02222",
        "author": "Liying Xu, Hongliang He, Wei Han, Hanbin Huang, Siwei Feng, Guohong Fu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02222v1 Announce Type: cross \nAbstract: Nuclear instance segmentation and classification provide critical quantitative foundations for digital pathology diagnosis. With the advent of the foundational Segment Anything Model (SAM), the accuracy and efficiency of nuclear segmentation have improved significantly. However, SAM imposes a strong reliance on precise prompts, and its class-agnostic design renders its classification results entirely dependent on the provided prompts. Therefore, we focus on generating prompts with more accurate localization and classification and propose \\textbf{APSeg}, \\textbf{A}uto-\\textbf{P}rompt model with acquired and injected knowledge for nuclear instance \\textbf{Seg}mentation and classification. APSeg incorporates two knowledge-aware modules: (1) Distribution-Guided Proposal Offset Module (\\textbf{DG-POM}), which learns distribution knowledge through density map guided, and (2) Category Knowledge Semantic Injection Module (\\textbf{CK-SIM}), which injects morphological knowledge derived from category descriptions. We conducted extensive experiments on the PanNuke and CoNSeP datasets, demonstrating the effectiveness of our approach. The code will be released upon acceptance."
      },
      {
        "id": "oai:arXiv.org:2504.02234v1",
        "title": "LLM Social Simulations Are a Promising Research Method",
        "link": "https://arxiv.org/abs/2504.02234",
        "author": "Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02234v1 Announce Type: cross \nAbstract: Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted these methods. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a literature survey of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions with prompting, fine-tuning, and complementary methods. We believe that LLM social simulations can already be used for exploratory research, such as pilot experiments for psychology, economics, sociology, and marketing. More widespread use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and evaluations that can be iteratively deployed and refined at pace with ongoing AI advances."
      },
      {
        "id": "oai:arXiv.org:2504.02241v1",
        "title": "Quantum Deep Sets and Sequences",
        "link": "https://arxiv.org/abs/2504.02241",
        "author": "Vladimir Vargas-Calder\\'on",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02241v1 Announce Type: cross \nAbstract: This paper introduces the quantum deep sets model, expanding the quantum machine learning tool-box by enabling the possibility of learning variadic functions using quantum systems. A couple of variants are presented for this model. The first one focuses on mapping sets to quantum systems through state vector averaging: each element of the set is mapped to a quantum state, and the quantum state of the set is the average of the corresponding quantum states of its elements. This approach allows the definition of a permutation-invariant variadic model. The second variant is useful for ordered sets, i.e., sequences, and relies on optimal coherification of tristochastic tensors that implement products of mixed states: each element of the set is mapped to a density matrix, and the quantum state of the set is the product of the corresponding density matrices of its elements. Such variant can be relevant in tasks such as natural language processing. The resulting quantum state in any of the variants is then processed to realise a function that solves a machine learning task such as classification, regression or density estimation. Through synthetic problem examples, the efficacy and versatility of quantum deep sets and sequences (QDSs) is demonstrated."
      },
      {
        "id": "oai:arXiv.org:2504.02263v1",
        "title": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism",
        "link": "https://arxiv.org/abs/2504.02263",
        "author": "Ruidong Zhu, Ziheng Jiang, Chao Jin, Peng Wu, Cesar A. Stuardo, Dongyang Wang, Xinlei Zhang, Huaping Zhou, Haoran Wei, Yang Cheng, Jianzhe Xiao, Xinyi Zhang, Lingjun Liu, Haibin Lin, Li-Wen Chang, Jianxi Ye, Xiao Yu, Xuanzhe Liu, Xin Jin, Xin Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02263v1 Announce Type: cross \nAbstract: Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity. However, its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs. We present MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models. MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules. To fully exploit disaggregation in the presence of MoE's sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference. Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization. To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization. Experimental results indicate that MegaScale-Infer achieves up to 1.90x higher per-GPU throughput than state-of-the-art solutions."
      },
      {
        "id": "oai:arXiv.org:2504.02269v1",
        "title": "Engineering Artificial Intelligence: Framework, Challenges, and Future Direction",
        "link": "https://arxiv.org/abs/2504.02269",
        "author": "Jay Lee, Hanqi Su, Dai-Yan Ji, Takanobu Minami",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02269v1 Announce Type: cross \nAbstract: Over the past ten years, the application of artificial intelligence (AI) and machine learning (ML) in engineering domains has gained significant popularity, showcasing their potential in data-driven contexts. However, the complexity and diversity of engineering problems often require the development of domain-specific AI approaches, which are frequently hindered by a lack of systematic methodologies, scalability, and robustness during the development process. To address this gap, this paper introduces the \"ABCDE\" as the key elements of Engineering AI and proposes a unified, systematic engineering AI ecosystem framework, including eight essential layers, along with attributes, goals, and applications, to guide the development and deployment of AI solutions for specific engineering needs. Additionally, key challenges are examined, and nine future research directions are highlighted. By providing a comprehensive perspective, this paper aims to advance the strategic implementation of AI, fostering the development of next-generation engineering AI solutions."
      },
      {
        "id": "oai:arXiv.org:2504.02280v1",
        "title": "LLM-Guided Evolution: An Autonomous Model Optimization for Object Detection",
        "link": "https://arxiv.org/abs/2504.02280",
        "author": "YiMing Yu, Jason Zutty",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02280v1 Announce Type: cross \nAbstract: In machine learning, Neural Architecture Search (NAS) requires domain knowledge of model design and a large amount of trial-and-error to achieve promising performance. Meanwhile, evolutionary algorithms have traditionally relied on fixed rules and pre-defined building blocks. The Large Language Model (LLM)-Guided Evolution (GE) framework transformed this approach by incorporating LLMs to directly modify model source code for image classification algorithms on CIFAR data and intelligently guide mutations and crossovers. A key element of LLM-GE is the \"Evolution of Thought\" (EoT) technique, which establishes feedback loops, allowing LLMs to refine their decisions iteratively based on how previous operations performed. In this study, we perform NAS for object detection by improving LLM-GE to modify the architecture of You Only Look Once (YOLO) models to enhance performance on the KITTI dataset. Our approach intelligently adjusts the design and settings of YOLO to find the optimal algorithms against objective such as detection accuracy and speed. We show that LLM-GE produced variants with significant performance improvements, such as an increase in Mean Average Precision from 92.5% to 94.5%. This result highlights the flexibility and effectiveness of LLM-GE on real-world challenges, offering a novel paradigm for automated machine learning that combines LLM-driven reasoning with evolutionary strategies."
      },
      {
        "id": "oai:arXiv.org:2504.02288v1",
        "title": "FEASE: Shallow AutoEncoding Recommender with Cold Start Handling via Side Features",
        "link": "https://arxiv.org/abs/2504.02288",
        "author": "Edward DongBo Cui, Lu Zhang, William Ping-hsun Lee",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02288v1 Announce Type: cross \nAbstract: User and item cold starts present significant challenges in industrial applications of recommendation systems. Supplementing user-item interaction data with metadata is a common solution-but often at the cost of introducing additional biases. In this work, we introduce an augmented EASE model, i.e. FEASE, that seamlessly integrates both user and item side information to address these cold start issues. Our straightforward, autoencoder-based method produces a closed-form solution that leverages rich content signals for cold items while refining user representations in data-sparse environments. Importantly, our method strikes a balance by effectively recommending cold start items and handling cold start users without incurring extra bias, and it maintains strong performance in warm settings. Experimental results demonstrate improved recommendation accuracy and robustness compared to previous collaborative filtering approaches. Moreover, our model serves as a strong baseline for future comparative studies."
      },
      {
        "id": "oai:arXiv.org:2504.02302v1",
        "title": "Causal Self-supervised Pretrained Frontend with Predictive Code for Speech Separation",
        "link": "https://arxiv.org/abs/2504.02302",
        "author": "Wupeng Wang, Zexu Pan, Xinke Li, Shuai Wang, Haizhou Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02302v1 Announce Type: cross \nAbstract: Speech separation (SS) seeks to disentangle a multi-talker speech mixture into single-talker speech streams. Although SS can be generally achieved using offline methods, such a processing paradigm is not suitable for real-time streaming applications. Causal separation models, which rely only on past and present information, offer a promising solution for real-time streaming. However, these models typically suffer from notable performance degradation due to the absence of future context. In this paper, we introduce a novel frontend that is designed to mitigate the mismatch between training and run-time inference by implicitly incorporating future information into causal models through predictive patterns. The pretrained frontend employs a transformer decoder network with a causal convolutional encoder as the backbone and is pretrained in a self-supervised manner with two innovative pretext tasks: autoregressive hybrid prediction and contextual knowledge distillation. These tasks enable the model to capture predictive patterns directly from mixtures in a self-supervised manner. The pretrained frontend subsequently serves as a feature extractor to generate high-quality predictive patterns. Comprehensive evaluations on synthetic and real-world datasets validated the effectiveness of the proposed pretrained frontend."
      },
      {
        "id": "oai:arXiv.org:2504.02324v1",
        "title": "Dynamic Assortment Selection and Pricing with Censored Preference Feedback",
        "link": "https://arxiv.org/abs/2504.02324",
        "author": "Jung-hun Kim, Min-hwan Oh",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02324v1 Announce Type: cross \nAbstract: In this study, we investigate the problem of dynamic multi-product selection and pricing by introducing a novel framework based on a \\textit{censored multinomial logit} (C-MNL) choice model. In this model, sellers present a set of products with prices, and buyers filter out products priced above their valuation, purchasing at most one product from the remaining options based on their preferences. The goal is to maximize seller revenue by dynamically adjusting product offerings and prices, while learning both product valuations and buyer preferences through purchase feedback. To achieve this, we propose a Lower Confidence Bound (LCB) pricing strategy. By combining this pricing strategy with either an Upper Confidence Bound (UCB) or Thompson Sampling (TS) product selection approach, our algorithms achieve regret bounds of $\\tilde{O}(d^{\\frac{3}{2}}\\sqrt{T/\\kappa})$ and $\\tilde{O}(d^{2}\\sqrt{T/\\kappa})$, respectively. Finally, we validate the performance of our methods through simulations, demonstrating their effectiveness."
      },
      {
        "id": "oai:arXiv.org:2504.02334v1",
        "title": "Determining Sphere Radius through Pairwise Distances",
        "link": "https://arxiv.org/abs/2504.02334",
        "author": "Boris Sukhovilov",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02334v1 Announce Type: cross \nAbstract: We propose a novel method for determining the radius of a spherical surface based on the distances measured between points on this surface. We consider the most general case of determining the radius when the distances are measured with errors and the sphere has random deviations from its ideal shape. For the solution, we used the minimally necessary four points and an arbitrary N number of points. We provide a new closed form solution for the radius of the sphere through the matrix of pairwise distances. We also determine the standard deviation of the radius estimate caused by measurement errors and deviations of the sphere from its ideal shape. We found optimal configurations of points on the sphere that provide the minimum standard deviation of the radius estimate. This paper describes our solution and provides all the mathematical derivations. We share the implementation of our method as open source code at https://github.com/boris-sukhovilov/Sphere_Radius."
      },
      {
        "id": "oai:arXiv.org:2504.02361v1",
        "title": "MG-Gen: Single Image to Motion Graphics Generation with Layer Decomposition",
        "link": "https://arxiv.org/abs/2504.02361",
        "author": "Takahiro Shirakawa, Tomoyuki Suzuki, Daichi Haraguchi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02361v1 Announce Type: cross \nAbstract: General image-to-video generation methods often produce suboptimal animations that do not meet the requirements of animated graphics, as they lack active text motion and exhibit object distortion. Also, code-based animation generation methods typically require layer-structured vector data which are often not readily available for motion graphic generation. To address these challenges, we propose a novel framework named MG-Gen that reconstructs data in vector format from a single raster image to extend the capabilities of code-based methods to enable motion graphics generation from a raster image in the framework of general image-to-video generation. MG-Gen first decomposes the input image into layer-wise elements, reconstructs them as HTML format data and then generates executable JavaScript code for the reconstructed HTML data. We experimentally confirm that \\ours{} generates motion graphics while preserving text readability and input consistency. These successful results indicate that combining layer decomposition and animation code generation is an effective strategy for motion graphics generation."
      },
      {
        "id": "oai:arXiv.org:2504.02367v1",
        "title": "CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design",
        "link": "https://arxiv.org/abs/2504.02367",
        "author": "Zhendong Cao, Lei Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02367v1 Announce Type: cross \nAbstract: Reinforcement fine-tuning has instrumental enhanced the instruction-following and reasoning abilities of large language models. In this work, we explore the applications of reinforcement fine-tuning to the autoregressive transformer-based materials generative model CrystalFormer (arXiv:2403.15734) using discriminative machine learning models such as interatomic potentials and property prediction models. By optimizing reward signals-such as energy above the convex hull and material property figures of merit-reinforcement fine-tuning infuses knowledge from discriminative models into generative models. The resulting model, CrystalFormer-RL, shows enhanced stability in generated crystals and successfully discovers crystals with desirable yet conflicting material properties, such as substantial dielectric constant and band gap simultaneously. Notably, we observe that reinforcement fine-tuning enables not only the property-guided novel material design ability of generative pre-trained model but also unlocks property-driven material retrieval from the unsupervised pre-training dataset. Leveraging rewards from discriminative models to fine-tune materials generative models opens an exciting gateway to the synergies of the machine learning ecosystem for materials."
      },
      {
        "id": "oai:arXiv.org:2504.02373v1",
        "title": "HPGN: Hybrid Priors-Guided Network for Compressed Low-Light Image Enhancement",
        "link": "https://arxiv.org/abs/2504.02373",
        "author": "Hantang Li, Jinhua Hao, Lei Xiong, Shuyuan Zhu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02373v1 Announce Type: cross \nAbstract: In practical applications, conventional methods generate large volumes of low-light images that require compression for efficient storage and transmission. However, most existing methods either disregard the removal of potential compression artifacts during the enhancement process or fail to establish a unified framework for joint task enhancement of images with varying compression qualities. To solve this problem, we propose the hybrid priors-guided network (HPGN), which enhances compressed low-light images by integrating both compression and illumination priors. Our approach fully utilizes the JPEG quality factor (QF) and DCT quantization matrix (QM) to guide the design of efficient joint task plug-and-play modules. Additionally, we employ a random QF generation strategy to guide model training, enabling a single model to enhance images across different compression levels. Experimental results confirm the superiority of our proposed method."
      },
      {
        "id": "oai:arXiv.org:2504.02382v1",
        "title": "Benchmark of Segmentation Techniques for Pelvic Fracture in CT and X-ray: Summary of the PENGWIN 2024 Challenge",
        "link": "https://arxiv.org/abs/2504.02382",
        "author": "Yudi Sang, Yanzhen Liu, Sutuke Yibulayimu, Yunning Wang, Benjamin D. Killeen, Mingxu Liu, Ping-Cheng Ku, Ole Johannsen, Karol Gotkowski, Maximilian Zenk, Klaus Maier-Hein, Fabian Isensee, Peiyan Yue, Yi Wang, Haidong Yu, Zhaohong Pan, Yutong He, Xiaokun Liang, Daiqi Liu, Fuxin Fan, Artur Jurgas, Andrzej Skalski, Yuxi Ma, Jing Yang, Szymon P{\\l}otka, Rafa{\\l} Litka, Gang Zhu, Yingchun Song, Mathias Unberath, Mehran Armand, Dan Ruan, S. Kevin Zhou, Qiyong Cao, Chunpeng Zhao, Xinbao Wu, Yu Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02382v1 Announce Type: cross \nAbstract: The segmentation of pelvic fracture fragments in CT and X-ray images is crucial for trauma diagnosis, surgical planning, and intraoperative guidance. However, accurately and efficiently delineating the bone fragments remains a significant challenge due to complex anatomy and imaging limitations. The PENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance automated fracture segmentation by benchmarking state-of-the-art algorithms on these complex tasks. A diverse dataset of 150 CT scans was collected from multiple clinical centers, and a large set of simulated X-ray images was generated using the DeepDRR method. Final submissions from 16 teams worldwide were evaluated under a rigorous multi-metric testing scheme. The top-performing CT algorithm achieved an average fragment-wise intersection over union (IoU) of 0.930, demonstrating satisfactory accuracy. However, in the X-ray task, the best algorithm attained an IoU of 0.774, highlighting the greater challenges posed by overlapping anatomical structures. Beyond the quantitative evaluation, the challenge revealed methodological diversity in algorithm design. Variations in instance representation, such as primary-secondary classification versus boundary-core separation, led to differing segmentation strategies. Despite promising results, the challenge also exposed inherent uncertainties in fragment definition, particularly in cases of incomplete fractures. These findings suggest that interactive segmentation approaches, integrating human decision-making with task-relevant information, may be essential for improving model reliability and clinical applicability."
      },
      {
        "id": "oai:arXiv.org:2504.02408v1",
        "title": "Translation of Fetal Brain Ultrasound Images into Pseudo-MRI Images using Artificial Intelligence",
        "link": "https://arxiv.org/abs/2504.02408",
        "author": "Naomi Silverstein, Efrat Leibowitz, Ron Beloosesky, Haim Azhari",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02408v1 Announce Type: cross \nAbstract: Ultrasound is a widely accessible and cost-effective medical imaging tool commonly used for prenatal evaluation of the fetal brain. However, it has limitations, particularly in the third trimester, where the complexity of the fetal brain requires high image quality for extracting quantitative data. In contrast, magnetic resonance imaging (MRI) offers superior image quality and tissue differentiation but is less available, expensive, and requires time-consuming acquisition. Thus, transforming ultrasonic images into an MRI-mimicking display may be advantageous and allow better tissue anatomy presentation. To address this goal, we have examined the use of artificial intelligence, implementing a diffusion model renowned for generating high-quality images. The proposed method, termed \"Dual Diffusion Imposed Correlation\" (DDIC), leverages a diffusion-based translation methodology, assuming a shared latent space between ultrasound and MRI domains. Model training was obtained utilizing the \"HC18\" dataset for ultrasound and the \"CRL fetal brain atlas\" along with the \"FeTA \" datasets for MRI. The generated pseudo-MRI images provide notable improvements in visual discrimination of brain tissue, especially in the lateral ventricles and the Sylvian fissure, characterized by enhanced contrast clarity. Improvement was demonstrated in Mutual information, Peak signal-to-noise ratio, Fr\\'echet Inception Distance, and Contrast-to-noise ratio. Findings from these evaluations indicate statistically significant superior performance of the DDIC compared to other translation methodologies. In addition, a Medical Opinion Test was obtained from 5 gynecologists. The results demonstrated display improvement in 81% of the tested images. In conclusion, the presented pseudo-MRI images hold the potential for streamlining diagnosis and enhancing clinical outcomes through improved representation."
      },
      {
        "id": "oai:arXiv.org:2504.02439v1",
        "title": "Estimating Scene Flow in Robot Surroundings with Distributed Miniaturized Time-of-Flight Sensors",
        "link": "https://arxiv.org/abs/2504.02439",
        "author": "Jack Sander, Giammarco Caroleo, Alessandro Albini, Perla Maiolino",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02439v1 Announce Type: cross \nAbstract: Tracking motions of humans or objects in the surroundings of the robot is essential to improve safe robot motions and reactions. In this work, we present an approach for scene flow estimation from low-density and noisy point clouds acquired from miniaturized Time of Flight (ToF) sensors distributed on the robot body. The proposed method clusters points from consecutive frames and applies Iterative Closest Point (ICP) to estimate a dense motion flow, with additional steps introduced to mitigate the impact of sensor noise and low-density data points. Specifically, we employ a fitness-based classification to distinguish between stationary and moving points and an inlier removal strategy to refine geometric correspondences. The proposed approach is validated in an experimental setup where 24 ToF are used to estimate the velocity of an object moving at different controlled speeds. Experimental results show that the method consistently approximates the direction of the motion and its magnitude with an error which is in line with sensor noise."
      },
      {
        "id": "oai:arXiv.org:2504.02450v1",
        "title": "CHARMS: Cognitive Hierarchical Agent with Reasoning and Motion Styles",
        "link": "https://arxiv.org/abs/2504.02450",
        "author": "Jingyi Wang, Duanfeng Chu, Zejian Deng, Liping Lu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02450v1 Announce Type: cross \nAbstract: To address the current challenges of low intelligence and simplistic vehicle behavior modeling in autonomous driving simulation scenarios, this paper proposes the Cognitive Hierarchical Agent with Reasoning and Motion Styles (CHARMS). The model can reason about the behavior of other vehicles like a human driver and respond with different decision-making styles, thereby improving the intelligence and diversity of the surrounding vehicles in the driving scenario. By introducing the Level-k behavioral game theory, the paper models the decision-making process of human drivers and employs deep reinforcement learning to train the models with diverse decision styles, simulating different reasoning approaches and behavioral characteristics. Building on the Poisson cognitive hierarchy theory, this paper also presents a novel driving scenario generation method. The method controls the proportion of vehicles with different driving styles in the scenario using Poisson and binomial distributions, thus generating controllable and diverse driving environments. Experimental results demonstrate that CHARMS not only exhibits superior decision-making capabilities as ego vehicles, but also generates more complex and diverse driving scenarios as surrounding vehicles. We will release code for CHARMS at https://github.com/WUTAD-Wjy/CHARMS."
      },
      {
        "id": "oai:arXiv.org:2504.02461v1",
        "title": "Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness",
        "link": "https://arxiv.org/abs/2504.02461",
        "author": "Juliett Su\\'arez Ferreira, Marija Slavkovik, Jorge Casillas",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02461v1 Announce Type: cross \nAbstract: Current fairness metrics and mitigation techniques provide tools for practitioners to asses how non-discriminatory Automatic Decision Making (ADM) systems are. What if I, as an individual facing a decision taken by an ADM system, would like to know: Am I being treated fairly? We explore how to create the affordance for users to be able to ask this question of ADM. In this paper, we argue for the reification of fairness not only as a property of ADM, but also as an epistemic right of an individual to acquire information about the decisions that affect them and use that information to contest and seek effective redress against those decisions, in case they are proven to be discriminatory. We examine key concepts from existing research not only in algorithmic fairness but also in explainable artificial intelligence, accountability, and contestability. Integrating notions from these domains, we propose a conceptual framework to ascertain fairness by combining different tools that empower the end-users of ADM systems. Our framework shifts the focus from technical solutions aimed at practitioners to mechanisms that enable individuals to understand, challenge, and verify the fairness of decisions, and also serves as a blueprint for organizations and policymakers, bridging the gap between technical requirements and practical, user-centered accountability."
      },
      {
        "id": "oai:arXiv.org:2504.02465v1",
        "title": "RASP: Revisiting 3D Anamorphic Art for Shadow-Guided Packing of Irregular Objects",
        "link": "https://arxiv.org/abs/2504.02465",
        "author": "Soumyaratna Debnath, Ashish Tiwari, Kaustubh Sadekar, Shanmuganathan Raman",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02465v1 Announce Type: cross \nAbstract: Recent advancements in learning-based methods have opened new avenues for exploring and interpreting art forms, such as shadow art, origami, and sketch art, through computational models. One notable visual art form is 3D Anamorphic Art in which an ensemble of arbitrarily shaped 3D objects creates a realistic and meaningful expression when observed from a particular viewpoint and loses its coherence over the other viewpoints. In this work, we build on insights from 3D Anamorphic Art to perform 3D object arrangement. We introduce RASP, a differentiable-rendering-based framework to arrange arbitrarily shaped 3D objects within a bounded volume via shadow (or silhouette)-guided optimization with an aim of minimal inter-object spacing and near-maximal occupancy. Furthermore, we propose a novel SDF-based formulation to handle inter-object intersection and container extrusion. We demonstrate that RASP can be extended to part assembly alongside object packing considering 3D objects to be \"parts\" of another 3D object. Finally, we present artistic illustrations of multi-view anamorphic art, achieving meaningful expressions from multiple viewpoints within a single ensemble."
      },
      {
        "id": "oai:arXiv.org:2504.02473v1",
        "title": "Adaptive path planning for efficient object search by UAVs in agricultural fields",
        "link": "https://arxiv.org/abs/2504.02473",
        "author": "Rick van Essen, Eldert van Henten, Lammert Kooistra, Gert Kootstra",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02473v1 Announce Type: cross \nAbstract: This paper presents an adaptive path planner for object search in agricultural fields using UAVs. The path planner uses a high-altitude coverage flight path and plans additional low-altitude inspections when the detection network is uncertain. The path planner was evaluated in an offline simulation environment containing real-world images. We trained a YOLOv8 detection network to detect artificial plants placed in grass fields to showcase the potential of our path planner. We evaluated the effect of different detection certainty measures, optimized the path planning parameters, investigated the effects of localization errors and different numbers of objects in the field. The YOLOv8 detection confidence worked best to differentiate between true and false positive detections and was therefore used in the adaptive planner. The optimal parameters of the path planner depended on the distribution of objects in the field, when the objects were uniformly distributed, more low-altitude inspections were needed compared to a non-uniform distribution of objects, resulting in a longer path length. The adaptive planner proved to be robust against localization uncertainty. When increasing the number of objects, the flight path length increased, especially when the objects were uniformly distributed. When the objects were non-uniformly distributed, the adaptive path planner yielded a shorter path than a low-altitude coverage path, even with high number of objects. Overall, the presented adaptive path planner allowed to find non-uniformly distributed objects in a field faster than a coverage path planner and resulted in a compatible detection accuracy. The path planner is made available at https://github.com/wur-abe/uav_adaptive_planner."
      },
      {
        "id": "oai:arXiv.org:2504.02477v1",
        "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
        "link": "https://arxiv.org/abs/2504.02477",
        "author": "Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An, Changwei Wang, Li Guo, Weiliang Meng, Xiaopeng Zhang, Rongtao Xu, Shibiao Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02477v1 Announce Type: cross \nAbstract: Robot vision has greatly benefited from advancements in multimodal fusion techniques and vision-language models (VLMs). We systematically review the applications of multimodal fusion in key robotic vision tasks, including semantic scene understanding, simultaneous localization and mapping (SLAM), 3D object detection, navigation and localization, and robot manipulation. We compare VLMs based on large language models (LLMs) with traditional multimodal fusion methods, analyzing their advantages, limitations, and synergies. Additionally, we conduct an in-depth analysis of commonly used datasets, evaluating their applicability and challenges in real-world robotic scenarios. Furthermore, we identify critical research challenges such as cross-modal alignment, efficient fusion strategies, real-time deployment, and domain adaptation, and propose future research directions, including self-supervised learning for robust multimodal representations, transformer-based fusion architectures, and scalable multimodal frameworks. Through a comprehensive review, comparative analysis, and forward-looking discussion, we provide a valuable reference for advancing multimodal perception and interaction in robotic vision. A comprehensive list of studies in this survey is available at https://github.com/Xiaofeng-Han-Res/MF-RV."
      },
      {
        "id": "oai:arXiv.org:2504.02511v1",
        "title": "Analytical Discovery of Manifold with Machine Learning",
        "link": "https://arxiv.org/abs/2504.02511",
        "author": "Yafei Shen, Huan-Fei Ma, Ling Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02511v1 Announce Type: cross \nAbstract: Understanding low-dimensional structures within high-dimensional data is crucial for visualization, interpretation, and denoising in complex datasets. Despite the advancements in manifold learning techniques, key challenges-such as limited global insight and the lack of interpretable analytical descriptions-remain unresolved. In this work, we introduce a novel framework, GAMLA (Global Analytical Manifold Learning using Auto-encoding). GAMLA employs a two-round training process within an auto-encoding framework to derive both character and complementary representations for the underlying manifold. With the character representation, the manifold is represented by a parametric function which unfold the manifold to provide a global coordinate. While with the complementary representation, an approximate explicit manifold description is developed, offering a global and analytical representation of smooth manifolds underlying high-dimensional datasets. This enables the analytical derivation of geometric properties such as curvature and normal vectors. Moreover, we find the two representations together decompose the whole latent space and can thus characterize the local spatial structure surrounding the manifold, proving particularly effective in anomaly detection and categorization. Through extensive experiments on benchmark datasets and real-world applications, GAMLA demonstrates its ability to achieve computational efficiency and interpretability while providing precise geometric and structural insights. This framework bridges the gap between data-driven manifold learning and analytical geometry, presenting a versatile tool for exploring the intrinsic properties of complex data sets."
      },
      {
        "id": "oai:arXiv.org:2504.02565v1",
        "title": "MAD: A Magnitude And Direction Policy Parametrization for Stability Constrained Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.02565",
        "author": "Luca Furieri, Sucheth Shenoy, Danilo Saccani, Andrea Martin, Giancarlo Ferrari Trecate",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02565v1 Announce Type: cross \nAbstract: We introduce magnitude and direction (MAD) policies, a policy parameterization for reinforcement learning (RL) that preserves Lp closed-loop stability for nonlinear dynamical systems. Although complete in their ability to describe all stabilizing controllers, methods based on nonlinear Youla and system-level synthesis are significantly affected by the difficulty of parameterizing Lp-stable operators. In contrast, MAD policies introduce explicit feedback on state-dependent features - a key element behind the success of RL pipelines - without compromising closed-loop stability. This is achieved by describing the magnitude of the control input with a disturbance-feedback Lp-stable operator, while selecting its direction based on state-dependent features through a universal function approximator. We further characterize the robust stability properties of MAD policies under model mismatch. Unlike existing disturbance-feedback policy parameterizations, MAD policies introduce state-feedback components compatible with model-free RL pipelines, ensuring closed-loop stability without requiring model information beyond open-loop stability. Numerical experiments show that MAD policies trained with deep deterministic policy gradient (DDPG) methods generalize to unseen scenarios, matching the performance of standard neural network policies while guaranteeing closed-loop stability by design."
      },
      {
        "id": "oai:arXiv.org:2504.02577v1",
        "title": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning",
        "link": "https://arxiv.org/abs/2504.02577",
        "author": "Erik Arakelyan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02577v1 Announce Type: cross \nAbstract: The recent advancements in Deep Learning models and techniques have led to significant strides in performance across diverse tasks and modalities. However, while the overall capabilities of models show promising growth, our understanding of their internal reasoning processes remains limited, particularly concerning systematic inconsistencies or errors patterns of logical or inferential flaws. These inconsistencies may manifest as contradictory outputs, failure to generalize across similar tasks, or erroneous conclusions in specific contexts. Even detecting and measuring such reasoning discrepancies is challenging, as they may arise from opaque internal procedures, biases and imbalances in training data, or the inherent complexity of the task. Without effective methods to detect, measure, and mitigate these errors, there is a risk of deploying models that are biased, exploitable, or logically unreliable. This thesis aims to address these issues by producing novel methods for deep learning models that reason over knowledge graphs, natural language, and images. The thesis contributes two techniques for detecting and quantifying predictive inconsistencies originating from opaque internal procedures in natural language and image processing models. To mitigate inconsistencies from biases in training data, this thesis presents a data efficient sampling method to improve fairness and performance and a synthetic dataset generation approach in low resource scenarios. Finally, the thesis offers two techniques to optimize the models for complex reasoning tasks. These methods enhance model performance while allowing for more faithful and interpretable exploration and exploitation during inference. Critically, this thesis provides a comprehensive framework to improve the robustness, fairness, and interpretability of deep learning models across diverse tasks and modalities."
      },
      {
        "id": "oai:arXiv.org:2504.02605v1",
        "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
        "link": "https://arxiv.org/abs/2504.02605",
        "author": "Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, Liang Xiang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02605v1 Announce Type: cross \nAbstract: The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI."
      },
      {
        "id": "oai:arXiv.org:2504.02627v1",
        "title": "Incorporating the ChEES Criterion into Sequential Monte Carlo Samplers",
        "link": "https://arxiv.org/abs/2504.02627",
        "author": "Andrew Millard, Joshua Murphy, Daniel Frisch, Simon Maskell",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02627v1 Announce Type: cross \nAbstract: Markov chain Monte Carlo (MCMC) methods are a powerful but computationally expensive way of performing non-parametric Bayesian inference. MCMC proposals which utilise gradients, such as Hamiltonian Monte Carlo (HMC), can better explore the parameter space of interest if the additional hyper-parameters are chosen well. The No-U-Turn Sampler (NUTS) is a variant of HMC which is extremely effective at selecting these hyper-parameters but is slow to run and is not suited to GPU architectures. An alternative to NUTS, Change in the Estimator of the Expected Square HMC (ChEES-HMC) was shown not only to run faster than NUTS on GPU but also sample from posteriors more efficiently. Sequential Monte Carlo (SMC) samplers are another sampling method which instead output weighted samples from the posterior. They are very amenable to parallelisation and therefore being run on GPUs while having additional flexibility in their choice of proposal over MCMC. We incorporate (ChEEs-HMC) as a proposal into SMC samplers and demonstrate competitive but faster performance than NUTS on a number of tasks."
      },
      {
        "id": "oai:arXiv.org:2504.02628v1",
        "title": "Towards Computation- and Communication-efficient Computational Pathology",
        "link": "https://arxiv.org/abs/2504.02628",
        "author": "Chu Han, Bingchao Zhao, Jiatai Lin, Shanshan Lyu, Longfei Wang, Tianpeng Deng, Cheng Lu, Changhong Liang, Hannah Y. Wen, Xiaojing Guo, Zhenwei Shi, Zaiyi Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02628v1 Announce Type: cross \nAbstract: Despite the impressive performance across a wide range of applications, current computational pathology models face significant diagnostic efficiency challenges due to their reliance on high-magnification whole-slide image analysis. This limitation severely compromises their clinical utility, especially in time-sensitive diagnostic scenarios and situations requiring efficient data transfer. To address these issues, we present a novel computation- and communication-efficient framework called Magnification-Aligned Global-Local Transformer (MAGA-GLTrans). Our approach significantly reduces computational time, file transfer requirements, and storage overhead by enabling effective analysis using low-magnification inputs rather than high-magnification ones. The key innovation lies in our proposed magnification alignment (MAGA) mechanism, which employs self-supervised learning to bridge the information gap between low and high magnification levels by effectively aligning their feature representations. Through extensive evaluation across various fundamental CPath tasks, MAGA-GLTrans demonstrates state-of-the-art classification performance while achieving remarkable efficiency gains: up to 10.7 times reduction in computational time and over 20 times reduction in file transfer and storage requirements. Furthermore, we highlight the versatility of our MAGA framework through two significant extensions: (1) its applicability as a feature extractor to enhance the efficiency of any CPath architecture, and (2) its compatibility with existing foundation models and histopathology-specific encoders, enabling them to process low-magnification inputs with minimal information loss. These advancements position MAGA-GLTrans as a particularly promising solution for time-sensitive applications, especially in the context of intraoperative frozen section diagnosis where both accuracy and efficiency are paramount."
      },
      {
        "id": "oai:arXiv.org:2504.02643v1",
        "title": "A Dynamic, Ordinal Gaussian Process Item Response Theoretic Model",
        "link": "https://arxiv.org/abs/2504.02643",
        "author": "Yehu Chen, Jacob Montgomery, Roman Garnett",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02643v1 Announce Type: cross \nAbstract: Social scientists are often interested in using ordinal indicators to estimate latent traits that change over time. Frequently, this is done with item response theoretic (IRT) models that describe the relationship between those latent traits and observed indicators. We combine recent advances in Bayesian nonparametric IRT, which makes minimal assumptions on shapes of item response functions, and Gaussian process time series methods to capture dynamic structures in latent traits from longitudinal observations. We propose a generalized dynamic Gaussian process item response theory (GD-GPIRT) as well as a Markov chain Monte Carlo sampling algorithm for estimation of both latent traits and response functions. We evaluate GD-GPIRT in simulation studies against baselines in dynamic IRT, and apply it to various substantive studies, including assessing public opinions on economy environment and congressional ideology related to abortion debate."
      },
      {
        "id": "oai:arXiv.org:2504.02647v1",
        "title": "Adaptive Frequency Enhancement Network for Remote Sensing Image Semantic Segmentation",
        "link": "https://arxiv.org/abs/2504.02647",
        "author": "Feng Gao, Miao Fu, Jingchao Cao, Junyu Dong, Qian Du",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02647v1 Announce Type: cross \nAbstract: Semantic segmentation of high-resolution remote sensing images plays a crucial role in land-use monitoring and urban planning. Recent remarkable progress in deep learning-based methods makes it possible to generate satisfactory segmentation results. However, existing methods still face challenges in adapting network parameters to various land cover distributions and enhancing the interaction between spatial and frequency domain features. To address these challenges, we propose the Adaptive Frequency Enhancement Network (AFENet), which integrates two key components: the Adaptive Frequency and Spatial feature Interaction Module (AFSIM) and the Selective feature Fusion Module (SFM). AFSIM dynamically separates and modulates high- and low-frequency features according to the content of the input image. It adaptively generates two masks to separate high- and low-frequency components, therefore providing optimal details and contextual supplementary information for ground object feature representation. SFM selectively fuses global context and local detailed features to enhance the network's representation capability. Hence, the interactions between frequency and spatial features are further enhanced. Extensive experiments on three publicly available datasets demonstrate that the proposed AFENet outperforms state-of-the-art methods. In addition, we also validate the effectiveness of AFSIM and SFM in managing diverse land cover types and complex scenarios. Our codes are available at https://github.com/oucailab/AFENet."
      },
      {
        "id": "oai:arXiv.org:2504.02648v1",
        "title": "Controlled Social Learning: Altruism vs. Bias",
        "link": "https://arxiv.org/abs/2504.02648",
        "author": "Raghu Arghal, Kevin He, Shirin Saeedi Bidokhti, Saswati Sarkar",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02648v1 Announce Type: cross \nAbstract: We introduce a model of sequential social learning in which a planner may pay a cost to adjust the private signal precision of some agents. This framework presents a new optimization problem for social learning that sheds light on practical policy questions, such as how the socially optimal level of ad personalization changes according to current beliefs or how a biased planner might derail social learning. We then characterize the optimal policies of an altruistic planner who maximizes social welfare and a biased planner who seeks to induce a specific action. Even for a planner who has equivalent knowledge to an individual, cannot lie or cherry-pick information, and is fully observable, we demonstrate that it can dramatically influence social welfare in both positive and negative directions. An important area for future exploration is how one might prevent these latter outcomes to protect against the manipulation of social learning."
      },
      {
        "id": "oai:arXiv.org:2504.02670v1",
        "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
        "link": "https://arxiv.org/abs/2504.02670",
        "author": "Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, J\\'on Gunnar Hannesson, Grzegorz Kwa\\'sniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02670v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants."
      },
      {
        "id": "oai:arXiv.org:2504.02688v1",
        "title": "Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL",
        "link": "https://arxiv.org/abs/2504.02688",
        "author": "Achilles Kiwanuka Machumilane, Alberto Gotta, Pietro Cassar\\`a",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02688v1 Announce Type: cross \nAbstract: Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings. Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks. However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands. In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \\textit{flight time, handover, connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR. We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment. The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms."
      },
      {
        "id": "oai:arXiv.org:2504.02694v1",
        "title": "Semiparametric Counterfactual Regression",
        "link": "https://arxiv.org/abs/2504.02694",
        "author": "Kwangho Kim",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02694v1 Announce Type: cross \nAbstract: We study counterfactual regression, which aims to map input features to outcomes under hypothetical scenarios that differ from those observed in the data. This is particularly useful for decision-making when adapting to sudden shifts in treatment patterns is essential. We propose a doubly robust-style estimator for counterfactual regression within a generalizable framework that accommodates a broad class of risk functions and flexible constraints, drawing on tools from semiparametric theory and stochastic optimization. Our approach uses incremental interventions to enhance adaptability while maintaining consistency with standard methods. We formulate the target estimand as the optimal solution to a stochastic optimization problem and develop an efficient estimation strategy, where we can leverage rapid development of modern optimization algorithms. We go on to analyze the rates of convergence and characterize the asymptotic distributions. Our analysis shows that the proposed estimators can achieve $\\sqrt{n}$-consistency and asymptotic normality for a broad class of problems. Numerical illustrations highlight their effectiveness in adapting to unseen counterfactual scenarios while maintaining parametric convergence rates."
      },
      {
        "id": "oai:arXiv.org:2504.02723v1",
        "title": "Computing High-dimensional Confidence Sets for Arbitrary Distributions",
        "link": "https://arxiv.org/abs/2504.02723",
        "author": "Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02723v1 Announce Type: cross \nAbstract: We study the problem of learning a high-density region of an arbitrary distribution over $\\mathbb{R}^d$. Given a target coverage parameter $\\delta$, and sample access to an arbitrary distribution $D$, we want to output a confidence set $S \\subset \\mathbb{R}^d$ such that $S$ achieves $\\delta$ coverage of $D$, i.e., $\\mathbb{P}_{y \\sim D} \\left[ y \\in S \\right] \\ge \\delta$, and the volume of $S$ is as small as possible. This is a central problem in high-dimensional statistics with applications in finding confidence sets, uncertainty quantification, and support estimation.\n  In the most general setting, this problem is statistically intractable, so we restrict our attention to competing with sets from a concept class $C$ with bounded VC-dimension. An algorithm is competitive with class $C$ if, given samples from an arbitrary distribution $D$, it outputs in polynomial time a set that achieves $\\delta$ coverage of $D$, and whose volume is competitive with the smallest set in $C$ with the required coverage $\\delta$. This problem is computationally challenging even in the basic setting when $C$ is the set of all Euclidean balls. Existing algorithms based on coresets find in polynomial time a ball whose volume is $\\exp(\\tilde{O}( d/ \\log d))$-factor competitive with the volume of the best ball.\n  Our main result is an algorithm that finds a confidence set whose volume is $\\exp(\\tilde{O}(d^{2/3}))$ factor competitive with the optimal ball having the desired coverage. The algorithm is improper (it outputs an ellipsoid). Combined with our computational intractability result for proper learning balls within an $\\exp(\\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results provide an interesting separation between proper and (improper) learning of confidence sets."
      },
      {
        "id": "oai:arXiv.org:2504.02735v1",
        "title": "Pushing the Limit of PPG Sensing in Sedentary Conditions by Addressing Poor Skin-sensor Contact",
        "link": "https://arxiv.org/abs/2504.02735",
        "author": "Manh Pham Hung, Matthew Yiwen Ho, Yiming Zhang, Dimitris Spathis, Aaqib Saeed, Dong Ma",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02735v1 Announce Type: cross \nAbstract: Photoplethysmography (PPG) is a widely used non-invasive technique for monitoring cardiovascular health and various physiological parameters on consumer and medical devices. While motion artifacts are well-known challenges in dynamic settings, suboptimal skin-sensor contact in sedentary conditions - a critical issue often overlooked in existing literature - can distort PPG signal morphology, leading to the loss or shift of essential waveform features and therefore degrading sensing performance. In this work, we propose CP-PPG, a novel approach that transforms Contact Pressure-distorted PPG signals into ones with the ideal morphology. CP-PPG incorporates a novel data collection approach, a well-crafted signal processing pipeline, and an advanced deep adversarial model trained with a custom PPG-aware loss function. We validated CP-PPG through comprehensive evaluations, including 1) morphology transformation performance on our self-collected dataset, 2) downstream physiological monitoring performance on public datasets, and 3) in-the-wild performance. Extensive experiments demonstrate substantial and consistent improvements in signal fidelity (Mean Absolute Error: 0.09, 40% improvement over the original signal) as well as downstream performance across all evaluations in Heart Rate (HR), Heart Rate Variability (HRV), Respiration Rate (RR), and Blood Pressure (BP) estimation (on average, 21% improvement in HR; 41-46% in HRV; 6% in RR; and 4-5% in BP). These findings highlight the critical importance of addressing skin-sensor contact issues for accurate and dependable PPG-based physiological monitoring. Furthermore, CP-PPG can serve as a generic, plug-in API to enhance PPG signal quality."
      },
      {
        "id": "oai:arXiv.org:2504.02737v1",
        "title": "RBR4DNN: Requirements-based Testing of Neural Networks",
        "link": "https://arxiv.org/abs/2504.02737",
        "author": "Nusrat Jahan Mozumder, Felipe Toledo, Swaroopa Dola, Matthew B. Dwyer",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02737v1 Announce Type: cross \nAbstract: Deep neural network (DNN) testing is crucial for the reliability and safety of critical systems, where failures can have severe consequences. Although various techniques have been developed to create robustness test suites, requirements-based testing for DNNs remains largely unexplored -- yet such tests are recognized as an essential component of software validation of critical systems. In this work, we propose a requirements-based test suite generation method that uses structured natural language requirements formulated in a semantic feature space to create test suites by prompting text-conditional latent diffusion models with the requirement precondition and then using the associated postcondition to define a test oracle to judge outputs of the DNN under test. We investigate the approach using fine-tuned variants of pre-trained generative models. Our experiments on the MNIST, CelebA-HQ, ImageNet, and autonomous car driving datasets demonstrate that the generated test suites are realistic, diverse, consistent with preconditions, and capable of revealing faults."
      },
      {
        "id": "oai:arXiv.org:2504.02748v1",
        "title": "Atrial constitutive neural networks",
        "link": "https://arxiv.org/abs/2504.02748",
        "author": "Mathias Peirlinck, Kevin Linka, Ellen Kuhl",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02748v1 Announce Type: cross \nAbstract: This work presents a novel approach for characterizing the mechanical behavior of atrial tissue using constitutive neural networks. Based on experimental biaxial tensile test data of healthy human atria, we automatically discover the most appropriate constitutive material model, thereby overcoming the limitations of traditional, pre-defined models. This approach offers a new perspective on modeling atrial mechanics and is a significant step towards improved simulation and prediction of cardiac health."
      },
      {
        "id": "oai:arXiv.org:2504.02767v1",
        "title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
        "link": "https://arxiv.org/abs/2504.02767",
        "author": "Andres Algaba, Vincent Holst, Floriano Tori, Melika Mobini, Brecht Verbeken, Sylvia Wenmackers, Vincent Ginis",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02767v1 Announce Type: cross \nAbstract: The spread of scientific knowledge depends on how researchers discover and cite previous work. The adoption of large language models (LLMs) in the scientific research process introduces a new layer to these citation practices. However, it remains unclear to what extent LLMs align with human citation practices, how they perform across domains, and may influence citation dynamics. Here, we show that LLMs systematically reinforce the Matthew effect in citations by consistently favoring highly cited papers when generating references. This pattern persists across scientific domains despite significant field-specific variations in existence rates, which refer to the proportion of generated references that match existing records in external bibliometric databases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers, we find that LLM recommendations diverge from traditional citation patterns by preferring more recent references with shorter titles and fewer authors. Emphasizing their content-level relevance, the generated references are semantically aligned with the content of each paper at levels comparable to the ground truth references and display similar network effects while reducing author self-citations. These findings illustrate how LLMs may reshape citation practices and influence the trajectory of scientific discovery by reflecting and amplifying established trends. As LLMs become more integrated into the scientific research process, it is important to understand their role in shaping how scientific communities discover and build upon prior work."
      },
      {
        "id": "oai:arXiv.org:2504.02792v1",
        "title": "Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets",
        "link": "https://arxiv.org/abs/2504.02792",
        "author": "Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, Abhishek Gupta",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02792v1 Announce Type: cross \nAbstract: Imitation learning has emerged as a promising approach towards building generalist robots. However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations. Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation required for most contemporary methods. In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. We show that by simply controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator. Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling. Videos and code are available at https://weirdlabuw.github.io/uwm/."
      },
      {
        "id": "oai:arXiv.org:2504.02793v1",
        "title": "A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models",
        "link": "https://arxiv.org/abs/2504.02793",
        "author": "Gaurav Verma, Jiawei Zhou, Mohit Chandra, Srijan Kumar, Munmun De Choudhury",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02793v1 Announce Type: cross \nAbstract: Large artificial intelligence (AI) models have garnered significant attention for their remarkable, often \"superhuman\", performance on standardized benchmarks. However, when these models are deployed in high-stakes verticals such as healthcare, education, and law, they often reveal notable limitations. For instance, they exhibit brittleness to minor variations in input data, present contextually uninformed decisions in critical settings, and undermine user trust by confidently producing or reproducing inaccuracies. These challenges in applying large models necessitate cross-disciplinary innovations to align the models' capabilities with the needs of real-world applications. We introduce a framework that addresses this gap through a layer-wise abstraction of innovations aimed at meeting users' requirements with large models. Through multiple case studies, we illustrate how researchers and practitioners across various fields can operationalize this framework. Beyond modularizing the pipeline of transforming large models into useful \"vertical systems\", we also highlight the dynamism that exists within different layers of the framework. Finally, we discuss how our framework can guide researchers and practitioners to (i) optimally situate their innovations (e.g., when vertical-specific insights can empower broadly impactful vertical-agnostic innovations), (ii) uncover overlooked opportunities (e.g., spotting recurring problems across verticals to develop practically useful foundation models instead of chasing benchmarks), and (iii) facilitate cross-disciplinary communication of critical challenges (e.g., enabling a shared vocabulary for AI developers, domain experts, and human-computer interaction scholars)."
      },
      {
        "id": "oai:arXiv.org:2504.02822v1",
        "title": "Do Two AI Scientists Agree?",
        "link": "https://arxiv.org/abs/2504.02822",
        "author": "Xinghong Fu, Ziming Liu, Max Tegmark",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02822v1 Announce Type: cross \nAbstract: When two AI models are trained on the same scientific task, do they learn the same theory or two different theories? Throughout history of science, we have witnessed the rise and fall of theories driven by experimental validation or falsification: many theories may co-exist when experimental data is lacking, but the space of survived theories become more constrained with more experimental data becoming available. We show the same story is true for AI scientists. With increasingly more systems provided in training data, AI scientists tend to converge in the theories they learned, although sometimes they form distinct groups corresponding to different theories. To mechanistically interpret what theories AI scientists learn and quantify their agreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI Scientists, trained on standard problems in physics, aggregating training results across many seeds simulating the different configurations of AI scientists. Our findings suggests for AI scientists switch from learning a Hamiltonian theory in simple setups to a Lagrangian formulation when more complex systems are introduced. We also observe strong seed dependence of the training dynamics and final learned weights, controlling the rise and fall of relevant theories. We finally demonstrate that not only can our neural networks aid interpretability, it can also be applied to higher dimensional problems."
      },
      {
        "id": "oai:arXiv.org:2305.06361v4",
        "title": "Efficient Training of Multi-task Neural Solver for Combinatorial Optimization",
        "link": "https://arxiv.org/abs/2305.06361",
        "author": "Chenguang Wang, Zhang-Hua Fu, Pinyan Lu, Tianshu Yu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.06361v4 Announce Type: replace \nAbstract: Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. Naive application of conventional multi-task learning approaches often falls short in delivering a high-quality, unified neural solver. This deficiency primarily stems from the significant computational demands and a lack of adequate consideration for the complexities inherent in COPs. In this paper, we propose a general and efficient training paradigm to deliver a unified combinatorial multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. By employing theoretically grounded approximations, our method significantly enhances overall performance, regardless of whether it is within constrained training budgets, across equivalent training epochs, or in terms of generalization capabilities, when compared to conventional training schedules. On the real-world datasets of TSPLib and CVRPLib, our method also achieved the best results compared to single task learning and multi-task learning approaches. Additionally, the influence matrix provides empirical evidence supporting common practices in the field of learning to optimize, further substantiating the effectiveness of our approach. Our code is open-sourced and available at https://github.com/LOGO-CUHKSZ/MTL-COP."
      },
      {
        "id": "oai:arXiv.org:2306.03819v4",
        "title": "LEACE: Perfect linear concept erasure in closed form",
        "link": "https://arxiv.org/abs/2306.03819",
        "author": "Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2306.03819v4 Announce Type: replace \nAbstract: Concept erasure aims to remove specified features from an embedding. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the embedding as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called \"concept scrubbing,\" which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure."
      },
      {
        "id": "oai:arXiv.org:2311.16176v5",
        "title": "Mitigating Shortcut Learning with Diffusion Counterfactuals and Diverse Ensembles",
        "link": "https://arxiv.org/abs/2311.16176",
        "author": "Luca Scimeca, Alexander Rubinstein, Damien Teney, Seong Joon Oh, Yoshua Bengio",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.16176v5 Announce Type: replace \nAbstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut learning, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose DiffDiv an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalization and diversification on par with prior work that relies on auxiliary data collection."
      },
      {
        "id": "oai:arXiv.org:2312.12050v2",
        "title": "Extension of the Dip-test Repertoire -- Efficient and Differentiable p-value Calculation for Clustering",
        "link": "https://arxiv.org/abs/2312.12050",
        "author": "Lena G. M. Bauer, Collin Leiber, Christian B\\\"ohm, Claudia Plant",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2312.12050v2 Announce Type: replace \nAbstract: Over the last decade, the Dip-test of unimodality has gained increasing interest in the data mining community as it is a parameter-free statistical test that reliably rates the modality in one-dimensional samples. It returns a so called Dip-value and a corresponding probability for the sample's unimodality (Dip-p-value). These two values share a sigmoidal relationship. However, the specific transformation is dependent on the sample size. Many Dip-based clustering algorithms use bootstrapped look-up tables translating Dip- to Dip-p-values for a certain limited amount of sample sizes. We propose a specifically designed sigmoid function as a substitute for these state-of-the-art look-up tables. This accelerates computation and provides an approximation of the Dip- to Dip-p-value transformation for every single sample size. Further, it is differentiable and can therefore easily be integrated in learning schemes using gradient descent. We showcase this by exploiting our function in a novel subspace clustering algorithm called Dip'n'Sub. We highlight in extensive experiments the various benefits of our proposal."
      },
      {
        "id": "oai:arXiv.org:2401.15610v2",
        "title": "Prevalidated ridge regression is a highly-efficient drop-in replacement for logistic regression for high-dimensional data",
        "link": "https://arxiv.org/abs/2401.15610",
        "author": "Angus Dempster, Geoffrey I. Webb, Daniel F. Schmidt",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2401.15610v2 Announce Type: replace \nAbstract: Logistic regression is a ubiquitous method for probabilistic classification. However, the effectiveness of logistic regression depends upon careful and relatively computationally expensive tuning, especially for the regularisation hyperparameter, and especially in the context of high-dimensional data. We present a prevalidated ridge regression model that closely matches logistic regression in terms of classification error and log-loss, particularly for high-dimensional data, while being significantly more computationally efficient and having effectively no hyperparameters beyond regularisation. We scale the coefficients of the model so as to minimise log-loss for a set of prevalidated predictions derived from the estimated leave-one-out cross-validation error. This exploits quantities already computed in the course of fitting the ridge regression model in order to find the scaling parameter with nominal additional computational expense."
      },
      {
        "id": "oai:arXiv.org:2402.02791v4",
        "title": "PanGu-$\\pi$ Pro:Rethinking Optimization and Architecture for Tiny Language Models",
        "link": "https://arxiv.org/abs/2402.02791",
        "author": "Yehui Tang, Kai Han, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Yunhe Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.02791v4 Announce Type: replace \nAbstract: The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, \\ie, neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas. Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\\pi$-1B Pro. Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance. The code is available at https://github.com/YuchuanTian/RethinkTinyLM."
      },
      {
        "id": "oai:arXiv.org:2402.16442v3",
        "title": "On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions",
        "link": "https://arxiv.org/abs/2402.16442",
        "author": "Maximilian B\\\"other, Abraham Sebastian, Pranjal Awasthi, Ana Klimovic, Srikumar Ramalingam",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.16442v3 Announce Type: replace \nAbstract: Modern datasets span billions of samples, making training on all available data infeasible. Selecting a high quality subset helps in reducing training costs and enhancing model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving such subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset in DRAM. At billion datapoint scale, even the subset may not fit a single machine, and the sequential algorithms are prohibitively slow. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, partition-based distributed greedy algorithm to identify the remaining subset. We discuss how to implement these algorithms in a distributed data processing framework and empirically analyze different configurations. We find high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in quality compared to centralized methods, and scale to a dataset with 13 billion points."
      },
      {
        "id": "oai:arXiv.org:2403.16075v2",
        "title": "IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History",
        "link": "https://arxiv.org/abs/2403.16075",
        "author": "Yi Xu, Weiran Shen, Xiao Zhang, Jun Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.16075v2 Announce Type: replace \nAbstract: Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming problem by utilizing the behavioral evolution history of the batched contextual bandit with inaccessible rewards. We demonstrate that IBCB is a unified framework for both deterministic and randomized bandit policies. The experimental results indicate that IBCB outperforms several existing imitation learning algorithms on synthetic and real-world data and significantly reduces running time. Additionally, empirical analyses reveal that IBCB exhibits better out-of-distribution generalization and is highly effective in learning the bandit policy from the interaction history of novice experts."
      },
      {
        "id": "oai:arXiv.org:2403.16843v4",
        "title": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games",
        "link": "https://arxiv.org/abs/2403.16843",
        "author": "Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.16843v4 Announce Type: replace \nAbstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \\emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \\emph{unsupervised} training loss of \\emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases."
      },
      {
        "id": "oai:arXiv.org:2403.17651v2",
        "title": "Exploring Dynamic Transformer for Efficient Object Tracking",
        "link": "https://arxiv.org/abs/2403.17651",
        "author": "Jiawen Zhu, Xin Chen, Haiwen Diao, Shuai Li, Jun-Yan He, Chenyang Li, Bin Luo, Dong Wang, Huchuan Lu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.17651v2 Announce Type: replace \nAbstract: The speed-precision trade-off is a critical problem for visual object tracking which usually requires low latency and deployment on constrained resources. Existing solutions for efficient tracking mainly focus on adopting light-weight backbones or modules, which nevertheless come at the cost of a sacrifice in precision. In this paper, inspired by dynamic network routing, we propose DyTrack, a dynamic transformer framework for efficient tracking. Real-world tracking scenarios exhibit diverse levels of complexity. We argue that a simple network is sufficient for easy frames in video sequences, while more computation could be assigned to difficult ones. DyTrack automatically learns to configure proper reasoning routes for various inputs, gaining better utilization of the available computational budget. Thus, it can achieve higher performance with the same running speed. We formulate instance-specific tracking as a sequential decision problem and attach terminating branches to intermediate layers of the entire model. Especially, to fully utilize the computations, we introduce the feature recycling mechanism to reuse the outputs of predecessors. Furthermore, a target-aware self-distillation strategy is designed to enhance the discriminating capabilities of early predictions by effectively mimicking the representation pattern of the deep model. Extensive experiments on multiple benchmarks demonstrate that DyTrack achieves promising speed-precision trade-offs with only a single model. For instance, DyTrack obtains 64.9% AUC on LaSOT with a speed of 256 fps."
      },
      {
        "id": "oai:arXiv.org:2404.00699v4",
        "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
        "link": "https://arxiv.org/abs/2404.00699",
        "author": "Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.00699v4 Announce Type: replace \nAbstract: With the rise of Large Language Models (LLMs) in recent years, abundant new opportunities are emerging, but also new challenges, among which contamination is quickly becoming critical. Business applications and fundraising in Artificial Intelligence (AI) have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a major issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes real capability improvement in the field of NLP, yet, there remains a lack of methods on how to efficiently detect contamination. In this paper, we survey all recent work on contamination detection with LLMs, analyzing their methodologies and use cases to shed light on the appropriate usage of contamination detection methods. Our work calls the NLP research community's attention into systematically taking into account contamination bias in LLM evaluation."
      },
      {
        "id": "oai:arXiv.org:2404.02865v3",
        "title": "End-To-End Self-Tuning Self-Supervised Time Series Anomaly Detection",
        "link": "https://arxiv.org/abs/2404.02865",
        "author": "Boje Deforce, Meng-Chieh Lee, Bart Baesens, Estefan\\'ia Serral Asensio, Jaemin Yoo, Leman Akoglu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.02865v3 Announce Type: replace \nAbstract: Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA \"on autoPilot\", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type. Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters. In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types."
      },
      {
        "id": "oai:arXiv.org:2404.07773v5",
        "title": "ConsistencyDet: A Few-step Denoising Framework for Object Detection Using the Consistency Model",
        "link": "https://arxiv.org/abs/2404.07773",
        "author": "Lifan Jiang, Zhihui Wang, Changmiao Wang, Ming Li, Jiaxu Leng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.07773v5 Announce Type: replace \nAbstract: Object detection, a quintessential task in the realm of perceptual computing, can be tackled using a generative methodology. In the present study, we introduce a novel framework designed to articulate object detection as a denoising diffusion process, which operates on the perturbed bounding boxes of annotated entities. This framework, termed \\textbf{ConsistencyDet}, leverages an innovative denoising concept known as the Consistency Model. The hallmark of this model is its self-consistency feature, which empowers the model to map distorted information from any time step back to its pristine state, thereby realizing a \\textbf{``few-step denoising''} mechanism. Such an attribute markedly elevates the operational efficiency of the model, setting it apart from the conventional Diffusion Model. Throughout the training phase, ConsistencyDet initiates the diffusion sequence with noise-infused boxes derived from the ground-truth annotations and conditions the model to perform the denoising task. Subsequently, in the inference stage, the model employs a denoising sampling strategy that commences with bounding boxes randomly sampled from a normal distribution. Through iterative refinement, the model transforms an assortment of arbitrarily generated boxes into definitive detections. Comprehensive evaluations employing standard benchmarks, such as MS-COCO and LVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in performance metrics. Our code is available at https://anonymous.4open.science/r/ConsistencyDet-37D5."
      },
      {
        "id": "oai:arXiv.org:2404.12494v3",
        "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
        "link": "https://arxiv.org/abs/2404.12494",
        "author": "Yu Feng, Ben Zhou, Weidong Lin, Dan Roth",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.12494v3 Announce Type: replace \nAbstract: Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision-making and planning tasks. Current large language models (LLMs) are insufficient for accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts. In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation. We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step. We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines. These estimates further contribute to better and more trustworthy decision making."
      },
      {
        "id": "oai:arXiv.org:2404.14745v5",
        "title": "You Think, You ACT: The New Task of Arbitrary Text to Motion Generation",
        "link": "https://arxiv.org/abs/2404.14745",
        "author": "Runqi Wang, Caoyuan Ma, Guopeng Li, Hanrui Xu, Yuke Li, Zheng Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.14745v5 Announce Type: replace \nAbstract: Text to Motion aims to generate human motions from texts. Existing settings rely on limited Action Texts that include action labels, which limits flexibility and practicability in scenarios difficult to describe directly. This paper extends limited Action Texts to arbitrary ones. Scene texts without explicit action labels can enhance the practicality of models in complex and diverse industries such as virtual human interaction, robot behavior generation, and film production, while also supporting the exploration of potential implicit behavior patterns. However, newly introduced Scene Texts may yield multiple reasonable output results, causing significant challenges in existing data, framework, and evaluation. To address this practical issue, we first create a new dataset HUMANML3D++ by extending texts of the largest existing dataset HUMANML3D. Secondly, we propose a simple yet effective framework that extracts action instructions from arbitrary texts and subsequently generates motions. Furthermore, we also benchmark this new setting with multi-solution metrics to address the inadequacies of existing single-solution metrics. Extensive experiments indicate that Text to Motion in this realistic setting is challenging, fostering new research in this practical direction."
      },
      {
        "id": "oai:arXiv.org:2405.05256v2",
        "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2405.05256",
        "author": "Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, Stefano Soatto",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.05256v2 Announce Type: replace \nAbstract: Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \"Type I hallucinations\". Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term \"Type II hallucinations\". Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline. Code is now available at https://github.com/amazon-science/THRONE ."
      },
      {
        "id": "oai:arXiv.org:2405.08044v3",
        "title": "On the Volatility of Shapley-Based Contribution Metrics in Federated Learning",
        "link": "https://arxiv.org/abs/2405.08044",
        "author": "Arno Geimer, Beltran Fiz, Radu State",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.08044v3 Announce Type: replace \nAbstract: Federated learning (FL) is a collaborative and privacy-preserving Machine Learning paradigm, allowing the development of robust models without the need to centralize sensitive data. A critical challenge in FL lies in fairly and accurately allocating contributions from diverse participants. Inaccurate allocation can undermine trust, lead to unfair compensation, and thus participants may lack the incentive to join or actively contribute to the federation. Various remuneration strategies have been proposed to date, including auction-based approaches and Shapley-value-based methods, the latter offering a means to quantify the contribution of each participant. However, little to no work has studied the stability of these contribution evaluation methods. In this paper, we evaluate participant contributions in federated learning using gradient-based model reconstruction techniques with Shapley values and compare the round-based contributions to a classic data contribution measurement scheme. We provide an extensive analysis of the discrepancies of Shapley values across a set of aggregation strategies, and examine them on an overall and a per-client level. We show that, between different aggregation techniques, Shapley values lead to unstable reward allocations among participants. Our analysis spans various data heterogeneity distributions, including independent and identically distributed (IID) and non-IID scenarios."
      },
      {
        "id": "oai:arXiv.org:2405.10347v4",
        "title": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey",
        "link": "https://arxiv.org/abs/2405.10347",
        "author": "Jing Liu, Yang Liu, Jieyu Lin, Jielin Li, Liang Cao, Peng Sun, Bo Hu, Liang Song, Azzedine Boukerche, Victor C. M. Leung",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.10347v4 Announce Type: replace \nAbstract: The increasing utilization of surveillance cameras in smart cities, coupled with the surge of online video applications, has heightened concerns regarding public security and privacy protection, which propelled automated Video Anomaly Detection (VAD) into a fundamental research task within the Artificial Intelligence (AI) community. With the advancements in deep learning and edge computing, VAD has made significant progress and advances synergized with emerging applications in smart cities and video internet, which has moved beyond the conventional research scope of algorithm engineering to deployable Networking Systems for VAD (NSVAD), a practical hotspot for intersection exploration in the AI, IoVT, and computing fields. In this article, we delineate the foundational assumptions, learning frameworks, and applicable scenarios of various deep learning-driven VAD routes, offering an exhaustive tutorial for novices in NSVAD. In addition, this article elucidates core concepts by reviewing recent advances and typical solutions and aggregating available research resources accessible at https://github.com/fdjingliu/NSVAD. Lastly, this article projects future development trends and discusses how the integration of AI and computing technologies can address existing research challenges and promote open opportunities, serving as an insightful guide for prospective researchers and engineers."
      },
      {
        "id": "oai:arXiv.org:2405.11573v3",
        "title": "Quantile Activation: Correcting a Failure Mode of ML Models",
        "link": "https://arxiv.org/abs/2405.11573",
        "author": "Aditya Challa, Sravan Danda, Laurent Najman, Snehanshu Saha",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.11573v3 Announce Type: replace \nAbstract: Standard ML models fail to infer the context distribution and suitably adapt. For instance, the learning fails when the underlying distribution is actually a mixture of distributions with contradictory labels. Learning also fails if there is a shift between train and test distributions. Standard neural network architectures like MLPs or CNNs are not equipped to handle this.\n  In this article, we propose a simple activation function, quantile activation (QAct), that addresses this problem without significantly increasing computational costs. The core idea is to \"adapt\" the outputs of each neuron to its context distribution. The proposed quantile activation (QAct) outputs the relative quantile position of neuron activations within their context distribution, diverging from the direct numerical outputs common in traditional networks.\n  A specific case of the above failure mode is when there is an inherent distribution shift, i.e the test distribution differs slightly from the train distribution. We validate the proposed activation function under covariate shifts, using datasets designed to test robustness against distortions. Our results demonstrate significantly better generalization across distortions compared to conventional classifiers and other adaptive methods, across various architectures. Although this paper presents a proof of concept, we find that this approach unexpectedly outperforms DINOv2 (small), despite DINOv2 being trained with a much larger network and dataset."
      },
      {
        "id": "oai:arXiv.org:2405.12084v2",
        "title": "Distributional Semantics, Holism, and the Instability of Meaning",
        "link": "https://arxiv.org/abs/2405.12084",
        "author": "Jumbly Grindrod, J. D. Porter, Nat Hansen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.12084v2 Announce Type: replace \nAbstract: Large Language Models are built on the so-called distributional semantic approach to linguistic meaning that has the distributional hypothesis at its core. The distributional hypothesis involves a holistic conception of word meaning: the meaning of a word depends upon its relations to other words in the model. A standard objection to holism is the charge of instability: any change in the meaning properties of a linguistic system (a human speaker, for example) would lead to many changes or a complete change in the entire system. We examine whether the instability objection poses a problem for distributional models of meaning. First, we distinguish between distinct forms of instability that these models could exhibit, and argue that only one such form is relevant for understanding the relation between instability and communication: what we call differential instability. Differential instability is variation in the relative distances between points in a space, rather than variation in the absolute position of those points. We distinguish differential and absolute instability by constructing two of our own smaller language models. We demonstrate the two forms of instability by showing these models change as the corpora they are constructed from increase in size. We argue that the instability that these models display is constrained by the structure and scale of relationships between words, such that the resistance to change for a word is roughly proportional to its frequent and consistent use within the language system. The differential instability that language models exhibit allows for productive forms of meaning change while not leading to the problems raised by the instability objection."
      },
      {
        "id": "oai:arXiv.org:2405.14672v2",
        "title": "Invisible Backdoor Attack against Self-supervised Learning",
        "link": "https://arxiv.org/abs/2405.14672",
        "author": "Hanrong Zhang, Zhenting Wang, Boheng Li, Fulin Lin, Tingxu Han, Mingyu Jin, Chenlu Zhan, Mengnan Du, Hongwei Wang, Shiqing Ma",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.14672v2 Announce Type: replace \nAbstract: Self-supervised learning (SSL) models are vulnerable to backdoor attacks. Existing backdoor attacks that are effective in SSL often involve noticeable triggers, like colored patches or visible noise, which are vulnerable to human inspection. This paper proposes an imperceptible and effective backdoor attack against self-supervised models. We first find that existing imperceptible triggers designed for supervised learning are less effective in compromising self-supervised models. We then identify this ineffectiveness is attributed to the overlap in distributions between the backdoor and augmented samples used in SSL. Building on this insight, we design an attack using optimized triggers disentangled with the augmented transformation in the SSL, while remaining imperceptible to human vision. Experiments on five datasets and six SSL algorithms demonstrate our attack is highly effective and stealthy. It also has strong resistance to existing backdoor defenses. Our code can be found at https://github.com/Zhang-Henry/INACTIVE."
      },
      {
        "id": "oai:arXiv.org:2405.16738v3",
        "title": "CARL: A Framework for Equivariant Image Registration",
        "link": "https://arxiv.org/abs/2405.16738",
        "author": "Hastings Greer, Lin Tian, Francois-Xavier Vialard, Roland Kwitt, Raul San Jose Estepar, Marc Niethammer",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.16738v3 Announce Type: replace \nAbstract: Image registration estimates spatial correspondences between a pair of images. These estimates are typically obtained via numerical optimization or regression by a deep network. A desirable property of such estimators is that a correspondence estimate (e.g., the true oracle correspondence) for an image pair is maintained under deformations of the input images. Formally, the estimator should be equivariant to a desired class of image transformations. In this work, we present careful analyses of the desired equivariance properties in the context of multi-step deep registration networks. Based on these analyses we 1) introduce the notions of $[U,U]$ equivariance (network equivariance to the same deformations of the input images) and $[W,U]$ equivariance (where input images can undergo different deformations); we 2) show that in a suitable multi-step registration setup it is sufficient for overall $[W,U]$ equivariance if the first step has $[W,U]$ equivariance and all others have $[U,U]$ equivariance; we 3) show that common displacement-predicting networks only exhibit $[U,U]$ equivariance to translations instead of the more powerful $[W,U]$ equivariance; and we 4) show how to achieve multi-step $[W,U]$ equivariance via a coordinate-attention mechanism combined with displacement-predicting refinement layers (CARL). Overall, our approach obtains excellent practical registration performance on several 3D medical image registration tasks and outperforms existing unsupervised approaches for the challenging problem of abdomen registration."
      },
      {
        "id": "oai:arXiv.org:2406.02424v2",
        "title": "Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints",
        "link": "https://arxiv.org/abs/2406.02424",
        "author": "Zifeng Zhao, Feiyu Jiang, Yi Yu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.02424v2 Announce Type: replace \nAbstract: We study contextual dynamic pricing problems where a firm sells products to $T$ sequentially-arriving consumers, behaving according to an unknown demand model. The firm aims to minimize its regret over a clairvoyant that knows the model in advance. The demand follows a generalized linear model (GLM), allowing for stochastic feature vectors in $\\mathbb R^d$ encoding product and consumer information. We first show the optimal regret is of order $\\sqrt{dT}$, up to logarithmic factors, improving existing upper bounds by a $\\sqrt{d}$ factor. This optimal rate is materialized by two algorithms: a confidence bound-type algorithm and an explore-then-commit (ETC) algorithm. A key insight is an intrinsic connection between dynamic pricing and contextual multi-armed bandit problems with many arms with a careful discretization. We further study contextual dynamic pricing under local differential privacy (LDP) constraints. We propose a stochastic gradient descent-based ETC algorithm achieving regret upper bounds of order $d\\sqrt{T}/\\epsilon$, up to logarithmic factors, where $\\epsilon>0$ is the privacy parameter. The upper bounds with and without LDP constraints are matched by newly constructed minimax lower bounds, characterizing costs of privacy. Moreover, we extend our study to dynamic pricing under mixed privacy constraints, improving the privacy-utility tradeoff by leveraging public data. This is the first time such setting is studied in the dynamic pricing literature and our theoretical results seamlessly bridge dynamic pricing with and without LDP. Extensive numerical experiments and real data applications are conducted to illustrate the efficiency and practical value of our algorithms."
      },
      {
        "id": "oai:arXiv.org:2406.06777v5",
        "title": "MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension",
        "link": "https://arxiv.org/abs/2406.06777",
        "author": "Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.06777v5 Announce Type: replace \nAbstract: Large Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding. However, their proficiency within the chemistry domain remains restricted, especially in solving professional molecule-related tasks. This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e., SMILES strings. In this study, we seek to enhance the ability of LLMs to comprehend molecules by equipping them with a multi-modal external module, namely MolX. In particular, instead of directly using a SMILES string to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM. Moreover, a handcrafted molecular fingerprint is incorporated to leverage its embedded domain knowledge. Then, to establish an alignment between MolX and the LLM's textual input space, the whole model in which the LLM is frozen, is pre-trained with a versatile strategy including a diverse set of tasks. Experimental evaluations show that our proposed method outperforms baselines across 4 downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM, while only introducing a small number of trainable parameters 0.53% and 0.82%, respectively."
      },
      {
        "id": "oai:arXiv.org:2406.06965v4",
        "title": "Evolving from Single-modal to Multi-modal Facial Deepfake Detection: Progress and Challenges",
        "link": "https://arxiv.org/abs/2406.06965",
        "author": "Ping Liu, Qiqi Tao, Joey Tianyi Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.06965v4 Announce Type: replace \nAbstract: As synthetic media, including video, audio, and text, become increasingly indistinguishable from real content, the risks of misinformation, identity fraud, and social manipulation escalate. This survey traces the evolution of deepfake detection from early single-modal methods to sophisticated multi-modal approaches that integrate audio-visual and text-visual cues. We present a structured taxonomy of detection techniques and analyze the transition from GAN-based to diffusion model-driven deepfakes, which introduce new challenges due to their heightened realism and robustness against detection. Unlike prior surveys that primarily focus on single-modal detection or earlier deepfake techniques, this work provides the most comprehensive study to date, encompassing the latest advancements in multi-modal deepfake detection, generalization challenges, proactive defense mechanisms, and emerging datasets specifically designed to support new interpretability and reasoning tasks. We further explore the role of Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) in strengthening detection robustness against increasingly sophisticated deepfake attacks. By systematically categorizing existing methods and identifying emerging research directions, this survey serves as a foundation for future advancements in combating AI-generated facial forgeries. A curated list of all related papers can be found at \\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}."
      },
      {
        "id": "oai:arXiv.org:2406.10891v3",
        "title": "Noisy Annotations in Semantic Segmentation",
        "link": "https://arxiv.org/abs/2406.10891",
        "author": "Moshe Kimhi, Omer Kerem, Eden Grad, Ehud Rivlin, Chaim Baskin",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.10891v3 Announce Type: replace \nAbstract: Obtaining accurate labels for instance segmentation is particularly challenging due to the complex nature of the task. Each image necessitates multiple annotations, encompassing not only the object class but also its precise spatial boundaries. These requirements elevate the likelihood of errors and inconsistencies in both manual and automated annotation processes. By simulating different noise conditions, we provide a realistic scenario for assessing the robustness and generalization capabilities of instance segmentation models in different segmentation tasks, introducing COCO-N and Cityscapes-N. We also propose a benchmark for weakly annotation noise, dubbed COCO-WAN, which utilizes foundation models and weak annotations to simulate semi-automated annotation tools and their noisy labels. This study sheds light on the quality of segmentation masks produced by various models and challenges the efficacy of popular methods designed to address learning with label noise."
      },
      {
        "id": "oai:arXiv.org:2406.14349v2",
        "title": "When Can You Trust Your Explanations? A Robustness Analysis on Feature Importances",
        "link": "https://arxiv.org/abs/2406.14349",
        "author": "Ilaria Vascotto, Alex Rodriguez, Alessandro Bonaita, Luca Bortolussi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.14349v2 Announce Type: replace \nAbstract: Recent legislative regulations have underlined the need for accountable and transparent artificial intelligence systems and have contributed to a growing interest in the Explainable Artificial Intelligence (XAI) field. Nonetheless, the lack of standardized criteria to validate explanation methodologies remains a major obstacle to developing trustworthy systems. We address a crucial yet often overlooked aspect of XAI, the robustness of explanations, which plays a central role in ensuring trust in both the system and the provided explanation. To this end, we propose a novel approach to analyse the robustness of neural network explanations to non-adversarial perturbations, leveraging the manifold hypothesis to produce new perturbed datapoints that resemble the observed data distribution. We additionally present an ensemble method to aggregate various explanations, showing how merging explanations can be beneficial for both understanding the model's decision and evaluating the robustness. The aim of our work is to provide practitioners with a framework for evaluating the trustworthiness of model explanations. Experimental results on feature importances derived from neural networks applied to tabular datasets highlight the importance of robust explanations in practical applications."
      },
      {
        "id": "oai:arXiv.org:2406.17281v4",
        "title": "DRTR: Distance-Aware Graph Representation Learning",
        "link": "https://arxiv.org/abs/2406.17281",
        "author": "Dong Liu, Yanxuan Yu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.17281v4 Announce Type: replace \nAbstract: We propose \\textbf{DRTR}, a novel graph learning framework that integrates distance-aware multi-hop message passing with dynamic topology refinement. Unlike standard GNNs that rely on shallow, fixed-hop aggregation, DRTR leverages both static preprocessing and dynamic resampling to capture deeper structural dependencies. A \\emph{Distance Recomputator} prunes semantically weak edges using adaptive attention, while a \\emph{Topology Reconstructor} establishes latent connections among distant but relevant nodes. This joint mechanism enables more expressive and robust representation learning across evolving graph structures. Extensive experiments demonstrate that DRTR outperforms baseline GNNs in both accuracy and scalability, especially in complex and noisy graph environments."
      },
      {
        "id": "oai:arXiv.org:2406.17949v2",
        "title": "The Overcooked Generalisation Challenge",
        "link": "https://arxiv.org/abs/2406.17949",
        "author": "Constantin Ruhdorfer, Matteo Bortoletto, Anna Penzkofer, Andreas Bulling",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.17949v2 Announce Type: replace \nAbstract: We introduce the Overcooked Generalisation Challenge (OGC) - the first benchmark to study agents' zero-shot cooperation abilities when faced with novel partners and levels in the Overcooked-AI environment. This perspective starkly contrasts a large body of previous work that has trained and evaluated cooperating agents only on the same level, failing to capture generalisation abilities required for real-world human-AI cooperation. Our challenge interfaces with state-of-the-art dual curriculum design (DCD) methods to generate auto-curricula for training general agents in Overcooked. It is the first cooperative multi-agent environment specially designed for DCD methods and, consequently, the first benchmarked with state-of-the-art methods. It is fully GPU-accelerated, built on the DCD benchmark suite minimax, and freely available under an open-source license: https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC. We show that current DCD algorithms struggle to produce useful policies in this novel challenge, even if combined with recent network architectures that were designed for scalability and generalisability. The OGC pushes the boundaries of real-world human-AI cooperation by enabling the research community to study the impact of generalisation on cooperating agents."
      },
      {
        "id": "oai:arXiv.org:2406.17961v2",
        "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
        "link": "https://arxiv.org/abs/2406.17961",
        "author": "Md Mahadi Hasan Nahid, Davood Rafiei",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.17961v2 Announce Type: replace \nAbstract: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks."
      },
      {
        "id": "oai:arXiv.org:2406.18770v2",
        "title": "ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of Large Language Models",
        "link": "https://arxiv.org/abs/2406.18770",
        "author": "Yuxuan Yin, Yu Wang, Boxun Xu, Peng Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.18770v2 Announce Type: replace \nAbstract: Analog circuit design requires substantial human expertise and involvement, which is a significant roadblock to design productivity. Bayesian Optimization (BO), a popular machine learning based optimization strategy, has been leveraged to automate analog design given its applicability across various circuit topologies and technologies. Traditional BO methods employ black box Gaussian Process surrogate models and optimized labeled data queries to find optimization solutions by trading off between exploration and exploitation. However, the search for the optimal design solution in BO can be expensive from both a computational and data usage point of view, particularly for high dimensional optimization problems. This paper presents ADO-LLM, the first work integrating large language models (LLMs) with Bayesian Optimization for analog design optimization. ADO-LLM leverages the LLM's ability to infuse domain knowledge to rapidly generate viable design points to remedy BO's inefficiency in finding high value design areas specifically under the limited design space coverage of the BO's probabilistic surrogate model. In the meantime, sampling of design points evaluated in the iterative BO process provides quality demonstrations for the LLM to generate high quality design points while leveraging infused broad design knowledge. Furthermore, the diversity brought by BO's exploration enriches the contextual understanding of the LLM and allows it to more broadly search in the design space and prevent repetitive and redundant suggestions. We evaluate the proposed framework on two different types of analog circuits and demonstrate notable improvements in design efficiency and effectiveness."
      },
      {
        "id": "oai:arXiv.org:2407.06249v3",
        "title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates",
        "link": "https://arxiv.org/abs/2407.06249",
        "author": "Zeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi, Greg Durrett",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.06249v3 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly being used to synthesize and reason about source code. However, the static nature of these models' knowledge does not reflect the fact that libraries and API functions they invoke are continuously evolving, with functionality being added or changing. While numerous benchmarks evaluate how LLMs can generate code, no prior work has studied how an LLMs' knowledge about code API functions can be updated. To fill this gap, we present CodeUpdateArena, a benchmark for knowledge editing in the code domain. An instance in our benchmark consists of a synthetic API function update paired with a program synthesis example that uses the updated functionality; our goal is to update an LLM to be able to solve this program synthesis example without providing documentation of the update at inference time. Compared to knowledge editing for facts encoded in text, success here is more challenging: a code LLM must correctly reason about the semantics of the modified function rather than just reproduce its syntax. Our dataset is constructed by first prompting GPT-4 to generate atomic and executable function updates. Then, for each update, we generate program synthesis examples whose code solutions are prone to use the update. Our benchmark covers updates of various types to 54 functions from seven diverse Python packages, with a total of 670 program synthesis examples. Our experiments show that prepending documentation of the update to open-source code LLMs (i.e., DeepSeek, CodeLlama) does not allow them to incorporate changes for problem solving, and existing knowledge editing techniques also have substantial room for improvement. We hope our benchmark will inspire new methods for knowledge updating in code LLMs."
      },
      {
        "id": "oai:arXiv.org:2407.09495v3",
        "title": "Image captioning in different languages",
        "link": "https://arxiv.org/abs/2407.09495",
        "author": "Emiel van Miltenburg",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.09495v3 Announce Type: replace \nAbstract: This short position paper provides a manually curated list of non-English image captioning datasets (as of May 2024). Through this list, we can observe the dearth of datasets in different languages: only 23 different languages are represented. With the addition of the Crossmodal-3600 dataset (Thapliyal et al., 2022, 36 languages) this number increases somewhat, but still this number is small compared to the +/-500 institutional languages that are out there. This paper closes with some open questions for the field of Vision & Language."
      },
      {
        "id": "oai:arXiv.org:2407.11606v4",
        "title": "The Foundations of Tokenization: Statistical and Computational Concerns",
        "link": "https://arxiv.org/abs/2407.11606",
        "author": "Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim Vieira, Ryan Cotterell",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.11606v4 Announce Type: replace \nAbstract: Tokenization - the practice of converting strings of characters from an alphabet into sequences of tokens over a vocabulary - is a critical step in the NLP pipeline. The use of token representations is widely credited with increased model performance but is also the source of many undesirable behaviors, such as spurious ambiguity or inconsistency. Despite its recognized importance as a standard representation method in NLP, the theoretical underpinnings of tokenization are not yet fully understood. In particular, the impact of tokenization on language model estimation has been investigated primarily through empirical means. The present paper contributes to addressing this theoretical gap by proposing a unified formal framework for representing and analyzing tokenizer models. Based on the category of stochastic maps, this framework enables us to establish general conditions for a principled use of tokenizers and, most importantly, the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. In addition, we discuss statistical and computational concerns crucial for designing and implementing tokenizer models, such as inconsistency, ambiguity, finiteness, and sequentiality. The framework and results advanced in this paper contribute to building robust theoretical foundations for representations in neural language modeling that can inform future theoretical and empirical research."
      },
      {
        "id": "oai:arXiv.org:2408.01581v3",
        "title": "Huge Ensembles Part II: Properties of a Huge Ensemble of Hindcasts Generated with Spherical Fourier Neural Operators",
        "link": "https://arxiv.org/abs/2408.01581",
        "author": "Ankur Mahesh, William Collins, Boris Bonev, Noah Brenowitz, Yair Cohen, Peter Harrington, Karthik Kashinath, Thorsten Kurth, Joshua North, Travis OBrien, Michael Pritchard, David Pruitt, Mark Risser, Shashank Subramanian, Jared Willard",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.01581v3 Announce Type: replace \nAbstract: In Part I, we created an ensemble based on Spherical Fourier Neural Operators. As initial condition perturbations, we used bred vectors, and as model perturbations, we used multiple checkpoints trained independently from scratch. Based on diagnostics that assess the ensemble's physical fidelity, our ensemble has comparable performance to operational weather forecasting systems. However, it requires orders of magnitude fewer computational resources. Here in Part II, we generate a huge ensemble (HENS), with 7,424 members initialized each day of summer 2023. We enumerate the technical requirements for running huge ensembles at this scale. HENS precisely samples the tails of the forecast distribution and presents a detailed sampling of internal variability. HENS has two primary applications: (1) as a large dataset with which to study the statistics and drivers of extreme weather and (2) as a weather forecasting system. For extreme climate statistics, HENS samples events 4$\\sigma$ away from the ensemble mean. At each grid cell, HENS increases the skill of the most accurate ensemble member and enhances coverage of possible future trajectories. As a weather forecasting model, HENS issues extreme weather forecasts with better uncertainty quantification. It also reduces the probability of outlier events, in which the verification value lies outside the ensemble forecast distribution."
      },
      {
        "id": "oai:arXiv.org:2408.01934v3",
        "title": "A Survey and Evaluation of Adversarial Attacks for Object Detection",
        "link": "https://arxiv.org/abs/2408.01934",
        "author": "Khoi Nguyen Tiet Nguyen, Wenyu Zhang, Kangkang Lu, Yuhuan Wu, Xingjian Zheng, Hui Li Tan, Liangli Zhen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.01934v3 Announce Type: replace \nAbstract: Deep learning models achieve remarkable accuracy in computer vision tasks, yet remain vulnerable to adversarial examples--carefully crafted perturbations to input images that can deceive these models into making confident but incorrect predictions. This vulnerability pose significant risks in high-stakes applications such as autonomous vehicles, security surveillance, and safety-critical inspection systems. While the existing literature extensively covers adversarial attacks in image classification, comprehensive analyses of such attacks on object detection systems remain limited. This paper presents a novel taxonomic framework for categorizing adversarial attacks specific to object detection architectures, synthesizes existing robustness metrics, and provides a comprehensive empirical evaluation of state-of-the-art attack methodologies on popular object detection models, including both traditional detectors and modern detectors with vision-language pretraining. Through rigorous analysis of open-source attack implementations and their effectiveness across diverse detection architectures, we derive key insights into attack characteristics. Furthermore, we delineate critical research gaps and emerging challenges to guide future investigations in securing object detection systems against adversarial threats. Our findings establish a foundation for developing more robust detection models while highlighting the urgent need for standardized evaluation protocols in this rapidly evolving domain."
      },
      {
        "id": "oai:arXiv.org:2408.09928v3",
        "title": "Enforcing View-Consistency in Class-Agnostic 3D Segmentation Fields",
        "link": "https://arxiv.org/abs/2408.09928",
        "author": "Corentin Dumery, Aoxiang Fan, Ren Li, Nicolas Talabot, Pascal Fua",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.09928v3 Announce Type: replace \nAbstract: Radiance Fields have become a powerful tool for modeling 3D scenes from multiple images. However, they remain difficult to segment into semantically meaningful regions. Some methods work well using 2D semantic masks, but they generalize poorly to class-agnostic segmentations. More recent methods circumvent this issue by using contrastive learning to optimize a high-dimensional 3D feature field instead. However, recovering a segmentation then requires clustering and fine-tuning the associated hyperparameters. In contrast, we aim to identify the necessary changes in segmentation field methods to directly learn a segmentation field while being robust to inconsistent class-agnostic masks, successfully decomposing the scene into a set of objects of any class.\n  By introducing an additional spatial regularization term and restricting the field to a limited number of competing object slots against which masks are matched, a meaningful object representation emerges that best explains the 2D supervision. Our experiments demonstrate the ability of our method to generate 3D panoptic segmentations on complex scenes, and extract high-quality 3D assets from radiance fields that can then be used in virtual 3D environments."
      },
      {
        "id": "oai:arXiv.org:2408.10669v2",
        "title": "Tensor tree learns hidden relational structures in data to construct generative models",
        "link": "https://arxiv.org/abs/2408.10669",
        "author": "Kenji Harada, Tsuyoshi Okubo, Naoki Kawashima",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.10669v2 Announce Type: replace \nAbstract: Based on the tensor tree network with the Born machine framework, we propose a general method for constructing a generative model by expressing the target distribution function as the amplitude of the quantum wave function represented by a tensor tree. The key idea is dynamically optimizing the tree structure that minimizes the bond mutual information. The proposed method offers enhanced performance and uncovers hidden relational structures in the target data. We illustrate potential practical applications with four examples: (i) random patterns, (ii) QMNIST handwritten digits, (iii) Bayesian networks, and (iv) the pattern of stock price fluctuation pattern in S&amp;P500. In (i) and (ii), the strongly correlated variables were concentrated near the center of the network; in (iii), the causality pattern was identified; and in (iv), a structure corresponding to the eleven sectors emerged."
      },
      {
        "id": "oai:arXiv.org:2408.11535v4",
        "title": "SAM-REF: Introducing Image-Prompt Synergy during Interaction for Detail Enhancement in the Segment Anything Model",
        "link": "https://arxiv.org/abs/2408.11535",
        "author": "Chongkai Yu, Ting Liu, Anqi Li, Xiaochao Qu, Chengjing Wu, Luoqi Liu, Xiaolin Hu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.11535v4 Announce Type: replace \nAbstract: Interactive segmentation is to segment the mask of the target object according to the user's interactive prompts. There are two mainstream strategies: early fusion and late fusion. Current specialist models utilize the early fusion strategy that encodes the combination of images and prompts to target the prompted objects, yet repetitive complex computations on the images result in high latency. Late fusion models extract image embeddings once and merge them with the prompts in later interactions. This strategy avoids redundant image feature extraction and improves efficiency significantly. A recent milestone is the Segment Anything Model (SAM). However, this strategy limits the models' ability to extract detailed information from the prompted target zone. To address this issue, we propose SAM-REF, a two-stage refinement framework that fully integrates images and prompts by using a lightweight refiner into the interaction of late fusion, which combines the accuracy of early fusion and maintains the efficiency of late fusion. Through extensive experiments, we show that our SAM-REF model outperforms the current state-of-the-art method in most metrics on segmentation quality without compromising efficiency."
      },
      {
        "id": "oai:arXiv.org:2408.11748v4",
        "title": "Understanding Depth and Height Perception in Large Visual-Language Models",
        "link": "https://arxiv.org/abs/2408.11748",
        "author": "Shehreen Azad, Yash Jain, Rishit Garg, Yogesh S Rawat, Vibhav Vineet",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.11748v4 Announce Type: replace \nAbstract: Geometric understanding - including depth and height perception - is fundamental to intelligence and crucial for navigating our environment. Despite the impressive capabilities of large Vision Language Models (VLMs), it remains unclear how well they possess the geometric understanding required for practical applications in visual perception. In this work, we focus on evaluating the geometric understanding of these models, specifically targeting their ability to perceive the depth and height of objects in an image. To address this, we introduce GeoMeter, a suite of benchmark datasets - encompassing 2D and 3D scenarios - to rigorously evaluate these aspects. By benchmarking 18 state-of-the-art VLMs, we found that although they excel in perceiving basic geometric properties like shape and size, they consistently struggle with depth and height perception. Our analysis reveal that these challenges stem from shortcomings in their depth and height reasoning capabilities and inherent biases. This study aims to pave the way for developing VLMs with enhanced geometric understanding by emphasizing depth and height perception as critical components necessary for real-world applications."
      },
      {
        "id": "oai:arXiv.org:2408.16295v3",
        "title": "Modeling Emotional Dynamics in Social Networks: Uncovering the Positive Role of Information Cocoons in Group Emotional Stabilization",
        "link": "https://arxiv.org/abs/2408.16295",
        "author": "Jinhu Ren, Xifei Fu, Tianlong Fan, Linyuan L\\\"u",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.16295v3 Announce Type: replace \nAbstract: Information cocooning-amplified by algorithmic filtering-poses complex challenges for emotional dynamics in online social networks. This study explores how algorithmically reinforced information cocooning shapes information diffusion and group emotional dynamics in online social networks. We propose a viewpoint-based network evolution model that simulates struc-tural transformations driven by user preferences. To model the hidden influence of personalized comment recommendations, we introduce the Hidden Comment Area Cocoon (H-CAC)-a novel higher-order structure that captures cocooning at the comment level. This structure is integrated into an emotion spreading mod-el, enabling the quantification of how cocooning affects collective sentiment. By defining Recommendation Accuracy (RA) as a tunable parameter, we systematically evaluate its impact on emo-tional volatility and polarization. Extensive simulations, validated with real-world data, reveal that while cocooning reduces content diversity, it can significantly enhance emotional resilience within groups. Our findings offer a new computational lens on the dual role of cocooning and provide actionable insights for designing emotionally stable, algorithmically governed social platforms."
      },
      {
        "id": "oai:arXiv.org:2409.03944v2",
        "title": "HUMOS: Human Motion Model Conditioned on Body Shape",
        "link": "https://arxiv.org/abs/2409.03944",
        "author": "Shashank Tripathi, Omid Taheri, Christoph Lassner, Michael J. Black, Daniel Holden, Carsten Stoll",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.03944v2 Announce Type: replace \nAbstract: Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page https://CarstenEpic.github.io/humos/."
      },
      {
        "id": "oai:arXiv.org:2409.06417v2",
        "title": "Fast nonparametric inference of network backbones for weighted graph sparsification",
        "link": "https://arxiv.org/abs/2409.06417",
        "author": "Alec Kirkley",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.06417v2 Announce Type: replace \nAbstract: Network backbones provide useful sparse representations of weighted networks by keeping only their most important links, permitting a range of computational speedups and simplifying network visualizations. A key limitation of existing network backboning methods is that they either require the specification of a free parameter (e.g. significance level) that determines the number of edges to keep in the backbone, or impose specific restrictions on the topology of the backbone (e.g. that it is a spanning tree). Here we develop a completely nonparametric framework for inferring the backbone of a weighted network that overcomes these limitations and automatically selects the optimal set of edges to retain using the Minimum Description Length (MDL) principle. We develop objective functions for global and local network backboning which evaluate the importance of an edge in the context of the whole network and individual node neighborhoods respectively and are generalizable to any weight distribution under canonical and microcanonical Bayesian model specifications. We then construct an efficient and provably optimal greedy algorithm to identify the backbone minimizing our objectives for a large class of weight distributions, whose runtime complexity is log-linear in the number of edges. We demonstrate our methods by comparing them with existing methods in a range of tasks on real and synthetic networks, finding that both the global and local backboning methods can preserve network connectivity, weight heterogeneity, and spreading dynamics while removing a substantial fraction of edges."
      },
      {
        "id": "oai:arXiv.org:2409.06845v2",
        "title": "Face Mask Removal with Region-attentive Face Inpainting",
        "link": "https://arxiv.org/abs/2409.06845",
        "author": "Minmin Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.06845v2 Announce Type: replace \nAbstract: During the COVID-19 pandemic, face masks have become ubiquitous in our lives. Face masks can cause some face recognition models to fail since they cover significant portion of a face. In addition, removing face masks from captured images or videos can be desirable, e.g., for better social interaction and for image/video editing and enhancement purposes. Hence, we propose a generative face inpainting method to effectively recover/reconstruct the masked part of a face. Face inpainting is more challenging compared to traditional inpainting, since it requires high fidelity while maintaining the identity at the same time. Our proposed method includes a Multi-scale Channel-Spatial Attention Module (M-CSAM) to mitigate the spatial information loss and learn the inter- and intra-channel correlation. In addition, we introduce an approach enforcing the supervised signal to focus on masked regions instead of the whole image. We also synthesize our own Masked-Faces dataset from the CelebA dataset by incorporating five different types of face masks, including surgical mask, regular mask and scarves, which also cover the neck area. The experimental results show that our proposed method outperforms different baselines in terms of structural similarity index measure, peak signal-to-noise ratio and l1 loss, while also providing better outputs qualitatively. The code will be made publicly available. Code is available at GitHub."
      },
      {
        "id": "oai:arXiv.org:2409.11367v2",
        "title": "OSV: One Step is Enough for High-Quality Image to Video Generation",
        "link": "https://arxiv.org/abs/2409.11367",
        "author": "Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang, Wenhan Luo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.11367v2 Announce Type: replace \nAbstract: Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. While efforts have been made to accelerate video diffusion by reducing inference steps (through techniques like consistency distillation) and GAN training (these approaches often fall short in either performance or training stability). In this work, we introduce a two-stage training framework that effectively combines consistency distillation with GAN training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenWebVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94)."
      },
      {
        "id": "oai:arXiv.org:2409.13108v2",
        "title": "Disentangling Recognition and Decision Regrets in Image-Based Reinforcement Learning",
        "link": "https://arxiv.org/abs/2409.13108",
        "author": "Alihan H\\\"uy\\\"uk, Arndt Ryo Koblitz, Atefeh Mohajeri, Matthew Andrews",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.13108v2 Announce Type: replace \nAbstract: In image-based reinforcement learning (RL), policies usually operate in two steps: first extracting lower-dimensional features from raw images (the \"recognition\" step), and then taking actions based on the extracted features (the \"decision\" step). Extracting features that are spuriously correlated with performance or irrelevant for decision-making can lead to poor generalization performance, known as observational overfitting in image-based RL. In such cases, it can be hard to quantify how much of the error can be attributed to poor feature extraction vs. poor decision-making. To disentangle the two sources of error, we introduce the notions of recognition regret and decision regret. Using these notions, we characterize and disambiguate the two distinct causes behind observational overfitting: over-specific representations, which include features that are not needed for optimal decision-making (leading to high decision regret), vs. under-specific representations, which only include a limited set of features that were spuriously correlated with performance during training (leading to high recognition regret). Finally, we provide illustrative examples of observational overfitting due to both over-specific and under-specific representations in maze environments and the Atari game Pong."
      },
      {
        "id": "oai:arXiv.org:2410.01100v3",
        "title": "Unlocking Korean Verbs: A User-Friendly Exploration into the Verb Lexicon",
        "link": "https://arxiv.org/abs/2410.01100",
        "author": "Seohyun Song, Eunkyul Leah Jo, Yige Chen, Jeen-Pyo Hong, Kyuwon Kim, Jin Wee, Miyoung Kang, KyungTae Lim, Jungyeul Park, Chulwoo Park",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.01100v3 Announce Type: replace \nAbstract: The Sejong dictionary dataset offers a valuable resource, providing extensive coverage of morphology, syntax, and semantic representation. This dataset can be utilized to explore linguistic information in greater depth. The labeled linguistic structures within this dataset form the basis for uncovering relationships between words and phrases and their associations with target verbs. This paper introduces a user-friendly web interface designed for the collection and consolidation of verb-related information, with a particular focus on subcategorization frames. Additionally, it outlines our efforts in mapping this information by aligning subcategorization frames with corresponding illustrative sentence examples. Furthermore, we provide a Python library that would simplify syntactic parsing and semantic role labeling. These tools are intended to assist individuals interested in harnessing the Sejong dictionary dataset to develop applications for Korean language processing."
      },
      {
        "id": "oai:arXiv.org:2410.02179v2",
        "title": "HATFormer: Historic Handwritten Arabic Text Recognition with Transformers",
        "link": "https://arxiv.org/abs/2410.02179",
        "author": "Adrian Chan, Anupam Mijar, Mehreen Saeed, Chau-Wai Wong, Akram Khater",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02179v2 Announce Type: replace \nAbstract: Arabic handwritten text recognition (HTR) is challenging, especially for historical texts, due to diverse writing styles and the intrinsic features of Arabic script. Additionally, Arabic handwriting datasets are smaller compared to English ones, making it difficult to train generalizable Arabic HTR models. To address these challenges, we propose HATFormer, a transformer-based encoder-decoder architecture that builds on a state-of-the-art English HTR model. By leveraging the transformer's attention mechanism, HATFormer captures spatial contextual information to address the intrinsic challenges of Arabic script through differentiating cursive characters, decomposing visual representations, and identifying diacritics. Our customization to historical handwritten Arabic includes an image processor for effective ViT information preprocessing, a text tokenizer for compact Arabic text representation, and a training pipeline that accounts for a limited amount of historic Arabic handwriting data. HATFormer achieves a character error rate (CER) of 8.6% on the largest public historical handwritten Arabic dataset, with a 51% improvement over the best baseline in the literature. HATFormer also attains a comparable CER of 4.2% on the largest private non-historical dataset. Our work demonstrates the feasibility of adapting an English HTR method to a low-resource language with complex, language-specific challenges, contributing to advancements in document digitization, information retrieval, and cultural preservation."
      },
      {
        "id": "oai:arXiv.org:2410.02660v2",
        "title": "How to Train Long-Context Language Models (Effectively)",
        "link": "https://arxiv.org/abs/2410.02660",
        "author": "Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.02660v2 Announce Type: replace \nAbstract: We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs."
      },
      {
        "id": "oai:arXiv.org:2410.03408v2",
        "title": "Predictive Coding for Decision Transformer",
        "link": "https://arxiv.org/abs/2410.03408",
        "author": "Tung M. Luu, Donghoon Lee, Chang D. Yoo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03408v2 Announce Type: replace \nAbstract: Recent work in offline reinforcement learning (RL) has demonstrated the effectiveness of formulating decision-making as return-conditioned supervised learning. Notably, the decision transformer (DT) architecture has shown promise across various domains. However, despite its initial success, DTs have underperformed on several challenging datasets in goal-conditioned RL. This limitation stems from the inefficiency of return conditioning for guiding policy learning, particularly in unstructured and suboptimal datasets, resulting in DTs failing to effectively learn temporal compositionality. Moreover, this problem might be further exacerbated in long-horizon sparse-reward tasks. To address this challenge, we propose the Predictive Coding for Decision Transformer (PCDT) framework, which leverages generalized future conditioning to enhance DT methods. PCDT utilizes an architecture that extends the DT framework, conditioned on predictive codings, enabling decision-making based on both past and future factors, thereby improving generalization. Through extensive experiments on eight datasets from the AntMaze and FrankaKitchen environments, our proposed method achieves performance on par with or surpassing existing popular value-based and transformer-based methods in offline goal-conditioned RL. Furthermore, we also evaluate our method on a goal-reaching task with a physical robot."
      },
      {
        "id": "oai:arXiv.org:2410.03804v2",
        "title": "Mixture of Attentions For Speculative Decoding",
        "link": "https://arxiv.org/abs/2410.03804",
        "author": "Matthieu Zimmer, Milan Gritta, Gerasimos Lampouras, Haitham Bou Ammar, Jun Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.03804v2 Announce Type: replace \nAbstract: The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy. Speculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the LLM in parallel. Small models that utilise activations from the LLM currently achieve the fastest decoding speeds. However, we identify several limitations of SD models including the lack of on-policyness during training and partial observability. To address these shortcomings, we propose a more grounded architecture for small models by introducing a Mixture of Attentions for SD. Our novel architecture can be applied in two scenarios: a conventional single device deployment and a novel client-server deployment where the small model is hosted on a consumer device and the LLM on a server. In a single-device scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%. In a client-server setting, our experiments demonstrate: 1) state-of-the-art latencies with minimal calls to the server for different network conditions, and 2) in the event of a complete disconnection, our approach can maintain higher accuracy compared to other SD methods and demonstrates advantages over API calls to LLMs, which would otherwise be unable to continue the generation process."
      },
      {
        "id": "oai:arXiv.org:2410.04133v3",
        "title": "An Electrocardiogram Foundation Model Built on over 10 Million Recordings with External Evaluation across Multiple Domains",
        "link": "https://arxiv.org/abs/2410.04133",
        "author": "Jun Li, Aaron Aguirre, Junior Moura, Che Liu, Lanhai Zhong, Chenxi Sun, Gari Clifford, Brandon Westover, Shenda Hong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.04133v3 Announce Type: replace \nAbstract: Artificial intelligence (AI) has demonstrated significant potential in ECG analysis and cardiovascular disease assessment. Recently, foundation models have played a remarkable role in advancing medical AI. The development of an ECG foundation model holds the promise of elevating AI-ECG research to new heights. However, building such a model faces several challenges, including insufficient database sample sizes and inadequate generalization across multiple domains. Additionally, there is a notable performance gap between single-lead and multi-lead ECG analyses. We introduced an ECG Foundation Model (ECGFounder), a general-purpose model that leverages real-world ECG annotations from cardiology experts to broaden the diagnostic capabilities of ECG analysis. ECGFounder was trained on over 10 million ECGs with 150 label categories from the Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease diagnosis through ECG analysis. The model is designed to be both an effective out-of-the-box solution, and a to be fine-tunable for downstream tasks, maximizing usability. Importantly, we extended its application to lower rank ECGs, and arbitrary single-lead ECGs in particular. ECGFounder is applicable to supporting various downstream tasks in mobile monitoring scenarios. Experimental results demonstrate that ECGFounder achieves expert-level performance on internal validation sets, with AUROC exceeding 0.95 for eighty diagnoses. It also shows strong classification performance and generalization across various diagnoses on external validation sets. When fine-tuned, ECGFounder outperforms baseline models in demographic analysis, clinical event detection, and cross-modality cardiac rhythm diagnosis. The trained model and data will be publicly released upon publication through the bdsp.io. Our code is available at https://github.com/PKUDigitalHealth/ECGFounder"
      },
      {
        "id": "oai:arXiv.org:2410.08896v2",
        "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL",
        "link": "https://arxiv.org/abs/2410.08896",
        "author": "Claas A Voelcker, Marcel Hussing, Eric Eaton, Amir-massoud Farahmand, Igor Gilitschenski",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.08896v2 Announce Type: replace \nAbstract: Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD), uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning."
      },
      {
        "id": "oai:arXiv.org:2410.12159v3",
        "title": "NSSI-Net: A Multi-Concept GAN for Non-Suicidal Self-Injury Detection Using High-Dimensional EEG in a Semi-Supervised Framework",
        "link": "https://arxiv.org/abs/2410.12159",
        "author": "Zhen Liang, Weishan Ye, Qile Liu, Li Zhang, Gan Huang, Yongjie Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.12159v3 Announce Type: replace \nAbstract: Non-suicidal self-injury (NSSI) is a serious threat to the physical and mental health of adolescents, significantly increasing the risk of suicide and attracting widespread public concern. Electroencephalography (EEG), as an objective tool for identifying brain disorders, holds great promise. However, extracting meaningful and reliable features from high-dimensional EEG data, especially by integrating spatiotemporal brain dynamics into informative representations, remains a major challenge. In this study, we introduce an advanced semi-supervised adversarial network, NSSI-Net, to effectively model EEG features related to NSSI. NSSI-Net consists of two key modules: a spatial-temporal feature extraction module and a multi-concept discriminator. In the spatial-temporal feature extraction module, an integrated 2D convolutional neural network (2D-CNN) and a bi-directional Gated Recurrent Unit (BiGRU) are used to capture both spatial and temporal dynamics in EEG data. In the multi-concept discriminator, signal, gender, domain, and disease levels are fully explored to extract meaningful EEG features, considering individual, demographic, disease variations across a diverse population. Based on self-collected NSSI data (n=114), the model's effectiveness and reliability are demonstrated, with a 5.44% improvement in performance compared to existing machine learning and deep learning methods. This study advances the understanding and early diagnosis of NSSI in adolescents with depression, enabling timely intervention. The source code is available at https://github.com/Vesan-yws/NSSINet."
      },
      {
        "id": "oai:arXiv.org:2410.12491v2",
        "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse Reinforcement Learning",
        "link": "https://arxiv.org/abs/2410.12491",
        "author": "Jared Joselowitz, Ritam Majumdar, Arjun Jagota, Matthieu Bou, Nyal Patel, Satyapriya Krishna, Sonali Parbhoo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.12491v2 Announce Type: replace \nAbstract: Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. We conduct experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 85\\% accuracy in predicting human preferences. Our analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems."
      },
      {
        "id": "oai:arXiv.org:2410.14121v2",
        "title": "FedMSE: Semi-supervised federated learning approach for IoT network intrusion detection",
        "link": "https://arxiv.org/abs/2410.14121",
        "author": "Van Tuan Nguyen, Razvan Beuran",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.14121v2 Announce Type: replace \nAbstract: This paper proposes a novel federated learning approach for improving IoT network intrusion detection. The rise of IoT has expanded the cyber attack surface, making traditional centralized machine learning methods insufficient due to concerns about data availability, computational resources, transfer costs, and especially privacy preservation. A semi-supervised federated learning model was developed to overcome these issues, combining the Shrink Autoencoder and Centroid one-class classifier (SAE-CEN). This approach enhances the performance of intrusion detection by effectively representing normal network data and accurately identifying anomalies in the decentralized strategy. Additionally, a mean square error-based aggregation algorithm (MSEAvg) was introduced to improve global model performance by prioritizing more accurate local models. The results obtained in our experimental setup, which uses various settings relying on the N-BaIoT dataset and Dirichlet distribution, demonstrate significant improvements in real-world heterogeneous IoT networks in detection accuracy from 93.98$\\pm$2.90 to 97.30$\\pm$0.49, reduced learning costs when requiring only 50\\% of gateways participating in the training process, and robustness in large-scale networks."
      },
      {
        "id": "oai:arXiv.org:2410.15316v2",
        "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
        "link": "https://arxiv.org/abs/2410.15316",
        "author": "Alan Dao (Gia Tuan Dao), Dinh Bach Vu, Huy Hoang Ha",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15316v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models."
      },
      {
        "id": "oai:arXiv.org:2410.17242v2",
        "title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias",
        "link": "https://arxiv.org/abs/2410.17242",
        "author": "Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, Zexiang Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.17242v2 Announce Type: replace \nAbstract: We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods -- from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps) -- addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/ ."
      },
      {
        "id": "oai:arXiv.org:2410.22296v4",
        "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks",
        "link": "https://arxiv.org/abs/2410.22296",
        "author": "Angelica Chen, Samuel D. Stanton, Frances Ding, Robert G. Alberstein, Andrew M. Watkins, Richard Bonneau, Vladimir Gligorijevi\\'c, Kyunghyun Cho, Nathan C. Frey",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.22296v4 Announce Type: replace \nAbstract: Although large language models (LLMs) have shown promise in biomolecule optimization problems, they incur heavy computational costs and struggle to satisfy precise constraints. On the other hand, specialized solvers like LaMBO-2 offer efficiency and fine-grained control but require more domain expertise. Comparing these approaches is challenging due to expensive laboratory validation and inadequate synthetic benchmarks. We address this by introducing Ehrlich functions, a synthetic test suite that captures the geometric structure of biophysical sequence optimization problems. With prompting alone, off-the-shelf LLMs struggle to optimize Ehrlich functions. In response, we propose LLOME (Language Model Optimization with Margin Expectation), a bilevel optimization routine for online black-box optimization. When combined with a novel preference learning loss, we find LLOME can not only learn to solve some Ehrlich functions, but can even outperform LaMBO-2 on moderately difficult Ehrlich variants. However, LLOME is comparable to LaMBO-2 on very easy or difficult variants, exhibits some likelihood-reward miscalibration, and struggles without explicit rewards. Our results indicate LLMs can provide significant benefits in some cases, but specialized solvers are still competitive and incur less overhead."
      },
      {
        "id": "oai:arXiv.org:2411.00915v5",
        "title": "Empower Vision Applications with LoRA LMM",
        "link": "https://arxiv.org/abs/2411.00915",
        "author": "Liang Mi, Weijun Wang, Wenming Tu, Qingfeng He, Rui Kong, Xinyu Fang, Yazhu Dong, Yikang Zhang, Yunchun Li, Meng Li, Haipeng Dai, Guihai Chen, Yunxin Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.00915v5 Announce Type: replace \nAbstract: Large Multimodal Models (LMMs) have shown significant progress in various complex vision tasks with the solid linguistic and reasoning capacity inherited from large language models (LMMs). Low-rank adaptation (LoRA) offers a promising method to integrate external knowledge into LMMs, compensating for their limitations on domain-specific tasks. However, the existing LoRA model serving is excessively computationally expensive and causes extremely high latency. In this paper, we present an end-to-end solution that empowers diverse vision tasks and enriches vision applications with LoRA LMMs. Our system, VaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism that manages application requests and LoRA adapters to achieve the lowest average response latency. We prototype VaLoRA on five popular vision tasks on three LMMs. Experiment results reveal that VaLoRA improves 24-62% of the accuracy compared to the original LMMs and reduces 20-89% of the latency compared to the state-of-the-art LoRA model serving systems."
      },
      {
        "id": "oai:arXiv.org:2411.03862v2",
        "title": "ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization",
        "link": "https://arxiv.org/abs/2411.03862",
        "author": "Huayang Huang, Yu Wu, Qian Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.03862v2 Announce Type: replace \nAbstract: Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods. Code is available at https://github.com/Hannah1102/ROBIN."
      },
      {
        "id": "oai:arXiv.org:2411.05841v3",
        "title": "FLEXtime: Filterbank learning to explain time series",
        "link": "https://arxiv.org/abs/2411.05841",
        "author": "Thea Br\\\"usch, Kristoffer K. Wickstr{\\o}m, Mikkel N. Schmidt, Robert Jenssen, Tommy S. Alstr{\\o}m",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.05841v3 Announce Type: replace \nAbstract: State-of-the-art methods for explaining predictions from time series involve learning an instance-wise saliency mask for each time step; however, many types of time series are difficult to interpret in the time domain, due to the inherently complex nature of the data. Instead, we propose to view time series explainability as saliency maps over interpretable parts, leaning on established signal processing methodology on signal decomposition. Specifically, we propose a new method called FLEXtime that uses a bank of bandpass filters to split the time series into frequency bands. Then, we learn the combination of these bands that optimally explains the model's prediction. Our extensive evaluation shows that, on average, FLEXtime outperforms state-of-the-art explainability methods across a range of datasets. FLEXtime fills an important gap in the current time series explainability methodology and is a valuable tool for a wide range of time series such as EEG and audio. Code is available at https://github.com/theabrusch/FLEXtime."
      },
      {
        "id": "oai:arXiv.org:2411.06291v2",
        "title": "TinyML NLP Scheme for Semantic Wireless Sentiment Classification with Privacy Preservation",
        "link": "https://arxiv.org/abs/2411.06291",
        "author": "Ahmed Y. Radwan, Mohammad Shehab, Mohamed-Slim Alouini",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.06291v2 Announce Type: replace \nAbstract: Natural Language Processing (NLP) operations, such as semantic sentiment analysis and text synthesis, often raise privacy concerns and demand significant on-device computational resources. Centralized Learning (CL) on the edge provides an energy-efficient alternative but requires collecting raw data, compromising user privacy. While Federated Learning (FL) enhances privacy, it imposes high computational energy demands on resource-constrained devices. We introduce Split Learning (SL) as an energy-efficient, privacy-preserving Tiny Machine Learning (TinyML) framework and compare it to FL and CL in the presence of Rayleigh fading and additive noise. Our results show that SL significantly reduces computational power and CO2 emissions while enhancing privacy, as evidenced by a fourfold increase in reconstruction error compared to FL and nearly eighteen times that of CL. In contrast, FL offers a balanced trade-off between privacy and efficiency. This study provides insights into deploying privacy-preserving, energy-efficient NLP models on edge devices."
      },
      {
        "id": "oai:arXiv.org:2411.08297v2",
        "title": "TowerDebias: A Novel Unfairness Removal Method Based on the Tower Property",
        "link": "https://arxiv.org/abs/2411.08297",
        "author": "Norman Matloff, Aditya Mittal",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08297v2 Announce Type: replace \nAbstract: Decision-making processes have increasingly come to rely on sophisticated machine learning tools, raising critical concerns about the fairness of their predictions with respect to sensitive groups. The widespread adoption of commercial \"black-box\" models necessitates careful consideration of their legal and ethical implications for consumers. When users interact with such black-box models, a key challenge arises: how can the influence of sensitive attributes, such as race or gender, be mitigated or removed from its predictions? We propose towerDebias (tDB), a novel post-processing method designed to reduce the influence of sensitive attributes in predictions made by black-box models. Our tDB approach leverages the Tower Property from probability theory to improve prediction fairness without requiring retraining of the original model. This method is highly versatile, as it requires no prior knowledge of the original algorithm's internal structure and is adaptable to a diverse range of applications. We present a formal fairness improvement theorem for tDB and showcase its effectiveness in both regression and classification tasks using multiple real-world datasets."
      },
      {
        "id": "oai:arXiv.org:2411.08306v2",
        "title": "Evaluating Molecule Synthesizability via Retrosynthetic Planning and Reaction Prediction",
        "link": "https://arxiv.org/abs/2411.08306",
        "author": "Songtao Liu, Dandan Zhang, Zhengkai Tu, Hanjun Dai, Peng Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.08306v2 Announce Type: replace \nAbstract: A significant challenge in wet lab experiments with current drug design generative models is the trade-off between pharmacological properties and synthesizability. Molecules predicted to have highly desirable properties are often difficult to synthesize, while those that are easily synthesizable tend to exhibit less favorable properties. As a result, evaluating the synthesizability of molecules in general drug design scenarios remains a significant challenge in the field of drug discovery. The commonly used synthetic accessibility (SA) score aims to evaluate the ease of synthesizing generated molecules, but it falls short of guaranteeing that synthetic routes can actually be found. Inspired by recent advances in top-down synthetic route generation and forward reaction prediction, we propose a new, data-driven metric to evaluate molecule synthesizability. This novel metric leverages the synergistic duality between retrosynthetic planners and reaction predictors, both of which are trained on extensive reaction datasets. To demonstrate the efficacy of our metric, we conduct a comprehensive evaluation of round-trip scores across a range of representative molecule generative models."
      },
      {
        "id": "oai:arXiv.org:2411.14762v4",
        "title": "Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction",
        "link": "https://arxiv.org/abs/2411.14762",
        "author": "Huiwon Jang, Sihyun Yu, Jinwoo Shin, Pieter Abbeel, Younggyo Seo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.14762v4 Announce Type: replace \nAbstract: Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once."
      },
      {
        "id": "oai:arXiv.org:2411.16315v4",
        "title": "Local Learning for Covariate Selection in Nonparametric Causal Effect Estimation with Latent Variables",
        "link": "https://arxiv.org/abs/2411.16315",
        "author": "Zheng Li, Feng Xie, Xichen Guo, Yan Zeng, Hao Zhang, Zhi Geng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.16315v4 Announce Type: replace \nAbstract: Estimating causal effects from nonexperimental data is a fundamental problem in many fields of science. A key component of this task is selecting an appropriate set of covariates for confounding adjustment to avoid bias. Most existing methods for covariate selection often assume the absence of latent variables and rely on learning the global network structure among variables. However, identifying the global structure can be unnecessary and inefficient, especially when our primary interest lies in estimating the effect of a treatment variable on an outcome variable. To address this limitation, we propose a novel local learning approach for covariate selection in nonparametric causal effect estimation, which accounts for the presence of latent variables. Our approach leverages testable independence and dependence relationships among observed variables to identify a valid adjustment set for a target causal relationship, ensuring both soundness and completeness under standard assumptions. We validate the effectiveness of our algorithm through extensive experiments on both synthetic and real-world data."
      },
      {
        "id": "oai:arXiv.org:2411.19041v2",
        "title": "TAMT: Temporal-Aware Model Tuning for Cross-Domain Few-Shot Action Recognition",
        "link": "https://arxiv.org/abs/2411.19041",
        "author": "Yilong Wang, Zilin Gao, Qilong Wang, Zhaofeng Chen, Peihua Li, Qinghua Hu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.19041v2 Announce Type: replace \nAbstract: Going beyond few-shot action recognition (FSAR), cross-domain FSAR (CDFSAR) has attracted recent research interests by solving the domain gap lying in source-to-target transfer learning. Existing CDFSAR methods mainly focus on joint training of source and target data to mitigate the side effect of domain gap. However, such kind of methods suffer from two limitations: First, pair-wise joint training requires retraining deep models in case of one source data and multiple target ones, which incurs heavy computation cost, especially for large source and small target data. Second, pre-trained models after joint training are adopted to target domain in a straightforward manner, hardly taking full potential of pre-trained models and then limiting recognition performance. To overcome above limitations, this paper proposes a simple yet effective baseline, namely Temporal-Aware Model Tuning (TAMT) for CDFSAR. Specifically, our TAMT involves a decoupled paradigm by performing pre-training on source data and fine-tuning target data, which avoids retraining for multiple target data with single source. To effectively and efficiently explore the potential of pre-trained models in transferring to target domain, our TAMT proposes a Hierarchical Temporal Tuning Network (HTTN), whose core involves local temporal-aware adapters (TAA) and a global temporal-aware moment tuning (GTMT). Particularly, TAA learns few parameters to recalibrate the intermediate features of frozen pre-trained models, enabling efficient adaptation to target domains. Furthermore, GTMT helps to generate powerful video representations, improving match performance on the target domain. Experiments on several widely used video benchmarks show our TAMT outperforms the recently proposed counterparts by 13%$\\sim$31%, achieving new state-of-the-art CDFSAR results."
      },
      {
        "id": "oai:arXiv.org:2412.01380v2",
        "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking",
        "link": "https://arxiv.org/abs/2412.01380",
        "author": "Marco Federici, Davide Belli, Mart van Baalen, Amir Jalalirad, Andrii Skliar, Bence Major, Markus Nagel, Paul Whatmough",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01380v2 Announce Type: replace \nAbstract: While mobile devices provide ever more compute power, improvements in DRAM bandwidth are much slower. This is unfortunate for large language model (LLM) token generation, which is heavily memory-bound. Previous work has proposed to leverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce effective DRAM bandwidth per token. However, more recent LLMs use SwiGLU instead of ReLU, which results in little inherent sparsity. While SwiGLU activations can be pruned based on magnitude, the resulting sparsity patterns are difficult to predict, rendering previous approaches ineffective. To circumvent this issue, our work introduces Dynamic Input Pruning (DIP): a predictor-free dynamic sparsification approach, which preserves accuracy with minimal fine-tuning. DIP can further use lightweight LoRA adapters to regain some performance lost during sparsification. Lastly, we describe a novel cache-aware masking strategy, which considers the cache state and activation magnitude to further increase cache hit rate, improving LLM token rate on mobile devices. DIP outperforms other methods in terms of accuracy, memory and throughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP achieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$ 0.1 loss in perplexity when compared to streaming the dense model from Flash. The open source code for HW simulator, methods, and experiments in this paper is available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
      },
      {
        "id": "oai:arXiv.org:2412.01477v3",
        "title": "Improving Object Detection by Modifying Synthetic Data with Explainable AI",
        "link": "https://arxiv.org/abs/2412.01477",
        "author": "Nitish Mital, Simon Malzard, Richard Walters, Celso M. De Melo, Raghuveer Rao, Victoria Nockles",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01477v3 Announce Type: replace \nAbstract: Limited real-world data severely impacts model performance in many computer vision domains, particularly for samples that are underrepresented in training. Synthetically generated images are a promising solution, but 1) it remains unclear how to design synthetic training data to optimally improve model performance (e.g, whether and where to introduce more realism or more abstraction) and 2) the domain expertise, time and effort required from human operators for this design and optimisation process represents a major practical challenge. Here we propose a novel conceptual approach to improve the efficiency of designing synthetic images, by using robust Explainable AI (XAI) techniques to guide a human-in-the-loop process of modifying 3D mesh models used to generate these images. Importantly, this framework allows both modifications that increase and decrease realism in synthetic data, which can both improve model performance. We illustrate this concept using a real-world example where data are sparse; detection of vehicles in infrared imagery. We fine-tune an initial YOLOv8 model on the ATR DSIAC infrared dataset and synthetic images generated from 3D mesh models in the Unity gaming engine, and then use XAI saliency maps to guide modification of our Unity models. We show that synthetic data can improve detection of vehicles in orientations unseen in training by 4.6% (to mAP50 = 94.6%). We further improve performance by an additional 1.5% (to 96.1%) through our new XAI-guided approach, which reduces misclassifications through both increasing and decreasing the realism of different parts of the synthetic data. Our proof-of-concept results pave the way for fine, XAI-controlled curation of synthetic datasets tailored to improve object detection performance, whilst simultaneously reducing the burden on human operators in designing and optimising these datasets."
      },
      {
        "id": "oai:arXiv.org:2412.01543v2",
        "title": "6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting",
        "link": "https://arxiv.org/abs/2412.01543",
        "author": "Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01543v2 Announce Type: replace \nAbstract: Efficient and accurate object pose estimation is an essential component for modern vision systems in many applications such as Augmented Reality, autonomous driving, and robotics. While research in model-based 6D object pose estimation has delivered promising results, model-free methods are hindered by the high computational load in rendering and inferring consistent poses of arbitrary objects in a live RGB-D video stream. To address this issue, we present 6DOPE-GS, a novel method for online 6D object pose estimation \\& tracking with a single RGB-D camera by effectively leveraging advances in Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses and 3D object reconstruction. To achieve the necessary efficiency and accuracy for live tracking, our method uses incremental 2D Gaussian Splatting with an intelligent dynamic keyframe selection procedure to achieve high spatial object coverage and prevent erroneous pose updates. We also propose an opacity statistic-based pruning mechanism for adaptive Gaussian density control, to ensure training stability and efficiency. We evaluate our method on the HO3D and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of state-of-the-art baselines for model-free simultaneous 6D pose tracking and reconstruction while providing a 5$\\times$ speedup. We also demonstrate the method's suitability for live, dynamic object tracking and reconstruction in a real-world setting."
      },
      {
        "id": "oai:arXiv.org:2412.01619v2",
        "title": "Representation and Regression Problems in Neural Networks: Relaxation, Generalization, and Numerics",
        "link": "https://arxiv.org/abs/2412.01619",
        "author": "Kang Liu, Enrique Zuazua",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01619v2 Announce Type: replace \nAbstract: In this work, we address three non-convex optimization problems associated with the training of shallow neural networks (NNs) for exact and approximate representation, as well as for regression tasks. Through a mean-field approach, we convexify these problems and, applying a representer theorem, prove the absence of relaxation gaps. We establish generalization bounds for the resulting NN solutions, assessing their predictive performance on test datasets and, analyzing the impact of key hyperparameters on these bounds, propose optimal choices.\n  On the computational side, we examine the discretization of the convexified problems and derive convergence rates. For low-dimensional datasets, these discretized problems are efficiently solvable using the simplex method. For high-dimensional datasets, we propose a sparsification algorithm that, combined with gradient descent for over-parameterized shallow NNs, yields effective solutions to the primal problems."
      },
      {
        "id": "oai:arXiv.org:2412.03017v2",
        "title": "Pixel-level and Semantic-level Adjustable Super-resolution: A Dual-LoRA Approach",
        "link": "https://arxiv.org/abs/2412.03017",
        "author": "Lingchen Sun, Rongyuan Wu, Zhiyuan Ma, Shuaizheng Liu, Qiaosi Yi, Lei Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.03017v2 Announce Type: replace \nAbstract: Diffusion prior-based methods have shown impressive results in real-world image super-resolution (SR). However, most existing methods entangle pixel-level and semantic-level SR objectives in the training process, struggling to balance pixel-wise fidelity and perceptual quality. Meanwhile, users have varying preferences on SR results, thus it is demanded to develop an adjustable SR model that can be tailored to different fidelity-perception preferences during inference without re-training. We present Pixel-level and Semantic-level Adjustable SR (PiSA-SR), which learns two LoRA modules upon the pre-trained stable-diffusion (SD) model to achieve improved and adjustable SR results. We first formulate the SD-based SR problem as learning the residual between the low-quality input and the high-quality output, then show that the learning objective can be decoupled into two distinct LoRA weight spaces: one is characterized by the $\\ell_2$-loss for pixel-level regression, and another is characterized by the LPIPS and classifier score distillation losses to extract semantic information from pre-trained classification and SD models. In its default setting, PiSA-SR can be performed in a single diffusion step, achieving leading real-world SR results in both quality and efficiency. By introducing two adjustable guidance scales on the two LoRA modules to control the strengths of pixel-wise fidelity and semantic-level details during inference, PiSASR can offer flexible SR results according to user preference without re-training. Codes and models can be found at https://github.com/csslc/PiSA-SR."
      },
      {
        "id": "oai:arXiv.org:2412.06786v2",
        "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis",
        "link": "https://arxiv.org/abs/2412.06786",
        "author": "M. Hamza Mughal, Rishabh Dabral, Merel C. J. Scholman, Vera Demberg, Christian Theobalt",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.06786v2 Announce Type: replace \nAbstract: Non-verbal communication often comprises of semantically rich gestures that help convey the meaning of an utterance. Producing such semantic co-speech gestures has been a major challenge for the existing neural systems that can generate rhythmic beat gestures, but struggle to produce semantically meaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based gesture generation approach that leverages Retrieval Augmented Generation (RAG) to produce natural-looking and semantically rich gestures. Our neuro-explicit gesture generation approach is designed to produce semantic gestures grounded in interpretable linguistic knowledge. We achieve this by using explicit domain knowledge to retrieve exemplar motions from a database of co-speech gestures. Once retrieved, we then inject these semantic exemplar gestures into our diffusion-based gesture generation pipeline using DDIM inversion and retrieval guidance at the inference time without any need of training. Further, we propose a control paradigm for guidance, that allows the users to modulate the amount of influence each retrieval insertion has over the generated sequence. Our comparative evaluations demonstrate the validity of our approach against recent gesture generation approaches. The reader is urged to explore the results on our project page."
      },
      {
        "id": "oai:arXiv.org:2412.07237v3",
        "title": "ArtFormer: Controllable Generation of Diverse 3D Articulated Objects",
        "link": "https://arxiv.org/abs/2412.07237",
        "author": "Jiayi Su, Youhe Feng, Zheng Li, Jinhua Song, Yangfan He, Botao Ren, Botian Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07237v3 Announce Type: replace \nAbstract: This paper presents a novel framework for modeling and conditional generation of 3D articulated objects. Troubled by flexibility-quality tradeoffs, existing methods are often limited to using predefined structures or retrieving shapes from static datasets. To address these challenges, we parameterize an articulated object as a tree of tokens and employ a transformer to generate both the object's high-level geometry code and its kinematic relations. Subsequently, each sub-part's geometry is further decoded using a signed-distance-function (SDF) shape prior, facilitating the synthesis of high-quality 3D shapes. Our approach enables the generation of diverse objects with high-quality geometry and varying number of parts. Comprehensive experiments on conditional generation from text descriptions demonstrate the effectiveness and flexibility of our method."
      },
      {
        "id": "oai:arXiv.org:2412.07755v2",
        "title": "SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models",
        "link": "https://arxiv.org/abs/2412.07755",
        "author": "Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07755v2 Announce Type: replace \nAbstract: Reasoning about motion and space is a fundamental cognitive capability that is required by multiple real-world applications. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only focus on static spatial relationships, and not dynamic awareness of motion and space, i.e., reasoning about the effect of egocentric and object motions on spatial relationships. Manually annotating such object and camera movements is expensive. Hence, we introduce SAT, a simulated spatial aptitude training dataset comprising both static and dynamic spatial reasoning across 175K question-answer (QA) pairs and 20K scenes. Complementing this, we also construct a small (150 image-QAs) yet challenging dynamic spatial test set using real-world images. Leveraging our SAT datasets and 6 existing static spatial benchmarks, we systematically investigate what improves both static and dynamic spatial awareness. Our results reveal that simulations are surprisingly effective at imparting spatial aptitude to MLMs that translate to real images. We show that perfect annotations in simulation are more effective than existing approaches of pseudo-annotating real images. For instance, SAT training improves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an average 8% on multiple spatial benchmarks, including our real-image dynamic test set and spatial reasoning on long videos -- even outperforming some large proprietary models. While reasoning over static relationships improves with synthetic training data, there is still considerable room for improvement for dynamic reasoning questions."
      },
      {
        "id": "oai:arXiv.org:2412.07992v3",
        "title": "Concept Bottleneck Large Language Models",
        "link": "https://arxiv.org/abs/2412.07992",
        "author": "Chung-En Sun, Tuomas Oikarinen, Berk Ustun, Tsui-Wei Weng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.07992v3 Announce Type: replace \nAbstract: We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel framework for building inherently interpretable Large Language Models (LLMs). In contrast to traditional black-box LLMs that rely on limited post-hoc interpretations, CB-LLMs integrate intrinsic interpretability directly into the LLMs -- allowing accurate explanations with scalability and transparency. We build CB-LLMs for two essential NLP tasks: text classification and text generation. In text classification, CB-LLMs is competitive with, and at times outperforms, traditional black-box models while providing explicit and interpretable reasoning. For the more challenging task of text generation, interpretable neurons in CB-LLMs enable precise concept detection, controlled generation, and safer outputs. The embedded interpretability empowers users to transparently identify harmful content, steer model behavior, and unlearn undesired concepts -- significantly enhancing the safety, reliability, and trustworthiness of LLMs, which are critical capabilities notably absent in existing models. Our code is available at https://github.com/Trustworthy-ML-Lab/CB-LLMs."
      },
      {
        "id": "oai:arXiv.org:2412.09754v3",
        "title": "ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation",
        "link": "https://arxiv.org/abs/2412.09754",
        "author": "Ali Athar, Xueqing Deng, Liang-Chieh Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.09754v3 Announce Type: replace \nAbstract: Recent advances in multimodal large language models (MLLMs) have expanded research in video understanding, primarily focusing on high-level tasks such as video captioning and question-answering. Meanwhile, a smaller body of work addresses dense, pixel-precise segmentation tasks, which typically involve category-guided or referral-based object segmentation. Although both directions are essential for developing models with human-level video comprehension, they have largely evolved separately, with distinct benchmarks and architectures. This paper aims to unify these efforts by introducing ViCaS, a new dataset containing thousands of challenging videos, each annotated with detailed, human-written captions and temporally consistent, pixel-accurate masks for multiple objects with phrase grounding. Our benchmark evaluates models on both holistic/high-level understanding and language-guided, pixel-precise segmentation. We also present carefully validated evaluation measures and propose an effective model architecture that can tackle our benchmark. Project page: https://ali2500.github.io/vicas-project/"
      },
      {
        "id": "oai:arXiv.org:2412.12153v2",
        "title": "Revisiting Weight Averaging for Model Merging",
        "link": "https://arxiv.org/abs/2412.12153",
        "author": "Jiho Choi, Donggyun Kim, Chanhyuk Lee, Seunghoon Hong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.12153v2 Announce Type: replace \nAbstract: Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors. Our method demonstrates robust and scalable performance on vision benchmarks across varying numbers of tasks and model sizes. Furthermore, we observe that our approach is applicable to natural language processing tasks with competitive performance."
      },
      {
        "id": "oai:arXiv.org:2412.12386v2",
        "title": "Interpretable LLM-based Table Question Answering",
        "link": "https://arxiv.org/abs/2412.12386",
        "author": "Giang Nguyen, Ivan Brugere, Shubham Sharma, Sanjay Kariyappa, Anh Totti Nguyen, Freddy Lecue",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.12386v2 Announce Type: replace \nAbstract: Interpretability for Table Question Answering (Table QA) is critical, particularly in high-stakes industries like finance or healthcare. Although recent approaches using Large Language Models (LLMs) have significantly improved Table QA performance, their explanations for how the answers are generated are ambiguous. To fill this gap, we introduce Plan-of-SQLs (POS), an interpretable Table QA approach designed to improve users' understanding of model decision-making. Through qualitative and quantitative evaluations with human and LLM judges, we show that: First, POS is the highest-quality explanation method, helps human users understand model behaviors, and facilitates model prediction verification. Second, when evaluated on popular and standard Table QA datasets (TabFact, WikiTQ, and FetaQA), POS achieves QA accuracy that is competitive with or superior to existing methods, while also offering greater efficiency-requiring significantly fewer LLM calls and table database queries-and robust performance on large-sized tables. Finally, we observe high agreement (up to 90%) between LLMs and human users when making decisions based on the same explanations, suggesting that LLMs could serve as an effective proxy for humans in evaluating explanations. This finding enables faster, more affordable evaluation of AI explanations-possibly accelerating trustworthy AI research while maintaining reliable judgments on interpretability."
      },
      {
        "id": "oai:arXiv.org:2412.15119v3",
        "title": "Parallelized Autoregressive Visual Generation",
        "link": "https://arxiv.org/abs/2412.15119",
        "author": "Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.15119v3 Announce Type: replace \nAbstract: Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://yuqingwang1029.github.io/PAR-project."
      },
      {
        "id": "oai:arXiv.org:2412.17671v2",
        "title": "A Bias-Free Training Paradigm for More General AI-generated Image Detection",
        "link": "https://arxiv.org/abs/2412.17671",
        "author": "Fabrizio Guillaro, Giada Zingarini, Ben Usman, Avneesh Sud, Davide Cozzolino, Luisa Verdoliva",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17671v2 Announce Type: replace \nAbstract: Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset design, highlighting the need for further research on this topic. Code and data are publicly available at https://grip-unina.github.io/B-Free/."
      },
      {
        "id": "oai:arXiv.org:2412.17811v3",
        "title": "ChatGarment: Garment Estimation, Generation and Editing via Large Language Models",
        "link": "https://arxiv.org/abs/2412.17811",
        "author": "Siyuan Bian, Chenghao Xu, Yuliang Xiu, Artur Grigorev, Zhen Liu, Cewu Lu, Michael J. Black, Yao Feng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17811v3 Announce Type: replace \nAbstract: We introduce ChatGarment, a novel approach that leverages large vision-language models (VLMs) to automate the estimation, generation, and editing of 3D garments from images or text descriptions. Unlike previous methods that struggle in real-world scenarios or lack interactive editing capabilities, ChatGarment can estimate sewing patterns from in-the-wild images or sketches, generate them from text descriptions, and edit garments based on user instructions, all within an interactive dialogue. These sewing patterns can then be draped on a 3D body and animated. This is achieved by finetuning a VLM to directly generate a JSON file that includes both textual descriptions of garment types and styles, as well as continuous numerical attributes. This JSON file is then used to create sewing patterns through a programming parametric model. To support this, we refine the existing programming model, GarmentCode, by expanding its garment type coverage and simplifying its structure for efficient VLM fine-tuning. Additionally, we construct a large-scale dataset of image-to-sewing-pattern and text-to-sewing-pattern pairs through an automated data pipeline. Extensive evaluations demonstrate ChatGarment's ability to accurately reconstruct, generate, and edit garments from multimodal inputs, highlighting its potential to simplify workflows in fashion and gaming applications. Code and data are available at https://chatgarment.github.io/ ."
      },
      {
        "id": "oai:arXiv.org:2412.17867v2",
        "title": "Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types",
        "link": "https://arxiv.org/abs/2412.17867",
        "author": "Ziming Guo, Chao Ma, Yinggang Sun, Tiancheng Zhao, Guangyao Wang, Hai Huang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.17867v2 Announce Type: replace \nAbstract: Recent advancements in large language models (LLMs) have significantly advanced text-to-SQL systems. However, most LLM-based methods often narrowly focus on SQL generation, neglecting the complexities of real-world conversational queries. This oversight can lead to unreliable responses, particularly for ambiguous questions that cannot be directly addressed with SQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q\\&amp;A interactions. Using MMSQL, we assessed the performance of popular LLMs, including both open-source and closed-source models, and identified key factors impacting their performance in such scenarios. Moreover, we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies. Our experiments demonstrate that this approach significantly enhances the model's ability to navigate the complexities of conversational dynamics, effectively handling the diverse and complex nature of user queries. Our dataset and code are publicly available at https://mcxiaoxiao.github.io/MMSQL."
      },
      {
        "id": "oai:arXiv.org:2412.18870v2",
        "title": "TSceneJAL: Joint Active Learning of Traffic Scenes for 3D Object Detection",
        "link": "https://arxiv.org/abs/2412.18870",
        "author": "Chenyang Lei, Meiying Zhang, Weiyuan Peng, Qi Hao, Chengzhong Xu, Chunlin Ji, Guang Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.18870v2 Announce Type: replace \nAbstract: Most autonomous driving (AD) datasets incur substantial costs for collection and labeling, inevitably yielding a plethora of low-quality and redundant data instances, thereby compromising performance and efficiency. Many applications in AD systems necessitate high-quality training datasets using both existing datasets and newly collected data. In this paper, we propose a traffic scene joint active learning (TSceneJAL) framework that can efficiently sample the balanced, diverse, and complex traffic scenes from both labeled and unlabeled data. The novelty of this framework is threefold: 1) a scene sampling scheme based on a category entropy, to identify scenes containing multiple object classes, thus mitigating class imbalance for the active learner; 2) a similarity sampling scheme, estimated through the directed graph representation and a marginalize kernel algorithm, to pick sparse and diverse scenes; 3) an uncertainty sampling scheme, predicted by a mixture density network, to select instances with the most unclear or complex regression outcomes for the learner. Finally, the integration of these three schemes in a joint selection strategy yields an optimal and valuable subdataset. Experiments on the KITTI, Lyft, nuScenes and SUScape datasets demonstrate that our approach outperforms existing state-of-the-art methods on 3D object detection tasks with up to 12% improvements."
      },
      {
        "id": "oai:arXiv.org:2412.20522v2",
        "title": "MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks",
        "link": "https://arxiv.org/abs/2412.20522",
        "author": "Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, Xiao Sun",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.20522v2 Announce Type: replace \nAbstract: While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in novel view synthesis and real-time rendering, the high memory consumption due to the use of millions of Gaussians limits its practicality. To mitigate this issue, improvements have been made by pruning unnecessary Gaussians, either through a hand-crafted criterion or by using learned masks. However, these methods deterministically remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized reconstruction performance from a long-term perspective. To address this issue, we introduce MaskGaussian, which models Gaussians as probabilistic entities rather than permanently removing them, and utilize them according to their probability of existence. To achieve this, we propose a masked-rasterization technique that enables unused yet probabilistically existing Gaussians to receive gradients, allowing for dynamic assessment of their contribution to the evolving scene and adjustment of their probability of existence. Hence, the importance of Gaussians iteratively changes and the pruned Gaussians are selected diversely. Extensive experiments demonstrate the superiority of the proposed method in achieving better rendering quality with fewer Gaussians than previous pruning methods, pruning over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be found at: https://github.com/kaikai23/MaskGaussian"
      },
      {
        "id": "oai:arXiv.org:2412.21127v2",
        "title": "What Makes for a Good Stereoscopic Image?",
        "link": "https://arxiv.org/abs/2412.21127",
        "author": "Netanel Y. Tamir, Shir Amir, Ranel Itzhaky, Noam Atia, Shobhita Sundaram, Stephanie Fu, Ron Sokolovsky, Phillip Isola, Tali Dekel, Richard Zhang, Miriam Farber",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.21127v2 Announce Type: replace \nAbstract: With rapid advancements in virtual reality (VR) headsets, effectively measuring stereoscopic quality of experience (SQoE) has become essential for delivering immersive and comfortable 3D experiences. However, most existing stereo metrics focus on isolated aspects of the viewing experience such as visual discomfort or image quality, and have traditionally faced data limitations. To address these gaps, we present SCOPE (Stereoscopic COntent Preference Evaluation), a new dataset comprised of real and synthetic stereoscopic images featuring a wide range of common perceptual distortions and artifacts. The dataset is labeled with preference annotations collected on a VR headset, with our findings indicating a notable degree of consistency in user preferences across different headsets. Additionally, we present iSQoE, a new model for stereo quality of experience assessment trained on our dataset. We show that iSQoE aligns better with human preferences than existing methods when comparing mono-to-stereo conversion methods."
      },
      {
        "id": "oai:arXiv.org:2501.00164v2",
        "title": "Measuring Large Language Models Capacity to Annotate Journalistic Sourcing",
        "link": "https://arxiv.org/abs/2501.00164",
        "author": "Subramaniam Vincent, Phoebe Wang, Zhan Shi, Sahas Koka, Yi Fang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00164v2 Announce Type: replace \nAbstract: Since the launch of ChatGPT in late 2022, the capacities of Large Language Models and their evaluation have been in constant discussion and evaluation both in academic research and in the industry. Scenarios and benchmarks have been developed in several areas such as law, medicine and math (Bommasani et al., 2023) and there is continuous evaluation of model variants. One area that has not received sufficient scenario development attention is journalism, and in particular journalistic sourcing and ethics. Journalism is a crucial truth-determination function in democracy (Vincent, 2023), and sourcing is a crucial pillar to all original journalistic output. Evaluating the capacities of LLMs to annotate stories for the different signals of sourcing and how reporters justify them is a crucial scenario that warrants a benchmark approach. It offers potential to build automated systems to contrast more transparent and ethically rigorous forms of journalism with everyday fare. In this paper we lay out a scenario to evaluate LLM performance on identifying and annotating sourcing in news stories on a five-category schema inspired from journalism studies (Gans, 2004). We offer the use case, our dataset and metrics and as the first step towards systematic benchmarking. Our accuracy findings indicate LLM-based approaches have more catching to do in identifying all the sourced statements in a story, and equally, in matching the type of sources. An even harder task is spotting source justifications."
      },
      {
        "id": "oai:arXiv.org:2501.03262v2",
        "title": "REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models",
        "link": "https://arxiv.org/abs/2501.03262",
        "author": "Jian Hu, Jason Klein Liu, Shen Wei",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.03262v2 Announce Type: replace \nAbstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. While state-of-the-art applications like ChatGPT/GPT-4 commonly employ Proximal Policy Optimization (PPO), the inclusion of a critic network introduces significant computational overhead. REINFORCE-based methods, such as REINFORCE Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO), address this limitation by eliminating the critic network. However, these approaches face challenges in accurate advantage estimation. Specifically, they estimate advantages independently for responses to each prompt, which can lead to overfitting on simpler prompts and vulnerability to reward hacking. To address these challenges, we introduce REINFORCE++, a novel approach that removes the critic model while using the normalized reward of a batch as the baseline. Our empirical evaluation demonstrates that REINFORCE++ exhibits robust performance across various reward models without requiring prompt set truncation. Furthermore, it achieves superior generalization in both RLHF and long chain-of-thought (CoT) settings compared to existing REINFORCE-based methods. The implementation is available at https://github.com/OpenRLHF/OpenRLHF."
      },
      {
        "id": "oai:arXiv.org:2501.06035v2",
        "title": "Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction",
        "link": "https://arxiv.org/abs/2501.06035",
        "author": "Cecilia Curreli, Dominik Muhle, Abhishek Saroha, Zhenzhang Ye, Riccardo Marin, Daniel Cremers",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.06035v2 Announce Type: replace \nAbstract: Probabilistic human motion prediction aims to forecast multiple possible future movements from past observations. While current approaches report high diversity and realism, they often generate motions with undetected limb stretching and jitter. To address this, we introduce SkeletonDiffusion, a latent diffusion model that embeds an explicit inductive bias on the human body within its architecture and training. Our model is trained with a novel nonisotropic Gaussian diffusion formulation that aligns with the natural kinematic structure of the human skeleton. Results show that our approach outperforms conventional isotropic alternatives, consistently generating realistic predictions while avoiding artifacts such as limb distortion. Additionally, we identify a limitation in commonly used diversity metrics, which may inadvertently favor models that produce inconsistent limb lengths within the same sequence. SkeletonDiffusion sets a new benchmark on real-world datasets, outperforming various baselines across multiple evaluation metrics. Visit our project page at https://ceveloper.github.io/publications/skeletondiffusion/ ."
      },
      {
        "id": "oai:arXiv.org:2501.07534v2",
        "title": "Investigating Map-Based Path Loss Models: A Study of Feature Representations in Convolutional Neural Networks",
        "link": "https://arxiv.org/abs/2501.07534",
        "author": "Ryan G. Dempsey, Jonathan Ethier, Halim Yanikomeroglu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.07534v2 Announce Type: replace \nAbstract: Path loss prediction is a beneficial tool for efficient use of the radio frequency spectrum. Building on prior research on high-resolution map-based path loss models, this paper studies convolutional neural network input representations in more detail. We investigate different methods of representing scalar features in convolutional neural networks. Specifically, we compare using frequency and distance as input channels to convolutional layers or as scalar inputs to regression layers. We assess model performance using three different feature configurations and find that representing scalar features as image channels results in the strongest generalization."
      },
      {
        "id": "oai:arXiv.org:2501.07742v3",
        "title": "RePoseD: Efficient Relative Pose Estimation With Known Depth Information",
        "link": "https://arxiv.org/abs/2501.07742",
        "author": "Yaqing Ding, Viktor Kocur, V\\'aclav V\\'avra, Zuzana Berger Haladov\\'a, Jian Yang, Torsten Sattler, Zuzana Kukelova",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.07742v3 Announce Type: replace \nAbstract: Recent advances in monocular depth estimation methods (MDE) and their improved accuracy open new possibilities for their applications. In this paper, we investigate how monocular depth estimates can be used for relative pose estimation. In particular, we are interested in answering the question whether using MDEs improves results over traditional point-based methods. We propose a novel framework for estimating the relative pose of two cameras from point correspondences with associated monocular depths. Since depth predictions are typically defined up to an unknown scale or even both unknown scale and shift parameters, our solvers jointly estimate the scale or both the scale and shift parameters along with the relative pose. We derive efficient solvers considering different types of depths for three camera configurations: (1) two calibrated cameras, (2) two cameras with an unknown shared focal length, and (3) two cameras with unknown different focal lengths. Our new solvers outperform state-of-the-art depth-aware solvers in terms of speed and accuracy. In extensive real experiments on multiple datasets and with various MDEs, we discuss which depth-aware solvers are preferable in which situation. The code will be made publicly available."
      },
      {
        "id": "oai:arXiv.org:2501.08174v2",
        "title": "Object-Centric 2D Gaussian Splatting: Background Removal and Occlusion-Aware Pruning for Compact Object Models",
        "link": "https://arxiv.org/abs/2501.08174",
        "author": "Marcel Rogge, Didier Stricker",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.08174v2 Announce Type: replace \nAbstract: Current Gaussian Splatting approaches are effective for reconstructing entire scenes but lack the option to target specific objects, making them computationally expensive and unsuitable for object-specific applications. We propose a novel approach that leverages object masks to enable targeted reconstruction, resulting in object-centric models. Additionally, we introduce an occlusion-aware pruning strategy to minimize the number of Gaussians without compromising quality. Our method reconstructs compact object models, yielding object-centric Gaussian and mesh representations that are up to 96% smaller and up to 71% faster to train compared to the baseline while retaining competitive quality. These representations are immediately usable for downstream applications such as appearance editing and physics simulation without additional processing."
      },
      {
        "id": "oai:arXiv.org:2501.08628v2",
        "title": "Transformer-based Multivariate Time Series Anomaly Localization",
        "link": "https://arxiv.org/abs/2501.08628",
        "author": "Charalampos Shimillas, Kleanthis Malialis, Konstantinos Fokianos, Marios M. Polycarpou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.08628v2 Announce Type: replace \nAbstract: With the growing complexity of Cyber-Physical Systems (CPS) and the integration of Internet of Things (IoT), the use of sensors for online monitoring generates large volume of multivariate time series (MTS) data. Consequently, the need for robust anomaly diagnosis in MTS is paramount to maintaining system reliability and safety. While significant advancements have been made in anomaly detection, localization remains a largely underexplored area, though crucial for intelligent decision-making. This paper introduces a novel transformer-based model for unsupervised anomaly diagnosis in MTS, with a focus on improving localization performance, through an in-depth analysis of the self-attention mechanism's learning behavior under both normal and anomalous conditions. We formulate the anomaly localization problem as a three-stage process: time-step, window, and segment-based. This leads to the development of the Space-Time Anomaly Score (STAS), a new metric inspired by the connection between transformer latent representations and space-time statistical models. STAS is designed to capture individual anomaly behaviors and inter-series dependencies, delivering enhanced localization performance. Additionally, the Statistical Feature Anomaly Score (SFAS) complements STAS by analyzing statistical features around anomalies, with their combination helping to reduce false alarms. Experiments on real world and synthetic datasets illustrate the model's superiority over state-of-the-art methods in both detection and localization tasks."
      },
      {
        "id": "oai:arXiv.org:2501.09466v2",
        "title": "DEFOM-Stereo: Depth Foundation Model Based Stereo Matching",
        "link": "https://arxiv.org/abs/2501.09466",
        "author": "Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, Rui Huang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.09466v2 Announce Type: replace \nAbstract: Stereo matching is a key technique for metric depth estimation in computer vision and robotics. Real-world challenges like occlusion and non-texture hinder accurate disparity estimation from binocular matching cues. Recently, monocular relative depth estimation has shown remarkable generalization using vision foundation models. Thus, to facilitate robust stereo matching with monocular depth cues, we incorporate a robust monocular relative depth model into the recurrent stereo-matching framework, building a new framework for depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature extraction stage, we construct the combined context and matching feature encoder by integrating features from conventional CNNs and DEFOM. In the update stage, we use the depth predicted by DEFOM to initialize the recurrent disparity and introduce a scale update module to refine the disparity at the correct scale. DEFOM-Stereo is verified to have much stronger zero-shot generalization compared with SOTA methods. Moreover, DEFOM-Stereo achieves top performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks, ranking $1^{st}$ on many metrics. In the joint evaluation under the robust vision challenge, our model simultaneously outperforms previous models on the individual benchmarks, further demonstrating its outstanding capabilities."
      },
      {
        "id": "oai:arXiv.org:2501.14846v4",
        "title": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval",
        "link": "https://arxiv.org/abs/2501.14846",
        "author": "Libo Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.14846v4 Announce Type: replace \nAbstract: In view of the gap in the current large language model in sharing memory across dialogues, this research proposes a wormhole memory module (WMM) to realize memory as a Rubik's cube that can be arbitrarily retrieved between different dialogues. Through simulation experiments, the researcher built an experimental framework based on the Python environment and used setting memory barriers to simulate the current situation where memories between LLMs dialogues are difficult to share. The CoQA development data set was imported into the experiment, and the feasibility of its cross-dialogue memory retrieval function was verified for WMM's nonlinear indexing and dynamic retrieval, and a comparative analysis was conducted with the capabilities of Titans and MemGPT memory modules. Experimental results show that WMM demonstrated the ability to retrieve memory across dialogues and the stability of quantitative indicators in eight experiments. It contributes new technical approaches to the optimization of memory management of LLMs and provides experience for the practical application in the future."
      },
      {
        "id": "oai:arXiv.org:2502.00380v2",
        "title": "CoHiRF: A Scalable and Interpretable Clustering Framework for High-Dimensional Data",
        "link": "https://arxiv.org/abs/2502.00380",
        "author": "Bruno Belucci, Karim Lounici, Katia Meziani",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00380v2 Announce Type: replace \nAbstract: Clustering high-dimensional data poses significant challenges due to the curse of dimensionality, scalability issues, and the presence of noisy and irrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF), a novel clustering method designed to address these challenges effectively. CoHiRF leverages random feature selection to mitigate noise and dimensionality effects, repeatedly applies K-Means clustering in reduced feature spaces, and combines results through a unanimous consensus criterion. This iterative approach constructs a cluster assignment matrix, where each row records the cluster assignments of a sample across repetitions, enabling the identification of stable clusters by comparing identical rows. Clusters are organized hierarchically, enabling the interpretation of the hierarchy to gain insights into the dataset. CoHiRF is computationally efficient with a running time comparable to K-Means, scalable to massive datasets, and exhibits robust performance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and OPTICS. Experimental results on synthetic and real-world datasets confirm the method's ability to reveal meaningful patterns while maintaining scalability, making it a powerful tool for high-dimensional data analysis."
      },
      {
        "id": "oai:arXiv.org:2502.00536v2",
        "title": "CAD: Confidence-Aware Adaptive Displacement for Semi-Supervised Medical Image Segmentation",
        "link": "https://arxiv.org/abs/2502.00536",
        "author": "Wenbo Xiao, Zhihao Xu, Guiping Liang, Yangjun Deng, Yi Xiao",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00536v2 Announce Type: replace \nAbstract: Semi-supervised medical image segmentation aims to leverage minimal expert annotations, yet remains confronted by challenges in maintaining high-quality consistency learning. Excessive perturbations can degrade alignment and hinder precise decision boundaries, especially in regions with uncertain predictions. In this paper, we introduce Confidence-Aware Adaptive Displacement (CAD), a framework that selectively identifies and replaces the largest low-confidence regions with high-confidence patches. By dynamically adjusting both the maximum allowable replacement size and the confidence threshold throughout training, CAD progressively refines the segmentation quality without overwhelming the learning process. Experimental results on public medical datasets demonstrate that CAD effectively enhances segmentation quality, establishing new state-of-the-art accuracy in this field. The source code will be released after the paper is published."
      },
      {
        "id": "oai:arXiv.org:2502.00631v2",
        "title": "MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density Prediction",
        "link": "https://arxiv.org/abs/2502.00631",
        "author": "Xuyin Qi, Zeyu Zhang, Huazhan Zheng, Mingxi Chen, Numan Kutaiba, Ruth Lim, Cherie Chiang, Zi En Tham, Xuan Ren, Wenxin Zhang, Lei Zhang, Hao Zhang, Wenbing Lv, Guangzhen Yao, Renda Han, Kangsheng Wang, Mingyuan Li, Hongtao Mao, Yu Li, Zhibin Liao, Yang Zhao, Minh-Son To",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.00631v2 Announce Type: replace \nAbstract: Bone density prediction via CT scans to estimate T-scores is crucial, providing a more precise assessment of bone health compared to traditional methods like X-ray bone density tests, which lack spatial resolution and the ability to detect localized changes. However, CT-based prediction faces two major challenges: the high computational complexity of transformer-based architectures, which limits their deployment in portable and clinical settings, and the imbalanced, long-tailed distribution of real-world hospital data that skews predictions. To address these issues, we introduce MedConv, a convolutional model for bone density prediction that outperforms transformer models with lower computational demands. We also adapt Bal-CE loss and post-hoc logit adjustment to improve class balance. Extensive experiments on our AustinSpine dataset shows that our approach achieves up to 21% improvement in accuracy and 20% in ROC AUC over previous state-of-the-art methods."
      },
      {
        "id": "oai:arXiv.org:2502.02885v2",
        "title": "Expertized Caption Auto-Enhancement for Video-Text Retrieval",
        "link": "https://arxiv.org/abs/2502.02885",
        "author": "Baoyao Yang, Junxiang Chen, Wanyun Li, Wenbin Yao, Yang Zhou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.02885v2 Announce Type: replace \nAbstract: Video-text retrieval has been stuck in the information mismatch caused by personalized and inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders an effective cross-modal representation alignment, resulting in ambiguous retrieval results. Although text rewriting methods have been proposed to broaden text expressions, the modality gap remains significant, as the text representation space is hardly expanded with insufficient semantic enrichment.Instead, this paper turns to enhancing visual presentation, bridging video expression closer to textual representation via caption generation and thereby facilitating video-text matching.While multimodal large language models (mLLM) have shown a powerful capability to convert video content into text, carefully crafted prompts are essential to ensure the reasonableness and completeness of the generated captions. Therefore, this paper proposes an automatic caption enhancement method that improves expression quality and mitigates empiricism in augmented captions through self-learning.Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, further exploring the utilization potential of caption augmentation.Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo. Our code is publicly available at https://github.com/CaryXiang/ECA4VTR."
      },
      {
        "id": "oai:arXiv.org:2502.03123v2",
        "title": "Disentanglement in Difference: Directly Learning Semantically Disentangled Representations by Maximizing Inter-Factor Differences",
        "link": "https://arxiv.org/abs/2502.03123",
        "author": "Xingshen Zhang, Lin Wang, Shuangrong Liu, Xintao Lu, Chaoran Pang, Bo Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.03123v2 Announce Type: replace \nAbstract: In this study, Disentanglement in Difference(DiD) is proposed to address the inherent inconsistency between the statistical independence of latent variables and the goal of semantic disentanglement in disentanglement representation learning. Conventional disentanglement methods achieve disentanglement representation by improving statistical independence among latent variables. However, the statistical independence of latent variables does not necessarily imply that they are semantically unrelated, thus, improving statistical independence does not always enhance disentanglement performance. To address the above issue, DiD is proposed to directly learn semantic differences rather than the statistical independence of latent variables. In the DiD, a Difference Encoder is designed to measure the semantic differences; a contrastive loss function is established to facilitate inter-dimensional comparison. Both of them allow the model to directly differentiate and disentangle distinct semantic factors, thereby resolving the inconsistency between statistical independence and semantic disentanglement. Experimental results on the dSprites and 3DShapes datasets demonstrate that the proposed DiD outperforms existing mainstream methods across various disentanglement metrics."
      },
      {
        "id": "oai:arXiv.org:2502.06829v2",
        "title": "Convolution-Based Converter : A Weak-Prior Approach For Modeling Stochastic Processes Based On Conditional Density Estimation",
        "link": "https://arxiv.org/abs/2502.06829",
        "author": "Chaoran Pang, Lin Wang, Shuangrong Liu, Shikun Tian, WenHao Yue, Xingshen Zhang, Bo Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06829v2 Announce Type: replace \nAbstract: In this paper, a Convolution-Based Converter (CBC) is proposed to develop a methodology for removing the strong or fixed priors in estimating the probability distribution of targets based on observations in the stochastic process. Traditional approaches, e.g., Markov-based and Gaussian process-based methods, typically leverage observations to estimate targets based on strong or fixed priors (such as Markov properties or Gaussian prior). However, the effectiveness of these methods depends on how well their prior assumptions align with the characteristics of the problem. When the assumed priors are not satisfied, these approaches may perform poorly or even become unusable. To overcome the above limitation, we introduce the Convolution-Based converter (CBC), which implicitly estimates the conditional probability distribution of targets without strong or fixed priors, and directly outputs the expected trajectory of the stochastic process that satisfies the constraints from observations. This approach reduces the dependence on priors, enhancing flexibility and adaptability in modeling stochastic processes when addressing different problems. Experimental results demonstrate that our method outperforms existing baselines across multiple metrics."
      },
      {
        "id": "oai:arXiv.org:2502.07156v2",
        "title": "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "link": "https://arxiv.org/abs/2502.07156",
        "author": "Joseph Paul Cohen, Louis Blankemeier, Akshay Chaudhari",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.07156v2 Announce Type: replace \nAbstract: Counterfactual explanations enhance the interpretability of deep learning models in medical imaging, yet adapting them to 3D CT scans poses challenges due to volumetric complexity and resource demands. We extend the Latent Shift counterfactual generation method from 2D applications to explain 3D computed tomography (CT) scans classifiers. We address the challenges associated with 3D classifiers, such as limited training samples and high memory demands, by implementing a slice-based autoencoder and gradient blocking except for specific chunks of slices. This method leverages a 2D encoder trained on CT slices, which are subsequently combined to maintain 3D context. We demonstrate this technique on two models for clinical phenotype prediction and lung segmentation. Our approach is both memory-efficient and effective for generating interpretable counterfactuals in high-resolution 3D medical imaging."
      },
      {
        "id": "oai:arXiv.org:2502.11123v3",
        "title": "DuplexMamba: Enhancing Real-time Speech Conversations with Duplex and Streaming Capabilities",
        "link": "https://arxiv.org/abs/2502.11123",
        "author": "Xiangyu Lu, Wang Xu, Haoyu Wang, Hongyun Zhou, Haiyan Zhao, Conghui Zhu, Tiejun Zhao, Muyun Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11123v3 Announce Type: replace \nAbstract: Real-time speech conversation is essential for natural and efficient human-machine interactions, requiring duplex and streaming capabilities. Traditional Transformer-based conversational chatbots operate in a turn-based manner and exhibit quadratic computational complexity that grows as the input size increases. In this paper, we propose DuplexMamba, a Mamba-based end-to-end multimodal duplex model for speech-to-text conversation. DuplexMamba enables simultaneous input processing and output generation, dynamically adjusting to support real-time streaming. Specifically, we develop a Mamba-based speech encoder and adapt it with a Mamba-based language model. Furthermore, we introduce a novel duplex decoding strategy that enables DuplexMamba to process input and generate output simultaneously. Experimental results demonstrate that DuplexMamba successfully implements duplex and streaming capabilities while achieving performance comparable to several recently developed Transformer-based models in automatic speech recognition (ASR) tasks and voice assistant benchmark evaluations. Our code and model are released."
      },
      {
        "id": "oai:arXiv.org:2502.11167v3",
        "title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
        "link": "https://arxiv.org/abs/2502.11167",
        "author": "Bohan Lyu, Siqiao Huang, Zichen Liang, Qi-An Sun, Jiaming Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.11167v3 Announce Type: replace \nAbstract: Neural surrogate models have emerged as powerful and efficient tools in data mining. Meanwhile, large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks. We investigate a novel application: using LLMs as surrogate models for code execution prediction. Given LLMs' unique ability to understand and process diverse programs, they present a promising direction for building general-purpose surrogate models. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark with $1160$ problems covering $8$ key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. Through extensive empirical analysis of $21$ open-source and proprietary LLMs, we examine scaling laws, data efficiency, and predictive accuracy. Our findings reveal important insights about the feasibility of LLMs as efficient surrogates for computational processes, with implications for automated software testing, program analysis, and computational resource optimization in data mining applications. Code and dataset are released at https://github.com/Imbernoulli/SURGE."
      },
      {
        "id": "oai:arXiv.org:2502.13734v2",
        "title": "CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models",
        "link": "https://arxiv.org/abs/2502.13734",
        "author": "Nikolaos Dionelis, Jente Bosmans, Nicolas Long\\'ep\\'e",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.13734v2 Announce Type: replace \nAbstract: Performing accurate confidence quantification and assessment in pixel-wise regression tasks, which are downstream applications of AI Foundation Models for Earth Observation (EO), is important for deep neural networks to predict their failures, improve their performance and enhance their capabilities in real-world applications, for their practical deployment. For pixel-wise regression tasks, specifically utilizing remote sensing data from satellite imagery in EO Foundation Models, confidence quantification is a critical challenge. The focus of this research work is on developing a Foundation Model using EO satellite data that computes and assigns a confidence metric alongside regression outputs to improve the reliability and interpretability of predictions generated by deep neural networks. To this end, we develop, train and evaluate the proposed Confidence-Aware Regression Estimation (CARE) Foundation Model. Our model CARE computes and assigns confidence to regression results as downstream tasks of a Foundation Model for EO data, and performs a confidence-aware self-corrective learning method for the low-confidence regions. We evaluate the model CARE, and experimental results on multi-spectral data from the Copernicus Sentinel-2 satellite constellation to estimate the building density (i.e. monitoring urban growth), show that the proposed method can be successfully applied to important regression problems in EO and remote sensing. We also show that our model CARE outperforms other baseline methods."
      },
      {
        "id": "oai:arXiv.org:2502.14614v3",
        "title": "FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis",
        "link": "https://arxiv.org/abs/2502.14614",
        "author": "Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.14614v3 Announce Type: replace \nAbstract: Retrieval-Augmented Large Language Models (LLMs), which integrate external knowledge into LLMs, have shown remarkable performance in various medical domains, including clinical diagnosis. However, existing RAG methods struggle to effectively assess task difficulty to make retrieval decisions, thereby failing to meet the clinical requirements for balancing efficiency and accuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained \\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework that improves the reliability of RAG in disease diagnosis scenarios. FIND incorporates a fine-grained adaptive control module to determine whether retrieval is necessary based on the information density of the input. By optimizing the retrieval process and implementing a knowledge filtering module, FIND ensures that the retrieval is better suited to clinical scenarios. Experiments on three Chinese electronic medical record datasets demonstrate that FIND significantly outperforms various baseline methods, highlighting its effectiveness in clinical diagnosis tasks."
      },
      {
        "id": "oai:arXiv.org:2502.16923v2",
        "title": "A Systematic Survey of Automatic Prompt Optimization Techniques",
        "link": "https://arxiv.org/abs/2502.16923",
        "author": "Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen, Haibo Ding, Panpan Xu, Lin Lee Cheong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.16923v2 Announce Type: replace \nAbstract: Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework."
      },
      {
        "id": "oai:arXiv.org:2502.19790v2",
        "title": "Mixtera: A Data Plane for Foundation Model Training",
        "link": "https://arxiv.org/abs/2502.19790",
        "author": "Maximilian B\\\"other, Xiaozhe Yao, Tolga Kerimoglu, Dan Graur, Viktor Gsteiger, Ana Klimovic",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.19790v2 Announce Type: replace \nAbstract: State-of-the-art large language and vision models are trained over trillions of tokens that are aggregated from a large variety of sources. As training data collections grow, manually managing the samples becomes time-consuming, tedious, and prone to errors. Yet recent research shows that the data mixture and the order in which samples are visited during training can significantly influence model accuracy. We build and present Mixtera, a data plane for foundation model training that enables users to declaratively express which data samples should be used in which proportion and in which order during training. Mixtera is a centralized, read-only layer that is deployed on top of existing training data collections and can be declaratively queried. It operates independently of the filesystem structure and supports mixtures across arbitrary properties (e.g., language, source dataset) as well as dynamic adjustment of the mixture based on model feedback. We experimentally evaluate Mixtera and show that our implementation does not bottleneck training and scales to 256 GH200 superchips. We demonstrate how Mixtera supports recent advancements in mixing strategies by implementing the proposed Adaptive Data Optimization (ADO) algorithm in the system and evaluating its performance impact. We also explore the role of mixtures for vision-language models."
      },
      {
        "id": "oai:arXiv.org:2503.00383v2",
        "title": "Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems",
        "link": "https://arxiv.org/abs/2503.00383",
        "author": "Song Xia, Yi Yu, Wenhan Yang, Meiwen Ding, Zhuo Chen, Ling-Yu Duan, Alex C. Kot, Xudong Jiang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.00383v2 Announce Type: replace \nAbstract: By locally encoding raw data into intermediate features, collaborative inference enables end users to leverage powerful deep learning models without exposure of sensitive raw data to cloud servers. However, recent studies have revealed that these intermediate features may not sufficiently preserve privacy, as information can be leaked and raw data can be reconstructed via model inversion attacks (MIAs). Obfuscation-based methods, such as noise corruption, adversarial representation learning, and information filters, enhance the inversion robustness by obfuscating the task-irrelevant redundancy empirically. However, methods for quantifying such redundancy remain elusive, and the explicit mathematical relation between this redundancy minimization and inversion robustness enhancement has not yet been established. To address that, this work first theoretically proves that the conditional entropy of inputs given intermediate features provides a guaranteed lower bound on the reconstruction mean square error (MSE) under any MIA. Then, we derive a differentiable and solvable measure for bounding this conditional entropy based on the Gaussian mixture estimation and propose a conditional entropy maximization (CEM) algorithm to enhance the inversion robustness. Experimental results on four datasets demonstrate the effectiveness and adaptability of our proposed CEM; without compromising feature utility and computing efficiency, plugging the proposed CEM into obfuscation-based defense mechanisms consistently boosts their inversion robustness, achieving average gains ranging from 12.9\\% to 48.2\\%. Code is available at \\href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}."
      },
      {
        "id": "oai:arXiv.org:2503.04188v2",
        "title": "Measuring temporal effects of agent knowledge by date-controlled tool use",
        "link": "https://arxiv.org/abs/2503.04188",
        "author": "R. Patrick Xian, Qiming Cui, Stefan Bauer, Reza Abbasi-Asl",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04188v2 Announce Type: replace \nAbstract: Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet an improper configuration affects the quality of the agent's responses. Here, we assess the agent behavior using distinct date-controlled tools (DCTs) as stress test to measure the knowledge variability of large language model (LLM) agents. We demonstrate the temporal effects of an LLM agent as a writing assistant, which uses web search to complete scientific publication abstracts. We show that the temporality of search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent design and evaluations should take a dynamical view and implement measures to account for the temporal influence of external resources to ensure reliability."
      },
      {
        "id": "oai:arXiv.org:2503.04918v2",
        "title": "Fine-Tuning Transformer-Based Vision-Language Models for Robust Object Detection in Unstructured Environments",
        "link": "https://arxiv.org/abs/2503.04918",
        "author": "Aysegul Ucar, Soumyadeep Ro, Sanapala Satwika, Pamarthi Yasoda Gayathri, Mohmmad Ghaith Balsha",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.04918v2 Announce Type: replace \nAbstract: Vision-Language Models (VLMs) have emerged as powerful tools in artificial intelli-gence, capable of integrating textual and visual data for a unified understanding of complex scenes. While models such as Florence2, built on transformer architectures, have shown promise across general tasks, their performance in object detection within unstructured or cluttered environments remains underexplored. In this study, we fi-ne-tuned the Florence2 model for object detection tasks in non-constructed, complex environments. A comprehensive experimental framework was established involving multiple hardware configurations (NVIDIA T4, L4, and A100 GPUs), optimizers (AdamW, SGD), and varied hyperparameters including learning rates and LoRA (Low-Rank Adaptation) setups. Model training and evaluation were conducted on challenging datasets representative of real-world, disordered settings. The optimized Florence2 models exhibited significant improvements in object detection accuracy, with Mean Average Precision (mAP) metrics approaching or matching those of estab-lished models such as YOLOv8, YOLOv9, and YOLOv10. The integration of LoRA and careful fine-tuning of transformer layers contributed notably to these gains. Our find-ings highlight the adaptability of transformer-based VLMs like Florence2 for do-main-specific tasks, particularly in visually complex environments. The study under-scores the potential of fine-tuned VLMs to rival traditional convolution-based detec-tors, offering a flexible and scalable approach for advanced vision applications in re-al-world, unstructured settings."
      },
      {
        "id": "oai:arXiv.org:2503.06212v2",
        "title": "GraphGen+: Advancing Distributed Subgraph Generation and Graph Learning On Industrial Graphs",
        "link": "https://arxiv.org/abs/2503.06212",
        "author": "Yue Jin, Yongchao Liu, Chuntao Hong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06212v2 Announce Type: replace \nAbstract: Graph-based computations are crucial in a wide range of applications, where graphs can scale to trillions of edges. To enable efficient training on such large graphs, mini-batch subgraph sampling is commonly used, which allows training without loading the entire graph into memory. However, existing solutions face significant trade-offs: online subgraph generation, as seen in frameworks like DGL and PyG, is limited to a single machine, resulting in severe performance bottlenecks, while offline precomputed subgraphs, as in GraphGen, improve sampling efficiency but introduce large storage overhead and high I/O costs during training. To address these challenges, we propose \\textbf{GraphGen+}, an integrated framework that synchronizes distributed subgraph generation with in-memory graph learning, eliminating the need for external storage while significantly improving efficiency. GraphGen+ achieves a \\textbf{27$\\times$} speedup in subgraph generation compared to conventional SQL-like methods and a \\textbf{1.3$\\times$} speedup over GraphGen, supporting training on 1 million nodes per iteration and removing the overhead associated with precomputed subgraphs, making it a scalable and practical solution for industry-scale graph learning."
      },
      {
        "id": "oai:arXiv.org:2503.06790v2",
        "title": "GenDR: Lightning Generative Detail Restorator",
        "link": "https://arxiv.org/abs/2503.06790",
        "author": "Yan Wang, Shijie Zhao, Kai Chen, Kexin Zhang, Junlin Li, Li Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.06790v2 Announce Type: replace \nAbstract: Recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable success. However, fundamental misalignments between T2I and SR targets result in a dilemma between inference speed and detail fidelity. Specifically, T2I tasks prioritize multi-step inversion to synthesize coherent outputs aligned with textual prompts and shrink the latent space to reduce generating complexity. Contrariwise, SR tasks preserve most information from low-resolution input while solely restoring high-frequency details, thus necessitating sufficient latent space and fewer inference steps. To bridge the gap, we present a one-step diffusion model for generative detail restoration, GenDR, distilled from a tailored diffusion model with larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via representation alignment to expand latent space without enlarging the model size. Regarding step-distillation, we propose consistent score identity distillation (CiD) that incorporates SR task-specific loss into score distillation to leverage more SR priors and align the training target. Furthermore, we extend CiD with adversarial learning and representation alignment (CiDA) to enhance perceptual quality and accelerate training. We also polish the pipeline to achieve a more efficient inference. Experimental results demonstrate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity."
      },
      {
        "id": "oai:arXiv.org:2503.08111v2",
        "title": "MaRI: Material Retrieval Integration across Domains",
        "link": "https://arxiv.org/abs/2503.08111",
        "author": "Jianhui Wang, Zhifei Yang, Yangfan He, Huixiong Zhang, Yuxuan Chen, Jingwei Huang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.08111v2 Announce Type: replace \nAbstract: Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods."
      },
      {
        "id": "oai:arXiv.org:2503.11056v2",
        "title": "Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization",
        "link": "https://arxiv.org/abs/2503.11056",
        "author": "Kyle Sargent, Kyle Hsu, Justin Johnson, Li Fei-Fei, Jiajun Wu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.11056v2 Announce Type: replace \nAbstract: Since the advent of popular visual generation frameworks like VQGAN and latent diffusion models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard recipe in which images are compressed and reconstructed subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet-1K reconstruction. We propose FlowMo, a transformer-based diffusion autoencoder that achieves a new state-of-the-art for image tokenization at multiple compression rates without using convolutions, adversarial losses, spatially-aligned two-dimensional latent codes, or distilling from other tokenizers. Our key insight is that FlowMo training should be broken into a mode-matching pre-training stage and a mode-seeking post-training stage. In addition, we conduct extensive analyses and explore the training of generative models atop the FlowMo tokenizer. Our code and models will be available at http://kylesargent.github.io/flowmo ."
      },
      {
        "id": "oai:arXiv.org:2503.15275v2",
        "title": "Challenges and Trends in Egocentric Vision: A Survey",
        "link": "https://arxiv.org/abs/2503.15275",
        "author": "Xiang Li, Heqian Qiu, Lanxiao Wang, Hanwen Zhang, Chenghao Qi, Linfeng Han, Huiyu Xiong, Hongliang Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15275v2 Announce Type: replace \nAbstract: With the rapid development of artificial intelligence technologies and wearable devices, egocentric vision understanding has emerged as a new and challenging research direction, gradually attracting widespread attention from both academia and industry. Egocentric vision captures visual and multimodal data through cameras or sensors worn on the human body, offering a unique perspective that simulates human visual experiences. This paper provides a comprehensive survey of the research on egocentric vision understanding, systematically analyzing the components of egocentric scenes and categorizing the tasks into four main areas: subject understanding, object understanding, environment understanding, and hybrid understanding. We explore in detail the sub-tasks within each category. We also summarize the main challenges and trends currently existing in the field. Furthermore, this paper presents an overview of high-quality egocentric vision datasets, offering valuable resources for future research. By summarizing the latest advancements, we anticipate the broad applications of egocentric vision technologies in fields such as augmented reality, virtual reality, and embodied intelligence, and propose future research directions based on the latest developments in the field."
      },
      {
        "id": "oai:arXiv.org:2503.15289v2",
        "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification",
        "link": "https://arxiv.org/abs/2503.15289",
        "author": "Junnan Zhu, Min Xiao, Yining Wang, Feifei Zhai, Yu Zhou, Chengqing Zong",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15289v2 Announce Type: replace \nAbstract: LLMs have achieved remarkable fluency and coherence in text generation, yet their widespread adoption has raised concerns about content reliability and accountability. In high-stakes domains such as healthcare, law, and news, it is crucial to understand where and how the content is created. To address this, we introduce the Text pROVEnance (TROVE) challenge, designed to trace each sentence of a target text back to specific source sentences within potentially lengthy or multi-document inputs. Beyond identifying sources, TROVE annotates the fine-grained relationships (quotation, compression, inference, and others), providing a deep understanding of how each target sentence is formed. To benchmark TROVE, we construct our dataset by leveraging three public datasets covering 11 diverse scenarios (e.g., QA and summarization) in English and Chinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and long-document settings essential for provenance. To ensure high-quality data, we employ a three-stage annotation process: sentence retrieval, GPT provenance, and human provenance. We evaluate 11 LLMs under direct prompting and retrieval-augmented paradigms, revealing that retrieval is essential for robust performance, larger models perform better in complex relationship classification, and closed-source models often lead, yet open-source models show significant promise, particularly with retrieval augmentation."
      },
      {
        "id": "oai:arXiv.org:2503.15567v2",
        "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
        "link": "https://arxiv.org/abs/2503.15567",
        "author": "Yanchen Luo, Zhiyuan Liu, Yi Zhao, Sihang Li, Kenji Kawaguchi, Tat-Seng Chua, Xiang Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.15567v2 Announce Type: replace \nAbstract: 3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose \\textbf{U}nified Variational \\textbf{A}uto-\\textbf{E}ncoder for \\textbf{3D} Molecular Latent Diffusion Modeling (\\textbf{UAE-3D}), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both \\textit{de novo} and conditional 3D molecule generation, achieving leading efficiency and quality."
      },
      {
        "id": "oai:arXiv.org:2503.16692v2",
        "title": "Limits of trust in medical AI",
        "link": "https://arxiv.org/abs/2503.16692",
        "author": "Joshua Hatherley",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16692v2 Announce Type: replace \nAbstract: Artificial intelligence (AI) is expected to revolutionize the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in a variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI's progress in medicine, however, has led to concerns regarding the potential effects of this technology upon relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied upon, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely upon AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice."
      },
      {
        "id": "oai:arXiv.org:2503.16980v3",
        "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
        "link": "https://arxiv.org/abs/2503.16980",
        "author": "Haichao Zhang, Yun Fu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.16980v3 Announce Type: replace \nAbstract: Token-based video representation has emerged as a promising approach for enabling LLMs to interpret video content. However, existing token reduction, such as token pruning and token merging, often disrupt essential spatial-temporal positional embeddings, failing to adequately balance computational efficiency with fewer tokens. Consequently, these methods result in lengthy token sequences, limiting their applicability in scenarios requiring extreme token compression, such as video large language models. In this paper, we introduce the novel task of extreme short token reduction, aiming to represent extensive video sequences with a minimal number of tokens. To address this challenge, we propose Token Dynamics, a new video representation framework that dynamically reduces token count while preserving spatial-temporal coherence. Specifically, we disentangle video representations by separating visual embeddings from grid-level motion information, structuring them into: 1. a concise token hash table, created by clustering tokens that describe object-level content; 2. a token indices key map, capturing detailed spatial-temporal motion patterns across grids; 3. a token hash function, which vector-quantizes the token hash table to reconstruct the token sequence from the key map. Furthermore, we introduce a cross-dynamics attention mechanism that integrates motion features into the token base without increasing token length, thereby maintaining compactness and spatial-temporal integrity. The experiments demonstrate a reduction of token count to merely 0.07% of the original tokens, with only a minor performance drop of 1.13%. Additionally, we propose two novel subtasks within extreme token reduction (fixed-length and adaptive-length compression). Our method offers significantly lower theoretical complexity, fewer tokens, and enhanced throughput, thus providing an efficient solution for video LLMs."
      },
      {
        "id": "oai:arXiv.org:2503.18886v2",
        "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
        "link": "https://arxiv.org/abs/2503.18886",
        "author": "Weichen Fan, Amber Yijia Zheng, Raymond A. Yeh, Ziwei Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.18886v2 Announce Type: replace \nAbstract: Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)"
      },
      {
        "id": "oai:arXiv.org:2503.19901v2",
        "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization",
        "link": "https://arxiv.org/abs/2503.19901",
        "author": "Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, Jingbo Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.19901v2 Announce Type: replace \nAbstract: Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/"
      },
      {
        "id": "oai:arXiv.org:2503.20297v2",
        "title": "Traversing Distortion-Perception Tradeoff using a Single Score-Based Generative Model",
        "link": "https://arxiv.org/abs/2503.20297",
        "author": "Yuhan Wang, Suzhi Bi, Ying-Jun Angela Zhang, Xiaojun Yuan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20297v2 Announce Type: replace \nAbstract: The distortion-perception (DP) tradeoff reveals a fundamental conflict between distortion metrics (e.g., MSE and PSNR) and perceptual quality. Recent research has increasingly concentrated on evaluating denoising algorithms within the DP framework. However, existing algorithms either prioritize perceptual quality by sacrificing acceptable distortion, or focus on minimizing MSE for faithful restoration. When the goal shifts or noisy measurements vary, adapting to different points on the DP plane needs retraining or even re-designing the model. Inspired by recent advances in solving inverse problems using score-based generative models, we explore the potential of flexibly and optimally traversing DP tradeoffs using a single pre-trained score-based model. Specifically, we introduce a variance-scaled reverse diffusion process and theoretically characterize the marginal distribution. We then prove that the proposed sample process is an optimal solution to the DP tradeoff for conditional Gaussian distribution. Experimental results on two-dimensional and image datasets illustrate that a single score network can effectively and flexibly traverse the DP tradeoff for general denoising problems."
      },
      {
        "id": "oai:arXiv.org:2503.20871v3",
        "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
        "link": "https://arxiv.org/abs/2503.20871",
        "author": "Silin Gao, Sheryl Mathew, Li Mi, Sepideh Mamooler, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Syrielle Montariol, Antoine Bosselut",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20871v3 Announce Type: replace \nAbstract: Visual narrative generation transforms textual narratives into sequences of images illustrating the content of the text. However, generating visual narratives that are faithful to the input text and self-consistent across generated images remains an open challenge, due to the lack of knowledge constraints used for planning the stories. In this work, we propose a new benchmark, VinaBench, to address this challenge. Our benchmark annotates the underlying commonsense and discourse constraints in visual narrative samples, offering systematic scaffolds for learning the implicit strategies of visual storytelling. Based on the incorporated narrative constraints, we further propose novel metrics to closely evaluate the consistency of generated narrative images and the alignment of generations with the input textual narrative. Our results across three generative vision models demonstrate that learning with VinaBench's knowledge constraints effectively improves the faithfulness and cohesion of generated visual narratives."
      },
      {
        "id": "oai:arXiv.org:2503.20960v2",
        "title": "Multi-Modal Framing Analysis of News",
        "link": "https://arxiv.org/abs/2503.20960",
        "author": "Arnav Arora, Srishti Yadav, Maria Antoniak, Serge Belongie, Isabelle Augenstein",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.20960v2 Announce Type: replace \nAbstract: Automated frame analysis of political communication is a popular task in computational social science that is used to study how authors select aspects of a topic to frame its reception. So far, such studies have been narrow, in that they use a fixed set of pre-defined frames and focus only on the text, ignoring the visual contexts in which those texts appear. Especially for framing in the news, this leaves out valuable information about editorial choices, which include not just the written article but also accompanying photographs. To overcome such limitations, we present a method for conducting multi-modal, multi-label framing analysis at scale using large (vision-)language models. Grounding our work in framing theory, we extract latent meaning embedded in images used to convey a certain point and contrast that to the text by comparing the respective frames used. We also identify highly partisan framing of topics with issue-specific frame analysis found in prior qualitative work. We demonstrate a method for doing scalable integrative framing analysis of both text and image in news, providing a more complete picture for understanding media bias."
      },
      {
        "id": "oai:arXiv.org:2503.21157v2",
        "title": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?",
        "link": "https://arxiv.org/abs/2503.21157",
        "author": "Ashish Sardana",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21157v2 Announce Type: replace \nAbstract: This article surveys Evaluation models to automatically detect hallucinations in Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark of their performance across six RAG applications. Methods included in our study include: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation Model (HHEM), and the Trustworthy Language Model (TLM). These approaches are all reference-free, requiring no ground-truth answers/labels to catch incorrect LLM responses. Our study reveals that, across diverse RAG applications, some of these approaches consistently detect incorrect RAG responses with high precision/recall."
      },
      {
        "id": "oai:arXiv.org:2503.21939v2",
        "title": "Flexible Moment-Invariant Bases from Irreducible Tensors",
        "link": "https://arxiv.org/abs/2503.21939",
        "author": "Roxana Bujack, Emily Shinkle, Alice Allen, Tomas Suk, Nicholas Lubbers",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21939v2 Announce Type: replace \nAbstract: Moment invariants are a powerful tool for the generation of rotation-invariant descriptors needed for many applications in pattern detection, classification, and machine learning. A set of invariants is optimal if it is complete, independent, and robust against degeneracy in the input. In this paper, we show that the current state of the art for the generation of these bases of moment invariants, despite being robust against moment tensors being identically zero, is vulnerable to a degeneracy that is common in real-world applications, namely spherical functions. We show how to overcome this vulnerability by combining two popular moment invariant approaches: one based on spherical harmonics and one based on Cartesian tensor algebra."
      },
      {
        "id": "oai:arXiv.org:2503.21970v2",
        "title": "Q-MambaIR: Accurate Quantized Mamba for Efficient Image Restoration",
        "link": "https://arxiv.org/abs/2503.21970",
        "author": "Yujie Chen, Haotong Qin, Zhang Zhang, Michelo Magno, Luca Benini, Yawei Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21970v2 Announce Type: replace \nAbstract: State-Space Models (SSMs) have attracted considerable attention in Image Restoration (IR) due to their ability to scale linearly sequence length while effectively capturing long-distance dependencies. However, deploying SSMs to edge devices is challenging due to the constraints in memory, computing capacity, and power consumption, underscoring the need for efficient compression strategies. While low-bit quantization is an efficient model compression strategy for reducing size and accelerating IR tasks, SSM suffers substantial performance drops at ultra-low bit-widths (2-4 bits), primarily due to outliers that exacerbate quantization error. To address this challenge, we propose Q-MambaIR, an accurate, efficient, and flexible Quantized Mamba for IR tasks. Specifically, we introduce a Statistical Dynamic-balancing Learnable Scalar (DLS) to dynamically adjust the quantization mapping range, thereby mitigating the peak truncation loss caused by extreme values. Furthermore, we design a Range-floating Flexible Allocator (RFA) with an adaptive threshold to flexibly round values. This approach preserves high-frequency details and maintains the SSM's feature extraction capability. Notably, RFA also enables pre-deployment weight quantization, striking a balance between computational efficiency and model accuracy. Extensive experiments on IR tasks demonstrate that Q-MambaIR consistently outperforms existing quantized SSMs, achieving much higher state-of-the-art (SOTA) accuracy results with only a negligible increase in training computation and storage saving."
      },
      {
        "id": "oai:arXiv.org:2503.22036v2",
        "title": "Cognitive Prompts Using Guilford's Structure of Intellect Model",
        "link": "https://arxiv.org/abs/2503.22036",
        "author": "Oliver Kramer",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22036v2 Announce Type: replace \nAbstract: Large language models (LLMs) demonstrate strong language generation capabilities but often struggle with structured reasoning, leading to inconsistent or suboptimal problem-solving. To mitigate this limitation, Guilford's Structure of Intellect (SOI) model - a foundational framework from intelligence theory - is leveraged as the basis for cognitive prompt engineering. The SOI model categorizes cognitive operations such as pattern recognition, memory retrieval, and evaluation, offering a systematic approach to enhancing LLM reasoning and decision-making. This position paper presents a novel cognitive prompting approach for enforcing SOI-inspired reasoning for improving clarity, coherence, and adaptability in model responses."
      },
      {
        "id": "oai:arXiv.org:2503.22444v2",
        "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
        "link": "https://arxiv.org/abs/2503.22444",
        "author": "Pengsong Zhang, Heng Zhang, Huazhe Xu, Renjun Xu, Zhenting Wang, Cong Wang, Animesh Garg, Zhibin Li, Arash Ajoudani, Xinyu Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22444v2 Announce Type: replace \nAbstract: Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers."
      },
      {
        "id": "oai:arXiv.org:2503.22879v2",
        "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
        "link": "https://arxiv.org/abs/2503.22879",
        "author": "Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22879v2 Announce Type: replace \nAbstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba."
      },
      {
        "id": "oai:arXiv.org:2503.22976v2",
        "title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D",
        "link": "https://arxiv.org/abs/2503.22976",
        "author": "Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.22976v2 Announce Type: replace \nAbstract: Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning."
      },
      {
        "id": "oai:arXiv.org:2503.23200v2",
        "title": "A GAN-Enhanced Deep Learning Framework for Rooftop Detection from Historical Aerial Imagery",
        "link": "https://arxiv.org/abs/2503.23200",
        "author": "Pengyu Chen, Sicheng Wang, Cuizhen Wang, Senrong Wang, Beiao Huang, Lu Huang, Zhe Zang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23200v2 Announce Type: replace \nAbstract: Precise detection of rooftops from historical aerial imagery is essential for analyzing long-term urban development and human settlement patterns. Nonetheless, black-and-white analog photographs present considerable challenges for modern object detection frameworks due to their limited spatial resolution, absence of color information, and archival degradation. To address these challenges, this research introduces a two-stage image enhancement pipeline based on Generative Adversarial Networks (GANs): image colorization utilizing DeOldify, followed by super-resolution enhancement with Real-ESRGAN. The enhanced images were subsequently employed to train and evaluate rooftop detection models, including Faster R-CNN, DETReg, and YOLOv11n. The results demonstrate that the combination of colorization with super-resolution significantly enhances detection performance, with YOLOv11n achieving a mean Average Precision (mAP) exceeding 85\\%. This signifies an enhancement of approximately 40\\% over the original black-and-white images and 20\\% over images enhanced solely through colorization. The proposed method effectively bridges the gap between archival imagery and contemporary deep learning techniques, facilitating more reliable extraction of building footprints from historical aerial photographs. Code and resources for reproducing our results are publicly available at \\href{https://github.com/Pengyu-gis/Historical-Aerial-Photos}{github.com/Pengyu-gis/Historical-Aerial-Photos}."
      },
      {
        "id": "oai:arXiv.org:2503.24108v2",
        "title": "PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis",
        "link": "https://arxiv.org/abs/2503.24108",
        "author": "Anwesa Choudhuri, Zhongpai Gao, Meng Zheng, Benjamin Planche, Terrence Chen, Ziyan Wu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24108v2 Announce Type: replace \nAbstract: Early detection, accurate segmentation, classification and tracking of polyps during colonoscopy are critical for preventing colorectal cancer. Many existing deep-learning-based methods for analyzing colonoscopic videos either require task-specific fine-tuning, lack tracking capabilities, or rely on domain-specific pre-training. In this paper, we introduce PolypSegTrack, a novel foundation model that jointly addresses polyp detection, segmentation, classification and unsupervised tracking in colonoscopic videos. Our approach leverages a novel conditional mask loss, enabling flexible training across datasets with either pixel-level segmentation masks or bounding box annotations, allowing us to bypass task-specific fine-tuning. Our unsupervised tracking module reliably associates polyp instances across frames using object queries, without relying on any heuristics. We leverage a robust vision foundation model backbone that is pre-trained unsupervisedly on natural images, thereby removing the need for domain-specific pre-training. Extensive experiments on multiple polyp benchmarks demonstrate that our method significantly outperforms existing state-of-the-art approaches in detection, segmentation, classification, and tracking."
      },
      {
        "id": "oai:arXiv.org:2503.24121v2",
        "title": "IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration",
        "link": "https://arxiv.org/abs/2503.24121",
        "author": "Valentin Boussot, C\\'edric H\\'emon, Jean-Claude Nunes, Jason Downling, Simon Rouz\\'e, Caroline Lafond, Ana\\\"is Barateau, Jean-Louis Dillenseger",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.24121v2 Announce Type: replace \nAbstract: Image registration is fundamental in medical imaging, enabling precise alignment of anatomical structures for diagnosis, treatment planning, image-guided interventions, and longitudinal monitoring. This work introduces IMPACT (Image Metric with Pretrained model-Agnostic Comparison for Transmodality registration), a novel similarity metric designed for robust multimodal image registration. Rather than relying on raw intensities, handcrafted descriptors, or task-specific training, IMPACT defines a semantic similarity measure based on the comparison of deep features extracted from large-scale pretrained segmentation models. By leveraging representations from models such as TotalSegmentator, Segment Anything (SAM), and other foundation networks, IMPACT provides a task-agnostic, training-free solution that generalizes across imaging modalities. These features, originally trained for segmentation, offer strong spatial correspondence and semantic alignment capabilities, making them naturally suited for registration. The method integrates seamlessly into both algorithmic (Elastix) and learning-based (VoxelMorph) frameworks, leveraging the strengths of each. IMPACT was evaluated on five challenging 3D registration tasks involving thoracic CT/CBCT and pelvic MR/CT datasets. Quantitative metrics, including Target Registration Error and Dice Similarity Coefficient, demonstrated consistent improvements in anatomical alignment over baseline methods. Qualitative analyses further highlighted the robustness of the proposed metric in the presence of noise, artifacts, and modality variations. With its versatility, efficiency, and strong performance across diverse tasks, IMPACT offers a powerful solution for advancing multimodal image registration in both clinical and research settings."
      },
      {
        "id": "oai:arXiv.org:2504.00030v2",
        "title": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding",
        "link": "https://arxiv.org/abs/2504.00030",
        "author": "Aayush Gautam, Susav Shrestha, Narasimha Reddy",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00030v2 Announce Type: replace \nAbstract: Speculative decoding accelerates large language model (LLM) inference by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, selecting an optimal speculation length is critical for maximizing speedup while minimizing wasted computation. We introduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive algorithms that dynamically adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism. Evaluated on SpecBench across multiple tasks and model pairs, our method outperforms other heuristic-based approaches and fixed-length speculative decoding, achieving an average speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%) with \\textit{GammaTune+}, while reducing performance variance. This makes \\textit{GammaTune} a robust and efficient solution for real-world deployment."
      },
      {
        "id": "oai:arXiv.org:2504.00349v2",
        "title": "Reducing Smoothness with Expressive Memory Enhanced Hierarchical Graph Neural Networks",
        "link": "https://arxiv.org/abs/2504.00349",
        "author": "Thomas Bailie, Yun Sing Koh, S. Karthik Mukkavilli, Varvara Vetrova",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00349v2 Announce Type: replace \nAbstract: Graphical forecasting models learn the structure of time series data via projecting onto a graph, with recent techniques capturing spatial-temporal associations between variables via edge weights. Hierarchical variants offer a distinct advantage by analysing the time series across multiple resolutions, making them particularly effective in tasks like global weather forecasting, where low-resolution variable interactions are significant. A critical challenge in hierarchical models is information loss during forward or backward passes through the hierarchy. We propose the Hierarchical Graph Flow (HiGFlow) network, which introduces a memory buffer variable of dynamic size to store previously seen information across variable resolutions. We theoretically show two key results: HiGFlow reduces smoothness when mapping onto new feature spaces in the hierarchy and non-strictly enhances the utility of message-passing by improving Weisfeiler-Lehman (WL) expressivity. Empirical results demonstrate that HiGFlow outperforms state-of-the-art baselines, including transformer models, by at least an average of 6.1% in MAE and 6.2% in RMSE. Code is available at https://github.com/TB862/ HiGFlow.git."
      },
      {
        "id": "oai:arXiv.org:2504.00457v3",
        "title": "Distilling Multi-view Diffusion Models into 3D Generators",
        "link": "https://arxiv.org/abs/2504.00457",
        "author": "Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00457v3 Announce Type: replace \nAbstract: We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher's probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: https://qinbaigao.github.io/DD3G_project/"
      },
      {
        "id": "oai:arXiv.org:2504.00564v2",
        "title": "Geometric Median Matching for Robust k-Subset Selection from Noisy Data",
        "link": "https://arxiv.org/abs/2504.00564",
        "author": "Anish Acharya, Sujay Sanghavi, Alexandros G. Dimakis, Inderjit S Dhillon",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00564v2 Announce Type: replace \nAbstract: Data pruning -- the combinatorial task of selecting a small and representative subset from a large dataset, is crucial for mitigating the enormous computational costs associated with training data-hungry modern deep learning models at scale. Since large scale data collections are invariably noisy, developing data pruning strategies that remain robust even in the presence of corruption is critical in practice. However, existing data pruning methods often fail under high corruption rates due to their reliance on empirical mean estimation, which is highly sensitive to outliers.\n  In response, we propose Geometric Median (GM) Matching, a novel k-subset selection strategy that leverages Geometric Median -- a robust estimator with an optimal breakdown point of 1/2; to enhance resilience against noisy data. Our method iteratively selects a k-subset such that the mean of the subset approximates the GM of the (potentially) noisy dataset, ensuring robustness even under arbitrary corruption. We provide theoretical guarantees, showing that GM Matching enjoys an improved O(1/k) convergence rate -- a quadratic improvement over random sampling, even under arbitrary corruption. Extensive experiments across image classification and image generation tasks demonstrate that GM Matching consistently outperforms existing pruning approaches, particularly in high-corruption settings and at high pruning rates; making it a strong baseline for robust data pruning."
      },
      {
        "id": "oai:arXiv.org:2504.00824v2",
        "title": "ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations",
        "link": "https://arxiv.org/abs/2504.00824",
        "author": "Yubo Wang, Xueguang Ma, Ping Nie, Huaye Zeng, Zhiheng Lyu, Yuxuan Zhang, Benjamin Schneider, Yi Lu, Xiang Yue, Wenhu Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00824v2 Announce Type: replace \nAbstract: Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their ability to support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], which is then used to query a citation database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to improve efficiency. Our model is built upon Qwen-2.5-7B and trained on 500K papers from arXiv. It achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality -- measured across relevance, coherence, academic rigor, completeness, and innovation -- significantly surpassing all existing models, including much larger ones like the Retrieval-Augmented Qwen2.5-72B-Instruct. Human studies further demonstrate that ScholarCopilot, despite being a 7B model, significantly outperforms ChatGPT, achieving 100% preference in citation quality and over 70% in overall usefulness."
      },
      {
        "id": "oai:arXiv.org:2504.00877v2",
        "title": "An Investigation into the Causal Mechanism of Political Opinion Dynamics: A Model of Hierarchical Coarse-Graining with Community-Bounded Social Influence",
        "link": "https://arxiv.org/abs/2504.00877",
        "author": "Valeria Widler, Barbara Kaminska, Andre C. R. Martins, Ivan Puga-Gonzalez",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00877v2 Announce Type: replace \nAbstract: The increasing polarization in democratic societies is an emergent outcome of political opinion dynamics. Yet, the fundamental mechanisms behind the formation of political opinions, from individual beliefs to collective consensus, remain unknown. Understanding that a causal mechanism must account for both bottom-up and top-down influences, we conceptualize political opinion dynamics as hierarchical coarse-graining, where microscale opinions integrate into a macro-scale state variable. Using the CODA (Continuous Opinions Discrete Actions) model, we simulate Bayesian opinion updating, social identity-based information integration, and migration between social identity groups to represent higher-level connectivity. This results in coarse-graining across micro, meso, and macro levels. Our findings show that higher-level connectivity shapes information integration, yielding three regimes: independent (disconnected, local convergence), parallel (fast, global convergence), and iterative (slow, stepwise convergence). In the iterative regime, low connectivity fosters transient diversity, indicating an informed consensus. In all regimes, time-scale separation leads to downward causation, where agents converge on the aggregate majority choice, driving consensus. Critically, any degree of coherent higher-level information integration can overcome misalignment via global downward causation. The results highlight how emergent properties of the causal mechanism, such as downward causation, are essential for consensus and may inform more precise investigations into polarized political discourse."
      },
      {
        "id": "oai:arXiv.org:2504.01128v2",
        "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety",
        "link": "https://arxiv.org/abs/2504.01128",
        "author": "Andrei Dumitriu, Florin Tatui, Florin Miron, Aakash Ralhan, Radu Tudor Ionescu, Radu Timofte",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01128v2 Announce Type: replace \nAbstract: Rip currents are strong, localized and narrow currents of water that flow outwards into the sea, causing numerous beach-related injuries and fatalities worldwide. Accurate identification of rip currents remains challenging due to their amorphous nature and the lack of annotated data, which often requires expert knowledge. To address these issues, we present RipVIS, a large-scale video instance segmentation benchmark explicitly designed for rip current segmentation. RipVIS is an order of magnitude larger than previous datasets, featuring $184$ videos ($212,328$ frames), of which $150$ videos ($163,528$ frames) are with rip currents, collected from various sources, including drones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse visual contexts, such as wave-breaking patterns, sediment flows, and water color variations, across multiple global locations, including USA, Mexico, Costa Rica, Portugal, Italy, Greece, Romania, Sri Lanka, Australia and New Zealand. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic scenarios, supplemented by an additional $34$ videos ($48,800$ frames) without rip currents. We conduct comprehensive experiments with Mask R-CNN, Cascade Mask R-CNN, SparseInst and YOLO11, fine-tuning these models for the task of rip current segmentation. Results are reported in terms of multiple metrics, with a particular focus on the $F_2$ score to prioritize recall and reduce false negatives. To enhance segmentation performance, we introduce a novel post-processing step based on Temporal Confidence Aggregation (TCA). RipVIS aims to set a new standard for rip current segmentation, contributing towards safer beach environments. We offer a benchmark website to share data, models, and results with the research community, encouraging ongoing collaboration and future contributions, at https://ripvis.ai."
      },
      {
        "id": "oai:arXiv.org:2504.01281v2",
        "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
        "link": "https://arxiv.org/abs/2504.01281",
        "author": "Sakhinana Sagar Srinivas, Venkataramana Runkana",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01281v2 Announce Type: replace \nAbstract: We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications."
      },
      {
        "id": "oai:arXiv.org:2504.01298v2",
        "title": "Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation",
        "link": "https://arxiv.org/abs/2504.01298",
        "author": "Shiyong Liu, Zhihao Li, Xiao Tang, Jianzhuang Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01298v2 Announce Type: replace \nAbstract: Most model-based 3D hand pose and shape estimation methods directly regress the parametric model parameters from an image to obtain 3D joints under weak supervision. However, these methods involve solving a complex optimization problem with many local minima, making training difficult. To address this challenge, we propose learning direction-aware hybrid features (DaHyF) that fuse implicit image features and explicit 2D joint coordinate features. This fusion is enhanced by the pixel direction information in the camera coordinate system to estimate pose, shape, and camera viewpoint. Our method directly predicts 3D hand poses with DaHyF representation and reduces jittering during motion capture using prediction confidence based on contrastive learning. We evaluate our method on the FreiHAND dataset and show that it outperforms existing state-of-the-art methods by more than 33% in accuracy. DaHyF also achieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the metric of Mean Joint Error (after scale and translation alignment). Compared to the second-best results, the largest improvement observed is 10%. We also demonstrate its effectiveness in real-time motion capture scenarios with hand position variability, occlusion, and motion blur."
      },
      {
        "id": "oai:arXiv.org:2504.01346v2",
        "title": "GTR: Graph-Table-RAG for Cross-Table Question Answering",
        "link": "https://arxiv.org/abs/2504.01346",
        "author": "Jiaru Zou, Dongqi Fu, Sirui Chen, Xinrui He, Zihao Li, Yada Zhu, Jiawei Han, Jingrui He",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01346v2 Announce Type: replace \nAbstract: Beyond pure text, a substantial amount of knowledge is stored in tables. In real-world scenarios, user questions often require retrieving answers that are distributed across multiple tables. GraphRAG has recently attracted much attention for enhancing LLMs' reasoning capabilities by organizing external knowledge to address ad-hoc and complex questions, exemplifying a promising direction for cross-table question answering. In this paper, to address the current gap in available data, we first introduce a multi-table benchmark, MutliTableQA, comprising 60k tables and 25k user queries collected from real-world sources. Then, we propose the first Graph-Table-RAG framework, namely GTR, which reorganizes table corpora into a heterogeneous graph, employs a hierarchical coarse-to-fine retrieval process to extract the most relevant tables, and integrates graph-aware prompting for downstream LLMs' tabular reasoning. Extensive experiments show that GTR exhibits superior cross-table question-answering performance while maintaining high deployment efficiency, demonstrating its real-world practical applicability."
      },
      {
        "id": "oai:arXiv.org:2504.01515v2",
        "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis",
        "link": "https://arxiv.org/abs/2504.01515",
        "author": "Zixuan Wang, Duo Peng, Feng Chen, Yuwei Yang, Yinjie Lei",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01515v2 Announce Type: replace \nAbstract: Conditional image synthesis is a crucial task with broad applications, such as artistic creation and virtual reality. However, current generative methods are often task-oriented with a narrow scope, handling a restricted condition with constrained applicability. In this paper, we propose a novel approach that treats conditional image synthesis as the modular combination of diverse fundamental condition units. Specifically, we divide conditions into three primary units: text, layout, and drag. To enable effective control over these conditions, we design a dedicated alignment module for each. For the text condition, we introduce a Dense Concept Alignment (DCA) module, which achieves dense visual-text alignment by drawing on diverse textual concepts. For the layout condition, we propose a Dense Geometry Alignment (DGA) module to enforce comprehensive geometric constraints that preserve the spatial configuration. For the drag condition, we introduce a Dense Motion Alignment (DMA) module to apply multi-level motion regularization, ensuring that each pixel follows its desired trajectory without visual artifacts. By flexibly inserting and combining these alignment modules, our framework enhances the model's adaptability to diverse conditional generation tasks and greatly expands its application range. Extensive experiments demonstrate the superior performance of our framework across a variety of conditions, including textual description, segmentation mask (bounding box), drag manipulation, and their combinations. Code is available at https://github.com/ZixuanWang0525/DADG."
      },
      {
        "id": "oai:arXiv.org:2504.01591v2",
        "title": "Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval",
        "link": "https://arxiv.org/abs/2504.01591",
        "author": "Adriano Fragomeni, Dima Damen, Michael Wray",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01591v2 Announce Type: replace \nAbstract: Video retrieval requires aligning visual content with corresponding natural language descriptions. In this paper, we introduce Modality Auxiliary Concepts for Video Retrieval (MAC-VR), a novel approach that leverages modality-specific tags -- automatically extracted from foundation models -- to enhance video retrieval. We propose to align modalities in a latent space, along with learning and aligning auxiliary latent concepts, derived from the features of a video and its corresponding caption. We introduce these auxiliary concepts to improve the alignment of visual and textual latent concepts, and so are able to distinguish concepts from one other. We conduct extensive experiments on five diverse datasets: MSR-VTT, DiDeMo, TGIF, Charades and YouCook2. The experimental results consistently demonstrate that modality-specific tags improve cross-modal alignment, outperforming current state-of-the-art methods across three datasets and performing comparably or better across the other two."
      },
      {
        "id": "oai:arXiv.org:2504.01659v2",
        "title": "Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks",
        "link": "https://arxiv.org/abs/2504.01659",
        "author": "Haosheng Li, Junjie Chen, Yuecong Xu, Kemi Ding",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01659v2 Announce Type: replace \nAbstract: Unsupervised domain adaptation (UDA) frameworks have shown good generalization capabilities for 3D point cloud semantic segmentation models on clean data. However, existing works overlook adversarial robustness when the source domain itself is compromised. To comprehensively explore the robustness of the UDA frameworks, we first design a stealthy adversarial point cloud generation attack that can significantly contaminate datasets with only minor perturbations to the point cloud surface. Based on that, we propose a novel dataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds. With the generated corrupted data, we further develop the Adversarial Adaptation Framework (AAF) as the countermeasure. Specifically, by extending the key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss) and utilizing a decoder branch, our approach enables the model to focus on long-tail classes during the pre-training phase and leverages high-confidence decoded point cloud information to restore point cloud structures during the adaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where the results demonstrate that our AAF method can mitigate performance degradation under source adversarial perturbations for UDA in the 3D point cloud segmentation application."
      },
      {
        "id": "oai:arXiv.org:2504.01667v2",
        "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish",
        "link": "https://arxiv.org/abs/2504.01667",
        "author": "Cedric Lothritz, Jordi Cabot",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01667v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have become an increasingly important tool in research and society at large. While LLMs are regularly used all over the world by experts and lay-people alike, they are predominantly developed with English-speaking users in mind, performing well in English and other wide-spread languages while less-resourced languages such as Luxembourgish are seen as a lower priority. This lack of attention is also reflected in the sparsity of available evaluation tools and datasets. In this study, we investigate the viability of language proficiency exams as such evaluation tools for the Luxembourgish language. We find that large models such as ChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller models show weak performances. We also find that the performances in such language exams can be used to predict performances in other NLP tasks."
      },
      {
        "id": "oai:arXiv.org:2504.01707v2",
        "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation",
        "link": "https://arxiv.org/abs/2504.01707",
        "author": "Bowen Cao, Deng Cai, Wai Lam",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01707v2 Announce Type: replace \nAbstract: In-context learning (ICL) is critical for large language models (LLMs), but its effectiveness is constrained by finite context windows, particularly in ultra-long contexts. To overcome this, we introduce InfiniteICL, a framework that parallels context and parameters in LLMs with short- and long-term memory in human cognitive systems, focusing on transforming temporary context knowledge into permanent parameter updates. This approach significantly reduces memory usage, maintains robust performance across varying input lengths, and theoretically enables infinite context integration through the principles of context knowledge elicitation, selection, and consolidation. Evaluations demonstrate that our method reduces context length by 90% while achieving 103% average performance of full-context prompting across fact recall, grounded reasoning, and skill acquisition tasks. When conducting sequential multi-turn transformations on complex, real-world contexts (with length up to 2M tokens), our approach surpasses full-context prompting while using only 0.4% of the original contexts. These findings highlight InfiniteICL's potential to enhance the scalability and efficiency of LLMs by breaking the limitations of conventional context window sizes."
      },
      {
        "id": "oai:arXiv.org:2504.01719v2",
        "title": "Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline Reinforcement Learning",
        "link": "https://arxiv.org/abs/2504.01719",
        "author": "Ke Jiang, Wen Jiang, Yao Li, Xiaoyang Tan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01719v2 Announce Type: replace \nAbstract: We address the challenge of offline reinforcement learning using realistic data, specifically non-expert data collected through sub-optimal behavior policies. Under such circumstance, the learned policy must be safe enough to manage distribution shift while maintaining sufficient flexibility to deal with non-expert (bad) demonstrations from offline data.To tackle this issue, we introduce a novel method called Outcome-Driven Action Flexibility (ODAF), which seeks to reduce reliance on the empirical action distribution of the behavior policy, hence reducing the negative impact of those bad demonstrations.To be specific, a new conservative reward mechanism is developed to deal with distribution shift by evaluating actions according to whether their outcomes meet safety requirements - remaining within the state support area, rather than solely depending on the actions' likelihood based on offline data.Besides theoretical justification, we provide empirical evidence on widely used MuJoCo and various maze benchmarks, demonstrating that our ODAF method, implemented using uncertainty quantification techniques, effectively tolerates unseen transitions for improved \"trajectory stitching,\" while enhancing the agent's ability to learn from realistic non-expert data."
      },
      {
        "id": "oai:arXiv.org:2504.01722v2",
        "title": "GSR4B: Biomass Map Super-Resolution with Sentinel-1/2 Guidance",
        "link": "https://arxiv.org/abs/2504.01722",
        "author": "Kaan Karaman, Yuchang Jiang, Damien Robert, Vivien Sainte Fare Garnot, Maria Jo\\~ao Santos, Jan Dirk Wegner",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01722v2 Announce Type: replace \nAbstract: Accurate Above-Ground Biomass (AGB) mapping at both large scale and high spatio-temporal resolution is essential for applications ranging from climate modeling to biodiversity assessment, and sustainable supply chain monitoring. At present, fine-grained AGB mapping relies on costly airborne laser scanning acquisition campaigns usually limited to regional scales. Initiatives such as the ESA CCI map attempt to generate global biomass products from diverse spaceborne sensors but at a coarser resolution. To enable global, high-resolution (HR) mapping, several works propose to regress AGB from HR satellite observations such as ESA Sentinel-1/2 images. We propose a novel way to address HR AGB estimation, by leveraging both HR satellite observations and existing low-resolution (LR) biomass products. We cast this problem as Guided Super-Resolution (GSR), aiming at upsampling LR biomass maps (sources) from $100$ to $10$ m resolution, using auxiliary HR co-registered satellite images (guides). We compare super-resolving AGB maps with and without guidance, against direct regression from satellite images, on the public BioMassters dataset. We observe that Multi-Scale Guidance (MSG) outperforms direct regression both for regression ($-780$ t/ha RMSE) and perception ($+2.0$ dB PSNR) metrics, and better captures high-biomass values, without significant computational overhead. Interestingly, unlike the RGB+Depth setting they were originally designed for, our best-performing AGB GSR approaches are those that most preserve the guide image texture. Our results make a strong case for adopting the GSR framework for accurate HR biomass mapping at scale. Our code and model weights are made publicly available (https://github.com/kaankaramanofficial/GSR4B)."
      },
      {
        "id": "oai:arXiv.org:2504.01724v2",
        "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
        "link": "https://arxiv.org/abs/2504.01724",
        "author": "Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01724v2 Announce Type: replace \nAbstract: While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/."
      },
      {
        "id": "oai:arXiv.org:2504.01905v2",
        "title": "Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries",
        "link": "https://arxiv.org/abs/2504.01905",
        "author": "Furkan \\c{C}olhak, Hasan Co\\c{s}kun, Tsafac Nkombong Regine Cyrille, Tedi Hoxa, Mert \\.Ilhan Ecevit, Mehmet Nafiz Ayd{\\i}n",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01905v2 Announce Type: replace \nAbstract: The Internet of Vehicles (IoV) may face challenging cybersecurity attacks that may require sophisticated intrusion detection systems, necessitating a rapid development and response system. This research investigates the performance advantages of GPU-accelerated libraries (cuML) compared to traditional CPU-based implementations (scikit-learn), focusing on the speed and efficiency required for machine learning models used in IoV threat detection environments. The comprehensive evaluations conducted employ four machine learning approaches (Random Forest, KNN, Logistic Regression, XGBoost) across three distinct IoV security datasets (OTIDS, GIDS, CICIoV2024). Our findings demonstrate that GPU-accelerated implementations dramatically improved computational efficiency, with training times reduced by a factor of up to 159 and prediction speeds accelerated by up to 95 times compared to traditional CPU processing, all while preserving detection accuracy. This remarkable performance breakthrough empowers researchers and security specialists to harness GPU acceleration for creating faster, more effective threat detection systems that meet the urgent real-time security demands of today's connected vehicle networks."
      },
      {
        "id": "oai:arXiv.org:2504.01919v2",
        "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation",
        "link": "https://arxiv.org/abs/2504.01919",
        "author": "Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01919v2 Announce Type: replace \nAbstract: The advent of Large Language Models (LLMs) has significantly reshaped the landscape of machine translation (MT), particularly for low-resource languages and domains that lack sufficient parallel corpora, linguistic tools, and computational infrastructure. This survey presents a comprehensive overview of recent progress in leveraging LLMs for MT. We analyze techniques such as few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning that enable effective adaptation to under-resourced settings. The paper also explores synthetic data generation strategies using LLMs, including back-translation and lexical augmentation. Additionally, we compare LLM-based translation with traditional encoder-decoder models across diverse language pairs, highlighting the strengths and limitations of each. We discuss persistent challenges such as hallucinations, evaluation inconsistencies, and inherited biases while also evaluating emerging LLM-driven metrics for translation quality. This survey offers practical insights and outlines future directions for building robust, inclusive, and scalable MT systems in the era of large-scale generative models."
      },
      {
        "id": "oai:arXiv.org:2504.01934v2",
        "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
        "link": "https://arxiv.org/abs/2504.01934",
        "author": "Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01934v2 Announce Type: replace \nAbstract: We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/."
      },
      {
        "id": "oai:arXiv.org:2504.01956v2",
        "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step",
        "link": "https://arxiv.org/abs/2504.01956",
        "author": "Hanyang Wang, Fangfu Liu, Jiawei Chi, Yueqi Duan",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01956v2 Announce Type: replace \nAbstract: Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene"
      },
      {
        "id": "oai:arXiv.org:2504.01957v2",
        "title": "Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting",
        "link": "https://arxiv.org/abs/2504.01957",
        "author": "Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.01957v2 Announce Type: replace \nAbstract: Bird's-eye view (BEV) perception has gained significant attention because it provides a unified representation to fuse multiple view images and enables a wide range of down-stream autonomous driving tasks, such as forecasting and planning. Recent state-of-the-art models utilize projection-based methods which formulate BEV perception as query learning to bypass explicit depth estimation. While we observe promising advancements in this paradigm, they still fall short of real-world applications because of the lack of uncertainty modeling and expensive computational requirement. In this work, we introduce GaussianLSS, a novel uncertainty-aware BEV perception framework that revisits unprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm, and enhances them with depth un-certainty modeling. GaussianLSS represents spatial dispersion by learning a soft depth mean and computing the variance of the depth distribution, which implicitly captures object extents. We then transform the depth distribution into 3D Gaussians and rasterize them to construct uncertainty-aware BEV features. We evaluate GaussianLSS on the nuScenes dataset, achieving state-of-the-art performance compared to unprojection-based methods. In particular, it provides significant advantages in speed, running 2.5x faster, and in memory efficiency, using 0.3x less memory compared to projection-based methods, while achieving competitive performance with only a 0.4% IoU difference."
      },
      {
        "id": "oai:arXiv.org:2304.03069v3",
        "title": "Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series",
        "link": "https://arxiv.org/abs/2304.03069",
        "author": "Jarek Duda",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2304.03069v3 Announce Type: replace-cross \nAbstract: The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like ARMA-ARCH assume arbitrary type of dependence. To avoid their bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\\sum_{\\tau<t} (1-\\eta)^{t-\\tau} \\ln(\\rho_\\theta (x_\\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $m_p=E[|x-\\mu|^p]$ evolving for one or multiple powers $p\\in\\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + \\eta (|x_t-\\mu_t|^p-m_{p,t})$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies. While standard ARMA-ARCH approaches provide evolution of $\\mu$ and $\\sigma$, here we also get evolution of $\\nu$ describing $\\rho(x)\\sim |x|^{-\\nu-1}$ tail shape, probability of extreme events - which might turn out catastrophic, destabilizing the market."
      },
      {
        "id": "oai:arXiv.org:2305.09046v2",
        "title": "Convex optimization over a probability simplex",
        "link": "https://arxiv.org/abs/2305.09046",
        "author": "James Chok, Geoffrey M. Vasil",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.09046v2 Announce Type: replace-cross \nAbstract: We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex problems over the probability simplex $\\{w\\in\\mathbb{R}^n\\ |\\ \\sum_i w_i=1\\ \\textrm{and}\\ w_i\\geq0\\}$. Specifically, we map the simplex to the positive quadrant of a unit sphere, envisage gradient descent in latent variables, and map the result back in a way that only depends on the simplex variable. Moreover, proving rigorous convergence results in this formulation leads inherently to tools from information theory (e.g., cross-entropy and KL divergence). Each iteration of the Cauchy-Simplex consists of simple operations, making it well-suited for high-dimensional problems. In continuous time, we prove that $f(x_T)-f(x^*) = {O}(1/T)$ for differentiable real-valued convex functions, where $T$ is the number of time steps and $w^*$ is the optimal solution. Numerical experiments of projection onto convex hulls show faster convergence than similar algorithms. Finally, we apply our algorithm to online learning problems and prove the convergence of the average regret for (1) Prediction with expert advice and (2) Universal Portfolios."
      },
      {
        "id": "oai:arXiv.org:2305.10015v4",
        "title": "Utility Theory of Synthetic Data Generation",
        "link": "https://arxiv.org/abs/2305.10015",
        "author": "Shirong Xu, Will Wei Sun, Guang Cheng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2305.10015v4 Announce Type: replace-cross \nAbstract: Synthetic data algorithms are widely employed in industries to generate artificial data for downstream learning tasks. While existing research primarily focuses on empirically evaluating utility of synthetic data, its theoretical understanding is largely lacking. This paper bridges the practice-theory gap by establishing relevant utility theory in a statistical learning framework. It considers two utility metrics: generalization and ranking of models trained on synthetic data. The former is defined as the generalization difference between models trained on synthetic and on real data. By deriving analytical bounds for this utility metric, we demonstrate that the synthetic feature distribution does not need to be similar as that of real data for ensuring comparable generalization of synthetic models, provided proper model specifications in downstream learning tasks. The latter utility metric studies the relative performance of models trained on synthetic data. In particular, we discover that the distribution of synthetic data is not necessarily similar as the real one to ensure consistent model comparison. Interestingly, consistent model comparison is still achievable even when synthetic responses are not well generated, as long as downstream models are separable by a generalization gap. Finally, extensive experiments on non-parametric models and deep neural networks have been conducted to validate these theoretical findings."
      },
      {
        "id": "oai:arXiv.org:2311.01727v2",
        "title": "Noise-Agnostic Quantum Error Mitigation with Data Augmented Neural Models",
        "link": "https://arxiv.org/abs/2311.01727",
        "author": "Manwen Liao, Yan Zhu, Giulio Chiribella, Yuxiang Yang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.01727v2 Announce Type: replace-cross \nAbstract: Quantum error mitigation, a data processing technique for recovering the statistics of target processes from their noisy version, is a crucial task for near-term quantum technologies. Most existing methods require prior knowledge of the noise model or the noise parameters. Deep neural networks have a potential to lift this requirement, but current models require training data produced by ideal processes in the absence of noise. Here we build a neural model that achieves quantum error mitigation without any prior knowledge of the noise and without training on noise-free data. To achieve this feature, we introduce a quantum augmentation technique for error mitigation. Our approach applies to quantum circuits and to the dynamics of many-body and continuous-variable quantum systems, accommodating various types of noise models. We demonstrate its effectiveness by testing it both on simulated noisy circuits and on real quantum hardware."
      },
      {
        "id": "oai:arXiv.org:2311.02003v2",
        "title": "Efficient Model-Based Deep Learning via Network Pruning and Fine-Tuning",
        "link": "https://arxiv.org/abs/2311.02003",
        "author": "Chicago Y. Park, Weijie Gan, Zihao Zou, Yuyang Hu, Zhixin Sun, Ulugbek S. Kamilov",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.02003v2 Announce Type: replace-cross \nAbstract: Model-based deep learning (MBDL) is a powerful methodology for designing deep models to solve imaging inverse problems. MBDL networks can be seen as iterative algorithms that estimate the desired image using a physical measurement model and a learned image prior specified using a convolutional neural net (CNNs). The iterative nature of MBDL networks increases the test-time computational complexity, which limits their applicability in certain large-scale applications. Here we make two contributions to address this issue: First, we show how structured pruning can be adopted to reduce the number of parameters in MBDL networks. Second, we present three methods to fine-tune the pruned MBDL networks to mitigate potential performance loss. Each fine-tuning strategy has a unique benefit that depends on the presence of a pre-trained model and a high-quality ground truth. We show that our pruning and fine-tuning approach can accelerate image reconstruction using popular deep equilibrium learning (DEQ) and deep unfolding (DU) methods by 50% and 32%, respectively, with nearly no performance loss. This work thus offers a step forward for solving inverse problems by showing the potential of pruning to improve the scalability of MBDL. Code is available at https://github.com/wustl-cig/MBDL_Pruning ."
      },
      {
        "id": "oai:arXiv.org:2311.08548v2",
        "title": "Topology of surface electromyogram signals: hand gesture decoding on Riemannian manifolds",
        "link": "https://arxiv.org/abs/2311.08548",
        "author": "Harshavardhana T. Gowda, Lee M. Miller",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2311.08548v2 Announce Type: replace-cross \nAbstract: $\\textit{Objective.}$ In this article, we present data and methods for decoding hand gestures using surface electromyogram (EMG) signals. EMG-based upper limb interfaces are valuable for amputee rehabilitation, artificial supernumerary limb augmentation, gestural control of computers, and virtual and augmented reality applications. $\\textit{Approach.}$ To achieve this, we collect EMG signals from the upper limb using surface electrodes placed at key muscle sites involved in hand movements. Additionally, we design and evaluate efficient models for decoding EMG signals. $\\textit{Main results.}$ Our findings reveal that the manifold of symmetric positive definite (SPD) matrices serves as an effective embedding space for EMG signals. Moreover, for the first time, we quantify the distribution shift of these signals across individuals. $\\textit{Significance.}$ Overall, our approach demonstrates significant potential for developing efficient and interpretable methods for decoding EMG signals. This is particularly important as we move toward the broader adoption of EMG-based wrist interfaces."
      },
      {
        "id": "oai:arXiv.org:2402.16792v3",
        "title": "Rate-Optimal Rank Aggregation with Private Pairwise Rankings",
        "link": "https://arxiv.org/abs/2402.16792",
        "author": "Shirong Xu, Will Wei Sun, Guang Cheng",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2402.16792v3 Announce Type: replace-cross \nAbstract: In various real-world scenarios, such as recommender systems and political surveys, pairwise rankings are commonly collected and utilized for rank aggregation to derive an overall ranking of items. However, preference rankings can reveal individuals' personal preferences, highlighting the need to protect them from exposure in downstream analysis. In this paper, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from a general comparison model. A common privacy protection strategy in practice is the use of the randomized response mechanism to perturb raw pairwise rankings. However, a critical challenge arises because the privatized rankings no longer adhere to the original model, resulting in significant bias in downstream rank aggregation tasks. To address this, we propose an adaptive debiasing method for rankings from the randomized response mechanism, ensuring consistent estimation of true preferences and enhancing the utility of downstream rank aggregation. Theoretically, we provide insights into the relationship between overall privacy guarantees and estimation errors in private ranking data, and establish minimax rates for estimation errors. This enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with privacy protection. We also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection affects the specification of top-$K$ item sets and complete rankings. Our findings are validated through extensive simulations and a real-world application."
      },
      {
        "id": "oai:arXiv.org:2403.00746v2",
        "title": "A time-stepping deep gradient flow method for option pricing in (rough) diffusion models",
        "link": "https://arxiv.org/abs/2403.00746",
        "author": "Antonis Papapantoleon, Jasper Rou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2403.00746v2 Announce Type: replace-cross \nAbstract: We develop a novel deep learning approach for pricing European options in diffusion models, that can efficiently handle high-dimensional problems resulting from Markovian approximations of rough volatility models. The option pricing partial differential equation is reformulated as an energy minimization problem, which is approximated in a time-stepping fashion by deep artificial neural networks. The proposed scheme respects the asymptotic behavior of option prices for large levels of moneyness, and adheres to a priori known bounds for option prices. The accuracy and efficiency of the proposed method is assessed in a series of numerical examples, with particular focus in the lifted Heston model."
      },
      {
        "id": "oai:arXiv.org:2404.01551v2",
        "title": "Safety-Aware Multi-Agent Learning for Dynamic Network Bridging",
        "link": "https://arxiv.org/abs/2404.01551",
        "author": "Raffaele Galliera, Konstantinos Mitsopoulos, Niranjan Suri, Raffaele Romagnoli",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2404.01551v2 Announce Type: replace-cross \nAbstract: Addressing complex cooperative tasks in safety-critical environments poses significant challenges for multi-agent systems, especially under conditions of partial observability. We focus on a dynamic network bridging task, where agents must learn to maintain a communication path between two moving targets. To ensure safety during training and deployment, we integrate a control-theoretic safety filter that enforces collision avoidance through local setpoint updates. We develop and evaluate multi-agent reinforcement learning safety-informed message passing, showing that encoding safety filter activations as edge-level features improves coordination. The results suggest that local safety enforcement and decentralized learning can be effectively combined in distributed multi-agent tasks."
      },
      {
        "id": "oai:arXiv.org:2405.17537v4",
        "title": "CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale",
        "link": "https://arxiv.org/abs/2405.17537",
        "author": "ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2405.17537v4 Announce Type: replace-cross \nAbstract: Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies."
      },
      {
        "id": "oai:arXiv.org:2406.03230v5",
        "title": "Defending Large Language Models Against Attacks With Residual Stream Activation Analysis",
        "link": "https://arxiv.org/abs/2406.03230",
        "author": "Amelia Kawasaki, Andrew Davis, Houssam Abbas",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2406.03230v5 Announce Type: replace-cross \nAbstract: The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models. These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs. In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM. We apply a novel methodology for analyzing distinctive activation patterns in the residual streams for attack prompt classification. We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset. Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks. The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate."
      },
      {
        "id": "oai:arXiv.org:2407.03262v3",
        "title": "Ridge Leverage Score Sampling for $\\ell_p$ Subspace Approximation",
        "link": "https://arxiv.org/abs/2407.03262",
        "author": "David P. Woodruff, Taisuke Yasuda",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.03262v3 Announce Type: replace-cross \nAbstract: The $\\ell_p$ subspace approximation problem is an NP-hard low rank approximation problem that generalizes the median hyperplane ($p = 1$), principal component analysis ($p = 2$), and center hyperplane problems ($p = \\infty$). A popular approach to cope with the NP-hardness is to compute a strong coreset, which is a weighted subset of input points that simultaneously approximates the cost of every $k$-dimensional subspace, typically to $(1+\\epsilon)$ relative error for a small constant $\\epsilon$.\n  We obtain an algorithm for constructing a strong coreset for $\\ell_p$ subspace approximation of size $\\tilde O(k\\epsilon^{-4/p})$ for $p<2$ and $\\tilde O(k^{p/2}\\epsilon^{-p})$ for $p>2$. This offers the following improvements over prior work:\n  - We construct the first strong coresets with nearly optimal dependence on $k$ for all $p\\neq 2$. In prior work, [SW18] constructed coresets of modified points with a similar dependence on $k$, while [HV20] constructed true coresets with polynomially worse dependence on $k$. - We recover or improve the best known $\\epsilon$ dependence for all $p$. In particular, for $p > 2$, the [SW18] coreset of modified points had a dependence of $\\epsilon^{-p^2/2}$ and the [HV20] coreset had a dependence of $\\epsilon^{-3p}$.\n  Our algorithm is based on sampling by root ridge leverage scores, which admits fast algorithms, especially for sparse or structured matrices. Our analysis avoids the use of the representative subspace theorem [SW18], which is a critical component of all prior dimension-independent coresets for $\\ell_p$ subspace approximation.\n  Our techniques also lead to the first nearly optimal online strong coresets for $\\ell_p$ subspace approximation with similar bounds as the offline setting, resolving a problem of [WY23]. All prior approaches lose $\\mathrm{poly}(k)$ factors in this setting, even when allowed to modify the original points."
      },
      {
        "id": "oai:arXiv.org:2407.12957v2",
        "title": "R+X: Retrieval and Execution from Everyday Human Videos",
        "link": "https://arxiv.org/abs/2407.12957",
        "author": "Georgios Papagiannis, Norman Di Palo, Pietro Vitiello, Edward Johns",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2407.12957v2 Announce Type: replace-cross \nAbstract: We present R+X, a framework which enables robots to learn skills from long, unlabelled, first-person videos of humans performing everyday tasks. Given a language command from a human, R+X first retrieves short video clips containing relevant behaviour, and then executes the skill by conditioning an in-context imitation learning method (KAT) on this behaviour. By leveraging a Vision Language Model (VLM) for retrieval, R+X does not require any manual annotation of the videos, and by leveraging in-context learning for execution, robots can perform commanded skills immediately, without requiring a period of training on the retrieved videos. Experiments studying a range of everyday household tasks show that R+X succeeds at translating unlabelled human videos into robust robot skills, and that R+X outperforms several recent alternative methods. Videos and code are available at https://www.robot-learning.uk/r-plus-x."
      },
      {
        "id": "oai:arXiv.org:2408.03100v3",
        "title": "Huge Ensembles Part I: Design of Ensemble Weather Forecasts using Spherical Fourier Neural Operators",
        "link": "https://arxiv.org/abs/2408.03100",
        "author": "Ankur Mahesh, William Collins, Boris Bonev, Noah Brenowitz, Yair Cohen, Joshua Elms, Peter Harrington, Karthik Kashinath, Thorsten Kurth, Joshua North, Travis OBrien, Michael Pritchard, David Pruitt, Mark Risser, Shashank Subramanian, Jared Willard",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2408.03100v3 Announce Type: replace-cross \nAbstract: Studying low-likelihood high-impact extreme weather events in a warming world is a significant and challenging task for current ensemble forecasting systems. While these systems presently use up to 100 members, larger ensembles could enrich the sampling of internal variability. They may capture the long tails associated with climate hazards better than traditional ensemble sizes. Due to computational constraints, it is infeasible to generate huge ensembles (comprised of 1,000-10,000 members) with traditional, physics-based numerical models. In this two-part paper, we replace traditional numerical simulations with machine learning (ML) to generate hindcasts of huge ensembles. In Part I, we construct an ensemble weather forecasting system based on Spherical Fourier Neural Operators (SFNO), and we discuss important design decisions for constructing such an ensemble. The ensemble represents model uncertainty through perturbed-parameter techniques, and it represents initial condition uncertainty through bred vectors, which sample the fastest growing modes of the forecast. Using the European Centre for Medium-Range Weather Forecasts Integrated Forecasting System (IFS) as a baseline, we develop an evaluation pipeline composed of mean, spectral, and extreme diagnostics. Using large-scale, distributed SFNOs with 1.1 billion learned parameters, we achieve calibrated probabilistic forecasts. As the trajectories of the individual members diverge, the ML ensemble mean spectra degrade with lead time, consistent with physical expectations. However, the individual ensemble members' spectra stay constant with lead time. Therefore, these members simulate realistic weather states, and the ML ensemble thus passes a crucial spectral test in the literature. The IFS and ML ensembles have similar Extreme Forecast Indices, and we show that the ML extreme weather forecasts are reliable and discriminating."
      },
      {
        "id": "oai:arXiv.org:2409.18257v2",
        "title": "Developing a Dual-Stage Vision Transformer Model for Lung Disease Classification",
        "link": "https://arxiv.org/abs/2409.18257",
        "author": "Anirudh Mazumder, Jianguo Liu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2409.18257v2 Announce Type: replace-cross \nAbstract: Lung diseases have become a prevalent problem throughout the United States, affecting over 34 million people. Accurate and timely diagnosis of the different types of lung diseases is critical, and Artificial Intelligence (AI) methods could speed up these processes. A dual-stage vision transformer is built throughout this research by integrating a Vision Transformer (ViT) and a Swin Transformer to classify 14 different lung diseases from X-ray scans of patients with these diseases. The proposed model achieved an accuracy of 92.06% on a label-level when making predictions on an unseen testing subset of the dataset after data preprocessing and training the neural network. The model showed promise for accurately classifying lung diseases and diagnosing patients who suffer from these harmful diseases."
      },
      {
        "id": "oai:arXiv.org:2410.11377v2",
        "title": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups",
        "link": "https://arxiv.org/abs/2410.11377",
        "author": "Theresa Pekarek Rosin, Vanessa Hassouna, Xiaowen Sun, Luca Krohm, Henri-Leon Kordt, Michael Beetz, Stefan Wermter",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.11377v2 Announce Type: replace-cross \nAbstract: To facilitate natural and intuitive interactions with diverse user groups in real-world settings, social robots must be capable of addressing the varying requirements and expectations of these groups while adapting their behavior based on user feedback. While previous research often focuses on specific demographics, we present a novel framework for adaptive Human-Robot Interaction (HRI) that tailors interactions to different user groups and enables individual users to modulate interactions through both minor and major interruptions. Our primary contributions include the development of an adaptive, ROS-based HRI framework with an open-source code base. This framework supports natural interactions through advanced speech recognition and voice activity detection, and leverages a large language model (LLM) as a dialogue bridge. We validate the efficiency of our framework through module tests and system trials, demonstrating its high accuracy in age recognition and its robustness to repeated user inputs and plan changes."
      },
      {
        "id": "oai:arXiv.org:2410.17233v3",
        "title": "ICPL: Few-shot In-context Preference Learning via LLMs",
        "link": "https://arxiv.org/abs/2410.17233",
        "author": "Chao Yu, Qixin Tan, Hong Lu, Jiaxuan Gao, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.17233v3 Announce Type: replace-cross \nAbstract: Preference-based reinforcement learning is an effective way to handle tasks where rewards are hard to specify but can be exceedingly inefficient as preference learning is often tabula rasa. We demonstrate that Large Language Models (LLMs) have native preference-learning capabilities that allow them to achieve sample-efficient preference learning, addressing this challenge. We propose In-Context Preference Learning (ICPL), which uses in-context learning capabilities of LLMs to reduce human query inefficiency. ICPL uses the task description and basic environment code to create sets of reward functions which are iteratively refined by placing human feedback over videos of the resultant policies into the context of an LLM and then requesting better rewards. We first demonstrate ICPL's effectiveness through a synthetic preference study, providing quantitative evidence that it significantly outperforms baseline preference-based methods with much higher performance and orders of magnitude greater efficiency. We observe that these improvements are not solely coming from LLM grounding in the task but that the quality of the rewards improves over time, indicating preference learning capabilities. Additionally, we perform a series of real human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop."
      },
      {
        "id": "oai:arXiv.org:2411.18526v2",
        "title": "NeuroAI for AI Safety",
        "link": "https://arxiv.org/abs/2411.18526",
        "author": "Patrick Mineault, Niccol\\`o Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2411.18526v2 Announce Type: replace-cross \nAbstract: As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety."
      },
      {
        "id": "oai:arXiv.org:2412.01460v4",
        "title": "A Comprehensive Study of Shapley Value in Data Analytics",
        "link": "https://arxiv.org/abs/2412.01460",
        "author": "Hong Lin, Shixin Wan, Zhongle Xie, Ke Chen, Meihui Zhang, Lidan Shou, Gang Chen",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.01460v4 Announce Type: replace-cross \nAbstract: Over the recent years, Shapley value (SV), a solution concept from cooperative game theory, has found numerous applications in data analytics (DA). This paper provides the first comprehensive study of SV used throughout the DA workflow, clarifying the key variables in defining DA-applicable SV and the essential functionalities that SV can provide for data scientists. We condense four primary challenges of using SV in DA, namely computation efficiency, approximation error, privacy preservation, and interpretability, then disentangle the resolution techniques from existing arts in this field, analyze and discuss the techniques w.r.t. each challenge and potential conflicts between challenges. We also implement SVBench, a modular and extensible open-sourced framework for developing SV applications in different DA tasks, and conduct extensive evaluations to validate our analyses and discussions. Based on the qualitative and quantitative results, we identify the limitations of current efforts for applying SV to DA and highlight the directions of future research and engineering."
      },
      {
        "id": "oai:arXiv.org:2412.09567v2",
        "title": "Temporal Triadic Closure: Finding Dense Structures in Social Networks That Evolve",
        "link": "https://arxiv.org/abs/2412.09567",
        "author": "Tom Davot, Jessica Enright, Jayakrishnan Madathil, Kitty Meeks",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2412.09567v2 Announce Type: replace-cross \nAbstract: A graph G is c-closed if every two vertices with at least c common neighbors are adjacent to each other. Introduced by Fox, Roughgarden, Seshadhri, Wei and Wein [ICALP 2018, SICOMP 2020], this definition is an abstraction of the triadic closure property exhibited by many real-world social networks, namely, friends of friends tend to be friends themselves. Social networks, however, are often temporal rather than static -- the connections change over a period of time. And hence temporal graphs, rather than static graphs, are often better suited to model social networks. Motivated by this, we introduce a definition of temporal c-closed graphs, in which if two vertices u and v have at least c common neighbors during a short interval of time, then u and v are adjacent to each other around that time. Our pilot experiments show that several real-world temporal networks are c-closed for rather small values of c. We also study the computational problems of enumerating maximal cliques and similar dense subgraphs in temporal c-closed graphs; a clique in a temporal graph is a subgraph that lasts for a certain period of time, during which every possible edge in the subgraph becomes active often enough, and other dense subgraphs are defined similarly. We bound the number of such maximal dense subgraphs in a temporal c-closed graph that evolves slowly, and thus show that the corresponding enumeration problems admit efficient algorithms; by slow evolution, we mean that between consecutive time-steps, the local change in adjacencies remains small. Our work also adds to a growing body of literature on defining suitable structural parameters for temporal graphs that can be leveraged to design efficient algorithms."
      },
      {
        "id": "oai:arXiv.org:2501.00398v2",
        "title": "TSPE: Task-Specific Prompt Ensemble for Improved Zero-Shot Audio Classification",
        "link": "https://arxiv.org/abs/2501.00398",
        "author": "Nishit Anand, Ashish Seth, Ramani Duraiswami, Dinesh Manocha",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00398v2 Announce Type: replace-cross \nAbstract: Audio-language models (ALMs) excel in zero-shot audio classification, a task where models classify previously unseen audio clips at test time by leveraging descriptive natural language prompts. We introduce TSPE (Task-Specific Prompt Ensemble), a simple, training-free hard prompting method that boosts ALEs' zero-shot performance by customizing prompts for diverse audio classification tasks. Rather than using generic template-based prompts like \"Sound of a car\" we generate context-rich prompts, such as \"Sound of a car coming from a tunnel\". Specifically, we leverage label information to identify suitable sound attributes, such as \"loud\" and \"feeble\", and appropriate sound sources, such as \"tunnel\" and \"street\" and incorporate this information into the prompts used by Audio-Language Models (ALMs) for audio classification. Further, to enhance audio-text alignment, we perform prompt ensemble across TSPE-generated task-specific prompts. When evaluated on 12 diverse audio classification datasets, TSPE improves performance across ALMs by showing an absolute improvement of 1.23-16.36% over vanilla zero-shot evaluation."
      },
      {
        "id": "oai:arXiv.org:2501.01811v2",
        "title": "QuantumBind-RBFE: Accurate Relative Binding Free Energy Calculations Using Neural Network Potentials",
        "link": "https://arxiv.org/abs/2501.01811",
        "author": "Francesc Saban\\'es Zariquiey, Stephen E. Farr, Stefan Doerr, Gianni De Fabritiis",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.01811v2 Announce Type: replace-cross \nAbstract: Accurate prediction of protein-ligand binding affinities is crucial in drug discovery, particularly during hit-to-lead and lead optimization phases, however, limitations in ligand force fields continue to impact prediction accuracy. In this work, we validate relative binding free energy (RBFE) accuracy using neural network potentials (NNPs) for the ligands. We utilize a novel NNP model, AceFF 1.0, based on the TensorNet architecture for small molecules that broadens the applicability to diverse drug-like compounds, including all important chemical elements and supporting charged molecules. Using established benchmarks, we show overall improved accuracy and correlation in binding affinity predictions compared with GAFF2 for molecular mechanics and ANI2-x for NNPs. Slightly less accuracy but comparable correlations with OPLS4. We also show that we can run the NNP simulations at 2 fs timestep, at least two times larger than previous NNP models, providing significant speed gains. The results show promise for further evolutions of free energy calculations using NNPs while demonstrating its practical use already with the current generation. The code and NNP model are publicly available for research use."
      },
      {
        "id": "oai:arXiv.org:2501.17042v2",
        "title": "Emergence of network communities driven by local rules",
        "link": "https://arxiv.org/abs/2501.17042",
        "author": "Alexei Vazquez",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.17042v2 Announce Type: replace-cross \nAbstract: Natural systems are modeled by networks with nodes and links. Often the nodes are segregated into communities with different connectivity patterns. Node heterogeneity such as political affiliation in social networks or biological function in gene networks are highlighted as key factors driving the segregation of nodes into communities. Here I demonstrate that node heterogeneity is not a necessary requirement. To this end I introduce the Ramsey community number, $r_ \\kappa$, the minimum graph size that warranties the emergence of network communities with almost certainty. Using the stochastic block model for community detection with correction for degree sequence, I show that networks generated by local rules have finite $r_ \\kappa$ values while their randomized versions do not have emergent communities. I conclude that network communities are an emergent property of networks evolving with local rules."
      },
      {
        "id": "oai:arXiv.org:2501.18708v2",
        "title": "Combining physics-based and data-driven models: advancing the frontiers of research with Scientific Machine Learning",
        "link": "https://arxiv.org/abs/2501.18708",
        "author": "Alfio Quarteroni, Paola Gervasio, Francesco Regazzoni",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.18708v2 Announce Type: replace-cross \nAbstract: Scientific Machine Learning (SciML) is a recently emerged research field which combines physics-based and data-driven models for the numerical approximation of differential problems. Physics-based models rely on the physical understanding of the problem, subsequent mathematical formulation, and numerical approximation. Data-driven models instead aim to extract relations between input and output data without arguing any causality principle underlining the available data distribution. In recent years, data-driven models have been rapidly developed and popularized. Such a diffusion has been triggered by a huge availability of data, increasingly cheap computing power, and the development of powerful ML algorithms. SciML leverages the physical awareness of physics-based models and the efficiency of data-driven algorithms. With SciML, we can inject physics and mathematical knowledge into ML algorithms. Yet, we can rely on data-driven algorithms' capability to discover complex and nonlinear patterns from data and improve the descriptive capacity of physics-based models. After recalling the mathematical foundations of digital modelling and ML algorithms and presenting the most popular ML architectures, we discuss the great potential of a broad variety of SciML strategies in solving complex problems governed by PDEs. Finally, we illustrate the successful application of SciML to the simulation of the human cardiac function, a field of significant socioeconomic importance that poses numerous challenges on both the mathematical and computational fronts. Despite the robustness and accuracy of physics-based models, certain aspects, such as unveiling constitutive laws for cardiac cells and myocardial material properties, as well as devising efficient reduced order models to dominate the extraordinary computational complexity, have been successfully tackled by leveraging data-driven models."
      },
      {
        "id": "oai:arXiv.org:2502.01358v2",
        "title": "Diffusion at Absolute Zero: Langevin Sampling Using Successive Moreau Envelopes [conference paper]",
        "link": "https://arxiv.org/abs/2502.01358",
        "author": "Andreas Habring, Alexander Falk, Thomas Pock",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.01358v2 Announce Type: replace-cross \nAbstract: In this article we propose a novel method for sampling from Gibbs distributions of the form $\\pi(x)\\propto\\exp(-U(x))$ with a potential $U(x)$. In particular, inspired by diffusion models we propose to consider a sequence $(\\pi^{t_k})_k$ of approximations of the target density, for which $\\pi^{t_k}\\approx \\pi$ for $k$ small and, on the other hand, $\\pi^{t_k}$ exhibits favorable properties for sampling for $k$ large. This sequence is obtained by replacing parts of the potential $U$ by its Moreau envelopes. Sampling is performed in an Annealed Langevin type procedure, that is, sequentially sampling from $\\pi^{t_k}$ for decreasing $k$, effectively guiding the samples from a simple starting density to the more complex target. In addition to a theoretical analysis we show experimental results supporting the efficacy of the method in terms of increased convergence speed and applicability to multi-modal densities $\\pi$."
      },
      {
        "id": "oai:arXiv.org:2502.06152v2",
        "title": "The Value of Information in Human-AI Decision-making",
        "link": "https://arxiv.org/abs/2502.06152",
        "author": "Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.06152v2 Announce Type: replace-cross \nAbstract: Multiple agents -- including humans and AI models -- are often paired on decision tasks with the expectation of achieving complementary performance, where the combined performance of both agents outperforms either one alone. However, knowing how to improve the performance of a human-AI team is often difficult without knowing more about what particular information and strategies each agent employs. We provide a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information -- in AI-assisted decision workflows. We demonstrate the use of the framework for model selection, empirical evaluation of human-AI performance, and explanation design. We propose a novel information-based explanation technique that adapts SHAP, a saliency-based explanation, to explain information value in decision making."
      },
      {
        "id": "oai:arXiv.org:2502.08786v2",
        "title": "MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic Computed Tomography",
        "link": "https://arxiv.org/abs/2502.08786",
        "author": "Xinkai Wang, Yue Yang, Kehong Zhou, Xue Xie, Lifeng Zhu, Aiguo Song, Bruce Daniel",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.08786v2 Announce Type: replace-cross \nAbstract: Chinese acupuncture practitioners primarily depend on muscle memory and tactile feedback to insert needles and accurately target acupuncture points, as the current workflow lacks imaging modalities and visual aids. Consequently, new practitioners often learn through trial and error, requiring years of experience to become proficient and earn the trust of patients. Medical students face similar challenges in mastering this skill. To address these challenges, we developed an innovative system, MRUCT, that integrates ultrasonic computed tomography (UCT) with mixed reality (MR) technology to visualize acupuncture points in real-time. This system offers offline image registration and real-time guidance during needle insertion, enabling them to accurately position needles based on anatomical structures such as bones, muscles, and auto-generated reference points, with the potential for clinical implementation. In this paper, we outline the non-rigid registration methods used to reconstruct anatomical structures from UCT data, as well as the key design considerations of the MR system. We evaluated two different 3D user interface (3DUI) designs and compared the performance of our system to traditional workflows for both new practitioners and medical students. The results highlight the potential of MR to enhance therapeutic medical practices and demonstrate the effectiveness of the system we developed."
      },
      {
        "id": "oai:arXiv.org:2502.09654v2",
        "title": "Heterogeneous Mixture of Experts for Remote Sensing Image Super-Resolution",
        "link": "https://arxiv.org/abs/2502.09654",
        "author": "Bowen Chen, Keyan Chen, Mohan Yang, Zhengxia Zou, Zhenwei Shi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.09654v2 Announce Type: replace-cross \nAbstract: Remote sensing image super-resolution (SR) aims to reconstruct high-resolution remote sensing images from low-resolution inputs, thereby addressing limitations imposed by sensors and imaging conditions. However, the inherent characteristics of remote sensing images, including diverse ground object types and complex details, pose significant challenges to achieving high-quality reconstruction. Existing methods typically employ a uniform structure to process various types of ground objects without distinction, making it difficult to adapt to the complex characteristics of remote sensing images. To address this issue, we introduce a Mixture of Experts (MoE) model and design a set of heterogeneous experts. These experts are organized into multiple expert groups, where experts within each group are homogeneous while being heterogeneous across groups. This design ensures that specialized activation parameters can be employed to handle the diverse and intricate details of ground objects effectively. To better accommodate the heterogeneous experts, we propose a multi-level feature aggregation strategy to guide the routing process. Additionally, we develop a dual-routing mechanism to adaptively select the optimal expert for each pixel. Experiments conducted on the UCMerced and AID datasets demonstrate that our proposed method achieves superior SR reconstruction accuracy compared to state-of-the-art methods. The code will be available at https://github.com/Mr-Bamboo/MFG-HMoE."
      },
      {
        "id": "oai:arXiv.org:2502.20576v4",
        "title": "Smart Routing: Cost-Effective Multi-LLM Serving for Multi-Core AIOS",
        "link": "https://arxiv.org/abs/2502.20576",
        "author": "Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2502.20576v4 Announce Type: replace-cross \nAbstract: As large language models (LLMs) are increasingly deployed as service endpoints in systems, the surge in query volume creates significant scheduling challenges. Existing scheduling frameworks mainly target at latency optimization while neglecting the capability of LLMs to serve different level of queries, which could lead to computational resource waste. For example, those simple queries can be safely handled by small, fast and cheap LLMs, while those complex and difficult queries need to be handled by large, slow, and expensive LLMs. This paper addresses this challenge by proposing an efficient capability-cost coordinated scheduling framework, ECCOS, for multi-LLM serving, which explicitly constrains response quality and workload to optimize LLM inference cost. Specifically, it introduces the two-stage scheduling by designing a multi-objective predictor and a constrained optimizer. The predictor estimates both model capabilities and computational costs through training-based and retrieval-based approaches, while the optimizer determines cost-optimal assignments under quality and workload constraints. It also introduces QAServe, a dataset for sample-wise response quality and costs collected by zero-shot prompting different LLMs on knowledge QA and mathematical reasoning. Extensive experiments demonstrate that ECCOS improves success rates by 6.30% while reducing costs by 10.15% compared to existing methods, consuming less than 0.5% of LLM response time. The code is available at: https://github.com/agiresearch/ECCOS, and the proposed smart routing mechanism has been integrated into AIOS, the AI Agent Operating System, at https://github.com/agiresearch/AIOS."
      },
      {
        "id": "oai:arXiv.org:2503.09184v2",
        "title": "Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs",
        "link": "https://arxiv.org/abs/2503.09184",
        "author": "Aidan Ferguson, Perry Gibson, Lara D'Agata, Parker McLeod, Ferhat Yaman, Amitabh Das, Ian Colbert, Jos\\'e Cano",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.09184v2 Announce Type: replace-cross \nAbstract: The deployment of deep neural networks (DNNs) in privacy-sensitive environments is constrained by computational overheads in fully homomorphic encryption (FHE). This paper explores unstructured sparsity in FHE matrix multiplication schemes as a means of reducing this burden while maintaining model accuracy requirements. We demonstrate that sparsity can be exploited in arbitrary matrix multiplication, providing runtime benefits compared to a baseline naive algorithm at all sparsity levels. This is a notable departure from the plaintext domain, where there is a trade-off between sparsity and the overhead of the sparse multiplication algorithm. In addition, we propose three sparse multiplication schemes in FHE based on common plaintext sparse encodings. We demonstrate the performance gain is scheme-invariant; however, some sparse schemes vastly reduce the memory storage requirements of the encrypted matrix at high sparsity values. Our proposed sparse schemes yield an average performance gain of 2.5x at 50% unstructured sparsity, with our multi-threading scheme providing a 32.5x performance increase over the equivalent single-threaded sparse computation when utilizing 64 cores."
      },
      {
        "id": "oai:arXiv.org:2503.19949v2",
        "title": "Automated Video-EEG Analysis in Epilepsy Studies: Advances and Challenges",
        "link": "https://arxiv.org/abs/2503.19949",
        "author": "Valerii A. Zuev, Elena G. Salmagambetova, Stepan N. Djakov, Lev V. Utkin",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.19949v2 Announce Type: replace-cross \nAbstract: Epilepsy is typically diagnosed through electroencephalography (EEG) and long-term video-EEG (vEEG) monitoring. The manual analysis of vEEG recordings is time-consuming, necessitating automated tools for seizure detection. Recent advancements in machine learning have shown promise in real-time seizure detection and prediction using EEG and video data. However, diversity of seizure symptoms, markup ambiguities, and limited availability of multimodal datasets hinder progress. This paper reviews the latest developments in automated video-EEG analysis and discusses the integration of multimodal data. We also propose a novel pipeline for treatment effect estimation from vEEG data using concept-based learning, offering a pathway for future research in this domain."
      },
      {
        "id": "oai:arXiv.org:2503.21983v2",
        "title": "Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs",
        "link": "https://arxiv.org/abs/2503.21983",
        "author": "Abed Kareem Musaffar, Anand Gokhale, Sirui Zeng, Rasta Tadayon, Xifeng Yan, Ambuj Singh, Francesco Bullo",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.21983v2 Announce Type: replace-cross \nAbstract: As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models -- one inspired by literature and the other data-driven -- and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies."
      },
      {
        "id": "oai:arXiv.org:2503.23037v2",
        "title": "Agentic Large Language Models, a survey",
        "link": "https://arxiv.org/abs/2503.23037",
        "author": "Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, Kees Joost Batenburg",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23037v2 Announce Type: replace-cross \nAbstract: There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society."
      },
      {
        "id": "oai:arXiv.org:2503.23241v2",
        "title": "Geometry in Style: 3D Stylization via Surface Normal Deformation",
        "link": "https://arxiv.org/abs/2503.23241",
        "author": "Nam Anh Dinh, Itai Lang, Hyunwoo Kim, Oded Stein, Rana Hanocka",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23241v2 Announce Type: replace-cross \nAbstract: We present Geometry in Style, a new method for identity-preserving mesh stylization. Existing techniques either adhere to the original shape through overly restrictive deformations such as bump maps or significantly modify the input shape using expressive deformations that may introduce artifacts or alter the identity of the source shape. In contrast, we represent a deformation of a triangle mesh as a target normal vector for each vertex neighborhood. The deformations we recover from target normals are expressive enough to enable detailed stylizations yet restrictive enough to preserve the shape's identity. We achieve such deformations using our novel differentiable As-Rigid-As-Possible (dARAP) layer, a neural-network-ready adaptation of the classical ARAP algorithm which we use to solve for per-vertex rotations and deformed vertices. As a differentiable layer, dARAP is paired with a visual loss from a text-to-image model to drive deformations toward style prompts, altogether giving us Geometry in Style. Our project page is at https://threedle.github.io/geometry-in-style."
      },
      {
        "id": "oai:arXiv.org:2503.23515v2",
        "title": "Optimal Invariant Bases for Atomistic Machine Learning",
        "link": "https://arxiv.org/abs/2503.23515",
        "author": "Alice E. A. Allen, Emily Shinkle, Roxana Bujack, Nicholas Lubbers",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2503.23515v2 Announce Type: replace-cross \nAbstract: The representation of atomic configurations for machine learning models has led to the development of numerous descriptors, often to describe the local environment of atoms. However, many of these representations are incomplete and/or functionally dependent. Incomplete descriptor sets are unable to represent all meaningful changes in the atomic environment. Complete constructions of atomic environment descriptors, on the other hand, often suffer from a high degree of functional dependence, where some descriptors can be written as functions of the others. These redundant descriptors do not provide additional power to discriminate between different atomic environments and increase the computational burden. By employing techniques from the pattern recognition literature to existing atomistic representations, we remove descriptors that are functions of other descriptors to produce the smallest possible set that satisfies completeness. We apply this in two ways: first we refine an existing description, the Atomistic Cluster Expansion. We show that this yields a more efficient subset of descriptors. Second, we augment an incomplete construction based on a scalar neural network, yielding a new message-passing network architecture that can recognize up to 5-body patterns in each neuron by taking advantage of an optimal set of Cartesian tensor invariants. This architecture shows strong accuracy on state-of-the-art benchmarks while retaining low computational cost. Our results not only yield improved models, but point the way to classes of invariant bases that minimize cost while maximizing expressivity for a host of applications."
      },
      {
        "id": "oai:arXiv.org:2504.00034v2",
        "title": "Quantum Generative Models for Image Generation: Insights from MNIST and MedMNIST",
        "link": "https://arxiv.org/abs/2504.00034",
        "author": "Chi-Sheng Chen, Wei An Hou, Hsiang-Wei Hu, Zhen-Sheng Cai",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.00034v2 Announce Type: replace-cross \nAbstract: Quantum generative models offer a promising new direction in machine learning by leveraging quantum circuits to enhance data generation capabilities. In this study, we propose a hybrid quantum-classical image generation framework that integrates variational quantum circuits into a diffusion-based model. To improve training dynamics and generation quality, we introduce two novel noise strategies: intrinsic quantum-generated noise and a tailored noise scheduling mechanism. Our method is built upon a lightweight U-Net architecture, with the quantum layer embedded in the bottleneck module to isolate its effect. We evaluate our model on MNIST and MedMNIST datasets to examine its feasibility and performance. Notably, our results reveal that under limited data conditions (fewer than 100 training images), the quantum-enhanced model generates images with higher perceptual quality and distributional similarity than its classical counterpart using the same architecture. While the quantum model shows advantages on grayscale data such as MNIST, its performance is more nuanced on complex, color-rich datasets like PathMNIST. These findings highlight both the potential and current limitations of quantum generative models and lay the groundwork for future developments in low-resource and biomedical image generation."
      }
    ]
  },
  "https://rss.arxiv.org/rss/cs.SD+eess.AS": {
    "feed": {
      "title": "cs.SD, eess.AS updates on arXiv.org",
      "link": "http://rss.arxiv.org/rss/cs.SD+eess.AS",
      "updated": "Fri, 04 Apr 2025 04:01:52 +0000",
      "published": "Fri, 04 Apr 2025 00:00:00 -0400"
    },
    "entries": [
      {
        "id": "oai:arXiv.org:2504.02302v1",
        "title": "Causal Self-supervised Pretrained Frontend with Predictive Code for Speech Separation",
        "link": "https://arxiv.org/abs/2504.02302",
        "author": "Wupeng Wang, Zexu Pan, Xinke Li, Shuai Wang, Haizhou Li",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02302v1 Announce Type: new \nAbstract: Speech separation (SS) seeks to disentangle a multi-talker speech mixture into single-talker speech streams. Although SS can be generally achieved using offline methods, such a processing paradigm is not suitable for real-time streaming applications. Causal separation models, which rely only on past and present information, offer a promising solution for real-time streaming. However, these models typically suffer from notable performance degradation due to the absence of future context. In this paper, we introduce a novel frontend that is designed to mitigate the mismatch between training and run-time inference by implicitly incorporating future information into causal models through predictive patterns. The pretrained frontend employs a transformer decoder network with a causal convolutional encoder as the backbone and is pretrained in a self-supervised manner with two innovative pretext tasks: autoregressive hybrid prediction and contextual knowledge distillation. These tasks enable the model to capture predictive patterns directly from mixtures in a self-supervised manner. The pretrained frontend subsequently serves as a feature extractor to generate high-quality predictive patterns. Comprehensive evaluations on synthetic and real-world datasets validated the effectiveness of the proposed pretrained frontend."
      },
      {
        "id": "oai:arXiv.org:2504.02402v1",
        "title": "EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling",
        "link": "https://arxiv.org/abs/2504.02402",
        "author": "Hao Yin, Shi Guo, Xu Jia, Xudong XU, Lu Zhang, Si Liu, Dong Wang, Huchuan Lu, Tianfan Xue",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02402v1 Announce Type: new \nAbstract: When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method."
      },
      {
        "id": "oai:arXiv.org:2504.02407v1",
        "title": "F5R-TTS: Improving Flow Matching based Text-to-Speech with Group Relative Policy Optimization",
        "link": "https://arxiv.org/abs/2504.02407",
        "author": "Xiaohui Sun, Ruitong Xiao, Jianye Mo, Bowen Wu, Qun Yu, Baoxun Wang",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02407v1 Announce Type: new \nAbstract: We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Gradient Reward Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (relatively 29.5\\% WER reduction) and speaker similarity (relatively 4.6\\% SIM score increase) compared to conventional flow-matching based TTS systems. Audio samples are available at https://frontierlabs.github.io/F5R."
      },
      {
        "id": "oai:arXiv.org:2504.02586v1",
        "title": "Deep learning for music generation. Four approaches and their comparative evaluation",
        "link": "https://arxiv.org/abs/2504.02586",
        "author": "Razvan Paroiu, Stefan Trausan-Matu",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02586v1 Announce Type: new \nAbstract: This paper introduces four different artificial intelligence algorithms for music generation and aims to compare these methods not only based on the aesthetic quality of the generated music but also on their suitability for specific applications. The first set of melodies is produced by a slightly modified visual transformer neural network that is used as a language model. The second set of melodies is generated by combining chat sonification with a classic transformer neural network (the same method of music generation is presented in a previous research), the third set of melodies is generated by combining the Schillinger rhythm theory together with a classic transformer neural network, and the fourth set of melodies is generated using GPT3 transformer provided by OpenAI. A comparative analysis is performed on the melodies generated by these approaches and the results indicate that significant differences can be observed between them and regarding the aesthetic value of them, GPT3 produced the most pleasing melodies, and the newly introduced Schillinger method proved to generate better sounding music than previous sonification methods."
      },
      {
        "id": "oai:arXiv.org:2504.02061v1",
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "link": "https://arxiv.org/abs/2504.02061",
        "author": "Yuxin Guo, Shuailei Ma, Shijie Ma, Xiaoyi Bao, Chen-Wei Xie, Kecheng Zheng, Tingyu Weng, Siyang Sun, Yun Zheng, Wei Zou",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02061v1 Announce Type: cross \nAbstract: Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual Large Language Models (AV-LLMs) exhibit deficiencies in exploiting audio information, leading to weak understanding and hallucinations. To solve the issues, we delve into the model architecture and dataset. (1) From the architectural perspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent alignment of audio and visual modalities in both temporal and spatial dimensions ensures a comprehensive and accurate understanding of videos. Specifically, we devise an audio-visual multi-scale adapter for multi-scale information aggregation, which achieves spatial alignment. For temporal alignment, we propose audio-visual interleaved merging. (2) From the dataset perspective, we curate an audio-visual caption and instruction-tuning dataset, called AVU. It comprises 5.2 million diverse, open-ended data tuples (video, audio, question, answer) and introduces a novel data partitioning strategy. Extensive experiments show our model not only achieves remarkable performance in audio-visual understanding, but also mitigates potential hallucinations."
      },
      {
        "id": "oai:arXiv.org:2504.02386v1",
        "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
        "link": "https://arxiv.org/abs/2504.02386",
        "author": "Kim Sung-Bin, Jeongsoo Choi, Puyuan Peng, Joon Son Chung, Tae-Hyun Oh, David Harwath",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02386v1 Announce Type: cross \nAbstract: We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications."
      },
      {
        "id": "oai:arXiv.org:2504.02398v1",
        "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
        "link": "https://arxiv.org/abs/2504.02398",
        "author": "Gallil Maimon, Michael Hassid, Amit Roth, Yossi Adi",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02398v1 Announce Type: cross \nAbstract: Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims."
      },
      {
        "id": "oai:arXiv.org:2504.02604v1",
        "title": "LinTO Audio and Textual Datasets to Train and Evaluate Automatic Speech Recognition in Tunisian Arabic Dialect",
        "link": "https://arxiv.org/abs/2504.02604",
        "author": "Hedi Naouara, Jean-Pierre Lorr\\'e, J\\'er\\^ome Louradour",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2504.02604v1 Announce Type: cross \nAbstract: Developing Automatic Speech Recognition (ASR) systems for Tunisian Arabic Dialect is challenging due to the dialect's linguistic complexity and the scarcity of annotated speech datasets. To address these challenges, we propose the LinTO audio and textual datasets -- comprehensive resources that capture phonological and lexical features of Tunisian Arabic Dialect. These datasets include a variety of texts from numerous sources and real-world audio samples featuring diverse speakers and code-switching between Tunisian Arabic Dialect and English or French. By providing high-quality audio paired with precise transcriptions, the LinTO audio and textual datasets aim to provide qualitative material to build and benchmark ASR systems for the Tunisian Arabic Dialect.\n  Keywords -- Tunisian Arabic Dialect, Speech-to-Text, Low-Resource Languages, Audio Data Augmentation"
      },
      {
        "id": "oai:arXiv.org:2501.00398v2",
        "title": "TSPE: Task-Specific Prompt Ensemble for Improved Zero-Shot Audio Classification",
        "link": "https://arxiv.org/abs/2501.00398",
        "author": "Nishit Anand, Ashish Seth, Ramani Duraiswami, Dinesh Manocha",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2501.00398v2 Announce Type: replace \nAbstract: Audio-language models (ALMs) excel in zero-shot audio classification, a task where models classify previously unseen audio clips at test time by leveraging descriptive natural language prompts. We introduce TSPE (Task-Specific Prompt Ensemble), a simple, training-free hard prompting method that boosts ALEs' zero-shot performance by customizing prompts for diverse audio classification tasks. Rather than using generic template-based prompts like \"Sound of a car\" we generate context-rich prompts, such as \"Sound of a car coming from a tunnel\". Specifically, we leverage label information to identify suitable sound attributes, such as \"loud\" and \"feeble\", and appropriate sound sources, such as \"tunnel\" and \"street\" and incorporate this information into the prompts used by Audio-Language Models (ALMs) for audio classification. Further, to enhance audio-text alignment, we perform prompt ensemble across TSPE-generated task-specific prompts. When evaluated on 12 diverse audio classification datasets, TSPE improves performance across ALMs by showing an absolute improvement of 1.23-16.36% over vanilla zero-shot evaluation."
      },
      {
        "id": "oai:arXiv.org:2410.15316v2",
        "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
        "link": "https://arxiv.org/abs/2410.15316",
        "author": "Alan Dao (Gia Tuan Dao), Dinh Bach Vu, Huy Hoang Ha",
        "published": "Fri, 04 Apr 2025 00:00:00 -0400",
        "summary": "arXiv:2410.15316v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models."
      }
    ]
  }
}